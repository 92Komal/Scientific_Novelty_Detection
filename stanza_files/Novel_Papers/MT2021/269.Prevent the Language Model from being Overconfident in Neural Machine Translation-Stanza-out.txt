title
Prevent the Language Model from being Overconfident in Neural Machine Translation
abstract
The Neural Machine Translation ( NMT ) model is essentially a joint language model conditioned on both the source sentence and partial translation .
Therefore , the NMT model naturally involves the mechanism of the Language Model ( LM ) that predicts the next token only based on partial translation .
Despite its success , NMT still suffers from the hallucination problem , generating fluent but inadequate translations .
The main reason is that NMT pays excessive attention to the partial translation while neglecting the source sentence to some extent , namely overconfidence of the LM .
Accordingly , we define the Margin between the NMT and the LM , calculated by subtracting the predicted probability of the LM from that of the NMT model for each token .
The Margin is negatively correlated to the overconfidence degree of the LM .
Based on the property , we propose a Margin- based Token - level Objective ( MTO ) and a Margin- based Sentencelevel Objective ( MSO ) to maximize the Margin for preventing the LM from being overconfident .
Experiments on WMT14 Englishto-German , WMT19 Chinese-to-English , and WMT14 English - to - French translation tasks demonstrate the effectiveness of our approach , with 1.36 , 1.50 , and 0.63 BLEU improvements , respectively , compared to the Transformer baseline .
The human evaluation further verifies that our approaches improve translation adequacy as well as fluency .
1
Introduction Neural Machine Translation ( NMT ) has achieved great success in recent years ( Sutskever et al. , 2014 ; Luong et al. , 2015 ; Vaswani et al. , 2017 ; Meng and Zhang , 2019 ; Zhang et al. , 2019a ; Yan et al. , 2020 b ) , which generates accurate and fluent translation through modeling the next word conditioned on both the source sentence and partial translation .
However , NMT faces the hallucination problem , i.e. , translations are fluent but inadequate to the source sentences .
One important reason is that the NMT model pays excessive attention to the partial translation to ensure fluency while failing to translate some segments of the source sentence ( Weng et al. , 2020 b ) , which is actually the overconfidence of the Language Model ( LM ) .
In the rest of this paper , the LM mentioned refers to the LM mechanism involved in NMT .
Many recent studies attempt to deal with the inadequacy problem of NMT from two main aspects .
One is to improve the architecture of NMT , such as adding a coverage vector to track the attention history ( Tu et al. , 2016 ) , enhancing the crossattention module ( Meng et al. , 2016 Weng et al. , 2020 b ) , and dividing the source sentence into past and future parts ( Zheng et al. , 2019 ) .
The other aims to propose a heuristic adequacy metric or objective based on the output of NMT .
Tu et al. ( 2017 ) and Kong et al . ( 2019 ) enhance the model 's reconstruction ability and increase the coverage ratio of the source sentences by translations , respectively .
Although some researches ( Tu et al. , 2017 ; Kong et al. , 2019 ; Weng et al. , 2020 b ) point out that the lack of adequacy is due to the overconfidence of the LM , unfortunately , they do not propose effective solutions to the overconfidence problem .
From the perspective of preventing the overconfidence of the LM , we first define an indicator of the overconfidence degree of the LM , called the Margin between the NMT and the LM , by subtracting the predicted probability of the LM from that of the NMT model for each token .
A small Mar-gin implies that the NMT might concentrate on the partial translation and degrade into the LM , i.e. , the LM is overconfident .
Accordingly , we propose a Margin- based Token - level Objective ( MTO ) to maximize the Margin .
Furthermore , we observe a phenomenon that if target sentences in the training data contain many words with negative Margin , they always do not correspond to the source sentences .
These data are harmful to model performance .
Therefore , based on the MTO , we further propose a Margin- based Sentence - level Objective ( MSO ) by adding a dynamic weight function to alleviate the negative effect of these " dirty data " .
We validate the effectiveness and superiority of our approaches on the Transformer ( Vaswani et al. , 2017 ) , and conduct experiments on large-scale WMT14 English-to - German , WMT19 Chinese-to-English , and WMT14 English - to - French translation tasks .
Our contributions are : ?
We explore the connection between inadequacy translation and the overconfidence of the LM in NMT , and thus propose an indicator of the overconfidence degree , i.e. , the Margin between the NMT and the LM .
?
Furthermore , to prevent the LM from being overconfident , we propose two effective optimization objectives to maximize the Margin , i.e. , the Margin- based Token - level Objective ( MTO ) and the Margin- based Sentence - level Objective ( MSO ) .
? Experiments on WMT14 English-to - German , WMT19 Chinese-to-English , and WMT14 English - to - French show that our approaches bring in significant improvements by + 1.36 , + 1.50 , +0.63 BLEU points , respectively .
Additionally , the human evaluation verifies that our approaches can improve both translation adequacy and fluency .
Background Given a source sentence x = {x 1 , x 2 , ... , x N } , the NMT model predicts the probability of a target sentence y = {y 1 , y 2 , ... , y T } word by word : P ( y|x ) = T t=1 p(y t |y <t , x ) , ( 1 ) where y <t = {y 1 , y 2 , ... , y t?1 } is the partial translation before y t .
From Eq. 1 , the source sentence x and partial translation y <t are considered in the meantime , suggesting that the NMT model is es-sentially a joint language model and the LM is instinctively involved in NMT .
Based on the encoder-decoder architecture , the encoder of NMT maps the input sentence x to hidden states .
At time step t , the decoder of NMT employs the output of the encoder and y <t to predict y t .
The training objective of NMT is to minimize the negative log-likelihood , which is also known as the cross entropy loss function : L N M T ce = ?
T t=1 log p(y t |y <t , x ) .
( 2 ) The LM measures the probability of a target sentence similar to NMT but without knowledge of the source sentence x : P ( y ) = T t=1 p(y t |y < t ) .
( 3 )
The LM can be regarded as the part of NMT decoder that is responsible for fluency , only takes y <t as input .
The training objective of the LM is almost the same as NMT except for the source sentence x : L LM ce = ?
T t=1 log p(y t |y < t ) .
( 4 )
The NMT model predicts the next word y t according to the source sentence x and meanwhile ensures that y t is fluent with the partial translation y < t .
However , when NMT pays excessive attention to translation fluency , some source segments may be neglected , leading to inadequacy problem .
This is exactly what we aim to address in this paper .
The Approach
In this section , we firstly define the Margin between the NMT and the LM ( Section 3.1 ) , which reflects the overconfidence degree of the LM .
Then we put forward the token - level ( Section 3.2 ) and sentencelevel ( Section 3.3 ) optimization objectives to maximize the Margin .
Finally , we elaborate our twostage training strategy ( Section 3.4 ) .
Margin between the NMT and the LM
When the NMT model excessively focuses on partial translation , i.e. , the LM is overconfident , the NMT model degrades into the LM , resulting in hallucinated translations .
To prevent the overconfidence problem , we expect that the NMT model outperforms the LM as much as possible in predicting golden tokens .
Consequently , we define the Margin between the NMT and the LM at the t-th time step by the difference of the predicted probabilities of them :
Note that sometimes , the model needs to focus more on the partial translation such as the word to be predicted is a determiner in the target language .
In this case , although small ?( t ) does not indicate the LM is overconfident , enlarging the ?( t ) can still enhance the NMT model .
?( t ) = p N M T (
Margin- based Token-level Objective Based on the Margin , we firstly define the Margin loss L M and then fuse it into the cross entropy loss function to obtain the Margin- based Tokenevel Optimization Objective ( MTO ) .
Formally , we define the Margin loss L M to maximize the Margin as follow : small , the NMT model is urgently to be optimized on the token thus the weight of M ( ? ( t ) ) should be enlarged .
Therefore , as the weight of M ( ? ( t ) ) , 1 ? p N M T ( t ) enables the model treat tokens wisely .
L M = T t=1 ( 1 ? p N M T ( t ) ) M ( ? ( t ) ) , ( 6 ) Variations of M ( ? ) .
We abbreviate Margin function M ( ? ( t ) ) as M ( ? ) hereafter .
A simple and intuitive definition is the Linear function : M ( ? ) = 1 ? ? , which has the same gradient for different ?.
However , as illustrated in Section 3.1 , different ? has completely various meaning and needs to be treated differently .
Therefore , we propose three non-linear Margin functions M ( ? ) as follows : ? Cube : ( 1 ? ? 3 ) /2 . ? Quintic ( fifth power ) : ( 1 ? ? 5 ) /2 . ? Log : 1 ? log ( 1 ? 1 + ? ) + 0.5 . where ? is a hyperparamater for Log .
As shown in Figure 1 , the four variations 3 have quite different slopes .
Specifically , the three nonlinear functions are more stable around ? = 0 ( e.g. , ? ? [?0.5 , 0.5 ] ) than Linear , especially Quintic .
We will report the performance of the four M ( ? ) concretely and analyze why the three non-linear M ( ? ) perform better than Linear in Section 5.4 .
Finally , based on L M , we propose the Marginbased Token - level Objective ( MTO ) : L T = L N M T ce + ? M L M , ( 7 ) where L N M T ce is the cross-entropy loss of the NMT model defined in Eq. 2 and ?
M is the hyperparameter for the Margin loss L M .
Source ? ? ? ? ? , ? ? ? ? .
Target
How did your mother succeed in keeping the peace between these two very different men ?
Expert Translation
Although they are twins , they are quite different in character .
Figure 2 : The parallel sentences , i.e. , the source and target sentences , are sampled from the WMT19 Chineseto - English training dataset .
We also list an expert translation of the source sentence .
The words in bold red have negative Margin .
This target sentence has more than 50 % tokens with negative Margin , and these tokens are almost irrelevant to the source sentence .
Apparently , the target sentence is a hallucination and will harm the model performance .
Margin- based Sentence-level Objective Furthermore , through analyzing the Margin distribution of target sentences , we observe that the target sentences in the training data which have many tokens with negative Margin are almost " hallucinations " of the source sentences ( i.e. , dirty data ) , thus will harm the model performance .
Therefore , based on MTO , we further propose the Marginbased Sentence - level Objective ( MSO ) to address this issue .
Compared with the LM , the NMT model predicts the next word with more prior knowledge ( i.e. , the source sentence ) .
Therefore , it is intuitive that when predicting y t , the NMT model should predict more accurately than the LM , as follow : p N M T (y t |y <t , x ) > p LM ( y t |y < t ) .
( 8 ) Actually , the above equation is equivalent to ?( t ) > 0 .
The larger ?( t ) is , the more the NMT model exceeds the LM .
However , there are many tokens with negative Margin through analyzing the Margin distribution .
We conjecture the reason is that the target sentence is not corresponding to the source sentence in the training corpus , i.e. , the target sentence is a hallucination .
Actually , we also observe that if a large proportion of tokens in a target sentence have negative Margin ( e.g. , 50 % ) , the sentence is probably not corresponding to the source sentence , such as the case in Figure 2 .
These " dirty " data will harm the performance of the NMT model .
To measure the " dirty " degree of data , we define the Sentence - level Negative Margin Ratio of parallel sentences ( x , y ) as follow : R( x , y ) = # { y t ? y : ?( t ) < 0 } # { y t : y t ? y} , ( 9 ) where # { y t ? y : ?( t ) < 0 } denotes the number of tokens with negative ?( t ) in y , and # { y t : y t ? y} is the length of the target sentence y. When R ( x , y ) is larger than a threshold k ( e.g. , k=50 % ) , the target sentence may be desperately inadequate , or even completely unrelated to the source sentence , as shown in Figure 2 .
In order to eliminate the impact of these seriously inadequate sentences , we ignore their loss during training by the Margin- based Sentence - level Objective ( MSO ) : L S = I R ( x , y ) < k ? L T , ( 10 ) where I R ( x , y ) < k is a dynamic weight function in sentence level .
The indicative function I R ( x , y ) < k equals to 1 if R ( x , y ) < k , else 0 , where k is a hyperparameter .
L T is MTO defined in Eq. 7 . I R ( x , y ) < k is dynamic at the training stage .
During training , as the model gets better , its ability to distinguish hallucinations improves thus I R ( x , y ) < k becomes more accurate .
We will analyze the changes of I R ( x , y ) < k in Section 5.4 .
Two -stage Training
We elaborate our two -stage training in this section , 1 ) jointly pretraining an NMT model and an auxiliary LM , and 2 ) finetuning the NMT model .
Jointly Pretraining .
The language model mechanism in NMT cannot be directly evaluated , thus we train an auxiliary LM to represent it .
We pretrain them together using a fusion loss function : L pre = L N M T ce + ? LM L LM ce , ( 11 ) where L N M T ce and L LM ce are the cross entropy loss functions of the NMT model and the LM defined in Eq. 2 and Eq. 4 , respectively .
?
LM is a hyperparameter .
Specifically , we jointly train them through sharing their decoders ' embedding layers and their pre-softmax linear transformation layers ( Vaswani et al. , 2017 ) .
There are two reasons for joint training : ( 1 ) making the auxiliary LM as consistent as possible with the language model mechanism in NMT ; ( 2 ) avoiding abundant extra parameters .
Finetuning .
We finetune the NMT model by minimizing the MTO ( L T in Eq. 7 ) and MSO ( L S in Eq. 10 ) .
4 Note that the LM is not involved at the inference stage .
Experimental Settings
We conduct experiments on three large-scale NMT tasks , i.e. , WMT14 English - to - German ( En? De ) , WMT14 English -to - French ( En? Fr ) , and WMT19 Chinese-to- English ( Zh?En ) .
Datasets .
For En? De , we use 4.5 M training data .
Following the same setting in ( Vaswani et al. , 2017 ) , we use newstest2013 as validation set and newstest2014 as test set , which contain 3000 and 3003 sentences , respectively .
For En?Fr , the training dataset contains about 36 M sentence pairs , and we use newstest2013 with 3000 sentences as validation set and newstest2014 with 3003 sentences as test set .
For Zh?En , we use 20.5 M training data and use newstest2018 as validation set and new-stest2019 as test set , which contain 3981 and 2000 sentences , respectively .
For Zh?En , the number of merge operations in byte pair encoding ( BPE ) ( Sennrich et al. , 2016a ) is set to 32 K for both source and target languages .
For En?De and En?Fr , we use a shared vocabulary generated by 32 K BPEs .
Evaluation .
We measure the case-sensitive BLEU scores using multi-bleu.perl 5 for En?De and En?Fr. For Zh?En , case-sensitive BLEU scores are calculated by Moses mteval - v13a.pl script 6 . Moreover , we use the paired bootstrap resampling ( Koehn , 2004 ) for significance test .
We select the model which performs the best on the validation sets and report its performance on the test sets for evaluation .
Model and Hyperparameters .
We conduct experiments based on the Transformer ( Vaswani et al. , 2017 ) and implement our approaches with the opensource tooklit Opennmt- py ( Klein et al. , 2017 ) .
Following the Transformer - Base setting in ( Vaswani et al. , 2017 ) , we set the hidden size to 512 and the encoder / decoder layers to 6 .
All three tasks are trained with 8 NVIDIA V100 GPUs , and the batch size for each GPU is 4096 tokens .
The beam size is 5 and the length penalty is 0.6 .
Adam optimizer ( Kingma and Ba , 2014 ) is used in all the models .
The LM architecture is the decoder of the Transformer excluding the cross-attention layers , sharing the embedding layer and the pre-softmax linear transformation with the NMT model .
For En?De , Zh?En , and En?Fr , the number of training steps is 150K for jointly pretraining stage and 150 K for finetuning 7 .
During pretraining , we set ?
LM to 0.01 for all three tasks 8 . Experimental results shown in Appendix A indicate that the LM has converged after pretraining for all the three tasks .
During finetuning , the Margin function M ( ? ) in Section 3.2 is set to Quintic , and we will analyze the four M ( ? ) in Section 5.4 . ?
M in Eq. 7 is set to 5 , 8 , and 8 on En?De , En?Fr and Zh?En , respectively .
For MSO , the threshold k in Eq. 10 is set to 30 % for En?De and Zh?En , 40 % for En?Fr
Results and Analysis
We first evaluate the main performance of our approaches ( Section 5.1 and 5.2 ) .
Then , the human evaluation further confirms the improvements of translation adequacy and fluency ( Section 5.3 ) .
Finally , we analyze the positive impact of our models on the distribution of Margin and explore how each fragment of our method works ( Section 5.4 ) .
Results on En?De
The results on WMT14 English - to - German ( En? De ) are summarized in Table 1 .
We list the results from ( Vaswani et al. , 2017 ) and several related competitive NMT systems by various methods , such as Minimum Risk Training ( MRT ) objective ( Shen et al. , 2016 ) , Simple Fusion of NMT and LM ( Stahlberg et al. , 2018 ) , optimizing adequacy metrics ( Kong et al. , 2019 ; Feng et al. , 2019 ) and improving the Transformer architecture Zheng et al. , 2019 ; Yang et al. , 2019 ; Weng et al. , 2020 b ; Yan et al. , 2020a ) .
We re -
System En?De ? Existing NMT systems Transformer ( Vaswani et al. , 2017 ) 27.3 - MRT *
( Shen et al. , 2016 ) 27.71 - Simple Fusion **
( Stahlberg et al. , 2018 ) 27.88 - Localness 28.11 - Context - Aware ( Yang et al. , 2019 ) 28.26 - AOL ( Kong et al. , 2019 ) 28.01 - Eval .
Module ( Feng et al. , 2019 ) 27.55 - Past &Future ( Zheng et al. , 2019 ) 28.10 - Dual ( Yan et al. , 2020a ) 27.86 - Multi-Task ( Weng et al. , 2020 b ) 28
Results on En?Fr and Zh?En
The results on WMT14 English - to - French ( En? Fr ) and WMT19 Chinese-to- English ( Zh?En ) are shown in Table 2 .
We also list the results of ( Vaswani et al. , 2017 ) and our reimplemented Transformer as the baselines .
On En?Fr , our reimplemented result is higher than the result of ( Vaswani et al. , 2017 ) , since we update 300 K steps while Vaswani et al . ( 2017 ) only update 100K steps .
Many studies obtain similar results to ours ( e.g. , 41.1 BLEU scores from ( Ott et al. , 2019 ) ) .
Compared with the baseline , NMT + LM yields + 0.07 and + 0.15 BLEU improvements on En?Fr and Zh?En , respectively .
The improvement of NMT +LM on En?De in Table 1 ( i.e. , + 0.75 ) is greater than these two datasets .
We conjecture the reason is that the amount of training data of En?De is much smaller than that of En?Fr and Zh?En , thus NMT + LM is more likely to improve the model performance on En?De. Compared with NMT + LM , our MTO achieves further improvements with + 0.42 and + 1.04 BLEU scores on En?Fr and Zh?En , respectively , which demonstrates the performance improvement is mainly due to our Margin - based objective rather than joint training .
Moreover , based on MTO , our MSO further yields + 0.14 and + 0.31 BLEU improvements .
In summary , our approaches improve up to + 0.63 and + 1.50 BLEU scores on En?Fr and Zh?
En compared with the baselines , respectively , which demonstrates the effectiveness and generalizability of our approaches .
Human Evaluation
We conduct the human evaluation for translations in terms of adequacy and fluency .
Firstly , we ran- domly sample 100 sentences from the test set of WMT19 Zh?En .
Then we invite three annotators to evaluate the translation adequacy and fluency .
Five scales have been set up , i.e. , 1 , 2 , 3 , 4 , 5 .
For adequacy , " 1 " means totally irrelevant to the source sentence , and " 5 " means equal to the source sentence semantically .
For fluency , " 1 " represents not fluent and incomprehensible ; " 5 " represents very " native " .
Finally , we take the average of the scores from the three annotators as the final score .
The results of the baseline and our approaches are shown in Table 3 . Compared with the NMT baseline , NMT + LM , MTO and MSO improve adequacy with 0.08 , 0.22 , and 0.37 scores , respectively .
Most improvements come from our Margin- based methods MTO and MSO , and MSO performs the best .
For fluency , NMT +LM achieves 0.2 improvement compared with NMT .
Based on NMT +LM , MTO and MSO yield further improvements with 0.01 and 0.05 scores , respectively .
Human evaluation indicates that our MTO and MSO approaches remarkably improve translation adequacy and slightly enhance translation fluency .
Table 4 : The percent of ? < 0 and average ? of models computed from the 100K sentence pairs introduced in Figure 3 . Compared with NMT + LM , both MTO and MSO effectively reduce the percent of ? < 0 and improve the average ?.
Analysis Margin between the NMT and the LM .
Firstly , we analyze the distribution of the Margin between the NMT and the LM ( i.e. , ? in Eq. 5 ) .
As shown in Figure 3 , for the joint training model NMT + LM , although most of the Margins are positive , there are still many tokens with negative Margin and a large amount of Margins around 0 .
This indicates that the LM is probably overconfident for many tokens , and addressing the overconfidence problem is meaningful for NMT .
By comparison , the Margin distribution of MSO is dramatically different with NMT + LM : the tokens with Margin around 0 are significantly reduced , and the tokens with Margin in [ 0.75 , 1.0 ] are increased apparently .
More precisely , we list the percentage of tokens with negative Margin and the average Margin for each model in Table 4 . Compared with NMT + LM , MTO and MSO reduce the percentage of negative Margin by 2.28 and 1.56 points , respectively .
We notice MSO performs slightly worse than MTO , because MSO neglects the hallucinations during training .
As there are many tokens with negative Margin in hallucinations , the ability of MSO to reduce the proportion of ? < 0 is weakened .
We further analyze effects of MTO and MSO on the average of Margin .
Both MTO and MSO improve the average of the Margin by 33 % ( from 0.33 to 0.44 ) .
In conclusion , MTO and MSO both indeed increase the Margin between the NMT and the LM. Variations of M ( ? ) .
We compare the performance of the four Margin functions M ( ? ) defined in Section 3.2 .
We list the BLEU scores of the Transformer baseline , NMT +LM and our MTO approach with the four M ( ? ) in Table 5 .
All the four variations bring improvements over NMT and NMT +LM .
The results of Log with different ? are similar to Linear , while far lower than Cube and Quintic .
And Quintic performs the best among all the four variations .
We speculate the reason is that ? ? [?0.5 , 0.5 ] is the main range for improvement , and Quintic updates more careful on this range ( i.e. , with smaller slopes ) as shown in Figure 1 .
Effects of the Weight of M ( ? ) .
In MTO , we propose the weight 1 ?p N M T ( t ) of the Margin function M ( ? ) in Eq. 6 .
To validate the importance of it , we remove the weight and the Margin loss degrades to L M = T t=1 M ( ? ( t ) ) .
The results are listed in Table 6 . Compared with NMT + LM , MTO without weight performs worse with 0.25 and 0.05 BLEU decreases on the validation set and test set , respectively .
Compared with MTO with weight , it decreases 0.73 and 1.09 BLEU scores on the validation set and test set , respectively .
This demonstrates that the weight 1 ? p N M T ( t ) is indispensable for our approach .
Changes of I R( x , y ) <k During Training .
In MSO , we propose a dynamic weight function I R ( x , y ) < k in Eq. 10 .
Figure 4 shows the changes of I R ( x , y ) < k in MSO and the BLEU scores of MSO and MTO during finetuning .
As the training continues , our model gets more competent , and the proportion of sentences judged to be " dirty data " by our model increases rapidly at first and then Case Study .
To better illustrate the translation quality of our approach , we show several translation examples in Appendix C .
Our approach grasps more segments of the source sentences , which are mistranslated or neglected by the Transformer .
Related Work Translation Adequacy of NMT .
NMT suffers from the hallucination and inadequacy problem for a long time ( Tu et al. , 2016 ; M?ller et al. , 2020 ; Wang and Sennrich , 2020 ; Lee et al. , 2019 ) .
Many studies improve the architecture of NMT to alleviate the inadequacy issue , including tracking translation adequacy by coverage vectors ( Tu et al. , 2016 ; Mi et al. , 2016 ) , modeling a global representation of source side ( Weng et al. , 2020a ) , dividing the source sentence into past and future parts ( Zheng et al. , 2019 ) , and multi-task learning to improve encoder and cross-attention modules in decoder ( Meng et al. , 2016 Weng et al. , 2020 b ) .
They inductively increase the translation adequacy , while our approaches directly maximize the Margin between the NMT and the LM to prevent the LM from being overconfident .
Other studies enhance the translation adequacy by adequacy metrics or additional optimization objectives .
Tu et al . ( 2017 ) minimize the difference between the original source sentence and the reconstruction source sentence of NMT .
Kong et al. ( 2019 ) pro-pose a coverage ratio of the source sentence by the model translation .
Feng et al. ( 2019 ) evaluate the fluency and adequacy of translations with an evaluation module .
However , the metrics or objectives in the above approaches may not wholly represent adequacy .
On the contrary , our approaches are derived from the criteria of the NMT model and the LM , thus credible .
Language Model Augmented NMT .
Language Models are always used to provide more information to improve NMT .
For low-resource tasks , the LM trained on extra monolingual data can rerank the translations by fusion ( G?lc ? ehre et al. , 2015 ; Sriram et al. , 2017 ; Stahlberg et al. , 2018 ) , enhance NMT 's representations ( Clinchant et al. , 2019 ; Zhu et al. , 2020 ) , and provide prior knowledge for NMT ( Baziotis et al. , 2020 ) .
For data augmentation , LMs are used to replace words in sentences ( Kobayashi , 2018 ; Gao et al. , 2019 ) .
Differently , we mainly focus on the Margin between the NMT and the LM , and no additional data is required .
Stahlberg et al. ( 2018 ) propose the Simple Fusion approach to model the difference between NMT and LM .
Differently , it is trained to optimize the residual probability , positively correlated to p N M T /p LM which is hard to optimize and the LM is still required in inference , slowing down the inference speed largely .
Data Selection in NMT .
Data selection and data filter methods have been widely used in NMT .
To balance data domains or enhance the data quality generated by back - translation ( Sennrich et al. , 2016 b ) , many approaches have been proposed , such as utilizing language models ( Moore and Lewis , 2010 ; van der Wees et al. , 2017 ; Zhang et al. , 2020 ) , translation models ( Junczys - Dowmunt , 2018 ; Wang et al. , 2019a ) , and curriculum learning ( Zhang et al. , 2019 b ; Wang et al. , 2019 b ) .
Different from the above methods , our MSO dynamically combines language models with translation models for data selection during training , making full use of the models .
Conclusion
We alleviate the problem of inadequacy translation from the perspective of preventing the LM from being overconfident .
Specifically , we firstly propose an indicator of the overconfidence degree of the LM in NMT , i.e. , Margin between the NMT and the LM .
Then we propose Margin- based Token-level and Sentence-level objectives to maximize the Margin .
Experimental results on three large-scale translation tasks demonstrate the effectiveness and superiority of our approaches .
The human evaluation further verifies that our methods can improve translation adequacy and fluency .
