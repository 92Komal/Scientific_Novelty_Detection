title
Tencent AI Lab Machine Translation Systems for the WMT21 Biomedical Translation Task
abstract
This paper describes the Tencent AI Lab submission of the WMT2021 shared task on biomedical translation in eight language directions : English -German , English - French , English -Spanish and English -Russian .
We utilized different Transformer architectures , pre-training and back -translation strategies to improve the translation quality .
Concretely , we explore mBART to demonstrate the effectiveness of the pretraining strategy .
Our submissions ( Tencent AI Lab Machine Translation , TMT ) in German / French / Spanish ?
English are ranked 1st respectively according to the official evaluation results in terms of BLEU scores .
Introduction
This paper describes the Tencent AI Lab submission of the WMT2021 shared task on biomedical translation .
Last year , we participated in three translation tasks : News ( Wu et al. , 2020 ) , Chat ( Wang et al. , 2020a ) , and Biomedical ( Wang et al. , 2020 b ) .
In biomedical translation , we adopt DEEP TRANSFORMER ( Dou et al. , 2018 ( Dou et al. , , 2019 , HYBRID TRANSFORMER ( Hao et al. , 2019 ) and DATA REJUVENATION 1 ( Jiao et al. , 2020 ) .
This year , we participated in eight language directions : English - German ( En- De ) , English - French ( En- Fr ) , English - Spanish ( En-Es ) and English -Russian ( En - Ru ) in the biomedical translation .
In this paper , we also apply the pre-train and fine-tune paradigm for the biomedical translation task .
The pre-train model is first trained on the the large-scale monolingual data in a self-supervised manner , then is fine-tuned on downstream bilingual data .
Specifically , we adopt the encoder-decoder pre-trained model mBART to implement the pre-training strategy .
1 https://github.com/wxjiao/ Data-Rejuvenation
The rest of this paper is organized as below .
Section 2 presents our system : Transformer and pretrained model mBART .
Section 3 describes the training and validation data used in our system .
Section 4 reports experimental results in the participated eight language directions .
Finally , we conclude our work in Section 5 .
System
Our systems are implemented with Transformer ( Vaswani et al. , 2017 ) and the pre-trained model mBART .
The training details of theses models are described in Section 4 .
Transformer
We adopt the BIG and LARGE Transformer models used in the previous year ( Wang et al. , 2020 b ) as the basic Transformer models .
BIG and LARGE Transformer models contain 6 - layer and 20 - layer encoders with TRANSFORMER - BIG setting ( Vaswani et al. , 2017 ) , respectively .
Pre-train Model
For the sequence- to- sequence pre-training , we adopt mBART25 as the pre-train model for our experiments , which consists of 12 encoder and decoder layers with the default size of hidden state is 1024 .
The model is pre-trained with the denoising objective on the large-scale monolingual data and is fine-tuned on the downstream tasks .
mBART has achieved significant improvements on many low resource language paris .
Data
In this section , we present the training and validation data used in our system .
Besides the in- domain data provided by organisers , we collect the out-of- domain bilingual data from WMT news translation shared task .
?
En-De : Europarl - v10 2 , Common Crawl corpus 3 , ParaCrawl 4 , News Commentary - v15 5 and Wiki Titles - v2 6 .
En- De ? En-Fr : Europarl - v7 7 , Common Crawl corpus , News Commentary 8 , English - French Giga Corpus 9 . ? En-Es : Europarl - v7 10 , Common Crawl corpus , News Commentary 11 , ParaCrawl 12 . ? En-Ru : Common Crawl corpus , News Commentary 13 , ParaCrawl 14 , Yandex Corpus 15 , Wiki Titles - v2 , Back-translated news 16 . For the validation data , we use the Khresmoi development data 17 ( En- De , En-Fr , En-Es ) as the validation sets .
We also use the HimL test sets 2015 and 2017 18 to enlarge the En- De validation set .
For En-Ru , we randomly sample 4000 examples from the training data as the validation set .
The statistics of the in-domain and out-ofdomain training data and the validation data are listed in Table 1 .
To enlarge the in- domain bilingual corpus , we follow Wang et al . ( 2020 b ) to adopt backtranslation method to generate synthetic bilingual sentence pairs .
For English - X pair , we train a English -X LARGE model on the combination of indomain and out - of- domain data , and use the model to generate synthetic bilingual data .
We also collect the En- Ru bilingual biomedical data ( about 1.0 M sentence pairs ) from Internet as the in-domain data .
In this work , all corpora are tokenized by sentence - piece ( Kudo and Richardson , 2018 ) model 19 without any pre-processing procedures .
Experiments
For the corpus filtering , we follow Wang et al . ( 2020 b ) to filter duplicate sentence pairs ( Khayrallah and Koehn , 2018 ) , sentence pairs with wrong language ( Khayrallah and Koehn , 2018 ) or length problem ( Ott et al. , 2018 ) .
For the synthetic bilingual data generation , we adopt iterative knowledge distillation ( Li et al. , 2019 ) to improve the translation quality .
Our iterative knowledge distillation is performed with 3 BIG Transformer teachers and 3 iterations .
We also try to use the Right- to - Left ( R2L ) training ( Wu et al. , 2020 ) but fail in achieving significant improvements on the test sets .
We follow Wang et al . ( 2020 b ) to train the BIG and LARGE Transformer models .
Specifically , we first use the combination of the out-of- domain data and the in-domain data to train the teacher model .
Then we use the teacher model to generate the synthetic bilingual data .
Finally , we train the student model on the combination of the synthetic and real bilingual data ( Jiao et al. , 2021 ) .
The learning rate is set to 0.0007 .
All models are trained for 600K steps on 8 Tesla V100 GPUs where each is allocated with a batch size of 8192 tokens .
For the pre-train model , we adopt the publicly available mBART25 20 model and fine- tune the mBART25 on the in-domain data .
In the finetuning phase , we minimize the label smoothed cross entropy with the smoothing factor of 0.2 .
We use the Adam ( Kingma and Ba , 2015 ) optimizer with ?
1 = 0.9 , ? 2 = 0.98 , and = 1e?6 .
The learning rate is scheduled to increase from 0 to the maximum value in the warm - up phase and decreases linearly to 0 in the remaining steps .
The dropout rate is 0.3 for each residual connection and 0.1 for attention matrices .
System
We carry out ablation study on De?En transla-tion task .
The results are shown in Table 4 .
The in- domain data improves the baseline Transformer - Big model with 0.42 BLEU point .
We then apply the Data Rejuvenation , Back-translation and model ensemble strategies and achieve the further improvement .
We adopt the In-domain Data , Data Rejuvenation , Back-translation as the default setting and apply the setting to Transformer - Big and Transformer - Large models on the eight language directions .
We train 5 BIG and 5 LARGE Transformer models with different random seeds initialization .
With the trained models , we employ the model ensemble strategy with the greedy based ensemble ( Li et al. , 2019 ; Wu et al. , 2020 ) to get the final translation outputs .
For model inference , the length penalty is set to 0.6 and the beam size is set to 4 .
Translation results are reported in term of BLEU score in Table 2 and Table 3 .
From the tables , we find that 1 ) utilizing different Transformer architectures , pretraining and back -translation strategies achieve strong performance on the De?En , En?Fr and Es?
En translation tasks .
2 ) the lack of the large-scale in- domain data makes our En- Ru NMT system significantly lower than the state- ofthe - art systems , demonstrating that the in-domain data plays a critical role in the development of NMT system .
System En Post-process
We find that several long sentences exist in the 2021 test sets , which pose a great challenge for our NMT system .
Take the following two sentences for example : Sentence 6 in doc73 in medline_fr2en_fr.txt : " Nous avons constat ?
que : ( i ) malgr ?
le fardeau de plus en plus lourd des maladies non transmissibles , nombre de pays ? faible et moyen revenu ne poss ?
daient pas les fonds suffisants pour assurer des services de pr?vention ; ( ii ) les professionnels de sant ?
au sein des communaut ?s manquaient fr?quemment de ressources , de soutien et de formation ; ( iii ) les frais non remboursables d?passaient 40 % des d?penses de sant ?
dans la moiti ?
des pays ?tudi?s , ce qui entra?ne des in?galit?s ; et enfin , ( iv ) les r?gimes d'assurance maladie ? taient entrav ?s par la fragmentation des syst ?
mes publics et priv?s , le sous-financement , la corruption et la pi?tre mobilisation des travailleurs informels . "
Sentence 3 in doc27 in medline_es2en_es.txt : " Este art ?
culo tiene como objeto el an? lisis de los ensayos cl?nicos que permitieron dicha autorizaci ?n , as ?
como la revisi ? n de nuevas terapias para el tratamiento del carcinoma urotelial localmente avanzado o metast?sico .
M?TODO : B?squeda bibliogr?fica realizada en Pub-Med y ClinicalTrials .gov mediante la combinaci ?n de las palabras clave , en espa?ol e ingl ?s : " carcinoma urotelial " , " c?ncer de vejiga " , " localmente avanzado " , " metast?sico " , " inmunoterapia " , " CTLA - 4 " , " PD1 " , " PDL - 1 " , " atezolizumab " , " nivolumab " , " ipilimubab " , " pembrolizumab " , " avelumab " , " durvalumab " , " tremelimumab " , " terapia antiangiog?nica " , " terapia molecular dirigida " e " inhibidores VEGF " . "
To address the problem , we manually split the long sentences into multiple sentences , and use the splitted ones as the system input to perform the translation .
We also find our system may generate wrong translations for the very short input sentences , e.g. , " R?SUM ? : " ( Sentence 1 in doc92 in med-line_fr2en_fr.txt ) , " ( " ( Sentence 4 in doc11 in med-line_es2en_es.txt ) .
To overcome the problem , we extract the target translation from the SMT phrase table and use it as the final translation output , as the NMT and SMT models are identical in modeling the bilingual knowledge ( He et al. , 2020 ) .
Official Results
The official automatic evaluation results of our submissions for WMT 2021 biomedical translation task are shown in Table 5 .
Our final systems in German / French / Spanish ?
English are ranked 1st respectively , in terms of BLEU score .
Conclusion
In this paper , we present Tencent AI Lab machine translation systems for the WMT21 biomedical translation shared task .
we participated in eight language directions : English - German ( En- De ) , English - French ( En- Fr ) , English - Spanish ( En-Es ) and English -Russian ( En - Ru ) .
Our systems German / French / Spanish ?
English are ranked 1st according to the official evaluation results in terms of BLEU scores .
It is worth mentioning that most advanced technologies reported in this paper are also adapted to our systems for news translation task ( Wang et al. , 2021 ) , which achieve the 1st rank in Chinese ?
English task .
In the future , we plan to explore Non-Autoregressive machine Translation ( NAT ) models to improve the system performance ( Zhou et al. , 2020 ; Ding et al. , 2020 ; Hao et al. , 2021 ) Table 1 : 1 En- Fr En-Es En-Ru
The detailed statistics of training and validation data used in our system .
Out-of-domain 37.8M 28.0M 30.3M 92.0M In-domain 2.5M 3.5M 1.6 M 43.0 K Validation set 9.8K 1.5K 1.5 K 4.0K
