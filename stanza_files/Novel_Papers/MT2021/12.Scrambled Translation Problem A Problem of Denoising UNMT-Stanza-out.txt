title
Scrambled Translation Problem : A Problem of Denoising UNMT
abstract
In this paper , we identify an interesting kind of error in the output of Unsupervised Neural Machine Translation ( UNMT ) systems like Undreamt 1 . We refer to this error type as Scrambled Translation problem .
We observe that UNMT models which use word shuffle noise ( as in case of Undreamt ) can generate correct words , but fail to stitch them together to form phrases .
As a result , words of the translated sentence look scrambled , resulting in decreased BLEU .
We hypothesise that the reason behind scrambled translation problem is ' shuffling noise ' which is introduced in every input sentence as a denoising strategy .
To test our hypothesis , we experiment by retraining UNMT models with a simple retraining strategy .
We stop the training of the Denoising UNMT model after a pre-decided number of iterations and resume the training for the remaining iterations - which number is also pre-decided - using original sentence as input without adding any noise .
Our proposed solution achieves significant performance improvement UNMT models that train conventionally .
We demonstrate these performance gains on four language pairs , viz. , English - French , English -German , English - Spanish , Hindi-Punjabi .
Our qualitative and quantitative analysis shows that the retraining strategy helps achieve better alignment as observed by attention heatmap and better phrasal translation , leading to statistically significant improvement in BLEU scores .
Introduction
Training a machine translation system using only the monolingual corpora of the two languages was successfully demonstrated by ( Artetxe et al. , 2018c ; Lample et al. , 2018 ) .
They train the machine translation system using denoising auto-encoder ( DAE ) and backtranslation ( BT ) iteratively .
Recently , pre-training of large language models ( Conneau and Lample , 2019 ; Song et al. , 2019 ; Liu et al. , 2020 ) using monolingual corpus is used to initialize the weights of the encoder-decoder models .
These encoder-decoder models are later fine-tuned using backtranslated sentences for the task of Unsupervised Neural Machine Translation ( UNMT ) .
While we appreciate language model ( LM ) pre-training to better initialise the models , it is important to understand the shortcomings of earlier approaches .
In this paper , we explore in this direction .
We observe that the translation quality of undreamt models ( Artetxe et al. , 2018 c ) suffers partially due to wrong positioning of the target words in the translated sentence .
For many instances , though the reference sentence and its corresponding generated sentence are formed with almost the same set of words , the sequence of words is different resulting in the sentence being ungrammatical and / or loss of meaning .
This results in a difference in syntax and semantic rules .
We define such generated sentences as scrambled sentences and the problem as scramble translation problem .
Scrambled sentences can be either disfluent or fluent- but- inadequate .
Here , if the LM decoder is not learnt well , we observe disfluent translations .
If the LM decoder is learnt well , we observe fluent- but - inadequate translations .
An example of fluent- but - inadequate translation will be ' leaving better kids for our planet ' instead of ' leaving better planet for our kids ' .
Due to this phenomenon , during BLEU computation n-gram matching lessens , for n >
1 .
However , this error is absent in translation generated from recent state - of- the - art systems ( Conneau and Lample , 2019 ; Song et al. , 2019 ; Liu et al. , 2020 ) .
We hypothesise , DAE introduces uncertainty to the previous UNMT ( Lample et al. , 2018 ; Artetxe et al. , 2018 c Artetxe et al. , , 2019
Wu et al. , 2019 ) models , specifically to the encoders .
It has been observed that encoders are sensitive to the exact ordering of the input sequence ( Michel and Neubig , 2018 ; Murthy V et al. , 2019 ; Ahmad et al. , 2019 ) .
By performing random word-shuffle in all the source sentences , encoder may lose important information about the sentence composition .
The DAE fails to learn informative representation which affects the decoder resulting in wrong translations generated .
If our hypothesis is true , retraining these previous UNMT system models with noise - free sentences as input should resolve the problem for previous systems ( Artetxe et al. , 2018c ; Lample et al. , 2018 ) .
Moreover , using this retraining strategy will not benefit recent approaches ( Conneau and Lample , 2019 ; Song et al. , 2019 ) as they do not shuffle words of input sentence while training with back - translated data .
In this paper , we prove our hypothesis by showing that a simple retraining strategy mitigates the ' scrambled translation problem ' .
We observe consistent improvements in BLEU score and word-alignment over the denoising UNMT approach by Artetxe et al . ( 2018 c ) for four language pairs .
We do not wish to beat the state - of- the - art UNMT systems with pre-training , instead , we demonstrate a limitation of previous denoising UNMT ( Artetxe et al. , 2018c ; Lample et al. , 2018 ) systems and prove why it happens .
Related Work Neural machine translation ( NMT ) ( Cho et al. , 2014 ; Sutskever et al. , 2014 ; Bahdanau et al. , 2015 ) typically needs lot of parallel data to be trained on .
However , parallel data is expensive and rare for many language -pairs .
To solve this problem , unsupervised approaches to train machine translation ( Artetxe et al. , 2018c ; Lample et al. , 2018 ; Yang et al. , 2018 ) was proposed in the literature which uses only monolingual data to train a translation system .
Artetxe et al. ( 2018 b ) and Lample et al . ( 2018 ) introduced denoising - based U - NMT which utilizes cross-lingual embeddings and trains a RNN - based encoder- decoder model ( Bahdanau et al. , 2015 ) .
Architecture proposed by Artetxe et al . ( 2018 c ) contains a shared encoder and two language -specific decoders while architecture proposed by Lample et al . ( 2018 ) contains a shared encoder and a shared decoder .
In the approach by Lample et al . ( 2018 ) , the training starts with word- by - word translation followed by denoising and backtranslation .
Here , noise in the input sentences in the form of shuffling of words and deletion of random words from sentences was performed .
Conneau and Lample ( 2019 ) ( XLM ) proposed a two -stage approach for training a UNMT system .
The pre-training phase involves training of the model on the combined monolingual corpora of the two languages using Masked Language Modelling ( MLM ) objective ( Devlin et al. , 2019 ) .
The pre-trained model is later fine-tuned using denoising auto-encoding objective and backtranslated sentences .
Song et al. ( 2019 ) proposed a sequence to sequence pre-training strategy .
Unlike XLM , the pre-training is performed via MAsked Sequence to Sequence ( MASS ) objective .
Here , random ngrams in the input is masked and the decoder is trained to generate the missing ngrams in the pre-training phase .
The pre-trained model is later fine-tuned using backtranslated sentences .
demonstrated that LSTM encoders of the NMT system are sensitive to the word-ordering of the source language .
They considered the scenario of zero-shot translation from language l 3 to l 2 .
They train a NMT system for l 1 ? l 2 languages and use l 1 -l 3 languages bilingual embeddings .
This enables the trained model to perform zero-shot translation from l 3 ? l 2 .
However , if the word-order of the languages l 1 and l 3 are different , the translation quality from l 1 -l 3 is hampered .
Michel and Neubig ( 2018 ) have also made a similar observation albeit in the monolingual setting .
They observe that accuracy of the machine translation system gets adversely affected due to noise in the input sentences .
They discuss various sources of noise with one of them being word emission / insertion / repetition or grammatical errors .
The lack of robustness to such errors could be attributed to the sequential processing of LSTM or Transformer encoders .
As the encoder processes the input as a sequence and generates encoder representation at each time-step , such errors would lead to bad encoder representations resulting in bad translations generated .
Similar observations have also been made by Ahmad et al . ( 2019 ) for cross-lingual transfer of dependency parsing .
They observe that self-attention encoder with relative position representations is more robust to word-order divergence and enable better cross-lingual transfer for dependency parsing task compared to RNN encoders .
Here , shuffling is performed by swapping neighboring words l/2 times , where l is the number of words in the sentence .
Baseline Approach
For completeness , we also experimented with XLM UNMT ( Conneau and Lample , 2019 ) with initialise the model with MLM objective followed by finetuning it with DAE and BT iteratively .
In this approach , they do not add noise with the input sentence while training with backtranslated data .
Proposed Retraining Strategy
Our proposed strategy to train a denoising - based UNMT system consists of two phases .
In the first phase , we proceed with training using denoised sentences similar to the baseline system ( Artetxe et al. , 2018 c ) for M number of iterations .
Adding random shuffling in the input side , however , could introduce uncertainty to the model leading to inconsistent encoder representations .
To overcome this , in the second phase , we retrain the model with simple AE and on - the-fly BT using sentences with the correct ordering of words for ( N - M ) iterations as shown in Fig.
2 . Here , N is the total number of iterations and M < N .
More concretely , this training approach consists of 4 more sub-processes other than the 4 subprocesses of the baseline system .
These are : ( v) AE src : Auto-encoding of source sentences in which we train shared - encoder , source - decoder , and attention .
( vi ) AE trg : Auto-encoding of target sentences in which we train shared - encoder , target - decoder , and attention .
( vii ) BT src : Training shared - encoder , target - decoder , and attention with back - translated source sentences as input and actual target sentences as output .
( viii ) BT trg :
Training shared - encoder , source-decoder , and attention with back - translated target sentences as input and actual source sentences as output .
The second phase ensures that the encoder learns to generate context representation with information about the correct ordering of words .
For XLM ( Conneau and Lample , 2019 ) , we add these 4 subprocesses only with fine-tuning step .
We do not change anything in LM pretraining step .
Experimental Setup
We test our hypothesis with undreamt as a previous approach and XLM as a SOTA approach .
We applied our retraining strategy on both the approaches and observed the result .
For undreamt , we have used monolingual data of six languages , i.e. English ( en ) , French ( fr ) , German ( de ) , Spanish ( es ) , Hindi ( hi ) , and Punjabi ( pa ) .
Among these languages , Hindi and Punjabi are of SOV word-order where the other four languages are of SVO word order .
In our experiments , we choose language - pairs such that the word-order of source language matches with that of target language .
We have used the NewsCrawl corpora for en , fr , de of WMT14 , and for es of WMT13 .
For hi-pa , we use Wikipedia dumps of the august 2019 snapshot for training .
The en-fr and en-de models are tested using WMT14 test- data and en-es models using WMT13 test-data , and hi-pa models using ILCI test data ( Jha , 2010 ) .
We have preprocessed the corpus for normalization , tokenization and lowercasing using the scripts available in Moses ( Koehn et al. , 2007 ) and Indic NLP Library ( Kunchukuttan , 2020 ) , for BPE segmentation using subword - NMT ( Sennrich et al. , 2016 ) with number of merge operations set to 50k .
We use the monolingual corpora to independently train the embeddings for each language using skip-gram model of word2vec ( Mikolov et al. , 2013 ) .
To map embeddings of two languages to a shared space , we use Vecmap 2 by Artetxe et al . ( 2018a ) .
We use undreamt 3 tool to train the UNMT system proposed by Artetxe et al . ( 2018 c ) .
We train the baseline model untill convergence and noted the number of steps N required to reach convergence .
We now train our proposed system for N/2 steps and re-train the model after removing denoising noise for the remaining N/2 steps .
They converge between 500k to 600k steps depending on the language pairs .
Further details of dataset and network parameters are available in Appendix .
We also report results on XLM 4 approach ( Conneau and Lample , 2019 ) .
XLM employs two -stage training of UNMT model .
The pre-training stage trains encoder and decoder with masked language modeling objective .
The retraining stage employs denoising along with iterative back -translation .
However , XLM uses a different denoising ( word shuffle ) mechanism compared to Artetxe et al . ( 2018 c ) .
We replace the denoising mechanism by Conneau and Lample ( 2019 ) with the denoising mechanism used by Artetxe et al . ( 2018 c ) .
We use the pre-trained models for English - French , English - German , and English -Romanian provided by Conneau and Lample ( 2019 ) .
We retrain the XLM model until convergence using the denoising approach which makes the baseline system .
We later retrain the pre-trained XLM model using our proposed approach where we remove the denoising component after N/2 steps .
We report both BLEU scores and n-gram BLEU scores using multi-bleu.perl of Moses .
We have tested statistical significance of BLEU improvements ( Koehn , 2004 ) .
To analyse the systems , we have produced heatmaps of attention generated by the models .
Table 1 : The Translation performance using the Baseline approach and our Approach .
Trained for a total of N iterations for all approaches .
Undreamt and XLM results are results from our replication using the code provided by the authors .
? indicates statistically significant improvements using paired bootstrap re-sampling ( Koehn , 2004 ) for a p-value less than 0.05 .
Language ( a ) English ? French ( b) French ? English Figure 3 : Change in translation accuracy using undreamt- baseline vs. our approach with increasing number of iterations for English - French ( BLEU scores reported ) .
Results and Analysis Table 1 reports BLEU score of the trained models using the undreamt ( Artetxe et al. , 2018 c ) and XLM ( Conneau and Lample , 2019 ) and retraining them with our approach .
Undreamt and XLM results are results from our replication using the code provided by the authors .
In Table 1a we observe that the proposed re-training strategy of AE used in conjunction with BT results in statistically significant improvements ( p- value < 0.05 ) across all language pairs when compared to the undreamt baseline approach ( Artetxe et al. , 2018 c ) .
We report results on XLM ( Conneau and Lample , 2019 ) with our retraining approach in Table 1 b . XLM is one of the state - of- the- art ( SOTA ) UNMT approaches for these language pairs .
The approach by XLM ( Conneau and Lample , 2019 ) does not add noise to the input backtranslated sentence during training .
Therefore , our retraining strategy does not benefit here .
( a) English ? Spanish ( b) Spanish ? English Figure 5 : Change in translation accuracy using undreamt- baseline vs. our approach with increasing number of iterations for English - Spanish ( BLEU scores reported ) .
Table 2 : Improvements in n-BLEU ( represented in % ) on using our approach over baseline for en-fr , en-de , en-es , hi-pa test sets .
Figure 6 : Change in translation accuracy using undreamt- baseline vs. our approach with increasing number of iterations for Hindi-Punjabi ( BLEU scores reported ) .
Language ? BLEU -1 ? BLEU -2 ? BLEU -3 ? BLEU - German der us-senat genehmigte letztes jahr ein 90 millionen dollar teures pilotprojekt , das 10.000 autos umfasst h?tte .
English reference the u . s . senate approved a $ 90 - million pilot project last year that would have involved about 10,000 cars .
Artetxe et al. 2018 the u . s . district of the last $ 90 million a year , it would have 10,000 cars .
Our approach the u . s . district last year approved 90 million initiative that would have included 10,000 cars .
Spanish el anuncio del probable descubrimiento del bos ? n de higgs gener ?
una gran conmoci?n el verano pasado , y con raz ? n . English reference the announcement of the probable discovery of the higgs boson created quite a stir last summer , and with good reason .
Artetxe et al. 2018 the likely announcement of the discovery of the higgs boson triggered a major shock last summer , and with reason .
Our approach the announcement of the likely discovery of the higgs boson generated a major shock last summer , and with reason .
We also observe robustness of the pre-trained language models to the scrambled translation problem .
In India , China and other countries , other people work from fifteen to one .
Our approach en inde , en chine et de nombreux autres pays , les gens travaillent quinze ? douze heures un jour .
( Google translation )
In India , China and many other countries , people work fifteen to twelve hours a day . increasing number of iterations on test-data .
We observe that our proposed approach leads to increase in BLEU score in the re-training phase as the denoising strategy is removed .
The baseline system suffers from drop in BLEU score due to denoising strategy introducing ambiguity into the model .
Quantitative analysis
We hypothesize that the baseline UNMT model using DAE is able to generate correct word translation but fails to stitch them together to generate phrases .
To validate the hypothesis , we calculate the percentage improvement on using our approach over the baseline system in terms of individual n-gram ( n= 1,2,3,4 ) specific BLEU scores for each language - pair and a particular value of n .
The results presented in Table 2 indicate that our method achieves higher improvements in n-gram BLEU for higher n?grams ( n > 1 ) compared to the improvement in n-gram BLEU for lower values of n , indicating better phrasal matching .
This could be attributed to the proposed approach not suffering from the scrambled translation problem introduced by the DAE .
Qualitative analysis
We observe several instances where our proposed approach results in better translations compared to the baseline .
On manual analysis of translation outputs generated by the baseline system , we have found out some instances of scrambled translation problem .
Due to uncertainty introduced by shuffling of words before training , the baseline model chooses to generate sentences that are more acceptable by a language model .
Fig 7 shows such an example in our test data .
Here , two German phrases ' ein 90 millionen ' ( ' a 90 million ' ) and ' letztes jahr ' ( ' last year ' ) are mixed up and translated as ' last $ 90 million a year ' in English .
However , our approach handled the issue correctly .
Fig 8 shows an example of a situation where the baseline model prefers to generate a word in multiple probable positions .
Here , the source Punjabi sentence consists of a phrase ' jAM phira ' ( ' or ' ) meaning ' yA phira ' ( ' or ' ) in Hindi .
In the translation produced by the baseline model , the correct phrase is generated along with the word ' phira ' wrongly occurring again forming another phrase ' phira se ' ( ' again ' ) .
Note that , both the phrases are commonly used in Hindi .
In Fig 9 , the model trained on baseline system produced the word ' likely ' , which is a synonym of ' probably ' , in the wrong position .
In Fig 10 , the model trained on baseline system produced the word ' autres ' ( ' other ' ) in the multiple positions .
Attention Analysis : Attention distributions generated by our proposed systems have lesser confusion when compared with the attention distribution generated by baseline systems , as shown in Heatmaps of Fig. 11 . Production of word-aligned attention distribution was easy for the attention models , which we retrained on sentences without noise .
Conclusion and Future work
In this paper , we addressed ' scrambled translation problem ' , a shortcoming of previous denoisingbased UNMT approaches like UndreaMT approach ( Artetxe et al. , 2018c ; Lample et al. , 2018 ) .
We demonstrated that adding shuffling noise to all input sentences is the reason behind it .
Our simple retraining strategy , i.e. retraining the trained models by removing the denoising component from auto-encoder objective ( AE ) , results in significant improvements in BLEU scores for four language pairs .
We observe larger improvements in n-gram specific BLEU scores for higher value of n indicating better phrasal translations .
We also observe robustness of the pre-trained language models to the scrambled translation problem .
We would also like to explore applicability of our approach in other ordering -sensitive DAE - based tasks .
Figure 1 : 1 Figure 1 : Our baseline training procedure : Undreamt .
DAE src : Denoising of source sentences ; DAE trg : Denoising of target sentences ; BT S src : Training with shuffled back - translated source sentences ; BT S trg : Training with shuffled back - translated target sentences .
