title
ISTIC 's Triangular Machine Translation System for WMT ' 2021
abstract
This paper describes the ISTIC 's submission to the Triangular Machine Translation Task of Russian - to - Chinese machine translation for WMT ' 2021 .
In order to fully utilize the provided corpora and promote the translation performance from Russian to Chinese , the pivot method is used in our system which pipelines the Russian - to - English translator and the English - to - Chinese translator to form a Russian - to - Chinese translator .
Our system is based on the Transformer architecture and several effective strategies are adopted to improve the quality of translation , including corpus filtering , data pre-processing , system combination , model averaging , model ensemble and reranking .
Introduction
The Institute of Scientific and Technical Information of China ( ISTIC ) participated in the Triangular Machine Translation Task of Russian - to - Chinese in the Sixth Conference on Machine Translation 1 ( WMT ' 2021 ) .
This paper demonstrates the overall framework of the ISTIC 's submission and its technical details .
In this evaluation , we adopted the neural machine translation architecture of Google Transformer ( Vaswani et al. , 2017 ) as a part of our system .
We use the three parallel corpora released by the evaluation organizer and adopted a two -stage method for data pre-processing .
Several filtering methods of the corpus are explored to reduce the data noise and improve the data quality .
As for model construction , we use the pivot method to get a Russian - to - Chinese translator by bridging the trained Russian - to - English translator and Englishto - Chinese translator .
Model averaging ( Claeskens and Hjort , 2008 ) , model ensemble ( Lutellier et al. , 2020 ) and reranking strategies are adopted to generate the final output translation .
We removed spaces between words and restored the target language translation results to the prescribed file format in data post-processing .
In our experiment , the performance of the system under different settings was compared and further analyzed the experimental results .
The structure of this paper is as follows : the second part introduces the technical architecture of our machine translation system ; the third part describes the data pre-processing , parameter settings , experimental results , and related analysis ; the fourth part gives the conclusion and future work .
System Overview
The overall framework of the ISTIC 's triangular machine translation system is shown in Figure 1 .
Single Transformer System
Our baseline single system used in participated evaluation tasks is the Transformer based encoderdecoder architecture .
Transformer is completely based on a self-attention mechanism .
It can achieve algorithm parallelism , speed up model training , further alleviate long-distance dependence and improve translation quality ( Zhang and Zong , 2020 ) .
The encoder and the decoder are formed by stacking N identical layer blocks , where N is set to 6 .
Context - based Combination System
As shown in Figure 2 , based on the Transformer model , our team adopts a context- based ( Voita et al. , 2018 ) system combination method , which is an encoder-decoder structure composed of n identical network layers , where n is set to 6 .
Two different methods of system combination are designed according to the fusion in different positions , which are Encoder Combination method and Decoder Combination method .
Both of them adopt multiencoder to encode the source sen- tences and the context information from machine translation results of the source sentence .
In the Encoder Combination method , the hidden layer information of context ( multi-system translation ) is transformed into new representation through attention network , and merges the hidden layer information of source sentence through gating mechanism at encoder end ;
In Decoder Combination method , the hidden layer information of multi-system translation and the hidden layer information of source sentence is calculated at the decoder to obtain the fusion vector .
The attention calculation method is the same as the original transformer model , to obtain a higher quality fusion translation .
The Encoder Combination model ( see Figure 3 ) uses multiple system translations , and then converts the system translations into new representations through the attention network , integrating the hidden layer information of homologous language sentences for attention fusion through the gating mechanism in the Encoder .
In the Encoder Combination mode and the Self-Attention of the multi-system translation Encoder , Q , K , and V are all from the upper layer output of the multi-system translation Encoder ; in the Self-Attention of the source language Encoder , Q , K , and V are all from the upper layer output of the source language En - ( 1 ) H = M utiHead ( H Tr , H s ) ( 2 ) The Decoder Combination model ( see Figure 4 ) combines the hidden layer information of multiple encoders with attention in the decoder .
The Decoder can process multiple encoders separately , and then fuse them using the gating mechanism inside the Decoder to obtain the combined vector .
In the Decoder Combination mode and the Self-Attention of the target language Decoder , Q , K , and V are all from the output of the previous layer of the target language Decoder ; in the Translation Attention of the target language Decoder , Q comes from the ( 3 )
Direct Method
In the direct method ( see Figure 5 ) , we use the preprocessed Russian / Chinese parallel corpus to train a direct Russian - to - Chinese translator by means of the single Transformer System or the context - based Combination System , depending on which kind of system performs best .
Pivot Method
In the pivot method ( see Figure 5 ) ( Park and Zhao , 2019 ) , firstly , we use the pre-processed Russian / English parallel corpus to train a Russianto - English translator ; secondly , we use the preprocessed English / Chinese parallel corpus to train an English - to - Chinese translator ; finally , we pipeline them to form a pivot Russian - to - Chinese translator .
All translators can be trained by means of the single Transformer System or the contextbased Combination System .
By comparing the experimental results , the system with optimal performance is accepted for Russian - to - Chinese translation .
3 Experiments
Data Pre-processing
The evaluation organizers provide three parallel corpora : the Chinese / Russian corpus is crawled from the web and aligned at the segment level , and combined with different public resources ; the Chinese / English corpus combines several public resources ; the Russian / English corpus gathers multiple public resources .
A two -stage method ( Wei et al. , 2020 ) is used for data pre-processing , consist of a general pre-processing stage and a specific pre-processing stage .
The general pre-processing stage includes conversion from traditional Chinese to simplified Chinese by the hanziconv 2 package , conversion between full angle and half-angle , special character filtering , same content filtering , sentence length filtering , and sentence length ratio filtering .
Among them , sentence length of the Chinese language is calculated in the unit of " character " and sentence length of non - Chinese language is calculated in the unit of " token " .
Sentence length filtering removes sentence pairs which source sentence length or target sentence length exceeds the range of [ 5 , 200 ] .
Sentence length ratio filtering excludes the sentence pairs whose ratio of source sentence length and target sentence length exceeds the range of [ 0.2 , 20 ] .
In the specific pre-processing stage , the word segmentation of English and Chinese sentences is implemented using the lexical tool Urheen 3 and the word segmentation of Russian sentences is implemented using the lexical tool Natasha 4 .
The scales of sentence pairs of all corpora before and after data pre-processing are shown in Table 1 .
After data preprocessing , we split the corpora into training set , development set and test set .
The scales of the data partition are shown in Table 2 .
System Settings
The open-source project fairseq 5 is chosen for this evaluation system .
The main parameters are set as follows .
Each model uses 1 - 3 GPUs for training , and the batch size is 2048 .
The embedding size and hidden size are set to 1024 , the dimension of the feed -forward layer is 4096 .
We use six self-attention layers for both encoder and decoder , and the multi-head self-attention mechanism has 16 heads .
The dropout mechanism ( Provilkov et al. , 2020 ) was adopted , and dropout probabilities are set to 0.3 .
BPE ( Sennrich et al. , 2016 ) is used in all experiments , where the merge operations is set to 32000 .
The maximum number of tokens is set to 4096 .
The loss function is set to " label_smoothed_cross_entropy " .
The parameter " adam_betas " is set to ( 0.9 , 0.997 ) .
For the baseline system , the initial learning rate is 0.0007 , the warm - up steps are set to 4000 , and the maximum epoch number is set to 15 .
For the Encoder Combination system and Decoder Combination system 6 , the initial learning rate is 0.0001 , the warm - up steps are set to 4000 , and the maximum epoch number is set to 10 .
Experimental results
In the training of Russian- to - English translator , English - to - Chinese translator and Russian - to - Chinese translator , the single Transformer systems
Our primary submission uses the pivot method , which use English translation as the bridge .
The Russian sentences are translated into English intermediate results by the well - trained Russian - to - English translator and then the English intermediate results are translated into Chinese output by the well - trained English - to - Chinese translator .
Our contrast submission uses the direct method , which uses the well - trained Russian - to - Chinese translator to generate the target output .
As a result , our primary submission achieves a BLEU score of 19.2 and ranked the fourth among all participating teams .
Our contrast submission achieves a BLEU score of 18.1 ( shown in Table 4 ) .
Conclusions
This paper introduces the main technologies and methods of ISTIC 's submission in WMT 2021 .
To sum up , our model is constructed on the Transformer architecture of self-attention mechanism and context - based system combination method .
In the aspect of data pre-processing , we explore several corpus filtering methods .
In the process of translation output , the strategies of model ensemble and reranking are adopted .
Experimental results show that these methods can effectively improve the quality of translation .
It is worth mentioning that the pivot language translation bridge method outperforms the direct translation method .
Figure Figure 1 : Overall framework
Figure 2 : 2 Figure 2 : Context - based combination system
Figure 4 : 4 Figure 4 : Decoder combination model
Figure 5 : 5 Figure 5 : Direct and pivot method
Table 1 : 1 Data pre-processing results Direction Before Pre-processing After Pre-processing Russian-English 69217438 42939395 English - Chinese 28579587 22233706 Russian - Chinese 33422682 21892537 Direction Train Set Dev Set Test Set Russian-English 42935974 2000 1421 English - Chinese 22231506 1100 1100 Russian - Chinese 21891537 965 1000
Table 2 : 2 Data partition results
Table 3 : 3 BLEU results on self- built test set System Russian - English English - Chinese Russian - Chinese Transformer 20.89 17.83 16.07 Transformer + Encoder 21.53 18.87 16.66 Combination Transformer + Decoder 21.76 18.91 16.79 Combination Method BLEU Primary : Pivot Method 19.2 Contrast : Direct Method 18.1
Table 4 : 4 BLEU results on released test set are trained for 15 epochs .
The context - based combination systems with Encoder Combination model or Decoder Combination model are trained for 10 epochs .
The best epoch model and the last epoch model are ensembled to generate better results .
The BLEU ( Papineni et al. , 2002 ) scoring results on the self - built test set are shown in Table3 .
The context- based combination systems with Decoder Combination model are used as our final submission since they outperform other systems .
https://github.com/berniey/hanziconv 3 https://www.nlpr.ia.ac.cn/cip/ software.html
https://github.com/natasha/natasha 5 https://github.com/pytorch/fairseq/ tree /v0.6.2 6 https://github.com/libeineu/ Context - Aware
