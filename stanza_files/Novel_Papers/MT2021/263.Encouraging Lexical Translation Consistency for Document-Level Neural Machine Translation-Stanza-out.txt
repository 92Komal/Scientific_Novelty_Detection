title
Encouraging Lexical Translation Consistency for Document -Level Neural Machine Translation
abstract
Recently a number of approaches have been proposed to improve translation performance for document - level neural machine translation ( NMT ) .
However , few are focusing on the subject of lexical translation consistency .
In this paper we apply " one translation per discourse " in NMT , and aim to encourage lexical translation consistency for document - level NMT .
This is done by first obtaining a word link for each source word in a document , which tells the positions where the source word appears at .
Then we encourage the translations of those words within a link to be consistent in two ways .
On the one hand , when encoding sentences within a document we properly exchange context information of those words .
On the other hand , we propose an auxiliary loss function to better constrain that their translations should be consistent .
Experimental results on Chinese ?
English and English ?
French translation tasks show that our approach not only achieves state - of - the - art performance in BLEU scores , but also greatly improves lexical translation consistency .
Introduction
Unlike sentence - level neural machine translation ( NMT ) , document- level NMT needs to not only model intra-sentence dependencies , but also consider a wide variety of inter-sentence discourse phenomena , such as coreference , lexical cohesion , semantic coherence , discourse relations .
Motivated by the success of " one translation per discourse " in statistical machine translation ( SMT ) ( Merkel , 1996 ; Carpuat , 2009 ; T?re et al. , 2012 ; Guillou , 2013 ; Al Khotaba and Al Tarawneh , 2015 ) , in this paper our goal is to encourage lexical translation consistency for document- level NMT .
Figure 1 shows an example of an input document and its output translated by a state - of - the - art sentence - level NMT system .
The technical term ?/ fang_di_chan_ye , occurring four times within a document , surprisingly obtains different translations while in its reference ( human translation ) it is translated consistently .
Such inconsistent translations , however , tend to confuse readers in some cases .
Recent years have witnessed an increasing interest in document- level NMT , but most previous studies explore various context - aware models for better incorporating document - level context to improve translation performance without handling a specific discourse phenomenon ( Maruf and Haffari 2018 ; Miculicich et al. 2018 ; Maruf et al. 2019 , to name a few ) .
As a way to encourage lexical translation consistency , Kuang et al . ( 2017 ) and Tu et al . ( 2018 ) cache recently translated words and / or their translations for translating future sentences .
However , cache- based approaches may potentially guide the translation of future sentences in a wrong way since the cached translation could be incorrect .
Rather than explicitly presenting lexical translations used in previous sentences as in cache- based approaches , in this paper we aim at improving lexical translation consistency in a softer way : we encourage translations of the same word in different positions of a document to be consistent .
Specifically , we first obtain a word link for each source word in a document if it has , which tells the positions the source word appears at .
To encourage translation consistency for words within a link , we exchange their context information when encoding sentences in a document .
Moreover , we properly propose an auxiliary loss function to better constrain that the translations of these words should be consistent .
Overall , we make the following contributions . ?
We propose a metric to properly measure lexical translation consistency , and provide a detailed study on lexical translation consistency in both Chinese ?
English translation . ?
We propose a novel approach to improve lexical translation consistency for document- level NMT .
One nice property of our approach is that our models could synchronously translate sentences in a document , rather than translating them one by one as in cached - based approaches .
?
Experimental results show that our approach outperforms various context - aware NMT models in BLEU .
More importantly , our approach greatly improves lexical translation consistency .
Motivation Given a parallel document pair ( S , T ) , a sourceside word w ( stemmed to eliminate morphological differences if necessary ) is one of words of our interest if it is a non-stop word and occurs two or more times in S. For w , we conjecture that the translations ( stemmed too if necessary ) of w in T tend to be same .
As shown in Figure 1 , source word ?/fang_di_chan_ye is consistently translated into ( the ) real estate sector in reference translation .
Lexical Translation Consistency Metric .
To properly evaluate lexical translation consistency , we propose lexical translation consistency ratio ( LTCR ) , which is based on word-alignment .
Let us assume that source word w appears k times in S. Based on word alignment between S and T , we obtain its k translations , 1 i.e. , ( t1 , ? ? ? , t k ) , where ti LTCR ( w ) = k i=1 k j=i+1 1 ( ti = tj ) C 2 k ? 100 % ( 1 ) where the denominator C 2 k denotes the size of the combination of translation set ( t1 , ? ? ? , t k ) , and function 1 ( t i = t j ) returns 1 if t i is same as t j , otherwise 0 .
The metric illustrates how frequent translation pairs of w is same within a document .
The higher the metric value is , the more likely w is translated consistently .
Taking source word ? ?/fang_di_chan_ye in Figure 1 as an example , its LTCR is 100 % for reference translation and 0 % for sentence - level NMT .
In above we calculate LTCR for a single word in a document .
Likewise , we could apply the metric to all source words that are of our interest in a parallel document pair , or a document- level parallel dataset by summing up all these words ' corresponding numerators and denominators , respectively .
Statistics on Reference Translation and Automatic Translation .
To better understand lexical consistency in translation , we take a concrete Chinese -English ( ZH - EN ) manually word-aligned document - level parallel corpus ( LDC2015T06 ) as representative to study how consistent the lexical translation is in ZH?EN and EN ?
ZH translation .
The corpus consists of 268 documents with 6741 sentences in total from domains including broadcast , newswire , and web data .
Moreover , for sentence - level NMT translation we perform word alignment to obtain word-level translation .
2 Table 1 compares the lexical translation consistency in ZH?EN and EN ?ZH translation of LDC2015T06 .
From it , we observe that although translation diversity is usually encouraged , LTCR still reaches 74.24 % and 63.11 % in ZH?EN and EN ?
ZH reference translation , respectively .
This confirms our conjecture that the translations of same source words tend to be consistent .
We also note that the consistency is different among different types of words .
For example , the consistency for nouns is much higher than those of other word types in both translation directions .
Unfortunately , the consistency in automatic translation is much lower than that in reference translation , indicating there exists much room to improve lexical consistency in document - level machine translation .
Finally , it also shows that the percentages of words of
Encouraging Lexical Translation Consistency via Word Links
As our goal is to encourage lexical consistency in document- level translation , we first obtain word links , each of which tells the positions that a word appears in a document ( Section 3.1 ) .
To encourage translation consistency among words in the same link , on the one hand we exchange their information when encoding sentences within a document ( Section 3.2 ) .
On the other hand , we properly propose an auxiliary loss function to better constrain the translations of these words being consistent ( Section 3.3 ) .
Obtaining Word Links
We define some notations before describing our approach .
Given a document- level parallel pair ( S , T ) = ( Si , Ti ) |
N i=1 with N sentence pairs , we assume that each source sentence Si = ( si , j ) | n j=1 consists of n words .
Given document S , we use V to denote the collection of words of our interest in S , which are non-stop words and appear two or more times .
For word si , j if it exists in V , we maintain a link list Li , j = ( a i , j , k , b i , j , k , m i , j , k ) |
K k=1 with K triples , which tells the other K positions where si , j appears .
3 Specifically , in a triple ( a , b , m ) , a and b indicate the sentence index and word index of a position respectively while m ? { 0 , 1 } is a padding mask and indicates ( a , b ) is a real position pair or a fake one .
Specially , for cases where si , j appears more than K times in S , we choose the top K closest ones to construct its word link .
4 3 We do not include si , j itself in Li , j .
4 According to our preliminary experimentation , the effect of different ways of choosing K positions is negligible .
Encoding Documents with Word Links
Now each word of our interest in a document is equipped with a word link .
In encoding , we take documents as input units by synchronously encoding sentences within a document .
Figure 2 shows our encoder layer which encodes documents with word links .
Sentence Position Embedding Since words in a link list may appear in different sentences , a Transformer encoder can not distinguish the sentence positions of the linked words and the current word .
Therefore , we introduce sentence position embedding to distinguish the positions of these words .
Formally , given the i-th sentence Si in S , we project each word si , j into a word embedding ei , j ? R d , a ( intra-sentence ) position embedding pej ?
R d , and a sentence position embedding spei ?
R d , where d is the size of embedding and hidden state throughout the entire model .
Then , we perform an addition operation to unify them into a single input , i.e. , ei , j + pej + spei .
Note that both the word embeddings and the sentence position embeddings are trainable parameters while the ( intra-sentence ) position embeddings are sinusoidal ( Vaswani et al. , 2017 ) .
Encoder
As shown in Figure 2 , the encoder consists of M identical encoder layer , which consists of three sublayers , i.e. , a self-attention sub-layer , a word-linkattention sub-layer , and a feed-forward sub-layer .
Next we use sentence Si = ( si , j ) | n j=1 to illustrate the encoding process .
Self-Attention Sub-Layer .
In the m-th encoder layer , it takes A ( m ) i ?
R n?dm as input and computes a new sequence B ( m ) i with the same length via multi-head attention function : B ( m ) i = LayerNorm MultiHead A ( m ) i , A ( m ) i , A ( m ) i + A ( m ) i , ( 2 ) where LayerNorm is the layer normalization function ( Ba et al. , 2016 ) , and the output B ( m ) i is of shape R n?d .
For the first encoder layer , A ( 1 ) i is the input of the encoder while for other layers , A ( m ) i is the output of the ( m ? 1 ) - th encoder layer .
Word-Link-Attention Sub-Layer .
Since we encode sentences within document Si| N i=1 synchronously , we obtain B ( m ) i N i=1 from the selfattention sub-layer of the m-th layer .
Let us assume that word si , j in sentence Si is of our interest and has a word link list Li , j .
Then we use the list to index the states of its K linked words from B ( m ) i N i=1 .
We use C ( m ) i , j ? R K?d to denote the indexed states .
Consequently , this sub-layer uses another multihead attention function to exchange information among linked words : D ( m ) i , j = LayerNorm MultiHead B ( m ) i , j , C ( m ) i , j , C ( m ) i , j + B ( m ) i , j . ( 3 ) Specifically , if si , j is out of our interest and does not have a word link list , we set D ( m ) i , j = B ( m ) i , j . Feed-Forward Sub-Layer .
In the m-th encoder layer , this sub-layer is applied to each position separately and identically by two linear transformations with a ReLU activation in between .
E ( m ) i = LayerNorm max 0 , D ( m ) i W F 1 + b F 1 W F 2 + b F 2 + D ( m ) i , ( 4 ) where W F 1 , W F 2 ? R d?d , and b F 1 , b F 2 ? R d are model parameters .
The output of the final layer , i.e. , E ( M ) i will be used as the output of the encoder .
Consistency Constraint Loss
After encoding sentences within a document , we properly extract useful information from documentlevel context via deliberately obtained word links .
We expect the extracted information from document - level context can enhance the translations of the same words being more consistent , i.e. , the states of the same words within a document being closer .
Let us assume that word sx ,y , i.e. , the y-th word in the x-th sentence is in the word- link list of word si , j .
We use E ( M ) i , j and E ( M ) x,y to denote their hidden states of our encoder with word-link attention sub-layer .
Meanwhile we use E ( M ) i , j and E ( M ) x,y to denote their hidden states of a vanilla Transformer encoder , i.e. , the encoder without the wordlink attention sub-layer .
Since our encoder has exchanged context information between si , j and sx , y while the vanilla encoder has not , we expect that the two states E ( M ) i , j and E ( M ) x,y are closer than E ( M ) i , j and E ( M ) x,y .
5 According to Section 3.2 , our encoder returns E ( M ) i N i=1 for document Si| N i=1 .
We use E ( M ) i N i=1 to denote the outputs of its corresponding vanilla encoder .
6
To encourage that our encoder would generate closer hidden states for a pair of linked words than the vanilla encoder , we follow previous work on visual semantic embedding ( Kiros et al. , 2014 ) and define a consistency constraint loss .
In practice , similar to Chen et al . ( 2020 ) , we introduce a small neural network projection head that maps representations , i.e. E ( M ) i N i=1 , E ( M ) i N i=1 , to a space where a consistency constraint loss is applied during training .
We use MLP with one hidden layer to obtain Z and Z ( i.e. Z ( M ) i N i=1 , Z ( M ) i N i= 1 ) by Z = g ( E ) = W ( 1 ) ?( W ( 2 ) E ) ) and Z = g( E ) , where ? is a ReLU non-linearity , and W ( 1 ) , W ( 2 ) ?
R d?d are model parameters .
As shown in Appendix C , we find it beneficial to define the consistency constraint loss on Z , Z 's rather than E , E 's .
After that , the consistency constraint loss is defined as follow : JCC ( ? ) = S i , j , k max 0 , ? ? D Z ( M ) i , j , Z ( M ) a i , j , k , b i , j , k + D Z ( M ) i , j , Z ( M ) a i , j , k , b i , j , k ( 5 ) where ? are the parameters in our model , D is a distance function , i.e. , cosine distance between two vectors , and ? is a margin , a i , j , k and b i , j , k denote the sentence and word indexes of word si , j's k-th linked word , respectively .
7 Finally , the joint objective function of our model J ( ? ) is define as : J ( ? ) = J NMT ( ? ) + ?J CC ( ? ) ( 6 ) where ? determines the contribution of consistency constraint loss , and J N M T ( ? ) is the cross entropy loss function , i.e. , J NMT ( ? ) = ?
( S, T ) i , j log p ( ti , j|ti , <j , S ) ( 7 )
Experimentation
To verify the effectiveness of our proposed approach , we carry out experiments on ZH?EN translation tasks of two different domains : news and TED talks .
As inspired by the conclusion in Guillou ( 2013 ) that lexical consistency is encouraged in English - French human translation , we also validate our approach on EN ?
FR translation .
Experimental Setup Datasets .
For ZH?EN ( News ) , the training data is composed from LDC .
We use the NIST2006 dataset as the development set and combine NIST2002 , 2003 NIST2002 , , 2004 NIST2002 , , 2005 NIST2002 , and 2008 For ZH?EN ( TED ) , the dataset is from the IWSLT 2014 and 2015 ( Cettolo et al. , 2012 ( Cettolo et al. , , 2015 evaluation .
We use dev2010 as the development set and combine tst2010 - 2013 as the test set .
For both ZH ?EN translations , every source sentence has one translation reference .
For EN?FR , we use IWSLT 2015 ( Cettolo et al. , 2015 ) evaluation as training data .
For development and testing , we use dev2010 as the development set and combine tst2010 - 2013 as test set and every source sentence has one translation reference .
See Appendix
A for more statistics and preprocessing of the experimental datasets .
Training Strategy .
To compute the consistency constraint loss JCC ( ? ) , sentences are required to be encoded twice , i.e. , one for encoding with the wordlink attention sub-layer and the other for encoding without it .
Therefore , including this loss function from the beginning may break the balance between optimizing the encoder and the decoder , and make it hard for the training to properly converge .
To alleviate this problem , we divide the whole training process into two stages .
In the first stage , we train the models to convergence with the cross entropy loss JNMT ( ? ) only while in the second stage , we combine the consistency constraint loss JCC ( ? ) and train the models with the joint loss .
Actually , the second training stage acts like a fine-tuning , in which we use a smaller learning rate and fewer training steps .
Model Setting .
We use OpenNMT ( Klein et al. , 2017 ) as the implementation of the Transformer and extend it .
For the number of linked words with the current word , we set K = 6 .
The margin size ? in the consistency constraint loss is set to 0.2 while the weight ? in joint objective function is set to 0.01 .
Other model settings are in Appendix B. Evaluation .
For all translation tasks , we report case - insensitive BLEU score as calculated by the multi-bleu .
perl script .
Experimental Result Besides sentence- level Transformer , we also compare our approach to three previous Transformerbased context - aware NMT models : HAN ( Miculicich et al. , 2018 ) , 8 SAN ( Maruf et al. , 2019 ) MCN ( Zheng et al. , 2020 ) . 10
For fair comparison , we run their source code with our model settings .
Note that the above context- aware NMT models aim to improve the translation accuracy ( i.e. , BLEU ) without focusing on resolving a particular discourse phenomenon .
Chinese -English Translation .
Table 2 lists the performance of ZH?EN translation on both News and TED talk domains .
From the table , we have the following observations .
?
Exchanging information via words within word links ( i.e. , + word-link ) achieves significant improvement in BLEU over ( sentence - level )
Transformer , suggesting that extracting information from document - level context via our deliberately designed word links is effective .
Upon the setting of + word-link , constraining the translations of 10 MCN : https://github.com/Blickwinkel1107/making-themost-of-context-nmt words within a link ( i.e. , + CC -loss ) to be consistent with our proposed loss function achieves further significant improvement in BLEU .
Comparing to Transformer , our approach gains + 2.23 and + 2.05 BLEU on the two domains , respectively . ?
In terms of LTCR , both + word-link and + CCloss greatly improve lexical translation consistency .
For example , with + word- link + CC - loss our approach achieves + 7.74 % and + 10.28 % LTCR on the two domains , respectively .
?
Though the three previous context- aware NMT models significantly outperform Transformer in terms of BLEU , their performance of LTCR is very close to that of Transformer , suggesting that these models have very limited effect in encouraging lexical translation consistency .
Compared to these models , our approach achieves better performance in BLEU while more importantly , it greatly improves the performance in LTCR .
?
With the word-link attention sub-layer , our approach introduces additional 10.87 % parameters and have similar number of parameters as the previous context - aware NMT models .
English - Chinese Translation . + 1.34 on the two domains over Transformer , respectively .
Meanwhile , we achieve + 7.68 % LTCR and + 8.90 % , respectively .
English - French Translation .
Table 4 shows the performance results of EN ?
FR translation on the TED domain .
From it ,
We also observe a similar performance trend as ZH ?EN translation .
Our approach gains + 2.18 BLEU and + 6.96 % LTCR over Transformer , respectively .
Discussion Next , we take ZH ?EN translation on news domain as a representative to discuss how our proposed approach improves translation performance .
See Appendix for more discussion .
Effect of Hyper-parameter K Among the words of our interests , the valid lengths of their word links differ greatly .
As shown in Table 5 , about 79.68 % of our interested words have a word link whose valid length is 6 or less .
A significant hyper-parameter in our proposed model is K , i.e. , the number of words in every word link ( Section 3.1 ) .
A low value makes the information exchanging among sentences within a document not sufficient while a high value increases the cost of computation .
We compare the performance and training consumed time for five different K values .
Note that our model is equivalent to sentence - level Transformer when K is 0 .
Figure 3 shows the performance over different values of K .
It shows that when K increases from 0 to 6 , we observe consistent improvement on both BLEU and LTCR .
The performance tends to be stable at K = 6 since no further improvement is achieved by increasing K to 8 .
Meanwhile , increasing K slightly slows down the training speed .
Compared to Transformer ( i.e. , K = 0 , 12700 toks / sec ) , our approach with K = 6 ( 7800 toks / sec ) spends 39 % more training time , consumed by the wordlink attention sub-layers and the computation of consistency constraint loss .
Effect of Random Linked Word Positions
As shown in Section 3.1 , the word link of word si , j contains the other positions where si , j appears at .
To validate that the improvement achieved indeed comes from exchanging information among words with same stem , we perform a contrastive experiment by replacing the positions in word links with random positions .
Note that in this way it does not make sense to apply the consistency constraint loss ( + CC - loss ) since the linked words are random .
Table 6 compares the performance .
On the one hand , replacing words in word lists with random words still achieves + 0.49 BLEU over Transformer .
This suggests that even randomly exchanging information cross sentences is helpful .
On the other hand , using random linked words does not bring LTCR improvement over Transformer .
This in turn may suggest that the BLEU improvement achieved by our approach is mainly contributed by improved lexical translation consistency .
Performance on LDC2015T06
In Section 2 we use word-aligned document- level parallel corpus LDC2015T06 to analyze lexical consistency in translation .
Table 7 compares the LTCR performance of our approach to those of the gold and sentence - level NMT scenarios .
It shows that our approach ( e.g. , + word-link + CCloss ) achieves higher LTCR than Transformer over all POS tags , especially for nouns .
Meanwhile , the performance gap behind that of reference translation suggests that there still exists room for further improvement .
Pronoun Translation
We follow Miculicich et al. ( 2018 ) and Tan et al . ( 2019 ) to evaluate coreference and anaphora using the reference - based metric : accuracy of pronoun translation ( Werlen and Popescu-Belis , 2017 ) .
Table 8 lists the performance of pronoun translation .
From it we observe that our approach also improves the performance of pronoun translation while exchanging context information among linked words ( i.e. , + word-link ) contributes more than the consistency constraint loss ( i.e. , + CCloss ) .
Human Evaluation
We conduct a human evaluation on 500 sentences randomly selected from our test set .
Let us assume that the i-th sentence Si in a document- level parallel pair ( S , T ) is selected .
Then we provide two annotators with a group of source sentences and translations , i.e. , ( Si?2 , Si?1 , Si , Si +1 , Si +2 ) and ( Ti?2 , Ti?1 , ? , Ti +1 , Ti+2 ) , where ? is Si 's translation of either our approach or the sentence - level Transformer .
Besides , translation ? is provided in random order with no indication which model it is from .
Following Voita et al. ( 2019a ) , the task is to pick one of the three options : ( 1 ) the first translation is better , ( 2 ) the second translation is better , and ( 3 ) the translations are equal quality .
The two annotators are asked to avoid the third option if they could give preference to one of the translations .
Table 9 shows the human evaluation results .
In average the annotators mark 47 % cases as having equal quality .
Among the others , our approach outperforms Transformer in 64 % cases , suggesting that overall the annotators have a strong preference for our approach over Transformer .
Effect of Sentence Position Embedding As shown in Section 3.2.1 , we introduce sentence position embedding ( SPE ) to indicate the sentence position of words .
To analyze that the effects of it on our proposed approach , we perform a contrastive experiment .
Table 10 compares the performance .
The SPE slightly improves BLEU ( + 0.49 ) and LTCR ( + 0.84 % ) over word- link Transformer without SPE .
This is suggest that SPE for document- level NMT is helpful .
We will explore more about it in the future work .
Analysis of Exchanging Information among Linked Words
As shown in Section 3.2.2 , we use the multi-head attention function to exchange information among linked words .
To valid the effectiveness of this method , we perform a contrastive experiment by replacing multi-head attention function in Eq. 3 with the average pooling function Eq. 8 . D ( m ) i , j = LayerNorm Avg C ( m ) i , j + B ( m ) i , j . ( 8 ) Table 11 lists the performance of translation when we use different functions to exchange information among linked words .
From it we observe that the multi-head attention function performs better .
This in turn may suggest that simply averaging hidden states of linked words to exchange information lead to the mediocrity of cross-sentence information .
Related Work
There has been substantial work in SMT that either encourages or enforces lexical translation consistency .
For example , Xiao et al . ( 2011 ) and Garcia et al . ( 2014 Garcia et al. ( , 2017 propose post-editing approaches to re-translate those source words which have been translated differently in a document .
Tiedemann ( 2010 a , b) and Gong et al . ( 2011 ) propose cache - based approaches to remember translation history .
Discriminative learning approaches are also proposed to fix lexical translation non-consistency .
Besides , Carpuat ( 2009 ) and T?re et al. ( 2012 ) demonstrate that applying " one translation per discourse " constraint in SMT leads to better translation quality .
Moving to NMT , most of document- level NMT studies have proposed various context - aware NMT models to leverage either local context , e.g. , previous sentences Wang et al. , 2017 ; Bawden et al. , 2018 ; Voita et al. , 2018 Voita et al. , , 2019 b
Yang et al. , 2019 ) , or entire document ( Maruf and Haffari , 2018 ; Mace and Servan , 2019 ; Maruf et al. , 2019 ; Tan et al. , 2019 ; Zheng et al. , 2020 ; Kang et al. , 2020 ) .
However , different from ours , these studies aim to improve the translation accuracy without handling a specific discourse phenomena .
Kuang et al. ( 2017 ) and Tu et al . ( 2018 ) cache recently translated words and / or their translations which could be used to increase lexical consistency when translate future sentences .
However , cache- based approaches require to translate sentences in a document one by one and may potentially guide the translation of future sentences in a wrong way since the cached translations could be incorrect .
Experimental re-sults in related studies Miculicich et al. , 2018 ) have shown that the improvement of cache- based approaches is limited in BLEU over ( sentence - level ) Transformer .
Our approach is different from cached - based approach as we translate sentences within a document synchronously , and more importantly it does not explicitly suggest any translation .
There also exists many studies in NMT that aim to resolve discourse phenomena in post-process .
For example , to make translation outputs of a document more coherent , Voita et al . ( 2019a ) propose DocRepair trained on monolingual target language documents to correct the inconsistencies in sentence - level translation while Yu et al . ( 2020 ) train a context - aware language model to re-rank sentence - level translation candidates .
Conclusion
In this paper , we apply " one translation per discourse " in NMT , and have proposed an approach to encourage lexical translation consistency .
This is done by first obtaining a word link for each source word in a document , which tells the positions the source word appears at .
Then we encourage the translations of words within a link to be consistent by both exchanging their context information in encoding , and using an auxiliary loss to constrain their translation being consistent .
Experimental results on Chinese ?
English and English ?
French translation tasks show that our approach not only achieves higher BLEU scores than various contextaware NMT models , but also greatly improves lexical translation consistency .
