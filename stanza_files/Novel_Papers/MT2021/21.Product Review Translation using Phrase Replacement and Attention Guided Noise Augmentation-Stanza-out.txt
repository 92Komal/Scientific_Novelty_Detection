title
Product Review Translation using Phrase Replacement and Attention Guided Noise Augmentation
abstract
Product reviews provide valuable feedback of the customers , however , they are available today only in English on most of the e-commerce platforms .
The nature of reviews provided by customers in any multilingual country poses unique challenges for machine translation such as code-mixing , ungrammatical sentences , presence of colloquial terms , lack of e-commerce parallel corpus etc .
Given that 44 % of Indian population speaks and operates in Hindi language , we address the above challenges by presenting an English - to - Hindi neural machine translation ( NMT ) system to translate the product reviews available on e-commerce websites by creating an in-domain parallel corpora and handling various types of noise in reviews via two data augmentation techniques , viz . ( i ) .
a novel phrase augmentation technique ( PhrRep ) where the syntactic noun phrases in the sentences are replaced by the other noun phrases carrying different meanings but in the similar context ; and ( ii ) .
a novel attention guided noise augmentation ( AttnNoise ) technique to make our NMT model robust towards various noise .
Evaluation shows that using the proposed augmentation techniques we achieve a 6.67 BLEU score improvement over the baseline model .
In order to show that our proposed approach is not language -specific , we also perform experiments for two other language pairs , viz .
En - Fr ( MTNT18 corpus ) and En-De ( IWSLT17 ) that yield the improvements of 2.55 and 0.91 BLEU points , respectively , over the baselines .
Introduction Product reviews written by the users on e-commerce websites are useful to get the feedback about the products and provide valuable insights to the user for making the buying decision .
The product reviews available on different e-commerce websites are mainly in English language .
India is a multilingual country with great linguistic and cultural diversities .
There are 22 officially spoken languages , and many of them such as Hindi , Bengali , etc. come into the top 10 most spoken languages all over in the world .
Since English is not a first language in India and most of the population ( approximately , 68.9 % ) 1 from the rural areas do not have the proper understanding of English language , it becomes difficult for them to read a review or write a review in English with proper vocabulary and grammar .
This makes the availability of product reviews in vernacular languages essential for the vast majority of Indian e-commerce customers .
However , building an automated translation system for the large amount of reviews poses unique challenges to the machine translation community .
We illustrate some of the challenges with examples as shown in Table 1 .
In example A , the word osm appears as a short form of the word awesome ; also there is no space between the words product and i .
The model is not able to translate these correctly .
Similarly , in example B , NYC and quilty are the short forms and misspelled versions of the words nice and quality , respectively .
Presence of emojis in example - C also causes translation difficulty .
We address the above challenges with the main contributions or attributes of our work as follows : ?
We build an NMT system for product reviews in low-resource scenarios .
To the best of our knowledge , this is the very first attempt towards building a machine translation system for English to Indian language review translation .
?
We build data resources by crawling reviews from an e-commerce portal , translate them into Hindi using our in-house open domain English - Hindi MT system , and perform manual verification for the correctness ( c.f. Section 3.1 ) . ?
We introduce novel data augmentation techniques to handle the noise and the scarcity of in- domain training data as follows : 1 . We introduce a novel similar phrase replacement technique ( PhrRep ) which generates more diverse synthetic parallel samples compared to the word augmentation techniques ( c.f. Section 4.3 ) .
2 . We use Part-of- Speech ( PoS ) guided word embedding based and context aware word augmentation techniques for synthetic data creation ( c.f. Section 4.1 and Section 4.2 ) , and show that our proposed PhrRep approach significantly outperforms the word based augmentation methods .
3 . We introduce a novel attention guided noise augmentation ( AttnNoise ) technique to make the NMT model robust towards noisy inputs ( c.f. Section 5.1 ) .
We show that AttnNoise method significantly outperforms the random noise injection ( RndNoise ) techniques .
Related Work
There are two main challenges for translating the product reviews , viz . ( i ) . nonavailability of parallel corpus ; and ( ii ) .
noisy sentences in product and / or service reviews .
Machine translation with noisy text is , itself , a very challenging task .
The typical noises that pose challenges for machine translation include improper grammatical structures , misspellings , punctuation , emojis etc ( c.f. Section 3.1 ) ( Michel and Neubig , 2018 ) .
In the literature , there are a few works concerning the noise in the text and to increase the robustness of the translation model .
Michel and Neubig ( 2018 ) presented a noisy dataset and discussed the challenges of noisy contents .
Belinkov and Bisk ( 2018 ) and Karpukhin et al . ( 2019 ) showed that small noise in the input text can reduce the quality of translation .
To improve the robustness of the translation model they introduced synthetic errors like character swapping , deletion and insertion in the corpus .
Vaibhav et al. ( 2019 ) also inserted synthetic noises and back - translated noise in the original corpus .
Apart from the spelling distortion , to make the model immune to the grammatical errors , Anastasopoulos et al . ( 2019 ) augmented training data with the grammatical errors .
They focused on articles , prepositions , subject - verb agreements etc .
Considering the challenges , Berard et al . ( 2019 ) analyzed the performance of NMT model over a small French - English corpus of restaurant reviews .
Unlike this , we do not inject any random noise , rather we introduce an attention guided noise augmentation ( AttnNoise ) technique to insert the synthetic noise at the source ( English ) side .
To address the second challenge related to the availability of training data , we make use of the data augmentation techniques to increase the training samples and noise handling techniques to increase the robustness of the model .
Fadaee et al. ( 2017 ) replaced the common words by rare words to provide better evidence and contexts for the rare words .
Gao et al. ( 2019 ) introduced a soft contextual augmentation method where a word 's embedding is replaced by a weighted average of its similar words .
Kobayashi ( 2018 ) used a bi-directional language model to predict the replacement by using the sentence context .
Wu et al . ( 2019 ) used the BERT ( Bidirectional Encoder Representations from Transformers ) Devlin et al. ( 2019 ) model to predict the randomly masked word .
Inspired by Wu et al . ( 2019 ) , we mask the noun and adjective words in the source sentence and predict the appropriate nouns and adjectives as substitutes based on the sentence context .
We introduce a phrase replacement based data augmentation technique ( PhrRep ) to replace the whole syntactic noun phrase ( multiple words in a single attempt ) with other diverse but contextually similar noun phrases .
Parallel Corpus Creation
In this section we describe the steps followed for parallel corpus creation and the necessary statistics .
Crawling reviews and challenges in pre-processing We crawl English product reviews from the e-commerce portal , Flipkart .
Product reviews are user generated contents and contain various noises ( inconsistencies ) as shown in Table 1 . 2016 ) and D : IWSLT2017 English - German dataset .
Pre-processing
We remove the emojis from the English sentence by providing their unicode range using regular expressions .
Any character having repetition of more than two times is trimmed and then checked for its compatible correct word using spell- checker 2 and a list provided by Facebook 3 Edizel et al . ( 2019 ) .
Writing the complete sentence in upper case is also very common in user generated content ( i.e .
NICE PHONE IN LOW BUDGET ) .
Normalization is done to convert all such instances into the lower case .
Since we focus on the product reviews data , we make the first character of brand 's name 4 ( Google , Moto , Nokia etc. ) as capital .
After the pre-processing steps as mentioned above ( emoji removal , character repetition , casing etc. ) , we found that approximately 62.3 % sentences from the total crawled sentences are correct .
Gold Corpus Creation by Human Post-editing After pre-processing , we obtain 22,595 standard English sentences as mentioned in Table 2 . Instead of translating sentences from scratch , we use our in-house judicial domain system to generate the initial target sentences and post-edit .
It is trained for English -Hindi translation using 0.45 million parallel judicial domain samples and additional English -Hindi corpus Kunchukuttan et al . ( 2018 ) having 1.6 million parallel samples .
It achieves 55.67 BLEU ( En-to - Hi ) points on our in-house judicial domain testset .
After translation into Hindi , manual verification for the correctness of the translation is done by three language experts .
The experts are post-graduates in linguistics and have good command in Hindi and English both .
The experts read the English sentences and their Hindi translation .
They were instructed to make the correction in the sentences , if required .
The human post-edited parallel corpus as shown in tively .
The gold standard corpus , and the parallel corpus created synthetically is made available 5 .
We also crawl the Hindi sentences and back - translate them into English .
We build a Hindi-to - English NMT model to back - translate the crawled Hindi sentences .
We use the IIT Bombay Hindi-English general domain parallel corpus Kunchukuttan et al . ( 2018 ) to train a Hindi-to - English NMT model , and then fine - tune it over the human post-edited review domain parallel corpus .
The fine- tuned Hindi-to - English NMT model is used to back - translate the crawled monolingual Hindi sentences into English .
These back - translated ( BT ) English -Hindi synthetic parallel sentences are augmented with the human post-edited parallel sentences and referred to as ' Base + BT ' , shown in Table 2 .
Data Augmentation
We further enrich the training corpus ( in low-resource language ) following the data augmentation techniques as discussed below .
Word Embedding based Data Augmentation ( WDA ) Let us take one example : Original sample :
This phone is not good .
and New sample :
This handset is not nice .
In the original sample , the words ' phone ' and ' good ' are replaced by their most semantically close words ' handset ' and ' nice ' , respectively , based on the cosine similarity between their word embeddings .
To reduce the alignment complexity , we choose noun and adjective words as the replacement candidates because : ?
Hindi is morphologically richer than English .
One English verb token may be aligned to more than one Hindi tokens .
But nouns and adjectives are most likely to generate only one Hindi token .
For example : translation of word ' started ( verb ) ' ( 1 token ) can be '?
? ? ?' ' shuroo kar diya ' ( 3 tokens ) or ' ?
? ?' ' shuroo kiya ' ( 2 tokens ) .
Here , we see that for the word ' started ' , more than one translations possible with different token lengths .
To select the noun and adjectives for replacement , we use NLTKLoper and Bird ( 2002 ) Part- of- Speech ( PoS ) tagger for the English sentences .
A word2vec skip-gram model 6 Mikolov et al . ( 2013 ) is trained using the WMT14 monolingual English dataset and English sentences from the gold corpus .
Now for all the noun and adjective words , we find the most similar words using our trained word2vec model .
The words having the cosine similarity more than 0.75 will be considered as the substitutes .
A mapping dictionary is created with the triplet consisting of the ' original English word ' , 'its replacement English word ' and ' Hindi translation of the replacement word ' .
Now using the mapping dictionary , the tokens in the original corpus are replaced .
Source-target word alignment information using GIZA ++ tool ( Och and Ney , 2003 ) is used to replace the aligned Hindi tokens in the Hindi side .
But WDA does not guarantee to replace the original word with a similar context word as shown by an example in Table 3 . pre-trained model for the prediction which is trained using the default hyper parameters : 12 layers , 728 hidden units and 12 attention heads .
Here , we also find the replacements for nouns and adjectives only .
A list of noun and adjective tokens is created and in each English sentence , we mask the tokens by replacing the tokens with ' [ MASK ] ' which are in the list .
Now , the masked sequence is passed through the trained BERT model .
Since BERT contains the bidirectional sequence information , it can predict the most appropriate token for position ' i ' by considering the previous and next context words within the sentence .
For generating more augmented samples , we take the top 3 predicted words for position ' i ' and generate different samples .
We use Giza ++ alignment information to obtain the aligned positions between English and Hindi sentences , and the translated Hindi word of the newly predicted English word is placed at the Hindi side too .
A mapping dictionary similar to WDA is needed here to obtain the parallel counterpart of an augmented word .
Using CDA , multiple replacements can be found for a single masked token based on the context ( because here no fixed mapping dictionary is used ) .
Also , the substitute token suits the syntactic and semantic structure of the sentence .
In Table 3 , we can see in the example , " There are many offers for this smartphone " , ' applications ' , ' designs ' and ' models ' are predicted at the place of original hidden word ' offers ' .
Data Augmentation using Phrase Replacement ( PhrRep )
Here , we introduce a novel approach for data augmentation using similar phrase replacement strategy .
The method generates more diverse samples ( a phrase of multiple tokens is replaced with similar phrases of different token lengths ) in a single attempt .
Unlike the previous word augmentation techniques Fadaee et al . ( 2017 ) ; Gao et al. ( 2019 ) , here we replace a noun phrase ( NP ) with its semantically similar noun phrase ( NP ) .
To extract NP from the English sentences , we use the Stanford parser 7 and obtain the corresponding constituency trees .
To reduce the complexity in alignment mapping and trivial replacements , we filter out very large ( >8 tokens ) and very short ( < 3 tokens ) NPs .
Here , we refer to the replacements of very small NPs as trivial replacements since most likely they are already part of larger NPs , and get replaced when larger NPs are replaced .
To find the similarity among phrase embeddings , we use a BERT based sentence - transformer 8 Reimers and Gurevych ( 2019 ) .
For an original phrase P oi , its similar phrase P si is : P si = P j , [ i = ( 1 , ...
.,n ) and j = ( 1 , .... , n ) ] ( 1 ) P j = arg min j d(h i , h j ) ( 2 ) n is the number of NPs .
h is the hidden representation of the phrases .
d represents the Euclidean distance between the two vectors .
Equation 2 returns the index j of a phrase having minimum Euclidean distance d with the phrase at index i .
As shown in equation 1 , the respective phrase P j at index j is the most similar phrase to the original phrase P oi .
Figure 1 shows the mapping of the original phrase ' powerful selfie phones ' with phrase ' phones with powerful selfie camera ' having the Euclidean distance d = 0.094 , minimum in the distances with all the other phrases .
Further , Hindi counterparts of the English NPs are extracted from the original parallel data itself using the alignment information .
Noise Augmentation
We create a noisy copy of the original corpus .
To deal with character missing , article missing , punctuation missing and the dropping offs of starting noun-pronouns , we introduce various noise in the original training corpus .
In similar ways to the prior works Vaibhav et al . ( 2019 ) ; Anastasopoulos et al. ( 2019 ) , we also drop the characters randomly from the source ( English ) side , but with some additional rules .
?
It is observed in the reviews that ' vowels ' are most likely to be dropped by the users .
For example , for a word ' phone ' , ' phne ' and " phon ' are most likely to occur compared to the " pone ' and " phoe ' .
So in each English sentence , along with dropping the random characters we make sure that vowels are also dropped in a few words .
?
We randomly drop the articles ' the ' , ' a ' and ' an ' from the English side because we observe that in reviews users often drop the articles .
?
Users often write reviews without mentioning the starting nouns or pronouns .
We drop the starting nouns and pronouns randomly from the sentences .
The PoS tagger was used to mark the words to be dropped .
For example , " was planning to buy this " or " am happy with the phone " .
Here , when we pick the tokens randomly for noise injection ( char drop ) we call it random noise ( RndNoise ) insertion .
All these noises are introduced into a copy of the original corpus .
It is then augmented with the original corpus .
This provides noisy and correct source versions for a target sentence .
Attention Guided Noise Augmentation ( AttnNoise ) x 2019 ) introduced noise in the training data by randomly dropping characters from the source words .
To make our model robust towards misspellings , article missing , punctuation and word missing , we also drop the words or introduce the character inconsistencies in words .
Instead of executing these randomly , we follow a guided approach to drop a word or character ( s ) from these words .
To do this , we take the help of attention weights between the source-target pairs .
We call this technique as attention guided noise augmentation ( AttnNoise ) .
1 x 1 x 1 x 2 x 2 x 2 x n?1 x n?1 x n?1 x n x n x n y 1 y 1 y 1 W11 W12 . W1n y 2 y 2 y 2 W21 W22 . W2n y m?1 y m?1 y m?1 . . . Wm Algorithm 1 Attention guided noise augmentation ( AttnNoise ) Notations : s i s i s i ={x 1 , i ) do ? for each token if ind [ x j ] ? ? lM inAttn then if ind [ x j ] ?
lM axAttn then sN i .append ( dropChar ( x j ) ) else sN i .append( x j ) else if ind [ x j ] ? ? lM axP rob then dropWord( x j ) else sN i .append ( dropChar ( x j ) ) return ( sN i ) procedure Word - Prob(s i , S ) for k ? 0 , ... , len(s i ) do ? for each token p = ( # x k in S / # all tokens in S ) lP rob i .append ( p ) return ( lP rob i )
We have a corpus D with parallel pairs [ S , T ] , where S and T are the collection of source and target sentences , respectively .
s k and t k represent a pair of k th source and target sequences in S and T , respectively .
Each s k = {x 1 , x 2 , ... , x n } is a sequence of n source tokens and t k = {y 1 ,y 2 , ... , y m } is a sequence of m target tokens .
We calculate the average attention for each source token as shown in Table 4 .
All the attention heads are considered here .
We drop a fraction of tokens from the source sequence having low average attention weight , and introduce noise in a fraction of tokens having high average attention weight .
Method NOISE in Algorithm 1 describes the steps involved in the AttnNoise .
To decide if a token comes under the low or high attention weight category , we choose some percentage value as the threshold .
For example , we have a list AvgAttn i of source sequence s i which has 15 tokens .
For our experiments , we empirically decide to drop the bottom 10 % of total tokens in s i having minimum average attention weight ( i.e. 10 % of 15 = 2 tokens , so we drop 2 tokens having the lowest weights ) .
Similarly , top 25 % of tokens in s i having high weights are made noisy by dropping the characters from them .
We also calculate the occurrence probability of the source tokens of s i using the method WORD - PROB in Algorithm 1 to know whether any token is frequent or rare in the vocabulary .
A token with less occurrence probability is said to be rare and we do not drop any rare token even if it has the low average attention weight .
The rare Table 5 : BLEU and TER scores of different systems for different datasets of English -Hindi , English - French and English - German language pairs .
Also for En? Hi translation : ( 1.A ) Trained on IITB - Hin-Eng corpus and tested over newstest2014 , ( 1.B ) Trained on IITB - Hin-Eng corpus and tested over product review testset , ( 1.C )
Trained on product review corpus and tested over newstest 2014 .
tokens correspond to those having high attention weights , and instead of dropping these from the source sequence , we insert noise into it .
To prevent the dropping of any rare word having low attention weight , we increase the percentage value for the threshold .
Here , the top 50 % tokens in s i having low occurrence probabilities are considered as the rare tokens .
Since our target is to avoid the rare words to be dropped due to low attention weight , the threshold of 50 % is taken with an assumption that the rare tokens would fall in this range only otherwise that token is not rare .
After inserting the noise in all the source sentences , we make their pairing with their respective target sentences .
Finally , this noisy parallel corpus is augmented to the original parallel corpus for final source - to - target training .
Experiment Setup Our translation model is based on the Transformer architecture Vaswani et al . ( 2017 ) .
We use the Sockeye toolkit 9 Hieber et al . ( 2018 ) for our experiments .
Table 2 gives the size of the training samples for different systems .
We also experiment our proposed method on the IIT Bombay English - Hindi parallel corpus Kunchukuttan et al . ( 2018 ) .
To perform experiments for the English - to - French translation , we use a part ( for true resource - poor setting ) of the UN - corpus Ziemski et al . ( 2016 ) for training and newstest2015 Bojar et al . ( 2015 ) as the test set .
We also perform experiment for English - German translation and test over the IWSLT 2017 testset 10 .
The tokens of the training , test and validation sets are segmented into subword units Sennrich et al . ( 2016 ) by applying 4,000 BPE merge operations at the source and target sides .
Our training set- up details are given below : No. of layers at the encoder and decoder sides : 6 each ; 8 - head attention ; Hidden layer size : 512 ; Embedding vector size : 512 ; Learning rate : 0.0002 ; Minimum batch size : 4800 tokens ; early stopping is used to terminate the training .
Results and Analysis From Table 5 , we can see significant BLEU score improvement over the baseline using various data and noise augmentation techniques .
Using human translated and backtranslated corpus , we train the Base + BT model which yields the BLEU improvement of 0.83 .
Further , with data augmentation techniques , WDA and CDA , we obtain additional 3.48 and 4.47 BLEU score improvement , respectively .
The random noise augmentation
In total , with ' Base + BT + PhrRep + AttnNoise ' method , we achieve a total of 6.67 BLEU improvement over the ' Base ' model .
We also perform experiment over the MTNT testset which is a user generated English - French corpus .
' PhrRep ' method yields 1.92 BLEU over the baseline score .
Further , sing ' AttnNoise ' method with ' PhrRep ' gives additional 0.63 BLEU improvement .
We also apply our proposed PhrRep technique over the benchmark English -Hindi testset newstest2014 Bojar et al . ( 2014 ) .
As shown in Table 5 , we achieve a 1.14 BLEU score improvement over the baseline .
We perform statistical significance tests 11 Koehn ( 2004 ) , and found that the proposed model attains significant performance gain with 95 % confidence level ( with p=0.013 which is < 0.05 ) .
We also apply the PhrRep technique for English - to - French translation .
To test the performance in a low-resource scenario , we perform our experiment over a small part of data i.e.
300 k parallel sentences .
We achieve a gain of 1.55 BLEU ( statistically significant ) over the baseline .
For English - to - German translation task , it also yields significant improvement 12 of 0.91 BLEU over the baseline .
Analyzing the Robustness
To analyze the models ' performance on the product domain testset , we manually tag the test sentences on the basis of major inconsistencies .
We divide the testset into the following 7 categories : misspell ( MS ) : 10.09 % , wrong Grammar ( GR ) : 6.94 % , punctuation mistake ( Punc ) : 7.83 % , sub-verb disagreement ( SV ) : 2.56 % , word missing ( WM ) : 5.99 % , article missing ( AM ) : 1.94 % and word order ( WO ) : 3.67 % .
The distribution in percentage shows how much of the test sentences lie in which noise category .
Figure 2 depicts the performance of all the models in presence of different noises .
Augmented techniques outperform the ' Base ' and ' Base + BT ' models in all the major categories .
Evaluation results show that ' PhrRep + RndNoise ' model outperforms all the other word augmentation models .
Further , introducing ' AttnNoise ' in ' PhrRep + AttnNoise ' improves the performance over ' PhrRep + RndNoise ' .
It shows that the guided noise augmentation is better than the random noise augmentation based technique .
For AM
In this paper , we have presented an effective NMT model for English -to - Hindi product review translation .
As there was no parallel corpus in this domain , we , therefore , crawled English reviews , pre-processed , filtered , translated into Hindi and corrected using professional human translators .
Hindi descriptions of electronic gadgets are crawled and back - translated into English using human translated corpus and again augmented with human translated corpus .
We make the parallel corpus freely available .
We have introduced a novel phrase replacement based augmentation technique ( PhrRep ) which replaces the whole noun phrase ( multiple tokens at a time ) with an alternative noun phrase to generate the new training sample in fewer attempts .
For robustness in our model , we use a novel attention guided noise augmentation technique ( AttnNoise ) which drops the words or makes them noisy on the basis of attention weights .
Using phraseRep and AttnNoise , for En?
Hi review translation , we achieve an improvement of 6.67 BLEU over the baseline .
In order to show the generic behavior of our model , we also evaluate it on the English - French and English - German benchmark datasets , demonstrating the effectiveness of our proposed approach .
In future , we shall focus on the spelling variations and code-mixed challenges in the input and output sentences .
A bigger English-to -Indic multilingual product review translation system will be investigated .
Acknowledgement Authors gratefully acknowledge the unrestricted research grant received from the Flipkart Internet Private Limited to carry out the research .
Authors thank Muthusamy Chelliah for his continuous feedbacks and suggestions to improve the quality of work ; and to Anubhav Tripathee for gold standard parallel corpus creation and translation quality evaluation .
1 http://mohua.gov.in/cms/urban-growth.php
Proceedings of the 18th Biennial Machine Translation Summit Virtual USA , August 16 - 20 , 2021 , Volume 1 : MT Research Track Source ( A ) osm product .
i really love it .
osm camera quality ... nice one Reference ? ? ? ? ?. ? ? ? ? ? ? . ? ? ? ? ? ? ? ? ? ? ? ( Transliteration ) bahut badhiya prodakt .
mujhe yah pasand hai .
bahut badhiya kaim kvaalitee ?
achchha hai Gen-NMT ? ?. ? ? ? ? ? ? ? ? . ? ? ? ? ? ?... ? ? ( Transliteration ) osam utpaad .
main vaastav mein ise pyaar karata hoon .
osam kaimara kvaalitee ?
achchha hai Source ( B ) NYC product , and cloth quilty is too good Reference ? ? , ? ? ? ? ? ? ? ? ? ? ( Transliteration ) achchha prodakt , aur kapade kee kvaalitee bahut badhiya hai Gen-NMT NYC ? , ? ? ? ? ? ? ? ( Transliteration ) nyc utpaad , aur kapada rajaee bahut achchha hai Source ( C ) Nice Mobile and value for money Refernce ? ? ? ? ? ? ? ( Transliteration ) achchha mobail aur paisa vasool Gen-NMT ? ? ? ? ? ? ? ? ? money ( Transliteration ) achchha mobail aur paise ke lie mooly money Table 1 : Sample outputs for En? Hi translation from sources with various inconsistencies .
Here , Gen-NMT : Generic NMT ( A ) Abbreviations and colloquial terms , ( B ) Spelling mistake and ( C ) Emojis
Figure 1 : 1 Figure 1 : Synthetic sample generation using phrase replacement ( PhrRep )
Table 2 : 2 Parallel corpus size .
Here , A : Product review dataset , B : IIT -Bombay English -Hindi dataset Kunchukuttan et al. ( 2018 ) , C : UN - Corpus English - French dataset Ziemski et al . (
Table 3 : 3 Table 2 is divided into training , development and test set consisting of 19,457 , 599 and 2,539 parallel sentences , respec-Samples generated using WDA , CDA and PhrRep approaches .
Sentence
There are many offers for this smartphone WDA
There are many provides for this smartphone CDA
There are many applications / designs / models for this smartphone PhrRep
There are multiple features in my new smartphone
Table 4 : 4 Attention weight matrix during source- to- target inference .
Here , Wij : attention weight between i th target token and j th source token Most of the existing literature Vaibhav et al . ( 2019 ) ; Anastasopoulos et al . (
x 2 , ... , x n } , i th sequence .
AvgAttn i AvgAttn i AvgAttn i : list of avg . attention weights of tokens in s i lP rob i lP rob i lP rob i : list of probability ( occurrence frequency ) of tokens in s i sN i sN i sN i : i th noisy source sequence lMinAttn : indexes of bottom 10 % min values in AvgAttn i .
lMaxAttn : indexes of top 25 % max values in AvgAttn i .
lMaxProb : indexes of top max 50 % values in lP rob i . ind : index of a token in s i .
x j x j x j : token at j th position in s i . procedure Noise (s i , AvgAttn i , lP rob i ) for j ? 0 , ... , len(s
RndNoise ) in CDA model also shows additional improvement of 0.48 BLEU point .
In total , with noisy word augmentation methods , we achieve 5.78 BLEU improvement over the base model .
After using our proposed phrase replacement ( PhrRep ) technique , we outperform the word augmentation techniques ' Base + BT + CDA + RndNoise ' with 0.47 BLEU score .
As mentioned in Table2 , ' PhrRep + RndNoise ' model outperforms all the models with comparatively less parallel data .
Further adding AttnNoise with ' PhrRep ' the model ' Base + BT + PhrRep + AttnNoise ' gives 0.42 additional BLEU improvement .
Base Base +BT Base+BT+WDA Base+BT +CDA Base+BT +CDA + RndNoise Base+BT + Phrase + RndNoise Base+BT + Phrase + AttnNoise 50 45 40 35 30 MS GR Punc SV WM AM WO Figure 2 : BLEU scores of models in the presence of various kinds of noises in input sentence .
(
https://pypi.org/project/pyspellchecker/ 3 https://github.com/facebookresearch/moe/tree\\/master/data 4 https://en.wikipedia.org/wiki/List\_of\_mobile\_phone\_brands\_by\_country
https://www.iitp.ac.in/~ai-nlp-ml/resources/data/review-corpus.zip 6 https://code.google.com/archive/p/word2vec/
https://nlp.stanford.edu/software/lex-parser.shtml
8 https://github.com/UKPLab/sentence-transformers
https://github.com/awslabs/sockeye 10 https://wit3.fbk.eu/2017-01
https://github.com/moses-smt/mosesdecoder/blob/master/scripts/analysis/ bootstrap-hypothesis-difference-significance.pl 12 p<0.005
