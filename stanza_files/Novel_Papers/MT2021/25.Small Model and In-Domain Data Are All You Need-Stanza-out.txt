title
Small Model and In-Domain Data are All You Need
abstract
I participated in the WMT shared news translation task and focus on one high resource language pair : English and Chinese ( two directions , Chinese to English and English to Chinese ) .
The submitted systems ( ZengHuiMT ) focus on data cleaning , data selection , back translation and model ensemble .
The techniques I used for data filtering and selection include filtering by rules , language model and word alignment .
I used a base translation model trained on initial corpus to obtain the target versions of the WMT21 test sets , then I used language models to find out the monolingual data that is most similar to the target version of test set , such monolingual data was then used to do back translation .
On the test set , my best submitted systems achieve 35.9 and 32.2 BLEU for English to Chinese and Chinese to English directions respectively , which are quite high for a small model .
Monolingual Data Filtering Using Language Model
In terms of monolingual data , I collected more than 20 million Chinese sentences and more than 15 million English sentences from various websites .
Introduction
I participated in the WMT shared news translation task and focus on the English and Chinese language pair .
This language pair is challenging due to the plentiful in- domain bitext training data and abundant monolingual data .
High resource means fierce competition , many high - tech companies and universities chose this language pair also .
My neural machine translation system is developed using base transformer ( Vaswani et al. , 2017 ) architecture and the toolkit I used is THUMT ( Zhang et al. , 2020 ) .
Rules and word aligning model are used to clean parallel data .
Language model is used to clean monolingual 1 http://mteval.cipsc.org.cn:81/agreement/description data .
I use a base transformer ( Vaswani et al. , 2017 ) architecture since I have only one GPU .
The following techniques are used on model training : a. Increase the number of encoder layers to 12 to further improve the encoder 's representation capability ; b. Back translation are applied to fully utilize the monolingual corpus .
c. Shared vocabulary is used for better performance .
d. Four different models using diversified data are trained for ensemble decoding .
Data Filtering and Selection
The parallel data is mainly from CCMT Corpus 1 , and the monolingual data is collected from the internet .
I did not use any other datasets since I think they are not highly related to this news translation task .
To evaluate my model 's performance , I merged the test set from WMT2017 to WMT2020 to build a big development set .
The following rules are used for a simple cleaning : ? Remove duplicated sentences .
? Remove the sentences containing special characters .
? Remove the sentences containing html addresses or tags .
Afterwards , language models are used to filter the monolingual data .
For English sentences , lmscorer 2 is used to calculate a score for each sentence , which is the mean of tokens ' probabilities .
The pre-trained model used for English is GPT - 2 ( Radford et al. , 2019 ) .
3 For Chinese sentences , a pre-trained Chinese GPT - 2 ( Radford et al. , 2019 ) 4 model is used to calculate a score for each sentence .
Then , the English and Chinese sentences are filtered by their scores .
GPT - 2 ( Radford et al. , 2019 ) is a large transformer - based language model with 1.5 billion parameters , trained on a dataset of 8 million web pages .
GPT - 2 ( Radford et al. , 2019 ) is trained with a simple objective : predict the next word , given all of the previous words within some text .
The threshold I used is determined based on my personal evaluation on the text .
After calculating the scores for all the sentences , I sampled the sentences by their scores and perform a language quality check .
I started from the extremely low scores and the extremely high scores , and then gradually move the scale from the two ends to the middle until I find that the language quality is up to my standard .
There are about 16 million Chinese sentences and 10 million English sentences left after filtering using language model .
Parallel Data Filtering Using Rules For CCMT parallel Corpus and synthetic parallel corpus from back translation , I used the following rules to filter data .
a. Remove duplicated sentence pairs .
b. Remove the lines having identical source and target sentences .
c. Remove the sentence pairs containing special characters .
d. Remove the sentence pairs containing html addresses or tags .
e. Remove the sentence pairs with empty source or target side .
Parallel Data Filtering Using Word Alignment
In order to get word alignment results , fast_align ( Dyer et al. , 2013 ) is used on the CCMT Corpus filtered by rules , then extract - lex 5 is used to generate bilingual phrase tables .
The phrase tables are then pruned according to probabilities .
Afterwards , I use the pruned phrase table to measure the confidence of the sentence pairs being mutual translations .
The confidence score is calculated like this : check each token of the target sentence to find if it has a counterpart in the source side , then perform this operation in the reverse direction , the final confidence score is calculated by summing up the two percentages from two respective directions and then getting the average .
Then the confidence score is used to remove bad sentence pairs .
The sentence pairs with confidence scores below 0.6 are discarded .
In this way , I finally got a high quality parallel CCMT Corpus .
System Description
This section illustrate how I train the model step by step .
Data pre-processing
For data preprocessing , I use the tokenizer developed on my own to process both Chinese and English .
Chinese text ( including punctuations and numbers ) is split to single character level .
I keep the upper and lower case letters of English as they are , since I believe they are also important features for the model .
Numbers in English text are also split into single digits .
I use byte pair encoding ( BPE ) to create a shared vocabulary , so that the vocabulary size is reduced to 45467 .
I also wrote a post-processor to restore the Chinese and English text to normal form .
Normal Model Training To As shown in Table 1 , using the same filtered CCMT Corpus , the BLEU scores of models with deeper encoder ( 12 - layer-encoder , 6 - layerdecoder ) are slightly higher than that of the base version .
Back translation is a useful data augmentation technique to boost model performance with target side monolingual data .
The technique starts from training a target to source translation model using initial bilingual corpus , which is later used to translate the monolingual data in the target language back to source language .
Then the synthetic backtranslated corpus is concatenated with the original bilingual corpus to train the source to target translation model .
After the source to target model is enhanced , the same method can be applied again to train the back - translation system in the reversed direction .
I repeat this process using half of the filtered monolingual data for several iterations until the BLEU is not increasing .
Training on In-domain Data BERT ( Devlin et al. , 2019 ) is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers .
Before feeding word sequences into BERT ( Devlin et al. , 2019 ) , 15 % of the words in each sequence are replaced with a [ MASK ] token .
The model then attempts to predict the original value of the masked words , based on the context provided by the other , nonmasked , words in the sequence .
After the WMT2021 test set was released , I first translated the Chinese and English test sentences to target versions using the above models , then I generated feature representations for the target versions of the test sentences using pre-trained English BERT ( Devlin et al. , 2019 ) 6 and Chinese BERT ( Devlin et al. , 2019 ) 7 models .
The example representations are shown in Figure 1 and Figure 2 .
Then , the similarity scores are used to find out monolingual sentences that are most similar to the WMT2021 test set .
For each test set sentence , hundreds of monolingual sentences are extracted .
In order to determine a threshold score , I randomly sampled 100 test set sentences and their extracted counterparts .
Then I checked their similarities and scores using my personal linguistic competences in these two languages .
The determined threshold score was then used to automatically extract indomain data .
Finally , I extracted around 550 thousand Chinese sentences and 420 thousand English sentences as in-domain monolingual data .
These sentences are then divided into four equal portions .
On the basis of the best models using back translation and the first half of monolingual data , I use four portions of in-domain English data and four portions of in- domain Chinese data to do back translation until the BLEU stops increasing .
Therefore , I get four in- domain English to Chinese and four in - domain Chinese to English translation models .
These models are then ensembled to build two most powerful models for each direction .
Results
The BLEU scores on the aforesaid big development set ( I merged the test set from WMT2017 to WMT2020 to build a big development set ) for each corpus plus model combination are shown in Table 1 .
On the WMT 2021 test set , my best submitted systems achieve 35.9 and 32.2 BLEU for English to Chinese and Chinese to English directions respectively , which are even higher than most of the systems from famous high - tech companies .
Conclusion
This paper describes Hui Zeng 's translation systems ( ZengHuiMT ) for the WMT2021 news translation shared task .
The potential of small model plus in-domain data is explored .
I am pleased to argue that , with high quality in- domain data , small model could achieve BLEU scores comparable to that of huge models .
Figure 1 : 1 Figure1 : The BERT representation of " I like this competition very much " , the tensor shape is [ 1 , 9 , 768 ]
