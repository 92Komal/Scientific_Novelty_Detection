title
MISS@WMT21 : Contrastive Learning -reinforced Domain Adaptation in Neural Machine Translation
abstract
In this paper , we describe our MISS system that participated in the WMT21 news translation task .
We mainly participated in the evaluation of the three translation directions of English - Chinese and Japanese - English translation tasks .
In the systems submitted , we primarily considered wider networks , deeper networks , relative positional encoding , and dynamic convolutional networks in terms of model structure , while in terms of training , we investigated contrastive learningreinforced domain adaptation , self-supervised training , and optimization objective switching training methods .
According to the final evaluation results , a deeper , wider , and stronger network can improve translation performance in general , yet our data domain adaption method can improve performance even more .
In addition , we found that switching to the use of our proposed objective during the finetune phase using relatively small domain-related data can effectively improve the stability of the model 's convergence and achieve better optimal performance .
Introduction News translation ( Bojar et al. , 2017 ( Bojar et al. , , 2018 Barrault et al. , 2019 Barrault et al. , , 2020 is one of the most prominent and appealing tasks in machine translation evaluation ( Wu et al. , 2020 b ; ) .
Our MiSS system took part in the WMT21 news translation task , including English ?
Chinese ( En ? Zh ) , Chinese ? English ( Zh ? En ) , and Japanese ? English ( Ja ? En ) translation directions .
We developed translation systems for this year 's submission to investigate machine translation techniques from two perspectives : model structure and model training .
All of the data used by the submitted systems is constrained .
Due to a lack of training resources , the English -> Japanese translation direction is only investigated from the model structure perspective .
From the perspective of model structure , we choose the Transformer ( Vaswani et al. , 2017 ; Li et al. , 2021c ) model based on self-attention , which is extensively utilized in neural machine translation systems , as our basis Li et al. , 2020d ) .
On this strong foundation , we opt to simply deepen the model by increasing the number of encoder layers or widen the model by increasing the hidden size of the model to obtain a deeper or wider model .
When deepening or widening the model , we found that there is no need for additional sophisticated structure design ( e.g. , layer drop ( Fan et al. , 2020 ) / sublayer drop ( Li et al. , 2021a ) ) or training strategy when there is adequate training data available .
In addition to Transformer architecture , propose a dynamic convolution structure that can perform competitively or better to the self-attention structure .
Follow the practice in WMT20 ( Wu et al. , 2020a ) , we also applied the dynamic convolution architecture as another basis .
According to our preliminary results on the development set , domain has a significant impact on performance , despite the fact that we are working with the resource - rich En-Zh and En- Ja language pairs .
This year 's submissions are mostly concerned with utilizing training approaches to mitigate the impact of domain differences .
Specifically , we first use data in all hybrid domains to train the initial NMT model , and then , based on sentence embedding model enhanced by contrastive learning , the parallel / monolingual corpus is filtered monolingually or cross-lingually , and the filtered domainrelated parallel corpus is used for further finetuning , and the domain-related monolingual corpus is used for in-domain back - translation enhancement .
In addition , we also adopted a self-supervised training method to train the model on the given source text of the test set and its domain-related monolingual text obtained by filtering .
In self - supervised training , we combine our Data-dependent Gaussian Prior Objective ( D2 GPo ) objective to alleviate the collapse due to non-golden targets .
In the finetune stage with the domainrelated parallel corpus , we adopted the training strategy of switching the optimization objective from the MLE to our proposed Dual Skew Divergence ( DSD ) .
The results demonstrated that switching to the DSD objective resulted in improved convergence .
From the evaluation results , we observe substantial improvements over the strong baseline with 4.3 ( En ? Zh ) , 4.8 ( Zh ? En ) , 3.2 ( Ja ? En ) BLEU scores on the development sets , respectively .
The gains can be attributed to larger model capacity and better training strategies .
And the results suggest that the cost of domain adaptation to improve performance is less than the cost of increasing model capacity .
Model Perspective
With the development of deep learning in NLP Cai et al. , 2018 ; Li et al. , 2021d ) , model ensembling can usually produce better results than single models , and the bigger the difference between the models used for ensembling , within a certain limit , the higher the improvement will be .
As a result , we chose four distinct typical architectures as the basis for single NMT models and trained them on the same data .
The detailed parameters of each model architecture are shown in Table 1 . Deep Transformer Some related works Li et al. , 2020a Li et al. , , 2021a have revealed that deep networks have great advantages in NMT performance compared to shallow networks recently .
Based on the Transformer NMT model architecture , we found that in the presence of sufficient training data , merely increasing the number of stacked layers of the encoder can fulfill the goal of deep Transformer without the use of additional initialization , dropout , or layer skipping techniques .
Wide Transformer Recent researches
Wu et al. , 2020a ; Zhang et al. , 2020a ; Wu et al. , 2020 b ; Meng et al. , 2020 ) have demonstrated that , in addition to deepening the NMT model , widening the model can also effectively improve translation performance , with increasing the feed -forward network ( FFN ) size in the Trans - [ 3 , 7 , 15 , 31 , 31 , 31 , 31 , 31 , 31 , 31 , 31 , 31 , 31 , 31 , 31 , 31 , 31 , 31 , 31 , 31 ] and leave the other settings unchanged from the original model .
Relative Position Encoding Because selfattention in the convention Transformer model is position - independent , the encoded features must be enhanced with explicit positional information for natural language processing .
Absolute position encoding is usually employed in the Transformer NMT model .
Shaw et al. ( 2018 ) proposed to add relative position encoding ( RPE ) for improving self-attentional features and shown additional performance gains .
We also applied relative position encoding to the Wide Transformer model and created another strong baseline .
We use the identical vocabulary and data to train these four baseline models separately , and then average the best 5 checkpoints in each model 's training phase to generate the final model output ( 2020a ) 's experience , the best 5 checkpoints are determined based on the BLEU metric on the development set rather than the perplexity ( PPL ) metric .
Furthermore , we applied the D2 GPo objective in the training process to obtain more stable convergence and decrease the impacts of overfitting resulting from the training set 's noise .
Training Perspective Contrastive Learning -reinforced Domain Adaptation Data domain issues have been found to have a significant impact on machine translation performance ( Saunders , 2021 ) .
The official training data is of hybrid domain , despite the fact that the evaluation task is news translation .
And , while news translation corpora can be deemed to be in the news domain , there are significant variances in news styles within the same domain .
As a result , one of the keys to performance enhancement will be how to utilize the data training model that is closer to the evaluation data domain and style .
Using languages L 1 and L 2 as an example , the data that may be used comprises the parallel corpus D P L 1 ?L 2 , as well as their respective large-scale monolingual corpus D M L 1 and D M L 2 .
Parallel corpora are typically utilized for direct training of NMT models , whereas monolingual corpora are used for back - translation ( Edunov et al. , 2018 ) and self-supervised training ( Jiao et al. , 2021 ) .
The domain filtering method can be utilized in these three training procedures to create corpus whose domain is more similar to the development and test sets .
Instead of relying on the co-occurrence probabil-ity of the surface tokens in the sentence , we based the domain filtering on the hypothesis that the more similar the sentence representations generated by the Transformer encoder are , the more likely they are to be dispersed in the same domain .
Because the current Transformer encoder 's representation is based on the bidirectional and full attention of all tokens , the combination and order of tokens have a significant impact on the final representation , the sentence representation is adequate for capturing domain information .
As a result , we use the sentence embedding distance to measure the domain similarity .
We leveraged a universal paraphrastic sentence encoder ( Wieting et al. , 2016 ; Ethayarajh , 2018 ; to embed each given sentence to a dense representation .
On a large scale monolingual corpus , we train our own monolingual and multilingual sentence encoder , a Transformer that has been pre-trained using masked language modeling ( Devlin et al. , 2019 ; Zhang et al. , 2020c ; Li et al. , 2021 b ) , with the XLM toolkit ( Conneau et al. , 2020 ) and fine-tuned to maximize cosine similarity between similar sentences .
Contrastive learning seeks to acquire effective representation by pulling semantically close neighbors and pushing nonneighbors apart ( Hadsell et al. , 2006 ) .
Since this criterion precisely meets the requirements of sentence representation learning , we use contrastive learning to finetune the pre-trained sentence encoder .
According to the domain adaptation requirements in actual machine translation , the trained sentence encoder needs respond to four scenar-ios : Original Input Monolingual Filter , Translated Input Monolingual Filter , Original Input Crosslingual Filter , Translated Input Cross-lingual Filter .
Because the fourth scenario can be covered by the first , we only employ the first three scenarios in our experiment .
For all scenarios , we first follow Gao et al . ( 2021 ) 's approach to perform unsupervised training in which the input sentence itself is used as a positive instance due to there will be some differences between the sentence representations of the two pass input with the presence of the model dropout , and other sentences in the in - batch are used as negative instances .
The unsupervised contrastive learning -trained monolingual sentence encoder can be used directly as an evaluator of the similarity of sentences in the same language and to mine similar sentences from the sentence bank .
However , for the non-gold translated sentences filtering , we apply the baseline NMT models to translate parallel corpus and to back - translated monolingual corpus to generate pseudo- paraphrase corpus .
And then triplet loss is used to fine - tune the unsupervised sentence encoder : L( x , y ) = max ( 0 , ? ? cos ( x , y ) ) + cos( x , y n ) , where positive pairs ( x , y ) are paraphrases from translation or back - translation , y n are in - batch negative instances .
Likewise , we still need cross-language filtering , therefore we use parallel corpus instead of synthetic pseudo-restatement corpus and triplet loss for additional finetuning on the multilingual sentence encoder .
As shown in Figure 1 , taking the L 2 in-domain source sentences in development set as an example , we first use the initial NMT model to translate these sentences to L 1 translated text .
The different trained sentence encoder is then used to encode these sentences and the large-scale monolingual or parallel corpus based on different scenarios respectively .
Then , using the faiss toolkit 1 , a query procedure is performed to locate related in -domain monolingual or parallel corpora with similarity calculation and ranking .
Back-translation and Self-supervised Training Using the in- domain monolingual and parallel cor - 1 https://github.com/facebookresearch/ faiss pus , we may train the initial model using backtranslation and self-supervised training approaches .
For back - translation , we leverage the original multiple NMT models to translate these monolinguals into various pseudo-parallel corpora , and then combine them with the in-domain parallel corpus to finetune the NMT model .
For self - supervised training , we use a variety of models to perform ensemble translation on the in-domain monolingual text as the translation target and combine the indomain translation corpus to fine - tune the model .
In the specific implementation , we perform backtranslation and self-supervised training consecutively such that the self-supervised training stage can exploit the stronger NMT model trained during the back -translation stage .
Optimization Objective Switching Training
It is easier to fall into a local optimum in the process of back -translation and self-supervised training because there are relatively fewer in - domain data and input or output in part of the data utilized is not gold .
According to our experience in L DSD = ? 1 n n i=1 [?( t ) y i log ( ( 1 ? ? ) ? i + ?y i ) ?( 1 ? ?( t ) ) ?
i log (? i ) + ( 1 ? ?( t ) ) ?
i log ( ( 1 ? ?) y i + ? i ) ] , where y i is the i-th token in the target sequence y , ?i is the i-th predicted token , ? is a hyperparameter in ?- skew divergence ( Lee , 1999 ) , and ?( t ) is the controllable weight from the PID controller .
4 Data Setup for Chinese , we use pkuseg ( Luo et al. , 2019 ) as the word segmentor .
We adopt a joint byte pair encoding ( BPE ) ( Sennrich et al. , 2016 ) with 44 K operations for subword vocabulary in English and Chinese .
Punctuation normalization is not employed to preprocess the training data in order to prevent complex post-processing of punctuation restoration .
For English post-processing , we use the script in Moses to de-tokenize the translation , whereas for Chinese , we employ sacremoses 3 for de-segmentation .
English ?
Japanese
In the English ?
Japanese translation , data for training were combined from ParaCrawl v7.1 , News Commentary v16 , Wiki Titles v3 , WikiMatrix , The Kyoto Free Translation Task Corpus , and TED Talks .
Similarly , the Japanese sentences are segmented using the Mecab 4 segmentor , while the English sentences are processed using the Moses tokenizer .
The size of the English and Japanese joint BPE is also set to 44K .
In post-processing , Moses script and sacremoses are also employed for detokenization .
We merged the whole news - crawl corpus for monolingual data .
However , in Chinese and Japanese , news - crawl corpus alone is insufficient to train the sentence encoder , so we sampled some data from the common-crawl corpus and eventually produced the data in English , Chinese , and
Model Training
All of our NMT models are built using the Fairseq toolkit .
Except for the switching training phase , all models are optimized with Adam optimizer , and SGD optimizer is utilized for optimization training when switching to DSD loss .
During the baseline model training process , the learning rate is scheduled using the inverse sqrt scheduler with 4000 warm - up steps , maximum learning rate 5e - 4 , and betas ( 0.9 , 0.98 ) .
Each model is trained on 8 NVIDIA V100 GPUs , with batch size limited to 8192 tokens per GPU .
FP16 is emploted to save GPU memory and speed up calculations .
To increase the virtual batch size , we set the gradient update steps to 8 during the training phase .
The label smoothing and dropout values are both set to 0.1 .
In the finetuning stage , we utilize a smaller batch size , 4,096 tokens per GPU , and train the model at a fixed learning rate of 1e - 4 .
Sentence encoder models are developed with the XLM toolkit , and the architecture is based on the BERT - base .
The hidden size , heads , hidden layers , and FFN size are 768/12/12/3072 respectively .
During training , a early stop mechanism is applied in which the training will stop when the PPL on the development set does not decrease after 25 epochs .
Table 2 shows the results on the development sets as well as the official evaluation results on the WMT21 test sets .
First , when comparing Deep Transformer , Wide Transformer , and Transformer - big , we observed that increasing the number of model layers or widening the model to increase the number of model parameters can result in large performance benefits .
Second , Deep DynamicConv has shown comparable results to Deep Transformer in multiple data sets , demonstrating that DynamicConv is a viable replacement option for Transformer .
Third , the Deep Transformer w/ RPE model outperforms Deep Transformer model in most circumstances , demonstrating that machine translation benefits from additional relative position encoding information .
Fourth , in - domain back - translation ( ID - BT ) and in- domain self-supervised training ( ID - ST ) improve the model 's performance substantially more than the increased model parameters , indicating that the data domain is a primary factor limiting translation performance .
Furthermore , these enhancements demonstrate that our domain adaption approach of contrast learning - reinforced is a effective approach .
Finally , we performed ensemble on the four finetuned baselines and received even higher results , demonstrating that the models of the four architectures differ from each other .
Conclusion
In this paper , we introduce our MISS translation system , which participated in the WMT21 news translation task .
We developed a new contrast learning -reinforced domain adaptation strategy in this work , and the experimental findings suggest that this method may significantly increase translation performance .
Furthermore , we conducted experiments on a range of model architectures .
Our domain adaption strategy improved these strong baseline models significantly , illustrating the method 's generality and indicating that the performance deficiency is not due to a specific model structure .
Figure 1 : 1 Figure 1 : Illustration for contrastive learning -reinforced domain adaptation
