title
Findings of the 2021 Conference on Machine Translation ( WMT21 )
abstract
news.com.au ( 1 ) , novinite.com ( 1 ) , Chinese China News ( 76 ) , Hunan Ribao ( 5 ) , Jingji Guancha Bao ( 3 ) , Macao Government ( 2 ) , Nhan Dan ( 3 ) , RFI Chinese ( 6 ) , VOA Chinese ( 3 ) , Xinhua ( 57 ) , tsrus.cn ( 1 ) , Czech Aktu?ln ? ( 4 ) , Blesk ( 5 ) , Denik ( 3 ) , Dnes ( 1 ) , E15 ( 1 ) , Hal ? noviny ( 5 ) , Hospod?sk ? Noviny ( 1 ) , Idnes ( 2 ) , Lidovky ( 7 ) , Mediafax ( 6 ) , Novinky ( 6 ) , T?den ( 1 ) , Tydenek Homer Mostecka ( 1 ) , ?T24 ( 4 ) , ?esk ?
Pozice ( 6 ) , ?esk ?
Televize ( 4 ) , ?esk ? Noviny ( 4 ) , ?esk ?
Rozhlas ( 1 ) , German Aachener Nachrichten ( 1 ) , Abendzeitung Mnchen ( 1 ) , Abendzeitung N?rnberg ( 1 ) , Allgemeine Zeitung ( 1 ) , Augsburger -allgemeine ( 1 ) , Braunschweiger Zeitung ( 1 ) , Das Bild ( 3 ) , Dresdner Neueste Nachrichten ( 1 ) , Euronews ( 1 ) , Frankfurter Allgemeine Zeitung ( 1 ) , Freie Presse ( 1 ) , Handelsblatt ( 1 ) , Hessische / Niedersaechsische Allgemeine ( 1 ) , Infranken ( 3 ) , Kurier ( 2 ) , Lampertheimer Zeitung ( 3 ) , Landeszeitung ( 1 ) , Main-Netz ( 1 ) , Mainpost ( 1 ) , Mittelbayerische Zeitung ( 2 ) , Mitteldeutsche Zeitung ( 2 ) , Morgenpost ( 2 ) , Neue Presse ( Coburg ) ( 2 ) , Nordbayerischer Kurier ( 3 ) , OE24 ( 1 ) , Passauer Neue Presse ( 2 ) , Peiner Allgemeine Zeitung ( 2 ) , Pforzheimer Zeitung ( 1 ) , Potsdamer Neueste Nachrichten < ( 1 ) , Rhein Zeitung ( 2 ) , Rundschau online ( 1 ) , S?ster Anzeiger ( 1 ) , Salzburger Nachrichten ( 1 ) , Schw?bische ( 2 ) , Schw ?
bische post ( 2 ) , Schwarzw ?lder Bote ( 2 ) , Tiroler Tageszeitung ( 2 ) , Usinger Anzeiger ( 1 ) , Westf ? lische Nachrichten ( 2 ) , Wienerzeitung ( 1 ) , Hausa Deutsche Welle ( 7 ) , Freedom radio ( 22 ) , Leadership ( 19 ) , Premium Times ( 20 ) , RFI Hausa ( 10 ) , VOA Hausa ( 18 ) , VON Hausa ( 4 ) , Japanese Fukui Shimbun ( 1 ) , Hokkaido Shimbun ( 5 ) , Iwate Nippo ( 3 ) , Saga Shimbun ( 3 ) , Sanyo Shimbun ( 4 ) , Shizuoka Shimbun ( 11 ) , Ube nippo Shimbun ( 2 ) , Yaeyama mainichi shimbun ( 1 ) , Yahoo ( 49 ) , Yamagata Shimbun ( 2 ) ,
Russian Altapress ( 1 ) , Altyn-orda ( 1 ) , Argumenti Nedely ( 5 ) , Argumenty i Fakty ( 6 ) , Armenpress ( 1 ) , BBC Russian ( 1 ) , Delovoj Peterburg ( 1 ) , ERR ( 5 ) , Gazeta ( 4 ) , Interfax ( 3 ) , Izvestiya ( 11 ) , Kommersant ( 1 ) , Komsomolskaya Pravda ( 7 ) , Lenta ( 6 ) , Lgng ( 2 ) , Moskovskij Komsomolets ( 9 ) , Novye Izvestiya ( 1 ) , Ogirk ( 1 ) , Parlamentskaya Gazeta ( 3 ) , Rossiskaya Gazeta ( 5 ) , Russia Today ( 8 ) , Russkaya Planeta ( 1 ) , Sovsport ( 2 ) , Sport Express ( 9 ) , Tyumenskaya Oblast Segodnya ( 1 ) , VOA Russian ( 1 ) , Vedomosti ( 2 ) , Vesti ( 6 ) , Xinhua ( 3 ) , German ( economic ) Aachener Nachrichten ( 1 ) , Abendzeitung Mnchen ( 1 ) , Das Bild ( 1 ) , Der Spiegel ( 2 ) , Epoch Times ( 1 ) , Frankfurter Allgemeine Zeitung ( 6 ) , Handelsblatt ( 17 ) , Haz ( 2 ) , Kurier ( 4 ) , L?becker Nachrichten ( 1 ) , Mindener Tageblatt ( 1 ) , Mittelbayerische Zeitung ( 1 ) , NZZ ( 1 ) , Neue Westf?lische ( 1 ) , Onetz ( 1 ) , Passauer Neue Presse ( 2 ) , Rheinische Post ( 1 ) , Russia Today ( 3 ) , S?ddeutsche Zeitung ( 8 ) , Salzburger Nachrichten ( 2 ) , Tiroler Tageszeitung ( 1 ) , Volksstimme ( 1 ) , Yahoo ( 1 ) , come-on.de ( 1 ) , French ( econmic ) Alg? rie Presse Service ( 3 ) , Aujourd ' hui le Maroc ( 5 ) , Derni?re Heure ( 4 ) , Derni?res Nouvelles d'Alsace ( 1 ) , Euronews ( 2 ) , L'Independant ( 1 ) , L'express ( 2 ) , La Croix ( 4 ) , La Meuse ( 3 ) , La Tribune ( 4 ) ,
? biomedical translation ( Yeganova et al. , 2021 ) ? efficiency ( Heafield et al. , 2021 ) ? large-scale multilingual machine translation ( Wenzek et al. , 2021 ) ? machine translation using terminologies ( Alam et al. , 2021 ) ? metrics ( Freitag et al. , 2021 b ) ? quality estimation ( Specia et al. , 2021 ) ? unsupervised and very low-resource translation ( Libovick ?
and Fraser , 2021 )
In the news translation task ( Section 2 ) , participants were asked to translate a shared test set , optionally restricting themselves to the provided training data ( " constrained " condition ) .
We included 20 translation directions this year , with translation between English and each of Chinese , Czech , German , Japanese and Russian , as well as French ?
German being repeated from last year , and English to and from Hausa and Icelandic being new for this year , along with Bengali?
Hindi and Xhosa ?
Zulu .
The translation tasks covered a range of language families , and included both low-resource and high- resource pairs .
System outputs for each task were evaluated both automatically and manually , but we only include the manual evaluation here .
The human evaluation ( Section 3 ) involves asking human judges to score sentences output by anonymized systems .
We obtained large numbers of assessments from researchers who contributed evaluations proportional to the number of tasks they entered .
We collected additional assessments from a pool of linguists , as well as crowd-workers .
This year , the official manual evaluation metric is again based on judgments of adequacy on a 100 point scale , a method ( known as " direct assessment " , DA ) that we explored in the previous years with convincing results in terms of the trade - off between annotation effort and reliable distinctions between systems .
In addition , other golden standards with this year 's systems were collected .
The human- in- the-loop GENIE leaderboard ( Khashabi et al. , 2021 ) conducted de?en evaluations independently in a Likert scale ( Section 3.5 ) .
We refer the reader to Freitag et al . ( 2021 b ) for MQM scoring of en?de , en?ru , and zh?en .
The primary objectives of WMT are to evaluate the state of the art in machine translation , to disseminate common test sets and public training data with published performance numbers , and to refine evaluation and estimation methodologies for machine translation .
As before , all of the data , translations , and collected human judgments are publicly available .
2
We hope these datasets serve as a valuable resource for research into datadriven machine translation , automatic evaluation , or prediction of translation quality .
News translations are also available for interactive visualization and comparison of differences between systems at http://wmt.ufal.cz/ using MT - ComparEval ( Sudarikov et al. , 2016 ) , and also on Explain- aBoard 3 ( Liu et al. , 2021 b ) .
In order to gain further insight into the performance of individual MT systems , we again organized a call for dedicated " test suites " .
Test suites are custom additions to the inputs .
Anyone can provide a test suite for any subset of news translation task languages and we ensure that the test suite is requested from all participating MT systems .
The MT outputs are delivered back to test suite authors for evaluation , which can be manual , automatic or both , focusing on any possible aspect of the MT systems .
This year , five test suites were acquired and translated by participating MT systems but only two were then analyzed in time for these proceedings : ? Freitag et al. ( 2021 b ) , the metrics task paper , used TED talks as additional domain , scored them with MQM , and further used these outputs and scores to assess domain-dependence of MT evaluation metrics .
?
Macketanz et al. ( 2021 ) reports on the fourth application of a fine-grained test suite for German ?
English linguistic phenomena .
The previous instances ( Macketanz et al. , 2018 ; Avramidis et al. , 2019 Avramidis et al. , , 2020 use the same underlying collection of sentences and thus allow to observe the overall development of MT systems in clear categories .
This year , the major jump was observed in the category of idioms , especially due to a few exceptional MT systems .
Many phenomena are being solved almost perfectly , the difficult categories remain false friends , ambiguity and multi-word expressions .
The goal of the Similar Language Translation ( SLT ) task ( Section 4 ) is to evaluate the perfor-mance of MT systems taking into account the similarity between pairs of closely - related languages from the same language family .
Following the interest of the community in this topic ( Costajuss ?
et al. , 2018 ; and the success of the past two editions of the SLT task task at WMT 2019 and WMT 2020 , we organize a third iteration of the task at WMT 2021 .
SLT 2021 features a pair of similar Dravidian languages , namely Tamil - Telugu , and multiple pairs of Romance languages involving Catalan , Spanish , Portuguese , and Romanian in all possible combinations .
A new track with French and two similar low-resource Manding languages : Bambara and Maninka was also included to encourage participants to take advantage of the similarity between Bambara and Maninka and explore data augmentation techniques , a typical scenario of low-resource languages .
Finally , translations were evaluated in both directions using three automatic metrics : BLEU , RIBES , and TER .
The primary goals of the Triangular MT task ( Section 5 ) are to promote translation between non-English languages , to optimally mix direct and indirect parallel resources and exploit noisy web data sources to build an MT system .
Specifically , the task was Russian to Chinese machine translation , given parallel data comprising of direct ( Russian - Chinese ) and indirect ( Russian - English and English - Chinese ) sources .
The submitted systems were evaluated on a ( secret ) mixed - genre test set , drawn from the web and curated manually for high-quality segment pairs .
The multilingual low-resource translation for Indo-European languages task ( MLLR , Section 6 ) aims to investigate the best approaches to deal with multilingual translation .
Usually , multilingual translation is done with the help of a high- resourced language , e.g. English .
In MLLR , we evaluate translation quality for Icelandic-Norwegian Bokm?l- Swedish ( North - Germanic ) and Catalan-Italian - Occitan-Romanian ( Romance ) .
Higher resourced languages ( Danish , German , English , Spanish , French and Portuguese ) are allowed for training but not evaluated .
We focus on a specific domain : cultural heritage documents are extracted from Europeana and Wikipedia , a domain where named entities may also play a role in translation quality .
The evaluation is done at language family level with a combination of automatic metrics ( BLEU , TER , chrF , BertScore and COMET ) and complemented by a manual evaluation on a subset of language pairs .
The automatic post-editing ( APE ) task ( Section 7 ) focuses on another MT - related problem : the correction of machine - translated text generated by an unknown system .
In continuity with last year , in this seventh iteration of the task at WMT we focused on two language pairs ( English - German and English - Chinese ) , using data drawn from English Wikipedia articles and translated with neural MT systems .
The evaluation was carried out both automatically - with TER and BLEU respectively used as primary and secondary metric - and manually - with the same direct assessment method used for the news translation task .
News Translation Task
This recurring WMT task assesses the quality of MT on text from the news domain .
As in the previous year , we included Chinese , Czech , German , Japanese and Russian ( to and from English ) as well as French ?
German .
New language pairs for this year were Icelandic and Hausa ( to and from English ) as well as Bengali?
Hindi and Xhosa ?
Zulu .
Test Data
As in previous years , the test sets consist of unseen translations prepared specially for the task .
The test sets are publicly released to be used as translation benchmarks in the coming years .
Here we describe the production and composition of the test sets .
The source texts for the test sets were all extracted from online news sites , with the exception of Bengali?
Hindi and Xhosa ?
Zulu , which were part of the FLORES - 101 benchmark ( Goyal et al. , 2021 ) and extracted from Wikipedia .
The sources used for the online news are shown in Table 1 , and all articles are from the second half of 2020 .
For the French ?
German task , we specifically selected financial and economic news , whereas for the other news sources , we randomly selected articles from general online news , including politics , sports , international and local events .
For all language pairs , we aimed for a test set size of 1000 sentences , and to ensure that the test sets were " source-original " , in that the source text is the original article and the target text is the translation .
This is to avoid " translationese " effects on the source language , which can have a detrimental effect on the accuracy of evaluation ( Freitag et al. , 2019 ; Laubli et al. , 2020 ; . The exceptions were Chinese ?
English , where we used a larger test set of 1948 sentences , and the FLORES - 101 test sets which were around 500 sentences , and derived from English source documents .
For language pairs that were new this year ( i.e. Icelandic ?
English and Hausa? English ) we prepared development sets using the same process as the test set , but concatenating both translation directions into the same set .
For each translated article in the development set , the direction of translation is clearly identified .
For WMT20 , we experimented with using test sources with line ( segment ) boundaries at paragraphs ( not sentences ) for some language pairs , but we found no evidence that translators used their new freedom to reorganise sentences , and the longer lines possibly made evaluation more difficult , so we reverted to a sentence - per-line format this year .
For selected language sources ( Czech , German and English , when translated into the recurring languages ) we retained the paragraph boundaries from the original articles , but within the paragraphs , the sentences were in separate segments .
It was up to the participating systems to make use of the paragraph breaks or not , but the systems were expected to preserve the segment boundaries .
The test sets for WMT21 were released using a new XML format , replacing the " pseudo xml " SGML format which had been used for many years .
The advantages of the new format are : ( i ) it can be processed with standard XML tools , and there is no longer any doubt about how to treat special XML characters such as the ampersand ( " & " ) ; ( ii ) the source , all references and all submissions can be contained in one convenient XML file ; ( iii ) the metadata better matches the needs of the task , and can be extended as necessary .
We created simple tools for converting from text - based files to the new XML format .
4
The translation of the test sets was performed by professional translation agencies , according to the brief supplied in Appendix B. Several language pairs got special attention .
For Chinese ?
English , Russian ?
English and German ?
English , we obtained a second reference in each direction from a different translation agency , labelled " B " .
For German ?
English , the " B " reference was found to be a post-edited version of one of the participating online systems , so we had to discard it .
Microsoft then sponsored a third independent translation , labelled " C " , and the metrics task organizers with the support from Google later provided yet another German ?
English reference , discussed only in Freitag et al . ( 2021 b ) as " D " .
For Czech ?
English , the first reference ( labelled " A " ) which served in reference - based manual evaluations , was provided by a translation agency in both directions .
The second Czech ?
English reference ( labelled " B " ) which served as another system in the competition was provided by professional translators recruited from teachers and students of translation studies into Czech and three students and graduates of translation studies and one translator , English native speaker , into English .
Training Data
As in past years we provided a selection of parallel and monolingual corpora for model training , and development sets to tune system parameters .
Participants were permitted to use any of the provided corpora to train systems for any of the language pairs .
As well as providing updates on many of the previously released data sets , we included several new data sets , mainly to support the new language pairs .
Our training data includes the latest version of ParaCrawl ( Ba?n et al. , 2020 ) for all language pairs where it is available .
New for this year is a ParaCrawl corpus for Chinese ?
English , which contains 14 M sentences , as well as a small Hausa ?
English ParaCrawl .
The JParaCrawl corpus ( for Japanese ?
English ) is constructed in a similar way to ParaCrawl , but by a different group ( Morishita et al. , 2020 ) . For Icelandic ?
English we used the recently released ParIce ( Barkarson and Steingr?msson , 2019 ) a source of parallel data , and the Icelandic Gigaword corpus for monolingual data ( Steingr? msson et al. , 2018 ) . For Hausa ?
English , the data was mainly drawn from Opus ( Tiedemann and Nygaard , 2004 ) , which is mostly religious and IT localisation text .
We added a small ( < 6000 ) parallel sentence corpus extracted from the website of Ayatollah Khamenei , 5 3 , 074 , 921 , 453 2 , 872 , 785 , 485 333 , 498 , 145 1 , 168 , 529 , 851 Words 65 , 104 , 585 , 881 65 , 147 , 123 , 742 6 , 702 , 445 , 552 23 , 332 , 529 , 629 Dist. 342 , 149 , 665 338 , 410 , 238 48 , 788 , 665 90 , 497 ,177 Chinese Icelandic Hausa French Sent. 1 , 672 , 324 , 647 24 , 627 , 579 1 , 467 , 326 4 , 898 , 012 , 998 , 326 20 , 082 , 665 126 , 364 , 574 Wayback Machine .
6 For the two FLORES - 101 language pairs ( i.e. Bengali?
Hindi and Xhosa ? Zulu ) all training data is from the CC - Aligned corpus .
Other language pairs used the same data sets as last year , with updates wherever available .
The monolingual data we provided was similar to last year 's , with a 2020 news crawl 7 added to all the news corpora .
Note that news crawl now includes 59 languages , so is not limited to languages used in WMT .
In addition , we provided versions of the news corpora for Czech , English and German , with both the document and paragraph structure retained .
In other words , we did not apply sentence splitting to these corpora , and we retained the document boundaries and text ordering of the originals .
Some statistics about the training and test materials are given in Figures 1 , 2 , 3 and 4 .
Submitted Systems
In 2021 , we received a total of 173 submissions .
The participating institutions are listed in Table 2 and detailed in the rest of this section .
Each system did not necessarily appear in all translation tasks .
We also included online MT systems ( originating from 5 services ) , which we anonymized as ONLINE - A , B , G , W , Y .
All submissions , sources and references are made available via github 8 .
The collect submissions , we used the submission tool , OCELoT , 9 replacing the matrix that has been used up until 2019 .
Using OCELoT gives us more control over the submission and scoring process , for example we are able to limit the number of test submissions by each team , and we also display the submissions anonymously to avoid publishing any automatic scores .
For presentation of the results , systems are treated as either constrained or unconstrained .
When the system submitters report that they were only trained on the provided data , we class them as constrained .
The online systems are treated as unconstrained during the automatic and human evaluations , since we do not know how they were built .
In Appendix C , we provide brief details of the submitted systems , for those where the authors provided such details .
Human Evaluation
A human evaluation campaign is run each year to assess translation quality and to determine the official ranking of systems taking part in the news translation task .
This section describes how data for the human evaluation is prepared , the process of collecting human assessments , and computation of the official results of the shared task .
Direct Assessment
We have employed Direct Assessment ( DA , Graham et al. , 2013 Graham et al. , , 2014 as the primary mechanism for evaluating systems since running a comparison of DA and relative ranking in 2016 .
DA has several important features including accurate quality control of crowdsourcing .
With DA human evaluation , human assessors are asked to rate a given translation by how adequately it expresses the meaning of the corresponding reference translation or source language input on an analogue scale , which corresponds to an underlying absolute 0 - 100 rating scale .
10
Source and Reference - based Evaluations
The original definition of DA provides human assessors with a reference translation .
The benefit of this reference - based evaluation is that only speakers of the target language are needed , but the quality of the reference translation becomes critical and even if flawless , evaluating against a single reference translation could bias evaluators towards that reference .
In 2018 , we trialled source- based ( or " bilingual " ) evaluation for the first time , for English to Czech translation .
In this configuration , the human assessor is shown the source input and system output only ( with no reference translation shown ) .
The assessor thus has to understand both the source and target languages very well but the quality of the reference is no longer vital .
In fact , the human- generated reference can be included in the evaluation as an additional system to provide an estimate of human performance .
10
No sentence or document length restriction is applied during manual evaluation .
Direct Assessment is also employed for evaluation of video captioning systems at TRECvid Awad et al. , 2019
Awad et al. , , 2021 and multilingual surface realisation ( Mille et al. , 2018 ( Mille et al. , , 2019 ha-en , en-ha, en - is , de-en , en-ru , de-fr , ru-en , en-de , is-en ( no associated paper ) MIDEIND en-is , is-en ( J?nsson et al. , 2021 ) MISS en-zh, en -ja , zh-en , ja-en ( Li et al. , 2021 b ) MOVELIKEAJAGUAR
en-zh, en-ja , ja-en ( no associated paper ) MS - EGDC ha-en , bn-hi, en-ha , zu- xh , hi- bn , xh-zu ( Hendy et al. , 2021 ) NIUTRANS ha-en , en-zh , en -ha, en - is , en -ja , zh-en , en-ru , ru-en , ja-en , is-en NJUSC -TSC en-zh , zh-en ( no associated paper ) NUCLEAR - TRANS en-zh , en-de ( no associated paper ) NVIDIA -NEMO de-en , en-ru , ru-en , en-de ( Subramanian et al. , 2021 ) P3AI ha-en , en-zh , en -ha , fr-de , de-en , zh-en , de-fr , en-de ( Zhao et al. , 2021 ) SMU en-zh , de-en , zh-en ( no associated paper )
For both reference and source - based evaluation , we require human assessors to only evaluate translation into their native language .
Following WMT19 and WMT20 , we thus again use the source - based evaluation only for out - of - English language pairs .
This is especially relevant since we have a large group of volunteer human assessors with native language fluency in non-English languages and high fluency in English , while we generally lack the reverse , i.e. native English speakers with high fluency in non-English languages .
We use different implementation and human annotators for into-English and out - of- English .
We describe the approaches separately .
Referencebased ( monolingual ) into-English human evaluation is described in Section 3.2 , while sourcebased ( bilingual ) out - of - English and non-English human evaluation is described in Section 3.3 .
A third , simplified annotation was used for Bengali?
Hindi and Xhosa ?
Zulu , Section 3.4 .
Translationese Prior to WMT19 , all the test sets included a mix of sentence pairs that were originally in the source language , and then translated to the target language , and sentence pairs that were originally in the target language but translated to the source language .
The inclusion of the latter " reverse-created " sentence pairs has been shown to introduce biases into the evaluations , particularly in terms of BLEU scores .
Therefore we have avoided it for all language pairs , apart from Bengali?
Hindi and Xhosa ?
Zulu , where the texts are all translated from English .
Document Context
As mentioned already in our discussion in WMT18 and as also established within the community ( L?ubli et al. , 2018 b ; Toral et al. , 2018a ) , evaluating sentences out of their document context can skew the results .
The effect is particularly pronounced when comparing human and machine translation , where it is observed that evaluators tend to rate the human translation higher ( relative to the machine translation ) when the translations are viewed in context .
Human translators always have access to the document context when translating to create the references .
In WMT19 , we experimented with a DA style that considers document context in a simple way .
Dubbed " SR +DC " ( segment rating with document context ) , this method presents one segment at a time but the segments are no longer shuffled ( as in " SR ? DC " , segment rating without document context ) .
Instead , they are provided in the order in which they appear in the document .
The implementation still has the limitation that the assessors cannot go back to the previous segment .
An improved alternative to " SR + DC " is to offer the full document and allow the assessors to review their segment - level ratings .
We call this setup " SR + FD " ( segment ranking in a full document ) and illustrate the user interface in Appraise in Figure 5 . 11
This year , for all language pairs for which document context was available , we include it when evaluating translations .
Note that the ratings are nevertheless collected on the segment level , motivated by the power analysis described in and .
The particular details on how document context is made available to assessors depends on the translation direction , as described in more detail in Sections 3.2 to 3.4 .
Human Evaluation of Translation into-English
In terms of the News translation task manual evaluation for into-English language pairs , a total of 589 turker accounts were involved .
12 488,396 translation assessment scores were submitted in total by the crowd , of which 170,194 were provided by workers who passed quality control .
13 System rankings are produced from a large set of human assessments of translations , each of which indicates the absolute quality of the out-Figure 5 : Screen shot of the document- level DA ( SR + FD , segment rating within the full document ) configuration in the Appraise interface for an example assessment from the human evaluation campaign .
The annotator is presented with the entire translated document randomly selected from competing systems ( anonymized ) and is asked to rate the translation of individual segments and then entire document on sliding scales .
put of a system .
Table 3 shows total numbers of human assessments collected in WMT21 for into-English language pairs contributing to final scores for systems .
14
Crowd Quality Control Collection of segment- level ratings with document context ( SR +DC , Segment Rating + Document Context ) involved constructing HITs so that each sentence belonging to a given document ( produced by a single MT system ) was displayed to and rated in turn by the human annotator .
We then injected the three kinds of quality control translation pairs described in Table 4 : we repeat pairs expecting a similar judgment ( Repeat Pairs ) , damage MT outputs expecting significantly worse scores ( Bad Reference Pairs ) and use references instead of MT outputs expecting high scores ( Good Reference Pairs ) .
For each of these three types , we include the MT output , along with its corresponding control item .
HITs were then constructed as follows , with as close as possible to 100 segments in a single HIT : 1 . All documents produced by all systems are pooled ; 15 Repeat Pairs : Original System output ( 10 )
An exact repeat of it ( 10 ) ; Bad Reference Pairs : Original System output ( 10 ) A degraded version of it ( 10 ) ; Good Reference Pairs : Original System output ( 10 ) Its corresponding reference translation ( 10 ) .
2 . Documents are then sampled at random ( without replacement ) and assigned to the current HIT until the current HIT contains close to ( but less than ) 70 segments 3 . Once documents amounting to close to 70 segments have been assigned to the current HIT , we select a subset of these documents to be paired with quality control documents ; this subset is selected by repeatedly checking if the addition of the number of the segments belonging to a given document ( as quality control items ) will keep the total number of segments in the HIT below 100 ; if this is the case , it is included ; otherwise it is skipped until the addition of all documents has been checked .
In doing this , the HIT is structured to bring the total number of segments as close as possible to 100 segments .
4 . Once we have selected a core set of original system output documents and a subset of them to be paired with quality control versions for each HIT , quality control documents are automatically constructed by altering the sentences of a given document into a mixture of three kinds of quality control items used in the original DA segment - level quality control : bad reference translations , reference translations and exact repeats ( see below for details of bad reference generation and Table 5 for numbers of words replaced in document segments ) ;
5 . Finally , the documents belonging to a HIT are shuffled .
Construction of Bad References
As in previous years , bad reference pairs were created automatically by replacing a phrase within a given translation with a phrase of the same length , randomly selected from n-grams extracted from the full test set of reference translations belonging to that language pair .
This means that the replacement phrase will itself comprise a mostly fluent formance estimate , it is also considered a system during quality control set-up .
sequence of words ( making it difficult to tell that the sentence is low quality without reading the entire sentence ) while at the same time making its presence highly likely to sufficiently change the meaning of the MT output so that it causes a noticeable degradation .
The length of the phrase to be replaced is determined by the number of words in the original translation , as listed in Table 5 . Quality Filtering
When an analogue scale ( or 0 - 100 point scale , in practice ) is employed , agreement cannot be measured using the conventional Kappa coefficient , ordinarily applied to human assessment when judgments are discrete categories or preferences .
Instead , to measure consistency we filter crowd-sourced human assessors by how consistently they rate translations of known distinct quality using the bad reference pairs described previously .
Quality filtering via bad reference pairs is especially important for the crowdsourced portion of the manual evaluation .
Due to the anonymous nature of crowd- sourcing , when collecting assessments of translations , it is likely to encounter workers who attempt to game the service , as well as submission of inconsistent evaluations and even robotic ones .
We therefore employ DA 's quality control mechanism to filter out low quality data , facilitated by the use of DA 's analogue rating scale .
Assessments belonging to a given crowd-source worker who has not demonstrated that he / she can reliably score bad reference translations significantly lower than corresponding genuine system Table 6 shows the number of workers participating in the into-English translation evaluation who met our filtering requirement in WMT21 by showing a significantly lower score for bad reference items compared to corresponding MT outputs , and the proportion of those who simultaneously showed no significant difference in scores they gave to pairs of identical translations .
We removed data from the non-reliable workers in all language pairs .
Producing the Human Ranking
This year all rankings ( for to -English translation ) were arrived at via segment ratings presented one at a time in their original document order ( SR + DC ) .
In order to iron out differences in scoring strategies of distinct human assessors , human assessment scores for translations were first standardized according to each individual human assessor 's overall mean and standard deviation score .
Average standardized scores for individual segments belonging to a given system were then computed , before the final overall DA score for a given system is computed as the average of its segment scores ( Ave z in Table 7 ) .
Results are also reported for average scores for systems , computed in the same way but without any score standardization applied ( Ave % in Table 7 ) .
Human performance estimates arrived at by evaluation of human-produced reference translations are denoted by " HUMAN " in all tables .
Clusters are identified by grouping systems together according to which systems significantly outperform all others in lower ranking clusters , according to Wilcoxon rank - sum test .
Rank ranges are based on the same head - to - head statistical significance tests .
For instance , if a system is statistically significantly worse than 2 other systems , and not statistically different from 4 other systems , its rank is reported as 3 - 6 ( the top of the rank range is 2 + 1 , the bottom 2 + 4 ) .
All data collected during the human evaluation is available at http://www.statmt.org/wmt21/ results.html .
Appendix
A shows the underlying head - to - head significance test official results for all pairs of systems and also reports BLEU , chrF , and COMET scores .
Bilingual Human Evaluation Human evaluation for nine out - of - English and non-English translation directions used a sourcebased ( sometimes called " bilingual " ) direct assessment of individual segments in the full document context ( SR + FD ) , as established in WMT20 ( Barrault et al. , 2020 ) .
In an attempt to break more ties among the participating systems , we also ran a second stage of annotation using segment - level contrastive sourcebased DA ignoring document context ( labelled " contr : SR ? DC " ) for top - 10 systems ( plus human references ) for 3 out - of - English language pairs .
Details on the second stage are in Section 3.3.5 .
In the source - based DA campaign , we collected 303,627 assessments in total after excluding quality control items and users who did not pass the quality control .
The contrastive source - based DA campaign provided 64,031 translation assessments .
The total numbers of collected assessments per language pair are presented in Table 8 .
For data collection , we used the open-source Appraise Evaluation Framework ( Federmann , 2012 ) for both assessment types .
Table 8 : Amount of data collected in the WMT21 manual document - and segment - level evaluation campaigns for bilingual source - based evaluation out - of - English and non-English language pairs .
The system counts include the human references ( either 1 or 2 references , depending on language pair ) .
Sources of Human Annotators
We used three groups of annotators : participants in the News Shared Task , crowd-workers from the Toloka platform , and paid professional annotators sponsored by Microsoft .
We asked participants of the news task to contribute around 9 hours of annotation time ( which we estimated at 12 HITs ) per each primary system submitted , with each HIT including roughly 100 segment translations .
Furthermore , we collected information about the classification of their annotators type .
Unfortunately , only 65 % of the requested annotations were finished by participating teams .
The second annotator group was provided by Toloka AI .
16 Toloka AI is a global data labeling company that helps its customers generate machine learning data at scale by harnessing the wisdom of the crowd from around the world .
It relies on a geographically diverse crowd of several million registered users ( Pavlichenko et al. , 2021 ) .
17
Toloka tests proficiency of their annotator crowd and excludes from future annotations anyone who does not pass quality control in the Appraise tool .
The last part of annotations is sponsored by Microsoft , who contributed with their crowd of qualified paid bilingual speakers experienced in the annotation process .
Moreover , Microsoft tracks the performance of the annotators , and those who fail 16 https://toloka.ai/ 17 https://hackernoon.com/ evolution-of-the-data-production-paradigm-in-ai quality control are permanently removed from the pool of annotators .
This increases the overall quality of the human assessment .
For bilingual human evaluation , Microsoft contributed with 42 % , WMT News participants contributed with 37 % , and Toloka platform with 21 % of all valid annotations ( after removal of annotators that do not pass quality control ) .
The distribution of individual groups of annotators per each language is presented in Table 9 .
Document - Level Assessment
This year 's human evaluation for out - of - English and non-English language pairs features a document- level direct assessment configuration as presented last year ( Barrault et al. , 2020 ) .
We again use the segment level rating but provide the full document at once ( SR + FD , segment rating within a full document ) , for a more reliable evaluation Laubli et al. , 2020 ) .
Figure 5 above shows a screenshot of the fully document- level interface .
In the default scenario , an annotator scores individual segments one by one and , after scoring all of them , on the same screen , the annotator then judges the translation of the entire document displayed .
Annotators can , however , revisit and update scores of previously assessed segments at any point of the annotation of the given document .
It has been shown that presenting the entire document context on a screen may lead to higher quality segment - and document - level assessments improving the correlation between segment and document scores and increasing interannotator agreement for document scores .
A similar setup has been used by even for more than two systems compared at once .
Quality Control
For the document- level evaluation of out - of - English translations , HITs were generated using the same method as described for the SR + DC evaluation of into-English translations in Section 3.2.1 with a minor modification :
Since the annotations are made by researchers and professional translators who ensure a better quality of assessments than the crowd-sourced workers , only bad references are used as quality control items .
Including Human Translations Source- based DA allows us to include human references in the evaluation as another system to provide an estimate of human performance .
Human references were added to the pool of system outputs prior to sampling documents for tasks generation .
Each reference is assessed individually if multiple references are available , which is the case for English ?
German , English ?
Czech , English ?
Russian , and English ?
Chinese .
Contrastive Direct Assessment
This year we extended the bilingual source - based human evaluation with contrastive evaluation using segment - level pairwise direct assessments ( Novikova et al. , 2018 ; Sakaguchi and Van Durme , 2018 ) .
It has been pointed out ( Freitag et al. , 2021a ) that standard direct assessment may not be able to properly differentiate high-quality MT system outputs .
The contrastive approach to DA can strengthen the discriminative power as annotators judge translations in relation to each other .
When standard DA can likely provide better absolute quality assessment , the contrastive evaluation can provide better relative quality assessments between system pairs .
This may help create a more reliable ranking of systems if used on top of the standard approach described in Section 3.3 .
The contrastive evaluation is similar to the relative ranking used from WMT08 ( Callison - Burch et al. , 2008 ) to WMT16 , where annotators were presented with up to five system outputs and corresponding source and reference sentence and asked to rank these systems between each other .
The main differences in this year 's contrastive evaluation to the relative rankings are that 1 ) the evaluation is source - based , i.e. without the reference , 2 ) the continuous scale is used instead of ranks , and 3 ) only two system outputs are judged at the same time instead of five .
To reduce the cognitive load on annotators , we decided to trial this contrastive approach evaluating individual sentences independent of their context .
This is a very important difference compared to the the first stage ( Section 3.3 ) .
We ran the contrastive evaluation for English ?
Chinese , English ?
Czech and English ?
German , and we selected top - 10 best performing systems based on DA z-score from the ranking created using standard direct assessment for those languages ( Table 10 ) , and two human references .
This contrastive evaluation was sponsored by Microsoft and performed by the bilingual paid annotator group as described in Section 3.3.1 .
Assessments were collected using the opensource Appraise Evaluation Framework ( Federmann , 2012 ) .
A screenshot of the user interface used in this stage is shown in Figure 6 . Each annotator is presented with two randomly selected translated segments from competing systems ( anonymized ) and asked to rate both of them on a continuous scale of 0 - 100 .
Upon request by the annotator , the differences between the two translations were highlighted at the word level to help avoid missing differences .
This highlighting may however reduced the effectiveness of control items .
The annotator is presented with two translated segments randomly selected from competing system outputs ( anonymized ) and is asked to rate both of them on sliding scales .
Human Rankings
Table 10 shows official news task results for translation out - of - English , where lines indicate clusters according to Wilcoxon rank - sum test p < 0.05 .
Source - based DA scores were collected based on the document-level annotation interface , so context was available during annotation .
All systems are evaluated in isolation , based on the annotators ' perception of translation quality given the source text and document context .
Across all language pairs , human reference translations end up in the top-scoring cluster , indicative of a ( relatively ) high quality of these references .
For language pairs with large numbers of submissions , we observe little to no clustering .
Notably English ?
German has only two clusters , one of which contains all but one of the submitted systems , and English ?
Chinese ends up with a huge mono cluster containing all submissions .
While there are differences in average scores and z scores these are not statistically significant enough for effective clustering .
As a substitute , rank ranges give an indication of the respective system 's translation quality .
Table 11 shows contrastive news task results for translation out - of - English , where lines indicate clusters according to Wilcoxon rank - sum test p < 0.05 .
Contrastive , source - based DA scores ( contr : SR ? DC ) were collected using a segmentlevel annotation interface , so context was not been available to annotators .
Results for the source - based DA annotation phase ( SR + FD ) in Table 11 were computed on the subset of data for the ten systems and two references for which we have run the contrastive , source - based DA annotation phase .
We generally observe better clustering for the contr : SR?DC .
This is especially noteworthy as the number of annotations collected per system is much higher for the first , SR + FD , DA phase ( for two of the three language pairs on which contr : SR ?
DC was run ) .
It seems that pairwise comparison of system outputs is beneficial for determining whether differences between systems are statistically significant .
In contrast to the first annotation phase , we find that human reference translations are scored worse , and significantly worse than the top cluster .
We explain this by the fact that our contrastive setup was run on segment - level while the sourcebased DA annotators had access to the full document context .
A simple explanation that should nevertheless be empirically validated is that the wording of the sentence created for and within the context of the document does not sound flawless and natural when evaluated in isolation ( L?ubli et al. , 2018a ; Toral et al. , 2018 b ) .
Some machine translation systems do consider the surrounding sentences but their capacity of ' contextualizing ' the candidate sentences is probably limited .
Observing the striking difference in system ranking by SR + FD vs. contr : SR ? DC , esp. the discrepancy in the ranking of human translations , we conclude that evaluating MT systems without document context is no longer reliable for mid-and high-quality MT systems .
This is also supported by the surprising observation in Czech ?
English in Table 7 where humans seemed to be surpassed by all participating MT systems .
( Considering statistical significance , the claim is arguably weaker : humans share the second cluster with the majority of the systems . )
We acknowledge that it is possible that the Czech ?
English HUMAN -B references are of much worse quality than the English ?
Czech ones , 18 but we tend to put more trust in the reference quality than in the SR + DC method for two reasons : ( 1 )
The annotators did not see the whole document at once and cannot go back in their annotation , so their effective capability to consider context is limited .
( 2 ) It is possible that other effects of referencebased DA in the Czech ?
English start playing role when both the candidate and reference are human vs. when only the reference is human .
One possibility would be a stronger confidence of assessors when scoring human translations , leading e.g. to more polarized scores .
A detailed investigation into manual evaluation methods that word reliably for both human and machine translations is thus still needed .
Human Evaluation of Bengali?Hindi and Xhosa ?
Zulu Translation ( Wikipedia Data ) Translation quality for Bengali?
Hindi and Xhosa ?
Zulu was evaluated using Direct Assessment without considering document context ( SR ? DC ) with a scoring scale of 1 - 100 by vetted human evaluators .
The human evaluators were asked to provide a judgment that they felt most accurately reflected the perceived quality of each corresponding translation of the give source sentence .
Definitions of translation quality within 18
The quality assurance for each of " A " and " B " references for English ?
Czech was comparable ; not that the same translators would be producing both directions .
In fact , we expected the " B " translations to be better , because they were created by experienced students and teachers of translation studies , who are active translators themselves and who specifically attempted to produce as good translations as possible .
As the to - Czech scores suggest , our annotators preferred the translation agency " A " translations significantly more .
But even if the " A " translations were also better than " B " in from - Czech , we see it as very unlikely that the translatologist translations would be worse than all systems .
several scoring ranges were provided to assist evaluators in providing consistent annotations .
A participating system translation was displayed on the right next to its corresponding source sentence on the left .
The sentence pairs were then randomized and passed to a human evaluator for a single direct assessment .
The evaluation was performed on the sentence level and evaluators provided a direct assessment score for each sentence -translation pair .
The user interface was simpler than the one shown in Figure 5 : instead of a slider , the annotators had to enter the scores numerically .
Because evaluators were extremely difficult to recruit for these language pairs and the evaluation was thus low resource , no quality control items were injected and we focused on the vetting process of the evaluators prior to performing any assessment .
The only sanity check was that evaluators enter an integer between 1 and 100 as the scores .
All segments from the FLORES Wikimedia test set were included for the evaluation .
Each segment was annotated and assessed by one evaluator only once .
All four language directions were assessed by trusted evaluators who have been vetted by a localization vendor specializing in translation evaluation services , to have native fluency of the target language , fluent to native understanding of the source language , have lived in the target region for at least five years recently , and have had at least two to five years of professional translation experience .
For Hindi?Bengali and Bengali?
Hindi , two human evaluators were used with the translation data being split in half and randomly assigned to the respective evaluators .
Two human evaluators assessed for Xhosa ?
Zulu data and one evaluator assessed for Zulu ? Xhosa .
The number of evaluators and judgments they made is provided in Table 12 .
The final scores for Bengali?
Hindi and Xhosa ?
Zulu are provided in Table 13 .
GENIE DE-EN Evaluation
This year , human evaluations for German ?
English translation with the GE - NIE leaderboard were also carried out .
GENIE is an ongoing effort that centralizes and facilitates human evaluations for natural language generation tasks ( Khashabi et al. , 2021 ) .
In Table 12 : Amount of data collected in the WMT21 manual evaluation campaign for evaluation Hindi to / from Bengali and Zulu to / from Xhosa addition to all German ?
English submissions , four original transformer baselines with varying sizes and depths were trained and evaluated : GENIE - large - 6 - 6 ( transformer large with a 6 - layer encoder and a 6 - layer decoder ) , GENIE - base -6- 6 , GENIE - base -3 - 3 , and GENIE - base - 1 - 1. 19
These models were trained solely on the given training data without ensembling , backtranslation , or any other data augmentation method .
Similar to the official into-English evaluations , evaluations are done monolingually where Human - A is used as the reference .
Each HIT contains 5 segments that are randomly shuffled , and no document context is considered during evaluations .
Turkers are asked to decide whether they agree or disagree that the prediction adequately expresses the meaning of the reference .
Turkers are given the following additional instructions : a prediction is adequate if in the absence of the reference , the prediction perfectly conveys the meaning intended by the reference .
The user interface for annotating one candidate segment in the HIT is illustrated in Figure 7 .
For quality control , we first selected Amazon Mechanical Turkers who had completed at least 5000 HITs with a 99 + % approval rate and had a locale of US , GB , AU , or CA .
They were then asked to carefully read the instructions and finish 10 sample questions created from WMT 2019 submissions and references .
They were allowed to participate only when they correctly annotate 9 instances at least .
In addition to this quality control at the entry point , we kept monitoring to detect spamming behavior .
In particular , we randomly replaced 5 % of the model predictions with sentences identical to the corresponding reference ( Perfect Ref. , similar to good reference in Section 3.2.1 ) , and 5 % of the model predictions with the reference from a different question ( Wrong Ref . ) .
We then randomly selected 800 examples from the test set to annotate .
During annotation , we monitored how annotators labeled the Perfect Ref. and Wrong Ref. questions .
Annotators that failed to both assign a high score to the Perfect Ref. and a low score to the Wrong Ref. questions were removed from the annotator pool , and all of their annotations were discarded .
This qualification resulted in removing 5 % of the participants .
Since spammers invest little effort into completing each HIT , they can complete many more than other annotators ( we found they would have completed up to 50 % of the HITs in our preliminary experiments ) .
Therefore , removing the 5 % of participants that spammed annotations substantially improved the quality of our assessment .
In summary , there are several major differences from the setup used in the official evaluations : ?
Turkers assess the adequacy by a fivecategory Likert scale , which is later converted to scalar values : strongly agree ( 1.0 ) , agree ( 0.75 ) , neutral ( 0.5 ) , disagree ( 0.25 ) , and strongly disagree ( 0.0 ) . ?
All 5 segments are randomly chosen for each HIT , and the document context is disregarded .
?
For evaluating each system , we randomly sample 800 segments from the test set .
The randomly selected instances are shared across all systems .
?
To maximize the number of segments annotated for a given budget , each segment is annotated only once ( unilabeling ) .
Under a fixed annotation budget , unilabeling results are shown to be relatively stable compared to multilabeling ( i.e. , evaluating one segment by multiple annotators .
See Section 5.1 of Khashabi et al. , 2021 ) . ?
The overall scores are calculated by averaging raw numbers over the 800 segments .
No standardization is applied .
?
Different quality controls are applied as discussed above .
Table 14 shows results from the GENIE evaluation for German to English translation .
There are systems that are ranked highly , both in the official and GENIE evaluations , such as Online -A and VolcTrans - AT .
Conversely , happypoet and Manifold are given low scores consistently .
Further , the transformer baselines are ranked in the expected order : large - 6 - 6 , base - 6 - 6 , base - 3 - 3 , followed by base - 1 - 1 .
This confirms the validity of the evaluations .
Nonetheless , we see some noticeable difference from the official ranking .
In particular , HUMAN and the Watermelon systems are ranked high in contrast to the official evaluations .
It is left to future work to analyze which parts of the crowdsourcing setup are contributing to the diverging system rankings ; these analyses would help us improve our human evaluation method in the future .
Similar Language Translation
In this section we present the findings of the third SLT shared task organized at WMT 2021 .
The task follows the success of the two past SLT shared tasks organized at WMT 2019 and WMT 2020 .
SLT 2021 is motivated by the growing interest of the community in translating between similar lan-guages , low-resource languages , dialects , and language varieties , and the challenges faced by stateof - the - art systems in these settings evidenced in recent studies ( Hassani , 2017 ; Costa-juss ?
et al. , 2018 ; Tapo et al. , 2020 ) .
The main goal of the task is to evaluate the performance of state - of - the - art MT systems on translating between closely - related language pairs of languages from the same language family .
Past editions of the task ( Barrault et al. , 2019 ( Barrault et al. , , 2020 featured language pairs such as Spanish - Portuguese , Czech - Polish , and Hindi - Nepali to name a few .
This year 's SLT features multiple pairs of similar languages from the Indo-Aryan and Romance family .
Finally , SLT 2021 also features a track including French and two similar low-resource Manding languages spoken in West Africa , namely Bambara and Maninka , where participants were pro- vided with the opportunity to combine datasets of the two Manding languages taking advantage of their similarity .
As in past editions of the task , translations at SLT 2021 are evaluated in all directions using three automatic evaluation metrics : BLEU , RIBES , and TER .
Data Training
We have made available a number of data sources for the SLT shared task .
Some training datasets were used in the previous editions of the WMT News Translation shared task and were updated ( News Commentary v16 , Wiki Titles v3 ) , while some corpora were newly introduced .
We also used data collected from Opus ( Tiedemann and Nygaard , 2004 ; Tiedemann , 2012 ) 20 . For the Spanish - Catalan language pair we used parallel corpora : Wiki Titles v3 , ParaCrawl ( Ba?n et al. , 2020 ) , DOGC v2 , and monolingual : Europarl v10 ( Koehn , 2005 ) , News Commentary v16 , News Crawl , caWaC ( Ljube ?i? and Toral , 2014 ) ( see Table 15 ) .
Released corpora for the Spanish - Portuguese language pair included parallel datasets : Europarl v10 ( Koehn , 2005 ) ( Koehn , 2005 ) , News Commentary v16 , News Crawl ( see Table 16 ) .
Moreover , corpora for the Romanian - Spanish language pair ( see Table 17 ) and the Romanian - Portuguese language pair ( see Table 18 ) contained parallel datasets : Europarl v8 ( Koehn , 2005 ) , Wiki Titles v3 , Tilde MODEL ( Rozis and Skadin , ? , 2017 ) , JRC - Acquis ( Steinberger et al. , 2006 ) , and monolingual data : Europarl v10 ( Koehn , 2005 ) , News Commentary v16 , News Crawl , Common Crawl .
The released parallel Tamil-Telugu dataset was collected from news ( Siripragada et al. , 2020 ) , PMIndia and MKB ( Man Ki Baat ) datasets .
All data were initially combined , tokenized using indic-nlp tokenizer ( Kunchukuttan , 2020 ) and randomly shuffled .
A subset of data extracted from the dataset are used for test and development set .
The remaining data were considered as training set ( cf. Table 21 ) .
Finally , the parallel Bambara - French corpus is a part of the Bambara Reference Corpus 21 .
Development and Test Data
The development and test sets for Spanish -Catalan , Spanish -Portuguese , Romanian - Spanish and Romanian - Portuguese language pairs were created from a corpus provided by Pangeanic 22 . Catalan translations were provided by the Directorate -General for Language Policy at the Ministry of Culture , Government of Catalonia .
Each dev and test dataset was cleaned , deduplicated and shuffled , resulting in 969 and 999 sentences in dev and test sets respectively .
Participants and Approaches SEBAMAT SEBAMAT submitted their system for two language pairs , Spanish -Catalan and Spanish - Portuguese , in both directions .
The SE-BAMAT approach is based on the Marian NMT toolkit that leverages the Transformer architecture .
The systems were trained using only the parallel corpora that were made available for the participants .
For all the language pairs and directions , SEBAMAT submitted PRIMARY and CONTRASTIVE systems with different vocabulary sizes ( 40,000 and 85,000 , respectively ) .
Interestingly , in all the cases , the PRIMARY systems with a smaller vocabulary size performed better in terms of BLEU scores .
T4T
The T4T team participated in the SLT 2021 Romance languages track , submitting their system for Spanish ? Catalan and Spanish ?
Portuguese .
While their systems are built using outof-the- box OpenNMT toolkit , the team developed custom cleaning scripts and an adhoc tokenizer .
SentencePiece library was used for pre-processing and reducing the vocabulary size to 16,000 symbols .
Corpus
UBC-NLP
The UBC - NLP team submitted their Spanish ?
Portuguese , Catalan ? Spanish and French ?
Bambara systems to the SLT 2021 task .
Their systems are built using Transformers from the HuggingFace library .
The UBC - NLP team experimented with tokenized ( PRIMARY ) and untokenized ( CONTRASTIVE ) systems and compared them with models developed by fine-tuning pre-trained models as well as models trained from scratch .
The pre-trained models were developed using Marian NMT by Helsinki-NLP on Hugging - Face .
A3-108
Results
Similarly to the previous edition of the SLT shared task , participants could submit systems for the Spanish - Catalan and Spanish - Portuguese language pairs ( in both directions ) .
The best systems for Spanish -to - Portuguese ( see Table 25 ) achieved over 40 BLEU and around 85 RIBES .
While in the opposite direction ( Portuguese - to - Spanish ) the best performing system reached 47.71 of BLEU ( see Table 24 ) .
As the Spanish - Catalan dev and test sets were aligned with Spanish - Portuguese ones , we noticed that the best results for the Spanish - Catalan language pair are in general much better than for Spanish - Portuguese .
For Spanish - to - Catalan the best system attained over 79 BLEU and below 15 TER ( see Table 27 ) .
However , its RIBES score ( 95.76 ) was lower than the runner - up system 's ( 96.24 ) .
In the case of Catalan- to - Spanish , the best system scored over 82 BLEU and less than 11 TER ( see Table 26 ) .
As there were no submissions for Romanian - Spanish and Romanian - Portuguese , we do not provide any evaluations for these language pairs .
Summary
This section presented the results and findings of the third edition of the SLT shared task at WMT .
The third iteration of this competition featured data from multiple language pairs from three different language families : Dravidian , Manding , and Romance languages .
We evaluated the systems translating in both directions of the language pair using three automatic metrics : BLEU , RIBES , and TER .
Most teams this year participated in the Dravidian language pairs .
Following a trend observed in the past editions of the task , we observed that the performance varies widely between language pairs and domains .
Triangular MT
This section presents an overview of the Triangular MT shared task .
Given a low-resource language pair ( X / Y ) , the bulk of previous MT work has pursued one of two strategies .
?
Direct : Collect parallel X/Y data from the web , and train an X- to - Y translator , OR ? Pivot ( Utiyama and Isahara , 2007 ; Wu and Wang , 2009 ) : Collect parallel X / English and Y / English data ( often much larger than X /Y data ) , train two translators ( X - to - English + English - to - Y ) , and pipeline them to form an X- to - Y translator However , there are many other possible strategies for combining such resources .
These may involve , for example , ensemble methods , multisource training methods , multi-target training methods , or novel data augmentation methods .
For eg. ( Zoph et al. , 2016 ; Dholakia and Sarkar , 2014 ; Kim et al. , 2019 ) .
The Task
The goals of this shared task is to promote : ? translation between non-English languages , ? optimally mixing direct and indirect parallel resources , and ?
exploiting noisy , parallel web corpora
The task is Russian - to - Chinese machine translation .
We provided parallel corpora to the participating teams .
We evaluate system translations on a ( secret ) mixed - genre test set , drawn from the web and curated for high quality segment pairs .
After receiving test data , participants had one week to submit translations .
After all submissions are received , we posted a populated leaderboard that will continue to receive postevaluation submissions .
23
The evaluation metric for the shared task is 4 - gram character Bleu .
The script to be used for Bleu computation is Moses multi-bleu-detok .perl .
Instructions to run the script were released as part of the shared task .
24
The participants indicated their intent to participate via registration on the Codalab website for the shared task 25 and obtained the instructions and links to various resources .
Training Data
We provided three parallel corpora : ?
Chinese / Russian : crawled from the web and aligned at the segment level , and combined with different public resources .
?
Chinese / English : combining several public resources .
? Russian / English : combining several public resources .
The details of the training resources provided are shown in Table 30 .
The provenance of the collected parallel data is as follows .
We used a parallel data harvesting pipeline developed at DiDi to harvest Russian / Chinese parallel data on the Internet .
We downloaded parallel datasets available from Opus ( Tiedemann , 2009 ) for all the three language pairs - Russian / Chinese , Russian / English and English / Chinese .
Since united nations data and subtitles data ( Ru /En ) are very large sources of parallel data , we report statistics on these two types of Opus parallel sources .
In addition to Opus , we also curate parallel data from Wikimatrix ( Schwenk et al. , 2019 ) in all three language pairs and social media parallel data - Weibo and Twitter ( Ling et al. , 2013 ) .
We also release the provenance of each parallel segment , in case teams want to use this information to filter noisy data sources .
Creating the Test Dataset
We spent a considerable amount of time to curate high quality , parallel data online to be used as development and evaluation datasets .
This was a completely manual process undertaken by a native speaker of Russian who consulted with a native Chinese speaker from our team to ensure good quality translations ( that does not contain tell - tale signs of automatic translation ) .
Our workflow entailed finding websites and large chunks of parallel text , not necessarily from the same pages .
The sources selected were also hard to be harvested from a parallel data pipeline due to their difference in URL structure .
The sources selected were from a diverse range of non-traditional sources , and have a balance of different types of documents .
The topics would be famous works of literature , or tourism related news stories , and so on .
We copied large chunks of text from such sources and manually aligned the paragraphs , followed by manual sentence alignment , each done manually to ensure top quality parallel segments .
This was followed by a final filtering step to remove sentences and entire sources which had a significant overlap with training and development data .
The details of the development and test datasets are shown in Tables 31 and 32 .
Baselines and Final Results
We released a baseline system 26 as part of the shared task .
This is based on the Google Ten-sor2tensor 27 toolkit to train a Transformer - based NMT system .
We also provided the baseline bleu score on the development dataset ahead of the evaluation phase .
We had 2 simple baselines -( 1 ) Direct - Transformer model trained on the entire Russian / Chinese parallel dataset and decoded with ? = 1.0 and beam_size=4 .
( 2 ) Pivot model - 2 MT systems - Russian - to - English and Englishto - Chinese - each trained with the corresponding parallel data .
Both the Russian- to - English and the English - to - Chinese systems were decoded with alpha= 1.0 and beam_size=4 .
The baseline results on the development dataset as shown in Table 33 .
We had a total of six teams submitting their system outputs on the test dataset .
The evaluation metric was 4 - gram character bleu score .
The final evaluation results are shown in Table 34 . Team Name System Type BLEU ? RIBES ? TER ? UBC-NLP PRIMARY 3.62 36.17 -
Overview of the Submitted Systems Five out of the six participating systems submitted system description papers .
In this section we briefly discuss the outline of these systems .
For more details please refer to the proceedings .
- istic-team -2021
The team 's system is based on the Transformer architecture .
They used several corpus pre-processing steps such as special symbol filtering and filtering based on segment length .
In addition , they used contextbased system combination - which is a multiencoder to encode source sentence and contextual information from the machine translation results on the source sentence .
They tried with both a direct and pipeline - based pivot system and report that the latter outperforms the former .
- HW_TSC ( Li et al. , 2021a )
Huawei 's submission used a multilingual model which is a single neural machine translation model to translate among multiple languages .
Upon adding more parallel data , they report an increase in bleu score of upto 2 points using the multilingual model compared to the baseline model .
In addition they used several data pre-processing techniques to denoise the training data and data augmentation techniques such as back -translation to improve overall system performance .
- Papago ( Park et al. , 2021 )
Naver 's system reports that they get better performance by treating this as a bilingual machine translation task rather than as a multilingual translation task , based on their early experiments .
They use the transformer model with extensive data pre-processing , filtering and data augmentation .
To augment the direct bilingual data they synthetically generate bilingual sentence pairs using monlingual Chinese backtranslated to Russian and the 2 sets of indirect parallel dataset provided .
- DUT -MT ( Liu et al. , 2021a )
This team experimented with 2 different multilingual training models called mBART and mRASP , both of them based on underlying Transformer architecture .
They report boosted performance especially on rare words when using mRASP .
In addition , they also carry out data preprocessing and filtering to improve system performance .
- CFILT-IITB ( Mhaskar and Bhattacharyya , 2021 ) CFLIT -IITB team 's system used a pivotbased transfer learning technique .
In this technique they have 2 encoder- decoder models , source-pivot ( Russian - to- English ) and pivot-target ( English - to - Chinese ) , each of them trained on the respective training datasets .
They use the encoder of the former and the decoder of the latter to initialize a third encoder-decoder for the actual task of Russian - to - Chinese translation .
They fine tune this decoder using the given parallel data for Russian / Chinese .
They report this system has a better performance compared to either a direct or pivotbased cascaded system .
They do not experiment much with data pre-processing and filtering .
Source
Conclusion
The triangular machine translation shared task set out to explore various modeling possibilities when building a machine translation system for a non-English language pair .
We received enthusiastic participation from the participants .
Almost all of them performed data filtering and preprocessing to denoise the training datasets and that seemed to substantially help improve system performance .
The transformer model and its variants were used in all the system submissions confirming Transformer 's ubiquitous acceptance as the model of choice for building machine translation systems .
Many teams explored model ensembling and model averaging in addition to model reranking strategies .
Several teams explored backtranslation as an effective data-augmentation strategy .
There was a wide variety of modeling architectures experimented by the participants .
Almost everyone used all the parallel datasets provided underlining the importance of using parallel data in all directions to build a better machine translation system .
Overall we are happy that the shared task provided a platform to the participants to experiment with different modeling strategies .
We hope practitioners will find these techniques useful when working on machine translation between non-English language pairs .
