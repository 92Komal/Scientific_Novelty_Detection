title
RoBLEURT Submission for the WMT2021 Metrics Task
abstract
In this paper , we present our submission to Shared Metrics Task : RoBLEURT ( Robustly Optimizing the training of BLEURT ) .
After investigating the recent advances of trainable metrics , we conclude several aspects of vital importance to obtain a well - performed metric model by : 1 ) jointly leveraging the advantages of source- included model and referenceonly model , 2 ) continuously pre-training the model with massive synthetic data pairs , and 3 ) fine -tuning the model with data denoising strategy .
Experimental results show that our model reaching state - of - the - art correlations with the WMT2020 human annotations upon 8 out of 10 to -English language pairs .
Introduction
Automatically evaluating the adequacy of machine translation ( MT ) candidates is crucial for judging the quality of MT systems .
N-gram - based metrics , such as BLEU ( Papineni et al. , 2002 ) , TER ( Snover et al. , 2006 ) and chrF ++ ( Popovic , 2015 ( Popovic , , 2017 , have dominated in the topic of MT metric .
Despite the success , recent studies ( Smith et al. , 2016 ; Mathur et al. , 2020a ) also pointed out that , N-grambased metrics often fail to robustly match paraphrases and capture distant dependencies .
As MT systems become stronger in recent decades , these metrics show lower correlations with human judgements , leading the derived results unreliable .
One arising direction for metric task is using trainable model to evaluate the semantic consistency between candidates and golden references via predicting scores .
BERTScore ( Zhang et al. , 2020 ) , BLEURT ( Sellam et al. , 2020 ) and COMET ( Rei et al. , 2020 ) have shown higher correlations with human judgements than N-gram- based automatic metrics .
Benefiting from the powerful pre-trained language models ( LMs ) , e.g. , BERT ( Devlin et al. , 2019 ) , those fine-tuned metric models first derive the representation of each input , then introduce an extra linear regression module to give predicted score which describes to what degree the MT system output adequately expresses the semantic of source / reference contents .
Furthermore , related work ( Takahashi et al. , 2020 ; Rei et al. , 2020 ) reports that , metrics which additionally introduces source sentences into inputs can further boost the performance of metric model .
To push such " model as a metric " approach further , we present RoBLEURT - Robustly optimizing the training of BLEURT ( Sellam et al. , 2020 ) , to achieve a better consistency between model predictions and human assessments .
Specifically , for low-resource scenarios , using only hypotheses and references can give more accurate results , alleviating the sparsity of source - side language ; for the high- resource language pairs , we format the model input as the combination of source , hypothesis and reference sentences , making model attending to both source input and target reference when evaluating the consistency of semantics .
Then , we collect massive pseudo data from real MT engines tagged by pseudo scores with strong baselines for supervised model pre-training .
As to the fine-tuning phase , we rescore the noisy WMT metric data of previous years with strong metric baselines , which are then utilized to fine - tune our model .
Experimental results show that , following the setting of WMT2021 metric task , our RoBLEURT model outperforms the reported results of state - of - the - art metrics on multilingual - to - English language pairs .
RoBLEURT
Combining Multilingual and Monolingual Language Model Same as previous years , translation tasks cover both low-resource and high- resource scenarios .
To give higher reliable outputs , we believe our metric model can benefit from separately pre-trained and fine- tuned over each kind of scenarios : ?
For low-resource multilingual - to - English language pairs , we can hardly obtain massive parallel data with high quality , nor access wellperformed automatic translation systems to produce syntectic data for pre-training .
We mainly consider model outputs and gold references as our model inputs .
Thus we mainly consider the monolingual English language model ( called RoBLEURT - NOSRC ) in this scenario . ?
As to high- resource language pairs , they do not suffer from limitations above , thus can benefit from the information of source input , model output and target reference .
A multilingual version of pre-trained LM ( called RoBLEURT - SRC ) can be used for this scenario .
The main architecture of our model is TRANS - FORMER ( Vaswani et al. , 2017 ) , which has been widely used in recent researches .
As related studies point out that RoBERTa ( Liu et al. , 2019 ) outperforms conventional BERT ( Devlin et al. , 2019 ) , we employ the well - trained model checkpoint from RoBERTa family .
Besides , the conventional BLEURT model is trained based on uncased - BERT , which tokenizes the input sentences with the lower case format whereas RoBERTa uses casesensitive tokenizer , which may be helpful to distinguish more information .
Moreover , model with larger scale is generally related with better performance and higher capacity of available knowledges .
Recently , several approaches which further finetune RoBERTa model can give better performance over multiple natural language inference tasks .
To make sure our model can also benefit from this , we finally use RoBERTa - large-mnli 1 and RoBERTa - large- xnli 2 ( Conneau et al. , 2020 ) for low-resouce and high- resource language pairs , respectively .
Model Combination
We are also interested in exploring whether we can boost the performance of combine RoBLEURT - NOSRC and RoBLEURT - SRC .
Combining the out-puts from models trained with different settings is widely used in MT tasks ( Barrault et al. , 2020 ) .
In this paper , We simply use weighted combination of all available well - trained models .
Input Formatting
Our model consists of a well - trained RoBERTa model to obtain segment - level representations .
Here we also try with two solutions : the model input includes source sentence ( RoBLEURT - SRC ) or not ( RoBLEURT - NOSRC ) .
For the former , the model input is formatted as : <s> hyp ' </s> </s> ref </s>. ( 1 ) As the latter , due to the number of input sentences is larger than RoBERTa predefined training format , we redesigned the input format as : < s> src </s> </s> hyp ' </s> </s> ref </s>. ( 2 )
Prediction Module
To obtain a scalar value as predicted score , we directly derive the representation at the first position of input X ? R 1?d as the representation of input tuple , where d is the size of hidden states .
It is then fed to projection layer , after which we yield a scalar for describing how adequately the hypothesis express the semantics : s = WX + b , ( 3 ) where W ? R 1?d , b ?
R 1 are both trainable pa- rameters .
During training , the learning objective is to reduce the mean squared error ( MSE ) between model prediction s and annotated score score : L = ( s ? score ) 2 . ( 4 )
Continuous Pre-training with Synthetic Data Continuous
Pre-training the model on synthetic data is proven helpful to improve the performance ( Sellam et al. , 2020 ) ?
Available well - performed checkpoints .
We collect the officially released COMET 4 and BLEURT checkpoints 5 . After collecting the predictions with all checkpoints above , we identify the noisy data items by computing the variance of rankings within whole dataset .
Finally , we rescore those noisy items with those models , tagging pseudo labels for fine-tuning .
Besides , to guarantee the scores are unbiased , we re-normalize them within the dataset of each year by Z-score following Sellam et al . ( 2020 ) .
Experiments
Settings of Continuous Pre-training Synthetic Data Collection
To continue pretraining the model , we simply collect parallel data from the previous WMT conferences , taking the training data from MT track cs / de / ja/ru/zh-en language pairs to obtain high- resource pseudo data .
Finally , for each language pair , we collect 2.0 million quadruples for metric model pre-training .
For lowresource scenarios , we reuse the datasets above , where the only difference is removing the source sentences .
As to development set , we directly collect the direct assessment ( DA ) dataset from the WMT2020 Metrics task track .
We evaluate the model performance following DARR assessments ( Ma et al. , 2019 ; Rei et al. , 2020 ) , and choose the best checkpoint for fine-tuning .
Hyper-parameters
During the continuous pretraining , we determine the maximum learning rate as 5 ? 10 ?6 , training steps as 0.5 M and warm - up steps as 50K .
The learning rate first linearly warms up from 0 to maximum learning rate , then decays to ( Sellam et al. , 2020 ) 12.6 45.6 25.8 9.3 13.7 25.8 32.7 5.7 20.7 23.0 COMET ( Rei et al. , 2020 ) 12.9 48.5 27.4 15.6 17.1 28.1 29.8 9.9 15.8 24.1 SOTA Results ( Mathur et al. , 2020 b ) Table 1 : DARR Kendall correlation ( % ) over WMT2020 data for each language pair ( xx-en ) .
Results of baseline systems are conducted from official report ( Mathur et al. , 2020 b ) .
Best viewed in bold .
0 till the end of training .
To avoid over-fitting , we apply the dropout ratio as 0.1 .
We conduct the pretraining experiments with 8 Nvidia V100 GPUs , where each batch size for each GPU device contains 4 quadruplets .
To avoid memory issues during pre-training , we simply reduce the number of total tokens , leaving 128 and 192 for RoBLEURT - NOSRC and RoBLEURT - SRC , respectively .
Settings of Fine-tuning Data Collection
We fine- tune our model with the WMT2015 - 2019 dataset as training set , where the WMT2018 - 2019 subsets are processed with our data denoising strategy as discussed in ? 2.3 .
To directly confirm the effectiveness of our approach , we simply use WMT2020 dataset as dev set to compare reported results in WMT2020 metric task .
To select the model for participating the WMT2021 metric task , we divide the WMT2020 dataset into 4 folds , where the data items are firstly gathered with the identical source and reference sentence .
For each fold , we select the corresponding fold of the WMT2020 subset as the dev set , and use the combination of the WMT2015 - 2019 dataset and the other unused WMT2020 subsets as the training set .
Hyper-parameters
During fine-tuning , we set the training steps and warm - up steps as 20 K and 2K , respectively .
The other hyper-parameters are identical to those of pre-training phase .
For each fine-tuning experiment , we determine the batch size as 16 , and whole training process requires one single Nvidia V100 GPU .
Main Results
We first testify the effectiveness of our approach by comparing with the results from the WMT2020 Metrics Task submissions .
To be fairness , all of the model based metric baselines are trained on the WMT2015 - 2019 dataset .
As shown in Table 1 , comparing to baselines , our RoBLEURT achieves the best performance on cs/de/ja/ru/zh/iu/pl/ta-to-en settings , and achieves competitive results on km-to-en and ps-to-en .
Ablation Studies
Model Pedestal and Size
We first investigate the impact of model pedestal for metric task .
As shown in Table 3 , using RoBERTalarge instead of RoBERTa - base model as the base of RoBLEURT - SRC model gives a better performance .
Furthermore , using the fine-tuned checkpoint RoBERTa - large-xnli can further improves the performance .
This indicates our view , that powerful pre-trained LM , as well as the carefully reoptimized variants , can boost the performance of fine-tuned metric model .
Pre-training
To identify the improvement after introducing extra pre-training steps for metric model , we conduct the results in Table 4 for comparison .
As seen , the performance drops significantly without pre-training phase .
This caters to the previous findings ( Sellam et al. , 2020 ) , where pre-training with pseudo data helps the supervised learning of metric model .
Data Denoising Strategy
As reported in ( Sellam et al. , 2020 ) , the WMT2018-2019 DA subsets are bothered with noisy labels .
We also investigate the impact of those data , whether introducing them into model training , or even clean them via rescoring with stronger metric .
We thus arrange such ablation study during finetuning , and results are conducted in of full training set ( 237 K vs. 247K ) , the performance of RoBLEURT model trained without these noisy items does not diminish significantly .
After rescoring with available checkpoints , these data segments further improves model performance .
Model Combination
We first identify whether introducing source side information to metric model helps training .
As seen in Table 2 , accepting source ( row RoBLEURT - SRC ) than not ( row RoBLEURT - NOSRC ) as extra input significantly improves the correlation scores .
However , for low-resource scenarios , experimental results show that source-side information does not help much for model training .
This indicates that source information does not provide help for model training over low-resource scenarios , as the inadequacy of pre-training data may harms model training if source side is introduced .
To derive better performance , one general idea is to combine several well - trained models during inference .
We also explore whether combining both RoBLEURT -SRC and RoBLEURT - NOSRC models can give better performance .
As shown in Table 2 , directly averaging scores from multiple models lead to a significant performance drop .
On the contrary , our model , which takes models over both RoBLEURT - NOSRC and RoBLEURT - SRC settings can effectively leverage the predictions , achieving significant performance gain across all language pairs .
Conclusion
In this paper , we describe our submission metric - RoBLEURT , from the perspective of combining multilingual and monolingual language model , continuous pre-training with the massive synthetic data pairs , and fine-tuning with data denoising strategy .
Experimental results confirms the effectiveness of our pipeline , demonstrating state - of - the - art correlations with the WMT2020 human annotations upon 8 out of 10 to -English language pairs .
Table 5 . 5 Although the noisy portion contributes a great share Model cs de ja ru zh iu km pl ps ta RoBLEURT - NOSRC 13.5 46.9 27.4 10.8 14.8 28.2 30.6 8.3 14.7 25.0 RoBLEURT -SRC 14.1 47.9 28.7 11.7 14.9 27.5 29.9 6.4 16.0 24.0 RoBLEURT 15.2 49.3 29.1 17.3 17.7 29.0 31.4 13.2 20.1 25.4
