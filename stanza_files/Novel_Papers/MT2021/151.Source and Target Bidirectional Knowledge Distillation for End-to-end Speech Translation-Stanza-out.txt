title
Source and Target Bidirectional Knowledge Distillation for End-to-end Speech Translation
abstract
A conventional approach to improving the performance of end-to - end speech translation ( E2E - ST ) models is to leverage the source transcription via pre-training and joint training with automatic speech recognition ( ASR ) and neural machine translation ( NMT ) tasks .
However , since the input modalities are different , it is difficult to leverage source language text successfully .
In this work , we focus on sequencelevel knowledge distillation ( SeqKD ) from external text - based NMT models .
To leverage the full potential of the source language information , we propose backward SeqKD , SeqKD from a target- to-source backward NMT model .
To this end , we train a bilingual E2E -ST model to predict paraphrased transcriptions as an auxiliary task with a single decoder .
The paraphrases are generated from the translations in bitext via back - translation .
We further propose bidirectional SeqKD in which SeqKD from both forward and backward NMT models is combined .
Experimental evaluations on both autoregressive and non-autoregressive models show that SeqKD in each direction consistently improves the translation performance , and the effectiveness is complementary regardless of the model capacity .
Introduction End-to - end speech translation ( E2E - ST ) ( B?rard et al. , 2016 ) , which aims to convert source speech to text in another language directly , is an active research area .
Because direct ST is a more difficult task than automatic speech recognition ( ASR ) and machine translation ( MT ) , various techniques have been proposed to ease the training process by using source transcription .
Examples include pretraining ( B?rard et al. , 2018 ; Wang et al. , 2020c ; Bansal et al. , 2019 ; Wang et al. , 2020d ) , multi-task learning ( Weiss et al. , 2017 ; B?rard et al. , 2018 ; Bahar et al. , 2019 ) , knowledge distillation , meta-learning ( Indurthi et al. , 2020 ) , twopass decoding ( Anastasopoulos and Chiang , 2018 ; Sperber et al. , 2019 ) , and interactive decoding Le et al. , 2020 ) .
However , as input modalities between ST and MT tasks are different , an auxiliary MT task is not always helpful , especially when additional bitext is not available ( Bahar et al. , 2019 ) .
Moreover , because monotonic speech- to-transcription alignments encourage the ASR task to see surface - level local information , an auxiliary ASR task helps the E2E -ST model to extract acoustic representations , not semantic ones , from speech .
Sequence -level knowledge distillation ( Se - qKD ) ( Kim and Rush , 2016 ) is another approach to transferring knowledge from one model to another .
Recent studies have shown that SeqKD has the effect of reducing the complexity of training data and thus eases the training of student models , e.g. , non-autoregressive ( NAR ) models ( Gu et al. , 2018 ; Zhou et al. , 2019a ; Ren et al. , 2020 ) .
Paraphrasing , which represents text in a different form but with the same meaning , can also be regarded as SeqKD when using neural paraphrasing via back - translation Wieting et al. , 2017 ; Federmann et al. , 2019 ) .
It has been studied to improve the reference diversity for MT system evaluations Bawden et al. , 2020a , b) and the performance of low-resource neural MT ( NMT ) models ( Zhou et al. , 2019 b ; Khayrallah et al. , 2020 ) .
In this work , due to its simplicity and effectiveness , we focus on SeqKD from text - based NMT models to improve the performance of a bilingual E2E -ST model .
In order to fully leverage source language information , we propose backward Se- qKD , which targets paraphrased source transcriptions generated from a target - to-source backward NMT model as an auxiliary task .
Then , a single ST decoder is trained to predict both source and target language text as in a multilingual setting .
This way , the decoder is biased to capture semantic representations from speech , un-like joint training with an auxiliary ASR task .
We also propose bidirectional SeqKD , which combines SeqKD from two NMT models in both language directions .
Therefore , the E2E-ST models can fully exploit the knowledge embedded in both forward and backward NMT models .
Experimental evaluations demonstrate that Se-qKD from each direction consistently improves the translation performance of both autoregressive and non-autoregressive E2E -ST models .
We also confirm that bidirectional SeqKD outperforms unidirectional SeqKD and that the effectiveness is maintained in large models .
Method
In this section , we propose bidirectional SeqKD from both forward and backward NMT models that leverages machine - generated source paraphrases as another target in addition to the distilled translation to enhance the training of a bilingual E2E - ST model .
Let X denote input speech features in a source language and Y s and Y t denote the corresponding gold transcription and translation , respectively .
Let D st = { ( X i , Y s i , Y t i ) }
I i=1 be an ST dataset including I samples , and D asr = { ( X i , Y s i ) }
I i=1 and D mt = { ( Y s i , Y t i ) }
I i=1 denote the corresponding ASR and MT datasets , respectively .
1 We drop the subscript i when it is obvious .
Sequence-level knowledge distillation
We first train a text - based source - to- target forward NMT model M fwd with D mt .
2 Then , we perform beam search decoding with M fwd on D st to create a new dataset D fwd st = { ( X i , Y s i , ? t i ) }
I i=1 , where ?
t i is a distilled translation .
D fwd st is used to train the E2E -ST models , referred to as forward SeqKD ( or fwd SeqKD ) .
Paraphrase generation
To exploit semantic information in the source language , we leverage machine - generated paraphrases of source transcriptions .
We train a text - based target - to- source backward NMT model M bwd with D mt and then generate a new dataset D bwd st = { ( X i , ? s i , Y t i ) }
I i=1 , where ?
s i is a paraphrase of Y s i .
We use D bwd st for training the E2E-ST models .
As neural paraphrasing can be regarded as SeqKD from M bwd , we referred to it as backward SeqKD ( or bwd SeqKD ) .
In this work , we do not use large paraphrase datasets ( Wieting and Gimpel , 2018 ; Hu et al. , 2019 ) because their availability depends on languages and domains .
Moreover , neural paraphrasing is applicable to any source languages that lack a sufficient amount of paired paraphrase data .
We also propose combining forward SeqKD with backward SeqKD , referred to as bidirectional SeqKD ( or bidir SeqKD ) , and construct a new dataset D bidir st = { ( X i , ? s i , ? t i ) }
I i=1 .
When using two references per utterance ( 2 ref training ) ( Gordon and Duh , 2019 ) , we concatenate D fwd st and D bwd st , and the suitable combination is analyzed in Section 4.3 .
This way , we can distill the knowledge of both M fwd and M bwd to a single E2E -ST model .
Training
We train an E2E-ST model with a direct ST objective L st ( Y t or ? t | X ) and an auxiliary speechto-source text objective L src ( Y s or ? s | X ) .
We refer to joint training with L src ( Y s | X ) as joint ASR and with L src ( ? s | X ) as backward SeqKD .
Both losses are calculated from the same ST decoder .
To bias the model to generate the desired target language , we add language embedding to token embedding at every token position in the decoder ( Conneau and Lample , 2019 ) .
3
We then apply bidirectional SeqKD to both autoregressive ( AR ) and non-autoregressive ( NAR ) E2E -ST models .
Autoregressive E2E-ST model
We use the speech Transformer architecture in ( Karita et al. , 2019 ) with an additional language embedding .
The total training objective is formulated with a hyperparameter ? src ( ? 0 ) as L total = L st + ? src L src , ( 1 ) where both L st and L src are defined as crossentropy losses .
The entire encoder-decoder parameters are shared in both tasks .
Non-autoregressive E2E-ST model
We adopt Orthors ( Inaguma et al. , 2021 ) , in which a decoder based on a conditional masked language model ( CMLM ) ( Ghazvininejad et al. , 2019 ) is jointly trained with an additional AR decoder ( Ghazvininejad et al. , 2020 ) .
L st in Eq. ( 1 ) is modified as L st = L cmlm + ? ar L ar + ? lp L lp , ( 2 ) where L cmlm , L ar , and L lp are losses in NAR E2E-ST , AR E2E-ST , and length prediction tasks , respectively .
? * is the corresponding tunable loss weight .
During inference , the mask - predict algorithm is used for T iterations with a length beam width of l ( Ghazvininejad et al. , 2019 ) .
The best candidate at the last iteration is selected from the NAR decoder based on scores from the AR decoder ( Inaguma et al. , 2021 ) .
Note that we apply L src to the NAR decoder only .
Experimental setting Data We used Must -C En-De ( 408 hours ) and En- Fr ( 492 hours ) datasets ( Di Gangi et al. , 2019 ) .
Both language pairs consist of a triplet of ( X , Y s , Y t ) .
We performed the same data preprocessing as ( Inaguma et al. , 2020 ) ( see details in Appendix A.1 ) .
We report case-sensitive detokenized BLEU scores ( Papineni et al. , 2002 ) on the tst-COMMON set with the multi-bleu-detok .
perl script in Moses ( Koehn et al. , 2007 ) .
Model configuration
We used the Transformer ( Vaswani et al. , 2017 ) architecture having 12 encoder layers following two CNN blocks and six decoder layers for the ASR and E2E -ST tasks .
For the MT models , we used six encoder layers .
We built our models with the ESPnet - ST toolkit ( Inaguma et al. , 2020 ) .
See details in Appendix A.2 .
Training
We always initialized the encoder parameters of the E2E-ST model by those of the corresponding pre-trained ASR model ( B?rard et al. , 2018 ) .
We follow the same optimization strategies as in ( Inaguma et al. , 2021 ( Inaguma et al. , , 2020 .
When using joint ASR or backward SeqKD , we set ? src to 0.3 .
More details are described in Appendix A.3 and A.4 . Inference
For the AR models , we used a beam width of 4 .
For the NAR models , we set T = { 4 , 10 } and l = 9 as in ( Inaguma et al. , 2021 ) .
Results
Main results
We first report the paraphrasing quality , which is shown in Table 1 .
As confirmed by the BLEU and translation edit rate ( TER ) scores ( Snover et al. , 2006 ) , the paraphrased source text was not just a simple copy of the transcription ( see examples in Appendix A.5 ) .
Autoregressive models
The results are shown in Table 2 . Pre-training the ST decoder with the forward MT decoder ( A2 ) improved the baseline performance ( A1 ) .
Joint ASR showed a marginal improvement on En- De but a degraded performance on En- Fr ( A3 ) .
We attribute this to the fact that the ASR task was more trivial than the ST task and biased the shared decoder to capture surface - level textual information .
In contrast , backward SeqKD showed small but consistent improvements in both language directions ( A4 ) , and it was as effective as MT pre-training .
As the encoder was already pre-trained with the ASR model , paraphrases had an additional positive effect on the BLEU improvement .
Forward SeqKD significantly improved the performance , as previously reported in ( Inaguma et al. , 2021 ) .
However , the gains by MT pre-training and joint ASR were diminished .
Forward SeqKD was more effective than backward SeqKD solely ( A4 vs. B1 ) .
However , backward SeqKD was still beneficial on top of forward SeqKD ( C1 , i.e. , bidirectional SeqKD ) while joint ASR was less so ( B3 ) .
We also augmented the target translations by concatenating D st and D fwd st ( 2 ref training ) , which further improved forward SeqKD ( B4 ) .
Nevertheless , a combination of 2 ref training and backward Se-qKD ( i.e. , bidirectional SeqKD with D fwd st ?
D bwd st ) had a complementary effect and showed the best result ( C2 ) .
It even outperformed larger multilingual models ( Wang et al. , 2020a ) without using additional data in other language pairs .
Non-autoregressive models
The results are presented in Table 3 .
Following the standard practice in NAR models ( Gu et al. , 2018 ) , we always used forward SeqKD .
We did not use 2 ref training for the NAR models because it increases the multimodality .
Joint ASR improved the performance on all NAR models , except for En- Fr with the number of iterations T = 10 .
However , bidirectional SeqKD with D bidir st further improved the performance consistently regardless of T .
Since NAR models assume conditional independence for every token , they prefer monotonic input-output alignments with lower alignment complexity in theory .
However , paraphrasing collapses the monotonicity of the ASR task and increases the alignment complexity , making the auxiliary speech - to - source text task non-trivial .
Nevertheless , BLEU scores were improved by adding backward SeqKD .
This was probably because the complexity of transcriptions in the training data was reduced at the cost of the alignment complexity , which was more effective for the NAR models .
Analysis
We analyze the performance of bidirectional Se-qKD through a lens of complexity in the training data following ( Zhou et al. , 2019a ) .
We aligned words in every source and target sentence pair with ( Dyer et al. , 2013 ) .
Then , we calculated corpus-level conditional entropy C( D ) and faithfulness F ( D ) for both forward ( ? ? D ) and backward ( ? ? D ) language directions to evaluate the multimodality .
In short , conditional entropy measures uncertainty of translation , and faithfulness is defined as Kullback - Leibler divergence and measures how close the distilled data distribution is to the real data distribution .
See the mathematical definition in Appendix A.6 .
The results of entropy and faithfulness are shown in Tables 4 and 5 , respectively .
Consistent with ( Zhou et al. , 2019a ) , the entropy of target translations was reduced by forward SeqKD , indicating target translations were converted into a more deterministic and simplified form .
Interestingly , the entropy of the original translations was also reduced by backward SeqKD .
In other words , backward Se- qKD modified transcriptions so that the target translations can be predicted easier .
This would help E2E -ST models learn relationships between source and target languages from speech because E2E -ST models are not conditioned on text in another language explicitly .
Therefore , we presume that the encoder representations were enhanced by back - ward SeqKD .
Using machine - generated sequences in both languages increased the entropy , probably due to error accumulation .
However , E2E -ST models do not suffer from it because they are conditioned on the source speech .
We also confirmed similar trends in the reverse language direction .
Regarding faithfulness , distilled target sequences degraded faithfulness as expected .
However , an interesting finding was that the faithfulness of bidirectional SeqKD was better than that of forward SeqKD , meaning that the former reflected the true word alignment distribution more faithfully than the latter .
Although lexical choice might be degraded by targeting distilled text in both languages ( Ding et al. , 2021 ) , mixing the original and distilled text by 2 ref training would recover it .
Ablation study
We conduct an ablation study to verify the analysis in the previous section .
In Table 4 , we observed that it was better to have the original reference in the target sequence of either the source or target language .
For example , to reduce the entropy of German text in the training set , it was best to condition the distilled German translation on the original English transcription , and vice versa .
Therefore , we hypothesize that the best way to reduce the entropy in both source and target languages during 2 ref training is to combine ( ? s , Y t ) and ( Y s , ? t ) for each sample .
We compared four ways to leverage source text : gold transcription Y s only , distilled paraphrase ?
s only , and both .
5
The results are shown in Table 6 .
We confirmed that the model trained with the original reference in either language for every target achieved the best BLEU score , which verifies our hypothesis .
Increasing model capacity Finally , we investigate the effectiveness of bidirectional Seq -KD with 2 ref training when increasing the model capacity in this experiment is to verify our expectation that large models can model complex target distributions in multi-referenced training better .
In addition to simply increasing the model dimensions , we also investigate Conformer ( Gulati et al. , 2020 ) , a Transformer encoder augmented by a convolution module .
We confirmed that bidirectional Se-qKD always outperformed forward SeqKD in both language directions regardless of model configurations .
We also found that the Conformer encoder significantly boosted the translation performance of forward SeqKD , but the gains of bidirectional SeqKD were transferred .
Conclusion
To fully leverage knowledge in both source and target language directions for bilingual E2E -ST models , we have proposed bidirectional SeqKD , in which both forward SeqKD from a source - to- target NMT model and backward SeqKD from a target- tosource NMT model are combined .
Backward Se-qKD is performed by targeting source paraphrases generated via back -translation from the original translations in bitext .
Then , the E2E-ST model is enhanced by training to generate both source and target language text with a single decoder .
We experimentally confirmed that SeqKD from each direction boosted the translation performance of both autoregressive and non-autoregressive E2E -ST models , and the effectiveness was additive .
Multi-referenced training with the original and distilled text gave further gains .
We also showed that bidirectional SeqKD was effective regardless of model sizes .
Reference1
She took our order , and then went to the couple in the booth next to us , and she lowered her voice so much , I had to really strain to hear what she was saying .
Paraphrase1 ( Backward NMT )
She picked up our order , and then went to the pair in the niche next to us and lowered her voice so much that I had to really try to understand them .
