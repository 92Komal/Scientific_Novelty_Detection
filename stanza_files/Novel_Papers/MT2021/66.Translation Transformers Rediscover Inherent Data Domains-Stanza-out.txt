title
Translation Transformers Rediscover Inherent Data Domains
abstract
Many works proposed methods to improve the performance of Neural Machine Translation ( NMT ) models in a domain / multi-domain adaptation scenario .
However , an understanding of how NMT baselines represent text domain information internally is still lacking .
Here we analyze the sentence representations learned by NMT Transformers and show that these explicitly include the information on text domains , even after only seeing the input sentences without domains labels .
Furthermore , we show that this internal information is enough to cluster sentences by their underlying domains without supervision .
We show that NMT models produce clusters better aligned to the actual domains compared to pre-trained language models ( LMs ) .
Notably , when computed on document- level , NMT cluster-to- domain correspondence nears 100 % .
We use these findings together with an approach to NMT domain adaptation using automatically extracted domains .
Whereas previous work relied on external LMs for text clustering , we propose re-using the NMT model as a source of unsupervised clusters .
We perform an extensive experimental study comparing two approaches across two data scenarios , three language pairs , and both sentencelevel and document- level clustering , showing equal or significantly superior performance compared to LMs .
Introduction Neural machine translation ( NMT , Bahdanau et al. , 2015 ; Vaswani et al. , 2017 b ) heavily depends on training data and the text domains covered in it .
Full-scale NMT
Transfomer models ( Vaswani et al. , 2017 b ) are usually trained on a mix of corpora from several domains ( Barrault et al. , 2020 ) .
However , the field lacks an understanding of how these NMT models represent the training data domains in their inner vector spaces .
* Equal contribution
This paper consists of two main parts .
First , we analyze domain representations learned by the NMT Transformer .
We consider sentence - level as well as document-level representations via mean pooling of token contextual embeddings .
Our analysis shows that NMT models explicitly learn to include the domain information in their representational spaces across layers .
Furthermore , we show that text representations preserve enough domainspecific information to reveal the underlying domains with Principal Component Analysis and kmeans clustering without supervision .
In the case of document- level clustering , the result of k-means matches the original corpora almost perfectly .
In the case of sentence - level clustering , we observe some deviation between automatic clusters and the original corpora that the sentences belong to , showing corpus heterogeneity on the sentence level .
Aharoni and Goldberg ( 2020 ) previously revealed that a similar property exists in pre-trained language models ( LMs ) .
We compare LMs with NMT
Transformers in how well we can extract unsupervised domain clusters from them and show the superiority of NMT models .
In the second part of the paper , we show how to effectively utilize our analysis to improve an existing approach to NMT domain adaptation which uses automatically extracted domains ( Tars and Fishel , 2018 ; Currey et al. , 2020 ) .
This method targets the case when training domain labels are not precise ( e.g. Currey et al. , 2020 ) or missing overall , as in case of heterogeneous corpora ( e.g. Paracrawl , Espl ?
et al. , 2019 ) .
This framework has so far been used with external models for clustering , which automatically makes us rely on clusters not necessarily aligned with the objectives of translation or target data domains .
We propose exploiting clusters extracted from the NMT baseline ( already trained as a part of the framework ) to improve translation quality without relying on external language models .
We test our Figure 1 : PCA plots of sentence representations extracted from all layers of the 60th checkpoint of the trained baseline NMT model .
Representations are computed with English sentences .
The dots , denoting sentences , are colored according to the domain the corresponding sentences come from .
The model learns to distinguish between domains in its hidden space , despite not being explicitly provided with any information about domains .
L0 corresponds to fixed encoder embeddings , L1 - L6 are encoder layers ' representations , L7 shows fixed decoder embeddings and L8 - L13 - the decoder layers ' representations .
The figure shows that representations from the same domain cluster together .
proposal empirically , covering three language pairs and two data settings : a mix of corpora with known domain labels and a heterogeneous corpus without such labels .
We show that fine-tuning the NMT models to the automatically discovered clusters on average matches or surpasses tuning to the original corpus labels ( when available ) and deep LM - based clusters .
Our contributions are thus two -fold : 1 ? we analyze the NMT encoder 's representations , showing their ability to automatically discover inherent text domains and cluster unlabelled corpora , testing both sentencelevel and document- level representations ( Section 3 ) ; ? we utilize findings from our analysis to improve an existing Automatic Domains for NMT approach ( Section 4 ) and perform an extensive experimental study , showing the superiority of our method ( Section 5 ) ; 2 Related Work Aharoni and Goldberg ( 2020 ) found that BERT ( Devlin et al. , 2019 ) produces meaningful unsupervised domain clusters and used this finding for NMT data selection .
In this work we analyse ( sentence - level and document- level ) hidden representations produced by a baseline NMT model and find that it learns superior unsupervised clusters by itself .
In NMT , domain-specific information on the word level was recently analyzed by Jiang et al .
1 We release our code at https://github.com/ TartuNLP / inherent-domains-wmt21 ( 2020 ) in the context of domain mixing in a joint modular multi-domain NMT system .
They found that representations contain domain-specific information related to the multiple domains in different proportions on the word level .
We analyze representation on the sentence and document level , revealing that domain-specific information in representations converges to the one specific domain with a broader context .
Currey et al. ( 2020 ) used contextual embeddings and mean-pooled representation clustering for domain adaptation .
We compare our approach to Currey et al . ( 2020 ) , however in their case the representations were extracted from multilingual BERT ( mBERT ) .
We cluster based on the NMT encoder 's representations directly and also experiment with document - level representations in addition to sentence - level ones .
Before Currey et al. ( 2020 ) , the automatic domains framework has been used in NMT only with external models for clustering as well .
Tars and Fishel ( 2018 ) used fixed embeddings from Fast - Text ( Bojanowski et al. , 2017 ) for clustering meanpooled sentence representations and then either tuning NMT systems to these clusters or supplying the cluster identity to the NMT system as additional input for multi-domain translation .
Analysis
In this section , we perform an analysis of inherent domain representations in translation transformers .
We reveal how well the domain-specific information in text representations is preserved in NMT models .
We focus on " out - of - the-box " NMT systems without any changes and explore the extent to which we can use their internal representations to match the original text domains using Principal Component Analysis ( PCA ) and k-means clustering .
We also measure the effect of using broader document - level representations .
Additionally , we compare NMT representations to the ones extracted from a pre-trained language model , for which Aharoni and Goldberg ( 2020 ) revealed a high degree of domain-specific information .
Models and Data
In our analysis , we start by following Currey et al . ( 2020 ) and similarly to them use a multilingual LM ( XLM -R , Conneau et al. , 2020 ) to obtain clusters .
XLM -R is a multilingual masked language modeling transformer covering 100 languages .
We then train Transformer - base ( Vaswani et al. , 2017a ) NMT models , which have ?97 M parameters each .
We train the models on parallel data covering four corpora / text domains : parliament speeches ( Europarl , Koehn , 2005 ) , medical ( EMEA , Tiedemann , 2012 ) , subtitles ( OpenSubtitles , Lison and Tiedemann , 2016 ) and legal ( JRC - Acquis , Steinberger et al. , 2006 ) .
We sub-sampled the larger corpora in order to balance the size of training data across domains .
The NMT models were trained for 60 epochs .
A detailed description of the setup , models , and data is provided in Appendix B .
We focus on sentence -level and documentlevel representations , and two language pairs : English ?
Estonian ( EN - ET ) and German ?
English ( DE -EN ) .
Dimensionality Reduction
We start by unsupervised dimensionality reduction using PCA to visualize domain placement .
We take the development set data , extract token embeddings from each model 's layer , and average them to obtain sentence representations .
Then we apply cosine- based PCA and t-SNE dimensionality reduction to the representations to visualize the data in a 2D space , and post factum color each data point ( sentence ) according to its corresponding domain .
We show the resulting visualizations in Figure 1 ( best viewed in color ) for ET -EN ( and in Figure 4 for t-SNE in the Appendix A , which mirrors the PCA result ) .
Figure 1 shows that NMT partitions the domains quite well at all encoder hidden layers and deep decoder layers .
Encoder layer 0 corresponds to the fixed embeddings , and the latent space is not well partitioned there yet ; however , as we go deeper into the network , the separation increases .
Layer 7 is the decoder 's embedding layer , and there the same logic applies .
While the encoder learns to partition the hidden space based on domains from scratch , the decoder has access to the encoder hidden states via encoder-decoder attention , which might simplify its task .
In summary , Figure 1 is our initial evidence that the NMT encoder places the domains separably .
Clustering
Our primary method , however , is unsupervised kmeans clustering .
We consider four data clustering setups : sentence - level XLM -R clusters , sentencelevel NMT clusters , document- level XLM -R clusters , and document- level NMT clusters .
The first one is the baseline clustering approach investigated by Aharoni and Goldberg ( 2020 ) while the remaining three are our original contributions .
Per-layer Clustering Purity Metric
In our analysis , we estimate how well the NMT model preserves domain-specific information in its internal text representations .
To do that , we measure the goodness - of - fit between unsupervised clusters and oracle domains .
Specifically , we follow Aharoni and Goldberg ( 2020 ) and use the clustering purity metric .
To compute clustering purity , we align domains and clusters by the highest overlap in numbers of sentences .
The number of overlapping data points for each cluster- domain pair gives us the number of ' correctly predicted ' examples .
Then , the sum of all ' correctly predicted ' examples divided by the total number of examples will be the clustering purity score .
Embedding and Clustering
We first take the concatenation of a small subset of sentences ( 3 k ) from each of the four domains and try to partition them into four clusters based on the representations from each layer of XLM -R and NMT Transformer .
We only use source sentences since we do not have targets at runtime in NMT .
Specifically , we follow the steps below for each layer of each of the two models : 1 . For each sentence in the dataset , we extract contextualized token embeddings from a layer of the model .
2 . We use the average of contextualized token embeddings as sentence representations .
3 . We apply k-means clustering to sentence representations to assign a cluster label to each sentence .
4 . We compute clustering purity for predicted labels and oracle domains .
We perform ten random restarts of k-means clustering , selecting the iteration with the smallest withincluster variance .
Results
Figure 2 shows per-layer clustering purity computed for sentence representations for XLM -R and two NMT Baseline checkpoints ( after the first epoch and after the 60th epoch of training ) .
Figure 2 shows that NMT surpasses the language model in its ability to rediscover domains .
About 3.5x higher performance at the encoder layers shows that the encoder is the part that learned to be very aware of the input domains ( in an unsupervised way ) .
Figure 2 also shows that the checkpoint saved after the 1st training epoch rediscovers clusters slightly better then 60th checkpoint .
However , this does not suggest that an NMT model should be trained for one epoch since the translation quality is suboptimal early on .
Instead , we assume that the model quickly learns domain-specific information ( perhaps due to the common lexical statistics ) and then slightly " moves away " towards a higher level of abstraction as training progresses .
This abstraction is necessary to successfully learn a task as complex as NMT .
Large-scale Clustering Next , we repeat the same steps for the entire training dataset and include a second language pair .
Specifically , we pick one of the best performing layers
We also extend our analysis to the documentlevel scenario .
Specifically , we average over sentence representations to get document embeddings and cluster - based on them .
Then , we assign the predicted label for each document to each sentence in that document .
2 Results
We present large-scale clustering confusion matrices in Figure 3 and clustering purity in Table 1 .
These show that sentence - level NMT is generally better than sentence - level XLM - R at rediscovering domains .
However , they both show quite modest results for both language pairs .
At the same time , document - level clusters are much better at rediscovering original domains .
The reason for that might be that sentence - level clustering largely relies on the more shallow information in the text .
For example , we observed that both sentence - level NMT and XLM -R produced a cluster responsible for extremely short sentences ( the average sentence length is about four tokens for these clusters ) .
On the other hand , document - level representations factor out these shallow stylistic features by averaging over sentence representations .
Therefore , the models are inclined to cluster by topics .
An alternative explanation is that domainspecific lexical statistics , which not all sentences might preserve , get more robust as we average sentence embeddings to get a document embedding .
Even though sentence - level clustering maintains a general idea about oracle domains , they split sentences into clusters quite freely .
For example , JRC - Acquis consistently gets mixed with Europarl , which both belong to legal domains .
We can see it from NMT SENT for both language pairs .
For documents , the rediscovery trend is common and pronounced for both language pairs , and separation is generally consistent between train and test .
However , for EN -ET XLM -R DOC we can observe that EMEA and JRC - Acquis got split between two clusters in the training set .
Considering that we perform ten random k-means restarts and choose the best iteration , this suggests that XLM - R may become inconsistent ( as a source of sentence representations on the document level ) in some cases .
Practical Application
Our analysis in Section 3 revealed that NMT models represent domains in their embedding space separately , similarly to what pre-trained language models do ( Aharoni and Goldberg , 2020 ) .
We demonstrated that simple clustering on NMT representations allows recovering original data domains to a large degree .
This section proposes to utilize this finding to improve an existing framework of automatic domain generation for NMT .
In this framework , related work first clusters the training data using representations from an external encoder , and then the base - line NMT model is adapted ( fine-tuned ) on each cluster separately .
We propose to re-use the NMT baseline itself as the encoder in this framework .
Representations extracted from translation Transformers are specific to the task of translation .
We hypothesise that it might result in clusters most suitable for downstream translation tasks like finetuning to specific domains / clusters .
Moreover , an advantage of our scenario is that we cluster the same data ( with our NMT model ) that we use for NMT model training .
It is a frequent multi-domain NMT setup , where multiple target domains are available in training .
In the pre-trained language model setup , the data will be more outof-domain , despite the model 's generality .
Existing Framework ( Background )
In this subsection we describe an existing framework which uses automatic domains ( clusters ) to perform NMT domain adaptation ( Tars and Fishel , 2018 ) .
Recent work ( Currey et al. , 2020 ) employs large pretrained language models as part of the framework .
It consists of several steps .
In step 1.1 , we begin with a single heterogeneous dataset ( " Original Dataset " ) and train a baseline NMT model on it .
At the same time ( step 1.2 ) , we pass this dataset through the external pre-trained XLM -R model to extract hidden sentence / document representations for the whole dataset .
In step 2 , we use the extracted sentence / document representations to train a k-means clustering model .
In step 3 , we use this k-means model to separate the original dataset into subdatasets corresponding to the clusters .
Lastly , we use the cluster-specific datasets to fine - tune the baseline NMT model from step 1.1 on each dataset separately , resulting in a set of specialized models .
We use the k-means model at runtime to determine which NMT model to use to translate a new sentence / document .
If we only use sentence clusters , the approach is equivalent to the one proposed by Currey et al . ( 2020 ) .
Refer to Figure 6 from Appendix
A for the illustration of the steps described above .
Improved Framework ( Ours )
In this subsection we describe our modification to the existing automatic domains pipeline presented in Section 4.1 .
We propose reusing an NMT baseline to produce sentence representations for the clustering step instead of using an external encoder .
Specifically , in step 1 , we train a baseline NMT model just like in the existing framework .
However , we found we can omit using the XLM -R model ( step 1.2 ) .
Instead , to extract sentence / document representations for step 2 , we reuse the trained NMT baseline .
The rest of the pipeline remains the same .
Figure 7 ( Appendix A ) illustrates the updated framework .
Moreover , to produce clusters in both frameworks , we additionally study text representations on the level of documents .
Experiments
In this section we perform an extensive experimental study comparing performance of the existing automatic domains framework ( Section 4.1 ) with our proposed version ( Section 4.2 ) .
We experiment with both sentence - level and document-level representations as a basis for k-means algorithm on three language pairs and two data scenarios .
We first train baseline Transformer NMT models on concatenated data from all domains ( same baseline as in Section 3 ) and then cluster the training , development , and test data using either this same baseline or XLM -R.
Next , we fine- tune 3 our baseline models to the different obtained data partitions ( clusters ) and compare the translation quality of resulting fine-tuned ( adapted ) models .
Setup
We explore two data scenarios .
First , we perform experiments on a mixture of distinct corpora .
For these experiments , we reuse the data and concat baseline NMT model ( Transformer- base ) described in Section 3 ( EN - ET and DE -EN ) .
In this setting , we can compare the performance of models fine-tuned to automatically discovered domains to that of oracle models ( fine - tuned using known domains / datasets ) .
We also randomly partition the data ( into equal parts ) and fine- tune the baseline models to them to get our lower bound estimates .
Second , we explore a scenario with a single corpus , which is highly heterogenous , and thus may contain multiple domains which are unknown .
In this setting , we use the ParaCrawl ( Espl ?
et al. , 2019 ) parallel corpus 4 , which consists of diverse documents crawled from the web .
We use three language pairs : English ?
Estonian ( EN - ET ) , German?
English ( DE -EN ) , and English ?
Czech ( EN - CS ) .
We use ?3 M sentence pairs for all languages for training , and ?3,000 sentence - pairs for development and testing .
The exact experimental setup with data sizes , training and preprocessing details can be found in in Appendix B. For our concat baselines we follow the setup from Section 3.1 ( described in more detail in Appendix B ) .
In the mixture of corpora experiments , baseline fine- tuning is performed for 50 epochs , and in the single heterogenous corpus experiments for 25 epochs ( fine -tuning hyperparameters can be found in Appendix B ) .
For comparison , we also continue training the baseline models for longer as suggested by Gururangan et al . ( 2020 ) ( concatcont ) .
We continue training for the same number of epochs fine - tuning is done for in the corresponding experiment .
For each of the models , we evaluate the checkpoint which shows the highest BLEU score on the particular model 's development set , and translate the test sets with beam size set to 5 .
We use the BLEU score ( Papineni et al. , 2002 ) , specifically , the sacreBLEU implementation ( Post , 2018 ) to assess the models ' translation performance .
To test for statistical significance , we use paired bootstrap resampling ( Koehn , 2004 ) .
Labelled Domain Mix Experiments
In this section we consider a scenario which can be practically interesting in cases where the data consists of several distinct domains with the labels unavailable or corrupted as in Currey et al . ( 2020 ) .
Moreover , it serves as an oracle experiment showing how well automatic domains perform compared to the golden labels .
This way we have a better idea what to expect when applying them to unlabeled data as in Section 5.3 .
Table 2 shows the results for DE?EN .
We see that , for all corpora except Europarl , at least one model of the two that are based on document - level clustering always manages to surpass the oracle performance obtained by fine-tuning to known domains , and on Europarl the document - level models perform comparably to oracle .
In most cases , document- level models show significantly better translation quality than XLM -R sentence - level models , which have been used in previous work , while NMT sentence - level models closely match the performance of XLM -R sentence ones .
When scores are averaged over all four domains , document clustering obtained from the NMT encoder is the overall winner .
Table 3 shows results for the EN ?ET language pair .
While fine-tuning on oracle domains yields an average improvement of 0.8 BLEU points over the baseline , fine -tuning on unsupervised document clusters obtained from the NMT encoder allows us to match that performance .
However , for the EMEA test set XLM -R sentence clusters turn out to be the most successful approach , showing significantly higher BLEU scores than all other automatic partitions and outperforming the oracle by 1.2 BLEU points , while document- level NMT clustering also manages to surpass the oracle performance , albeit slightly .
For OpenSubtitles and JRC - Acquis , oracle shows the highest overall scores , with document- level NMT clustering a close second , outperforming XLM -R sentence clustering by a noticeable margin .
For OpenSubtitles , however , none of the automatic domain approaches manage to improve the baseline performance ( and neither does continued training of the baseline ) , and even the oracle partition does not manage to do so by a statistically significant degree .
For Europarl , all automatic domain approaches yield comparable BLEU scores , with none being significantly better or worse than XLM -R sentence clusters .
Document - level XLM -R automatic domains have a low average score due to underperforming on the EMEA test set .
We see from Figure 3 test set is mostly translated by the model fine-tuned on cluster 1 , whose training set predominantly consists of Europarl data .
Cluster 0 , which sees the most EMEA examples during fine-tuning , is not used to translate the test set at all , as we see from Figure 3 .
Heterogeneous Corpus Experiments
In this subsection we present results for our method applied to the Paracrawl dataset , which constitutes a heterogeneous corpus of data crawled from the web with no training - time domain information known .
EN-ET
EN - CS & DE-EN
As separating the data into 8 clusters yields the highest BLEU score among all fine-tuning scenarios for EN?ET , we choose this number of clusters for experiments on other language pairs .
Table 5 shows the BLEU scores for EN?ET , EN?CS , and DE?EN models finetuned to automatic domains .
For EN - CS , only the NMT sentence - level clustering manages to outperform the baseline , noticeably surpassing all other automatic domain extraction methods as well .
For DE - EN , none of the approaches outperform the baseline model by a considerable margin .
Sentence - level clustering based on XLM -R performs comparably to the baseline .
Document - level NMT clustering shows a slightly lower score , but the difference is not statistically significant .
At the same time , document XLM -R and sentence NMT perform worse than sentence XLM -R .
Additional Exploration
While automatic domains demonstrate reasonable performance for EN -ET and EN - CS language pairs , DE -EN does not seem to benefit from either XLM - R or NMT - based clustering .
In this section we perform additional experiments with DE -EN data to see whether there are conditions under which automatic domains could be beneficial in this case .
Data Size and Number of Clusters First , we increase the training data size and vary the number of clusters .
Specifically , we use 10M parallel sentence pairs for training instead of 3 M , and partition the dataset into 4 and 12 clusters instead of 8 .
The resulting BLEU scores for DE -EN are shown in Table 6 .
We do not observe any significant improvement over the concat-cont baseline for any of the methods .
With the data separated into 12 clusters , sentence - level NMT clustering significantly outperforms sentence - level XLM -R , but still does not beat continued training of the baseline .
Model Size
It is also possible that NMT needs different model capacity for handling different language pairs , so we experiment with decreasing the model size .
We use the same number of layers , but decrease the width of the model ( 4 attention heads , embeddings of size 160 , dimension of the feed- forward layer 320 ) so that the total number of parameters decreases five-fold .
We compute NMT clusters based on the new , smaller baseline model .
Our motivation for this is to understand whether automatic domains are not useful for DE-EN ParaCrawl at all , or could aid a weaker baseline .
The results are shown in Table 7 .
The smaller baseline does benefit from adaptation to automatic domains ( clusters ) .
While NMT clusters are generated by a model which is 5 times as small , XLM -R and NMT show equivalent performance .
Discussion
Our analysis is implicit inductive evidence for the high degrees of domain-specific information in sentence and document NMT representations .
However , it is still open to what kind of information is preserved ( topical / stylistic / lexical ) .
For example , our approach could result in clusters by domain / dataset due to standard lexical statistics and not sentence semantics .
However , on the practical side , we show that adapting NMT to these types of clusters is just as good or better as to other possible types of clusters since it benefits the baseline performance .
Moreover , previous work that uses pre-trained language models to obtain the clusters is likely to suffer from the same issue .
Moreover , while XLM -R is a general - purpose encoder , NMT models are only that helpful for domains we train them on .
However , the data constitutes all domains of interest by definition for a multi-domain NMT ( the task we tackle ) .
Thus , NMT models are a perfect fit that simplifies and outperforms an existing approach .
Conclusion
In this work , we made a two -fold contribution .
The first is to the field of NMT interpretation and analysis .
We have shown that a baseline Transformer NMT encoder preserves enough domain-specific information distinguish between oracle domains in a mixed corpus without supervision .
We showed an evolution of this property across the Transformer layer using PCA and k-means clustering on the level of sentences and documents .
Comparison to XLM -R based clusters demonstrated that both sentence - level and document- level NMT clusters show higher cluster purity ( similarity to original text domains ) .
Next , we utilized our analysis insights to improve an existing practical cluster - based multidomain NMT approach ( Tars and Fishel , 2018 ; Currey et al. , 2020 ) .
In a setting with preset domains ( i.e. , available corpus / domain labels ) , tuning to NMT clusters on average matches or surpasses XLM -R clusters .
Additionally , NMT cluster - based tuning mostly matches the translation quality when tuning to original corpus labels , with some exceptions that we also analyze and explain .
Finally , in the case of a heterogeneous corpus ( ParaCrawl ) , the performance of fine- tuned NMT models depends on the number of clusters , language pairs , and other parameters .
We see significant improvement for EN - ET and EN - CS translation when comparing XLM -R and NMT - based clusters ( on both sentence and document levels ) .
For DE - EN , the domain tuning results depend on the NMT model 's capacity for learning each language pair 's translation .
A.2 Language Model XLM -R Base
Our model of choice from the family of BERT - like models is the Base version of the XLM -R ( Conneau et al. , 2020 ) .
It is single multilingual model covering about 100 languages , which is very useful when dealing with machine translation systems , where for different language pairs we may not have a separate monolingual BERT for each source language .
We choose XLM -R as opposed to the multilingual BERT ( Devlin et al. , 2019 ) since it is a more recent and better performing ( Hu et al. , 2020 ) model .
We choose the Base version because it is most compatible to our NMT baseline in terms of capacity .
