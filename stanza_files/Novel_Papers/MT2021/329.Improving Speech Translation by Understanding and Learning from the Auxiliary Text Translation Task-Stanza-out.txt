title
Improving Speech Translation by Understanding and Learning from the Auxiliary Text Translation Task
abstract
Pretraining and multitask learning are widely used to improve the speech to text translation performance .
In this study , we are interested in training a speech to text translation model along with an auxiliary text to text translation task .
We conduct a detailed analysis to understand the impact of the auxiliary task on the primary task within the multitask learning framework .
Our analysis confirms that multitask learning tends to generate similar decoder representations from different modalities and preserve more information from the pretrained text translation modules .
We observe minimal negative transfer effect between the two tasks and sharing more parameters is helpful to transfer knowledge from the text task to the speech task .
The analysis also reveals that the modality representation difference at the top decoder layers is still not negligible , and those layers are critical for the translation quality .
Inspired by these findings , we propose three methods to improve translation quality .
First , a parameter sharing and initialization strategy is proposed to enhance information sharing between the tasks .
Second , a novel attention - based regularization is proposed for the encoders and pulls the representations from different modalities closer .
Third , an online knowledge distillation is proposed to enhance the knowledge transfer from the text to the speech task .
Our experiments show that the proposed approach improves translation performance by more than 2 BLEU over a strong baseline and achieves state - of- theart results on the MUST -C English - German , English - French and English - Spanish language pairs .
Introduction End-to- end methods have achieved significant progress in speech to text translation ( ST ) and even surpassed the traditional pipeline - based methods in some applications ( Niehues et al. , 2019 ; Salesky and Black , 2020 ) .
However , the success of endto-end methods relies on large amounts of training data , which is quite expensive to obtain and relatively small in practice .
Building ST systems from pretrained models with multitask learning ( MTL ) is widely used to overcome the limited training data issue ( Weiss et al. , 2017 ; Anastasopoulos and Chiang , 2018 ; Bahar et al. , 2019 ; Indurthi et al. , 2020 ; Wang et al. , 2020 b ; Li et al. , 2020 ) .
Nevertheless , little prior work has been devoted to understanding the interactions between different tasks .
Standley et al. ( 2020 ) conduct an empirical study on computer vision tasks for MTL .
They find many " assumptions " for MTL may not be held for specific applications .
For example , " similar " tasks do not necessarily train better together .
In this study , we focus on training the ST model along with an auxiliary text to text machine translation ( MT ) task .
We are interested in the task interactions with different modalities and in improving the primary ST task with the help from the auxiliary MT task .
The model is initialized with pretrained modules from automatic speech recognition ( ASR ) and MT .
Two types of analysis are conducted on the fine-tuned multitask learned models .
The first focuses on the model variation by comparing fine-tuned models with pretrained models for different tasks .
The second aims to measure internal representation differences due to different modalities .
The analysis leads to three main findings .
First , the analysis confirms that MTL tends to generate similar model representations for different input modalities and preserves more information from the pretrained MT modules .
Second , we do not observe significant negative transfer effect from the MT task to the corresponding ST task .
Sharing more parameters is helpful to transfer knowledge to the primary ST task .
Finally , the top layers in the ST decoder are more critical to the translation performance and they are also more sensitive to the modality difference .
The model representations from different modalities demonstrate larger difference for the top layers in our analysis .
Inspired by these findings , we propose three techniques to enhance the performance of the primary ST task .
First , we propose to maximize parameter sharing between the ST and MT tasks , i.e. the entire decoder and the top encoder layers .
Those shared parameters are initialized with the corresponding MT models .
Second , a cross-attentive regularization is introduced for the encoders .
It minimizes the L2 distance between two reconstructed encoder output sequences and encourages the encoder outputs from different modalities to be closer to each other .
Finally , an online knowledge distillation learning is introduced for MTL in order to enhance knowledge transfer from the MT to the ST task .
Our contributions are summarized as follows : 1 . A detailed analysis is conducted on the interaction between the primary ST task and the auxiliary MT task .
2 . A parameter sharing and initialization strategy are proposed to encourage information sharing between tasks .
3 . Cross-attentive regularization and online knowledge distillation are proposed to reduce the model representation difference between different modalities and enhance the knowledge transfer from the MT task to the ST task .
4 . Our system achieves state of the art results on the MUST -C English - German ( EN - DE ) , English - French ( EN - FR ) and English -Spanish ( EN - ES ) language pairs , with 2 or more BLEU gains over strong baselines .
Weiss et al. ( 2017 ) explore different multitask training strategies for ST , and they find the oneto-many strategy , in which an encoder is shared between the ST and ASR tasks , is more effective .
Anastasopoulos and Chiang ( 2018 ) further extend it to a triangle structure by concatenating ASR and ST models .
Bahar et al. ( 2019 ) compare different multitask strategies for the ST task , and they confirm many - to - one strategy , in which MT and ST are trained together and the decoder is shared between two tasks , is effective if extra bitext data is used .
In this work , we carefully study the relation between co-trained tasks in the many - to - one strategy , and the analysis results guide us to propose three techniques to learn more from the auxiliary MT task and enhance the ST performance further .
Model analysis Chatterji et al. ( 2020 ) propose criticality analysis to measure the importance of different modules from the trained model .
Parameters in the selected module or layer are partially rolled back to the initial values , and the module criticality or importance is measured by the performance drop after modification .
Larger performance drops indicate a more critical module .
Inspired by their work , we extend it to the analysis on the jointly trained models with different pretrained modules and schemes .
Raghu et al . ( 2017 ) ; Morcos et al. ( 2018 ) propose to employ canonical correlation to measure the similarity between different models given the same input .
We extend their work to study a model with inputs from different modalities .
Related Work
Methods
The proposed ST system is co-trained with the MT task as depicted in Figure 1 .
The modules in the primary ST task are connected with dark gray lines and the auxiliary MT task is illustrated with light gray lines .
The parameters in the blue modules are shared between the two tasks .
During inference with speech input , only modules related to the ST task are used .
The model has two encoders , a text encoder and a speech encoder , to take text and speech input respectively .
The decoder is shared between the two tasks .
To encourage knowledge sharing between the two tasks , the top encoder layers are also shared .
The parameters of the shared modules are initialized with a pretrained MT model .
A novel crossattentive regularization is proposed to reduce the distance between encoder outputs from different input modalities .
We also introduce a novel online knowledge distillation method where the output from the auxiliary MT task is used to guide the ST model training .
The cross-attentive regularization and online knowledge distillation are illustrated as orange modules in Figure 1 and the details are presented in the following two subsections .
Cross-Attentive Regularization
The cross-attentive regularization ( CAR ) is proposed to increase the similarity between the text encoder outputs and their corresponding speech encoder outputs .
Hence , the performance of the more difficult ST task can be improved by learning from the relatively easier MT task .
Encoder output sequences from different modalities can not be compared directly since they have different lengths .
In CAR , the two reconstructed sequences are calculated from the text output sequence via self-attention or the speech output sequence via cross attention over the text output sequence .
The two reconstructed sequences have the same length and the distance is simply measured as the L2 distance between the two sequences .
Formally , we denote a speech to text translation training sample as a triplet o = ( X s , x t , y ) .
X s ?
R ds?N , x t ?
R M , and y ?
R K are the speech feature input , text token input and target text output respectively .
N , M and K are the corresponding sequence lengths .
Assume H s = ( h s 1 , h s 2 , ? ? ? , h s N ) and H t = ( h t 1 , h t 2 , ? ? ? , h t M ) , h s n , h t m ?
R d h are outputs from the speech encoder and text encoder respectively , where d h is the dimension of the output states .
A similarity matrix S ? R N ?M is defined as the cosine distance between the tensors in the two sequences : s i , j = ( h s i ) ? h t j ||h s i || 2 ||h t j || 2 ( 1 ) where s i , j is the ith row and jth column component in S .
The text encoder outputs H t are reconstructed through the speech encoder outputs H s and similarity matrix S as below .
H s?t = H s ? softmax ( S ) ( 2 ) H t?t , the reconstruction of H t from itself , can be computed similarly via self-attention .
CAR is defined as the L2 distance between the two reconstruction encoder outputs : L CAR ( ? s ) = 1 M H s?t ? sg [ H t?t ] 2 ( 3 ) where sg [ ? ] is the stop-gradient operator and ? s are the ST model parameters .
By optimizing the model with CAR , the speech encoder is encouraged to learn from more accurate text encoder and generates similar encoder outputs after reconstruction .
CAR is inspired by the attention mechanism between the encoder and decoder where the decoder states are reconstructed through encoder output states via the attention mechanism .
Online Knowledge Distillation Knowledge distillation ( KD ) is widely used for model compression ( Hinton et al. , 2015 ; Kim and Rush , 2016 ) where a smaller student network is trained to mimic the original teacher network by minimizing the loss between the student and teacher outputs .
The ST task is considerably more difficult than the MT task since the speech input is noisier and more ambiguous than the text input .
The accuracy of the MT model is usually much higher than the corresponding ST model .
Knowledge distillation from a well trained MT model to a ST model has been proved to be an effective way to improve the ST performance ( Liu et al. , 2019 b ; Gaido et al. , 2020 ) .
In this work , we extend knowledge distillation to the MTL framework where both ST and MT are fine-tuned simultaneously with shared parameters .
Concretely , we assume an MTL model learns from a data set D with target vocabulary size | V |.
The training criterion is to minimize negative log likelihood ( NLL ) for each example o = ( X s , x t , y ) ?
D from the training data : L N LL (? s ) = ?
D o K k=1 | V | v=1 ?( y k = v ) log p(y k = v|y < k , X s , ? s ) ( 4 ) where ?(? ) is the indicator function and p the distribution from the ST model ( parameterized by ? s ) .
Assume the probability distribution for y k given text input x t and MT model ?
t is q(y k = v|y < k , x t , ? t ) , the knowledge distillation loss is defined as minimizing the cross-entropy with the MT 's probability distribution L KD ( ? s ) = ?
D o K k=1 | V | v=1 q(y k = v|y < k , x t , ? t ) log p(y k = v|y < k , X s , ? s ) ( 5 )
The overall loss is the combination of crossattentive regularization , knowledge distillation loss , negative log likelihood loss for both ST and MT , as follows : L (? s , ? t ) = ?L N LL (? s ) + ( 1 ? ?) L KD (? s ) +?L CAR ( ? s ) + L N LL ( ? t ) ( 6 ) where ? and ? are predefined hyper-parameters .
Experimental Setup Experiments are conducted on three MUST -C ( Gangi et al. , 2019a ) language pairs : EN -DE , EN -ES and EN - FR .
The models are developed and analyzed on the dev set and the final results are reported on the tst -COMMON set .
We use WMT parallel data from different years , 2013 for Spanish , 2014 for German , and 2016 for French , as extra text training corpus for MTL .
Case-sensitive detokenized BLEU is reported by SACREBLEU with default options ( Post , 2018 ) .
We use the " T- Md " configuration from ( Wang et al. , 2020a ) in all experiments .
The speech encoder has 12 transformer layers while the decoder is with 6 transformer layers .
For the MTL model , the text encoder has 6 transformer layers .
The transformer layer has an input embedding size of 512 and middle layer dimension 2048 .
We share parameters of all 6 text encoder transformer layers with the top 6 transformer layers in the speech encoder , hence both encoders use the same modules to generate the encoder outputs .
The Adam optimizer ( Kingma and Ba , 2014 ) with a learning rate 0.002 is employed in the experiments .
Label smoothing and dropout rate are both set to 0.1 .
We choose ? = 0.8 and ? = 0.02 in Equation 6 through grid search ( [ 0.1 , 1.0 ] for ? and [ 0.01 , 0.05 ] for ? ) .
Input speech is represented as 80D log melfilterbank coefficients computed every 10 ms with a 25 ms window .
Global channel mean and variance normalization is applied .
The SpecAugment ( Park et al. , 2019 ) data augmentation with the LB policy is applied in all experiments .
The input text tokens are converted into their corresponding pronunciation form as phoneme sequences ( Tang et al. , 2021 ; Renduchintala et al. , 2018 ) .
The grapheme to phoneme conversion is done through the " g2 p en " python package ( Lee and Kim , 2018 ) .
The leading phoneme in a word is appended with an extra " " to mark word boundaries .
In total , the vocabulary size for the input phonemes is 134 .
The target vocabulary consists of 10 k " unigram " subword units learned by SentencePiece ( Kudo and Richardson , 2018 ) with full character coverage of all training text data .
All ST or jointly trained models are initialized with pretrained ASR and MT modules .
The ASR model is trained on the same English speech training data from MUST - C with the " T- Md " configuration too .
The pretrained MT models are trained for each language pair with the aforementioned WMT data .
The MT encoder and decoder configurations are the same as the text encoder and decoder in the MTL model mentioned above .
The models are fine-tuned to 100 epochs using 8 V100 GPUs for approximate one day .
The batch size is 10,000 frames for speech to text translation samples and 10,000 tokens for parallel text samples per GPU .
The models are trained with FAIRSEQ ( Ott et al. , 2019 ; Wang et al. , 2020a ) .
The last 10 checkpoints are averaged for inference with beam size 5 . 1 .
MTL Analysis
Model Variation
We extend Chatterji et al. ( 2020 ) 's work to analyze a MTL model .
We initialize models with different pretrained modules and fine-tune them for ST and MT tasks within the MTL framework .
The pretrained modules come from ASR and MT tasks .
Criticality analysis is conducted on the ST model after the MTL fine -tuning step .
The parameters in the selected modules are interpolated with corresponding parameters in the pretrained modules .
MUST -C EN - DE dev set is used for BLEU computation .
With different interpolation ratios , we obtain different BLEU scores .
The BLEU difference comes from two sources .
The first one comes from the selected module itself .
If the module is important and sensitive , very small perturbation could result in a nontrivial BLEU difference as ( Chatterji et al. , 2020 ) .
Another source of difference is that if the selected module changes significantly to adapt to the ST task , rewinding the parameters back to the initial task may lead to a substantial decrease in BLEU .
We attempt to quantify the extent of the degradation from the second source , which can be indicative of the model variation from the pretrained task to the ST task .
This is accomplished by comparing the BLEU differences for the same module but using different initialization and training schemes .
Table 1 lists models initialized with different pretrained modules .
" ST " designates a ST model trained with the single ST task , " JT " corresponds to a ST model trained with the primary ST task and auxiliary MT task together .
" JT - S- ASR " and " JT - S- MT " are another two jointly trained models but 1
The source code will be released at https://github.com/pytorch/fairseq/tree/master/examples/speech text joint to text In " JT - S- MT " , the top 6 shared encoder layers are initialized with the pretrained MT encoder .
We illustrate their BLEU difference trajectories with dotted lines in Figure 5 ( a ) so they can be easily distinguished from other layers initialized from the ASR encoder .
The BLEU difference for the top encoder layer is down from 20.2 to 17.6 when the parameters are replaced with the ones in the pretrained ASR encoder .
It is further reduced to 10.0 if the shared layers are initialized with MT encoder layers .
The BLEU differences in the decoder layers are mixed .
The performance of " JT - S- ASR " degrades quickly in the criticality test for the top decoder layer , while " JT - S - MT performs similarly in the test as " JT " decoder .
We argue that the top layers in the finetuned ST encoder might be closer to the MT encoder than the ASR encoder .
It preserves more information from the MT task by sharing more parameters between two tasks and initializing them with pretrained MT modules .
This is a desirable property since we want to transfer more knowledge from the text corpus to the ST task .
Modality Variation
The jointly trained model takes input from two modalities , i.e. text or speech , and we are interested in the model internal representation difference for paired inputs .
Given text target y , we extract the decoder hidden state representations for the corresponding text input x t and speech input X s .
The decoder representation difference solely comes from different input modalities .
The difference is quantified by the correlation coefficient over all samples evaluated between two input modalities : r s,t ( l , d ) = ? st ( l , d ) ? s ( l , d ) ? t ( l , d ) ( 7 ) where ? z ( l , d ) , z ? [ s , t] is the standard deviations of decoder hidden states at layer l for component d in all samples , and ? st ( l , d ) is the corresponding covariance .
The layer - wise correlation coefficient is the average of all components : r s, t ( l ) = 1 D d r s , t ( l , d ) ( 8 ) Figure 6 depicts the correlation coefficient between speech input and text input for each decoder layer in the model " JT - S- MT " .
The x-axis is the number of training epochs and the y-axis represents the correlation coefficient for each layer .
There are two observations .
First , the correlation coefficients become larger and close to " 1.0 " as training converges .
Second , the higher the layer , the smaller the correlation coefficient .
We hypothesize that the inputs to the lower layers are dominated by the decoder text embeddings , which are the same for both modalities , and the inputs to the higher layers would contain more information from the encoder outputs , which result in the decoder internal representation differences .
The analysis shows a well trained MTL decoder has similar representations for paired text and speech input .
However , the top decoder layers still have nontrivial representation differences due to different modalities .
6 Experimental Results
Main Results
The main ST results are presented in Table 2 .
The first three rows are results from the literature .
" ST " and " JT " are models initialized as Table 1 and studied in section 5 .
The last row ( " JT Proposed " ) presents results from the proposed system , in which the top encoder layers and decoder are shared , and the models are optimized following Equation 6 . The second column ( " pars ( m ) " ) lists the number of parameters used during inference .
From Table 2 , our " ST " baseline is comparable to the previously reported results except ( " JT " v.s. " JT - S - ASR " ) .
Initializing the shared encoder layers with pretrained MT modules leads to BLEU increase for two of the three evaluated translation pairs ( " JT - S- ASR " v.s. " JT - S - MT " ) .
For EN - FR , the degradation is minimal ( - 0.1 BLEU ) .
Ablation Overall , sharing top encoder layers can increase BLEU by 0.2?0.7 ( " JT - S- MT " v.s. " JT " ) .
CAR further improves the translation by another 0.3?0.9 BLEU .
The best results are achieved by applying the shared top encoder layers , CAR and online KD together .
They are about 2.9 + BLEU better than the single task based system ( " ST " ) and achieve 2 + BLEU increase on top of the strong vanilla joint training system ( " JT " ) .
Figure 7 demonstrates the model variation for the proposed system on the MUST -C EN - DE dev set .
Compared with Figure 5 , the decoder shows less degradation during the criticality test and it shows CAR and online KD help to preserve more information from the MT task .
Figure 8 shows the corresponding correlation coefficients between paired text and speech input from the top decoder
Task Dependent Components
In MLT , many works ( Maninis et al. , 2019 ; Liu et al. , 2019a ; Zhang et al. , 2020 ; Pfeiffer et al. , 2020 ) employ task - dependent components to alleviate the negative transfer effect .
In Table 4 , we compare the " JT - S- MT " model with two variants using different task - dependent components .
The first one ( " JT - S- MT + Adapter " ) ( Bapna et al. , 2019 ) adds an extra adapter module on the top of the speech encoder .
Hence , the speech encoder outputs , which are generated from shared encoder layers , are further processed to reduce the difference between speech input and text input .
The adapter module consists of a linear layer and layer normalization layer .
The second variant ( " JT - S- MT + Dedicated Attention " ) ( Blackwood et al. , 2018 ) introduces dedicated decoder modules for different tasks .
Attention layers between encoder and decoder , and the layer normalization modules are not shared between the ST and MT tasks .
It gives the decoder more flexibility to handle information from different modalities .
The results show the extra adapter layer does n't bring gain while the task dependent attention module actually makes the performance worse .
It indicates that the negative transfer effect is not significant in this study and adding extra task - dependent components might not be necessary .
Impact on the MT Task
As shown in Table 2 , training ST models with an auxiliary MT task improves the translation quality substantially .
It may be interesting to examine the impact on the auxiliary task itself .
We evaluate the MT model jointly trained with the ST task .
Results are shown in Table 5 . " ST ( JT Proposed ) " in the first row corresponds to the best results obtained for the ST task .
The detailed experimental setup is described in Appendix A. For reference , we also include the MT evaluation results from MUST -C ( Gangi et al. , 2019a ) in the second row .
All MT models ( in the last 4 rows ) take phoneme sequences as input instead of SentencePiece .
" MT " ( row 3 ) shows the results from pretrained MT models on WMT .
In the " MT ( Tuned ) " row , the MT models pretrained on WMT are fine-tuned on the MUST - C datasets .
The large improvements clearly show a domain mismatch between WMT and MUST -C .
The MT models trained with WMT data are improved after fine-tuning , and they are comparable with the ones reported in ( Gangi et al. , 2019a ) , though the input token is in pronunciation form , which is more ambiguous than the corresponding SentencePiece unit .
" MT ( JT ) " and " MT ( JT Proposed ) " are results from the co-trained MT models in " JT " and " JT Proposed " respectively .
After fine- tuning using both MuST - C ( speech and text ) and WMT ( text only ) training data , the auxiliary MT models perform better than the corresponding ST models .
The proposed techniques further improve the co-trained MT models by 0.7?1.6 BLEU .
While this is a surprising result , we note that the dedicated MT models may be improved with better hyperparameter tuning .
In conclusion , the results show the proposed methods are effective to unify two tasks into one model with minimal negative transfer effect .
Conclusions
In this study , we focus on understanding the interactions between the ST and MT tasks under the MTL framework , and on boosting the performance of the primary ST model with the auxiliary MT task .
Two types of analysis on model variation and modality variation , are conducted on the MTL models .
The analysis demonstrates MTL helps to preserve information from the MT task and generates similar model representations for different modalities .
We observe a minimal negative transfer effect between the two tasks .
Sharing more parameters can further boost the information transfer from the MT task to the ST model .
The analysis also reveals that the model representation difference due to modality difference is nontrivial , especially for the top decoder layers , which are critical for the translation performance .
Inspired by the findings , we propose three techniques to increase knowledge transfer from the MT task to the ST task .
These techniques include parameter sharing and initialization strategy to improve the information sharing between tasks , CAR and online KD to encourage the ST system to learn more from the auxiliary MT task and then generate similar model representations from different modalities .
Our results show that the proposed methods improve translation performance and achieve state - of - the - art results on three MUST - C language pairs .
Figure 1 : 1 Figure 1 : Joint Training framework .
The speech to text translation task is depicted as dark gray line , text to text translation task is illustrated as light gray line .
The parameters in blue modules are shared between two tasks .
Figure 2 : 2 Figure 2 : Criticality analysis for the " ST " model .
Figure 3 : 3 Figure 3 : Criticality analysis for the " JT " model .
Figure 4 : 4 Figure 4 : Criticality analysis for the " JT - S- ASR " model .
The shared encoder layers are initialized with the layers from the ASR encoder .
( a) JT -S-MT Enc. ( b) JT -S-MT Dec .
Figure 5 : 5 Figure 5 : Criticality analysis for the " JT - S- MT " model .
The shared encoder layers are initialized with the layers from the MT encoder .
Figure 6 : 6 Figure 6 : Comparison of decoder layers correlation coefficients between text and speech input ( " JT - S- MT " ) .
BLEU on three language pairs in the MuST - C tst-COMMON datasets .
, who use a much larger model and additional weakly supervised speech training data .
As expected , the vanilla joint training baseline ( " JT " ) outperforms the " ST " baseline with the help of extra bitext training data .
Finally , the proposed joint training model ( " JT Proposed " ) achieves 2.0?2.7 BLEU gains over the strong joint training baseline ( " JT " ) .
Figure 7 : 7 Figure 7 : Criticality analysis for " JT Proposed " .
Figure 8 : 8 Figure 8 : Correlation coefficient for the top decoder layers ( epoch 100 ) .
Table 1 : 1 The model parameters are updated every 4 batches .
Speech training samples and text input samples are used to update the model alternatively .
Model initialization schemes Model Encoder Configuration Speech Text Shared ST ASR None None JT ASR MT None JT-S-ASR ASR MT ASR JT-S-MT ASR MT MT
Table 3 3 breaks down the performance gains into individual components / changes .
Sharing encoder layers improves the quality for all three language pairs EN - DE EN-ES EN -FR JT 24.1 29.0 35.1 JT-S-ASR 24.4 29.4 35.4 JT -S-MT 24.7 29.7 35.3 + CAR 25.0 30.4 36.2 + CAR + KD 26.8 31.0 37.4
Table 3 : 3 Ablation study .
Table 5 : 5 Comparison between ST and MT. EN -DE EN-ES EN -FR ST ( JT Proposed ) 26.8 31.0 37.4 MT ( Gangi et al. , 2019a ) 28.1 34.2 42.2 MT 25.4 27.7 33.5 MT ( Tuned ) 29.6 34.3 41.4 MT ( JT ) 28.9 33.9 41.6 MT ( JT Proposed ) 30.5 34.7 42.3
