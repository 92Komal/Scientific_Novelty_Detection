title
Comparing Feature-Engineering and Feature-Learning Approaches for Multilingual Translationese Classification
abstract
Traditional hand -crafted linguisticallyinformed features have often been used for distinguishing between translated and original non-translated texts .
By contrast , to date , neural architectures without manual feature engineering have been less explored for this task .
In this work , we ( i ) compare the traditional feature - engineering - based approach to the feature - learning - based one and ( ii ) analyse the neural architectures in order to investigate how well the hand- crafted features explain the variance in the neural models ' predictions .
We use pre-trained neural word embeddings , as well as several end-to - end neural architectures in both monolingual and multilingual settings and compare them to feature - engineering - based SVM classifiers .
We show that ( i ) neural architectures outperform other approaches by more than 20 accuracy points , with the BERT - based model performing the best in both the monolingual and multilingual settings ; ( ii ) while many individual hand -crafted translationese features correlate with neural model predictions , feature importance analysis shows that the most important features for neural and classical architectures differ ; and ( iii ) our multilingual experiments provide empirical evidence for translationese universals across languages .
Introduction
Texts originally written in a language exhibit properties that distinguish them from texts that are the result of a translation into the same language .
These properties are referred to as translationese ( Gellerstam , 1986 ) .
Earlier studies have shown that using various hand -crafted features for supervised learning can be effective for translationese classification ( Baroni and Bernardini , 2005 ; Volansky et al. , 2015 ; Rubino et al. , 2016 ) .
However , this approach has a number of limitations .
Firstly , * Equal contribution .
manually designed features may be partial and nonexhaustive in a sense that they are based on our linguistic intuitions , and thus may not be guaranteed to capture all discriminative characteristics of the input data seen during training .
Other limitations are related to the difficulties in obtaining linguistic annotation tools ( e.g. , parsers , taggers , etc. ) for some languages , reliance on n-gram counts , limited contexts , corpus specific characteristics , among others .
In this work , we compare a standard approach based on hand -crafted features with automatic feature learning based on data , task and learner without prior linguistic assumptions .
Moreover , most previous approaches have focused on classifying translationese in the monolingual setting , i.e. translations come from one or multiple source languages , but the language on which to perform the classification is always the same .
To the best of our knowledge , the multilingual setting with multiple source and target languages has not been explored yet .
If translationese features are language - independent or shared among languages , multilingual translationese classification experiments would show the effect .
We perform binary translationese classification not only in mono- , but also in multilingual settings to empirically verify the existence of translationese universals throughout different source and target languages .
In our work we investigate : ( i ) How automatic neural feature learning approaches to translationese classification compare to classical feature - engineering - based approaches on the same data .
To do this , we use pre-trained embeddings as well as several end-to - end neural architectures .
( ii )
Whether it is possible to effectively detect translationese in multilingual multi-source data , and how it compares to detecting translation in monolingual and single -source data in different languages .
( iii )
Whether a ) translationese features learned in one setting can be useful in a different setting and b ) the overhead of training separate monolingual models can be reduced by either multi-source monolingual models for a given target language or even better , a multilingual model .
For this we perform cross -data evaluation .
( iv ) Whether variation observed in predictions of neural models can be explained by linguistically inspired hand -crafted features .
We perform linear regression experiments to study the correlation between hand -crafted features and predictions of representation learning models as a starting point for investigating neural models which do not lend themselves easily to full explainability .
We show that : ? representation - learning approaches outperform hand -crafted feature -selection methods for translationese classification , with BERT giving the highest accuracy , ? it is possible to classify translationese in the multilingual data , but models trained on monolingual single -source data generally yield better performance than models trained on multisource and multilingual data , ? in contrast to hand -crafted feature - based models , neural models perform relatively well on different datasets ( cross-data evaluation ) , and single - source can , to a reasonable extent , be substituted by multi-source mono- and multilingual models , ? many traditional hand -crafted translationese features exhibit significant correlation with the predictions of the neural models .
However , a feature importance analysis shows that the most important features for neural networks and for classical architectures differ .
The paper is organized as follows .
Section 2 describes related work .
Section 3 introduces the architectures used in our study .
Section 4 discusses the data and presents the main classification results .
We perform cross-data evaluations in Section 5 and analyze feature importance and correlation in Section 6 .
Finally , we summarize and draw conclusions in Section 7 .
Related Work Recent work on translationese , both on humanand machine - translated texts , explores topics ranging from translationese characterization ( Volansky et al. , 2015 ; Bogoychev and Sennrich , 2019 ; Bizzoni et al. , 2020 ) to unsupervised classification , to exploring insights into the structure of language typologies with respect to different translationese properties ( Rabinovich et al. , 2017 ; Bjerva et al. , 2019 ; , to the effects on downstream tasks such as machine translation ( Stymne , 2017 ; Toral et al. , 2018 ; Zhang and Toral , 2019 ; Freitag et al. , 2019 ; Edunov et al. , 2020 ; Riley et al. , 2020 ; Graham et al. , 2020 ) , to translationese data collection Nisioi et al. , 2016 ; Amponsah - Kaakyire et al. , 2021 ) .
Traditional translationese classification approaches rely on manually designed features , such as n-gram frequencies on tokens , part- of-speech ( POS ) tags or lemmas ( Baroni and Bernardini , 2005 ; van Halteren , 2008 ; Kurokawa et al. , 2009 ) , function word frequencies ( Koppel and Ordan , 2011 ; Tolochinsky et al. , 2018 ) , character - level features ( Popescu , 2011 ; Avner et al. , 2016 ) , surface and lexical features ( Ilisei et al. , 2010 ; Volansky et al. , 2015 ) , syntactic features ( Ilisei et al. , 2010 ; Rubino et al. , 2016 ) , morpheme - based features ( Avner et al. , 2016 ; Volansky et al. , 2015 ) , information - density - based features ( Rubino et al. , 2016 ) , etc .
By contrast , to date neural approaches to translationese ( Bjerva et al. , 2019 ; have received less attention .
While Bjerva et al. ( 2019 ) have used learned language representations to show that the distance in the representational space reflects language phylogeny , Dutta Chowdhury et al . ( , 2021 use divergence from isomorphism between embedding spaces to reconstruct phylogenetic trees from translationese data .
Sominsky and Wintner ( 2019 ) train a BiLSTM for translation direction identification and report accuracy up to 81.0 % on Europarl data .
Architectures 3.1 Feature-Selection - Based Classification ( Handcr . +SVM )
We employ the INFODENS toolkit ( Taie et al. , 2018 ) to extract hand -crafted features to train and evaluate a classifier .
We use a support vector ma-chine classifier ( SVM ) with linear kernel , and fit the hyperparameter C on the validation set .
For the choice of features , we replicate the setup from ( Amponsah - Kaakyire et al. , 2021 ) , using a 108 dimensional feature vector , inspired by the feature set described in ( Rubino et al. , 2016 ) .
In particular , we use : 1 . surface features : average word length , syllable ratio , paragraph length .
These surface features can be connected to the simplification hypothesis ( Ilisei et al. , 2010 ; Volansky et al. , 2015 ) , as it is assumed that translations contain simpler shorter words than original texts .
2 . lexical features : lexical density , type-token ratio .
These lexical features can also be linked to the simplification hypothesis , due to the assumption that original texts have richer vocabulary than translated ones and contain a higher proportion of content words ( Laviosa , 1998 ; Baker et al. , 1993 ) . 3 . unigram bag-of - PoS : These features correspond to the source interference ( shiningthrough ) hypothesis ( Volansky et al. , 2015 ) , as POS n-grams reflect grammatical structure , which might be altered in translations due to the influence of the source language grammar .
4 . language modelling features : log probabilities and perplexities with and without considering the end-of-sentence token , according to forward and backward n-gram language models ( n ? [ 1 ; 5 ] ) built on tokens and POS tags .
It is hypothesized that the perplexity of translated texts may be increased because of simplification , explicitation and interference ( Rubino et al. , 2016 ) .
n-gram frequency distribution features : percentages of n-grams in the paragraph occurring in each quartile ( n ? [ 1 ; 5 ] ) .
This feature could be linked to the normalization hypothesis , according to which translated texts are expected to contain more collocations , i.e. high -frequency n-grams ( Toury , 1980 ; Kenny , 2001 ) .
In our experiments , language models and ngram frequency distributions are built on the training set .
The n-gram language models are estimated with SRILM ( Stolcke , 2002 ) and SpaCy 1 is used 1 https://spacy.io/ for POS - tagging .
Features are scaled by their maximum absolute values .
The full list of 108 features is given in the Appendix A.1 .
Embedding - based Classification 3.2.1
Average pre-trained embeddings + SVM ( Wiki + SVM )
We compute an average of all token vectors in the paragraph , and use this mean vector as a feature vector to train a SVM classifier with linear kernel .
We work with the publicly available language specific 300 - dimensional pre-trained Wiki word vector models trained on Wikipedia using fastText 2 ( Joulin et al. , 2016 ) .
Gaussian distributions for similarity - based classification ( Wiki + Gauss . +SVM )
We follow Das et al . ( 2015 ) ; Nikolentzos et al. ( 2017 ) and Gourru et al . ( 2020 ) and represent a text as a multivariate Gaussian distribution based on the distributed representations of its words .
We perform similarity - based classification with SVMs where the kernel represents similarities between pairs of texts .
We work with the same pre-trained Wikipedia embeddings as in Wiki + SVM for the words in the model and initialize the ones not contained in the model to random vectors .
Specifically , the method assumes that each word w is a sample drawn from a Gaussian distribution with mean vector ? and covariance matrix ? 2 : w ? N ( ? , ? 2 ) ( 1 ) A text is then characterized by the average of its words and their covariance .
The similarity between texts is represented by the convex combination of the similarities of their mean vectors ?
i and ? j and their covariances matrices ?
2 i and ?
2 j : similarity = ?( sim ( ? i , ?j ) ) + ( 1 ? ? ) ( sim (?
2 i , ? 2 j ) ) ( 2 ) where ? ? [ 0,1 ] and the similarities between the mean vectors and co-variances matrices are computed using cosine similarity and element - wise product , respectively .
Finally , a SVM classifier is employed using the kernel matrices of Equation 2 to perform the classification .
Neural Classification
fastText classifier ( FT ) fastText ( Joulin et al. , 2016 ) is an efficient neural network model with a single hidden layer .
The fast - Text model represents texts as a bag of words and bag of n-gram tokens .
Embeddings are averaged to form the final feature vector .
A linear transformation is applied before a hierarchical softmax function to calculate the class probabilities .
Word vectors are trained from scratch on our data .
Pre-trained embeddings + FT ( Wiki + FT )
In this model we work with the pre-trained word vectors from Wikipedia to initialize the fastText classifier .
The data setting makes this directly comparable to Wiki + SVM , a non-neural classifier .
Long short - term memory network ( LSTM )
We use a single- layer uni-directional LSTM ( Hochreiter and Schmidhuber , 1997 ) with embedding and hidden layer with 128 dimensions .
The embedding layer uses wordpiece subunits and is randomly -initialised .
We pool ( average ) all hidden states , and pass the output to a binary linear classifier .
We use a batch size of 32 , learning rate of 1?10 ?2 , and Adam optimiser with Pytorch defaults .
Simplified transformer ( Simpl .
Trf . )
We use a single- layer encoder-decoder transformer with the same hyperparameters and wordpiece embedding layer as the LSTM .
The architecture has no positional encodings .
Instead , we introduce a simple cumulative sum-based contextualisation .
The attention computation has been simplified to element -wise operations and there are no feedforward connections .
A detailed description is provided in Appendix A.2 .
Bidirectional Encoder Representations from Transformers ( BERT )
We use the BERT - base multilingual uncased model ( 12 layers , 768 hidden dimensions , 12 attention heads ) ( Devlin et al. , 2019 ) .
Fine-tuning is done with the simpletransformers 3 library .
For this , the representation of the [ CLS ] token goes through a pooler , where it is linearly projected , and a tanh activation is applied .
Afterwards it undergoes dropout with probability 0.1 and is fed into a binary linear classifier .
We use a batch size of 32 , learning rate of 4 ? 10 ?5 , and the Adam optimiser with epsilon 1 ? 10 ?8 . Models were fine-tuned on 4 GPUs .
We design and compare our " lean " single - layer LSTM and simplified transformer models with BERT in order to investigate whether the amount of data and the complexity of the task necessitate complex and large networks .
4 4 Translationese Classification
Data
We use monolingual and multilingual translationese corpora from Amponsah - Kaakyire et al . ( 2021 ) which contain annotated paragraphs ( avg. 80 tokens ) of the proceedings of the European parliament , the Multilingual Parallel Direct Europarl 5 ( MPDE ) .
Annotations indicate the source ( SRC ) and target languages ( TRG ) , the " original " or " translationese " label , and whether the translations are direct or undefined ( possibly translated through a pivot language ) .
As texts translated through a pivot language may have different characteristics from directly translated texts , here we only use the direct translations .
For the initial experiment we focus on 3 languages : German ( DE ) , English ( EN ) and Spanish ( ES ) .
We adopt the following format for data description : we refer to translationese corpora ( i.e. corpora where half of the data is originals , half translationese ) with the " TRG - SRC " notation ( with a dash ) : TRG is the language of the corpus , SRC is the source language , from which the translation into the TRG language was done in order to produce the translationese half .
The " TRG ?
SRC " notation ( with an arrow ) denotes the result of translating a text from SRC into TRG language .
We use it to refer only to the For all settings we perform binary classification : original vs. translated .
Results Paragraph - level translationese classification results with mean and standard deviations over 5 runs are reported in Table 2 .
Overall , the BERT model outperforms other architectures in all settings , followed closely by the other end-to - end neural architectures .
Using the pre-trained Wiki embeddings helps improving the accuracy of the fastText method in all cases .
Among the approaches with the SVM classifier , Wiki +SVM performs best in the single-source settings , but shows lower accuracy than Handcr .
+SVM in the multi-source ( TRG - ALL ) settings .
Wiki + Gauss .+SVM performs worst apart from on ES -EN and DE - ALL .
In the monolingual single-source settings , we observe that accuracy is slightly lower when the source language is typologically closer to the text language , i.e. it becomes more difficult to detect translationese .
Specifically , DE -EN tends to have lower accuracy than DE-ES ; EN - DE lower accuracy than EN -ES ; and ES -EN lower accuracy than ES - DE .
Accuracy generally drops when going from single-source to the multi-source setting , e.g. from DE-EN and DE - ES to DE - ALL .
The EN - ALL dataset is the most difficult for most of the models among the TRG - ALL datasets .
The ALL - ALL [ 3 ] setting exhibits comparable accuracy to the TRG - ALL setting for the neural models , but for the SVM there is a drop of around 9 points .
Throughout our discussion we always report absolute differences between systems .
The ALL - ALL [ 8 ] e.g. , trail BERT by ?20 accuracy points .
To make sure our hand -crafted - feature - based SVM results are competitive , we compare them with on our data .
show that training a SVM classifier on the top 1000 most frequent POS - or character - trigrams yields SOTA translationese classification results on Europarl data .
On our data , POS - trigrams yield around 5 points increase in accuracy for most of the datasets and character -trigrams tend to lower the accuracy by around 4 points ( Appendix A.3 ) .
For the remainder of the paper we continue to work with our handcrafted features , designed to capture various linguistic aspects of translationese .
Multilinguality and Cross-Language Performance Since neural architectures perform better than the non-neural ones , we perform the multilingual and cross-language analysis only with the neural models .
We evaluate the models trained on one dataset on the other ones , in order to verify : ?
Whether for a given target language , the model trained to detect translationese from one source language , can detect translationese from another source language : TRG - SRC 1 on TRG - SRC 2 , and TRG - SRC on TRG - ALL ; ?
How well the model trained to detect translationese from multiple source languages can detect translationese from a single source language : TRG - ALL on TRG - SRC , and ALL - ALL [ 3 ] on TRG - SRC ; ?
How well the model trained to detect translationese in multilingual data performs on monolingual data : ALL - ALL [ 3 ] on TRG - ALL , and ALL - ALL [ 3 ] on TRG - SRC .
Table 3 shows the results of cross-data testing for the monolingual models for the best-performing architecture ( BERT ) .
For the single-source monolingual models , we observe a relatively smaller drop ( up to 13 percentage points ) in performance when testing TRG - SRC on TRG - ALL ( as compared to testing TRG - SRC on TRG - SRC ) , and a larger drop ( up to 27 points ) when testing TRG - SRC 1 on TRG - SRC 2 ( as compared to testing TRG - SRC 1 on TRG - SRC 2 ) .
The fact that classification performance stays above 64 % confirms the hypothesis that translationese features are sourcelanguage - independent .
Another trend that can be observed is that in cross testing TRG - SRC 1 and TRG - SRC 2 , the model where the source language is more distant from the target suffers larger performance drop when tested on the test set with the closer-related source language , than the other way around .
For instance , the DE -ES model tested on the DE-EN data suffers a decrease of 17.8 points , and DE -EN model tested on the DE - ES data suffers a decrease of 9.8 points .
This may be due to DE -EN having learned more of the general translationese features , which helps the model to obtain higher accuracy on the data with a different source , while the DE - ES model may have learned to rely more on the language - pair-specific features , and therefore it gives lower accuracy on the data with the different source .
A similar observation has been made by Koppel and Ordan ( 2011 ) .
For the multi-source monolingual models ( TRG - ALL ) , testing on TRG - SRC 1 and TRG - SRC 2 datasets shows a slight increase in performance for a source language that is more distant from the target , and a slight decrease for the more closelyrelated source language ( as compared to testing TRG - ALL on TRG - ALL ) .
For the DE -ES set , the performance actually increases for the neural models , but not for the Handcr . +SVM .
We extended this experiment in Table 5 , testing the ALL - ALL [ 8 ] on all test sets to further complement our multilingual analysis with more diverse languages and observe a similar trend , which is in line with the accuracy of the ALL - ALL [ 3 ] models on all test sets .
We also compare the performance of ALL - ALL [ 3 ] on different test sets to the original performance of the models trained on these datasets ( in parentheses ) .
There is a relatively larger drop in accuracy for the TRG - SRC data , than for TRG - ALL data .
The largest drop for neural models is 6.7 accuracy points whilst the smallest performance drop for the Handcr . +SVM is 12.7 .
This highlights the ability of the neural models to learn features in a multilingual setting which generalize well to their component languages whereas the Handcr .
+SVM method does not seem to work well for such a case .
However , for ALL - ALL [ 8 ] models , Table 5 shows a large performance drop across all architectures as compared to the results from the models specifically trained for the task .
The actual models are trained on languagespecific features , whereas the ALL - ALL [ 8 ] model is trained on more diverse data containing typologically distant languages and thus captures less targeted translationese signals .
In summary , we observe that : ?
For a given target language , even though a neural model trained on one source language can decently identify translationese from another source language , the decrease in performance is substantial .
?
Neural models trained on multiple sources for a given target language perform reasonably well on single-source languages .
?
Neural models trained on multilingual data ALL - ALL [ 3 ] perform reasonably well on monolingual data , especially for multi-source monolingual data .
?
Using more source and target languages ( ALL - ALL [ 8 ] ) leads to a larger decrease in cross-testing accuracy .
6 Feature Importance and Relation to Neural Models
In this section we aim to quantify the feature importance of the hand-crafted linguistically inspired features used in Handcr .
+SVM according to different multilingual models ( ALL - ALL [ 3 ] setting ) .
As we use a Support Vector Machine with a linear kernel , we can interpret the magnitude of the feature weights as a feature importance measure .
Guyon et al. ( 2002 ) for instance , use the squared SVM weights for feature selection .
We rank the features by the absolute value of the weight .
The feature ranks are listed in Appendix A.1 .
Figure 1 shows the top 10 features .
Paragraph length is the most relevant feature , and we observe that most of the top features correspond to paragraph log probability .
These features characterize simplification in translationese .
To explore whether there is any correlation between the hand-crafted features and predictions of the trained neural models , we conduct the following experiment in the multilingual setting .
We fit a linear regression model for each hand -crafted feature , using the estimated probabilities of neural model as gold labels to be predicted .
More formally , with n paragraphs ( p i , i = 1... n ) in the test set and d features , for each feature vector x j ?
R n , j = 1 ... d we fit a model y = w j x j + b j , ( 3 ) where w j , b j ?
R are the model parameters , and y ?
R n is a vector of predictions of the neural model F ( LSTM , Simplified Transformer , BERT ) on the test set , with each dimension y i showing the probability of a data point to belong to the translationese class : y i = P ( F ( p i ) = 1 ) ( 4 ) We apply min-max normalization to the features .
We find that a large proportion of the linguistically motivated features are statistically significant for predicting the neural models ' predicted probabilities , namely 60 features ( out of 108 ) are significant for LSTM , 38 for the Simplified Transformer , and 56 for BERT , each with probability 99.9 % .
We also fit the per-feature linear models to predict the actual gold labels ( and not the predictions of the neural models ) to investigate which features correlate with the ground truth classes , and find 55 features to be statistically significant with 99.9 % probability .
The full list of statistically significant features for each model , as well as for the gold labels is given in the Appendix A.1 .
We observe that the features significant for the neural models largely overlap with the features significant for the gold labels : the F 1 - score ( as a measure of overlap ) is 0.89 for LSTM , 0.75 for Simplified Transformer and 0.99 for BERT .
This is expected , because highperforming neural models output probabilities that are generally close to the gold labels , therefore a similar correlation with hand -crafted features occurs .
The R 2 measure is further used to rank features based on the amount of explained variance in predictions of a model .
The top 10 features for predicting the predictions of each neural model and for predicting the actual gold labels are displayed in Figure 2 .
The order of top features is similar across the neural models ( pairwise rank correlations ?
Spearman of at least 0.76 ) , and similar to , but not identical to , the gold label results ( pairwise rank correlations ?
Spearman of at least 0.75 ) .
We observe that most of the top features are either POSperplexity - based , or bag- of - POS features .
These features characterize interference in translationese .
It also appears that more importance is attached to perplexities based on unigrams and bigrams than on other n-grams .
Notably , the order of feature importance for the neural models is highly dissimilar from the order of hand -crafted feature weights for the SVM ( pairwise rank correlations ? Spearman at most 0.23 ) .
This might be connected to an accuracy gap between these models .
We conclude that many of hand-crafted translationese features are statistically significant for predicting the predictions of the neural models ( and actual gold labels ) .
However , due to the low R 2 values , we cannot conclude that the handcrafted features explain the features learnt by the representation - learning models .
Summary and Conclusion
This paper presents a systematic comparison of the performance of feature - engineering - based and feature - learning - based models on binary translationese classification tasks in various settings , i.e. , monolingual single -source data , monolingual multi-source data , and multilingual multi-source data .
Additionally , we analyze neural architectures to see how well the hand-crafted features explain the variance in the predictions of neural models .
The results obtained in our experiments show that , ( i ) representation - learning - based approaches outperform hand -crafted linguistically inspired feature -selection methods for translationese classification on a wide range of tasks , ( ii ) the features learned by feature - learning based methods generalise better to different multilingual tasks and ( iii ) our multilingual experiments provide empirical support for the existence of language independent translationese features .
We also examine multiple neural architectures and confirm that translationese classification requires deep neural models for optimum results .
We have shown that many traditional hand -crafted translationese features significantly predict the output of representation learning models , but may not necessarily explain their performance due to the weak correlation .
Our experiments also show that even though single -source monolingual models yield the best performance , they can , to a reasonable extent , be substituted by multi-source mono- and multi-lingual models .
Our interpretability experiment provides only some initial insight into the neural models ' performance .
Even though there are significant relationships between many of the features and the neural models ' predicted probabilities , further experiments are required to verify that the neural models actually use something akin to these features .
Also our current approach ignores interaction between the features .
In the future , we plan to conduct a more detailed analysis of the neural models ' decision making .
A Appendix A.1 List of hand -crafted features Col. 3 : SVM feature importance ranks ( ranked by absolute feature weight ) for the model trained on the ALL - ALL [ 3 ] set .
Col. 4 - 7 : Statistical significance of the features as predictors in per-feature linear regression with respect to neural models ' predicted probabilities and gold labels ( 1 - significant with 99.9 % confidence level ) on the ALL - ALL [ 3 ] test set . .
The mean and the standard deviations over 5 runs are reported .
The difference from the Handcr .+SVM model is indicated in parentheses .
ID Figure 1 : 1 Figure 1 : Top 10 SVM features , as a function of the absolute value of its feature weight .
Figure 2 : 2 Figure 2 : Top 10 features as a function of R 2 ? 10 ?3 for the neural architectures and the gold labels .
Table 1 : 1 Number of paragraphs in each of the datasets .
Average paragraph length is around 80 tokens .
Corpus Training Dev. Test TRG -SRC 30 k 6 k 6 k TRG - ALL 30 k 6 k 6 k ALL - ALL [ 3 ] 89 k 19 k 19 k ALL - ALL [ 8 ] 67 k 14 k 14k
Table 2 : 2 Translationese classification average accuracy on the mono- and multilingual test sets ( average and standard deviation over 5 runs ) .
Handcr .
Wiki Wiki fastText Wiki LSTM Simpl. BERT + SVM + SVM + Gauss . ( FT ) + FT Trf. + SVM DE-EN 71.5?0.0 77.7?0.1 67.6?0.1 88.4?0.0 89.2?0.0 89.5?0.4 89.7?0.2 92.4?0.2 DE-ES 76.2?0.0 79.4?0.3 68.2?0.2 90.9?0.0 91.9?0.0 91.9?0.2 91.6?0.2 94.4?0.1 EN -DE 67.6?0.7 72.5?0.2 64.5?0.2 85.1?0.0 85.9?0.1 86.8?0.5 85.8?0.2 90.7?0.1 EN -ES 70.1?0.2 77.5?0.4 67.1?0.4 87.6?0.0 88.7?0.0 89.1?0.3 89.3?0.4 91.9?0.4 ES -DE 71.0?0.0 75.7?0.4 70.1?0.4 88.4?0.0 89.1?0.0 90.2?0.2 90.4?0.3 92.3?0.2 ES -EN 66.7?0.0 70.1?0.3 67.0?0.7 87.0?0.1 87.9?0.0 88.8?0.4 88.4?0.2 91.4?0.3 DE-ALL 72.6?0.0 64.3?0.0 65.1?0.1 87.4?0.0 88.3?0.0 88.5?0.2 88.6?0.4 90.9?0.3 EN -ALL 65.3?0.0 64.6?0.0 62.5?0.1 82.7?0.0 84.4?0.0 84.2?0.4 83.8?0.3 87.9?0.4 ES -ALL 67.4?0.0 67.3?0.0 66.5?0.2 84.9?0.0 85.9?0.0 87.0?0.3 86.9?0.3 89.9?0.1 ALL - ALL [ 3 ] 58.9?0.0 - - 85.0?0.0 - 84.4?0.3 84.5?0.2 89.6?0.2 ALL - ALL [ 8 ] 65.4?0.1 - - 70.4?0.1 - 77.2?0.3 77.9?0.1 84.6?0.2 translationese half of the corpus .
For our experiments we extract four datasets from MPDE with summary statistics in Table 1 . half of the data is DE original texts , and the other half contains equal proportions of DE?ES and DE?EN .
3 . Multilingual multi-source data : ALL - ALL [ 3 ] .
There is an equal number of originals : DE , EN and ES , which together make up 50 % of the examples .
The other 50 % which are translated are equal proportions of DE?EN , DE?ES , EN?DE , EN?ES , ES?DE and ES?EN .
EN , DE and ES are relatively close typologi- cally .
We conduct additional experiments in order to investigate how well the classification can be performed when more and more distant languages are involved : 4 . Multilingual multi-source data large : ALL - ALL [ 8 ] , balanced in the same way as ALL - ALL [ 3 ] , but with the addition of Greek ( EL ) , 1 . Monolingual single-source data : DE-EN , DE-ES , EN-DE , EN-ES , ES-DE , ES-EN .
For each corpus , there is an equal number of translated and original paragraphs .
2 . Monolingual multi-source data : DE-ALL , EN - ALL , ES - ALL .
For DE- ALL , e.g. , French ( FR ) , Italian ( IT ) , Dutch ( NL ) and Portuguese ( PT ) .
Table 3 : 3 data results in reduced accuracy for most architectures , except Handcr .+SVM .
Neural - classifier -based models substantially outperform the other architectures : the SVMs trained with hand-crafted linguistically - inspired features , BERT translationese classification accuracy of all TRG - SRC and TRG - ALL models on TRG - SRC and TRG - ALL test sets ( average and standard deviation over 5 runs ) .
Columns : training set ; rows : test set .
DE-EN DE-ES EN-DE EN-ES ES-DE ES-EN DE-ALL EN-ALL ES-ALL DE-EN 92.4?0.2 76.6?0.7 - - - - 90.5?0.3 - - DE-ES 82.6?1.1 94.4?0.1 - - - - 91.8?0.4 - - EN -DE - - 90.7?0.1 64.7?1.4 - - - 87.3?0.4 - EN -ES - - 72.9?0.9 91.9?0.4 - - - 88.6?0.4 - ES -DE - - - - 92.3?0.2 78.8?0.9 - - 90.6?0.1 ES-EN - - - - 78.8?1.6 91.4?0.3 - - 89.0?0.2 DE-ALL 87.3?0.6 85.3?0.4 - - - - 90.9?0.3 - - EN-ALL - - 81.7?0.5 78.3?0.7 - - - 87.9?0.4 - ES-ALL - - - - 85.9?0.9 85.0?0.6 - - 89.9?0.1
Table 4 4 displays the results of testing the Handcr .
fastText Simpl .
LSTM BERT + SVM ( FT ) Trf. DE-EN 58.5?0.0 ( ?13.0 ) 85.9?0.0 ( ?2.5 ) 85.5?0.5 ( ?4.3 ) 86.6?0.7 ( ?2.9 ) 90.5?0.3 ( ?1.9 ) DE-ES 57.0?0.0 ( ?19.2 ) 88.3?0.0 ( ?2.6 ) 87.2?0.5 ( ?4.3 ) 85.3?0.3 ( ?6.9 ) 91.5?0.2 ( ?2.9 ) EN -DE 50.0?0.0 ( ?17.6 ) 81.5?0.1 ( ?3.6 ) 81.1?0.3 ( ?4.7 ) 80.9?0.3 ( ?5.8 ) 87.2?0.4 ( ?3.5 ) EN -ES 50.5?0.0 ( ?19.6 ) 84.6?0.0 ( ?3.0 ) 83.5?0.5 ( ?5.9 ) 83.8?0.6 ( ?5.3 ) 88.9?0.3 ( ?3.0 ) ES -DE 50.0?0.0 ( ?21.0 ) 85.6?0.0 ( ?2.8 ) 86.2?0.4 ( ?4.3 ) 85.7?0.5 ( ?4.6 ) 90.4?0.4 ( ?1.9 ) ES-EN 51.3?0.0 ( ?15.4 ) 84.1?0.0 ( ?2.9 ) 84.6?0.4 ( ?0.4 ) 82.1?0.4 ( ?6.7 ) 89.0?0.4 ( ?2.4 ) DE-ALL 59.9?0.0 ( ?12.7 ) 87.2?0.0 ( ?0.2 ) 86.3?0.4 ( ?2.3 ) 85.9?0.5 ( ?2.6 ) 90.8?0.1 ( ?0.1 ) EN -ALL 50.2?0.0 ( ?15.1 ) 82.9?0.0 ( ?0.2 ) 82.0?0.1 ( ?1.8 ) 82.2?0.2 ( ?2.1 ) 88.1?0.5 ( ?0.2 ) ES - ALL 50.0?0.0 ( ?17.4 ) 84.8?0.0 ( ?0.1 ) 85.3?0.2 ( ?1.6 ) 85.2?0.5 ( ?1.8 ) 89.8?0.3 ( ?0.1 ) ALL - ALL [ 3 ] 58.9?0.0 ( 0.0 ) 85.0?0.0 ( 0.0 ) 84.5?0.2 ( 0.0 ) 84.4?0.3 ( 0.0 ) 89.6?0.2 ( 0.0 )
Table 4 : 4 Translationese classification accuracy of the ALL - ALL [ 3 ] model on all test sets ( average and standard deviations over 5 runs ) .
The difference from actual trained model performance is indicated in parentheses .
Handcr .
fastText Simpl . LSTM BERT + SVM ( FT ) Trf. DE-EN 53.0?0.5 ( ? 18,5 ) 71.0?0.3 ( ?17.4 ) 79.3?0.4 ( ?12.3 ) 79.9?0.5 ( ?9.6 ) 85.5?0.4 ( ?6.9 ) DE-ES 51.3?0.3 ( ?24.9 ) 73.2?0.3 ( ?17.7 ) 81.4?0.3 ( ?8.4 ) 79.0?0.5 ( ?12.9 ) 87.9?0.3 ( ?6.5 ) EN - DE 48.3?0.1 ( ?19.3 ) 65.8?0.2 ( ?19.3 ) 74.2?1.0 ( ?11.6 ) 72.9?0.4 ( ?13.8 ) 79.0?0.5 ( ?11.7 ) EN -ES 50.3?0.1 ( ?19.8 ) 68.9?0.3 ( ?18.7 ) 76.8?0.6 ( ?12.8 ) 75.6?0.8 ( ?13.5 ) 83.2?0.4 ( ?8.7 ) ES -DE 50.0?0.0 ( ?21.0 ) 71.1?0.2 ( ?17.3 ) 78.8?0.5 ( ?11.6 ) 76.0?0.7 ( ?14.2 ) 83.8?0.3 ( ?8.5 ) ES -EN 53.2?0.5 ( ?13.5 ) 69.9?0.2 ( ?17.1 ) 76.7?0.6 ( ?11.7 ) 75.4?0.7 ( ?13.4 ) 82.8?0.2 ( ?8.6 ) DE-ALL 53.1?0.5 ( ?19.5 ) 72.1?0.3 ( ?15.3 ) 80.5?0.4 ( ?8.1 ) 79.7?0.6 ( ?8.8 ) 86.8?0.2 ( ?4.1 ) EN - ALL 48.4?0.2 ( ?16.9 ) 67.0?0.2 ( ?15.7 ) 75.4?0.9 ( ?8.4 ) 74.4?0.5 ( ?9.9 ) 81.1?0.1 ( ?6.8 ) ES - ALL 50.8?0.3 ( ?16.6 ) 70.4?0.2 ( ?14.5 ) 77.9?0.6 ( ?9.1 ) 75.9?0.6 ( ?11.1 ) 83.2?0.3 ( ?6.7 ) ALL - ALL [ 3 ] 53.2?0.3 ( ?5.7 ) 70.5?0.2 ( ?14.5 ) 77.9?0.2 ( ?6.6 ) 76.7?0.5 ( ?7.7 ) 83.7?0.1 ( ?5.9 ) ALL - ALL [ 8 ] 65.4?0.1 ( 0.0 ) 70.4?0.1 ( 0.0 ) 77.9?0.1 ( 0.0 ) 77.2?0.3 ( 0.0 ) 84.6?0.2 ( 0.0 )
Table 5 : 5 As Table4 for the ALL - ALL [ 8 ] model .
multilingual ) models on all test sets for the neural architectures , as well as Handcr . +SVM .
We observe that the largest performance drop ( as compared to testing on ALL - ALL [ 3 ] test set ) happens for the EN - DE test set .
Table 7 : 7
Test accuracy of baseline systems implemented from
https://fasttext.cc/docs/en/ pretrained-vectors.html
github.com / ThilinaRajapakse / simpletransformers
The two lean architectures drastically decrease the number of core parameters .
The number of parameters is 85 M for BERT , 132 k for the LSTM and 768 for the simplified transformer when the embedding layer and the classifier , which are common to the 3 architectures , are not considered .
5 github.com / UDS-SFB -B6 - Datasets / Multilingual -Parallel-Direct-Europarl
