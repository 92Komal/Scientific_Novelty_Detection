title
Just Ask !
Evaluating Machine Translation by Asking and Answering Questions
abstract
In this paper , we show that automaticallygenerated questions and answers can be used to evaluate the quality of Machine Translation systems .
Building on recent work on the evaluation of abstractive text summarization , we propose a new metric for system-level Machine Translation evaluation , compare it with other state - of - the - art solutions , and show its robustness by conducting experiments for various translation directions .
Recently I flew from Moscow where I was studying , " said Andrei Borovikov .
Introduction
The goal of automatic Machine Translation ( MT ) evaluation is to automatically evaluate the output quality produced by MT systems .
Metrics used for this task assign a score by comparing the MT output to either a reference translation or to the source sentence ( the latter is called Quality Estimation ) .
The main indicator that is used to assess the performance of a specific metric is the correlation with human judgement computed for outputs from several systems .
It was recently shown that metrics based on contextualized embeddings , such as YISI ( Lo , 2019 ) or ESIM ( Mathur et al. , 2019 ) , are able to achieve better performance than the most widely used BLEU ( Papineni et al. , 2002 ) .
In this paper , we propose a new method for automatic evaluation of MT systems - MTEQA 1 ( Machine Translation Evaluation with Question Answering ) , building on previous works on evaluating abstractive summaries .
We build upon the fact that state - of- the - art ( neural ) MT systems tend to produce a fluent output but sometimes fail in adequacy of the translation .
We leverage the recent progress in Question Generation ( QG ) and Question Answering ( QA ) to formulate and answer human readable questions about the MT system output .
Our experiments show that the effectiveness of the proposed metric is comparable to performance 1 https://github.com/ufal/MTEQA of other automatic metrics , while considering only a certain amount of information from the whole translation .
We also examine the robustness of the metric by considering several translation directions and target languages .
The remainder of this paper is structured as follows .
In Section 2 , we introduce relevant research on question - based evaluation .
In Section 3 , we describe our metric in detail .
In Section 4 , we present and discuss the results of our experiments including s the influence of different human scoring methods .
Section 5 presents conclusions .
Related Work Metrics that are most widely used for automatic evaluation of MT outputs produce a score by comparing surface - level forms of hypothesis and reference translation .
The most dominant one , BLEU ( Papineni et al. , 2002 ) , is a version of n-gram precision calculated by averaging over different values of n with penalization for overly short translations ( brevity penalty ) .
Another one , CHRF ( Popovi ? , 2015 ) , considers the character - level n-grams , making it possible to reward partial token matches .
The standardised implementation provided in the sacreBLEU 2 package takes care of pre-processing and enables direct comparison between MT outputs .
Recently , various works ( e.g. , Lo , 2019 ; Mathur et al. , 2019 ; Bawden et al. , 2020 ) explored the usage of contextualized word - level or sentence - level embeddings to compare the numerical representations of reference and hypothesis .
Such metrics enable explicit regression towards the desired humanproduced labels .
Evaluation of Summarization
The task of automatic text summarization is to produce a concise summary of a given document that would preserve all the key information from the document .
One of the most popular metrics used for evaluating summary quality is ROUGE ( Lin , 2004 ) , which compares overlapping n-grams between the model output and the reference summary .
To step beyond the n-grams comparison , Eyal et al . ( 2019 ) proposed the APES metric .
They used the reference summary to produce fill - in- theblank type of questions by finding all possible entities using a NER system .
The APES score for a given summarization model is the percentage of questions that were answered correctly ( using an Question Answering system ) , averaged over the whole test-set .
The authors reported a higher correlation with the Pyramid method ( Nenkova et al. , 2007 ) for manual evaluation than the ROUGE metric .
Scialom et al. ( 2019 ) extended their work into unsupervised settings by generating questions from the source document .
Closest to our work are the metrics FEQA ( Durmus et al. , 2020 ) and QAGS ( Wang et al. , 2020 ) , which automatically generate the natural language questions from the summary and / or document .
Tomita et al. ( 1993 ) were the first to use the reading comprehension tests to measure the quality of MT systems .
They translated several passages from TOEFL ( Test of English as a Foreign Language ) guide book into Japanese , using a selection of MT systems , while corresponding questions and answers were translated into Japanese by professional translators .
The MT systems were evaluated by measuring the percentage of questions answered correctly by the Japanese speaking human annotators , using the MT output as a context .
Fuji et al. ( 2001 ) used the reading comprehension tests to examine the " usefulness " of machinetranslated text .
In their experiment , participants take the reading comprehension test in a foreign language ( English ) , while also being presented with the text translated by the MT system into their mother language ( Japanese ) .
Authors claim that presenting the MT output yields a higher comprehension performance .
Question - based Evaluation of MT Castilho and Guerberof Arenas ( 2018 ) examine the user satisfaction when completing the comprehension type of test , using the context translated by the MT system .
They collect the eye-tracking data to analyse the cognitive effort of the participants .
Scarton and Specia ( 2016 ) approached the prob-lem of document- level Quality Estimation ( QE ) by extending the CREG corpus ( Ott et al. , 2012 ) of German documents designed for reading comprehension exercises .
They use professional translators to translate the questions and answers to English .
They examine the document-level translation quality by translating the documents by MT systems and asking the human annotators to complete the reading comprehension test using the MT output as a context .
Forcada et al. ( 2018 ) used the same corpus to examine the usage of automatically generated gap-filling closure type of testing .
Berka et al. ( 2011 ) used the yes / no type of questions for manual evaluation of MT systems , examining the English - to - Czech direction .
The authors prepared a set of English texts from various domains and used human annotators to come up with three content - based question - answer pairs in Czech for each of the texts .
In the next step , the annotators were given the outputs from MT systems ( in Czech ) and were tasked to answer the questions using the corresponding translation as the context .
For each system , the percentage of properly answered questions was measured .
We believe no prior work examines the usage of automatically generated questions and answers to assess the quality of MT systems .
Keyphrase Extraction Keyphrases are representative and characteristic phrases from a text that express the key aspects of its content ( Papagiannopoulou and Tsoumakas , 2020 ) .
In our work , keyphrases play the role of answers , i.e. , the pieces of information which we test to be preserved in translation .
In recent years , a wide range of supervised and unsupervised keyphrase extraction methods have been proposed .
Unsupervised methods normally perform two main steps to extract keyphrases : 1 ) select candidate phrases based on some heuristics such as matching with a specific part- of-speech pattern ; 2 ) rank the candidates and select the top ones .
Various approaches have been proposed to address this problem such as statistics - based ( Won et al. , 2019 ) , graph- based ( Mihalcea and Tarau , 2004 ) , topic models - based ( Liu et al. , 2010 ) , and language model - based ( Tomokiyo and Hurst , 2003 ) methods .
On the other hand , supervised methods are relying on labeled data in which keyphrases are annotated in the documents .
Supervised methods
Reference MT output
Extracted Answers Plum fruit
Salahudin
Generated Questions
What did Salahudin send to Richard when he got sick ?
Who sent Richard Plum fruit when he got sick ?
It is said that when Richard got sick , Salahuddin sent him some aloof , which was kept in the snow .
MT Answers some aloof
Salahuddin in the snow in the snow
Where were the Plum fruit kept when Richard got sick ?
generally model the keyphrase extraction problem as binary classification to predict whether a candidate phrase is a keyphrase or not ( Wang and Li , 2017 ) , learning to rank to learn a ranking function that sorts the candidate phrases based on their score ( Zhang et al. , 2017 ) , and sequence labeling problem .
MTEQA
Our idea of evaluating MT quality by asking and answering questions is based on the assumption that a good translation should preserve all of the key information that one can extract from the reference .
We propose to use a question answering framework as the proxy to measure this .
To check whether a piece of information is preserved , we automatically generate pairs of a question and its ( gold- standard ) answer from the reference translation and employ a question answering system to provide a new ( test ) answer given the question and the MT output ( translation ) used as the context .
The generated ( test ) answer is then compared to the gold -standard answer .
We assume that if it was possible to answer a question looking only at the reference , it should also be possible to answer this question looking only at the MT output and that the two answers should be identical or very similar .
In principle , the proposed MTEQA metric requires solving the following tasks :
1 ) Answer extraction identifies the key information in a sentence ( keyphrases ) which should be also present in the MT output .
This extraction can be treated in a hierarchical / nested manner .
For instance , given the sentence " Today for dinner I had an organic pasta with garlic . " , the question " What did you have for dinner today ? " can be correctly answered by all the following phrases pasta , organic pasta and organic pasta with garlic .
Thus , answer extraction is performed first and the questions are generated afterwards for each of the answers independently .
The same question can be paired with multiple ( nested ) answers which allows capturing a partial correspondence .
2 ) Question generation , given a reference translation , produces a human readable question , for which a given keyphrase is the correct answer .
For each of the extracted answers , each question is generated independently from the other answers .
3 ) Question answering generates an answer , given a natural language question and a sentence used as a context .
Since we assume that the MT output should carry enough information to answer any question asked based on the reference , we do not consider the non-answerable questions .
4 ) Answer comparison assesses to what extent the generated answer is correct , given the gold - standard answer extracted from the reference .
Metrics based on exact match should be avoided because they are too strict .
For example , given the gold-standard answer " Tchaikovsky " , both the " Tchaikovski " and " Beethoven " would get the same score .
Scoring Procedure
The entire procedure of MTEQA is illustrated in Figure 1 . Formally , for a given segment s i , reference translation r i and MT system output t i , it proceeds as follows :
1 . Generate the gold-standard answers a i1 , a i2 , . . . , a ik from the reference r i 2 .
For each answer a ij and reference r i , generate a natural language question q ij 3 .
Answer each question q ij using the MT output t i as a context , obtaining answer ?ij 4 .
The final score for a given translation of a segment s i , is the average over all generated questions : M T EQA( t i ) = k 1 D( a ij , ?ij ) k , where D ( ? , ? ) is a string -comparison metric used to compare the two answers and k is the number of gold -standard answers extracted from the reference .
For the task of comparing MT systems on the entire test-set ( i.e. system - level comparison ) or at the document- level , we simply report the average of the segment - level scores .
When more than one reference ri is available for a given segment , we can use it to generate additional questions and answers .
Baseline Implementation
Our implementation of the proposed MTEQA metric is based on the state - of - the - art system capable of solving the initial three tasks of the procedure : answer extraction , question generation , question answering .
It is the T5 model ( Raffel et al. , 2020 ) fine -tuned on the SQuADv1 dataset ( Rajpurkar et al. , 2016 ) by Patil ( 2020 ) and available from GitHub 3 . Performance on the development set of SQuADv1 in Table 2 .
We report word- level F1 for question answering and BLEU and ROUGE -L for question generation .
The SQuAD dataset was created manually by tasking the crowd-workers to create up to five questions - answer pairs from a single paragraph from Wikipedia .
While the crowd-workers were encouraged to formulate the questions in their own words , the answers were restricted to be continuous sub-sequences of words from the given paragraph .
In MTEQA , the answers generated by this model are also continuous sub-sequences of words from the reference and test translations .
The same system is also used for question answering and question generation by prompting the model with a different initial token in the inputfor Question Answering : " question : { question_text } context : { context_text } " for Question Generation : " answer : { answer_text } context : { context_text } " .
Generating Additional Answers
Since the QG system generates a single question for each sub-sequence of words marked as an extracted answer , the limit factor is the number of gold -standard answers we extract .
To generate more questions , we need more keyphrases to formulate a question about .
Considering the whole predictive power of our metric is based on questions , we propose two methods of generating additional questions .
1 ) We exploit the MT output as an additional source of question / answer pairs .
After following the standard procedure , we swap the roles of MT output and reference - we generate gold -standard answers and questions from the MT output , and use reference as a context to answer it .
As a final score we take the sum of the two scores .
2 ) We add keyphrases extracted by linguistic processing of the sentences based on Part-of- Speech ( POS ) pattern matching and Named Entity Recognition ( NER ) .
Given a sentence as the input , first , we parse the sentence using UDPipe ( Straka et al. , 2016 ) to extract part of speech ( POS ) tags .
Then , we extract phrases that are matched with one of the patterns in our POS pattern bank .
The POS pattern bank is created by parsing the sentences from XQuAD ( Artetxe et al. , 2020 ) dataset , extracting the POS patterns corresponding to the gold-standard answers , and taking the most frequent patterns .
This dataset contains professional translations of the development set of SQuADv1 , translated into various languages from different language families and using different scripts .
Table 1 shows some examples of the extracted POS patterns .
Second , we extract named entities mentioned in the input sentence using a combination of two multilingual NER models , POLYGLOT - NER ( Al - Rfou et al. , 2015 ) , and Stanza ( Qi et al. , 2020 ) .
Finally , we output the union of the extracted phrases and named entities as the potential answers .
Choice of the D ( ? , ? ) Metric
As already pointed , selection of the D ( ? , ? ) might be crucial for optimal performance of the proposed metric and thus we consider several options .
Motivated by QA evaluation , we employ the word- level F1 ( Rajpurkar et al. , 2016 ; Trischler et al. , 2017 ; Chen et al. , 2019 ; Durmus et al. , 2020 ) . Motivated by MT evaluation we also consider the BLEU ( Papineni et al. , 2002 ) metric and the CHRF ( Popovi ? , 2015 ) metric .
Finally we also employ " exact match " ( Rajpurkar et al. , 2016 ) score , mainly for comparison .
All of the metrics we use operate on a surface level and assign a similarity score for a pair of strings .
In the future , it may be worth to explore e.g. cosine similarity between word embeddings .
Experiments
We evaluate the proposed MTEQA metric using the submissions to the WMT20 News translation task ( Barrault et al. , 2020 ) and their ( direct ) human assessments ( DA ) .
For each of the MT systems participating in the task , we compute a single score as the average of segment - level scores and report the system-level Pearson correlation with the human assessment .
We report individual results for selected translation directions into English plus aggregated results ( averages ) for all to -English directions which were part of the WTM20 Metric Task ( Mathur et al. , 2020 b ) evaluation campaign 4 .
Baseline
The baseline implementation is described in Section 3 .
It is based on the T5 model tuned on the SQuADv1 dataset and used to generate : 1 ) the gold-standard answers from the reference translations , 2 ) a question for each gold - standard answer , 3 ) a test answer for each question and MT output ( context ) pair .
The test answers are compared by the word- level F1 score ( Section 3.4 ) .
The results of this system are shown in Table 3 labeled as MTEQA F1 together with other metrics for comparison .
We experiment with the to -English direction , since the SQuADv1 dataset used for finetuning is in English .
On average , the baseline outperforms the traditional MT evaluation metrics ( SENTBLEU , BLEU ) as well as the recently proposed ones that performed very well in the WTM20 Metric Task ( PRISM ( Thompson and Post , 2020 ) , YISI - 2 ) , though for some of the translation directions ( e.g. Czech - English ) MTEQA F1 is much worse ( but for Czech - English , YISI - 2 also does not beat BLEU ) .
Variants of the D ( ? , ? ) metric
To assess the effect of choice of the D ( ? , ? ) metric , we modified the baseline to exploit other options ( see Section 3.4 ) .
The results are shown in the first section of Table 4 .
Unsurprisingly , the worst results are achieved by MTEQA EXACT which requires exact match of the test answer and the goldstandard one .
But overall , the differences here are not large .
Generating Additional Answers
In general , the T5 model fine-tuned on the SQuADv1 dataset does not generate plentiful question / answer pairs .
In fact , the average number of such pairs that are generated for an English sentence is only around two .
Table 5 ( row baseline ) presents exact figures from our experiments , i.e. , the average numbers of questions generated from a single segment of the newstest2020 reference files for selected translation directions and the average computed for all directions into English .
To increase the number of question / answer pairs , we implemented the two methods described in Section 3.3 and present the results in Table 4 .
The systems denoted as OUT exploit question / answer pairs extracted from the references and MT outputs and the systems denoted as KEYPHRASE extract the pairs by POS pattern matching and NER .
The average correlation obtained using the MT output to generate questions ( denoted as OUT ) was very similar , but slightly worse than the one using just the questions from the reference .
However , the method based on POS pattern matching and NER ( denoted as KEYPHRASE ) yielded improvements over various translation directions and answer comparison methods .
The average numbers of question / answer pairs obtained by this method is shown in Table 5 .
It increased by the factor of 4 ( approximately ) .
Together with the CHRF metric used for answer comparison , it forms the bestperforming configuration of the proposed metric .
We also include its results in Table 3 .
From now on , we will report our results using this variant .
See Appendix
A for examples of usage of different evaluation methods .
Non-English Reference
So far , all the experiments were conducted for the translations directions into English .
This is given by the limitation of the T5 model which was trained on English data and most importantly by the SQuADv1 dataset which was used for fine-tuning and which is in English .
To overcome that , we used the multilingual mT5 model ( Xue et al. , 2021 ) and fine- tuned it on machine translation of SQuADv1 dataset into German by and into Czech by ( Mackov? and Straka , 2020 ) .
The results for English - Czech and English - German are included in both Table 5 : Average number of questions generated from a single segment in the newstest2020 reference file by the baseline system ( fine-tuned T5 ) and the keyphrase extraction method ( POS pattern matching and NER ) .
The average is computed over all to -English directions .
Tables 3 and 4 . Overall , MTEQA still performs very well .
It is better than the traditional metrics ( SENTBLEU , BLEU ) and also YISI - 2 and comparable with PRISM for English - German .
However , it is substantially worse than PRISM for English - Czech .
Given the fact , that the system is multilingual and fine-tuned on machine - translated data , the results are encouraging and open doors for a crosslingual setting which would not require reference translations .
Comparison with MQM Scores Recently , Freitag et al. ( 2021 ) demonstrated that the WMT DA method traditionally used for human evaluations has actually lower correlation with expert-based labels than the Multidimensional Quality Metrics ( MQM ) scoring method developed in the EU QTLaunchPad and QT21 projects .
To provide a more complete picture of the performance of the proposed MTEQA metric , we also report correlation with the MQM assessments .
Table 6 presents the system-level Pearson correlation of the proposed metric with both the MQM and DA labels for 8 systems that were re-annotated by Freitag et al . ( 2021 ) and are available from GitHub 5 .
The results are surprising and to a large extent unintuitive .
Metrics performing well in comparison with MQM are bad in comparison with DA .
This issue was already discussed by Freitag et al . ( 2021 ) and we leave deeper analysis of the difference for the future when MQM labels will be available for more data and for more translation directions .
Conclusions
In this paper we introduced a new metric for automatic evaluation of Machine Translation systems .
We showed that the degree to which the MT output can be used to answer questions about the reference can be used as a proxy to evaluate the translation quality .
We proved that our metric is robust by conducting experiments over multiple translation directions .
We examined a linguistically motivated way of extracting key phrases from the sentence and showed that it boosts the final performance .
We checked the influence of various word-level comparison metrics used to compare the test and goldstandard answers , and reported how it affects the correlation with human scores .
In our work , we focused on translation directions into English .
The only limiting factor in applying our metric to other translation directions is the availability of Question Generation and Question Answering systems in a given language .
However , automatic translation of SQuAD can be an effective way to obtain data for training such systems .
Finally , we examined the performance against the MQM labels and compared the performance against the DA labels .
While for the DA labels our metric performs close to state - of - the - art solutions , for the MQM labels there is a noticeable drop in performance .
In the future , we plan to examine the crosslingual approach - instead of generating questions and answers from the reference , one may instead use the source directly .
Figure 1 : 1 Figure1 : An illustration of the MTEQA pipeline .
One of the MT answers is clearly wrong , one is correct but the other differs with just a single character , raising a question about the choice of the answer-comparison metric .
Table 1 : 1 The Panthers finished the regular season with a 15 - 1 record ... DET NOUN ADP NOUN the application of electricity Tesla theorized that the application of electricity to the brain ...
Examples of the most frequent POS patterns of gold-standard answers in the XQuAD dataset .
Pattern Extracted Answer Sentence NOUN Coldplay ... the British rock group Coldplay with special guest performers ... ADJ NOUN natural grass
As is customary for Super Bowl games played at natural grass stadiums ... DET NOUN a fumble ... including a fumble which they recovered for a touchdown ... NUM NOUN 10 times
The South Florida / Miami area has previously hosted the event 10 times ... PROPN PROPN Carolina Panthers ... the National Football Conference ( NFC ) champion Carolina Panthers ... DET ADJ NOUN
A professional fundraiser A professional fundraiser will aid in finding business sponsors ... DET VERB NOUN a broken arm ... went down with a broken arm in the NFC Championship Game ... NUM PUNCT NUM 15 -1 BLEU ROUGE-L F1 Question Answering - - 90.27 Question Generation 21.01 43.25 -
Table 2 : 2 Performance of the baseline model used in our experiments on the development set of SQuADv1 .
Table 3 : 3 System-level Pearson correlation for selected metrics used for measuring MT quality with DA human assessment over MT systems using the newstest2020 references .
Average ( avg ) is computed over all to -English directions available .
Number below the language pair indicates the number of systems considered .
Figures without cs-en de-en zh-en en-de en-cs 12 12 16 avg 14 12 MTEQA F1 0.782 * 0.997 * 0.952 * 0.893 * 0.946 * 0.845 * MTEQA CHRF KEYPHRASE 0.890 * 0.998 * 0.951 * 0.905 * 0.952 * 0.859 * SENTBLEU 0.844 0.978 0.948 0.859 0.934 0.840 BLEU 0.851 0.985 0.956 0.854 0.928 0.825 PRISM 0.818 0.998 0.957 0.880 0.958 0.949 YISI -2 0.764 0.988 0.964 0.821 0.899 0.714 * are taken fromMathur et al . ( 2020 a ) .
Table 4 : 4 System-level Pearson correlation for various variants of the proposed metric with DA human assessment over MT systems using the newstest2020 references .
Average is computed over all to -English directions available .
cs- en de-en zh-en ja-en ru-en ps-en en-de en-cs avg 14 12 MTEQA F1 0.782 0.997 0.952 0.982 0.908 0.982 0.893 0.946 0.845 MTEQA CHRF 0.796 0.996 0.959 0.982 0.901 0.980 0.887 0.950 0.815 MTEQA BLEU 0.762 0.998 0.954 0.983 0.925 0.985 0.894 0.957 0.840 MTEQA EXACT 0.762 0.998 0.954 0.966 0.910 0.986 0.883 0.950 0.874 MTEQA F1 OUT 0.808 0.998 0.949 0.980 0.917 0.984 0.891 - - MTEQA CHRF OUT 0.835 0.997 0.957 0.979 0.910 0.986 0.891 - - MTEQA BLEU OUT 0.809 0.998 0.950 0.981 0.929 0.984 0.896 - - MTEQA EXACT OUT 0.827 0.999 0.948 0.969 0.902 0.983 0.884 - - MTEQA F1 KEYPHRASE 0.851 0.998 0.944 0.978 0.930 0.986 0.896 0.941 0.877 MTEQA CHRF KEYPHRASE 0.890 0.998 0.951 0.978 0.927 0.981 0.905 0.952 0.859 MTEQA BLEU KEYPHRASE 0.844 0.998 0.939 0.973 0.945 0.991 0.900 0.943 0.873 MTEQA EXACT KEYPHRASE 0.858 0.997 0.938 0.959 0.936 0.990 0.893 0.948 0.915 MTEQA F1 OUT KEYPHRASE 0.831 0.998 0.942 0.978 0.914 0.992 0.893 - - MTEQA CHRF OUT KEYPHRASE 0.851 0.998 0.947 0.977 0.917 0.990 0.902 - - MTEQA BLEU OUT KEYPHRASE 0.842 0.998 0.938 0.971 0.913 0.990 0.895 - - MTEQA EXACT OUT KEYPHRASE 0.838 0.998 0.936 0.960 0.918 0.992 0.887 - -
Table 6 : 6 System-level Pearson correlation for selected metrics used for measuring MT quality with the DA and MQM labels , computed for the newstest2020 references and the 8 MT systems re-annotated byFreitag et al . ( 2021 ) .
zh-en en-de MQM DA MQM DA MTEQA CHRF KEYPHRASE 0.630 0.818 0.761 0.394 PRISM 0.778 0.351 0.989 0.607 COMET 0.889 0.188 0.965 0.628 PARBLEU 0.380 0.565 0.722 0.218 CHRF 0.523 0.579 0.853 0.576 TER 0.352 0.511 0.810 0.477
https://github.com/mjpost/sacrebleu
https://github.com/patil-suraj/ question_generation
cs , de , ja , pl , ru , ta , zh , iu , km , ps ? en
https://github.com/google/ wmt-mqm-human-evaluation
