title
Scheduled Sampling Based on Decoding Steps for Neural Machine Translation
abstract
Scheduled sampling is widely used to mitigate the exposure bias problem for neural machine translation .
Its core motivation is to simulate the inference scene during training by replacing ground - truth tokens with predicted tokens , thus bridging the gap between training and inference .
However , vanilla scheduled sampling is merely based on training steps and equally treats all decoding steps .
Namely , it simulates an inference scene with uniform error rates , which disobeys the real inference scene , where larger decoding steps usually have higher error rates due to error accumulations .
To alleviate the above discrepancy , we propose scheduled sampling methods based on decoding steps , increasing the selection chance of predicted tokens with the growth of decoding steps .
Consequently , we can more realistically simulate the inference scene during training , thus better bridging the gap between training and inference .
Moreover , we investigate scheduled sampling based on both training steps and decoding steps for further improvements .
Experimentally , our approaches significantly outperform the Transformer baseline and vanilla scheduled sampling on three large-scale WMT tasks .
Additionally , our approaches also generalize well to the text summarization task on two popular benchmarks .
Introduction Neural Machine Translation ( NMT ) has made promising progress in recent years ( Sutskever et al. , 2014 ; Bahdanau et al. , 2015 ; Vaswani et al. , 2017 ) .
Generally , NMT models are trained to maximize 1 To calculate the precision for training , we strictly match predicted tokens with ground - truth tokens word by word .
When inference , we relax the strict matching to the fuzzy matching within a local window of size 3 , and truncate or pad hypotheses to the same length of golden references .
We also explore n-gram matching in preliminary experiments and observe analogical results with different n.
For simplicity , we use the above unigram matching to calculate the translation precision ( similarly for the error rate ) in all experiments .
the likelihood of next token given previous golden tokens as inputs , i.e. , teacher forcing ( Salakhutdinov , 2014 ) .
However , at the inference stage , golden tokens are unavailable .
The model is exposed to an unseen data distribution generated by itself .
This discrepancy between training and inference is named as the exposure bias problem ( Ranzato et al. , 2016 ) .
With the growth of decoding steps , such discrepancy becomes more problematic due to error accumulations Zhang et al. , 2020a ) ( shown in Figure 1 ) .
Many techniques have been proposed to alleviate the exposure bias problem .
To our knowledge , they mainly fall into two categories .
The one is sentencelevel training , which treats the sentence - level metric ( e.g. , BLEU ) as a reward , and directly maximizes the expected rewards of generated sequences ( Ranzato et al. , 2016 ; Shen et al. , 2016 ; Rennie et al. , 2017 ; Pang and He , 2021 ) .
Although intuitive , they generally suffer from slow and unstable training due to the high variance of policy gradients and the credit assignment problem ( Sutton , 1984 ; Wiseman and Rush , 2016 ; Liu et al. , 2018 ; Wang et al. , 2018 ) .
Another category is sampling - based approaches , aiming to simulate the data distribution of the inference scene during training .
Scheduled sampling is a representative method , which samples tokens between golden references and model predictions with a scheduled probability .
further refine the sampling candidates by beam search .
Mihaylova and Martins ( 2019 ) and Duckworth et al . ( 2019 ) extend scheduled sampling to the Transformer with a novel two -pass decoder architecture .
Liu et al . ( 2021 ) develop a more fine- grained sampling strategy according to the model confidence .
Although these sampling - based approaches have been shown effective and training efficient , there still exists an essential issue in their sampling strategies .
In the real inference scene , the nature of sequential predictions quickly accumulates errors along with decoding steps , which yields higher error rates for larger decoding steps Zhang et al. , 2020a ) ( Figure 1 ) .
However , most sampling - based approaches are merely based on training steps and equally treat all decoding steps 2 . Namely , they simulate an inference scene with uniform error rates along with decoding steps , which is inconsistent with the real inference scene .
To alleviate this inconsistent issue , we propose scheduled sampling methods based on decoding steps , which increases the selection chance of predicted tokens with the growth of decoding steps .
In this way , we can more realistically simulate the inference scene during training , thus better bridging the gap between training and inference .
Furthermore , we investigate scheduled sampling based on both training steps and decoding steps , which yields further improvements .
It indicates that our proposals are complementary with existing studies .
Additionally , we provide in - depth analyses on the necessity of our proposals from the perspective of translation error rates and accumulated errors .
Experimentally , our approaches significantly outperform the Transformer baseline by 1.08 , 1.08 , and 1.27 BLEU points on WMT 2014 English - German , WMT 2014 English - French , and WMT 2019 Chinese -English , respectively .
When comparing with the stronger vanilla scheduled sampling method , our approaches bring further improvements by 0.58 , 0.62 , and 0.55 BLEU points on these WMT tasks , respectively .
Moreover , our approaches generalize well to the text summarization task and achieve consistently better performance on two popular benchmarks , i.e. , CNN / DailyMail ( See et al. , 2017 ) and Gigaword ( Rush et al. , 2015 ) .
The main contributions of this paper can be summarized as follows 3 : ?
To the best of our knowledge , we are the first that propose scheduled sampling methods based on decoding steps from the perspective of simulating the distribution of real translation errors , and provide in - depth analyses on the necessity of our proposals .
?
We investigate scheduled sampling based on both training steps and decoding steps , which yields further improvements , suggesting that our proposals complement existing studies .
?
Experiments on three large-scale WMT tasks and two popular text summarization tasks confirm the effectiveness and generalizability of our approaches .
?
Analyses indicate our approaches can better simulate the inference scene during training and significantly outperform existing studies .
2 Background
Neural Machine Translation Given a pair of source language X = {x 1 , x 2 , ? ? ? , x m } and target language Y = {y 1 , y 2 , ? ? ? , y n } , neural machine translation aims to model the following translation probability : P ( Y|X ) = n t=1 log P ( y t |y <t , X , ? ) ( 1 ) where t is the index of target tokens , y <t is the partial translation before y t , and ? is model parameter .
In the training stage , y <t are ground - truth tokens , and this procedure is also known as teacher forcing .
The translation model is generally trained with maximum likelihood estimation ( MLE ) .
Scheduled Sampling for the Transformer Scheduled sampling is initially designed for Recurrent Neural Networks , and further modifications are needed when applied to the Transformer ( Mihaylova and Martins , 2019 ; Duckworth et al. , 2019 ) .
As shown in Figure 2 , we follow the two -pass decoder architecture for the training of Transformers .
In the first pass , the
Self-Attention Feed Forward
Cross-Attention Self-Attention Feed Forward
Cross-Attention
Source Input Target Input ( Golden )
Self-Attention Feed Forward Target Input ( Golden + Predictions ) Output Probabilities Output Probabilities model conducts the same as a standard NMT model .
Its predictions are used to simulate the inference scene 4 .
In the second pass , the decoder 's inputs y <t are sampled from predictions of the first pass and ground - truth tokens with a certain probability .
Finally , predictions of the second pass are used to calculate the cross-entropy loss , and Equation ( 1 ) is modified as follow : P ( Y|X ) = n t=1 logP ( y t | y <t , X , ? ) ( 2 ) Note that the two decoders are identical and share the same parameters during training .
At inference , only the first decoder is used , that is just the standard Transformer .
How to schedule the above probability of sampling tokens for training is the key point , which is we aim to improve in this paper .
Decay Strategies Based on Training Steps Existing schedule strategies are based on training steps .
At the i-th training step , the probability of sampling golden tokens f ( i ) is calculated as follow : ? Linear Decay : f ( i ) = max ( , ki + b ) , where is the minimum value , and k < 0 and b is respectively the slope and offset of the decay .
? Exponential Decay : f ( i ) = k i , where k < 1 is the radix to adjust the decay .
? Sigmoid Decay 5 : f ( i ) = k k+e i k , where e is the mathematical constant , and k ?
1 is a hyperparameter to adjust the decay .
We draw some examples for different decay strategies based on training steps in Figure 3 . 3 Approaches
Definitions and the Overview
At the training stage , in the input of the secondpass decoder , each token is sampled either from the golden token or the predicted token by the first - pass decoder .
For clarity , we only define the probability of sampling golden tokens , e.g. , f ( i ) , and use 1 ? f ( i ) to represent the probability of sampling predicted tokens .
Specifically , we define the probability of sampling golden tokens as f ( i ) when sampling based on the training step i , as g ( t ) when sampling based on the decoding step t , and as h( i , t ) when sampling based on both training steps and decoding steps .
In this paper , when we mention a scheduled strategy , it is about the probability of sampling golden tokens at the model training stage .
In this section , we firstly point out the drawback of merely sampling based on training steps .
Secondly , we describe how to appropriately sample based on decoding steps .
Finally , we explore whether sampling based on both training steps and decoding steps can complement each other .
Sampling Based on Training Steps
As the number of the training step i increases , the model should be exposed to its own predictions more frequently .
Thus a decay strategy for sampling golden tokens f ( i ) ( in Section 2.3 ) is generally used in existing studies .
At a specific training step i , given a target sentence , f ( i ) is only related to i and equally conducts the same sampling probability for all decoding steps .
Therefore , f ( i ) simulates an inference scene with uniform error rates and still remains a gap with the real inference scene .
Sampling Based on Decoding Steps
We take a further step to bridge the above gap f ( i ) left .
Specifically , we propose sampling based on decoding steps and schedule the sampling probability g ( t ) under the guidance of real translation errors .
As mentioned earlier ( Figure 1 ) , translation error rates are growing rapidly along with decoding steps in the real inference stage .
To more realistically simulate such error distributions of the real inference scene during training , we expose more model predictions for larger decoding steps and more golden tokens for smaller decoding steps .
Thus it is intuitive to apply a decay strategy for sampling golden tokens based on the number of decoding steps t. Specifically , we directly inherit above decay strategies ( Section 2.3 ) for training steps f ( i ) to g ( t ) with a different set of hyperparameters ( listed in Table 2 ) .
To rigorously validate the necessity and effectiveness of our proposals , we further conduct the following method variants for comparisons : ? Always Sampling :
This model always samples from its own predictions .
? Uniform Sampling :
This model randomly samples golden tokens with a uniform probability ( 0.5 in our experiments ) .
? Increase Strategies :
These models reverse decay strategies to increase strategies , i.e. , g( t ) ? 1 ? g ( t ) .
We draw some representative strategies 6 in Figure 4 . Both ' Always Sampling ' ( blue line ) and ' Uniform Sampling ' ( green line ) parallel to the x-axis , namely irrelevant with t .
They serve as baseline models to verify whether a scheduled strategy is necessary on the dimension of t.
The exponential decay ( solid red line ) shows a similar trend with the real error rate ( black line ) : the larger decoding steps and the higher error rates .
On the other hand , the exponential increase ( dashed red line ) is entirely contrary to the real error rate .
However , we cannot take it for granted that the exponential increase is inappropriate , as it can still simulate the error accumulation phenomenon 7 .
Therefore , merely comparing error rates is not enough .
We need to step deeper into the dimension of error accumulations for further comparisons .
Error Accumulations .
At the decoding step t , the number of accumulated errors accum ( t ) is the definite integral of the probability of sampling model predictions 1 ? g ( t ) : accum ( t ) = t 0 ( 1 ? g ( x ) ) dx ( 3 ) As shown in Figure 5 , accum ( t ) is a monotonically increasing function , which can simulate the error accumulation phenomenon no matter which kind of scheduled strategy g ( t ) during training .
Nevertheless , we observe that different strategies show different speeds and distributions for simulating error accumulations .
For instance , decay strategies ( solid lines ) show a slower speed at the beginning of decoding steps and then rapidly accumulate errors with the growth of decoding steps , Figure 6 : Examples for different h( i , t ) .
The wavelengths of colors represent the probability of sampling golden tokens .
Namely , the closer the color to the red , the greater the probability .
Red circles are for the sake of highlights .
which is analogous with the real inference scene ( black line ) .
However , increase strategies ( dashed lines ) are just on the contrary .
They simulate a distribution with lots of errors at the beginning and an almost fixed number of errors in following decoding steps .
Moreover , although different decay strategies show similar trends for simulating error accumulations in the training stage , the degrees of their approximations with real error numbers are still different .
We will further validate whether the proximity is closely related to the final performance in Section 5.1 .
Sampling Based on Both Training Steps and Decoding Steps
When comparing above two types of approaches , i.e. , f ( i ) and g ( t ) , our approach g ( t ) focus on simulating the distribution of real translation errors , and the vanilla f ( i ) emphasizes the competence of the current model .
Thus it is intuitive to verify whether f ( i ) and g ( t ) complement each other .
How to combine them is the critical point .
At the training step i and decoding step t , we define the probability of sampling golden tokens h( i , t ) by the following joint distribution function : ?
Product : h( i , t ) = f ( i ) ? g( t ) ? Arithmetic Mean : h( i , t ) = f ( i ) + g( t ) 2 ? Composite 8 : h( i , t ) = g( t ? ( 1 ? f ( i ) ) )
One simple solution ( ' Product ' ) is to directly multiply f ( i ) and g ( t ) .
However , both f ( i ) and g ( t ) are less than or equal to 1 , thus their product quickly shrinks to a tiny value close to 0 .
Consequently , it exposes too few golden tokens and too many predicted tokens to the model ( Figure 6 ( a ) ) , which 2017 ) , and ( b ) Gigaword corpus ( Rush et al. , 2015 ) .
We list dataset statistics for all datasets in Table 1 .
Implementation Details Training Setup .
For the translation task , we follow the default setup of the Transformer base and Transformer big models ( Vaswani et al. , 2017 ) , and provide detailed setups in Appendix A ( Table 7 ) .
All Transformer models are first trained by teacher forcing with 100k steps , and then trained with different training objects or scheduled sampling approaches for 300k steps .
All experiments are conducted on 8 NVIDIA V100 GPUs , where each is allocated with a batch size of approximately 4096 tokens .
For the text summarization task , we base on the ProphetNet ( Qi et al. , 2020 ) and follow its training setups .
We set hyperparameters involved in various scheduled sampling strategies ( i.e. , f ( i ) and g ( t ) ) according to the performance on validation sets of each tasks and list k in Table 2 .
For the linear decay , we set and b to 0.2 and 1 , respectively .
Please note that scheduled sampling is only used during training instead of the inference stage .
Evaluation .
For the machine translation task , we set the beam size to 4 and the length penalty to 0.6 during inference .
We use multibleu .
perl to calculate cased sensitive BLEU scores for EN - DE and EN - FR , and use mteval - v13a.pl script to calculate cased sensitive BLEU scores for ZH-EN .
We use the paired bootstrap resampling methods ( Koehn , 2004 ) to compute the statistical significance of translation results .
We report mean and standard - error variation of BLEU scores over three runs .
For the text summarization task , we respectively set the beam size to 4/5 and length penalty to 1.0/1.2 for Gigaword and CNN / DailyMail dataset following previous studies ( Song et al. , 2019 ; Qi et al. , 2020 ) .
We report the F1 scores of ROUGE -1 , ROUGE - 2 , and ROUGE -L for both datasets .
Systems Mixer TeaForN .
Teacher forcing with n-grams ( Goodman et al. , 2020 ) enables the standard teacher forcing with a broader view by a n-grams optimization .
Sampling based on training steps .
For distinction , we name vanilla scheduled sampling as Sampling based on training steps .
We defaultly adopt the sigmoid decay following .
Sampling with sentence oracles .
refine the sampling candidates of scheduled sampling with sentence oracles , i.e. , predictions from beam search .
Note that its sampling strategy is based on training steps with the sigmoid decay .
Sampling based on decoding steps .
Sampling based on decoding steps with exponential decay .
Sampling based on training and decoding steps .
Our sampling based on both training steps and decoding steps with the ' Composite ' method .
Main Results Machine Translation .
We list translation qualities on three WMT tasks in ( Vaswani et al. , 2017 ) 27.30 - 38.10
Transformer base ( Vaswani et al. , 2017 ) ( Vaswani et al. , 2017 ) 28.40 - 41.80
Transformer big ( Vaswani et al. , 2017 ) show better translation qualities while preserving efficient training .
TeaForN also yields competitive translation qualities due to its long-term optimization .
Among all existing methods , our ' Sampling based on decoding steps ' shows consistent improvements on various datasets .
Moreover , ' Sampling based on training and decoding steps ' combines the advantages of both existing methods and our proposals , and achieves better performance .
Specifically for the Transformers base , it brings significant improvements by 1.08 , 1.08 , and 1.27 BLEU points on EN - DE , ZH-EN , and EN - FR , respectively .
Moreover , it significantly outperforms vanilla scheduled sampling by 0.58 , 0.62 , and 0.55 BLEU points on these tasks , respectively .
For the more powerful Transformers big , we observe similar experimental conclusions as above .
Specifically , ' Sampling based on training and decoding steps ' significantly outperforms the Transformers big by 1.26 , 0.88 and 1.24 BLEU points on EN - DE , ZH-EN , and EN - FR , respectively .
Text Summarization .
In Table 4 , we list F1 scores of ROUGE - 1 / ROUGE - 2 / ROUGE -L on test sets of both text summarization datasets .
We take the powerful ProphetNet large as our primary baseline 9 and apply different sampling - based approaches .
For vanilla scheduled sampling ( second last row of Table 4 ) , we observe marginal improvements on Gigaword and even degenerations on CNN / DailyMail .
We speculate that poor performance comes from their uniform sampling rate along with decoding steps , which violates the distribution of the real inference scene .
Namely , the model is overexposed to golden tokens and underexposed to predicted tokens at larger decoding steps .
Especially for CNN / DailyMail , its averaged target sequence length exceeds 64 , and more than 90 % of sentences are longer than 50 , which exacerbates the above issue in existing samplingbased approaches .
We further analyze the effects of different sampling approaches on different sequence lengths in Section 5.3 .
Nevertheless , our approaches are not affected by the above issue and show consistent improvements in all criteria of both Model RG - 1 / RG - 2 / RG -L CNN / DailyMail Gigaword RoBERTSHARE large ( Rothe et al. , 2020 ) 40.31 / 18.91 / 37.62 38.62 / 19.78 / 35.94 MASS ( Song et al. , 2019 42.12 / 19.50 / 39.01 38.73 / 19.71 / 35.96 UniLM ( Dong et al. , 2019 ) 43.33 / 20.21 / 40.51 38.45 / 19.45 / 35 .75 PEGASUS large ( Zhang et al. , 2020 b ) 44.17 / 21.47 / 41.11 39.12 / 19.86 / 36.24 PEGASUS large + TeaForN ( Goodman et al. , 2020 ) 44.20 / 21.70 / 41.32 39.16 / 20.16 / 36 .54 ERNIE-GEN large ( Xiao et al. , 2020 ) 44.31 / 21.35 / 41.60 39.46 / 20.34 / 36.74 BART +R3F ( Aghajanyan et al. , 2020 datasets .
Specifically , our approaches achieve consistently better performance than the baseline system on both datasets , and significantly improve the previous SOTA on ROUGE -L score of Gigaword to 37.24 ( + 0.5 ) .
In conclusion , the strong performance on the text summarization task indicates that our approaches have a good generalization ability across different tasks .
Analysis and Discussion
In this section , we provide in - depth analyses on the necessity of our proposals and conducts experiments on the validation set of WMT14 EN - DE with the Transformer base model .
Effects of Scheduled Strategies
In this section , we focus on the effects of different scheduled strategies based on the decoding step t , and aim to answer the following two questions : ( a) Is a Scheduled Strategy is Necessary ?
We take the Transformer without sampling as the baseline , then respectively apply ' Always Sampling ' , ' Uniform Sampling ' , and our ' Exponential Decay ' .
Results are listed in the part ( a ) of Table 5 .
We observe a noticeable drop when conducting ' Always Sampling ' , as the model is entirely exposed to its predictions and fails to converge fully .
As to ' Uniform Sampling ' , it is essentially a simulation of the vanilla ' Sampling based on Training Steps ' .
Although ' Uniform Sampling ' conducts an inappropriate sampling strategy , it still can simulate the data distribution of the inference scene to some extent and bring BLEU improvements modestly .
In propagated to subsequent decoding steps , which hinders the final performance .
On the contrary , all decay strategies bring consistent improvements with different degrees .
Moreover , we observe that the more a decay strategy approximates real error numbers ( Figure 5 ) , the more performance improvements .
In summary , we need to apply decay strategies instead of increase strategies based on decoding steps in the perspective of simulating real error accumulations .
Effects of Different h( i , t ) Strategies
We take our strong ' Sampling based on decoding steps ' as the baseline and then apply different combination methods h( i , t ) .
As shown in Table 6 , the performance drop of ' Product ' and ' Arithmetic Mean ' confirms our speculation in Section 3.4 .
Namely , the model is overexposed to its predictions at the beginning of training steps and decoding steps , thus fails to converge well .
In contrast , ' Composite ' brings certain improvements over the strong baseline model .
Since it stabilizes the model training and successfully combines the advantages of both dimensions of training steps and decoding steps .
In summary , a well - designed strategy is necessary when combining both f ( i ) and g ( t ) , and we provide an effective alternative ( i.e. , ' Composite ' ) .
Effects on Different Sequence Lengths
According to our early findings , the exposure bias problem gets worse as the sentence length grows .
Thus it is intuitive to verify whether our approaches improve translations of long sentences .
Since the size of WMT14 EN - De validation set ( 3 k ) is too small to cover scenarios with various sentence lengths , we randomly select training data with different sequence lengths .
Specifically , we divide WMT14 EN - DE training data into ten bins according to the source side 's sentence length .
The maximal length is 100 , and the interval size is 10 .
Then we randomly select 1000 sentence pairs from each bin and calculate BLEU scores for different approaches .
Specifically , we take the Transformer as the baseline , and draw absolute BLEU gains of scheduled sampling on training steps and decoding steps .
As shown in Figure 7 , BLEU gains of the vanilla scheduled sampling are relatively uniform over different sentence lengths .
In contrast , BLEU gains of our scheduled sampling on decoding steps gradually increase with sentence lengths .
Moreover , our approach consistently outperforms the vanilla one at most sentence length intervals .
Specifically , we observe more than 1.0 BLEU improvements when sentence lengths in [ 80 ; 100 ] .
Conclusion
In this paper , we propose scheduled sampling methods based on decoding steps from the perspective of simulating real translation error rates , and provide in - depth analyses on the necessity of our proposals .
We also confirm that our proposals are complementary with existing studies ( based on training steps ) .
Experiments on three large-scale WMT translation tasks and two text summarization tasks confirm the effectiveness of our approaches .
In the future , we will investigate low resource settings which may suffer from a more serious error accumulation problem .
In addition , more autoregressive - based tasks would be explored as future work .
Figure 1 : 1 Figure 1 : The translation precision for training ( blue line ) and inference ( red line ) at each decoding step .
The gap between training and inference ( black line ) increases rapidly with the growth of decoding steps .
We randomly sample 100k training data from WMT 2014 EN - DE and report the average precision of 1 k tokens for each decoding step 1 .
