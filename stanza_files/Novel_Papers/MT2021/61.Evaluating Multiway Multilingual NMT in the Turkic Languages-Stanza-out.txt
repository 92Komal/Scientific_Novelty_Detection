title
Evaluating Multiway Multilingual NMT in the Turkic Languages
abstract
Despite the increasing number of large and comprehensive machine translation ( MT ) systems , evaluation of these methods in various languages has been restrained by the lack of high-quality parallel corpora as well as engagement with the people that speak these languages .
In this study , we present an evaluation of state - of - the - art approaches to training and evaluating MT systems in 22 languages from the Turkic language family , most of which being extremely under-explored ( Joshi et al. , 2019 ) .
First , we adopt the TIL Corpus ( Mirzakhalov et al. , 2021 ) with a few key improvements to the training and the evaluation sets .
Then , we train 26 bilingual baselines as well as a multi-way neural MT ( MNMT ) model using the corpus and perform an extensive analysis using automatic metrics as well as human evaluations .
We find that the MNMT model outperforms almost all bilingual baselines in the out-of- domain test sets and finetuning the model on a downstream task of a single pair also results in a huge performance boost in both low - and high- resource scenarios .
Our attentive analysis of evaluation criteria for MT models in Turkic languages also points to the necessity for further research in this direction .
We release the corpus splits , test sets as well as models to the public 1 .
Name Codes Speakers Data MT ?
English en , eng 400.0M 38.6 M Russian ru , rus 258.0M 23.3 M
Turkish tr , tur 85.0M 52.6 M
Kazakh kk , kaz 13.2 M 5.3 M
Uzbek uz , uzb 27.0 M 2.9M Azerbaijani az , aze 23.0M 2.2M
Tatar tt , tat 5.2M
Introduction
The last few years have seen encouraging advances in low-resource MT development with the increasing availability of public multilingual corpora ( Agi? and Vuli? , 2019 ; Ortiz Su?rez et al. , 2019 ; Schwenk et al. , 2019 ; Tiedemann , 2020 ; Goyal et al. , 2021 ; and more inclusive multilingual MT models ( Arivazhagan et al. , 2019 ; Tiedemann and Thottingal , 2020 ; Fan et al. , 2020 ) .
In this study , we take the 1 https://github.com/ turkic-interlingua/til-mt
Turkic language family into focus , which has not been studied at large in MT research ( detailed review in Section 2 ) .
Most recently , in a wide evaluation of translation between hundreds of languages with a multilingual model ( M2 M - 124 ) trained on large web-mined parallel data , translation into , from , and between Turkic languages was shown to be very challenging compared to other language families ( Goyal et al. , 2021 ) .
With the promise of strong transfer capabilities of multilingual models especially for related languages , we hope that the inclusion of a wider set of Turkic languages into a joint model can unlock automatic translation even for the very low-resourced Turkic languages where no prior translation models exist ( Koehn , 2005 ; Choudhary and Jha , 2011 ; Post et al. , 2012 ; Nomoto et al. , 2018 ; Espl ?- Gomis et al. , 2019 ; . To this aim , we adopt the TIL Corpus ( Mirzakhalov et al. , 2021 ) compiled by the Turkic Inter-Table 1 : ( The table indicates the language codes used for the Turkic languages along with the number of L1 speakers , amount of available data ( in sentences ) in our corpus .
The column MT ?
indicates if there are currently available online machine translation systems for the language .
K : thousand , M : million . ) lingua 2 community ( Mirzakhalov , 2021 ) including X - WMT test sets with a few key improvements ( Section 3 ) .
We train a multi-way NMT ( MNMT ) model on the entire parallel corpus , which constitutes the first large-scale multilingual translation model specifically for Turkic languages ( Section 4 ) .
We perform an extensive analysis of the strengths and weaknesses of this model , comparing it to the bilingual baselines and evaluating it under a domain shift .
We find that the MNMT model outperforms almost all bilingual baselines in the out-of- domain tests while it performs comparably or underperforms in the in-domain tests .
We further analyze its capacity for transfer learning by fine-tuning the model on several language pairs all of which experience gains , both in - and out - of- domain scenarios .
In addition , we complement the automatic evaluation with a human evaluation study for multiple languages ( Section 5 ) , gaining insights into types of common mistakes that the model makes and the suitability of different automatic metrics for Tur - 2 https://turkicinterlingua.org/ kic languages .
We plan on releasing the improved corpus , evaluation sets , and all the models to the public .
This work will not only enrich the landscape of languages currently considered in MT research and spur future research on NLP for Turkic languages but will hopefully also inspire the building of new translation engines and derived technologies for populations with millions of native speakers ( Table 1 ) .
Related Work
This section discusses the previous work on MT of these languages including the available corpora and languages resources .
The 19 Turkic languages covered in the study are : Altai , Azerbaijani , Bashkir , Crimean Tatar , Chuvash , Gagauz , Karachay - Balkar , Karakalpak , Khakas , Kazakh , Kumyk , Kyrgyz , Sakha , Turkmen , Turkish , Tatar , Tuvan , Uyghur , and Uzbek .
There are several other widely spoken languages that are left out from our study such as Shor , Salar , Urum , Nogai , Khorasani Turkic , Qashqai , and Khalaj , due to the lack ( or very limited amount ) of any available parallel corpora .
Future work will focus on extending the corpus to these languages as well .
MT of Turkic Languages
The need for more comprehensive and diverse multilingual parallel corpora has sped up the creation of such large-scale resources for many language families and linguistic regions ( Koehn , 2005 ; Choudhary and Jha , 2011 ; Post et al. , 2012 ; Nomoto et al. , 2018 ; Espl ?- Gomis et al. , 2019 ; . Tiedemann ( 2020 ) released a large-scale corpus for over 500 languages covering thousands of translation directions .
The corpus currently includes 14 Turkic languages and provides bilingual baselines for all translation directions present in the corpus .
However , most of the 14 Turkic languages contain a few hundred or a dozen samples .
In addition , the varying and limited size of the test sets does not allow for the extensive analysis and comparisons between different model artifacts , linguistic features , and translation domains .
More recently , Goyal et al . ( 2021 ) extended the previous Flores benchmark by providing human translated evaluation sets for 101 languages , among which 5 of them are from the Turkic family : Azerbaijani , Kazakh , Kyrgyz , Turkish , and Uzbek .
Similarly , they train a large MNMT model and evaluate its performance using the benchmark .
A Russian- Turkic parallel corpus was curated for 6 different Turkic languages , and their bilingual baselines have been reported for both directions using different NMT - based approaches Khusainov et al . ( 2020 ) .
However , the dataset , test sets , and models are not released to the public which limits its use to serve as a comparable benchmark .
Additionally , a rule- based MT framework for Turkic languages has been presented with 4 language pairs Alk ? m and C ?ebi ( 2019 ) .
Also , several rule- based MT systems have been built for Turkic languages which are publicly available through the Apertium 3 website .
For individual languages in our corpus , there are several proposed MT systems and linguistic resources : Azerbaijani ( Hamzaoglu , 1993 ; Fatullayev et al. , 2008 ) , Bashkir ( Tyers et al. , 2012 , Crimean Tatar Alt?ntas ? , 2001 ) , Karakalpak ( Kadirov , 2015 ) , Kazakh ( Assylbekov and Nurkas , 2014 ; Sundetova et al. , 2015 ; Littell et al. , 2019 ; Briakou and Carpuat , 2019 ; Tukeyev et al. , 2019 ) , Kyrgyz ( C ?etin and Ismailova ) , Sakha , Turkmen ( Tantug and Adal ? , 2018 ) , Turkish ( Turhan , 1997 ; El - Kahlout and Oflazer , 2006 ; Bisazza and Federico , 2009 ; Tantug et al. , 2011 ; Ataman et al. , 2017 ) , Tatar ( Salimzyanov et al. , 2013 ; Khusainov et al. , 2018 ; Valeev et al. , 2019 ; , Tuvan ( Killackey , 2013 ) , Uyghur ( Mahsut et al. , 2004 ; Nimaiti and Izumi , 2012 ; Song and Dai , 2015 ; Wang et al. , 2020 ) , and Uzbek ( Axmedova et al. , 2019 ) .
Yet to our knowledge , there has not been a study that covers Turkic languages to such a large extent as ours , both in terms of multilingual parallel corpora and multiway NMT benchmarks across these languages .
TIL Corpus
As we adopt the TIL Corpus as the training data , we perform a few key modifications to better the quality of the datasets .
First , we notice that the alignments for the Bible 4 and TedTalks 5 datasets were not optimal as most " sentences " were actually comprised of multiple sentences in order to preserve the quality of the alignment with target sequence .
For example , in the case of TedTalks , the original speech utterance may have been 2 - 3 sentences in text but the translation of that speech may end up differing by 1 or even more sentences depending on the translator .
Common practice in this situation , as seen through multiple corpora across OPUS 6 , is to leave the entire utterance as is to preserve the quality of the alignment even if the number of sentences do not match .
Instead , we drop the examples where the total number of sentences do not match and split ( and realign ) the cases where they do .
This naturally increased the overall number of sentence alignments in both the Bible and TedTalks corpora for all language pairs .
Second , we perform a corpus-wide length and length-ration filtering where we drop sentence pairs that are single words as well as the entries where source and target ratio is over 2 .
Third , we re-curate the in-domain evaluation sets following the improvements to the corpus .
Details on the evaluation sets are described further in Section 3.1 .
Curation of evaluation sets
The original TIL Corpus introduced three evaluation sets with different domains ( Bible , TedTalks , and X - WMT ) .
To simplify the analysis of the models , we re-curate the in-domain evaluation sets by randomly sampling from each corpora .
X - WMT is used as the out-of- domain test set since it is from the news domain with substantial amount of new words / terms that most of the language pairs lack .
The curation steps for the test sets are presented below .
In-domain Evaluation Sets
In-domain development and test sets are randomly sampled from each language pair and can serve as evaluation sets for both bilingual and multilingual models .
X- WMT Test Set X - WMT is a challenging and human-translated test set in the news domain based on the professionally translated test sets in English -Russian from the WMT 2020 Shared Task ( Mathur et al. , 2020 ) .
It was originally introduced in the TIL Corpus and we adopt the test sets as they are .
Currently , the test set extends into 8 Turkic languages ( Bashkir , Uzbek , Turkish , Kazakh , Kyrgyz , Azerbaijani , Karakalpak , and Sakha ) paired with English and Russian .
Table 2 highlights the currently available test set directions .
Bolded entries in the table indicate the original direction of the translation .
4 Experimental Setup
Bilingual Experiments
To serve as initial baselines , we train 26 bilingual baselines using the corpus and report the performance on the in- domain test set as well as the X - WMT set ( out - of- domain ) as described in Section 3.1.2 .
The selection of the language pairs was constricted by the availability of both in - domain and out - of- domain test sets to enable more meaningful insights from the experiments .
Model details
All models are Transformers ( transformer - base ) ( Vaswani et al. , 2017 b ) and are trained using the JoeyNMT framework ( Kreutzer et al. , 2019 ) .
In the preprocessing stage , we use Sacremoses for tokenization and apply byte pair encoding ( BPE ) ( Sennrich et al. , 2015 ; Dong et al. , 2015 ) with a joint vocabulary size of 4 k and 32 k .
Models use 512dimensional word embeddings and hidden layers and are trained with the Adam optimizer ( Kingma and Ba , 2015 ) .
A learning rate of 3 * 10 ?4 is applied along with a dropout rate of 0.3 .
We use a batch size of 4096 BPE tokens with 8 accumulations to simulate training on 8 GPU machines .
All mod- els , except English -Turkish and Turkish - English , are trained on Google Colab 's freely availably preemptible GPUs .
Multilingual Experiments
To examine the extent of transfer learning and generalization within our corpus , we train a multiway multilingual NMT model on the entire dataset covering almost 400 language directions .
We then compare the performance of the model on the indomain and out - of- domain test sets across a range of language pairs .
Data Preprocessing Similar to the bilingual data preprocessing , the entire corpus has been tokenized using Sacremoses 7 and samples longer than 300 words have been filtered out .
In addition , we perform cross-filtering of test and dev sets of all language pairs from the training corpus , as it is very necessary to do so in any MNMT model using a multiway corpus .
Since the corpus is relatively unbalanced , we perform a temperature - based sampling with a value of 1.25 .
Although a higher temperature value between 2 and 3 would further balance our corpus , it would increase the dataset size by 8x with t=2 and 25x with t=3 .
This increase would limit our ability to train the model due to the restrained compute resources .
Originally , the overall training set size is at around 133 million samples and this increases to 244 million after the sampling procedure .
We ap- ply the sentencepiece 8 implementation of the byte pair encoding ( BPE ) ( Sennrich et al. , 2016 ) with a joint vocabulary size of 64 k .
Following the method from Ha et al . ( 2016 ) and Johnson et al . ( 2017 ) , we prepend a target language token to the source sentences to enable many - to -many translation .
Model details
We train the model using the Transformer architecture in the transformer - base configuration .
More specifically , we use the transformer wmt en de version from Fairseq ( Ott et al. , 2019 ) implementation 9 with 6 layers both in the encoder and decoder .
Configuration of the model closely follows the original implementation of the Transformer ( Vaswani et al. , 2017a ) with the model dimension set at 512 and hidden dimension size at 2048 .
We apply a dropout rate of 0.3 , the learning rate of 5 * 10 ?4 , and warm - up updates of 40k .
The effective batch size is 16,384 BPE tokens .
The model is trained using 4 NVIDIA V100 GPU machines for a little over 1 million steps which takes about 36 - 48 hours .
Evaluation of Models Automatic evaluation metrics used to compare the performance of bilingual baselines and MNMT are token - based corpus BLEU ( Papineni et al. , 2002 ) and character - based Chrf ( Popovi ? , 2015 ) .
While corpus BLEU is the de-facto standard in MT ( Marie et al. , 2021 ) ,
Chrf might work better for morphologically rich languages because it can reward partially correct words .
We also report the MNMT model 's internal perplexity to better highlight the language pairs in which the model struggles most .
We evaluate the models on the in-domain and X - WMT evaluation sets .
The gap between scores on in- domain versus out - of- domain translations is particularly interesting since it gives us an estimate of domain robustness and generalization , as well as mimics a realistic shift from the training domain to the domain of interest for potential users or downstream applications .
Bilingual baselines vs MNMT
Table 3 shows all the results for the bilingual baselines and MNMT as evaluated on two test tests .
The first obvious trend in the table is the dominance of the bilingual baselines on the in- domain test sets as they overperform the MNMT model in most of the high - to mid-resource language pairs .
As the train size decreases , the results become more comparable in terms of BLEU and even better for MNMT when evaluated in Chrf .
When tested under a domain shift with the X - WMT set , MNMT results in gains across almost all pairs .
However , it is important to note that there is a noticeable performance drop that follows the domain shift as can be seen in Figures 1 and 2 .
This highlights a realistic phenomenon of generalization and sets an expectation of the model 's capabilities in real-world use cases .
Another observation in Table 3 is that all of the language pairs having fewer than 100k training samples ( 8 total ) in our bilingual baselines barely pass the mark of 1 BLEU score or 0.2 Chrf in the out-of- domain test .
However , in the MNMT setup , the average BLEU and Chrf score for those 8 low-resource pairs are 5.98 and 0.27 respectively .
While these scores indicate that these pairs are still extremely low in quality and potentially unusable in practice , gains are promising given the amount of resources and a moderately - sized MNMT model .
To examine the generalization of the MNMT model into different language groups , we calculate the average gains for all pairs translating into English ( XX - En ) , from English ( En - XX ) , into Russian ( XX - Ru ) , from Russian ( Ru - XX ) , and direct pairs ( XX - XX ) .
Table 4 shows the average gains Adequacy Fluency Avg k LL UL Avg k LL UL en-tr 2.97 0.33 0.23 0.43 3.20 0.12 0.04 0.21 tr-en 2.95 0.45 0.36 0.55 3.18 0.40 0.30 0.50 en-uz 2.77 0.18 0.10 0.26 2.93 0.28 0.17 0.38 uz-en 3.05 0.28 0.20 0.37 3.19 0.29 0.18 0.39 ba-ru 2.74 0.58 0.48 0.67 3.34 0.63 0.54 0.73 ru-ba 2.81 0.27 0.17 0.37 3.06 0.19 0.09 0.29
Table 5 : Avg represents the average score for either Adequacy or Fluency given by the annotators for each language pair .
k represents the Cohen's Kappa score .
LL represents the Lower Limit within 95 % confidence .
UL represents the Upper Limit within 95 % confidence .
per category in terms of BLEU and Chrf .
As it looks , translating from and into English sees the most gains , which is very consistent with the findings from the community ( Arivazhagan et al. , 2019 ; Goyal et al. , 2021 ) .
A positive trend is the increasing quality of direct pairs which are very comparable to the non-Turkic pairs .
We hypothesize that one of the main reasons for this is that the TIL Corpus is a multi-centric dataset with training data between almost all language pairs which allows us to train a complete Multilingual Neural Machine Translation ( cMNMT ) ( Freitag and Firat , 2020 ) .
As shown in ( Freitag and Firat , 2020 ; Fan et al. , 2021 ) , MNMT models trained on multi-centric parallel corpora tend to result in performance gains between non-English pairs .
BLEU vs Chrf Figure 3 compares BLEU and Chrf for all bilingual and multilingual models on X-WMT .
We distinguish between translating into and from Turkic languages since all Turkic languages feature agglutination .
As hinted above , we suspect that BLEU might underestimate translation quality when translating into Turkic languages .
The graph shows a clear distinction that confirms this :
For translations into non-Turkic languages , the relation between Chrf and BLEU is almost linear , with a Pearson correlation of 0.98 and a rank correlation of 0.98 as well .
For translation into Turkic , the trend follows a more curved line , with a largely higher Chrf-to - BLEU ratio .
The Pearson correlation is much lower at 0.87 , but the rank correlation is only slightly lower than for non-Turkic languages at 0.92 .
Consequently , we can expect the same BLEU score to correspond to a higher Chrf score when translating into Turkic languages than from them .
This means that while Chrf and BLEU are likely to pro- duce similar rankings of systems ( at least in our scenario with standard comparable Transformer models ) , the Chrf score might better characterize the absolute translation quality .
Our human evaluation does not cover sufficient language pairs ( three from and three into Turkic languages ) to yield a reliable empirical confirmation for this hypothesis .
Future studies of larger scale as in the WMT metrics shared task ( Mathur et al. , 2020 ) will be needed .
Human Evaluation of MNMT
Human evaluation setup
To facilitate analysis on how well evaluation metrics measure the quality of the translations , we conduct human evaluations using the outputs from the MNMT model on the X - WMT set .
We use Direct Assessment ( DA ) and follow the TAUS guidelines 10 with the only exception being the number of annotators per language pair , where we employ 2 annotators per language pair instead of 4 11 .
In our DA , two hundred sentences of the MNMT model 's output per language pair are evaluated based on its adequacy and fluency on respective 1 - 4 point scales .
Annotators received an explanation of the rating scales with the task ( e.g . " Adequacy :
On a 4 - point scale rate how much of the meaning is represented in the translation : 4 : Everything 3 : Most 2 : Little 1 : None " ) .
To measure the inter-annotator agree-ment ( IAA ) between the two annotators of each language pair , we compute the Weighted Cohen 's Kappa statistic ( Cohen , 1960 ) .
The language pairs involved in this human study are English -Turkish , Turkish -English , Bashkir-Russian , Russian - Bashkir , Uzbek -English , and English - Uzbek .
These pairs were selected on the basis of language and script diversity , their performance on the X - WMT test set , and the availability of annotators .
Discussion and Results
The results of the average adequacy and fluency for each language pair are shown by Table 5 .
Most of the chosen language pairs received an average score of around 3 for both adequacy and fluency .
This indicates that the model was largely able to convey most intended meaning in a good grammatical sense to a native speaker .
Fluency is consistently rated higher than adequacy , which is a common theme in NMT evaluation ( Martindale et al. , 2019 ) .
The large difference in BLEU ( 5 BLEU points ) between en-uz and uz-en is still noticeable , but much smaller according to the human evaluation .
Chrf estimates a quality difference of 0.3 here , which is closer to the human estimate .
The Cohen 's Kappa scores for each language pair are present in Table 5 .
As Cohen 's Kappa is a measure from 0 - 1 of how well the two annotators agreed with their evaluations while removing possible agreements by chance , Cohen 's Kappa score serves as one metric in deciding the reliability of en-tr Adequacy : 3.00 - Fluency : 4.00 - Fluent Output with Inadequate Verbal Tense Reference Toyota , Subaru ' daki hissesini ' den fazla art?racag ?n? s?yledi .
Hypothesis Toyota , Subaru ' daki hisseyi ' den fazla art ?rd?g?n? s?yledi .
Adequacy : 4.00 - Fluency : 3.00 - Lexical choice preserves meaning , still not the natural construction Reference Bas ? ka birisi ag ?r yaraland ?.
Hypothesis Bas ? ka bir kis ? i k?t? yaraland ?.
tr-en Adequacy : 3.00 - Fluency : 2.00 - Some of the translations made lost the original meaning Reference
The schoolgirl who died from catastrophic injuries following a suspected hit - and - run in Newcastle has been pictured for the first time .
Hypothesis
After a suspicious hit -and - run in Newcastle 's , the student who died badly was first seen .
Adequacy : 3.00 - Fluency : 4.00 - Maintains grammatical form , but changes the meaning Reference
He further dismissed the embargo as an attack on the rights of citizens .
Hypothesis
He also denied the ambargo by defending an attack on citizens ' rights .
ba-ru Adequacy : 2.00 - Fluency : 3.00 -" kiss " translates to " kill " and changes the meaning completely Reference ? ? ? ? , ? ? ? ? ? ? , ? ? ? ? ?. Hypothesis ? ? ? ? ? , ? ? ? ? , ? ? ? ? ? ?.
Adequacy : 3.00 - Fluency : 2.00 - Incorrect pronoun ( " she " to " he " ) .
Few awkward translations Reference ? ? ? ? ? ? Fast Trak Management , ? ? , ? ? ? " ? ? ? ? ? ? ? " .
Hypothesis ? ? ? ? ? Fast Trak Management ? ? ? , ? ? ? " ? ? ? ? ? ? " . ru-ba Adequacy : 3.00 - Fluency : 2.00
- When several verbs are present , some are omitted from the translation Reference ? ? ? ? ? ? ? ? ? ?. Hypothesis ? ? ? access ? ? , ? ? ? ? ? ? ? ? ? ?.
Adequacy : 2.00 - Fluency : 3.00
- A whole part of the original sentence is omitted from the translation Reference iHandy ? ? ? ?-? ? ? , ? ? ? Google Play Store ? ? ? ?. Hypothesis iHandy Google Play Store -? ? ? ?-? ? ?. uz-en Adequacy : 2.00 - Fluency : 2.00 - Changed the order events Reference Antonio Brown has indicated he 's not retiring from the NFL , only a few days after announcing he was done with the league in a rant .
Hypothesis Antonio Braun said that after a few days after the NFL , he wo n't leave after he announced that he was engaged in league .
Adequacy : 2.00 - Fluency : 4.00 - Improper changes from original nouns , and different sense of " hold " Reference Harker says Fed should ' hold firm ' on interest rates Hypothesis Everyone thinks that this is how to hold the Federal rate percentages .
en-uz Adequacy : 3.00 - Fluency : 3.00 -" Gumonlanuvchi " : " a suspect " .
" Shubhachi " : " someone who suspects " Reference Keyin ushbu mashinadan uch nafar gumonlanuvchi tushayotganini ko ' rishdi .
Hypothesis Keyinchalik uchta shubhachi mashinadan chiqib ketganini ko'rishdi .
Adequacy : 2.00 - Fluency : 2.00
- Use of a correct but a foreign word ( bas ?ar? s ?z )
Reference WeWork 's Neumann muvaffaqiyatsiz IPO o ' tkazilgandan so 'ng o ' zini bosh direktor lavozimidan chetlatishga ovoz berdi Hypothesis Biz Work "s Neumann IPO bas ?ar?s ? z bo'lganidan so 'ng O'zbekiston Bosh direktori sifatida ovoz berdi the evaluations .
We see that the reliability varies across language pairs and between adequacy and fluency .
Translation into English or Russian has a higher agreement on average than in the opposite direction ( en / tr is a tie ) .
Qualitative Analysis
To gain better qualitative insight into the model outputs in each of the 6 language directions , we asked the annotators to identify 2 examples that highlight the most commonly witnessed mistakes during their review .
Table 6 showcases those examples along with a brief explanation for their scores .
From this analysis , it seems that the severity of mistakes that the MNMT model makes in adequacy tends to range from certain words being translated to a slightly different meaning to the original intention of the sentence being lost .
As for fluency , the errors seem to range from awkward wording to clear grammatical mistakes .
There are a few cases where there is an off-target translation for a word or a segment of the sentence .
6 The Promise of MNMT : Cross-lingual Knowledge Transfer
One of the biggest advantages of a large MNMT model is its capacity for transfer learning as can be accomplished through fine-tuning .
Since we plan on releasing the model to the public , we believe many understudied and underperforming language pairs could benefit from cross-lingual knowledge transfer .
This phenomenon is well -known in the broader NLP community as well as in MT research .
To test this hypothesis , we fine- tune our MNMT model on 4 language pairs ranging from high ( er ) resource to extremely low-resource in training data available .
Table 7 shows the results of the experiments .
As it can be seen , the performance of the models improves steadily across all resource types , low-resource cases experiencing gains up to 44 BLEU points ( or 0.4 Chrf ) from the bilingual baselines in the in-domain evaluation .
However , in out - of- domain scenarios , gains are not as signifi-cant .
Mid -to high- resource pairs improve modestly in the range of 1 - 5 BLEU points ( or 0-0.1 Chrf ) while a low-resource pair , Russian - Sakha gains up to 22 BLEU points ( 0.19 Chrf ) .
Future Work and Conclusion
In this work , we train and evaluate the first largescale MNMT model for the Turkic language family which consists of many underexplored languages .
Among many results , we find it very promising to train and finetune a MNMT model with a language family corpus as it boosts the cross-lingual knowledge transfer between the related languages and consistently improves over the strong bilingual baselines in out - of- domain scenarios .
Our analysis also shows that Chrf and BLEU do not correlate in the same when the target language group if different : BLEU underestimates the translations for the Turkic languages .
In the future work , we hope to include more of the underrepresented Turkic language pairs in the study and explore the potential of transfer learning into the translation of unseen languages and language pairs ( " zero-shot " ) .
Figure 1 : 1 Figure 1 : Performance comparison between bilingual baselines and the MNMT model on X - WMT test set .
Figure 2 : 2 Figure 2 : Performance comparison between bilingual baselines and the MNMT model on the in- domain test set .
Figure 3 : 3 Figure 3 : Correlations between BLEU and Chrf scores when the target language is Turkic and non-Turkic .
Table 2 : 2
The size of the development and test sets depends on the amount of training data available .
More specifically , development and test sizes are 5 k each if the train size is over 1 million parallel sentences , 2.5 k if over 100k , 1 k if over 10k , and 500 if over 2.5 k .
All test and development samples are removed from the training corpus for that language pair .
Overall , this yields development and test sets for exactly 400 language pairs .
en ru ba tr uz ky kk az sah kaa en - ru 1000 - ba 1000 1000 - tr 800 800 800 - uz 900 900 900 600 - ky 500 500 500 400 500 - kk 700 700 700 500 700 500 - az 600 600 600 500 600 500 500 - sah 300 300 300 300 300 300 300 300 - kaa 300 300 300 300 300 300 300 300 300 - X - WMT test sets .
Bolded entries indicate the original translation direction .
Table 3 : 3 Experiments results from bilingual baselines and MNMT model evaluated on the in-domain and out-ofdomain test sets .
BLEU and Chrf uses the SacreBLEU implementation and PPL refers to the internal perplexity of the MNMT model .
Table 4 : 4 Performance comparison with different language groups and their overall gains in the MNMT setup .
XX refers to the Turkic languages in the corpus .
Bilingual MNMT Gain BLEU Chrf BLEU Chrf BLEU Chrf XX - En 6.92 0.31 13.78 0.42 +6.86 +0.11 En -XX 4.57 0.30 9.37 0.36 +4.80 +0.06 XX - Ru 9.03 0.36 13.06 0.41 +4.03 +0.06 Ru-XX 8.86 0.38 11.21 0.40 +2.35 +0.02 XX - XX 8.99 0.38 12.49 0.41 +3.49 +0.04
Table 6 : 6 Qualitative Analysis of the MNMT model output for 6 language pairs .
The Reference sentence shows the intended translation while the Hypothesis shows the MNMT model output .
Pairs Train Size In-Domain X-WMT BLEU Chrf BLEU Chrf ru-ba 523.7 K 54.48 ( + 11.04 ) 0.743 ( + 0.07 ) 24.85 ( + 0.54 ) 0.569 ( -0.02 ) ky-en 312.6 K 24.21 ( + 6.2 ) 0.42 ( + 0.05 ) 10.26 ( + 5.61 ) 0.38 ( + 0.09 ) en-ba 34.3 K 30.43 ( + 13.14 ) 0.46 ( + 0.11 ) 4.56 ( + 4.4 ) 0.22 ( + 0.08 ) ru-sah 9.2 K 49.46 ( +44.00 ) 0.585 ( 0.4 ) 22.05 ( + 21.93 ) 0.348 ( +0.19 )
Table 7 : 7 Experiment results from the finetuning of the MNMT model .
https://www.apertium.org/ 4 https://bible.is/ 5 https://www.ted.com/participate/ translate
https://opus.nlpl.eu/
https://github.com/alvations/sacremoses
https://github.com/google/sentencepiece 9 https://github.com/pytorch/fairseq/ tree/master/examples / translation
https://rb.gy/eqlgbm
11
Due to limited resources .
https://uz.khanacademy.org/ 13 https://bsfond.ru/
