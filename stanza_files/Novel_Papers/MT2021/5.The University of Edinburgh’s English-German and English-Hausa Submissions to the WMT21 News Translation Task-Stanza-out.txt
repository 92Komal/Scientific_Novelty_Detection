title
The University of Edinburgh 's English -German and English-Hausa Submissions to the WMT21 News Translation Task
abstract
This paper presents the University of Edinburgh 's constrained submissions of English - German and English -Hausa systems to the WMT 2021 shared task on news translation .
We build En- De systems in three stages : corpus filtering , back - translation , and fine-tuning .
For En-Ha we use an iterative back -translation approach on top of pre-trained En - De models and investigate vocabulary embedding mapping .
Introduction
We describe the University of Edinburgh 's participation in English ?
German ( En? De ) and English ?
Hausa ( En?Ha ) at the WMT 2021 news translation task .
We apply distinct sets of techniques to the two language pairs separately , as the two pairs are very different in terms of language proximity and the availability of resources .
We follow the constrained condition where we only use the provided data available to all participants .
For En?De we first employ rule-based and dual conditional cross-entropy filtering to clean the datasets .
Then we add to training back - translations generated in a few ways : tagged , greedy , beam search and sampling .
We fine- tune our models on past years ' test sets , and finally tune a few configurations : length normalization , test sentence splitting , and German post-processing .
For En?
Ha we adopt iterative back - translation , where at each iteration we initialize the model parameters from an En- De model in the corresponding direction ( En? De for En? Ha and De?En for Ha?En ) .
These En- De models are trained in the same way as those submitted to the En- De task , except that their vocabulary includes subwords from the Hausa language .
Besides , we experiment with vocabulary mapping at the embedding level .
Some configurations are kept consistent across language pairs and systems .
Sentences are tok-enized using SentencePiece ( Kudo and Richardson , 2018 ) with a 32 K shared vocabulary , except that we added a few extra tokens for tagged backtranslation .
All models are trained following Marian 's Transformer - Big task preset ( Vaswani et al. , 2017 ; Junczys - Dowmunt et al. , 2018 ) unless otherwise specified : 6 encoder and decoder layers , 16 heads , 1024 hidden embedding size , tied embeddings ( Press and Wolf , 2017 ) , etc.
1 Section 2 and Section 3 describe the detailed model building process for En?De and En? Ha respectively .
While awaiting human evaluation results , we summarize our automatic metric scores on the WMT 2021 test sets computed by the task organizers in Table 1 .
English ?
German
Data and cleaning English - German is considered to be a highresource language pair , with over 90 million parallel and hundreds of millions monolingual sentences provided in the shared task .
Following our last year 's submission ( Germann , 2020 ) , we divide the data into three categories , and we use all the parallel data , as well as monolingual news from 2018 to 2020 : ? High-quality parallel : News Commentary , Europarl and Rapid . ? Crawled parallel : ParaCrawl , WikiMatrix , CommonCrawl , and WikiTitles .
?
Monolingual news : News Crawl
The majority of parallel data are mined and aligned sentences from the web ( Ba? n et al. , 2020 ; Schwenk et al. , 2021 ) , so our first step is corpus filtering to remove noisy sentences which could harm neural machine translation ( Khayrallah and Koehn , 2018 ) .
We run rule- based filtering using FastText language identification ( Joulin et al. , 2016 ) , and various handcrafted features such as sentence length , character ratio and length ratio .
Similar rules are applied on the monolingual data , omitting the features designed for parallel data .
More details can be found in our cleaning script which is made public .
2
We then train seed Transformer - Base models on the filtered high-quality data , as well as the crawled data separately , to ( self - ) score translation crossentropy of the crawled parallel sentences .
This enables us to rank and filter out sentences by their dual conditional cross-entropy ( Junczys - Dowmunt , 2018 ) .
The method prefers the sentences in a pair to have low and similar translation cross-entropy given each other .
After empirical trials , we find it is always better to score using models trained on the high-quality data , and we choose to keep the best 75 % of the crawled data .
The filtering efforts are reported in Table 2 . Next , we train Transformer - Big models on the combination of filtered highquality and crawled data .
These models serve as baselines and are used for back - translation later .
Amount of crawled
Back-translation
Since its introduction , back - translation ( Sennrich et al. , 2016 ) has been widely used to boost NMT .
We use ensembles of our best seed and baseline models trained on the filtered data , to generate back - translations from the monolingual news data from 2018 to 2020 , hoping that the domains are similar to that of the test .
For En?De we mix back - translations generated using greedy search , beam search , and sampling ; for De?En , we adopt tagged back - translation ( Caswell et al. , 2019 ) .
After merging the original and back - translated data , for each direction we train 4 standard Transformer - Big models , as well as a model with 8 encoder layers and 4 decoder layers .
Specifically for De?En , we have an extra pre-layer normalized variant .
As we observed last year , validation BLEU does not improve after we add back - translated data to training .
As a result , after the models converge , we continue training them on filtered parallel data only .
The models ' validation BLEU scores 3 on WMT19 test are displayed in Table 3 .
Configuration
Fine-tuning and submission
We grid search on length normalization during decoding , and find 1.2 to be ideal for En?De and 0.8 for De?En .
Particularly for En? De , we have two more steps to make German text read more natural : 1 ) continued training on 25 % title- cased parallel data to improve headline translation and 2 ) post-processing on German quotes to make them consistent .
Previous submissions show that fine- tuning on past years ' test data helps model performance ( Schamper et al. , 2018 ; .
In the early years of WMT news translation tasks , the test sentence pairs can originate in either source or target language , and are translated and merged into one set .
However , the current evaluation is on translating sentences originally in the source language only .
Therefore , we experiment with fine-tuning on the combined sets , as well as on sentence pairs originated from the source language .
We fine- tune all our models on WMT 2008 - 2019 test sets and validate on WMT 2020 test set .
While the training data contain mainly one sentence per line , the test set can have multiple sentences in the same segment .
As a result , we split each test instance into single sentences , translate , and rejoin them .
We experiment with fine-tuning and sentence splitting on the 8- encoder - 4 - decoder variant for both languages .
For each translation direction , we apply the best configuration to each model and ensemble them by averaging their predictions post-softmax .
Overall , we have a 5 - model ensemble for En?De , and a 6 - model ensemble De?En .
English ?
Hausa
Data
The main sources of English -Hausa parallel data are OPUS ( Tiedemann , 2012 ) and ParaCrawl .
We also include data from WikiTitles 4 and the Khamenei 5 corpora , which are however much smaller .
In total , we gather 759,061 parallel sentences .
For back - translation , we use 9.5 million monolingual Hausa sentences from Common Crawl , Extended Common Crawl , and News Crawl provided by the task organizers .
We randomly select 50 million English monolingual sentences from the News Crawl collections from 2018 , 2019 , and 2020 .
For training , we use a mix of back - translated monolingual data and parallel data .
Since the dataset sizes differ substantially , we over-sample the parallel data to achieve a balanced mix : 10 ? for English ?
Hausa , and 50 ? for Hausa ?
English .
Similar to our En- De models , we used tagged backtranslation to distinguish synthetic and authentic sentences in the data .
Iterative back -translation and fine-tuning
In our experiments , we combine a transfer learning approach ( Zoph et al. , 2016 ; Kocmi and Bojar , 2018 ) with 3 iterations of back - translation ( Hoang et al. , 2018 ; Edunov et al. , 2018 ) .
In each iteration , we initialize the En?Ha model with a pre-trained En?De Transformer - Big model ( and vice versa for the other direction ) .
Then , we fine- tune the model on the English -Hausa data created by the model from the previous back - translation iteration ( the initial model for the first iteration is fine- tuned on parallel data only ) .
We notice that the model generates a large number of empty translations .
We suppress this issue by taking the second - best candidate translation from the n-best list if the first one is empty .
Another problem is heavy overfitting in the models .
In many translations , the sentences begin with the prefix " Never miss an important update ! " , followed by the actual translation .
Unfortunately , we only noticed this issue after the submission .
Vocabulary embedding mapping
An additional approach we investigate is mapping the Hausa vocabulary to the German embeddings of the En? De model , when initializing the En?Ha model .
We train the models with a 32 K Senten-cePiece vocabulary obtained from datasets in all three languages .
Using the frequency - based metric introduced by ( Wang et al. , 2020 ) we assign each SentencePiece token to an English , German , Hausa or joint vocabulary .
This results in 9192 German tokens , 6485 Hausa tokens and a joint vocabulary of approximately 11k .
Having established a separate Hausa and German vocabulary it is then possible to map between the embeddings of the two .
In order to map the vocabularies , we independently train BWEs ( bilingual word embeddings ) using an implementation of Bivec ( Luong et al. , 2015 ) combined with FastText ( Bojanowski et al. , 2017 ) .
This implementation uses a joint learning objective as described by Liu et al . ( 2020 ) utilising alignments combined with sub-word information .
In lieu of a parallel De - Ha dataset an En?De NMT model is used to translate the English side of the En-Ha dataset .
We constrain SentencePiece encoding using the previously extracted vocabularies for example the Huasa data is encoded using only the Hausa tokens and the joint tokens .
Once both sides are encoded FastAlign is used to extract automatic alignments and the BWEs are trained .
We first map the Hausa tokens to their nearest neighbour using the Cross-Domain Similarity Local Scaling ( Lample et al. , 2018 ) distance metric in the order of Hausa tokens ' frequency , and only permit a German token to be mapped to exactly one Hausa token .
For tokens that do not have a one- to - one mapping , we adapt Gu et al . ( 2018 ) 's approach , whereby the embedding of a Hausa token is initialized to the weighted sum of all German embeddings .
The weights are given by a probability distribution derived from the distance of the Hausa token to each German token in the bilingual embedding space .
It is worth noting that we only map between the tokens in the Hausa and German vocabularies not any of the joint tokens .
Finally , we initialize the embedding table using the new embeddings and remove all tokens identified as German .
After initialization , we fine - tune the model using the parallel and back - translated data as described previously .
Our experiments show that although initializing the embedding table using a mapping - based approach results in faster model convergence , it does not improve the final BLEU score compared to just fine-tuning from the En- De models .
This was observed for both the parallel data and the combined parallel and back - translated data .
The outputs of the mapping approach to the baseline for the Ha- En system are qualitatively very similar and indicates that while the embedding mapping increases convergence there is no knowledge transfer from the German embeddings .
Conclusion
We describe our English -German and English -Hausa submissions to the news translation task at WMT 2021 .
For the En?De task , fine-tuning and splitting test instances significantly boosts BLEU while back - translation alone does not help .
In the En?Ha task , we experiment with interesting low resource NMT techniques , but unfortunately , our submission contains translations from overfitted models .
Table 3 : 3 Average BLEU scores of BT experiments on WMT19 test used as dev .
De?En En?De Baseline 42.2 43.4 + BT 41.8 43.0 + cont. training 42.5 43.6
