title
HW - TSC's Participation in the WMT 2021 Triangular MT Shared Task
abstract
This paper presents the submission of Huawei Translation Service Center ( HW - TSC ) to WMT 2021 Triangular MT Shared Task .
We participate in the Russian - to - Chinese task under the constrained condition .
We use Transformer architecture and obtain the best performance via a variant with larger parameter sizes .
We perform detailed data pre-processing and filtering on the provided large-scale bilingual data .
Several strategies are used to train our models , such as Multilingual Translation , Back Translation , Forward Translation , Data Denoising , Average Checkpoint , Ensemble , Fine-tuning , etc .
Our system obtains 32.5 BLEU on the dev set and 27.7 BLEU on the test set , the highest score among all submissions .
Introduction
This paper introduces our submission to the WMT21 Triangular task .
We adopt Transformer ( Vaswani et al. , 2017 ) architecture and strictly obey the constrained condition in terms of data usage .
On one hand , we perform multiple data filtering strategies to enhance data quality ; on the other hand , we leverage multilingual model ( Johnson et al. , 2017 ) , pivot language , forward and back translation ( Edunov et al. , 2018 ) , and data denoising ( Wang et al. , 2018 ) strategies to further enhance training effects .
In addition , we also adopt fine-tuning ( Sun et al. , 2019 ) and ensemble ( Garmash and Monz , 2016 ) , two widely used strategies , to further enhance system performance .
We compare and contrast different strategies based on our experiment results and give our analysis accordingly .
The overall training process is illustrated in Figure 1 . Section 2 mainly focuses on our training techniques , including model architecture , data processing and training strategies .
Section 3 describes our experiment settings and training process .
Section 4 presents the experiment results while section 5 analyze how our multilingual , data denoise and data augmentation strategies influence system performances .
Method
Model Architecture
Our system uses Transformer ( Vaswani et al. , 2017 ) model architecture , which adopts full self-attention mechanism to realize algorithm parallelism , accelerate model training speed , and improve translation quality .
In this shared task , Transformer - Deep is used , which features 35 - layer encoder , 6 - layer decoder , 768 dimensions of word vector , 3072 - hidden-state , 16 - head self-attention , and pre-norm .
Data Processing an Augmentation
We strictly comply with the constrained condition and use only the officially provided data .
Data Filtering
We perform the following steps to cleanse all data : ?
Filter out repeated sentences ( Khayrallah and Koehn , 2018 ; . ? Convert XML escape characters .
?
Normalize punctuations using Moses ( Koehn et al. , 2007 ) . ?
Delete html tags , non - UTF - 8 characters , unicode characters and invisible characters .
?
Filter out sentences with mismatched parentheses and quotation marks ; sentences of which punctuation percentage exceeds 0.3 ; sentences with the character - to - word ratio greater than 12 or less than 1.5 ; sentences of which the source - to - target token ratio higher than 3 or lowers than 0.3 ; sentences with more than 120 tokens .
?
Apply langid ( Joulin et al. , 2016 b , a ) to filter sentences in other languages .
?
Use fast-align ( Dyer et al. , 2013 ) to filter sentence pairs with poor alignment , about 10 % of the data is filtered .
We perform the additional steps to process Chinese data : ? Convert traditional Chinese characters to simplified ones .
?
Convert fullwidth forms to halfwidth forms .
Data sizes before and after cleansing are listed in Table 1 .
Data Augmentation Back-translation ( Edunov et al. , 2018 ) is an effective way to boost translation quality by using monolingual data to generate synthetic training parallel data .
As described in , similar to back translation , the monolingual corpus in source language can also be used to generate forward translation text with a trained MT model , and the generated forward and backward translation data can both be merged with the authentic bilingual data .
This strategy can increase the data size to a large extent .
Since there is no officially provided monolingual data , we use the target side of en2zh data and the source side of zh2ru data filtered out in section 2.2.1 for back translation .
We adopt the top-k sampling method .
Then , we use the source side of ru2en data for forward translation , which is done based on beam search .
Through sampling , we ensure that the sizes of data generated by forward and back translation are relatively equal .
In this paper , we refer to the combination of forward and sampling back translation as FTST .
Filter Using LaBSE
Apart from the commonly used data cleansing methods , we also explore other techniques based on neural networks .
LaBSE ( Feng et al. , 2020 ) is a multilingual BERT embedding model that can measure semantic similarities across languages .
In our experiment , we notice that traditional data cleansing methods described in section 2.2.1 are unable to produce high-quality data , so we further filter the data using pre-training model LaBSE .
For all parallel data , we calculated the similarity scores and For forward model , the training is divided into three steps :
1 ) Use all official provided data in three directions ( ru2zh , en2zh , and ru2en ) for training ;
2 ) Use all clean data selected by LaBSE for incremental training ;
3 ) Finally , use ru2zh clean data selected by LaBSE for incremental training .
For backward model , we only perform two steps : 1 ) Use all data ( en2ru , zh2ru ) for training ;
2 ) Use zh2ru clean data selected by LaBSE for incremental training .
Fine-tuning and Ensemble
To achieve better results , fine-tuning with smallsize in- domain data is necessary ( Sun et al. , 2019 ) .
An effective strategy for fine- tuning is to leverage the dev set available in this task .
The fine-tuning strategies employed in our experiment include : 1 ) Add noise to the target side of the dev set to generate synthetic training data ( Meng et al. , 2020 ) ; 2 ) Use multiple models to generate synthetic data through beam search decoding , and then add synthetic data to the dev test for fine-tuning .
Model ensemble is also a widely used technique in previous WMT workshops ( Garmash and Monz , 2016 ) , which can boost the performance by combining the predictions of several models at each decoding step .
We selected the best four models from the six we trained for ensemble .
Settings
Experiment Settings
We use the open-source fairseq ( Ott et al. , 2019 ) for training , and use sacreBLEU ( Post , 2018 ) to measure system performances instead of the BLEU script mentioned in the task .
The main parameters are as follows :
Each model is trained using 8 GPUs .
The size of each batch is set as 2048 , parameter update frequency as 32 , learning rate as 5e - 4 ( Vaswani et al. , 2017 ) and label smoothing as 0.1 ( Szegedy et al. , 2016 ) .
The number of warmup steps is 4000 , and the dropout is 0.1 .
We employ joint sentencepiece model ( Kudo and Richardson , 2018 ; Kudo , 2018 ) for word segmentation , with the size of the vocabulary set to 32 k .
Jieba tokenizer is used for Chinese word segmentation while Moses tokenizer for English and Russian word segmentation .
The three languages share a vocabulary of 45 K words .
In the inference phase , we use the opensource marian ( Junczys - Dowmunt et al. , 2018 )
Training Process
We combine multi-stage denoising training with data augmentation methods .
Figure 1 illustrates our training process : 1 ) We cleanse the training data using methods mentioned in 2.2.1 and train three forward models and one backward model .
2 ) We further denoise data using LaBSE ( as mentioned in 2.2.3 ) and conduct denoising training until the model converge on the dev set .
3 ) We perform data augmentation as described in 2.2.2 .
We collect a total of 45 M Russian monolingual data and split them into three sets , each with 15 M sentences .
We use three different forward models to generate three sets of training data .
Hoping to add diversity to incremental training , we use the data synthesized by one model to train the other two models .
For example , we use the synthetic data generated by forward model A to incremental train forward model B , C and so on .
We also collect a total of 15 M Chinese monolingual data and back translate the data using the backward model .
We repeat back translation for three times and obtain three sets of back translation data .
We incrementally train six models using the above synthetic data .
4 ) We average the last 5 checkpoints of each model and select the best four from the six models we trained for final ensemble .
Experiment Result
Our overall training strategy is to train a baseline model , conduct incremental training with techniques such as multilingual model , denoise training , data augmentation , and fine-tuning .
Our submitted results come from ensembled models .
achieves an increase of 5.9 BLEU .
Our baseline model is trained with data processed with methods mentioned in section 2.2.1 .
The BLEU score of the baseline model on the dev set is 26.6 .
Comparing with the baseline model , our multilingual strategy leads to a huge improvement of 2.7 BLEU .
Our simplified denoising training strategy contributes to an increase of 0.7 BLEU .
It should be noted that data augmentation techniques ( FTST method and LaBSE denoising on ru2zh data ) also result in a significant increase of 1.9 BLEU .
Finally , an increase of 0.6 BLEU is gained via ensemble .
Our submitted system gain 32.5 BLEU on the dev set , which demonstrate the effectiveness of our multiple strategies .
According to the organizer 's feedback , our submitted model gains 27.7 BLEU on the WMT21 test set .
Analysis
Multilingual Model and Model Performance
Our experiment results demonstrate that multilingual model has positive effects on system performance .
We have experimented on different multilingual models and compare their results .
Denoising Training and System Performance
Our experiment also demonstrates the contribution of denoising training to system performance .
Table 4 compares the results of baseline and denoising training model , from which we can see an increase of 1.4 BLEU .
We further compare the results measured at the three stages of denoising training .
We use the enhanced target and source model to conduct simplified denoising training .
Our experiment shows that full- data denoising training leads to an increase of 0.7 BLEU while ru2zh data denoising further leads to an increase of 0.5 BLEU .
The experimental results show that the denoise strategy is effective and can lead to at least 1 BLEU improvement even after multilingual model enhancement .
Data Augmentation and System Performance Data augmentation strategy also leads to huge BLEU improvements .
We try multiple data augmentation strategies , including back translation ( BT ) , forward translation ( FT ) , FTST ( 2.2.2 ) .
Sampling BT means sampling from the model conditional distribution and beam BT means using beam search , when generating synthetic data .
Table 5 shows the effects of different data enhancement methods .
Our results show that sampling back translation can lead to better results ( about 0.3 BLEU in our experiment ) .
We also conduct two forward translation experiments : FT is translating Russian to Chinese directly , and Pivot FT is using English as the pivot language , which achieve only an undesirable result .
We then using the FTST method and gain the best result with a BLEU score of 30.5 .
The experimental results show that the combination of sampling BT and FT data ( FTST ) can produce the best data augmentation effect .
Conclusion
This paper presents HW - TSC 's submission to WMT21 Triangular Machine Translation Task .
In general , we use Transformer architecture and explore multiple data filtering and selection methods .
In terms of training and data processing strategies , multilingual model , denoising training , data augmentation , and FTST we used can effectively improve system performance .
Our final result achieves an increase of 5.9 BLEU when comparing baseline model on the dev set and gain a BLEU score of 27.7 on the test which is the highest among all submissions .
Figure 1 : 1 Figure 1 : This figure shows the training process for the WMT 2021 Triangular MT Shared Task , which consists of three stages .
In stage 1 , three forward models and one backward model are trained .
In stage 2 , denoise corpus is used to train models incrementally .
In stage 3 , the synthetic data by FTST and denoise corpus are used to train models incrementally .
Finally , model ensemble is used to boost the performance .
