title
Supervised Neural Clustering via Latent Structured Output Learning : Application to Question Intents
abstract
Previous pre-neural work on structured prediction has produced very effective supervised clustering algorithms using linear classifiers , e.g. , structured SVM or perceptron .
However , these cannot exploit the representation learning ability of neural networks , which would make supervised clustering even more powerful , i.e. , general clustering patterns can be learned automatically .
In this paper , we design neural networks based on latent structured prediction loss and Transformer models to approach supervised clustering .
We tested our methods on the task of automatically recreating categories of intents from publicly available question intent corpora .
The results show that our approach delivers 95.65 % of F1 , outperforming the state of the art by 17.24 % .
Introduction Recent years have witnessed a vast spread of virtual assistants , such as Google Home , Siri and Alexa , which are based on the research areas of Conversational Agents and Question Answering .
When designing such systems , the creation of classes of expected questions , aka intents , is essential for building the main states of a dialog manager .
In particular , when an assistant is designed for a specific domain , a knowledge engineer needs to analyze typical user 's questions , answered by human operators .
This work would be greatly sped up , if the engineers could have questions clustered according to the different topics they ask for .
For example , the following questions / requests from the intent dataset by Larson et al . ( 2019 ) : i want to switch to direct deposit set up direct deposit for me how do i go about setting up direct deposit all have a common intent of making a direct deposit .
Thus , the dialog designer will create this intent , if the cluster captures a large number of requests .
However , for being effective , the clustering algorithm must demonstrate a sufficient accuracy , which is often not the case for completely unsupervised methods .
Thus , supervised clustering ( Finley and Joachims , 2005 ) , which exploits some training data of the target domain , e.g. , previously designed clusters , to discover new clusters , is a viable approach .
A seminal work on structured prediction was Latent Structural Support Vector Machines ( LSSVM ) by Yu and Joachims ( 2009 ) .
Recently , Haponchyk et al. ( 2018 ) have shown that LSSVM as well as the Latent Structured Perceptron ( LSP ) by Fernandes et al . ( 2014 ) , originally designed for coreference resolution , were also effective , when provided with the appropriate node similarity function , for clustering questions into intents .
These approaches used traditional feature engineering ( question similarity ) and a linear classifier , i.e. , SVM , which can be highly improved by neural networks , and pre-trained Transformers , e.g. , Devlin et al . ( 2019 ) .
Indeed , neural models enable representation learning , which can amplify the generalization ability of supervised clustering algorithms .
In this paper , we design neural supervised clustering ( NSC ) models using the structured prediction algorithms , LSSVM and LSP .
These are based on a latent representation of clusters using graph structures , which are used to compute an augmented loss .
The latter , in turn , is used together with the model score to globally select the maxviolating constraint at each learning step .
This is the clustering that maximally penalizes the current model , which is used for a model update .
We apply the same idea by computing the margin loss for our neural model and then back - propagating it , as any other differentiable loss .
The augmented loss does not depend on the neural model , thus our approach can be applied to arbitrary learning settings .
We applied NSC to two different question intent clustering tasks , defined by two datasets : IC&OOS ( Larson et al. , 2019 ) , which is an intent classifica-tion corpus , and Quora Intent Corpus ( Haponchyk et al. , 2018 ) .
Another interesting contribution of our work is the creation of a training set for clustering from IC&OOS , which enables an effective training of NSC .
Our corpus and software are available to the research community 1 .
The comparative results of NSC using traditional CNN networks and Transformer models against traditional methods , e.g. , spectral clustering , show an impressive boost in F1 of our NSC - BERT model , e.g. , 95.65 % vs. 78.38 % , more than 17 % of improvement over the best spectral clustering method .
This accuracy enables the use of our approach for dialog applications and opens up further directions for other clustering tasks .
Related Work
This paper touches two main research areas : structured prediction , in particular with neural models , and intent clustering , which are described below .
Structured prediction has shown powerful machine learning algorithms for solving NLP tasks requiring complex output , e.g. , syntactic parsing ( Smith , 2011 ) , coreference resolution ( Yu and Joachims , 2009 ; Fernandes et al. , 2014 ) .
This work has mainly regarded traditional frameworks , e.g. , SVMs , CRF , perceptron .
Only little work has been devoted to the integration of the above theory in neural networks ( LeCun et al. , 2006 ; Durrett and Klein , 2015 ; Weiss et al. , 2015 ; Kiperwasser and Goldberg , 2016 ; Peng et al. , 2018 ; Milidi ? and Rocha , 2018 ; Xu et al. , 2018 ; Wang et al. , 2019 ) , and , to the best of our knowledge , none to supervised clustering .
This is partially due to the fact that local solutions have usually produced optimal results .
For example , in case of supervised clustering , it is difficult to design a loss function that captures the global information about the clusters .
Work in neural coreference resolution , e.g. , ( Lee et al. , 2017 ) , uses simple losses , which deliver state - of - the - art results but do not strictly take into account the cluster structure .
Secondly , this is also due to the complexity associated with adapting the methods from previous work to neural frameworks .
For example , using ILP ( Roth and Yih , 2004 ) for clustering inference in SPIGOT ( Peng et al. , 2018 ) , which facilitates the backpropagation through argmax based on a projection onto the feasible set of structured outputs , would inevitably require reducing the com-putational overhead ( Miyauchi et al. , 2018 ) .
On the line of research of question clustering , Wen et al . ( 2001 ) proposed to cluster queries with respect to a group of web pages frequently selected by users .
Deepak ( 2016 ) describes a k-means like algorithm , MiXKmeans , that can cluster threads in Community Question Answering websites .
These methods are unsupervised and , thus , are likely sensitive to setting the optimal number of clusters or to a heuristic adopted for the clustering criterion .
Also among the classification approaches , there are semi-supervised and mixed classification methods which advance on the use of vast amounts of unlabelled queries .
Li et al. ( 2008 ) classify unlabeled queries using their proximity to labeled queries in a click graph .
Beitzel et al. ( 2007 ) classify queries from logs into topics using supervised or unsupervised methods .
The following classification approaches address new emerging intents .
Lin and Xu ( 2019 ) enable a neural model to detect unknown intents as outliers using a novelty detection technique .
This model , however , does not have the capability to distinguish between different unknown intents .
Xia et al. ( 2018 ) devise a capsule neural network able to discriminate between different emerging intents .
Its zero-shot learning ability critically depends on the definition of a similarity between existing and new intents .
Our approach does not hold any explicit representation of intents .
The recent work by Lin et al . ( 2020 ) proposes a deep intent clustering model which takes advantage of labeled data for discovering new user intents , but it requires the indication of the exact number of output clusters .
Finally , Zhang et al. ( 2021 ) propose Deep Aligned Clustering , a semisupervised method , to discover new intents using limited knowledge over intent data .
We believe their approach is completely compatible with ours , i.e. , our supervised clustering models can be integrated in their approach to improve intent discovering .
Structured Output for Clustering LSSVMs train a clustering function from a series of training examples {( x i , y i ) } n i=1 , where x i are input sets of elements , and y i are structured outputs , i.e. , gold clusters .
This function applied to unseen elements x predicts their clusters y.
Cluster inference
The clustering y of a set x is inferred over a fullyconnected graph G , which nodes represent the el-ements x k of x , and edges e = ( x i , x j ) - the pairwise links between them .
The inference step consists in finding a maximum spanning forest h on G , e.g. , using Kruskal 's algorithm ( Kruskal , 1956 ) .
The nodes appearing in the same connected component ( tree ) in h are placed together in the same cluster in y ( deterministically obtained from h ) .
Learning
The approach learns a linear scoring function which decomposes over the edges of h : s w ( x , y , h ) = e=( x i , x j ) ? h w ? ?( e ) , ( 1 ) where ?( e ) is a feature representation for edge e , describing a pair of elements of x .
Graph structures h are incorporated as latent variables into the latent formulation of LSSVM .
Haponchyk et al. ( 2018 ) adapt the latent structured perceptron ( LSP ) by Fernandes et al . ( 2014 ) to the graph structures h and apply the approach to question pairs , ( q i , q j ) , to cluster sets of questions into different user intents ; we compare to these methods .
Neural Structured Output Clustering
We propose a model for optimizing a structural clustering loss with neural networks .
Global max-margin objective
As a standard practice in structured prediction , our goal is to train a model with a scoring function s ? such that the correct clustering y is scored higher than incorrect clusterings ?.
LSSVM optimizes an upper bound , E , on the structural loss ? , which , in general terms , can be rewritten using the parameters ? as : ?(y , ?( ? ) , ?( ? ) ) ? E(y , ?( ? ) , ?( ? ) ) = max ( ? , ?)?Y ?H [?(y , ? , ? ) + s ? ( x , ? , ? ) ] ?
max h?H s ? ( x , y , h ) , ( 2 ) where ?(? ) is an output of the model with its auxiliary latent structure ?( ? ) ;
Y and H are the spaces of all possible clusters and latent trees ; ?(y , ? , ? ) is a standard structural loss , measuring the difference between the gold y and the output ? clusters ; and ( ?( ? ) , ?( ? ) ) = argmax ( y , h ) ?
Y ?H s ? ( x , y , h ) .
The right - hand side of Eq. 2 is essentially a margin-based objective with margin rescaled by the loss ?.
Its minimization forces maximum weighted incorrect ?
score lower than the maximum weighted correct h by at least the value of the loss ?(y , ? , ? ) on that ? under ? parameters .
Our neural model optimizes the objective E defined in Eq. 2 .
We use the loss ? of Yu and Joachims ( 2009 ) based on computing edge mistakes in ? , in which negative edge penalties are scaled with an r-parameter .
Differentiable scoring function
For optimizing E in Eq. 2 , we define a scoring function that decomposes over the edges of h : s ? ( x , y , h ) = e=( x i , x j ) ? h net ? ( e ) .
( 3 )
This enables the inference by Kruskal 's algorithm , where the network net ? activates on edge representations 2 . Eq. 3 indicates that our approach is applicable to any network net ? .
Our work is inspired by Kiperwasser and Goldberg ( 2016 ) , who pass the arcs of dependency parses through a multi-layer perceptron and optimize a structured margin loss .
Differently to them , we elaborate on the case of the margin rescaled with the structural loss , which includes max-violating inference .
The objective E in Eq. 2 is sub-differentiable as a summation of edge networks ; ? in E does not depend on ?.
We propagate the error from the margin loss E in Eq. 2 back to input layer of net ? .
One iteration of the algorithm operates on one sample of training data ( x , y ) , where , in the context of intent task , x = {x i } is a set of questions , y are gold clusters of the questions in x .
We pass all the pairs of questions , i.e. , edges e = ( x i , x j ) , i < j of a fully connected graph G , through net ? , and compute the global error E .
The error computation includes finding ( i ) the max-violating graph , ? , among all possible spanning graphs h of G ; and ( ii ) the max-scoring correct spanning graph , h * , over the set of graphs that comply with the gold label y .
If E > 0 , the backward pass of the model computes gradients for an update of the model .
The partial derivatives with respect to the parameters ? j in the last layer of the network are ?E ? j = e? ? ? net ? ( e ) ? j ? e?h * ? net ? ( e ) ? j , and so on down to the input nodes of net ? following the chain rule .
Our Baseline Network
We intentionally do not specify the architecture of net ? in Sec. 4.2 as it could be of any form once encoding a pair of questions ( x i , x j ) .
However , we further describe the architecture with which we experiment in this work .
We use a simple feedforward neural network , which consists of ( i ) an input layer encoding a pair ( x i , x j ) , ( ii ) one fully connected hidden layer with ReLU activation functions , and ( iii ) an output layer , which is a linear operation over the outputs of the hidden layer .
This way , for edge e , net ? ( e ) is a real number without any restriction on its range .
The pairwise encoder is practically trained to score good edges higher than bad edges .
However , doing it jointly for all the edges over a sample , in a structured way , has the goal of producing a more consistent decision in terms of clustering .
Input layer
We consider two ways for representing question pairs ?( e ) = ?( x i , x j ) ?
R d , using embeddings : ( 1 ) We use a sentence encoder ( Severyn and Moschitti , 2016 ) to map each question x i into a fixed - size intermediate vector representation ?( x i ) .
The encoder operates on a sentence matrix S , in which the k-th column corresponds to the k-th word in x i and is a concatenation of the word embedding and overlap embedding : S k = [ word_emb ( w k ) , ov_emb ( w k ) ] .
The ov_emb part for x i , in each pair , is formed in association with the other question of the pair , x j .
S is given as input to a series of convolution operations with ReLU activations followed by a maxpooling layer .
From the obtained question representations ?( x i ) and ?( x j ) , we compose a symmetrical pairwise representation as ?( x i , x j ) = [ max ?( x i ) , ?( x j ) , min ?( x i ) , ?( x j ) ] , where max and min are component - wise vector operations , i.e. , max and min are applied to pair of components , so that two final vectors are obtained .
( 2 ) We exploit BERT ( Devlin et al. , 2019 ) embeddings : ?( x i , x j ) = 1 2 ( bert_emb ( x i , x j ) + bert_emb ( x j , x i ) ) , where bert_emb for a pair of questions comes from the final hidden layer representations , i.e. , [ CLS ] token from the BERT model .
Question Clustering Data
In this section , we describe two datasets : ( i) IC&OOS for intent classification ; and ( ii ) Quora for intent clustering .
We illustrate our procedure to transform the former into a dataset for clustering .
Intent clustering with IC&OOS
The dataset for Intent Classification and Out- Of -Scope prediction by Larson et al . ( 2019 ) , which we denote IC&OOS , is a classification dataset , composed of user 's queries distributed into 150 different intent classes over 10 domains , plus out -ofscope ( OOS ) queries falling outside the pre-defined classes .
The data 3 contains 50 , 20 and 30 user 's queries per intent class in training , dev. and test parts , respectively .
Plus 100 OOS queries for training , 100 - for development , and 1000 for test .
For example , we may see class categories such as MEAL SUGGESTION , with queries such as , ' suggestions for thai food ' or ' help me find some new dinner recipes ' , which may be challenging to separate from the items of RESTAURANT REVIEWS , e.g. , 'at yakamoto how is their sushi ' , and even more difficult to discern from ' what are some good sushi restaurants in reno ' , belonging to RESTAU - RANT SUGGESTION class .
The data from all of the pre-defined classes are present in training , dev. and test parts .
The main steps for transforming this dataset in one for clustering are ( i ) merging the items of all the categories together ; and ( ii ) using the original class labels as indication of belonging to different clusters .
However , a real-world application scenario of automatic clustering would entail that new incoming data can contain items which constitute new clusters ( class categories ) .
Thus , in order to demonstrate the capability of the supervised clustering models to group together the items of unseen clusters , we use one set of intent classes for training and another set for evaluation , which is constituted by a completely different set of intent classes and questions .
This way , we retain the queries from one third of intent classes , i.e. , 50 , from the training part , the dev. queries from another third of intent classes , and the test queries from the remaining one third 4 of classes , and use them as new training , dev. and test parts , respectively .
Additionally , it should be noted that the original dataset contains OOS queries , which we keep all .
Thus , our new split can be also used to analyze OOS queries , which might not be put in any semantically meaningful cluster , as well as unseen intent items , for which , we know that their natural clusters ( original categories ) exist .
Data sampling and instance creation Training and test examples in a clustering problem are sets of items .
In order to be able to effectively update the structural clustering objective in Eq. 2 with NNs , we need to limit the size of training examples ( x , y ) .
Precisely we need to limit the size of input query sets x , as the number of edges e ( Eq. 3 ) to be passed trough the network grows exponentially with it .
Thus , we split the data into samples : ( i ) from test set , we just extract random disjoint samples of equal size M ; and ( ii ) from training and dev. sets , we form samples according to a more elaborated procedure to avoid having too many singletons in an instance .
More specifically , for each class C : ( i ) we shuffle its items in a random order ; and ( ii ) we split them into a set P of m disjoint parts ( mini-clusters ) of random sizes ( different sizes , necessarily ? 2 ) each , s.t. , ? p?P = C .
Then , to build the training clustering examples , x , we iterate for several times over the entire list of classes C , in a random order , and , ?C ? C , we select a mini-cluster p ?
P C , which we append to a current sample S ( initialized as empty ) , if | S ? p| ? M .
Otherwise , we start a new sample with S = { p} , and go to the next category , until all P C are exhausted ( this happens simultaneously ?C , as P C have the same size ) .
Now , our x sets consist of items contained in S's .
This procedure makes the presence of each category uniform ( binary presence , yes / no ) : after seeing each N number of samples S , we encounter elements of all the classes , however , without preserving the original relative proportions of the class distribution .
This way , by setting the sample size limit M = 100 , we obtain around 28 and 12 clustering examples from training and dev. sets , respectively , and 35 examples from the test set .
Quora question clustering The Quora dataset , made available by Lee et al . ( 2017 ) , was designed to learn and test question duplication classifiers .
That is , for automatically detecting if two questions are semantically duplicate or not .
We use the Quora Intent Corpus by Haponchyk et al . ( 2018 ) based on a sample of questions from Quora .
One main difference with IC&OOS is the fact that the negative examples selected by the organizers of the Quora challenge refer to pairs of questions that have always some degree of lexical overlap .
For example , the following pair
How much water on earth is consumable ?
and How much water is on earth ?
is not duplicate .
On the other hand , the two questions could be surely put in the same cluster WATER OF EARTH .
This means that a similarity function learned from Quora labels may not be enough accurate for clustering .
Also a simple scalar product between two embeddings would not be enough as it can only capture lexical overlap .
The latter would surely fail on the pairs of the following questions
What is a recursion tree ? ,
What does your family tree look like ? ,
How does your Christmas tree look like ?
since their specific semantics is different but the overlap is large .
These examples suggest that a clustering algorithm must learn a similarity that looks to the entire set of items to be clustered , not just to single pairs .
This requirement is inline with the characteristics of the methods we presented in Sec 4 .
Experiments
We present the results of our empirical evaluation of the neural clustering models using IC&OOS data , followed by that using Quora Intent Corpus .
We summarize afterwards the highlights of a deeper investigation , we conducted on the performance of our approach and of its errors .
Setup Data :
We created data from IC&OOS as discussed in Sec 6.1 , which contains , 2650 , 1100 , and 2470 queries and 28 , 12 , 35 clustering examples , in training , dev. , and test sets , respectively .
We also evaluate our approaches on Quora Intent Corpus 5 ( Haponchyk et al. , 2018 )
Models :
We experiment with two variants of our Neural model for Supervised Clustering ( NSC ) , based on two ways to encode question pairs outlined in Sec. 5.1 : ( i ) NSC - CNN , using word and word overlap embeddings , and ( ii ) NSC - BERT , using BERT embeddings of question pairs .
For NSC - CNN , we employ fastText 7 word embeddings , in dimension 300 , pre-trained for English language on Wikipedia ( Bojanowski et al. , 2017 ) .
We set the max length of questions to 50 and pad the shorter questions on the right .
The size of the hidden layer is set to 1 3 of the size of the input layer .
The convolution filter width varies from 1 to 3 .
For NSC - BERT , we use BERT BASE model , which we train for 3 epochs for fine-tuning on the question pair classification task .
Since the training samples vary in size , we clip the gradients to have their L ? norm less than or equal to 1 .
This is to prevent the updates being dominated by the samples of bigger size .
Evaluation :
We follow the evaluation setting of Haponchyk et al . ( 2018 ) .
Thus , we compute ( i ) clustering F1 measure , based on assigning each cluster to the most frequent ( gold / output ) cluster , ( ii ) coreference resolution CEAF e score ( Luo , 2005 ; Cai and Strube , 2010 ) .
Parameterization :
We use dev. set for tuning the loss parameter r , which takes values from { 0.1 , 0.5 , 1.0} , and selecting the best epoch with respect to clustering F1 .
Baselines
We consider a number of baselines based on the pairwise query similarities .
We experiment with the following sources of pairwise signals : ( i ) tf - idf scores , ( ii ) outputs of the binary question pair classifier , which we train in two modalities , CNN and BERT .
We group the pairwise signals into a clustering output using spectral clustering algorithm ( Ng et al. , 7 https://fasttext.cc/docs/en/pretrained-vectors.
html 2001 ) ( implementation from smile 8 library ) , which we run on a matrix of pairwise similarities between data points .
Spectral clustering is unsupervised , and requires the indication of the number of clusters k.
For each sample , we set the parameter k to the gold number of clusters .
This means that we are computing an upper bound and unrealistic performance , which can be used to provide a meaningful comparison ( especially if our approach outperforms it ) .
As an alternative to spectral clustering , we run Kruskal 's algorithm on the graph of pairwise edges , using 0.5 as a threshold for the question pair classifier scores on them ; pairs having the scores lower 0.5 are neglected .
Experiments on IC&OOS task In Tab .
1 , we present the results on IC&OOS dataset averaged over 10 different sample splits , obtained with 10 different random seeds .
First , we note that NSC consistently improves over all the baselines in terms of both F1 and CEAF .
It also shows a good precision / recall balance .
The significantly lower results of the unsupervised baseline , spectral clustering + tf-idf , suggest that , in IC&OOS , we are supposedly dealing with a rather non-trivial task , where queries expressing the same intent do not necessarily have surface closeness .
Even if the model is aware of the true number of present intents in a sample ( gold k ) .
The other four baselines capitalize on training a supervised scoring function for query pairs , to be further used as a clustering criterion .
From our experiments with IC&OOS data , we conclude that it is also not trivial to train a pairwise classifier to convey a notion of semantic similarity to the pairs of queries from unseen classes .
This is reflected in the results of using a supervised similarity function .
The performance of spectral clustering on the output of the pairwise classifier , equally low for CNN and BERT , and In NSC , we assume , this problem is mitigated by " collaborative " updates of the structural loss .
Overall , we note the impressive performance of NSC , especially when fed by BERT .
A clustering F1 of 95.65 suggests that NSC can replicate the clusters of questions that the human annotator / knowledge engineer devised .
Experiments on Quora Intent Corpus
We run each model with 10 different random seeds for shuffling training examples and report the averaged results on the manual , Tab. 2 , and automatic test sets , Tab .
3 .
Baselines
We compare NSC with two state - of- the - art structural approaches , LSSVM and LSP proposed in ( Haponchyk et al. , 2018 ) , reporting their numbers on the same data .
LSP py is our LSP reimplementation in python using text similarity .
We trained LSP py for 100 epochs .
Results NSC - CNN improves over the state of the art on both the test sets for both measures .
On the manual test set , NSC - BERT achieves , as expected , higher CEAF .
One possible explanation for its lower F1 on the manual test set is its small size , which probably does not enable an accurate evaluation .
In contrast , on the evaluation over the automatic test set , NSC - BERT largely outperforms any model , according to any measure .
This is mainly due to the fact that the automatic clusters are more consistent with the information present in the training data , used for fine-tuning BERT .
Indeed , we fine- tuned BERT BASE on the full Quora dataset of question pairs 9 ( except for the pairs containing questions from development and test parts of Quora Intent Corpus ) .
For the sake of transparency , in Tab.
4 , we report the accuracy of the fine- tuned BERT on Quora split by Wang et al . ( 2017 ) compared to the official results of their BiMPM model .
We believe the result of NSC - BERT is promising , and , in the scope of intent detection , by not being bounded to a particular set of intents , it contributes to the existing neural solutions ( Xia et al. , 2018 ; Lin and Xu , 2019 ; Lin et al. , 2020 ) .
Deeper Analysis
In this section , we investigate the general clustering ability of NSC , and in this way , enable the comparison to the upper bound of intent detection , i.e. , the intent classifier , and list its most common mistakes .
How good is the clustering per se ?
Here , we address the standard IC&OOS scenario with the original class distribution of dataset , where all the 150 intent classes are equally presented in the data .
Moreover , we explore the upper bound to any clustering algorithm , i.e. , the use of a supervised classifier in an unrealistic ( useless for clustering ) scenario , that is , having in the training data , all the clusters ( classes ) to be discovered .
To carry out this comparison , we trained two intent classifier models , CNN and BERT , with 150 + 1 ( OOS ) target classes .
In Tab.
5 , we report their performance in terms of in-class accuracy and OOS recall .
We also report the performance of the classification models from Larson et al . ( 2019 ) for reference .
As it can be observed , our models perform comparably , e.g. , our BERT model is just 1.5 points behind .
We trained NSC on the same data , split into samples , so that we could compare to the above classifiers .
For this purpose , we follow our sampling procedure in Sec. 6.1 , this time keeping all the classes , which gives us around 79 and 32 samples for training and dev. , respectively , and 66 samples for test .
To keep the two types of systems aligned , we evaluate the classifiers also in terms of clustering F1 on the same test samples ( of size M ) , which are then averaged .
Namely , we consider queries , within a sample , predicted by a classifier model , as the same class to form a distinct cluster , while those predicted as OOS - singletons .
We note that ( i ) as expected , the results of NSC in Tab .
6 improve with respect to the completely disjoint setting ( Tab.1 ) .
( ii ) NSC - CNN is able to almost replicate the result of the CNN classifier in terms of F1 , yielding only 1.3 of a point in terms of CEAF .
( iii ) Interestingly , the OOS Recall is more than 85 % ( 2 - 3 times the one of the classifiers ) , which means that 85 % of all OOS queries were detected by NSC - CNN ( predicted singletons in their corresponding samples ) .
Although , we recognize that it can be easier to detect OOS queries in a small sample than in a big set .
( iv ) NSC - BERT improves over the classifier model on the test samples by 1.5 in terms of clustering F1 and by more than 3 CEAF points , also achieving a better precision / recall balance ( same as for CNN modality ) .
We hypothesize here an advantage of the supervised clustering model might lie over the classification models , which are generally not as well adaptive to class imbalance in data .
( v) Again , the NSC - BERT highly improves ( at least 2 times ) the recall of the classifier for the OOS task .
Error analysis Analysing the output of NSC ( here , we limit the discussion to NSC - BERT in the disjoint scenario of Sec. 7.2 ) , we discovered that the majority of the mistakes made by the clustering algorithm can be traced back to several interpretable causes .
A trivial case of word overlap or generally string matching in Ex .
4 made NSC put the examples of seemingly distinct classes together .
Actual ground truth intent classes are denoted in parentheses .
( 4 ) cluster : ( 1 ) what is the reason humans even exist ( mean-ing_of_life ) ( 2 ) let me know if you are a human or are a computer ( are_you_a_bot ) Next , we find the presence of the word-indicators of the same semantic category , i.e. , SPEED , in Ex. 5 , that misled NSC . ( 5 ) cluster : ( 1 ) speak more quickly ( change_speed ) ( 2 ) i'm in the mood for slow songs and nothing else ( play_music ) ( 3 ) talk faster ( change_speed )
A frequent type of NSC 's mistakes is merging together instances of different intent classes which belong to the same topic domain , especially in case of rather close subtopics as in cluster Ex .-s 6 - 7 . ( 6 ) cluster : ( 1 ) put on my 90s playlist ( play_music ) ( 2 ) put on some metallica music ( play_music ) ( 3 ) what kind of music on the speaker now ( what_song ) ( 7 ) cluster : ( 1 ) how do i freeze my bank account ( freeze_account ) ( 2 ) why is there a stop on my deposit account ( ac- count_blocked )
In addition , Ex . 7 has another complicating factor of using semantically very close expressions for distinct intent concepts .
Right the opposite situation of erroneously splitting the instances of the same intent class is also common , as in Ex .-s 8- 9 . ( 8 ) cluster : ( 1 ) bye bye then ( goodbye ) ( 9 ) cluster : ( 1 ) good speaking to you ( goodbye ) ( 2 ) it was great talking to you ( goodbye )
In general , we assume , that the last two types of mistakes can be reduced if the model sees on training the data from the corresponding intent classes .
NSC also drew some ( not absolutely meaningless ) connections between OOS queries ( Ex. 10 ) . ( 10 ) cluster : ( 1 ) what is the highest quality carpet available ( oos ) ( 2 ) find schematics for ikea desk assembly ( oos ) ( 3 ) i have a super runny nose and want to find a doctor ( oos ) ( 4 ) what was the latest tremor on the richter scale ( oos )
And finally , the clustering decision in Ex. 11 potentially highlights an annotation error of query ( 2 ) being a false positive OOS . ( 11 ) cluster : ( 1 ) where is the closest mcdonald 's to foxwoods casino ( directions ) ( 2 ) where is the closest driving range ( oos )
Discussion
In this section , we discuss some of the important findings of our paper :
First , our experiments suggest that the transformer model boosts the performance of our clustering approach .
This is justified by the mainstream research : with respect to the standard embeddings ( word2vec , glove , .. ) , transformer models provide contextual representation of words , i.e. , the embedding of a word is defined with respect to the others that are in the same piece of text .
They provide a very powerful representation of pieces of text .
Thus , we can obtain a precise similarity between pairs of questions .
Thanks to our structural loss function , we can back - propagate structure properties of the entire cluster back to the transformer models so that we enrich even more its contextual similarity .
Second , in the field of dialog systems , our approach can be extended to jointly predict intent and slot attributes .
NSC can use information about slots and the background knowledge given by attributes and values , to cluster questions into intents .
The latter will be then more related to the specific task defined by the available slot information .
Conversely , if we suppose the developer has already the intents , our clustering algorithm could be used to cluster values into attributes .
Then , since NSC can reach performance similar to supervised classification methods , it would be interesting to see if it can be more accurate than them , considering the critical problems of transfer learning ( i.e. , when the data for training is different from the one the deployed system receives ) .
Third , we showed the performance on NSC exactly on unseen clusters .
Our approach only uses some clusters of the data for training ( each cluster is a training example ) .
Then , it can predict unseen clusters in the test set .
In other words , our models generalize what they learn from some clusters to unseen clusters .
Finally , given one of our models trained on a set of clusters , we can easily continue its training with new examples , i.e. , new training clusters , as our neural architecture is an online framework .
One main scalability question could be : Given one domain for which we have clusters to train our approach , how can we scale to other domains ?
We will need new clusters for the new domains , i.e. , target domain data , which is typically used for effective transfer learning .
This does not mean that we need a large number of clusters , we just need some of them to transfer our clustering model from one domain to another .
The transferred models will be able to predict many more new clusters from the new target domain .
Conclusions
In this work , we firstly proposed supervised neural clustering based on traditional LSSVM and LSP models , which hinge on optimizing the structural margin loss .
This extends the structured prediction methods for supervised clustering to a neural setting .
Our experiments on IC&OOS and Quora Intent Corpora show an impressive improvement over the state of the art , 17.24 % absolute over unsupervised models , and 8 % points more than our proposed semi-supervised approaches .
This suggests that our neural structured prediction can ( i ) effectively optimize a structural clustering objective function on structured examples , such as sets of questions for intent detection , and ( ii ) uncover clusters of questions of unseen classes , i.e. , potential intents not seen in training .
Table 1 : 1 Comparison of clustering models : completely unsupervised , using supervised instance similarity function , and our supervised clustering on the test set of IC&OOS by Larson et al . ( 2019 ) ; disjoint scenario .
based on
Table 2 : 2 lower than model using tf - idf Comparison of our neural models to the structural baselines on the manual test set of Quora Intent Corpus .
3370
Table 3 : 3 Comparison of our neural models to the structural baselines on the automatic test set of Quora Intent corpus.
scores , is clear evidence for this .
When we run Kruskal 's on top of the output of the pairwise classifier , we observe a huge bias , towards precision in case of CNN and recall in case of BERT .
The use of a threshold does not seem robust , when training examples ( query pairs ) are treated as independent .
Table 4 : 4 Accuracy comparison on question duplicate detection task on Quora split by Wang et al . ( 2017 ) . BiMPM
Our fine- tuned BERT Accuracy 88.17 90.88 Model Inclass Acc. OOS Recall Larson et al. ( 2019 ) NN + avg. FastText emb. 84.50 23.20 CNN + Glove emb .
88.90 22.20 BERT 96.40 40.90 Our Models CNN 80.20 28.88 BERT 94.87 38.80
Table 5 : 5 Comparison of our intent classification baselines to the intent classification models from Larson et al . ( 2019 ) on IC&OOS test set .
Table 6 : 6 Comparison of the neural clusterings models to the classification baselines on the test set of IC&OOS dataset by Model Clustering measure Precision Recall F1 CEAFe OOS Recall CNN intent classifier 88.70 93.14 90.76?0.93 86.12?0.75 28.88 BERT intent classifier 90.53 98.36 94.29?0.24 89.10?0.40 38.80 NSC-CNN 89.32 91.25 90.24?1.09 84.86?1.34 85.49?3.94 NSC -BERT 93.58 97.27 95.38?0.34 92.05?0.58 71.45?4.35 Larson et al. ( 2019 ) ; full scenario .
https://github.com/iKernels/intent-qa
The scoring function follows the standard formulation of structured prediction tasks , where the score of a structure is computed by aggregating the scores of its constituent parts .
In our case , it is a summation of edge scores .
The reader may refer to the work on dependency parsing by Kiperwasser and Goldberg ( 2016 ) .
We use the Small variant of the dataset . 4
The split of the classes into three sets is done randomly without reference to the 10 original topic domains .
https://ikernels-portal.disi.unitn.it/ repository/ intent-qa 6 https://www.kaggle.com/c/quora-question-pairs
http://haifengl.github.io/smile/
https://www.quora.com/q/quoradata/
First-Quora-Dataset-Release-Question-Pairs
