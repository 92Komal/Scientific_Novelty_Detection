title
Capturing Row and Column Semantics in Transformer Based Question Answering over Tables
abstract
Transformer based architectures are recently used for the task of answering questions over tables .
In order to improve the accuracy on this task , specialized pre-training techniques have been developed and applied on millions of open-domain web tables .
In this paper , we propose two novel approaches demonstrating that one can achieve superior performance on table QA task without even using any of these specialized pre-training techniques .
The first model , called RCI interaction , leverages a transformer based architecture that independently classifies rows and columns to identify relevant cells .
While this model yields extremely high accuracy at finding cell values on recent benchmarks , a second model we propose , called RCI representation , provides a significant efficiency advantage for online QA systems over tables by materializing embeddings for existing tables .
Experiments on recent benchmarks prove that the proposed methods can effectively locate cell values on tables ( up to ?98 % Hit@1 accuracy on WikiSQL lookup questions ) .
Also , the interaction model outperforms the state - of - the - art transformer based approaches , pre-trained on very large table corpora ( TAPAS and TABERT ) , achieving ?3.4 % and ?18.86 % additional precision improvement on the standard WikiSQL benchmark 1 .
Introduction Tabular data format is a commonly used layout in domain specific enterprise documents as well as open domain webpages to store structured information in a compact form ( Pasupat and Liang , 2015 ; Canim et al. , 2019 ) .
In order to make use of these resources , many techniques have been proposed for the retrieval of tables ( Cafarella et al. , 2008 ; Zhang and Balog , 2018 ; Venetis et al. , 2011 ; Shraga et al. , 2020 ; Sun et al. , 2016 ) .
Given a large corpus of documents , the goal in these studies is to retrieve top-k relevant tables based on given keyword ( s ) .
The user is then expected to skim through these tables and locate the relevant cell values which is a tedious and time consuming task .
More recently , popular search engines made significant improvement in understanding natural language questions and finding the answers within passages , owing to the developments in transformer based machine reading comprehension ( MRC ) systems ( Rajpurkar et al. , 2016 ( Rajpurkar et al. , , 2018 Kwiatkowski et al. , 2019 ; Pan et al. , 2019 ; Alberti et al. , 2019a ) .
One natural extension of these systems is to answer questions over tables .
These questions are broadly classified into two types : Lookup and Aggregation .
Lookup questions require returning exact strings from tables such as cell values whereas Aggregation questions are executed by performing an arithmetic operation on a subset of the column cells , such as Min ( ) , Max ( ) , Average ( ) and Count ( ) .
For look - up questions , the users can verify if the returned cell values from the table ( s ) are correct , while this is not applicable for Aggregation questions because a scalar value is returned as an answer .
Our primary focus in this paper is on Lookup questions since the answers are verifiable by users although our proposed techniques outperform the state - of- the- art ( SOTA ) approaches on both question types .
In this paper , we propose a new approach to table QA that independently predicts the probability of containing the answer to a question in each row and column of a table .
By taking the Row and Column Intersection ( RCI ) of these probabilistic predictions , RCI gives a probability for each cell of the table .
These probabilities are either used to answer questions directly or highlight the relevant regions of tables as a heatmap , helping users to easily locate the answers over tables ( See Figure 1 for a question answered with the help of a heatmap ) .
We developed two models for RCI , called RCI interaction and RCI representation .
In order to evaluate these approaches , we also propose a weakly supervised MRC system as a strong baseline to identify / " read " relevant cells of a table .
In this baseline approach , we convert tables into passages and extract a relevant span of text within these passages .
The interaction model is designed to provide very high accuracy on finding cell values over tables for a given natural language question .
We demonstrate that without even using any specialized pre-trained models , we can achieve up - to ?98 %
Hit@1 accuracy on finding cell values of tables for lookup questions from the WikiSQL benchmark .
Also , the interaction model outperforms the state - of - the - art transformer based approaches , TAPAS ( Herzig et al. , 2020 ) and TABERT ( Yin et al. , 2020 ) , achieving ?3.4 % and ?18.86 % additional precision improvement on the standard Wik-iSQL benchmark , containing both lookup and aggregation questions .
While the interaction model yields very high accuracy on the benchmarks , the representation model has the advantage of pre-computing the embeddings for all tables in a corpus and storing them for online query processing .
Once a user query is received , the most relevant tables can be retrieved from a table retrieval system and relevant cell values can be highlighted using the existing embeddings of the tables , resulting in less computation per received user query , as opposed to running tables over expensive transformer architecture for every received query .
The specific contributions of this paper are as follows : ?
An MRC based strong baseline for table QA task :
We investigate a transfer learning approach by utilizing a fully supervised reading comprehension system built on top of a large pre-trained language model .
Specifically , it is first fine-tuned on SQuAD then on Natural Questions and lastly trained on the table datasets .
The final model is used to iden-tify relevant cells of a table for a given natural language question .
?
A transformer based interaction model for the table QA task :
We propose a model for table QA task that concatenates a textual representation of each row ( or column ) to the text of the question and classifies the sequence pair as positive ( the row / column contains the answer ) or negative ( the row / column does not contain the answer ) .
The proposed approach yields very high accuracy on our benchmarks , outperforming the SOTA models .
?
A transformer based representation model for the table QA task :
We propose a representation model that builds vector representations of the question and each row ( or column ) to compare the resulting vectors to determine if the row ( or column ) contains the answer .
The proposed approach is preferred for efficiency purposes on online table retrieval systems since it enables materializing embeddings for existing tables and re-using them during online question answering over multiple tables .
In the following sections , we first review the prior work on QA systems over tables as well as table search from large corpora in Section 2 .
We then describe a weakly supervised machine reading comprehension ( MRC ) system as a baseline that is capable of answering questions over tables in Section 3 .
In Section 4 , we introduce two models that decompose TableQA as the intersection between rows and columns of a table using a transformer architecture .
Experimental results are reported and discussed in Section 5 and finally Section 6 concludes the paper and discusses the future work .
Related Work QA from text :
There is plenty of work on QA from plain text ( Brill et al. , 2002 ; Lin , 2007 ; Pa?ca , 2003 ; Kwiatkowski et al. , 2019 ; Pan et al. , 2019 ) .
Typical strategies rely on token overlap between the question and passage text either based on a bag of word statistics or contextualized language model representations .
In either case , tabular structure is not leveraged to capture semantic relationships between rows and columns .
As we show in Section 5 , these strategies are insufficient for answering questions over tables with high precision .
QA over tables :
Our work mostly relates to the previous research on QA over tables ( Pasupat and Liang , 2015 ; Sun et al. , 2016 ; Dasigi et al. , 2019 ) .
They center around answering factoid questions and return the exact cell of a table that answers the query .
We briefly describe here how these works are different .
Pasupat and Liang ( 2015 ) assume access to the ' gold ' table that contains the answer to the input question .
They build a semantic parser that parses the query to a logical form .
They likewise convert the table into a knowledgegraph and execute the logical form on it to get the answer .
A more advanced semantic parsing based methodology has been recently proposed by Dasigi et al . ( 2019 ) .
This system is pre-trained on Wik-iTablesQuestions ( Pasupat and Liang , 2015 ) .
The proposed approach leverages an LSTM encoderdecoder model where tables are first converted to a knowledge - graph and word tokens in the questions are linked to table entities ( columns and cells ) .
The questions and linked table entities are then encoded into representation vectors which are decoded to executable ?- DCS logical forms .
This logical forms are executed over a knowledge graph to get answer predictions .
Our approach is different , since we do not convert natural language questions into logical forms and execute them on tables .
Instead , we leverage transformer architectures pre-trained on large corpora and further trained on finding cell values on tables .
In Section 5 , we show that we achieve significant improvement over this approach without using any semantic parser technique .
Sun et al. ( 2016 ) focus on the table retrieval problem over table corpora by leveraging the content of cell values and headers .
For a given query , they extract answers from millions of tables in the provided corpus .
They construct a unified chain representation of both the input question and the table cells and then find the table cell chain that best matches the question chain .
As opposed to this work , we primarily focus on answering questions over a single table rather than the retrieval of top -k tables from a corpus .
More recently , transformer based pre-training approaches have been introduced in TABERT ( Yin et al. , 2020 ) and TAPAS ( Herzig et al. , 2020 ) to improve accuracy for table QA .
TABERT has been pre-trained on 26 million tables and NL sentences extracted from Wikipedia and WDC WebTable Corpus ( Yin et al. , 2020 ) .
The model can be plugged into a neural semantic parser as an encoder to pro-vide contextual embeddings for tables .
Herzig et al. on the other hand , claim that semantic parsers incur an extra overhead of computing intermediate logical representations which can be avoided by leveraging fine-tuned models to answer questions over tables .
The model in TAPAS has been pretrained on about 6 million tables extracted from Wikipedia content .
Our work is different from both TAPAS and TABERT .
First and foremost , our focus in this paper is not on pre-training a new model for table QA , but rather on leveraging the existing language models to find the connection between a question and table columns / rows with very high accuracy .
Second , our goal is to provide a heatmap over tables on an end-to - end table retrieval system to help users to quickly identify the regions of tables where the answers would most likely appear .
Because the transformer architectures are quite expensive to query , the representation model we propose radically reduces the computational overhead during online query processing .
Table search over the web : Another active research area in NLP is searching over web tables .
There are numerous search algorithms that have been explored such as keyword search ( Cafarella et al. , 2008 ; Zhang and Balog , 2018 ; Venetis et al. , 2011 ; Shraga et al. , 2020 ) , retrieve similar tables ( Das Sarma et al. , 2012 ) , retrieve tables based on column names ( Pimplikar and Sarawagi , 2012 ) and adding new columns to existing entity lists ( Yakout et al. , 2012 ; Zhang and Chakrabarti , 2013 ) .
This thread of work focuses on retrieval of top -k tables with high precision from large corpora , rather than finding relevant rows and columns within tables .
MRC Model
We provide a brief description of our underlying Machine Reading Comprehension ( MRC ) model architecture , which we use as a strong baseline .
The architecture is inspired by ( Alberti et al. , 2019 b ; Pan et al. , 2019 ; Glass et al. , 2020 ) and direct interested readers to their papers for more details .
Our MRC model follows the approach introduced by of starting with a pretrained transformer based language model ( LM ) and then fine-tuning MRC specific feed -forward layers on both general question answering datasets ( SQuAD 2.0 and NQ ) as well as the table specific question answers associated with the datasets in Section 5 .
We use ALBERT ( Lan et al. , 2020 ) as the underlying LM similar to models which achieve SOTA on the SQuAD 2.0 leaderboard ( Zhang et al. , 2020 b , a ) at the time of writing .
More specifically , we show results starting from the weights and dimensions of the base v2 version ( 25 M parameters ) of the LM shared by ( Lan et al. , 2020 ) .
We also experiment with the xxlarge v2 version ( 235 M parameters ) as well .
The input to the model is a token sequence ( X ) consisting of a question , passage , and special markers ( a [ CLS ] token for answerability classification and [ SEP ] tokens to dileneate between the query and passage ) .
The input token sequence is passed through a deep Transformer ( Vaswani et al. , 2017 ) network to output a sequence of contextualized token representations H. MRC then adds two dense layers followed by a softmax : ? b = sof tmax ( W 1 H ) , ? e = sof tmax ( W 2 H ) , where W 1 , W 2 ? R 1 ? De . D e denotes the dimensionality of the embeddings ( 768 for base v2 ) .
?
t b and ?
t e denote the probability of the t th token in the sequence being the answer beginning and end , respectively .
The model is trained using binary cross-entropy loss at each token position based on whether or not the annotated correct answer begins or ends at the t th token .
Unanswerable questions have their begin and end offsets set to the [ CLS ] token position .
At prediction time , a score is calculated for each possible span by summing the ?
t j b and ?
t i e at each possible i and j combination to identify the max scoring answer span .
The sum of the ? [ CLS ] b and ? [ CLS ] e is then subtracted from this max scoring answer span to produce a final score that can be used for thresholding ( i.e. , deciding whether to predict an answer or refrain from answering a question ) .
A few modifications are made in line with ( Alberti et al. , 2019 b ) to use MRC for the NQ dataset which introduces additional answer types [ short , long , yes , no , null ] .
Refer to the appendix for these details .
We fine- tune the model with the SQuAD 2.0 dataset and then the NQ dataset in line with ( Pan et al. , 2019 ; Glass et al. , 2020 ) , to produce a generic RC model comparable to the current SOTA .
We then train for an additional epoch on the subset of NQ which consists of short answer questions that need to be answered by lookup inside an HTML table .
This is about 5 % of the total NQ data ( ? 15 , 500 question - answer pairs ) .
Note that in these cases , the input " passage " text consists of textual representation of tables ( i.e. , we introduce tabs between columns and new line characters between rows ) ; so it is devoid of true row and column structure .
This pre-training and task adaptation strategy is inline with prior art ( Gururangan et al. , 2020 )
RCI Model Architecture
The Row-Column Intersection model ( RCI ) is motivated by the idea of decomposing lookup Table QA into two operations : the column selection and the row selection .
Combining the predicted answer probability of each row and the probability of each column gives a score for all cells in the table .
The highest scoring cell may then be returned as an answer , or highlighting may be applied to the table to aid a user in locating the relevant information .
Unlike the pointer network of an adapted Machine Reading Comprehension system ( described in Section 3 ) , the RCI model always gives a ranked list of cells rather than answer spans that may cross cell boundaries .
We observe that the process of identifying the correct column is often about matching the column header and the type of values in the column to the expected answer type of the question .
For example in Table 1 , the question has a lexical answer type of ' party ' and the column header for the correct column is ' Party ' and contains values that are political parties .
Identifying the correct row is often more difficult .
In the example given in Table 1 , it is sufficient to match either of the names in the question to the value in the ' Name ' column of the row .
Note that with weak supervision ( Min et al. , 2019 ) we do not know the correct row , so all occurrences of ' Pro-Administration ' are considered correct .
Both the Row and Column models of RCI are sequence - pair classifiers .
The question is one sequence and the text sequence representation of the row or column is the second sequence .
We consider two approaches to the sequence - pair classification task in RCI : Interaction and Representation .
Interaction models use the self attention of a transformer over the concatenated two sequences .
This is the standard approach to sequence - pair classification tasks , e.g. textual entailment ) ( Wang et al. , 2018 ) , in transformer based systems .
Representation models independently project each sequence of the sequence - pair to a vector , then compare those vectors .
Representation models are motivated by the need to improve efficiency for a practical system .
Considering the column classifier , the interaction model requires running a transformer over each question plus column sequence .
In contrast , the representation model can pre-process the collection of tables , producing a vector representation of each column for each table , independent of any query .
Then , at query time , the query is projected to a vector which is then combined with the vector for each column and classified with a single - layer network .
On the WikiTableQuestions - Lookup dev set , we see the column model 's time drop from 40 seconds to 0.8 seconds on a K80 GPU when ten queries are batch processed at once .
Let a table with m rows and n columns be defined as a header , H = [ h 1 , h 2 , ... , h n ] and cell values V = [ v i , j ] , 1 ? i ? m , 1 ? j ? n. A TableQA instance consists of a table , a question and a ground truth set of cell indices , T ? I ?
J , I = 1 , 2 , ... , m , J = 1 , 2 , ... , n. In principle , these ground truth cell positions could be annotated with the correct occurrences of the correct values .
However , this form of supervision may be too difficult to obtain .
We use weak supervision : the ground truth cell indices are found by matching the ground truth answer strings in the table .
To train the row and column classifier we find ground truth row and column indices : T r = { i | ?j : ( i , j ) ? T } T c = {j | ?i : ( i , j ) ? T } Although it is possible to na?vely construct a sequence representation of columns and rows by simply space separating the contents of each row or column , better performance can be achieved by incorporating the table structure in the sequence representation .
We focus on tables with a single header for columns , but this method could also be applied to tables with a hierarchical header , by first flattening the header .
The row ( S r i ) and column ( S c j ) sequence representations are formatted as : S r i = n j=1 ? h ( h j ) ? ? v ( v i , j ) S c j = ? h ( h j ) ? m i=1 ? v ( v i , j ) Where ? indicates concatenation and the functions ?
h and ?
v delimit the header and cell value contents .
For ?
h we append a colon token ( ' : ' ) to the header string , and for ?
v we append a pipe token ( ' | ' ) to the cell value string .
The particular tokens used in the delimiting functions are not important .
Any distinctive tokens can serve since the transformer will learn an appropriate embedding to represent their role as header and cell value delimiters .
Considering again the example in Table 1 , the first row would be represented as : Both the interaction and the representation models use the sequence representation described above .
In the case of the interaction model this sequence is then appended to the question with standard [ CLS ] and [ SEP ] tokens to delimit the two sequences .
This sequence pair is then input to a transformer encoder , ALBERT .
The final hidden state for the [ CLS ] token is used in a linear layer followed by a softmax to classify the column as either containing the answer or not .
In the representation model shown in Figure 2 the representations of the question ( r q ) and the jth column sequence ( r c ) are first computed independently .
The representations are taken from the vector that the transformer model produces for the [ CLS ] input token .
These vectors are then concatenated ( indicated as :) with their element - wise product ( indicated as ? ) and the element - wise square of their differences .
The probability that this column is the target for the question is then given by a softmax over a linear layer .
r ? = r q ?
r c v qc = r q : r c : r q ? r c : r ? ? r ? p( j ? T c ) = sof tmax ( Wv qc + b) 0 ALBERT C CLS S c ALBERT Q CLS Question Linear Layer and Softmax r q r c r q : r c : r q ? r c : ( r q -r c ) ? ( r q -r c ) Linear
Taking a threshold on the cell level confidences of the RCI model and aggregating by the predicted question type produces the final answer , either a list of cells for lookup questions or a single number for aggregation questions .
This approach requires full supervision , we must know the cells to be aggregated to train the RCI row and column classifiers as well as the type of aggregation to train the question classifier .
This type of supervision is available in the WikiSQL dataset , but not in WikiTable Questions .
Evaluation
To evaluate these three approaches , we adapt three standard TableQA datasets : WikiSQL ( Zhong et al. , 2017 ) , WikiTableQuestions ( Pasupat and Liang , 2015 ) and TabMCQ ( Jauhar et al. , 2016 ) .
Wik-iSQL and WikiTable Questions include both lookup questions as well as aggregation questions .
As mentioned in Section 1 , our primary focus in this paper is on lookup questions that require selection and projection operations over tables ( i.e. , identifying the row and column of a table with very high precision for a given natural language question ) .
We are releasing the processing and evaluation code for the datasets to support reproducibility 3 . Table 2 gives a summary of these datasets .
In WikiSQL , the ground truth SQL query is provided for each question , so questions involving an aggregation operation can be automatically excluded .
The lookup questions are 72 % of the Wik-iSQL benchmark .
WikiSQL has some questions ( < 3 % ) with multiple answers .
We treat these as a list of relevant items and use information retrieval metrics to measure the quality of a predicted ranked list of cells .
TabMCQ is a multiple-choice , lookup TableQA dataset over general science tables .
We discard the multiple -choice setting and treat it as a standard open-ended QA task .
However , some TabMCQ tables are very large .
Of the 68 tables , 17 have more than 50 rows , with two tables containing over a thousand rows .
We down - sample the rows that are not relevant for a given question , limiting the largest table size to 50 rows .
Unlike the other two datasets , these tables are not Wikipedia tables and have an unusual format .
A sample TabMCQ table is provided in the appendix .
WikiTable Questions does not provide a definitive indication for what questions are lookup questions .
To identify these questions we first filter questions with words indicating an aggregation , such as ' average ' , ' min ' , ' max ' , etc .
These questions were further filtered manually to get the WikiTableQuestions - Lookup set .
In order to evaluate our proposed approaches on these datasets we built three different systems and also used three existing models : IS - SP , provided by ( Dasigi et al. , 2019 ) , TABERT ( Yin et al. , 2020 ) and TAPAS ( Herzig et al. , 2020 ) . IS - SP is a semantic parsing based model trained on WikiTa- blesQuestions ( Pasupat and Liang , 2015 ) dataset ( See Section 2 for the details of this work ) .
For building their model we used the code provided in ( Gardner et al. , 2020 ) .
For TABERT we trained the model for WikiSQL using the lookup subset , and for WikiTable Questions we used the full training set and applied to the lookup subset .
For TAPAS we used the trained BASE ( reset ) models 4 for Wik -iSQL and applied to the lookup subsets of the dev and test sets .
The MRC and MRC xxl models are based on Machine Reading Comprehension , using the base v2 and xxlarge v2 versions of ALBERT .
Because this model returns a span rather than a cell prediction , we match each of the top-k span predictions to the closest cell , the cell with the lowest difference in its character offsets .
In case multiple of the top-k predictions map to the same cell , these predictions are merged .
We also evaluate the two approaches to RCI : interaction ( RCI inter ) and representation ( RCI repr ) .
Both models use the base v2 version of ALBERT .
Using the xxlarge v2 ALBERT , we also train another RCI interaction model , RCI xxl .
For the representation model we found comparable performance on the column classifier but much lower performance on the row classifier .
Therefore the RCI repr model uses a representation based classifier for columns , while still using the interaction classifier for rows .
The RCI inter model uses interaction classifiers for both rows and columns .
Because WikiSQL is the largest dataset by far , for TabMCQ and Wik-iTable Questions we first train models on WikiSQL , then fine tune on the target dataset .
This gives small but significant gains for TabMCQ but is critical to good performance on WikiTableQuestions .
All models except TAPAS produce a ranked list of top-k predictions .
We evaluate these predictions using the metrics of Mean Reciprocal Rank ( MRR ) and Hit@1 .
Mean Reciprocal Rank is computed by finding the rank of the first correct cell prediction for each question and averaging its reciprocal .
If a correct cell is not present in the top-k predictions , it is considered to have an infinite rank .
Hit@1 simply measures the fraction of questions that are correctly answered by the first cell prediction .
Results
Table 3 shows the results on the lookup versions of WikiSQL , TabMCQ , and WikiTableQuestions .
Both the interaction and the representation models of RCI outperform all other methods on WikiSQL , TabMCQ , and WikiTableQuestions .
Using the representation model for the column classifier reduces performance by less than two percent on WikiSQL , and less than three percent on TabMCQ , but up to seven percent on WikiTableQuestions .
On two of the three datasets both RCI inter and the more efficient RCI repr outperform MRC xxl with far fewer parameters and computational cost .
Similarly , RCI with ALBERT - base outperforms even the large version of TAPAS trained on WikiSQL , getting 94.6 % Hit@1 compared to the 89.43 % Hit@1 of TAPAS large .
In Section 4 we described the method to transform a table into sequence representations of the rows and columns .
We do an ablation study on the two larger datasets to understand the impact of incorporating table structure into the sequence representation relative to simply space separating the cell contents .
Table 5 shows that we make moderate but significant and consistent gains with this approach , over two percent in Hit@1 .
WikiSQL TabMCQ Model MRR Hit@1 MRR Hit@1 RCI inter 0.963 94.48 % 0.746 67.01 % - formatting 0.947 92.26 % 0.733 64.82 %
We also decompose the performance of the tested systems in terms of row and column accuracy .
The top predicted cell , if wrong , could have the wrong row , the wrong column , or both .
Table 6 shows that predicting the correct column is generally easier than predicting the correct row .
An interesting exception occurs with MRC on the WikiSQL benchmark : the row prediction is more accurate than the column prediction .
For the MRC system , the table is a sequence of column headers , followed by a sequence of rows .
Since the table is serialized in row-major order , all of the relevant information for a row is present locally , while the information for columns is distributed through the table sequence representation .
The RCI inter model is the best at both tasks , with RCI repr having the same performance at the row level task , since it uses the same model for rows .
The TabMCQ column level performance of MRC is within two percent of RCI inter , which may be surprising , especially considering its performance on WikiSQL .
TabMCQ tables are constructed in an unusual way that permits high column prediction performance for an MRC system .
The rows in TabMCQ have the structure of sentences , which is helpful for a system trained on the SQuAD and NQ reading comprehension tasks ( Refer to the appendix for a sample TabMCQ table ) .
Error Analysis
To better understand the advantages and disadvantages of the Row-Column Intersection approach , we examine the 20 cases in the dev set of WikiTableQuestions - Lookup where RCI inter does not provide the correct answer in first position but MRC xxl does .
We find nine cases where we could identify nothing that in principle prevents the RCI inter model from answering correctly .
We find seven cases where multiple rows need to be considered together , while the RCI models always consider rows independently .
WikiTable Questions includes some questions like Table 7 .
Although the answer to this question is a cell in the table , it requires something like aggregation to answer .
All rows for a given year must be checked to see if there is a ' 1 st ' in the Place column .
This violates a key assumption of RCI : that rows may be examined independently .
The final four cases also violate the assumptions of RCI .
In two cases the answer is in the header of the table , while RCI assumes that it will be a cell .
In one case the table extraction failed , and in the final case the question asks about the string length of one of the columns where the answer ( 8 ) happens to be in the table .
We also examine the cases where MRC xxl does not find the correct answer in first position but RCI inter does .
The most frequent error , occurring in eight of the seventeen cases , is a ' near-miss ' .
Either MRC xxl chooses a value from the wrong column in the right row or a value from the row before or after .
This is illustrated in We also conduct an error analysis of RCI xxl on the first 50 aggregation questions it misses on the dev set of WikiSQL .
The largest category , with 24 cases , is correct answers by RCI xxl counted wrong by mistakes in the ground truth .
Usually ( 23 ) the ground truth indicates that there should be COUNT aggregation when no aggregation is correct .
For example , " What is the rank of manager Rob Mcdonald ? " where Rank is one of the table columns is mistakenly indicated as a COUNT aggregation question .
The second largest category ( 9 ) occurs when the cells are ranked correctly , and the correct aggregation is predicted , but the threshold for choosing the cells to aggregate is too low ( 1 ) or too high ( 8 ) .
Another common error ( 7 ) occurs when RCI xxl predicts a lookup question with the answer in a similar numeric column when aggregation is required .
For example , the question " How many votes were taken when the outcome was " 6th voted out day 12 " ? " is asked for a table with a Votes column .
RCI xxl predicts it as a lookup question with the answer ( " 2 - 2 - 1 3 - 0 " ) from this column , while the ground truth is a COUNT aggregation .
The final significant category ( 7 ) is cases of questions that are unanswerable .
This can occur because the table does not contain an answer or because the answer cannot be computed from a SQL query , such as when the answer is a sub-string of a cell .
The final three error cases are : a wrong column is selected ( the episode number in series rather than the episode number in season ) ; the question " What is the result when the 3rd throw is not 8 ? " is interpreted as " What is the result when the 3rd throw is something other than 8 ? " rather than the ground truth " What is the result when the 3rd throw is literally ' not 8 ' ? " ; and non-Latin characters must be matched to select the correct row .
Conclusion
In this paper we propose two novel techniques , RCI interaction and RCI representation , to tackle the problem of locating answers over tables for given natural language questions .
These transformer based models are fine-tuned on ground truth tables to predict the probability of containing the answer to a question in the rows and columns of tables independently .
These probabilities are either used to answer questions directly or highlight the relevant regions of tables as a heatmap , helping users to easily locate the answers over tables .
Our experiments prove that the RCI model outperforms the state - of - the - art transformer based approaches pre-trained on very large table corpora ( TAPAS ( Herzig et al. , 2020 ) and TABERT ( Yin et al. , 2020 ) ) , achieving ?3.4 % and ?18.86 % additional precision improvement on the standard Wik-iSQL benchmark including both Lookup and Aggregation questions .
The representation model , on the other hand , enables pre-processing the tables and producing the embeddings to store and further use during online query processing , providing significant efficiency advantages without compromising much on the accuracy of finding cell values in tables .
As for the future work , we plan to explore the exploitation of domain-specific taxonomies and embeddings generated for domain-specific corpora to tackle the problem of answering natural language questions over tables in domains such as finance , aviation and health care .
and subsequent work in building MRC models using transformer networks .
