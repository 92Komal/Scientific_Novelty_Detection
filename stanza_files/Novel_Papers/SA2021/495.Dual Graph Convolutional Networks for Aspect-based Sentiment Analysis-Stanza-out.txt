title
Dual Graph Convolutional Networks for Aspect- based Sentiment Analysis
abstract
Aspect - based sentiment analysis is a finegrained sentiment classification task .
Recently , graph neural networks over dependency trees have been explored to explicitly model connections between aspects and opinion words .
However , the improvement is limited due to the inaccuracy of the dependency parsing results and the informal expressions and complexity of online reviews .
To overcome these challenges , in this paper , we propose a dual graph convolutional networks ( Du-alGCN ) model that considers the complementarity of syntax structures and semantic correlations simultaneously .
Particularly , to alleviate dependency parsing errors , we design a SynGCN module with rich syntactic knowledge .
To capture semantic correlations , we design a SemGCN module with self-attention mechanism .
Furthermore , we propose orthogonal and differential regularizers to capture semantic correlations between words precisely by constraining attention scores in the SemGCN module .
The orthogonal regularizer encourages the SemGCN to learn semantically correlated words with less overlap for each word .
The differential regularizer encourages the SemGCN to learn semantic features that the SynGCN fails to capture .
Experimental results on three public datasets show that our DualGCN model outperforms state - of - theart methods and verify the effectiveness of our model .
Introduction Sentiment analysis has become a popular topic in natural language processing ( Liu , 2012 ; Li and Hovy , 2017 ) .
Aspect- based sentiment analysis ( ABSA ) talks an entity - level oriented fine - grained sentiment analysis task that aims to determine sentiment polarities of given aspects in a sentence .
In Figure 1 : An example sentence with its dependency tree from the restaurant reviews .
This sentence contains two aspects but with opposite sentiment polarities .
Figure 1 , the comment is about a restaurant review .
The sentiment polarity of the two aspects " price " and " service " are positive and negative , respectively .
Thus , ABSA can precisely identify user 's attitudes towards a certain aspect , rather than simply assigning a sentiment polarity for a sentence .
The key point in solving the ABSA task is to model the dependency relationship between an aspect and its corresponding opinion expressions .
Nevertheless , there probably exist multiple aspects and different opinion expressions in a sentence .
To judge the sentiment of a particular aspect , previous studies ( Wang et al. , 2016 ; Tang et al. , 2016a ; Ma et al. , 2017 ; Chen et al. , 2017 ; Fan et al. , 2018 ; Huang et al. , 2018 ; Gu et al. , 2018 ) have proposed various recurrent neural networks ( RNNs ) with attention mechanisms to generate aspect-specific sentence representations and have achieved appealing results .
However , an inherent defect makes the attention mechanism vulnerable to noise in the sentence .
Take Figure 1 as an example ; for the aspect " service " , the opinion word " reasonable " may receive more attention than the opinion word " poor " .
However , the " reasonable " refers to another aspect , i.e. , " price " .
More recent efforts ( Zhang et al. , 2019 ; Sun et al. , 2019 b ; Huang and Carley , 2019 ; Zhang and Qian , 2020 ; Chen et al. , 2020 ; Liang et al. , 2020 ; Wang et al. , 2020 ; Tang et al. , 2020 ) have been de-voted to graph convolutional networks ( GCNs ) and graph attention networks ( GATs ) over dependency trees , which explicitly exploit the syntactic structure of a sentence .
Consider the dependency tree in Figure 1 ; the syntactic dependency can establish connections between the words in a sentence .
For example , a dependency relation exists between the aspect " price " and the opinion word " reasonable " .
However , two challenges arise when applying syntactic dependency knowledge to the ABSA task : 1 ) the inaccuracy of the dependency parsing results and 2 ) GCNs over dependency trees do not work well as expected on datasets that are not sensitive to syntactic dependency due to the informal expression and complexity of online reviews .
In this paper , we propose a novel architecture , the dual graph convolution network ( DualGCN ) , as shown in Figure 2 , to solve the aforementioned challenges .
For the first challenge , we use the probability matrix of all dependency arcs from a dependency parser to build a syntax - based graph convolutional network ( SynGCN ) .
The idea behind this approach is that the probability matrix representing dependencies between words contains rich syntactic information compared with the final discrete output of a dependency parser .
For the second , we construct a semantic correlation - based graph convolutional network ( SemGCN ) by utilizing a self-attention mechanism .
The idea behind this approach is that the attention matrix shaped by self-attending , also viewed as an edge-weighted directed graph , can represent semantic correlations between words .
Moreover , motivated by the work of DGEDT ( Tang et al. , 2020 ) , we utilize a BiAffine module to bridge relevant information between the SynGCN and SemGCN modules .
Furthermore , we design two regularizers to enhance our DualGCN model .
We observe that the semantically related terms of each word should not overlap .
Therefore , we encourage the attention probability distributions over words to be orthogonal .
To this end , we incorporate an orthogonal regularizer on the attention probability matrix for the SemGCN module .
Moreover , the two representations learned from the SynGCN and SemGCN modules should contain significantly distinct information captured by the syntactic dependency and the semantic correlation .
Therefore , we expect that the SemGCN module could learn semantic representations different from syntactic representations .
Thus , we propose a differential regularizer between the SynGCN and SemGCN modules .
Our contributions are highlighted as follows : ?
We propose a DualGCN model for the ABSA task .
Our DualGCN considers both the syntactic structure and the semantic correlation within a given sentence .
Specifically , our DualGCN integrates the SynGCN and SemGCN networks through a mutual BiAffine module . ?
We propose orthogonal and differential regularizers .
The orthogonal regularizer encourages the SemGCN network to learn an orthogonal semantic attention matrix , whereas the differential regularizer encourages the SemGCN network to learn semantic features distinct from the syntactic ones built from the SynGCN network .
Related Work Traditional sentiment analysis tasks are sentencelevel or document-level oriented .
In contrast , ABSA is an entity - level oriented and a more finegrained task for sentiment analysis .
Earlier methods ( Titov and McDonald , 2008 ; Jiang et al. , 2011 ; Kiritchenko et al. , 2014 ; Vo and Zhang , 2015 ) are usually based on handcrafted features and fail to model the dependency between the given aspect and its context .
Recently , various attention - based neural networks have been proposed to implicitly model the semantic relation of an aspect and its context to capture the opinion expression component ( Wang et al. , 2016 ; Tang et al. , 2016 a , b ; Ma et al. , 2017 ; Chen et al. , 2017 ; Fan et al. , 2018 ; Huang et al. , 2018 ; Gu et al. , 2018 ; Li et al. , 2018a ; Tan et al. , 2019 ) .
For instance , ( Wang et al. , 2016 ) proposed attentionbased LSTMs for aspect-level sentiment classification .
( Tang et al. , 2016 b ) and ( Chen et al. , 2017 ) both introduced a hierarchical attention network to identify important sentiment information related to the given aspect .
( Fan et al. , 2018 ) exploited a multi-grained attention mechanism to capture the word-level interaction between aspects and their context .
( Tan et al. , 2019 ) designed a dual attention 6321 network to recognize conflicting opinions .
In addition , the pre-trained language model BERT ( Devlin et al. , 2019 ) has achieved remarkable performance in many NLP tasks , including ABSA .
( Sun et al. , 2019a ) transformed ABSA task into a sentence pair classification task by constructing an auxiliary sentence .
( Xu et al. , 2019 ) proposed a post-training approach on the BERT to enhance the performance of fine-tuning stage for the ABSA task .
Another trend explicitly leverages syntactic knowledge .
This type of knowledge helps to establish connections between the aspects and the other words in a sentence to learn syntax - aware feature representations of aspects .
( Dong et al. , 2014 ) proposed a recursive neural network to adaptively propagate the sentiment of words to the aspect along the dependency tree .
( He et al. , 2018 ) introduced an attention model that incorporated syntactic information to compute attention weights .
( Phan and Ogunbona , 2020 ) utilized the syntactic relative distance to reduce the impact of irrelevant words .
Following this line , a few works extend the GCN and GAT models by means of a syntactical dependency tree and develop several outstanding models ( Zhang et al. , 2019 ; Sun et al. , 2019 b ; Huang and Carley , 2019 ; Wang et al. , 2020 ; Tang et al. , 2020 ) .
These works explicitly exploit the syntactic structure information to learn node representations from adjacent nodes .
Thus , the dependency tree shortens the distance between the aspects and opinion words of a sentence and alleviates the problem of long-range dependency .
Most recently , several works explore the idea of combining different types of graph for ABSA task .
For instance , ( Chen et al. , 2020 ) combined a dependency graph and a latent graph to generate the aspect representation .
( Zhang and Qian , 2020 ) observed the characteristics of word co-occurrence in linguistics and designed hierarchical syntactic and lexical graphs .
( Liang et al. , 2020 ) constructed aspect-focused and inter-aspect graphs to learn dependency feature of the key aspect words and sentiment relations between different aspects .
In this paper , we propose a GCN based method combining syntactic and semantic features .
We use a dependency probability matrix with richer syntactic information and elaborately design orthogonal and differential regularizers to enhance the ability to precisely capture the semantic associations .
Graph Convolutional Network ( GCN ) Motivated by conventional convolutional neural networks ( CNNs ) and graph embedding , a GCN is an efficient CNN variant that operates directly on graphs ( Kipf and Welling , 2017 ) .
For graph structured data , a GCN can apply the convolution operation on directly connected nodes to encode local information .
Through the message passing of multilayer GCNs , each node in a graph can learn more global information .
Given a graph with n nodes , the graph can be represented as an adjacency matrix A ? R n?n .
Most previous work ( Zhang et al. , 2019 ; Sun et al. , 2019 b ) extend GCN models by encoding dependency trees and incorporating dependency paths between words .
They build the adjacency matrix A over the syntactical dependency tree of a sentence .
Thus , an element A ij in A indicates whether the i-th node is connected to the j-th node .
Specifically , A ij = 1 if the i-th node is connected to the j-th node , and A ij = 0 otherwise .
In addition , the adjacency matrix A , composed of 0 and 1 , can be deemed as the final discrete output of a dependency parser .
For the i-th node at the l-th layer , formally , its hidden state representation , denoted as h l i , is updated by the following equation : h l i = ? ? ?
n j=1 A ij W l h l?1 j + b l ? ? ( 1 ) where W l is a weight matrix , b l is a bias term , and ? is an activation function ( e.g. , ReLU ) .
Proposed DualGCN
Figure 2 provides an overview of DualGCN .
In the ABSA task , a sentence - aspect pair ( s , a ) is given , where a = { a 1 , a 2 , ... , a m } is an aspect .
It is also a sub-sequence of the entire sentence s = {w 1 , w 2 , ... , w n }.
Then , we utilize BiLSTM or BERT as sentence encoder to extract hidden contextual representations , respectively .
For the BiL-STM encoder , we first obtain the word embeddings
For the BERT encoder , we construct a sentenceaspect pair " [ CLS ] sentence [ SEP ] aspect [ SEP ] " as input to obtain aspect-aware hidden representations of the sentence .
Moreover , in order to match the wordpiece - based representations of BERT with the result of syntactic dependency based on word , we expand dependencies of a word into its all of subwords .
Then , the hidden representations of sentence are input into the SynGCN and SemGCN modules , respectively .
A BiAffine module is then adopted for effective information flow .
Finally , we aggregate all the aspect nodes ' representations from the SynGCN and SemGCN modules via pooling and concatenation to form the final aspect representation .
Next , we elaborate on the details of our proposed DualGCN model .
x = {x 1 , x 2 , ... ,
Syntax - based GCN ( SynGCN )
The SynGCN module takes the syntactic encoding as input .
To encode syntactic information , we utilize the probability matrix of all dependency arcs from a dependency parser .
Compared to the final discrete output of a dependency parser , the dependency probability matrix could capture rich structural information by providing all latent syntactic structures .
Therefore , the dependency probability matrix is used to alleviate dependency parsing errors .
Here , we use the state - of - the - art dependency parsing model LAL - Parser ( Mrini et al. , 2019 ) .
With the syntactic encoding of an adjacency matrix A syn ?
R n?n , the SynGCN module takes the hidden state vectors H from BiLSTM as initial node representations in the syntactic graph .
The syntactic graph representation H syn = {h syn 1 , h syn 2 , ... , h syn n } is then obtained from the Syn-GCN module using Eq. ( 1 ) .
Here , h syn i ?
R d is a hidden representation of the i th node .
Note that for aspect nodes , we use symbols {h syn a 1 , h syn a 2 , ... , h syn am } to denote their hidden representations .
Semantic- based GCN ( SemGCN ) Instead of utilizing additional syntactic knowledge , as in SynGCN , SemGCN obtains an attention matrix as an adjacency matrix via a self-attention mechanism .
On the one hand , self-attention can capture the semantically related terms of each word in a sentence , which is more flexible than the syntactic structure .
One the other hand , SemGCN can adapt to online reviews that are not sensitive to syntactic information .
Self-Attention Self-attention ( Vaswani et al. , 2017 ) computes the attention score of each pair of elements in parallel .
In our DualGCN , we compute the attention score matrix A sem ?
R n?n using a self-attention layer .
We then take the attention score matrix
A sem as the adjacency matrix of our SemGCN module , which can be formulated as : A sem = softmax QW Q ? KW K T ? d ( 2 ) where matrices Q and K are both equal to the graph representations of previous layer of our SemGCN module , while W Q and W K are learnable weight matrices .
In addition , d is the dimensionality of the input node feature .
Note that we use only one self-attention head to obtain an attention score matrix for a sentence .
Similar to the SynGCN module , the SemGCN module obtains the graph representation H sem .
Additionally , we use the symbols {h sem a 1 , h sem a 2 , ... , h sem am } to denote the hidden representations of all aspect nodes .
BiAffine Module
To effectively exchange relevant features between the SynGCN and SemGCN modules , we adopt a mutual BiAffine transformation as a bridge .
We formulate the process as follows : H syn = softmax H syn W 1 ( H sem ) T H sem ( 3 ) H sem = softmax H sem W 2 ( H syn ) T H syn ( 4 ) where W 1 and W 2 are trainable parameters .
Finally , we apply average pooling and concatenation operations on the aspect nodes of the SynGCN and SemGCN modules .
Thus , we obtain the final feature representation for the ABSA task , i.e. , h syn a = f h syn a 1 , h syn a 2 , ... , h syn am ( 5 ) h sem a = f h sem a 1 , h sem a 2 , ... , h sem am ( 6 ) r = [ h syn a , h sem a ] ( 7 ) where f ( ? ) is an average pooling function applied over the aspect node representations .
Then , the obtained representation r is fed into a linear layer , followed by a softmax function to produce a sentiment probability distribution p , i.e. , p( a ) = softmax ( W p r + b p ) ( 8 ) where W p and b p are the learnable weight and bias .
Regularizer
To improve the semantic representation , we propose two regularizers for the SemGCN module , i.e. , orthogonal and differential regularizers .
Orthogonal Regularizer Intuitively , the related items of each word should be in different regions in a sentence , so the attention score distributions rarely overlap .
Therefore , we expect a regularizer to encourage orthogonality among the attention score vectors of all words .
Given an attention score matrix A sem ?
R n?n , the orthogonal regularizer is formulated as follows : R O = A sem A semT ? I F ( 9 ) where I is an identity matrix .
The subscript F denotes the Frobenius norm .
As a result , each nondiagonal element of A sem A semT is minimized to maintain the matrix A sem orthogonal .
Differential Regularizer
We expect that two types of feature representations learned from the Syn-GCN and SemGCN modules represent distinct information contained within the syntactic dependency trees and semantic correlations .
Therefore , we adopt a differential regularizer between the two adjacency matrices of the SynGCN and SemGCN modules .
Note that the regularizer is only restrictive to A sem and is given as R D = 1 A sem ?
A syn F . ( 10 )
Loss Function
Our training goal is to minimize the following total objective function : T = C + ? 1 R O + ? 2 R D + ? 3 ? 2 ( 11 ) where ?
1 , ? 2 and ?
3 are regularization coefficients and ? represents all trainable model parameters .
C is a standard cross-entropy loss and is defined for the ABSA task as follows : C = ? ( s , a ) ?
D c?C log p ( a ) ( 12 ) where D contains all sentence - aspect pairs and C is the collection of distinct sentiment polarities .
Experiments
Datasets
We conduct experiments on three public standard datasets .
The Restaurant and Laptop datasets are made public from the SemEval ABSA challenge ( Pontiki et al. , 2014 ) .
Following ( Chen et al. , 2017 ) , we remove the instances using the " conflict " label .
In addition , the Twitter dataset is a collection of tweets ( Dong et al. , 2014 ) .
All three datasets have three sentiment polarities : positive , negative and neutral .
Each sentence in these datasets is annotated with marked aspects and their corresponding polarities .
Statistics for the three datasets are shown in Table 1 .
Implementation Details
The LAL - Parser ( Mrini et al. , 2019 ) , which is used for dependency parsing , provides an off-the-shelf parser 2 . For all the experiments , we use pretrained 300 - dimensional Glove 3 vectors ( Pennington et al. , 2014 ) to initialize the word embeddings .
The dimensionality of the position ( i.e. , the relative position of each word in a sentence with respect to the aspect ) embeddings and part-of-speech ( POS ) embeddings is set to 30 .
Thus , we concatenate the word , POS and position embeddings and then input them into a BiLSTM model , whose hidden size is set to 50 .
To alleviate overfitting , we apply dropout at a rate of 0.7 to the input word embeddings of the BiLSTM .
The dropout rate of the SynGCN and SemGCN modules is set to 0.1 , and the number of SynGCN and SemGCN layers is set to 2 .
All the model weights are initialized from a uniform distribution .
We use the Adam optimizer with a learning rate of 0.002 .
The DualGCN model is trained in 50 epochs with a batch size of 16 .
The regularization coefficients , ? 1 and ?
2 are set to ( 0.2 , 0.3 ) , ( 0.2 , 0.2 ) and ( 0.3 , 0.2 ) for the three datasets , respectively , and ?
3 is set to 10 ?4 . For DualGCN + BERT , we use the bert- base-uncased 4 English version .
See our code for more details about BERT 's experiments .
Additionally , following ( Marcheggiani and Titov , 2017 ) , we add a self-loop for each node in the SynGCN and SemGCN modules .
Baseline Methods
We compare DualGCN with state- of- the - art baselines .
The models are briefly described as follows .
1 ) ATAE -LSTM ( Wang et al. , 2016 ) utilizes aspect embedding and the attention mechanism in aspectlevel sentiment classification .
2 ) IAN ( Ma et al. , 2017 ) employs two LSTMs and an interactive attention mechanism to generate representations for the aspect and sentence .
3 ) RAM ( Chen et al. , 2017 ) uses multiple attention and memory networks to learn the sentence representation .
4 ) MGAN ( Fan et al. , 2018 ) designs a multigrained attention mechanism to capture word-level interactions between the aspect and context .
5 ) TNet ( Li et al. , 2018 b ) transforms BiLSTM embeddings into target-specific embeddings and uses CNN to extract final embeddings for classification .
6 ) ASGCN ( Zhang et al. , 2019 ) first proposed using GCN to learn the aspect-specific representations for aspect-based sentiment classification .
7 ) CDT
( Sun et al. , 2019 b ) utilizes a GCN over a dependency tree to learn aspect representations with syntactic information .
8 ) BiGCN ( Zhang and Qian , 2020 ) uses hierarchical graph structure to integrate word co-occurrence information and dependency type information .
9 ) kumaGCN
( Chen et al. , 2020 ) employs a latent graph structure to complement syntactic features .
10 ) InterGCN ( Liang et al. , 2020 ) utilizes a GCN over a dependency tree to learn aspect representations with syntactic information .
11 ) R-GAT
( Wang et al. , 2020 ) proposes a aspectoriented dependency tree structure and then encodes new dependency trees with a relational GAT .
12 ) DGEDT
( Tang et al. , 2020 ) proposes a dependency graph enhanced dual- transformer network by jointly considering flat representations and graphbased representations .
13 ) BERT ( Devlin et al. , 2019 ) is the vanilla BERT model by feeding the sentence -aspect pair and using the representation of [ CLS ] for predictions .
14 ) R-GAT +BERT
( Wang et al. , 2020 ) is the R-GAT model that uses a pre-trained BERT to replace BiLSTM as an encoder .
15 ) DGEDT +BERT
( Tang et al. , 2020 ) is the DGEDT model that uses a pre-trained BERT to replace BiLSTM as an encoder .
Comparison Results
To evaluate the ABSA models , we use the accuracy and macro-averaged F1 - score as the main evaluation metrics .
The main experimental results are reported in Table 2 . Our DualGCN model consistently outperforms all attention - based and syntax - based methods on the Restaurant , Laptop and Twitter datasets .
These results demonstrates that our DualGCN effectively integrates syntactic knowledge and semantic information .
In addition , the DualGCN accurately fits datasets that contain formal , informal or complicated reviews .
Compared to attention - based methods such as ATAE - LSTM , IAN and RAM , our DualGCN model utilizes syntactic knowledge to establish dependencies between words , so it can avoid noises introduced by the attention mechanism .
Moreover , the syntaxbased methods , such as ASGCN , CDT , R- GAT and so on , achieve better performance than attentionbased methods , but they ignore the semantic correlation between words .
However , when considering informal or complicated sentences , using only syntactic knowledge results in poor performance .
In Table 2 , on the other side , the results from the last group shows that the basic BERT outperforms most of the models based on static word embedding .
Moreover , based on BERT , our DualGCN + BERT achieves better performance .
Ablation Study
To further investigate the role of modules in the DualGCN model , we conduct extensive ablation studies .
The results are reported in Table 2 .
The SynGCN - head model uses the discrete outputs of a dependency parser to construct the adjacency matrix of the GCNs .
In contrast , SynGCN leverages the probability matrix generated in a dependency parser as the adjacency matrix .
Case Study
Table 4 shows a few sample cases analyzed using different models .
The notations P , N and O represent positive , negative and neutral sentiment , respectively .
We highlight the aspect words in red and in blue .
For the aspect " food " in the first sample , the attention - based methods , i.e. , ATAE - LSTM and IAN , are prone to attend to the noisy word " dreadful " .
Although the syntactic dependency can establish direct connections between an aspect and some words , no association exists between the aspect and the opinion words for complicated sentences .
Take the second sample as an example ; the aspect " apple os " is far from the opinion word " happy " in terms of syntactic distance .
Thus , the SynGCN model fails .
Additionally , in the third sample , feature representations of the key words " did not " are not captured by the SynGCN model .
In contrast , the SemGCN model can attend to the semantic correlation between words .
The last two samples demonstrate that our DualGCN , which fully considers the complementarity of syntactic knowledge and semantic information , can address complicated and informal sentences with the help of the orthogonal and differential regularizers .
Attention Visualization
To attention score matrix are the attention probability distributions of " safari " and " browser " , respectively .
The information to which " safari browser " pays attention is redundant and it does not pay more attention to the key opinion word " quick " .
Thus , the DualGCN w/o R O &R D failed .
In comparison , in Figure 3 ( b ) , the attention score matrix produced by our DualGCN is relatively sparse .
Both " safari " and " browser " are semantically related to " quick " , and their other attended items are also semantically reasonable .
In addition , the attention scores of the related terms of each words tend to be distinct and precise due to the semantic constraints of these two regularizers .
Therefore , our DualGCN model can readily predict the correct sentiment polarity of the aspect " safari browser " .
Impact of the DualGCN Layer Number
To investigate the impact of the DualGCN layer number , we evaluate our DualGCN model with one to eight layers on the Restaurant and Laptop datasets .
As shown in Figure 4 , our model with two DualGCN layers performs the best .
On one the hand , node representations cannot propagate far when the number of layers is small .
On the other hand , if the number of layers is excessive , the model will become unstable due to the vanishing gradient and information redundancy .
Conclusion
In this paper , we propose a DualGCN architecture to address the disadvantages of attention - based and dependency - based methods for ABSA tasks .
Our Figure 2 : 2 Figure 2 : The overall architecture of DualGCN , which is composed primarily of SynGCN and SemGCN .
SynGCN uses the probability matrix generated by the dependency parser , while SemGCN leverages the attention score matrix generated by the self-attention layer .
The orthogonal and differential regularizers are designed to further improve the ability of capturing semantic correlations .
Details of these components are described in the main text .
