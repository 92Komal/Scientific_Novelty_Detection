title
Robust Question Answering Through Sub-part Alignment
abstract
Current textual question answering ( QA ) models achieve strong performance on in- domain test sets , but often do so by fitting surfacelevel patterns , so they fail to generalize to outof-distribution settings .
To make a more robust and understandable QA system , we model question answering as an alignment problem .
We decompose both the question and context into smaller units based on off- the-shelf semantic representations ( here , semantic roles ) , and align the question to a subgraph of the context in order to find the answer .
We formulate our model as a structured SVM , with alignment scores computed via BERT , and we can train end-to - end despite using beam search for approximate inference .
Our use of explicit alignments allows us to explore a set of constraints with which we can prohibit certain types of bad model behavior arising in cross-domain settings .
Furthermore , by investigating differences in scores across different potential answers , we can seek to understand what particular aspects of the input lead the model to choose the answer without relying on post-hoc explanation techniques .
We train our model on SQuAD v1.1 and test it on several adversarial and out -of- domain datasets .
The results show that our model is more robust than the standard BERT QA model , and constraints derived from alignment scores allow us to effectively trade off coverage and accuracy .
Introduction Current text - based question answering models learned end-to - end often rely on spurious patterns between the question and context rather than learning the desired behavior .
They may ignore the question entirely ( Kaushik and Lipton , 2018 ) , focus primarily on the answer type ( Mudrakarta et al. , 2018 ) , or otherwise bypass the " intended " mode of reasoning for the task ( Chen and Durrett , 2019 ; Niven and Kao , 2019 ) .
Thus , these models are not robust to adversarial attacks ( Jia and Liang , What day was Super Bowl 50 played on ?
The game was played on February 7 , 2016 ? Super Bowl 50 was an American football game to determine the champion ?
Adversarial sentence Sentence with correct answer
Question
The Champ Bowl was played on the day of August 18 , 1991 2017 ; : they can be fooled by surface - level distractor answers that follow the spurious patterns .
Methods like adversarial training ( Miyato et al. , 2016 ; Wang and Bansal , 2018 ; , data augmentation ( Welbl et al. , 2020 ) , and posterior regularization ( Pereyra et al. , 2016 ; have been proposed to improve robustness .
However , these techniques often optimize for a certain type of error .
We want models that can adapt to new types of adversarial examples and work under other distribution shifts , such as on questions from different text domains ( Fisch et al. , 2019 ) .
In this paper , we explore a model for text - based question answering through sub-part alignment .
The core idea behind our method is that if every aspect of the question is well supported by the answer context , then the answer produced should be trustable ( Lewis and Fan , 2018 ) ; if not , we suspect that the model is making an incorrect prediction .
The sub-parts we use are predicates and arguments from Semantic Role Labeling ( Palmer et al. , 2005 ) , which we found to be a good semantic representation for the types of questions we studied .
We then view the question answering procedure as a constrained graph alignment problem ( Sachan and Xing , 2016 ) , where the nodes represent the predi-cates and arguments and the edges are formed by relations between them ( e.g. predicate - argument relations and coreference relations ) .
Our goal is to align each node in the question to a counterpart in the context , respecting some loose constraints , and in the end the context node aligned to the wh-span should ideally contain the answer .
Then we can use a standard QA model to extract the answer .
Figure 1 shows an adversarial example of SQuAD ( Jia and Liang , 2017 ) where a standard BERT QA model predicts the wrong answer August 18 , 1991 .
In order to choose the adversarial answer , our model must explicitly align Super Bowl 50 to Champ Bowl .
Even if the model still makes this mistake , this error is now exposed directly , making it easier to interpret and subsequently patch .
In our alignment model , each pair of aligned nodes is scored using BERT .
These alignment scores are then plugged into a beam search inference procedure to perform the constrained graph alignment .
This structured alignment model can be trained as a structured support vector machine ( SSVM ) to minimize alignment error with heuristically - derived oracle alignments .
The alignment scores are computed in a black - box way , so these individual decisions are n't easily explainable ( Jain and Wallace , 2019 ) ; however , the score of an answer is directly a sum of the score of each aligned piece , making this structured prediction phase of the model faithful by construction ( Jain et al. , 2020 ) .
Critically , this allows us to understand what parts of the alignment are responsible for a prediction , and if needed , constrain the behavior of the alignment to correct certain types of errors .
We view this interpretability and extensibility with constraints as one of the principal advantages of our model .
We train our model on the SQuAD - 1.1 dataset ( Rajpurkar et al. , 2016 ) and evaluate on SQuAD Adversarial ( Jia and Liang , 2017 ) , Universal Triggers on SQuAD , and several out - of- domain datasets from MRQA ( Fisch et al. , 2019 ) .
Our framework allows us to incorporate natural constraints on alignment scores to improve zero-shot performance under these distribution shifts , as well as explore coverage - accuracy tradeoffs in these settings .
Finally , our model 's alignments serve as " explanations " for its prediction , allowing us to ask why certain predictions are made over others and examine scores for hypothetical other answers the model could give .
QA as Graph Alignment
Our approach critically relies on the ability to decompose questions and answers into a graph over text spans .
Our model can in principle work for a range of syntactic and semantic structures , including dependency parsing , SRL ( Palmer et al. , 2005 ) , and AMR ( Banarescu et al. , 2013 ) .
We use SRL in this work and augment it with coreference links , due to the high performance and flexibility of current SRL systems ( Peters et al. , 2018 ) .
Throughout this work , we use the BERT - based SRL system from Shi and Lin ( 2019 ) and the SpanBERT - based coreference system from Joshi et al . ( 2020 ) .
An example graph we construct is shown in Figure 2 .
Both the question and context are represented as graphs where the nodes consist of predicates and arguments .
Edges are undirected and connect each predicate and its corresponding arguments .
Since SRL only captures the predicateargument relations within one sentence , we add coreference edges as well : if two arguments are in the same coreference cluster , we add an edge between them .
Finally , in certain cases involving verbal or clausal arguments , there might exist nested structures where an argument to one predicate contains a separate predicate - argument structure .
In this case , we remove the larger argument and add an edge directly between the two predicates .
This is shown by the edge from was to determine ( labeled as nested structure ) in Figure 2 ) .
Breaking down such large arguments helps avoid ambiguity during alignment .
Aligning questions and contexts has proven useful for question answering in previous work ( Sachan et al. , 2015 ; Sachan and Xing , 2016 ; Khashabi et al. , 2018 ) .
Our framework differs from theirs in that it incorporates a much stronger alignment model ( BERT ) , allowing us to relax the alignment constraints and build a more flexible , highercoverage model .
Alignment Constraints
Once we have the constructed graph , we can align each node in the question to its counterpart in the context graph .
In this work , we control the alignment behavior by placing explicit constraints on this process .
We place a locality constraint on the alignment : adjacent pairs of question nodes must align no more than k nodes apart in the context .
k = 1 means we are aligning the question to a connected sub-graph in the context , k = ? means we can align to a node anywhere in a connected component in the context graph .
In our experiments , we set k = 3 .
In the following sections , we will discuss more constraints .
Altogether , these constraints define a set A of possible alignments .
3 Graph Alignment Model
Model Let T represent the text of the context and question concatenated together .
Assume a decomposed question graph Q with nodes q 1 , q 2 , . . . , q m represented by vectors q 1 , q 2 , . . . , q m , and a decomposed context C with nodes c 1 , . . . , c n represented by vectors c 1 , . . . , c n .
Let a = ( a 1 , . . . , a m ) be an alignment of question nodes to context nodes , where a i ? { 1 , . . . , n} indicates the alignment of the ith question node .
Each question node is aligned to exactly one context node , and multiple question nodes can align to the same context node .
We frame question answering as a maximization of an alignment scoring function over possible alignments : max a?A f ( a , Q , C , T ) .
In this paper , we simply choose f to be the sum over the scores of all alignment pairs f ( a , Q , C , T ) = n i=1 S( q i , c a i , T ) , where S(q , c , T ) denotes the alignment score between a question node q and a context node c .
This function relies on BERT to compute embeddings of the question and context nodes and will be described more precisely in what follows .
We will train this model as a structured support vector machine ( SSVM ) , described in Section 3.2 .
Q q y 9 N Q v 6 O u + z t m f K S + L Q 4 j u Q = " > A A A C J 3 i c b V D L S s N A F J 3 4 r P U V d e l m s B Q q l J J U Q T e V Y j c u W + g L m h A m 0 0 k 7 d P J w Z i K U 0 L 9 x 4 6 + 4 E V R E l / 6 J 0 z Z I b T 1 w 4 X D O v d x 7 j x s x K q R h f G l r 6 x u b W 9 u Z n e z u 3 v 7 B o X 5 0 3 B Z h z D F p 4 Z C F v O s i Q R g N S E t S y U g 3 4 g T 5 L i M d d 1 S b + p 0 H w g U N g 6 Y c R 8 T 2 0 S C g H s V I K s n R b / J e A T n l i l l s F G v F 5 j m s Q M t H c u h 6 y f 3 E K U M L 9 0 P 5 K + G J Y 2 Y X f N P R c 0 b J m A G u E j M l O Z C i 7 u i v V j / E s U 8 C i R k S o m c a k b Q T x C X F j E y y V i x I h P A I D U h P 0 Q D 5 R N j J 7 M 8 J z C u l D 7 2 Q q w o k n K m L E w n y h R j 7 r u q c 3 i i W v a n 4 n 9 e L p X d t J z S I Y k k C P F / k x Q z K E E 5 D g 3 3 K C Z Z s r A j C n K p b I R 4 i j r B U 0 W Z V C O b y y 6 u k X S 6 Z F 6 V y 4 z J X v U 3 j y I B T c A Y K w A R X o A r u Q B 2 0 A A a P 4 B m 8 g X f t S X v R P r T P e e u a l s 6 c g D / Q v n 8 A P D m j 2 A = = < / l a t e x i t >
q2 < l a t e x i t s h a 1 _ b a s e 6 4 = " S D w 5 x I G u 2 j Y K W F O 9 q d w T L k A R H t w = " > A A A C J 3 i c b V D L S s N A F J 3 4 r P U V d e l m s B Q q l J J U Q T e V Y j c u W + g L m h A m 0 0 k 7 d P J w Z i K U 0 L 9 x 4 6 + 4 E V R E l / 6 J 0 z Z I b T 1 w 4 X D O v d x 7 j x s x K q R h f G l r 6 x u b W 9 u Z n e z u 3 v 7 B o X 5 0 3 B Z h z D F p 4 Z C F v O s i Q R g N S E t S y U g 3 4 g T 5 L i M d d 1 S b + p 0 H w g U N g 6 Y c R 8 T 2 0 S C g H s V I K s n R b / J e A T n l i l l s F G v F 5 j m s Q M t H c u h 6 y f 3 E K U M L 9 0 P 5 K + G J Y 2 Y X f U f P G S V j B r h K z J T k Q I q 6 o 7 9 a / R D H P g k k Z k i I n m l E 0 k 4 Q l x Q z M s l a s S A R w i M 0 I D 1 F A + Q T Y S e z P y c w r 5 Q + 9 E K u K p B w p i 5 O J M g X Y u y 7 q n N 6 o 1 j 2 p u J / X i + W 3 r W d 0 C C K J Q n w f J E X M y h D O A 0 N 9 i k n W L K x I g h z q m 6 F e I g 4 w l J F m 1 U h m M s v r 5 J 2 u W R e l M q N y 1 z 1 N o 0 j A 0 7 B G S g A E 1 y B K r g D d d A C G D y C Z / A G 3 r U n 7 U X 7 0 D 7 n r W t a O n M C / k D 7 / g E 9 v a P Z < / l a t e x i t >
c1 < l a t e x i t s h a 1 _ b a s e 6 4 = " 8 G f 0 S S h 4 r E F H x r z m Z f B 2 7 1 / G E x Y = " > A A A C J 3 i c b V D L S g M x F M 3 4 r P U 1 6 t J N s B Q q l D J T B d 1 U i t 2 4 b K E v 6 A x D J s 2 0 o Z m H S U Y o Q / / G j b / i R l A R X f o n p u 0 g t f V A 4 O S c e 7 n 3 H j d i V E j D + N L W 1 j c 2 t 7 Y z O 9 n d v f 2 D Q / 3 o u C 3 C m G P S w i E L e d d F g j A a k J a k k p F u x A n y X U Y 6 7 q g 2 9 T s P h A s a B k 0 5 j o j t o 0 F A P Y q R V J K j 3 + S 9 A n L K F b P Y K N a K z X N Y g Z a P 5 N D 1 k v u J U 4 Y W 7 o f y V 8 I T x 8 w u f h w 9 Z 5 S M G e A q M V O S A y n q j v 5 q 9 U M c + y S Q m C E h e q Y R S T t B X F L M y C R r x Y J E C I / Q g P Q U D Z B P h J 3 M 7 p z A v F L 6 0 A u 5 e o G E M 3 W x I 0 G + E G P f V Z X T H c W y N x X / 8 3 q x 9 K 7 t h A Z R L E m A 5 4 O 8 m E E Z w m l o s E 8 5 w Z K N F U G Y U 7 U r x E P E E Z Y q 2 q w K w V w + e Z W 0 y y X z o l R u X O a q t 2 k c G X A K z k A B m O A K V M E d q I M W w O A R P I M 3 8 K 4 9 a S / a h / Y 5 L 1 3 T 0 p 4 T 8 A f a 9 w 8 m 1 6 P K < / l a t e x i t >
c2 < l a t e x i t s h a 1 _ b a s e 6 4 = " 3 r 4 G l b q G Y N H X 2 y V 7 d R z W Z D e 3 Z C g = " > A A A C J 3 i c b V D L S g M x F M 3 4 r O O r 6 t J N s B Q q l D I z C r q p F L t x 2 U J f 0 J Y h k 2 b a 0 M z D J C O U Y f 7 G j b / i R l A R X f o n Z t o i t f V A 4 O S c e 7 n 3 H i d k V E j D + N L W 1 j c 2 t 7 Y z O / r u 3 v 7 B Y f b o u C W C i G P S x A E L e M d B g j D q k 6 a k k p F O y A n y H E b a z r i a + u 0 H w g U N / I a c h K T v o a F P X Y q R V J K d v c m 7 B W R b Z b N Y L 1 a L j X N Y h j 0 P y Z H j x v e J b c E e H g T y V 8 K J b e o L H 8 v O 5 o y S M Q V c J e a c 5 M A c N T v 7 2 h s E O P K I L z F D Q n R N I 5 T 9 G H F J M S O J 3 o s E C R E e o y H p K u o j j 4 h + P L 0 z g X m l D K A b c P V 8 C a f q Y k e M P C E m n q M q 0 x 3 F s p e K / 3 n d S L r X / Z j 6 Y S S J j 2 e D 3 I h B G c A 0 N D i g n G D J J o o g z K n a F e I R 4 g h L F a 2 u Q j C X T 1 4 l L a t k X p S s + m W u c j u P I w N O w R k o A B N c g Q q 4 A z X Q B B g 8 g m f w B t 6 1 J + 1 F + 9 A + Z 6 V r 2 r z n B P y B 9 v 0 D K F u j y w = = < / l a t e x i t > S ( q2 , c1 , T ) = q2 ? c1 < l a t e x i t s h a 1 _ b a s e 6 4 = " E p 4 + o F m V 3 c l 2 J A S h y d W r 7 0 w D p x I = " > A A A C I n i c b V D L S g M x F M 3 U V 6 2 v U Z d u g k W o U M p M F d S F U H T j s m J f 0 J Y h k 2 b a 0 E x m T D J C G e Z b 3 P g r b l w o 6 k r w Y 0 z b o W j r g c D J O f d y 7 z 1 u y K h U l v V l Z J a W V 1 b X s u u 5 j c 2 t 7 R 1 z d 6 8 h g 0 h g U s c B C 0 T L R Z I w y k l d U c V I K x Q E + S 4 j T X d 4 P f a b D 0 R I G v C a G o W k 6 6 M + p x 7 F S G n J M S / u C v d O u Q i x Y x d h x 0 d q 4 H p x L T m G l 7 O f 9 h P Y w b 1 A z S R d n T h m 3 i p Z E 8 B F Y q c k D 1 J U H f O j 0 w t w 5 B O u M E N S t m 0 r V N 0 Y C U U x I 0 m u E 0 k S I j x E f d L W l C O f y G 4 8 O T G B R 1 r p Q S 8 Q + n E F J + r v j h j 5 U o 5 8 V 1 e O d 5 T z 3 l j 8 z 2 t H y j v v x p S H k S I c T w d 5 E Y M q g O O 8 Y I 8 K g h U b a Y K w o H p X i A d I I K x 0 q j k d g j 1 / 8 i J p l E v 2 S a l 8 e 5 q v X K V x Z M E B O A Q F Y I M z U A E 3 o A r q A I N H 8 A x e w Z v x Z L w Y 7 8 b n t D R j p D 3 7 4 A + M 7 x 9 2 f 6 J 4 < / l a t e x i t >
Scoring Our alignment scoring process is shown in Figure 3 .
We first concatenate the question text with the document text into T and then encode them using the pre-trained BERT encoder .
We then compute a representation for each node in the question and context using a span extractor , which in our case is the self-attentive pooling layer of Lee et al . ( 2017 ) .
The node representation in the question can be computed in the same way .
Self-attentive Pooling
Then the score of a node pair is computed as a dot product S(q , c , T ) = q ? c. Answer Extraction
Our model so far produces an alignment between question nodes and context nodes .
We assume that one question node contains a wh-word and this node aligns to the context node containing the answer .
1 Ideally , we can use this aligned node to extract the actual answer .
However , in practice , the aligned context node may only contain part of the answer and in some cases answering the question only based the aligned context node can be ambiguous .
We therefore use the sentence containing the wh-aligned context node as the " new " context and use a standard BERT QA model to extract the actual answer post-hoc .
In the experiments , we also show the performance of our model by only use the aligned context node without the sentence , which is only slightly worse .
Training
We train our model as an instance of a structured support vector machine ( SSVM ) .
Ignoring the regularization term , this objective can be viewed as a sum over the training data of a structured hinge loss with the following formulation : N i=1 max ( 0 , max a?A [ f ( a , Q i , C i , T i ) + Ham(a , a * i ) ? f ( a * i , Q i , C i , T i ) ] ) 1254 where a denotes the predicted alignment , a * i is the oracle alignment for the ith training example , and Ham is the Hamming loss between these two .
To get the predicted alignment a during training , we need to run loss -augmented inference as we will discuss in the next section .
When computing the alignment for node j , if a j = a * j , we add 1 to the alignment score to account for the loss term in the above equation .
Intuitively , this objective requires the score of the gold prediction to be larger than any other hypothesis a by a margin of Ham( a , a * ) .
When training our system , we first do several iterations of local training where we treat each alignment decision as an independent prediction , imposing no constraints , and optimize log loss over this set of independent decisions .
The local training helps the global training converge more quickly and achieve better performance .
Inference
Since our alignment constraints do not strongly restrict the space of possible alignments ( e.g. , by enforcing a one- to- one alignment with a connected subgraph ) , searching over all valid alignments is intractable .
We therefore use beam search to find the approximate highest - scoring alignment : ( 1 ) Initialize the beam with top b highest aligned node pairs , where b is the beam size .
( 2 ) For each hypothesis ( partial alignment ) in the beam , compute a set of reachable nodes based on the currently aligned pairs under the locality constraint .
( 3 ) Extend the current hypothesis by adding each of these possible alignments in turn and accumulating its score .
Beam search continues until all the nodes in the question are aligned .
An example of one step of beam hypothesis expansion with locality constraint k = 2 is shown in Figure 4 .
In this state , the two played nodes are already aligned .
In any valid alignment , the neighbors of the played question node must be aligned within 2 nodes of the played context node to respect the locality constraint .
We therefore only consider aligning to the game , on Feb 7 , 2016 and Super Bowl 50 .
The alignment scores between these reachable nodes and the remaining nodes in the question are computed and used to extend the beam hypotheses .
Note that this inference procedure allows us to easily incorporate other constraints as well .
For instance , we could require a " hard " match on entity nodes , meaning that two nodes containing entities can only align if they share entities .
With this constraint , as shown in the figure , Super Bowl 50 can never be aligned to on February 7 , 2016 .
We discuss such constraints more in Section 5 .
Oracle Construction
Training assumes the existence of gold alignments a * , which must be constructed via an oracle given the ground truth answer .
This process involves running inference based on heuristically computed alignment scores S oracle , where S oracle ( q , c ) is computed by the Jaccard similarity between a question node q and a context node c. Instead of initializing the beam with the b best alignment pairs , we first align the wh-argument in the question with the node ( s ) containing the answer in the context and then initialize the beam with those alignment pairs .
If the Jaccard similarity between a question node and all other context nodes is zero , we set these as unaligned nodes .
During training , our approach can gracefully handle unaligned nodes by treating these as latent variables in structured SVM : the gold " target " is then highest scoring set of alignments consistent with the gold supervision .
This involves running a second decoding step on each example to impute the values of these latent variables for the gold alignment .
Experiments : Adversarial and Cross-domain Robustness
Our focus in this work is primarily robustness , interpretability , and controllability of our model .
We focus on adapting to challenging settings in order to " stress test " our approach .
denotes extracting the answer using only the wh-aligned node .
ans in wh denotes the percentage of answers found in the span aligned to the wh-span , and F1 denotes the standard QA performance measure .
Here for add Sent , we only consider the adversarial examples .
Note also that this evaluation is only on wh-questions .
Experimental Settings
For all experiments , we train our model only on the English SQuAD - 1.1 dataset ( Rajpurkar et al. , 2016 ) and examine how well it can generalize to adversarial and out - of- domain settings with minimal modification , using no fine- tuning on new data and no data augmentation that would capture useful transformations .
We evaluate on the addSent and addOneSent proposed by Jia and Liang ( 2017 ) , and the Universal Triggers on SQuAD .
We also test the performance of our SQuAD - trained models in zeroshot adaptation to new English domains , namely Natural Questions ( Kwiatkowski et al. , 2019 ) , NewsQA ( Trischler et al. , 2017 ) , BioASQ ( Tsatsaronis et al. , 2015 ) and TextbookQA ( Kembhavi et al. , 2017 ) , taken from the MRQA shared task ( Fisch et al. , 2019 ) .
Our motivation here was to focus on text from a variety of domains where transferred SQuAD models may at least behave credibly .
We excluded , for example , HotpotQA ( Yang et al. , 2018 ) and DROP ( Dua et al. , 2019 ) , since these are so far out - of- domain from the perspective of SQuAD that we do not see them as a realistic crossdomain target .
We compare primarily against a standard BERT QA system .
We also investigate a local version of our model , where we only try to align each node in the question to its oracle , without any global training ( ?
global train + inf ) , which can still perform reasonably because BERT embeds the whole question and context .
When comparing variants of our proposed model , we only consider the questions that have a valid SRL parse and have a wh word ( results in Table 1 , Table 2 , and Figure 5 ) .
When comparing with prior systems , for questions that do not have a valid SRL parse or wh word , we back off to the standard BERT QA system ( results in Table 3 ) .
We set the beam size b = 20 for the constrained alignment .
We use BERT - base-uncased for all of our experiments , and fine - tune the model using Adam ( Kingma and Ba , 2014 ) with learning rate set to 2e - 5 .
Our preprocessing uses a SpanBERT - based coreference system ( Joshi et al. , 2020 ) and a BERT - based SRL system ( Shi and Lin , 2019 ) .
We limit the length of the context to 512 tokens .
For our global model , we initialize the weights using a locally trained model and then fine - tune using the SSVM loss .
We find the initialization helps the model converge much faster and it achieves better performance than learning from scratch .
When doing inference , we set the locality constraint k = 3 .
Results on Challenging Settings
The results 2 on the normal SQuAD development set and other challenging sets are shown in Table 1 .
Our model is not as good as BERT QA on normal SQuAD but outperforms it in challenging settings .
Compared to the BERT QA model , our model is fitting a different data distribution ( learning a constrained structure ) which makes the task harder .
This kind of training scheme does cause some performance drop on normal SQuAD , but we can see that it consistently improves the F1 on the adversarial ( on SQuAD addSent , a 11.3 F1 improvement over BERT QA ) and cross-domain datasets except NewsQA ( where it is 0.4 F1 worse ) .
This demonstrates that learning the alignment helps improve the robustness of our model .
Global training and inference improve performance in adversarial settings , despite having no effect in- domain .
Normal SQuAD is a relatively easy dataset and the answer for most questions can be found by simple lexical matching between the question and context .
From the ablation of ?
global train+ inf , we can see that more than 80 % of answers can be located by matching the whargument .
We also observe a similar pattern on Natural Questions .
3
However , as there are very strong distractors in SQuAD addSent , the whargument matching is unreliable .
In such situations , the constraints imposed by other argument alignments in the question are useful to correct the wrong wh-alignment through global inference .
We see that the global training plus inference is consistently better than the local version on all other datasets .
Using the strict wh answer extraction still gives strong performance From the ablation of ? ans from full sent , we observe that our " strictest " system that extracts the answer only using the whaligned node is only worse by 3 - 4 points of F1 on most datasets .
Using the full sentence gives the system more context and maximal flexibility , and allows it to go beyond the argument spans introduced by SRL .
We believe that better semantic representations tailored for question answering ( Lamm et al. , 2020 ) will help further improvement in this regard .
Results on Universal Triggers
The results on subsets of the universal triggers dataset are shown in Table 2 .
We see that every trigger results in a bigger performance drop on BERT QA than our model .
Our model is much more stable , especially on who and where question types , in which case the performance only drops by around 2 % .
Several factors may contribute to the stability : ( 1 )
The triggers are ungrammatical and their arguments often contain seemingly random words , which are likely to get lower alignment scores .
( 2 ) Because our model is structured and trained to align all parts of the question , adversarial attacks on span-based question answering models may not fool our model as effectively as they do BERT .
Comparison to Existing Systems In Table 3 , we compare our best model ( not using constraints from Section 5 ) with existing adversarial QA models in the literature .
We note that the performance of our model on SQuAD - 1.1 data is relatively lower compared to those methods , yet we achieve the best overall performance ; we trade some in-distribution performance to improve the model 's robustness .
We also see that our model achieves the smallest normal vs.
adversarial gap on addSent and addOneSent , which demonstrates that our constrained alignment process can enhance the robustness of the model compared to prior methods like adversarial training or explicit knowledge integration ( Wang and Jiang , 2018 ) .
Generalizing by Alignment Constraints
One advantage of our explicit alignments is that we can understand and inspect the model 's behavior more deeply .
This structure also allows us to add constraints to our model to prohibit certain behaviors , which can be used to adapt our model to adversarial settings .
In this section , we explore how two types of constraints enable us to reject examples the model is less confident about .
Hard constraints can enable us to reject questions where the model finds no admissible answers .
Soft constraints allow us to set a calibration threshold for when to return our answer .
We focus on evaluating our model 's accuracy at various coverage points , the so-called selective question answering setting ( Kamath et al. , 2020 ) . Constraints on Entity Matches
By examining add Sent and addOneSent , we find the model is typically fooled when the nodes containing entities in the question align to " adversarial " entity nodes .
An intuitive constraint we can place on the alignment is that we require a hard entity matchfor each argument in the question , if it contains Table 3 : Performance of our systems compared to the literature on both addSent and addOneSent .
Here , overall denotes the performance on the full adversarial set , adv denotes the performance on the adversarial samples alone .
? represents the gap between the normal SQuAD and the overall performance on adversarial set .
entities , it can only align to nodes in the context sharing exact the same entities .
Constraints on Alignment Scores
The hard entity constraint is quite inflexible and does not generalize well , for example to questions that do not contain a entity .
However , the alignment scores we get during inference time are good indicators of how well a specific node pair is aligned .
For a correct alignment , every pair should get a reasonable alignment score .
However , if an alignment is incorrect , there should exist some bad alignment pairs which have lower scores than the others .
We can reject those samples by finding bad alignment pairs , which both improves the precision of our model and also serves as a kind of explanation as to why our model makes its predictions .
We propose to use a simple heuristic to identify the bad alignment pairs .
We first find the max score S max over all possible alignment pairs for a sample , then for each alignment pair ( q i , c j ) of the prediction , we calculate the worst alignment gap ( WAG ) g = min ( q , c ) ? a
( S max ? S ( q , c ) ) .
If g is beyond some threshold , it indicates that alignment pair is not reliable .
4 Comparison to BERT Desai and Durrett ( 2020 ) show that pre-trained transformers like BERT are well - calibrated on a range of tasks .
Since we are rejecting the unreliable predictions to improve the precision of our model , we reject the same number of examples for the baseline using the posterior probability of the BERT QA predictions .
To be specific , we rank the predictions of all examples by the sum of start and end posterior probabilities and compute the F1 score on the top k predictions . , what F1 does it achieve ?
For our model , the confidence is represented by our " worst alignment gap " ( WAG ) metric .
Smaller WAG indicates higher confidence .
For BERT , the confidence is represented by the posterior probability .
Results on Constrained Alignment On Adversarial SQuAD , the confidence scores of a normal BERT QA model do not align with its performance .
From Figure 5 , we find that the highest- confidence answers from BERT ( i.e. , in low coverage settings ) are very inaccurate .
One possible explanation of this phenomenon is that BERT overfits to the pattern of lexical overlap , and is actually most confident on adversarial examples highly similar to the input .
In general , BERT 's confidence is not an effective heuristic for increasing accuracy .
Hard entity constraints improve the precision but are not flexible .
Question :
Who created an engine using high pressure steam in 1801 ?
Oracle alignment : Around 1800 Richard Trevithick and , separately , Oliver Evans in 1801 introduced engines using high - pressure steam ; Adversarial alignment : Jeff Dean created an engine using low pressure steam in 1790 .
lower than what it achieves on normal SQuAD .
We examine some of the error cases and find that for a certain number of samples , there is no path from the node satisfying the constraint to the node containing the answer ( e.g. they hold a more complex discourse relation while we only consider coreference as cross-sentence relation ) .
In such cases , our method cannot find the answer .
A smaller worst alignment gap indicates better performance .
As opposed to BERT , our alignment score is well calibrated on those adversarial examples .
This substantiates our claim that those learned alignment scores are good indicators of how trustful alignment pairs are .
Also , we see that when the coverage is the same as the entity constraint , the performance under the alignment score constraint is even better .
The alignment constraints are simultaneously more flexible than the hard constraint and also more effective .
Case Study on Alignment Scores
In this section , we give several examples of the alignment and demonstrate how those scores can act as an explanation to the model 's behavior .
Those examples are shown in Figure 6 .
As shown by the dashed arrows , all adversarial alignments contain at least one alignment with significantly lower alignment score .
The model is overconfident towards the other alignments with a high lexical overlap as shown by the bold arrows .
These overconfident alignments also show that the predicate alignment learned on SQuAD - 1.1 is not reliable .
To further improve the quality of predicate alignment , either a more powerful training set or a new predicate alignment module is needed .
Crucially , with these scores , it is easy for us to interpret our model 's behavior .
For instance , in example ( a ) , the very confident predicate alignment forces Luther 's 95
Theses to have no choice but align to Jeff Dean , which is unrelated .
Because we have alignments over the sub-parts of a question , we can inspect our model 's behavior in a way that the normal BERT QA model does not allow .
We believe that this type of debuggability provides a path forward for building stronger QA systems in high-stakes settings .
Related Work Adversarial Attacks in NLP .
Adversarial attacks in NLP may take the form of adding sentences like adversarial SQuAD ( Jia and Liang , 2017 ) , universal adversarial triggers ( 2018 ) use controlled paraphrase generation .
The highly structured nature of our approach makes it more robust to such attacks and provides hooks to constrain the system to improve performance further .
Neural module networks .
Neural module networks are a class of models that decompose a task into several sub-tasks , addressed by independent neural modules , which make the model more robust and interpretable ( Andreas et al. , 2016 ; Hu et al. , 2017 ; Cirik et al. , 2018 ; Hudson and Manning , 2018 ; Jiang and Bansal , 2019 ) .
Like these , our model is trained end-to-end , but our approach uses structured prediction and a static network structure rather than dynamically assembling a network on the fly .
Our approach could be further improved by devising additional modules with distinct parameters , particularly if these are trained on other datasets to integrate additional semantic constraints .
Unanswerable questions
Our approach rejects some questions as unanswerable .
This is similar to the idea of unanswerable questions in SQuAD 2.0 ( Rajpurkar et al. , 2018 ) , which have been studied in other systems .
However , techniques to reject these questions differ substantially from ours - many SQuAD 2.0 questions require not only a correct alignment between the question and context but also need to model the relationship between arguments , which is beyond the scope of this work and could be a promising future work .
Also , the setting we consider here is more challenging , as we do not assume access to such questions at training time .
Graph - based QA Khashabi et al. ( 2018 ) propose to answer questions through a similar graph alignment using a wide range of semantic abstractions of the text .
Our model differs in two ways : ( 1 ) Our alignment model is trained end-to - end while their system mainly uses off - the-shelf natural language modules .
( 2 ) Our alignment is formed as node pair alignment rather than finding an optimal sub-graph , which is a much more constrained and less flexible formalism .
Sachan et al. ( 2015 ) ; Sachan and Xing ( 2016 ) propose to use a latent alignment structure most similar to ours .
However , our model supports a more flexible alignment procedure than theirs does , and can generalize to handle a wider range of questions and datasets .
Past work has also decomposed complex questions to answer them more effectively ( Talmor and Berant , 2018 ; Min et al. , 2019 ; Perez et al. , 2020 ) . Wolfson et al. ( 2020 ) further introduce a Question Decomposition Meaning Representation ( QDMR ) to explicitly model this process .
However , the questions they answer , such as those from HotpotQA ( Yang et al. , 2018 ) , are fundamentally designed to be multi-part and so are easily decomposed , whereas the questions we consider are not .
Our model theoretically could be extended to leverage these question decomposition forms as well .
Discussion and Conclusion
We note a few limitations and some possible future directions of our approach .
First , errors from SRL and coreference resolution systems can propagate through our system .
However , because our graph alignment is looser than those in past work , we did not observe this to be a major performance bottleneck .
The main issue here is the inflexibility of the SRL spans .
For example , not every SRL span in the question can be appropriately aligned to a single SRL span in the context .
Future works focusing on the automatic span identification and alignment like recent work on end-to - end coreference systems ( Lee et al. , 2017 ) , would be promising .
Second , from the error analysis we see that our proposed model is good at performing noun phrase alignment but not predicate alignment , which calls attention to the better modeling of the predicate alignment process .
For example , we can decompose the whole alignment procedure into separate noun phrase and predicate alignment modules , in which predicate alignment could be learned using different models or datasets .
Finally , because our BERT layer looks at the entire question and answer , our model can still leverage uninterpretable interactions in the text .
We believe that modifying the training objective to more strictly enforce piecewise comparisons could improve interpretability further while maintaining strong performance .
In this work , we presented a model for question answering through sub-part alignment .
By structuring our model around explicit alignment scoring , we show that our approach can generalize better to other domains .
Having alignments also makes it possible to filter out bad model predictions ( through score constraints ) and interpret the model 's behavior ( by inspecting the scores ) .
A Adversarial Datasets Added sentences Jia and Liang ( 2017 ) propose to append an adversarial distracting sentence to the normal SQuAD development set to test the robustness of a QA model .
In this paper , we use the two main test sets they introduced : addSent and addOneSent .
Both of the two sets augment the normal test set with adversarial samples annotated by Turkers that are designed to look similar to question sentences .
In this work , we mainly focus on the adversarial examples .
Universal
Triggers use a gradient based method to find a short trigger sequence .
When they insert the short sequence to the original text , it will trigger the target prediction in the sequence independent of the rest of the passage content or the exact nature of the question .
For QA , they generate different triggers for different types of questions including " who " , " when " , " where " and " why " .
Datasets from MRQA For Natural Questions ( Kwiatkowski et al. , 2019 ) , NewsQA ( Trischler et al. , 2017 ) , BioASQ ( Tsatsaronis et al. , 2015 ) and TextbookQA ( Kembhavi et al. , 2017 ) , we use the pre-processed datasets from MRQA ( Fisch et al. , 2019 ) .
They differ from the original datasets in that only the paragraph containing the answer is picked as the context and the maximum length of the context is cut to 800 tokens .
B Results on SQuAD addOneSent
The results of our model compared to BERT QA on SQuAD addOneSent is shown in Table 4 .
Here we see the results on addOneSent and addSent generally have the same trend .
The global train + inf helps more on the more difficult add Sent .
Figure 1 : 1 Figure 1 : A typical example on adversarial SQuAD .
By breaking the question and context down into smaller units , we can expose the incorrect entity match and use explicit constraints to fix it .
The solid lines denote edges from SRL and coreference , and the dotted lines denote the possible alignments between the arguments ( desired in red , actual in black ) .
Figure 2 : 2 Figure 2 : Example of our question - passage graph .
Edges come from SRL , coreference ( Super Bowl 50the game ) , and postprocessing of predicates nested inside arguments ( was - determine ) .
The oracle alignment ( Section 3.4 ) is shown with dotted lines .
Blue nodes are predicates and orange ones are arguments .
Figure 3 : 3 Figure 3 : Alignment scoring .
Here the alignment score is computed by the dot product between span representations of question and context nodes .
The final alignment score ( not shown ) is the sum of these edge scores .
Figure 4 : 4 Figure 4 : An example of constraints during beam search .
The blue node played is already aligned .
The orange nodes denote all the valid context nodes that can be aligned to for both Super Bowl 50 and what day in the next step of inference given the locality constraint with k = 2 .
Figure 5 : 5 Figure 5 : The F1 - coverage curve of our model compared with BERT QA .
If our model can choose to answer only the k percentage of examples it 's most confident about ( the coverage ) , what F1 does it achieve ?
For our model , the confidence is represented by our " worst alignment gap " ( WAG ) metric .
Smaller WAG indicates higher confidence .
For BERT , the confidence is represented by the posterior probability .
Figure 6 : 6 Figure 6 : Examples of alignment of our model on addOneSent : both the correct alignment and also adversarial alignment are shown .
The numbers are the actual alignment scores of the model 's output .
Dashed arrows denote the least reliable alignments and bolder arrows denote the alignment that contribute more to the model 's prediction .
, or sentence perturbations : Ribeiro et al. ( 2018 ) propose deriving transformation rules , Ebrahimi et al . ( 2018 ) use character - level flips , and Iyyer et al .
Table 1 : 1 The performance and ablations of our proposed model on the development sets of SQuAD , adversarial SQuAD , and four out - of- domain datasets .
Our Sub-part Alignment model uses both global training and inference as discussed in Section 3.2- 3.3 .
? global train + inf denotes the locally trained and evaluated model .
? ans from full sent SQuAD normal SQuAD addSent Natural Questions NewsQA BioASQ TBQA ans in wh F1 ans in wh F1 ans in wh F1 ans in wh F1 ans in wh F1 ans in wh F1 Sub-part Alignment 84.7 84.5 49.5 50.5 65.8 61.5 49.3 48.1 63.5 53.4 35.1 38.4 ? global train+ inf 85.8 85.2 45.0 46.8 65.9 62.3 48.9 47.1 62.5 52.1 31.9 34.6 ? ans from full sent 84.7 81.8 49.5 46.7 65.8 57.8 49.3 45.0 63.5 51.1 35.1 37.5 BERT QA ? 87.8 ? 39.2 ? 59.4 ? 48.5 ? 52.4 ? 25.3
Table 2 : 2 The performance of our model on the Universal Triggers on SQuAD dataset .
Sub-part Alignment BERT Type Normal Trigger ? Normal Trigger ? who 84.7 82.7 2.0 87.1 78.5 8.6 why 75.1 71.3 3.8 76.5 59.7 16.8 when 88.4 82.8 5.6 90.3 80.9 9.4 where 83.6 81.4 2.2 84.1 75.8 8.3 Compared with BERT , our model sees smaller perfor - mance drops on all triggers .
Oracle alignment : ? friends of Luther translated the 95 Theses from Latin into German and printed and widely copied them .
24.2 20.9 23.1 26.5 Question :
Who translated and printed Luther 's 95 These ?
24.9 26.8 25.5 20.4 Adversarial alignment : Jeff Dean translated and printed Vandross 's 98
These .
Oracle alignment : Barred by the government from settling in New Oracle alignment : Super Bowl 50 was ? ,
The game was played on France , Huguenots led by Jess ?
de Forest , sailed to North America in 1624 February 7 , 2016 , at Levi's Stadium in the San Francisco Bay Area 22.6 29.3 16.9 14.0 28.5 25.6 16.4 Question : Who led the North American Huguenot colonial expedition ?
Question :
Where did Super Bowl 50 take place ?
32.3 35.5 13.0 10.4 19.1 19.1 36.6 Adversarial alignment : Jeff Dean led the South British Acadian colonial Adversarial alignment : Champ Bowl 40 took place in Chicago .
expedition .
( c ) ( d ) Figure 5 also shows that by adding a hard entity constraint , we achieve a 71.4 F1 score which is an 8.6 improvement over the un- constrained model at a cost of only 60 % of samples being covered .
Under the hard entity constraint , the model is not able to align to the nodes in the adversarial sentence , but the performance is still 1258
We discuss what to do with other questions in Section 4.1 .
Here we omit SQuAD addOneSent for simplicity , since the performance on it has the same trend as SQuAD addSent .
Refer to the Appendix for the results on SQuAD addOneSent .
For the MRQA task , only the paragraph containing the short answer of NQ is provided as context , which eliminates many distractors .
In such cases , those NQ questions have a similar distribution as those in SQuAD - 1.1 , and similarly make no use of the global alignment .
The reason we look at differences from the max alignment is to calibrate the scores based on what " typical " scores look like for that instance .
We find that these are on different scales across different instances , so the gap is more useful than an absolute threshold .
