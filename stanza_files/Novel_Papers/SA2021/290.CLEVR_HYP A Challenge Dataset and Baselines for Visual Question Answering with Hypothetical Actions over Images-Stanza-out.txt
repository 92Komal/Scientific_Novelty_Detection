title
CLEVR_HYP : A Challenge Dataset and Baselines for Visual Question Answering with Hypothetical Actions over Images
abstract
Most existing research on visual question answering ( VQA ) is limited to information explicitly present in an image or a video .
In this paper , we take visual understanding to a higher level where systems are challenged to answer questions that involve mentally simulating the hypothetical consequences of performing specific actions in a given scenario .
Towards that end , we formulate a vision - language question answering task based on the CLEVR ( Johnson et al. , 2017a ) dataset .
Wethen modify the best existing VQA methods and propose baseline solvers for this task .
Finally , we motivate the development of better vision - language models by providing insights about the capability of diverse architectures to perform joint reasoning over image - text modality 1 . * corresponding author 1 Dataset setup scripts and code for baselines are made available at https://github.com/shailaja183/clevr_hyp.
For additional details about the dataset creation process , refer supplementary material .
Introduction
In 2014 , Michael Jordan , in an interview ( Gomes , 2014 ) said that " Deep learning is good at certain problems like image classification and identifying objects in the scene , but it struggles to talk about how those objects relate to each other , or how a person / robot would interact with those objects .
For example , humans can deal with inferences about the scene : what if I sit down on that ? , what if I put something on top of something ?
etc .
There exists a range of problems that are far beyond the capability of today 's machines . "
While this interview was six years ago , and since then there has been a lot of progress in deep learning and its applications to visual understanding .
Additionally , a large body of visual question answering ( VQA ) datasets ( Antol et al. , 2015 ; Ren et al. , 2015 ; Hudson and Manning , 2019 ) have been compiled and many models have been developed over them , but the above mentioned " inferences about the scene " issue stated by Jordan remains largely unaddressed .
In most existing VQA datasets , scene understanding is holistic and questions are centered around information explicitly present in the image ( i.e. objects , attributes and actions ) .
As a result , advanced object detection and scene graph techniques have been quite successful in achieving good performance over these datasets .
However , provided an image , humans can speculate a wide range of implicit information .
For example , the purpose of various objects in a scene , speculation about events that might have happened before , consider numerous imaginary situations and predicting possible future outcomes , intentions of a subject to perform particular actions , and many more .
Among the above , an ability to imagine taking specific actions and simulating probable results without actually acting or experiencing is an important aspect of human cognition ( Figure 1 gives an example of this ) .
Thus , we believe that having autonomous systems equipped with a similar capability will further advance AI research .
This is particu-larly useful for robots performing on -demand tasks in safety -critical situations or navigating through dynamic environments , where they imagine possible outcomes for various situations without executing instructions directly .
Motivated by the above , we propose a challenge that attempts to bridge the gap between state - ofthe - art AI and human-level cognition .
The main contributions of this paper 2 are as follows ; ?
We formalize a novel question answering task with respect to a hypothetical state of the world ( in a visual form ) when some action ( described in a textual form ) is performed .
?
We create a large-scale dataset for this task , and refer it as CLEVR_HYP i.e.
VQA with hypothetical actions performed over images in CLEVR ( Johnson et al. , 2017a ) style . ?
We first evaluate the direct extensions of top VQA and NLQA ( Natural language QA ) solvers on this dataset .
Then , we propose new baselines to solve CLEVR_HYP and report their results .
?
Through analysis and ablations , we provide insights about the capability of diverse architectures to perform joint reasoning over imagetext modality .
Related Work
In this section we situate and compare our work with related areas such as implicit text generation / retrieval for a visual , visual question answering ( VQA ) over synthetic images , question answering ( QA ) involving hypothetical reasoning , and language - based manipulation in visual domains closest to CLEVR_HYP .
Implicit Text Generation for a Visual : VisualComet ( Park et al. , 2020 ) and Video2 Commonsense ( Fang et al. , 2020 ) have made initial attempts to derive implicit information about images / videos contrary to traditional factual descriptions which leverage only visual attributes .
VisualComet aims to generate commonsense inferences about events that could have happened before , events that can happen after and people 's intents at present for each subject in a given image .
They use a vision- language transformer that takes a sequence of inputs ( image , event , place , inference ) and train a model to predict inference in a language -model style .
Video2 Commonsense focuses on generating video descriptions that can incorporate commonsense facts related to intentions , effects , and implicit attributes about actions being performed by a subject .
They extract top-ranked commonsense texts from the Atomic dataset and modify training objective to incorporate this information .
While both involve a visual-textual component and actions , their key focus is about generating plausible events and commonsense respectively .
Whereas , our work is related to performing certain actions and reasoning about its effect on the overall visual scene .
Language - based Manipulation in Visual Domain : Learning a mapping from natural language instructions to a sequences of actions to be performed in a visual environment is a common task in robotics ( Kanu et al. , 2020 ; Gaddy and Klein , 2019 ; Shridhar et al. , 2020 ) .
Another relevant task is vision- and - language navigation ( Anderson et al. , 2018 ; Nguyen et al. , 2019 ) , where an agent navigates in a visual environment to find goal location by following natural language instructions .
Both above works include visuals , natural language instructions and a set of actions that can be performed to achieve desired goals .
In this way , it is similar to our CLEVR_HYP , but in our case , models require reasoning about the effect of actions performed rather than determining which action to perform .
Also , we frame this in a QA style evaluation rather than producing instructions for low-level controls .
Manipulation of natural images with language is an emerging research direction in computer vision .
( Teney et al. , 2020 ) proposed a method for generating counterfactual of VQA samples using image in - painting and masking .
Also , there are works ( Dong et al. , 2017 ; Nam et al. , 2018 ; Reed et al. , 2016 ) which use Generative Adversarial Networks ( GANs ) ( Goodfellow et al. , 2014 ) for language conditioned image generation and manipulation .
However , both the above tasks are more focused at object and attribute level manipulation rather than at action level .
VQA over Synthetic Images :
While natural images - based VQA datasets reflect challenges one can encounter in real- life situations , the require - 1 .
TA : Paint the small green ball with cyan color .
QH : Are there equal yellow cubes on left of purple object and cyan spheres ?
( A : yes ) 2 . TA : Add a brown rubber cube behind the blue sphere that inherits its size from the green object .
QH : How many things are either brown or small ?
( A : 6 ) 3 . TA : John moves the small red cylinder on the large cube that is to the right of purple cylinder .
QH : What color is the object that is at the bottom of the small red cylinder ?
( A : yellow ) ment of costlier human annotations and vulnerability to biases are two major drawbacks .
Contrary to them , synthetic datasets allow controlled data generation at scale while being flexible to test specific reasoning skills .
For the above reasons , following benchmark VQA datasets have incorporated synthetic images ; COG ( Yang et al. , 2018 ) and Shapes ( Andreas et al. , 2016 ) contain images with rendered 2D shapes ; SHRDLU ( Winograd , 1971 ) , CLEVR ( Johnson et al. , 2017a ) , and CLEVR - dialog ( Kottur et al. , 2019 ) have rendered scenes with 3D objects ; DVQA ( Kafle et al. , 2018 ) and FigureQA ( Kahou et al. , 2017 ) have synthetically generated charts ( bar chart , pie chart , dot-line etc. ) ; VQAabstract ( Antol et al. , 2015 ) and IQA ( Gordon et al. , 2018 ) involves question - answering over synthetically rendered clipart-style scenes and interactive environments respectively .
Our proposed dataset CLEVR_HYP uses CLEVR ( Johnson et al. , 2017a ) style rendered scenes with 3D objects as a visual component .
It is distinct from all other synthetic VQA datasets for two key reasons ; first , integration of action domain in synthetic VQA and second , the requirement of mental simulation in order to answer the question .
QA involving Hypothetical Reasoning :
In the language domain , WIQA ( Tandon et al. , 2019 ) dataset tests the model 's ability to do what - if reasoning over procedural text as a 3 - way classification ( the influence between pair of events as positive , negative or no-effect ) .
In vision- language domains , a portion of TQA ( Kembhavi et al. , 2017 ) and VCR ( Zellers et al. , 2019 ) are relevant .
Questions in TQA and VCR involve hypothetical scenarios about multi-modal science contexts and movie scenes respectively .
However , none of the above two datasets ' key focus is on the model 's capability to imagine changes performed over the image .
As shown in Figure 3 , the setting of TIWIQ ( a benchmark dataset for " physical intelligence " ) ( Wagner et al. , 2018 ) has some similarity with ours .
It has synthetically rendered table - top scenes , four types of actions ( push , rotate , remove and drop ) being performed on an object and what - if questions .
To our best knowledge , TIWIQ dataset is not publicly available .
Based on our understanding from their manuscript , we observe following important distinction with this work .
Our questions focus on the impact of actions on the whole image , while in TIWIQ questions are about impact of actions on a specific object in the image .
Moreover , we frame CLEVR_HYP as a classification task , contrary to TIWIQ which is a generative task .
Our CLEVR_HYP dataset has 175 k automatically generated image-action text-question samples which is much larger compared to TIWIQ which has only 1020 samples and manually crafted ground -truths .
CLEVR_HYP
Task and Dataset Figure 2 gives a glimpse of CLEVR_HYP task .
We opt for synthetic dataset creation as it allows automated and controlled data generation at scale with minimal biases .
More details are described below .
3 Inputs : Image ( I ) , Action Text ( T A ) and Hypothetical Question ( Q H ) 1 . Image ( I ) :
It is a given visual for our task .
Each image in the dataset contains 4 - 10 randomly selected 3D objects rendered using Blender ( Blender Online Community , 2019 ) in CLEVR ( Johnson et al. , 2017a ) style .
Objects have 4 attributes listed in the Table 1 . Additionally , these objects can be referred using 5 relative spatial relations ( left , right , in front , behind and on ) .
We provide scene graphs 3 containing all ground -truth information about a scene , that can be considered as a visual oracle for a given image .
Attr. Possible
Action Text ( T A ) :
It is a natural language text describing various actions performed over the current scene .
The action can be one of four : ( i ) Add new object ( s ) to the scene ( ii ) Remove object ( s ) from the scene ( iii ) Change attributes of the object ( s ) ( iv ) Move object ( s ) within scene ( might be in plane i.e. left / right / front / back or out of plane i.e. move one object on top of another object 4 ) To generate action text , we start with manually written templates involving the aforementioned actions .
For example , action involving change in the attribute of object ( s ) to a given value , we have a template of the following kind ; ' Change the < A> of < Z>< C>< M>< S> to < V >'.
Where < A > , < Z > , < C > , <M > , <S > , <V> are placeholders for the attribute , size , color , material , shape and a value of attribute respectively .
Each action text in the CLEVR_HYP is associated with a functional program which if executed on an image 's scene graph , yields the new scene graph that simulates the effects of actions .
Functional programs for action texts 3 are built from the basic functions that correspond to elementary action operations ( right part of Figure 4a ) .
For the above mentioned ' change ' attribute action template , the equivalent functional program can be written as ; _color ( < C > , filter_material ( < M> filter_ shape ( < S > , scene ( ) ) ) ) ) , < V > ) ' .
It essentially means , first filter out the objects with desired attributes and then update the value of their current attribute A to value V .
Question about Hypothetical Situation ( Q H ) :
It is a natural language query that tests various visual reasoning abilities after simulating the effects of actions described in T A .
There are 5 possible reasoning types similar to CLEVR ; ( i ) Counting objects fulfilling the condition ( ii ) Verify existence of certain objects ( iii ) Query attribute of a particular object ( iv ) Compare attributes of two objects ( v ) Integer comparison of two object sets ( same , larger or smaller ) Similar to action texts , we have templates and corresponding programs for questions .
Functional programs for questions 3 are executed on the image 's updated scene graph ( after incorporating effects of the action text ) and yields the groundtruth answer to the question .
Functional programs for questions are made of primitive functions shown in left part of the Figure 4a ) .
Paraphrasing :
In order to create a challenging dataset from linguistic point of view and to prevent models from overfitting on templated representations , we leverage noun synonyms , object name paraphrasing and sentence - level paraphrasing .
For noun synonyms , we use a pre-defined dictionary ( such as cube block , sphere ball and so on ) .
We programmatically generate all possibilities to refer to an object in the image ( i.e. object name paraphrasing ) and randomly sample one among them .
For sentence level paraphrasing , we use Text-To - Text Transfer Transformer ( T5 ) ( Raffel et al. , 2020 ) fine -tuned over positive samples from Quora Question Pairs ( QQP ) dataset ( Iyer et al. , 2017 ) for question paraphrasing .
We use Fairseq for action text paraphrasing which uses round-trip translation and mixture of experts ( Shen et al. , 2019 ) .
Note that we keep the action text and question as separate inputs for the purpose of simplicity and keeping our focus on building solvers that can do mental simulation .
One can create a simple template like " < Q H > if < proper-noun / pronoun > < T A >? " or " If < proper-noun / pronoun >
< T A > , Output : Answer ( A ) to the Question ( Q H ) , which can be considered as a 27 - way classification over attributes ( 8 colors + 3 shapes + 2 sizes + 2 material ) , numeric ( 0 - 9 ) and boolean ( yes / no ) .
Dataset Partitions and Statistics :
We create CLEVR_HYP dataset containing 175 k imageaction text-question samples using the process men-tioned in Figure 4 b .
For each image , we generate 5 kinds of action texts ( one for each add , remove , move in - plane and move out - of - plane and change attribute ) .
For each action text type , we generate 5 questions ( one for each count , exist , compare integer , query attribute and compare attribute ) .
Hence , we get 5 * 5 unique action text-question pairs for each image , covering all actions and reasoning types in a balanced manner as shown in Figure 5a ( referred as Original partition ) .
However , it leads to a skewed distribution of answers as observed from 5 b .
Therefore , we curate a version of the dataset ( referred as Balanced partition ) consisting of 67.5 k samples where all answer choices are equally - likely as well .
Additionally , we create two small challenge test sets ( 1500 image-action text-question samples each ) - 2HopActionText ( 2HopT A ) and 2HopQuestion ( 2HopQ H ) to test generalization capability of the trained models .
In 2HopT
A , we create action text which requires model to understand two different actions being taken on the scene .
For example , ' Add a small blue metal cylinder to the right of large yellow cube and remove the large cylinder from the scene . ' and ' Move the purple object on top of small red cube then change its color to cyan . '.
In 2HopQ H , we create questions which require model to understand logical combinations of questions using ' and ' , ' or ' and ' not ' .
For example , ' How many objects are either red or cylinder ? ' and ' Are there any rubber cubes that are not green ? '.
In Table 2 , we provide size of the various partitions and measure the diversity of the dataset in various aspects .
For images , we calculate average number of objects present in the scene from the length of scene graph .
For balanced partition , the number of images are much less compared to original , but more average number of objects per image .
This is most likely due to the need to accommodate integers 4 - 9 more frequently as ground -truth answers .
For textual components , we show average lengths ( number of tokens separated by whitespaces ) and count unique utterances as a measure of diversity .
The original partition of the resulting dataset has 80 % and 83 % unique action text and questions respectively .
For balanced partition , length and unique utterances for action text are nearly same as the original partition but for questions , it decreases .
Questions in the original partition have been observed to enforce more strict and specific object references ( such as small red metal cubes ) compared to balanced partition ( small cubes , red metal objects etc. ) , reducing the average length and uniqueness .
It is intuitive for 2Hop partitions to have higher average length and uniqueness for T A and Q H respectively .
This shows that despite having created this dataset from templates and rendered images with a limited set of attributes , it is still fairly challenging .
4 Models that we experiment with Models trying to tackle CLEVR_HYP dataset have to address four key challenges ; ( i ) understand hypothetical actions and questions in complex natural language , ( ii ) correctly disambiguate the objects of interest and obtain the structured representation ( i.e. scene graphs or functional programs ) of various modalities if required by the solver , ( iii ) understand the dynamics of the world based on the various actions performed over it , ( iv ) perform various kind of reasoning to answer the question .
Random
The QA task in CLEVR_HYP dataset can be considered as a 27 - class classification problem .
Each answer choice is likely to be picked with a probability of 1/27 .
Therefore , the performance of the random baseline is 3.7 % .
Split # I Avg .
# Obj # T A Unique # T A Avg. T A Len . # Q H Unique # Q H Avg. Q H Len .
Human Performance
We performed human evaluation with respect to 500 samples from the CLEVR_HYP dataset .
Accuracy of human evaluations on original test , 2HopA T and 2HopQ H are 98.4 % , 96.2 % and 96.6 % respectively .
Transformer Architectures
Pre-trained transformer - based architectures have been observed to capture a rich hierarchy of language -structures ( text-only models ) and effectively map entities / words with corresponding image regions ( vision - language models ) .
We experiment with various transformer - based models to understand their capability to understand the effects of actions on a visual domain .
Baseline 1 - Machine Comprehension using RoBERTa : To evaluate the hypothetical VQA task through the text-only model , we convert images into the templated text using scene graphs .
The templated text contains two kind of sentences ; one describing properties of the objects i.e .
" There is a < Z> < C> < M> < S > " , the other one describing the relative spatial location i.e .
" The < Z> < C> < M> < S> is < R> the < Z1 > < C1 > < M1 > < S1 > " .
For example , " There is a small green metal cube . " and " The large yellow rubber sphere is to the left of the small green metal cube " .
Then we concatenate templated text with the action text to create a reading comprehension passage .
We use state - of- the - art machine comprehension baseline RoBERTa ( Liu et al. , 2019 ) finetuned on the RACE dataset ( Lai et al. , 2017 ) 5 . Finally , we pre-5 architecture =roberta large , epochs=5 , learning rate =1e ?05 , batch size=2 , update frequency =2 , dropout =0.1 , dict an answer to the question using this reading comprehension passage .
Baseline 2 - Visual Question Answering using LXMERT Proposed by ( Tan and Bansal , 2019 ) , LXMERT is one of the best transformer based pretrainable visual -linguistic representations which supports VQA as a downstream task .
Typical VQA systems take an image and a language input .
Therefore , to evaluate CLEVR_HYP in VQA style , we concatenate action text and question to form a single text input .
Since LXMERT is pre-trained on the natural images , we finetune it over CLEVR_HYP dataset 6 and then use it to predict answer .
Systematically incorporating effects of actions into neural models Baseline 3 - Text-editing Image Baseline :
In this method , we break - down the QA task with mental simulation in two parts ; first , learn to generate an updated image ( such that it has incorporated the effects of actions ) and then perform visual question answering with respect to the updated image .
We use the idea from Text Image Residual Gating proposed in ( Vo et al. , 2019 ) to implement the first part .
However there are two important distinctions ;
Their focus is on the retrieval from the given database .
We modify their objective and develop text - adaptive encoder-decoder with residual connections to generate new image .
Also , editing instructions in their CSS dataset ( Vo et al. , 2019 ) were quite simple .
For example , ' add red cube ' and ' remove yellow sphere ' .
In this case , one can add the red cube anywhere in the scene .
We modify their architecture to precisely place objects to their optimizer=adam with eps=1e ?06 .
3699 Nomenclature I : Image , SG : Scene Graph , TT : Templated Text , T A : Action Text , Q H : Hypothetical Question , A : Answer , FP : Functional Program , ' : Updated Modality Baseline 1 : relative spatial references ( on left / right / front / behind ) .
Once we get the updated image , we feed it to the LXMERT ( Tan and Bansal , 2019 ) finetuned over the CLEVR ( Johnson et al. , 2017a ) dataset along with the question and predict the answer .
I SG T T + T A RoBERT a RACE A Q H Baseline 3 : I I LXM ERT CLEV R A T A F P Q H Baseline 2 : I LXM ERT CLEV R_HY P A T A + Q H Baseline 4 : I SG SG ? Symbolic A T A F P Q H F P Baseline 4 - Scene Graph Update Model : Instead of directly manipulating images , in this method , we leverage image scene graphs to convert image-editing problem into graph-editing problem , conditioned on the action text .
This is an emerging research direction to deal with changes in the visual modality over time or with new sources of information , as observed from recent parallel works ( chang Chen et al. , 2020 ; He et al. , 2020 ) .
We first use Mask R-CNN
( He et al. , 2017 ) to get the segmentation mask of the objects and predict attributes ( color , material , size , and shape ) with an acceptance threshold of 0.9 .
Segmentation mask of each object along with original image is then passed through ResNet - 34 ( He et al. , 2016 ) to extract precise 3D coordinates of the object .
We get the structured scene graph for the image .
Then we use seq2seq with attention model originally proposed in ( Johnson et al. , 2017 b ) to generate functional programs ( FP ) for action text and question .
The execution engine executes programs on scene graph , implemented as a neural module network ( Andreas et al. , 2016 ) to update the scene representation and answer questions .
We learn to update scene graphs according to functional program for the action text using reinforcement learning 7 .
The reward function is as - 7 finetuning learning rate =1e ?05 , 1 M iterations with early sociated with our ground -truth program executor and generates reward if prediction exactly matches with ground - truth execution .
Once we get the updated scene representation , we use neural -symbolic model 8 proposed by ( Yi et al. , 2018 ) to obtain the final answer .
It is notable that ( Yi et al. , 2018 ) achieved near-perfect performance on the CLEVR QA task in addition to being fully explainable .
Baseline Results
In this section , we benchmark models described above on the CLEVR_HYP .
The dataset is formulated as a classification task with exactly one correct answer , so we use standard accuracy as evaluation metric .
We then analyze their performance according to question and action types .
Quantitative results from above experiments can be visualized in top part of the Table 3 .
Among the methods described above , the scene graph update model has the best overall performance 70.5 % on original test data .
Text -editing model is best over balanced set , but observed to have the poor generalization capability when two actions or reasoning capabilities have to be performed .
CLEVR_HYP requires models to reason about effect of hypothetical actions taken over images .
LXMERT is not directly trained for this objective therefore , it struggles to do well on this task .
The reason behind the poor performance of text -only baseline is due to its limitation to incorporate detailed spatial locations into the templates that we use to convert image into a machine comprehension passage .
Two of our models ( scene graph update and textediting image ) are transparent to visualize intermediate changes in the scene after performing actions .
We analyse their ability to understand actions and make appropriate changes as shown in below part of Table 3 .
For the scene graph method , we compare the ground -truth functional program with the generated program and measure their exact-match accuracy .
For the text-editing image method , we generate scene graphs for both images ( original image and image after text-editing ) and compare them .
For attributes , we do exact-match , whereas for location information we consider matching only on the basis of relative spatial location .
Both scene graph and text- editing models do quite well on ' remove ' and ' change ' actions whereas struggle when new objects are added or existing objects are moved around .
The observation is consistent when multiple actions are combined .
Therefore , actions remove+ change can be performed with maximum accuracy whereas other combinations of actions accomplish relatively lower performance .
It leads to the conclusion that understanding the effect of different actions are of varied complexity .
Most models demonstrate better performance over counting , existence and attribute query type of questions than comparison questions .
The scene graph update and text-editing methods show a performance drop of 6.1 % and 9.1 % respectively when multiple actions are performed on the scene .
However , there is less of a performance gap for models on 2HopQ
H compared to the test set , suggesting that models are able to better generalize with respect to multiple reasoning skills than complex actions .
Conclusion
We introduce CLEVR_HYP , a dataset to evaluate the ability of VQA systems after hypothetical actions are performed over the given image .
We create this dataset by extending the data generation framework of CLEVR ( Johnson et al. , 2017a ) that uses synthetically rendered images and templates for reasoning questions .
Our dataset is challenging because rather than asking to reason about objects already present in the image , it asks about what would happen in an alternative world where changes have occurred .
We provide ground - truth representations for images , hypothetical actions and questions to facilitate the development of models that systematically learn to reason about underlying process .
We create several baseline models to benchmark CLEVR_HYP and report their results .
Our analysis shows that the models are able to perform reasonably well ( 70.5 % ) on the limited number of actions and reasoning types , but struggle with complex scenarios .
While neural models have achieved almost perfect performance on CLEVR and considering human performance as upperbound ( 98 % ) , there is a lot of room for improvement on CLEVR_HYP .
Our future work would include relaxing constraints by allowing a larger variety of actions , attributes and reasoning types .
By extending this approach further for natural images , we aim to contribute in the development of better vision + language models .
