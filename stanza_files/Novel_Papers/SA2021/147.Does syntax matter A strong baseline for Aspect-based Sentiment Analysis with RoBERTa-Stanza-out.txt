title
Does syntax matter ?
A strong baseline for Aspect- based Sentiment Analysis with RoBERTa
abstract
Aspect - Based Sentiment Analysis ( ABSA ) , aiming at predicting the polarities for aspects , is a fine- grained task in the field of sentiment analysis .
Previous work showed syntactic information , e.g. dependency trees , can effectively improve the ABSA performance .
Recently , pre-trained models ( PTMs ) also have shown their effectiveness on ABSA .
Therefore , the question naturally arises whether PTMs contain sufficient syntactic information for ABSA so that we can obtain a good ABSA model only based on PTMs .
In this paper , we firstly compare the induced trees from PTMs and the dependency parsing trees on several popular models for the ABSA task , showing that the induced tree from finetuned RoBERTa ( FT - RoBERTa ) outperforms the parser-provided tree .
The further analysis experiments reveal that the FT - RoBERTa Induced Tree is more sentiment - word -oriented and could benefit the ABSA task .
The experiments also show that the pure RoBERTa - based model can outperform or approximate to the previous SOTA performances on six datasets across four languages since it implicitly incorporates the task - oriented syntactic information .
1 * Equal contribution .
Introduction Aspect- based sentiment analysis ( ABSA ) aims to do the fine-grained sentiment analysis towards aspects ( Pontiki et al. , 2014 ( Pontiki et al. , , 2016 .
Specifically , for one or more aspects in a sentence , the task calls for detecting the sentiment polarities for all aspects .
Take the sentence " great food but the service was dreadful " for example , the task is to predict the sentiments towards the underlined aspects , which expects to get polarity positive for aspect food and polarity negative for aspect service .
Generally , ABSA contains aspect extraction ( AE ) and aspect-level sentiment classification ( ALSC ) .
We only focus on the ALSC task .
Early works of ALSC mainly rely on manually designed syntactic features , which is laborintensive yet insufficient .
In order to avoid designing hand -crafted features ( Jiang et al. , 2011 ; Kiritchenko et al. , 2014 ) , various neural network models have been proposed in ALSC ( Dong et al. , 2014 ; Vo and Zhang , 2015 ; Wang et al. , 2016 ; Chen et al. , 2017 ; He et al. , 2018 ; . Since the dependency tree can help the aspects find their contextual words , most of the recently proposed State - of - the-art ( SOTA ) ALSC models utilize the dependency tree to assist in modeling connections between aspects and their opinion words Sun et al. , 2019 b ; . Generally , these dependency tree based ALSC models are implemented in three methods .
The first one is to use the topological structure of the dependency tree ( Dong et al. , 2014 ; Zhang et al. , 2019a ; Huang and Carley , 2019 ; Sun et al. , 2019 b ; Zheng et al. , 2020 ; Tang et al. , 2020 ) ;
The second one is to use the treebased distance , which counts the number of edges in a shortest path between two tokens in the dependency tree ( He et al. , 2018 ; Phan and Ogunbona , 2020 ) ;
The third one is to simultaneously use both the topological structure and the tree-based distance .
Except for the dependency tree , pre-trained models ( PTMs ) ( Qiu et al. , 2020 ) , such as BERT ( Devlin et al. , 2019 ) , have also been used to enhance the performance of the ALSC task ( Sun et al. , 2019a ; Tang et al. , 2020 ; Phan and Ogunbona , 2020 ; . From the view of interpretability of PTMs , Chen et al .
( 2019 ) ; Hewitt and Manning ( 2019 ) ; try to use probing methods to detect syntactic information in PTMs .
Empirical results reveal that PTMs capture some kind of dependency tree structures implicitly .
Therefore , two following questions arise naturally .
Q1 : Will the tree induced from PTMs achieve better performance than the tree given by a dependency parser when combined with different tree - based ALSC models ?
To answer this question , we choose one model from each of the three typical dependency tree based methods in ALSC , and compare their performance when combined with the parser -provided dependency tree and the off-the-shelf PTMs induced trees .
Q2 : Will PTMs adapt the implicitly entailed tree structure to the ALSC task during the finetuning ?
Therefore , in this paper , we not only use the trees induced from the off-the-shelf PTMs to enhance ALSC models , but also use the trees induced from the fine- tuned PTMs
( In short FT - PTMs ) which are fine-tuned on the ALSC datasets .
Experiments show that trees induced from FT - PTMs can help tree - based ALSC models achieve better performance than their counterparts before finetuning .
Besides , models with trees induced from the ALSC fine - tuned RoBERTa can even outperform trees from the dependency parser .
Last but not least , we find that the base RoBERTa with an MLP layer is enough to achieve State-ofthe -art ( SOTA ) or near SOTA performance on all six ALSC datasets across four languages , while incorporating tree structures into RoBERTa - based ALSC models does not achieve concrete improvement .
Therefore , our contributions can be summarized as : ( 1 ) We extensively study the induced trees from PTMs and FT - PTMs .
Experiments show that models using induced trees from FT - PTMs achieve better performance .
Moreover , models using induced trees from fine - tuned RoBERTa outperform other trees .
( 2 ) The analysis of the induced tree from FT - PTMs shows that it tends to be more sentimentword -oriented , making the aspect term directly connect to its sentiment adjectives .
( 3 ) We achieve SOTA or near SOTA performances on six ALSC datasets across four languages based on RoBERTa .
We find that the RoBERTa could better adapt to ALSC and help the aspects to find the sentiment words .
Related Work ALSC without Dependencies Vo and Zhang ( 2015 ) propose the early neural network model which does not rely on the dependency tree .
Along this line , diverse neural network models have been proposed .
Tang et al . ( 2016a ) use the long short term memory ( LSTM ) network to enhance the interactions between aspects and context words .
In order to model relations of aspects and their contextual words , Wang et al . ( 2016 ) ; Liu and Zhang ( 2017 ) ; Ma et al . ( 2017 ) ; Tay et al. ( 2018 ) incorporate the attention mechanism into the LSTM - based neural network models .
Other model structures such as convolutional neural network ( CNN ) Xue and Li , 2018 ) , gated neural network ( Zhang et al. , 2016 ; Xue and Li , 2018 ) , memory neural network ( Tang et al. , 2016 b ; Chen et al. , 2017 ; Wang et al. , 2018 ) , attention neural network ( Tang et al. , 2019 ) have also been applied in ALSC .
ALSC with Dependencies Early works of ALSC mainly employ traditional text classification methods focusing on machine learning algorithms and manually designed features , which took syntactic structures into consideration from the very beginning .
Kiritchenko et al. ( 2014 ) combine a set of features including sentiment lexicons and parsing dependencies , from which experiments show the effectiveness of context parsing features .
A myriad of works attempt to fuse dependency tree into neural network models in ALSC .
Dong et al. ( 2014 ) propose to convert the dependency tree into a binary tree first , then apply the adaptive recursive neural network to propagate information from the context words to aspects .
Despite the improvement of aspect-oriented feature modeling , converting the dependency tree into a binary tree might cause syntax related words separated away from each other .
In general , owing to the syntax parsing errors , early dependency tree based ALSC models do not show clear preponderance over models without the dependency tree .
However , the introduction of the neural network into the dependency parsing task enhances the parsing quality substantially ( Chen and Manning , 2014 ; Dozat and Manning , 2017 ) .
Recent advances , leveraging graph neural network ( GNN ) to model the dependency tree ( Zhang et al. , 2019a ; Huang and Carley , 2019 ; Sun et al. , 2019 b ; Tang et al. , 2020 ; , have achieved significant performance .
Among them , Zheng et al . ( 2020 ) ; attempt to convert the dependency tree into the aspect-oriented dependency tree .
Instead of using the topological structure of dependency tree , He et al .
( 2018 ) ; ; Phan and Ogunbona ( 2020 ) exploit the tree-based distance between two tokens in the dependency tree .
PTMs - based Dependency Probing
Over the past few years , the pre-trained models ( PTMs ) have dominated across various NLP tasks .
Therefore , many researchers are attracted to investigate what linguistic knowledge has been captured by PTMs ( Clark et al. , 2019 ; Hewitt and Liang , 2019 ; Hewitt and Manning , 2019 ; . Clark et al. ( 2019 ) try to use a single or a combination of head attention maps of BERT to infer the dependencies .
Since BERT has many attention heads , this method can hardly fully reveal the dependency between two tokens .
Hewitt and Manning ( 2019 ) propose a small learnable probing model to probe the syntax dependencies encoded in BERT .
Despite very few parameters been added , it may still be very hard to tell if the syntactic information is encoded by BERT itself or by the additional parameters from the probing model .
Therefore , the parameterfree dependency probing method proposed in might be more preferred .
Method
In this section , we first introduce how to induce trees from PTMs , then we describe three tree - based ALSC models , which are selected from three representative methods of incorporating the dependency tree in ALSC task .
Inducing Tree Structure from PTMs Perturbed Masking can induce trees from the pre-trained models without additional parameters .
Generally , a broad range of PTMs can be applied in the Perturbed Masking method .
For the sake of being representative and practical , we select BERT and RoBERTa as our base models .
In this subsection , we first briefly introduce the model structure of BERT and RoBERTa , then present the basic idea of the Perturbed Masking method .
More details about them can be found in their respective reference papers .
BERT and RoBERTa BERT ( Devlin et al. , 2019 ) and RoBERTa ( Liu et al. , 2019 ) both take Transformers ( Vaswani et al. , 2017 ) as backbone architecture .
Generally , they can be formulated as the following equations ?l = LN ( h l?1 + MHAtt ( h l?1 ) ) , ( 1 ) h l = LN ( ?l + FFN ( ?l ) ) , ( 2 ) where h 0 is the BERT / RoBERTa input representation , formed by the sum of token embeddings , position embeddings , and segment embeddings ; LN is the layer normalization layer ; MHAtt is the multi-head self-attention ; FFN contains three layers , the first one is a linear projection layer , then an activation layer , then another linear projection layer ; l is the depth of Transformer layers .
The base and large version of BERT and RoBERTa have 12 , 24 Transformer layers , respectively .
BERT is pre-trained on Masked Language Modeling ( MLM ) and Next Sentence Prediction ( NSP ) tasks .
In the MLM task , 15 % of the tokens in a sentence are manipulated in three ways .
Specifically , 10 % , 10 % , 80 % of them are replaced by a random token , itself , or a " [ MASK ] " token , respectively .
In the NSP task , two sentences A and B are concatenated before sending to BERT .
Given 50 % of the time when B is the next utterance of A , BERT needs to utilize the vector representation of " [ CLS ] " to figure out whether the input is continuous or not .
RoBERTa is only pre-trained on the MLM task .
Perturbed Masking Perturbed Masking aims to detect syntactic information from pre-trained models .
For a sentence x = [ x 1 , . . . , x T ] , BERT and RoBERTa will map each x i into a contextualized representation H ? ( x ) i .
Perturbed Masking is trying to derive the value f ( x i , x j ) that denotes the impact a token x j has on another token x i .
To derive this value , it first uses the " [ MASK ] " ( or " < mask > " in RoBERTa ) to replace the token x i , which returns a representation H ? ( x\{ x i } ) i for the masked x i ; secondly , it further masks the token x j , which returns a representation H ? ( x\{x i , x j }) i with both x i , x j being masked .
The impact value f ( x i , x j ) is calculated by the Euclidean distance as follows , f( x i , x j ) = ||H ? ( x\{ x i } ) i ?H ? ( x\{x i , x j } ) i || 2 ( 3 )
By repeating this process between every two tokens in the sentence , we can get an impact matrix M ? R T ?T and M i , j = f ( x i , x j ) .
The tree decoding algorithm , such as Eisner ( Eisner , 1996 ) and Chu-Liu / Edmonds ' algorithm ( Chu and Liu , 1965 ; Edmonds , 1967 ) , is then used to extract the dependency tree from the matrix M. The Perturbed Masking can exert on any layer of BERT or RoBERTa .
ALSC Models Based on Trees
In this subsection , we introduce three representative tree - based ALSC models .
Each of the model is from the methods mentioned in the Introduction part ( Section 1 ) .
For a fair comparison , all the selected models are of the most recently advanced tree - based ALSC models .
We briefly introduce these three models as follows .
Aspect-specific Graph Convolutional Networks ( ASGCN )
The Aspect-specific Graph Convolutional Networks ( ASGCN ) is proposed by Sun et al . ( 2019 b ) .
They utilize the dependency tree as a graph , where each word is viewed as a node and the dependencies between words are deemed as edges .
After converting the dependency tree into the graph , ASGCN uses the Graph Convolutional Network ( GCN ) to operate on this graph to model dependencies between each word .
Proximity -Weighted Convolution Network ( PWCN )
The Proximity - Weighted Convolution Network ( PWCN ) model is proposed by .
They try to help the aspect to find their contextual words .
For an input sentence , the PWCN first gets its dependency tree , and based on this tree it would assign a proximity value to each word in the sentence .
The proximity value for each word is calculated by the shortest path in the dependency tree between this word and the aspects .
Relational Graph Attention Network ( RGAT )
The Relational Graph Attention Network ( RGAT ) is proposed by .
In the RGAT model , they transform the dependency tree into an aspect-oriented dependency tree .
The aspectoriented dependency tree uses the aspect as the root node , and all other words depend on the aspect directly .
The relation between the aspect and other words is either based on the syntactic tag or the treebased distance in the dependency tree .
Specifically , the RGAT reserves syntactic tags for words with 1 tree- based distance to aspect , and assigns virtual tags to longer distance words , such as " 2:con " for " A 2 tree- based distance connection " .
Therefore , the RGAT model not only exploits the topological structure of the dependency tree but also the treebased distance between two words .
Experimental Setup
In this section , we present details about the datasets , the tree structures used in experiments , as well as the experiments implementations .
We conduct experiments on all six datasets across four languages .
But due to the limited space , we present our experiments on the non-English datasets in the Appendix .
Datasets
We run experiments on six benchmark datasets .
Three of them , namely , Rest14 , Laptop14 , and Twitter , are English datasets .
Rest14 and Laptop14 are from SemEval 2014 task 4 ( Pontiki et al. , 2014 ) , containing sentiment reviews from restaurant and laptop domains .
Twitter is from Dong et al . ( 2014 ) , which is processed from tweets .
The statistics of these datasets are presented in Table 6 . Details of the other three non-English datasets can be found in the Appendix .
Following previous works , we remove samples with conflicting polarities or with " NULL " aspects in all datasets .
Tree Structures
For each dataset , we obtain five kinds of trees from three sources .
( 1 )
The first one is derived from the off- the-shelf dependency tree parser , such as spaCy 2 and allenNLP 3 , written as " Dep . " .
For the three English datasets , we use the biaffine parser from the allenNLP package to get the dependency tree , which is reported in that the biaffine parser could achieve better performance .
( 2 ) We induce trees from the pre-trained BERT and RoBERTa by the Perturbed Masking method , written them as " BERT Induced Tree " and " RoBERTa Induced Tree " , respectively .
use the Perturbed Masking method to induce trees from the fine- tuned BERT and RoBERTa after finetuning in the corresponding datasets .
These two are written as " FT - BERT Induced Tree " and " FT - RoBERTa Induced Tree " .
Besides , we add " Left-chain " and " Right-chain " in our experiments .
" Left-chain " , " Right- chain " mean that every word deems its previous or next word as the dependent child word .
Implementation Details
In order to derive the FT - PTMs Induced Tree , we fine- tune BERT and RoBERTa on the ALSC datasets .
To introduce as few parameters as possible , a rather simple MLP is used and the overall structure of our fine-tuning model is presented in Figure 1 .
The fine-tuning experiments are with the batch size b = 32 , dropout rate d = 0.1 , learning rate ? = 2e - 4 using the AdamW optimizer with the default settings .
As for the Perturbed Masking method , we apply Chu-Liu / Edmonds ' algorithm for the tree decoding .
For the induced trees , we first induce trees from each layer of the PTMs , then test them by the model in Figure 1 on dev set which is composed by 20 % of training set .
Experiments show that the trees induced from the 11th layer of the PTMs could achieve the best performance among all layers , which is applied for all our experiments .
We conduct multiple experiments incorporating different trees ( Section 4.2 ) into the aforementioned tree - based models ( Section 3.2 ) .
Specifically , we use the 300 - dimension Glove ( Pennington et al. , 2014 ) embeddings for English datasets .
We keep the word embeddings fixed to avoid overfitting .
It is worth noting that in experiments with the RGAT model , since the induced tree does not provide syntactic tags , we assign virtual tags for every dependency in a uniform way , which slightly damage the performance of model .
Experimental Results
ALSC Performance with Different Trees
The comparison between models with different trees is presented in Table 2 , which comprises experiments results of English datasets .
The results of non-English datasets can be found in the Appendix .
We observe that among all the trees , incorporating FT - RoBERTa Induced Tree leads to the best results on all datasets .
On average , models based on the FT - RoBERTa Induced Tree outperform " Dep. " by about 1.1 % in accuracy .
This proves the effectiveness and advantage of FT - RoBERTa Induced Tree in this competitive comparison .
Models using BERT Induced Tree and RoBERTa Induced Tree from Table 2 show small performance difference in all but one dataset , and both are close to the " Left-chain " and " Right-chain " baselines .
To have a better sense , we visualize trees induced from RoBERTa in Figure 2 b .
It shows that RoBERTa Induced Tree has strong neighboring connection dependency pattern .
This behavior is expected since the masked language modeling pre-training task will make words favor depending more on its neighboring words .
This tendency may be the reason why PTMs induced trees perform similarly to the " Left-chain " and " Right-chain " baselines .
To answer the question Q1 in the Introduction part ( Section 1 ) , we need to compare the " Dep. " , BERT Induced Tree , and RoBERTa Induced Tree results .
The results show that models with dependency trees usually achieve better performance than PTMs induced trees .
This is predictable since the word in PTMs induced trees tends to depend on words in their either left or right side as shown in Figure 2 .
It is worth noting that this observation does not align with the observation in .
The experiments based on PWCN in show that BERT Induced Tree achieves comparable results with the " Dep. " , which is consistent with our PWCN results .
However , this observation does not hold when the induced trees are used in a broader range of tree - based ALSC models , especially for the RGAT model in the bottom of Table 2 .
More detailed analysis will be provided in the next section .
Although models with the PTMs induced trees usually perform worse than those with the dependency parsing trees , models with trees induced from ALSC fine - tuned RoBERTa can surpass both of them .
Take RoBERTa Induced Tree and FT -RoBERTa Induced Tree in Table 2 : The performance ( % ) of tree - based ALSC models incorporating different tree structures on three major English datasets .
Following previous work , Accuracy ( Acc. ) and Marco - F 1 ( F 1 ) are used for metric .
The reported results are averaged by 3 runs with random initialization .
Results named as cited format refer to performance reported in the original paper .
Dep. refers to the dependency tree generated from the well -known Biaffine Parser ( Dozat and Manning , 2017 ) .
As mentioned in Section 4.2 , BERT Induced Tree , RoBERTa Induced Tree , FT - BERT , and FT - RoBERTa Induced
Tree refer to tree structures induced from corresponding PTM .
We provide BiLSTM since the other three are different tree - based models over BiLSTM .
We highlight the best results of each model in bold .
compared with RoBERTa Induced Tree , models incorporating FT - RoBERTa Induced Tree achieves an average accuracy improvement of 1.56 % .
This trending is also observed between BERT Induced Tree and FT - BERT Induced Tree .
Table 3 : Proportion of neighboring connections of different trees in all datasets .
We use the short name of induced trees here as well as Table 4 and Table 5 .
Analysis
To further investigate the reasons for the difference between trees , we propose a set of quantitative metrics , presented in Table 3 and Table 4 .
The Proportion of Neighboring Connections is to calculate the proportion of neighboring connections in the sentence , shown in Table 3 .
A neighboring connection links the word to its left / right neighbor word .
From Table 3 , we observe that on average over 70 % relations in BERT / RoBERTa Induced
Tree are neighboring connections .
This will damage the performance of models using topological structures of trees .
Thus , PTMs induced trees usually perform worse than " Dep. " , with a slight ( c ) The FT -RoBERTa Induced Tree Figure 2 : Visualization of different trees .
The colored box refers to the aspect terms .
Since ROOT has no directional relation arcs , we omit the ROOT notation here .
For the same two sentences , trees from dependency parser , RoBERTa and fine- tuned RoBERTa are displayed .
As Figure 2 b shows , trees induced from RoBERTa tend to have more neighboring connections .
As the bottom two figures show , trees induced from fine - tuned RoBERTa tend to have connections between sentiment words and others words .
improvement over left / right- chains .
In comparison with RoBERTa Induced Tree , a significant decline of the proportion is shown in FT - RoBERTa Induced Tree in Table 3 .
We see the same tendency in BERT Induced Tree and FT - BERT Induced Tree .
This marks the consistent structure change in the fine-tuning process , indicating the transition to a more diverse structure .
As shown in Figure 2 b , RoBERTa Induced
Tree has a clear pattern to depend on words in their neighbor side .
Yet FT-RoBERTa Induced Tree in Figure 2 c shows a more diverse dependency pattern .
Aspects -sentiment Distance is the average distance between aspect and sentiment words .
We pre-define a sentiment words set C .
For a sentence S i in datasets S , the set of aspects words in S i is termed as w .
S i ?
C is the set of sentiment words appearing both in the sentence S i and the sentiment words set C .
The Aspects-sentiment Distance ( AsD ) is calculated as follows : AsD ( S i ) = w i w C i C =S i ?C dist ( C i , w i ) | w| | C | ( 4 ) AsD = S i S AsD ( S i ) | S| ( 5 ) where | ?
| is the number of elements in the set and dist ( x i , x j ) represents the relative distance be-tween x i and x j in the tree .
Specifically , C contains sentiment words counted on Amazon - 2 from Tian et al . ( 2020 ) , which can be found in the Appendix .
As for the Rest14 and Laptop14 , provides the paired sentiment words with its corresponding aspect .
We also calculate the paired Aspects-sentiment Distance ( pAsD ) on these two datasets , which only counts the distance between aspect and its corresponding sentiment words .
trees are more sentiment - word - oriented , which partially reveals that the fine-tuning in ALSC encourages the aspects to find sentiment words .
However , for the " Dep. " , we notice that some Twitter results in Table 2 can not be fully explained by these two metrics .
We conjecture that the grammar casualness features the Twitter corpus , which makes the parser hard to provide an accurate dependency parsing tree .
Still , these two metrics can be suitable for the induced trees .
Taken together , as the conclusion to Q2 , these analyses demonstrate that the fine-tuning on ALSC could adapt the induced tree implicitly .
On the one hand , less proportion of neighboring connections after fine-tuning indicates the increase of long range connections .
On the other hand , less Aspectssentiment Distance after fine-tuning illustrates the shorter distance between aspects and sentiment words , which helps to model connections between aspects and sentiment words .
Thus , as shown in Section 5.1 , fine-tuning RoBERTa in ALSC not only makes induced tree better suit the ALSC task but also outperform the dependency tree when combined with different tree - based ALSC models .
Comparison between ALSC models Additional , we explore how well the fine- tuned RoBERTa model could achieve in the ALSC task .
We select a set of top high- performing models of ALSC as state - of - the - art alternatives .
The compari-son results are shown in Table 5 .
Comparing with all these SOTA alternatives , surprisingly , the RoBERTa with an MLP layer achieve SOTA or near SOTA performance .
Especially , compared to other datasets , we notice that significant improvement is obtained on the Lap-top14 dataset .
We assume that the pre-training corpus of RoBERTa may be more friendly to the laptop domain since the RoBERTa - MLP already obtains much better results than the BERT - MLP on Laptop14 .
For these BERT - based models in the second row of Table 5 , similar experiments using RoBERTa are conducted .
However , limited improvements have been made over the RoBERTa - MLP .
We expect that induced trees from models specifically pre-trained for ALSC ( Tian et al. , 2020 ) may provide more information , which is left for the future works .
The FT -RoBERTa Induced
Tree could be beneficial to Glove based ALSC models .
However , incorporating trees over the RoBERTa brings no significant improvement , even the decline can be seen in some cases .
This may be caused by failure to reconcile the implicitly entailed tree with external tree .
We argue that incorporating trees over the RoBERTa in currently widely - used tree methods may be the loss outweighs the gain .
Additionally , in the review of previous ALSC works , we notice that very few works employ the RoBERTa as the base model .
We would attribute this to the difficulty of optimizing the RoBERTa - based ALSC models .
As the higher architecture , which is usually randomly initialized , needs a bigger learning rate compared to the RoBERTa .
The inappropriate hyperparameters may be the cause reason for the lagging performance of previous RoBERTa - based ALSC works ( Phan and Ogunbona , 2020 ) .
Conclusion
In this paper , we analyze several tree structures for the ALSC task including parser -provided dependency tree and PTMs - induced tree .
Specifically , we induce trees using the Perturbed Masking method from the original PTMs and ALSC finetuned PTMs respectively , and then compare the different tree structures on three typical tree - based ALSC models on six datasets across four languages .
Experiments reveal that fine-tuning on ALSC task forces PTMs to implicitly learn more sentimentword - oriented trees , which can bring benefits to Glove based ALSC models .
Benefited from its better implicit syntactic information , the fine- tuned RoBERTa with an MLP is enough to obtain SOTA or near SOTA results for ALSC task .
Our work can lead to several promising directions , such as PTMssuitable tree- based models and better tree-inducing methods from PTMs .
Table 8 : The results ( % ) of ALSC models incorporating with different tree structures on non-English datasets .
The definition of tree structures retains the same as the aforementioned .
The results with " " are retrieved from the original papers .
Figure 1 : 1 Figure 1 : Overall architecture of our fine-tuning model .
This structure is enough to achieve SOTA or near SOTA performance in six ALSC datasets based on RoBERTa .
