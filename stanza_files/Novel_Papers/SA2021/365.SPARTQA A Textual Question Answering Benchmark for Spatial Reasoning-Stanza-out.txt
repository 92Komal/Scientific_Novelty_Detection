title
SPARTQA : A Textual Question Answering Benchmark for Spatial Reasoning
abstract
This paper proposes a question - answering ( QA ) benchmark for spatial reasoning on natural language text which contains more realistic spatial phenomena not covered by prior work and is challenging for state - of - the - art language models ( LM ) .
We propose a distant supervision method to improve on this task .
Specifically , we design grammar and reasoning rules to automatically generate a spatial description of visual scenes and corresponding QA pairs .
Experiments show that further pretraining LMs on these automatically generated data significantly improves LMs ' capability on spatial understanding , which in turn helps to better solve two external datasets , bAbI , and boolQ .
We hope that this work can foster investigations into more sophisticated models for spatial reasoning over text .
Introduction Spatial reasoning is a cognitive process based on the construction of mental representations for spatial objects , relations , and transformations ( Clements and Battista , 1992 ) , which is necessary for many natural language understanding ( NLU ) tasks such as natural language navigation Roman Roman et al. , 2020 ; Kim et al. , 2020 ) , human-machine interaction ( Landsiedel et al. , 2017 ; Roman Roman et al. , 2020 ) , dialogue systems ( Udagawa et al. , 2020 ) , and clinical analysis ( Datta and Roberts , 2020 ) .
Modern language models ( LM ) , e.g. , BERT , ALBERT ( Lan et al. , 2020 ) , and XLNet ( Yang et al. , 2019 ) have seen great successes in natural language processing ( NLP ) .
However , there has been limited investigation into spatial reasoning capabilities of LMs .
To the best of our knowledge , bAbI ( Weston et al. , 2015 ) ( Fig 9 ) is the only dataset with direct textual spatial question answering ( QA ) ( Task 17 ) , but it is synthetic *
Work was done while at the Allen Institute for AI .
and overly simplified : ( 1 )
The underlying scenes are spatially simple , with only three objects and relations only in four directions .
( 2 ) The stories for these scenes are two short , templated sentences , each describing a single relation between two objects .
( 3 )
The questions typically require up to two -steps reasoning due to the simplicity of those stories .
To address these issues , this paper proposes a new dataset , SPARTQA 1 ( see Fig. 1 ) .
Specifically , ( 1 ) SPARTQA is built on NLVR 's ( Suhr et al. , 2017 ) images containing more objects with richer spatial structures ( Fig. 1 b ) . ( 2 ) SPARTQA 's stories are more natural , have more sentences , and richer in spatial relations in each sentence .
( 3 ) SPARTQA 's questions require deeper reasoning and have four types : find relation ( FR ) , find blocks ( FB ) , choose object ( CO ) , and yes / no ( YN ) , which allows for more fine- grained analysis of models ' capabilities .
We showed annotators random images from NLVR , and instructed them to describe objects and relationships not exhaustively at the cost of naturalness ( Sec. 3 ) .
In total , we obtained 1.1 k unique QA pair annotations on spatial reasoning , evenly distributed among the aforementioned types .
Similar to bAbI , we keep this dataset in relatively small scale and suggest to use as little training data as possible .
Experiments show that modern LMs ( e.g. , BERT ) do not perform well in this low-resource setting .
This paper thus proposes a way to obtain distant supervision signals for spatial reasoning ( Sec. 4 ) .
As spatial relationships are rarely mentioned in existing corpora , we take advantage of the fact that spatial language is grounded to the geometry of visual scenes .
We are able to automatically generate stories for NLVR images ( Suhr et al. , 2017 ) via our newly designed context free grammars ( CFG ) and context-sensitive rules .
In the process of story generation , we store the information about all ob - QUESTIONS : FB : Which block ( s ) has a medium thing that is below a black square ?
A , B , C FB : Which block ( s ) does n't have any blue square that is to the left of a medium square ?
A , B FR : What is the relation between the medium black square which is in block C and the medium square that is below a medium black square that is touching the bottom edge of a block ?
Left CO : Which object is above a medium black square ?
the medium black square which is in block C or medium black square number two ?
medium black square number two YN : Is there a square that is below medium square number two above all medium black squares that are touching the bottom edge of a block ?
Yes
STORY : We have three blocks , A , B and C .
Block B is to the right of block C and it is below block A. Block A has two black medium squares .
Medium black square number one is below medium black square number two and a medium blue square .
It is touching the bottom edge of this block .
The medium blue square is below medium black square number two .
Block B contains one medium black square .
Block C contains one medium blue square and one medium black square .
The medium blue square is below the medium black square .
jects and relationships , such that QA pairs can also be generated automatically .
In contrast to bAbI , we use various spatial rules to infer new relationships in these QA pairs , which requires more complex reasoning capabilities .
Hereafter , we call this automatically - generated dataset SPARTQA - AUTO , and the human-annotated one SPARTQA - HUMAN .
Experiments show that , by further pretraining on SPARTQA - AUTO , we improve LMs ' performance on SPARTQA - HUMAN by a large margin .
2
The spatially - improved LMs also show stronger performance on two external QA datasets , bAbI and boolQ ( Clark et al. , 2019 ) : BERT further pretrained on SPARTQA - AUTO only requires half of the training data to achieve 99 % accuracy on bAbI as compared to the original BERT ; on boolQ 's development set , this model shows better performance than BERT , with 2.3 % relative error reduction .
3 2 Further pretraining LMs has become a common practice and baseline method for transferring knowledge between tasks ( Phang et al. , 2018 ; Zhou et al. , 2020 ) .
We leave more advanced methods for future work .
3
To the best of our knowledge , the test set or leaderboard of boolQ has not been released yet .
Our contributions can be summarized as follows .
First , we propose the first human-curated benchmark , SPARTQA - HUMAN , for spatial reasoning with richer spatial phenomena than the prior synthetic dataset bAbI ( Task 17 ) .
Second , we exploit the scene structure of images and design novel CFGs and spatial reasoning rules to automatically generate data ( i.e. , SPARTQA - AUTO ) to obtain distant supervision signals for spatial reasoning over text .
Third , SPARTQA - AUTO proves to be a rich source of spatial knowledge that improved the performance of LMs on SPARTQA - HUMAN as well as on different data domains such as bAbI and boolQ .
Related work Question answering is a useful format to evaluate machines ' capability of reading comprehension and many recent works have been implementing this strategy to test machines ' understanding of linguistic formalisms :
He et al . ( 2015 ) ; ; Levy et al. ( 2017 ) ; Jia et al. ( 2018 ) ; Du and Cardie ( 2020 ) .
An important advantage of QA is using natural language to annotate natural language , thus having the flexibility to get annotations on complex phenomena such as spatial reasoning .
However , spatial reasoning phenomena have been covered minimally in the existing works .
To the best of our knowledge , Task 17 of the bAbI project ( Weston et al. , 2015 ) is the only QA dataset focused on textual spatial reasoning ( examples in Appendix F ) .
However , bAbI is synthetic and does not reflect the complexity of the spatial reasoning in natural language .
Solving Task 17 of bAbI typically does not require sophisticated reasoning , which is an important capability emphasized by more recent works ( e.g. , Dua et al .
( 2019 Spatial reasoning is arguably more prominent in multi-modal QA benchmarks , e.g. , NLVR ( Suhr et al. , 2017 ) , VQA ( Antol et al. , 2015 ) , GQA ( Hudson and Manning , 2019 ) , CLEVR ( Johnson et al. , 2017 ) .
However , those spatial reasoning phenomena are mostly expressed naturally through images , while this paper focuses on studying spatial reasoning on natural language .
Some other works on visual-spatial reasoning are based on geographical information inside maps and diagrams ( Huang et al. , 2019 ) and navigational instructions Anderson et al. , 2018 ) .
As another approach to evaluate spatial reasoning capabilities of models , a dataset proposed in Ghanimifard and Dobnik ( 2017 ) generates a synthetic training set of spatial sentences and evaluates the models ' ability to generate spatial facts and sentences containing composition and decomposition of relations on grounded objects .
SPARTQA-HUMAN
To mitigate the aforementioned problems of Task 17 of bAbI , i.e. , simple scenes , stories , and questions , we describe the data annotation process of SPARTQA - HUMAN , and explain how those problems were addressed in this section .
First , we randomly selected a subset of NLVR images , each of which has three blocks containing multiple objects ( see Fig 1 b ) .
The scenes shown by these images are more complicated than those described by bAbI because ( 1 ) there are more objects in NLVR images ; ( 2 ) the spatial relationships in NLVR are not limited to just four relative directions as objects are placed arbitrarily within blocks .
Figure 2 : For " A blue circle is above a big triangle .
To the left of the big triangle , there is a square , " if the question is : " Is the square to the left of the blue circle ? " , the answer is neither Yes nor No .
Thus , the correct answer is " Do not Know " ( DK ) in our setting .
Second , two student volunteers produced textual description of those objects and their corresponding spatial relationships based on these images .
Since the blocks are always horizontally aligned in each NLVR image , to allow for more flexibility , annotators could also rearrange these blocks ( see Fig. 1a ) .
Relationships between objects within the same block can take the forms of relative direction ( e.g. , left or above ) , qualitative distance ( e.g. , near or far ) , and topological relationship ( e.g. , touching or containing ) .
However , we instructed the annotators not to describe all objects and relationships , ( 1 ) to avoid unnecessarily verbose stories , and ( 2 ) to intentionally miss some information to enable more complex reasoning later .
Therefore , annotators describe only a random subset of blocks , objects , and relationships .
To query more interesting phenomena , annotators were then encouraged to write questions requiring detecting relations and reasoning over them using multiple spatial rules .
A spatial rule can be one of the transitivity ( A ? B , B ? C ? A ? C ) , symmetry ( A ? B ? B ? A ) , converse ( ( A , R , B ) ? ( B , reverse ( R ) , A ) ) , inclusion ( obj1 in A ) , and exclusion ( obj1 not in B ) rules .
There are four types of questions ( Q- TYPE ) .
( 1 ) FR : find relation between two objects .
( 2 ) FB : find the block that contains certain object ( s ) .
( 3 ) CO : choose between two objects mentioned in the question that meets certain criteria .
( 4 ) YN : a yes / no question that tests if a claim on spatial relationship holds .
FB , FR , and CO questions are formulated as multiple -choice questions 4 and receive a list of candidate answers , and YN questions ' answer is choosing from Yes , No , or " DK " ( Do not Know ) .
The " DK " option is due to the open-world assumption of the stories , where if something is not described in the text , it is not considered as false ( See Fig. 2 ) .
Finally , annotators were able to create 1.1 k QA pairs on spatial reasoning on the generated descriptions , distributed among the aforementioned types .
We intentionally keep this data in a relatively small scale due to two reasons .
First , there has been some consensus in our community that modern systems , given their sufficiently large model capacities , can easily find shortcuts and overfit a dataset if provided with a large training data Sen and Saffari , 2020 ) .
Second , collecting spatial reasoning QAs is very costly :
The two annotators spent 45 - 60 mins on average to create a single story with 8- 16 QA pairs .
We estimate that SPARTQA - HUMAN costed about 100 human hours in total .
The expert performance on 100 examples of SPARTQA - HUMAN 's test set measured by their accuracy of answering the questions is 92 % across four Q-TYPEs on average , indicating its high quality .
Distant Supervision : SPARTQA -AUTO
Since human annotations are costly , it is important to investigate ways to generate distant supervision signals for spatial reasoning .
However , unlike conventional distant supervision approaches ( e.g. , Mintz et al . ( 2009 ) ; Zeng et al . ( 2015 ) ; Zhou et al . ( 2020 ) ) where distant supervision data can be selected from large corpora by implementing specialized filtering rules , spatial reasoning does not appear often in existing corpora .
Therefore , similar to SPARTQA - HUMAN , we take advantage of the ground truth of NLVR images , design CFGs to generate stories , and use spatial reasoning rules to ask and answer spatial reasoning questions .
This automatically generated data is called SPARTQA - AUTO , and below we describe its generation process in detail .
Story generation Since NLVR comes with structured descriptions of the ground truth locations of those objects , we were able to choose random blocks and objects from each image programmatically .
The benefit is two -fold .
First , a random selection of blocks and objects allows us to create multiple stories for each image ; second , this randomness also creates spatial reasoning opportunities with missing information .
Once we decide on a set of blocks and objects to be included , we determine their relationships :
Those relationships between blocks are generated randomly ; as for those between objects , we refer to the ground truth of these images to determine them .
Now we have a scene containing a set of blocks and objects and their associated relationships .
To produce a story for this scene , we design CFGs to produce natural language sentences that describe those blocks / objects / relationships in various expressions ( see Fig.
3 for two portions of our CFG describing relative and nested relations between objects ) .
tems ( Heilman and Smith , 2009 ; Labutov et al. , 2015 ) , neural networks ( Du et al. , 2017 ) , and their combinations ( Dhole and Manning , 2020 ) .
However , in our approach , during generating each story , the program stores the information about the entities and their relationships .
Thus , without processing the raw text , which is error-prone , we generate questions by only looking at the stored data .
The question generation operates based on four primary functionalities , Choose-objects , Describe-objects , Find-all-relations , and Find-similar-objects .
These modules are responsible to control the logical consistency , correctness , and the number of steps required for reasoning in each question .
The big black shape is above the medium triangle .
Choose-objects randomly chooses up to three objects from the set of possible objects in a story under a set of constraints such as preventing selection of similar objects , or excluding objects with relations that are directly mentioned in the text .
Describe- Objects generates a mention phrase for an object using parts of its full name ( presented in the story ) .
The generated phrase is either pointing to a unique object or a group of objects such as " the big circle , " or " big circles . "
To describe a unique object , it chooses an attribute or a group of attributes that apply to a unique object among others in the story .
To increase the steps of reasoning , the description may include the relationship of the object to other objects instead of using a direct unique description .
For example , " the circle which is above the black triangle . "
Find-all-relations completes the relationship graph between objects by applying a set of spatial rules such as transitivity , symmetry , converse , inclusion , and exclusion on top of the direct relations described in the story .
As shown in Fig. 4 , it does an exhaustive search over all combinations of the relations that link two objects to each other .
Find-similar-objects finds all the mentions matching a description from the question to objects in the story .
For instance , for the question " is there any blue circle above the big blue triangle ? " , this module finds all the mentions in the story matching the description " a blue circle " .
Similar to the SPARTQA - HUMAN , we provide four Q-TYPEs FR , FB , CO , and YN .
To generate FR questions , we choose two objects using Choose-objects module and question their relationships .
The YN Q-TYPE is similar to FR , but the question specifies one relationship of interest chosen from all relation extracted by Find-all - relations module to be questioned about the objects .
Since most of the time , Yes / No questions are simpler problems , we make this question type more complex by adding quantifiers ( adding " all " and " any " ) .
These quantifiers help to evaluates the models ' capability to aggregate relations between more than two objects in the story and do the reasoning over all find relations to find the final answer .
In FB Q-TYPE , we mention an object by its indirect relation to another object using the nested relation in Describe-objects module and ask to find the blocks containing or not containing this object .
Finally , the CO question selects an anchor object ( Choose-objects ) and specifies a relationship ( using Find-all- relations ) in the question .
Two other objects are chosen as candidates to check whether the specified relationship holds between them and the anchor object .
We tend to force the algorithm to choose objects as candidates that at least have one relationship to the anchor object .
To see more details about different question ' templates see Table 7 in the Appendix .
Answer generation
We compute all direct and indirect relationships between objects using Findall-relations function and based on the Q-TYPEs generate the final answer .
For instance , in YN Q-TYPE if the asked relation exists in the found relations , the answer is " Yes " , if the inverse relation exists it must be " No " , and otherwise , it is " DK " 5 .
Corpus Statistics
We generate the train , dev , and test set splits based on the same splits of the images in the NLVR dataset .
On average , each story contains 9 sentences ( Min:3 , Max : 22 ) and 118 tokens ( Min : 66 , Max : 274 ) .
Also , the average tokens of each question ( on all Q-TYPE ) is 23 ( Min :6 , Max : 57 ) .
Table 1 shows the total number of each question type in SPARTQA - AUTO ( Check Appendix to see more statistic information about the labels in Tab 8 . )
Models for Spatial Reasoning over Language
This section describes the model architectures on different Q-TYPEs : FR , YN , FB , and CO .
All Q-TYPEs can be cast into a sequence classification task , and the three transformer - based LMs tested in this paper , BERT , ALBERT ( Lan et al. , 2020 ) , and XLNet ( Yang et al. , 2019 ) , can all handle this type of tasks by classifying the representation of [ CLS ] , a special token prepended to each target sequence ( see Appendix E ) .
Depending on the Q-TYPE , the input sequence and how we do inference may be different .
FR and YN both have a predefined label set as candidate answers , and their input sequences are both the concatenation of a story and a question .
While the answer to a YN question is a single label chosen from Yes , No , and DK , FR questions can have multiple correct answers .
Therefore , we treat each candidate answer to FR as an independent binary classification problem , and take the union as the final answer .
As for YN , we choose the label with the highest confidence ( Fig 8 b ) .
As the candidate answers to FB and CO are not fixed and depend on each story and its question the input sequences to these Q-TYPEs are concatenated with each candidate answer .
Since the defined YN and FR model has moderately less accurate results on FB and CO Q-TYPEs , we add a LSTM ( Hochreiter and Schmidhuber , 1997 ) layer to improve it .
Hence , to find the final answer , we run the model with each candidate answer and then apply an LSTM layer on top of all token representations .
Then , we use the last vector of the LSTM outputs for classification ( Fig 8a ) .
The final answers are selected based on Eq. ( 1 ) .
x i = [ s , c i , q ]
T i = [ t i 1 , ... , t i m i ] = LM ( x i ) [ h i 1 , ... , h i m i ] = LSTM ( T i ) y i = [y 0 i , y 1 i ] = Softmax ( h i T m i W ) )
Answer = {c i | arg max j ( y j i ) = 1 } ( 1 ) where s is the story , c i is the candidate answer , q is the question , [ ] indicates the concatenation of the listed vectors , and m i is tokens ' number in x i .
The parameter vector , W , is shared for all candidates .
Training and Inference
We train the models based on the summation of the cross-entropy losses of all binary classifiers in the architecture .
For FR and YN Q-TYPEs , there are multiple classifiers , while there is only one classifier used for CO and FB Q-TYPEs .
We remove inconsistent answers in postprocessing for FR and YN Q-TYPEs during inference phase .
For instance on FR , left and right relations between two objects cannot be valid at the same time .
For YN , as there is only one valid answer amongst the three candidates , we select the candidate with the maximal predicted probability of being the true answer .
Experiments
As fine-tuning LMs has become a common baseline approach to knowledge transfer from a source dataset to a target task , including but not limited to Phang et al .
( 2018 ) ; Zhou et al . ( 2020 ) ; He et al. ( 2020 b ) , we study the capability of spatial reasoning of modern LMs , specifically BERT , ALBERT , and XLNet , after fine-tuning them on SPARTQA - AUTO .
This fine-tuning process is also known as further pretraining , to distinguish with the finetuning process on one 's target task .
It is an open problem to find out better transfer learning techniques than simple further pretraining , as suggested in He et al . ( 2020a ) ; , which is beyond the scope of this work .
All experiments use the models proposed in Sec. 5 .
We use AdamW ( Loshchilov and Hutter , 2017 ) with 2 ? 10 ?6 learning rate and Focal Loss ( Lin et al. , 2017 ) with ? = 2 for training all the models .
6
Further pretraining on SPARTQA - AUTO improves spatial reasoning Table 2 shows performance on SPARTQA - HUMAN in a low-resource setting , where 0.6 k QA pairs from SPARTQA - HUMAN are used for fine-tuning these LMs and 0.5 k for testing ( see Table 1 for information on this split ) .
7 During our annotation , we found that the description of " near to " and " far from " varies largely between annotators .
Therefore , we ignore these two relations from FR Q-TYPE in our evaluations .
In Table 2 , System 5 , BERT ( SPARTQA - AUTO ) , is the proposed method of further pretraining BERT on SPARTQA - AUTO .
We can see that System 2 , the original BERT , performs consistently lower than System 5 , indicating that having SPARTQA - AUTO as a further pretraining task improves BERT 's spatial understanding .
Table 3 : Switching from accuracy in Table 2 to F 1 shows that the models are all performing better than the majority baseline on YN Q-TYPE .
In addition , we implement another two baselines .
System 3 , BERT ( Stories only ; MLM ) : further pretraining BERT only on the stories of SPARTQA - AUTO as a masked language model ( MLM ) task ; System 4 , BERT ( SPARTQA - AUTO ; MLM ) : we convert the QA pairs in SPARTQA - AUTO into textual statements and further pretrain BERT on the text as an MLM ( see Fig.
5 for an example conversion ) .
To convert each question and its answer into a sentence , we utilize static templates for each question type which removes the question words and rearranges other parts into a sentence .
We can see that System 3 slightly improves over System 2 , an observation consistent with many prior works that seeing more text generally helps an LM ( e.g. , Gururangan et al . ( 2020 ) ) .
The signif- icant gap between System 3 and the proposed System 5 indicates that supervision signals come more from our annotations in SPARTQA - AUTO rather than from seeing more unannotated text .
System 4 is another way to make use of the annotations in SPARTQA - AUTO , but it is shown to be not as effective as further pretraining BERT on SPARTQA - AUTO as a QA task .
While the proposed System 5 overall performs better than the other three baseline systems , one exception is its accuracy on YN , which is lower than that of System 3 .
Since all systems '
YN accuracies are also lower than the majority baseline 8 , we hypothesize that this is due to imbalanced data .
To verify it , we compute the F 1 score for YN Q-TYPE in Table 3 , where we see all systems effectively achieve better scores than the majority baseline .
However , further pretraining BERT on SPARTQA - AUTO still does not beat other baseline systems , which implies that straightforward pretraining is not necessarily helpful in capturing the complex reasoning phenomena required by YN questions .
The human performance is evaluated on 100 ran- dom questions from each SPARTQA - AUTO and SPARTQA - HUMAN test set .
The respondents are graduate students that were trained by some examples of the dataset before answering the final questions .
We can see from Table 2 that all systems ' performances fall behind human performance by a large margin .
We expand on the difficulty of SPARTQA in the next subsection .
SPARTQA is challenging
In addition to BERT , we continue to test another two LMs , ALBERT and XLNet ( Table 5 ) .
We further pretrain these LMs on SPARTQA - AUTO , and test them on SPARTQA - HUMAN ( the numbers of BERT are copied from Table 2 ) and two held - out test sets of SPARTQA - AUTO , Seen and Unseen .
Note that when a system is tested against SPARTQA - HUMAN , it is fine-tuned on SPARTQA - HUMAN 's training data following its further pretraining on SPARTQA - AUTO .
We use the unseen set to test to what extent the baseline models use shortcuts in the language surface .
This set applies minor modifications randomly on a number of stories and questions to change the names of shapes , colors , sizes , and relationships in the vocabulary of the stories , which do not influence the reasoning steps ( more details in Appendix C.1 ) .
All models perform worst in YN across all Q-TYPEs , which suggests that YN presents a more complex phenomena , probably due to additional quantifiers in the questions .
XLNet performs the best on all Q-TYPEs except its accuracy on SPARTQA - HUMAN 's YN section .
However , the drops in Unseen and human suggest overfitting on the training vocabulary .
The low accuracies on human test set from all models show that solving this benchmark is still a challenging problem and requires more sophisticated methods like considering spatial roles and relations extraction ( Kordjamshidi et al. , 2010 ; Rahgooy et al. , 2018 ) to understand stories and questions better .
To evaluate the reliability of the models , we also provide two extra consistency and contrast test sets .
Consistency set is made by changing a part of the question in a way that seeks for the same information ( Hudson and Manning , 2019 ; .
Given a pivot question and answer of a specific consistency set , answering other questions in the set does not need extra reasoning over the story .
Contrast set is made by minimal modification in a question to change its answer .
For contrast sets , there is a need to go back to the story to find the new answer for the question 's minor variations ( see Appendix C.2 for examples . )
The consistency and contrast sets are evaluated only on the correctly predicted questions to check if the actual understanding and reasoning occurs .
This ensures the reliability of the models .
Table 5 shows the result of this evaluation on four Q-TYPEs of SPARTQA - AUTO , where we can see , for another time , that the high scores on the Seen test set are likely due to overfitting on training data rather than correct detection of spatial terms and reasoning over them .
Extrinsic evaluation
In this subsection , we take BERT as an example to show , once pretrained on SPARTQA - AUTO , BERT can achieve better performance on two extrinsic evaluation datasets , namely bAbI and boolQ .
We draw the learning curve on bAbI , using the original BERT as a baseline and BERT further pretrained on SPARTQA - AUTO ( Fig. 6 ) .
Although both systems achieve perfect accuracy given large enough training data ( i.e. , 5 k and 10k ) , BERT ( SPARTQA - AUTO ) is showing better scores given less training data .
Specifically , to achieve an accuracy of 99 % 71.9 BERT ( SPARTQA - AUTO )
74.2 Table 6 : System performances on the dev set of boolQ ( since the test set is not available to us ) .
Top : numbers reported in ( Clark et al. , 2019 ) . Bottom : numbers from our experiments .
BERT ( SPARTQA - AUTO ) : further pretraining BERT on SPARTQA - AUTO as a QA task .
1 k training examples , while BERT requires twice as much .
We also notice that BERT ( SPARTQA - AUTO ) converges faster in our experiments .
As another evaluation dataset , we chose boolQ for two reasons .
First , we needed a QA dataset with Yes / No questions .
To our knowledge boolQ is the only available one used in the recent work .
Second , indeed , SPARTQA and boolQ are from different domains , however , boolQ needs multi-step reasoning in which we wanted to see if SPARTQA helps .
Table 6 shows that further pretraining BERT on SPARTQA - AUTO yields a better result than the original BERT and those reported numbers in Clark et al . ( 2019 ) , which also tested on various distant supervision signals such as SQuAD ( Rajpurkar et al. , 2016 ) , Google 's Natural Question dataset NQ , and QNLI from GLUE ( Wang et al. , 2018 ) .
We observe that many of the boolQ examples answered correctly by the BERT further pretrained on SPARTQA - AUTO require multi-step reasoning .
Our hypothesis is that since solving SPARTQA - AUTO questions needs multi-step reasoning , finetuning BERT on SPARTQA - AUTO generally improves this capability of the base model .
Conclusion Spatial reasoning is an important problem in natural language understanding .
We propose the first human-created QA benchmark on spatial reasoning , and experiments show that state - of - the - art pretrained language models ( LM ) do not have the capability to solve this task given limited training data , while humans can solve those spatial reasoning questions reliably .
To improve LMs ' capability on this task , we propose to use hand -crafted grammar and spatial reasoning rules to automatically generate a large corpus of spatial descriptions and corresponding question - answer annotations ; further pretraining LMs on this distant supervision dataset significantly enhances their spatial language understanding and reasoning .
We also show that a spatially - improved LM can have better results on two extrinsic datasets ( bAbI and boolQ ) .
