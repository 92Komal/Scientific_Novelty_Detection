title
WikiSum : Coherent Summarization Dataset for Efficient Human-Evaluation
abstract
Recent works have made significant advances on summarization tasks , facilitated by summarization datasets .
Several existing datasets have the form of coherent - paragraph summaries .
However , these datasets were curated from academic documents written for experts , making the essential step of assessing the summarization output through human-evaluation very demanding .
To overcome these limitations , we present a dataset 1 based on article summaries appearing on the WikiHow website , composed of howto articles and coherent - paragraph summaries written in plain language .
We compare our dataset attributes to existing ones , including readability and world -knowledge , showing our dataset makes human evaluation significantly more manageable and effective .
A human evaluation conducted on PubMed and the proposed dataset reinforces our findings .
Introduction Summarization is the task of preserving the key information in a text while reducing its length .
Recently , many summarization datasets were published and helped push the boundaries of new summarization systems .
These datasets differ on several properties , including the domain ( e.g. , academic or news ) and the summary form .
PubMed , arXiv , and BigPatent ( Cohan et al. , 2018 ; Sharma et al. , 2019 ) provide a summary in the form of coherent paragraphs ( i.e. , each sentence flows smoothly into the next ) .
In contrast , other summarization datasets ( Hermann et al. , 2015 ; Grusky et al. , 2018 ; Koupaee and Wang , 2018 ; Ladhak et al. , 2020 ) offer a summary in the form of a key points list ( i.e. , highlights ) .
In this paper , we focus on coherent paragraph summarization datasets .
Automatic evaluation of summarization systems , e.g. , by using the ROUGE metric , is challenging ( Lloret et al. , 2018 ) and is often inconsistent with human evaluation ( Liu and Liu , 2008 ; Cohan and Goharian , 2016 ; Tay et al. , 2019 ; Huang et al. , 2020 ) .
To understand - and later improve - the quality of summarization systems , it is necessary to conduct a human evaluation .
A human evaluation 's quality depends on the ease of reading and understanding of the measured text : a simple text does not require annotators with unique expertise , can be evaluated faster , and is easier to annotate correctly .
However , existing coherent - paragraph summarization datasets consist of academic papers and cannot be considered easy to read .
Evaluating such summarization samples requires unique expertise , takes time , and comes at a high cost .
In this work , we present WikiSum , a new summarization dataset from the WikiHow knowledge base 2 .
The WikiSum documents are written in simple English , and the summaries provide " nonobvious tips that mimic the advice a knowledgeable , empathetic friend might give . "
3 Unlike previous WikiHow summarization ( Koupaee and Wang , 2018 ; Ladhak et al. , 2020 ) from the news domain , the summaries of WikiSum are in the form of a coherent paragraph written by the document authors ( examples in Figure 1 ) .
Moreover , in contrast to other coherent - paragraph summarization datasets from the academic domain , WikiSum is written using simple English .
This critical property can help with the challenging task of evaluating summarization systems and provide insights that can go unnoticed using automatic evaluation methods .
The key attributes of WikiSum are : ( 1 ) Summaries written as a single , coherent passage .
( 2 ) Articles and summaries that are easy to read .
( 3 ) Articles and summaries require less world knowledge to understand .
We evaluate the dataset readability and estimate the required world -knowledge in Section 3 .
Moreover , we reinforce our results by conducting a human-evaluation of a summarization dataset in Section 4 .
Finally , to establish a baseline on the proposed dataset , we benchmark WikiSum using recent summarization systems and report their performance on Section 5 .
Related Work
The summarization landscape can be roughly divided into three primary summary - forms : ( 1 ) Single sentence ( Napoles et al. , 2012 ; Grusky et al. , 2018 ; Narayan et al. , 2018 ; Kim et al. , 2019 ) - summarize the document in a single sentence ; ( 2 ) Highlights ( Hermann et al. , 2015 ; Koupaee and Wang , 2018 ; Ladhak et al. , 2020 ) - a summary in the form of bullets listing the key points in the text ; ( 3 ) Coherent summary ( Sharma et al. , 2019 ; Cohan et al. , 2018 ) - short coherent paragraphs describing the salient information .
The summarization datasets from the news domain , which are commonly used for human evaluation , include summaries in the form of highlights or single-sentence summaries .
However , summarization datasets written in a co-herent format come from the academic domain , making them extremely difficult to annotate manually .
Our proposed WikiSum is the only dataset written in a coherent format , yet easy for human evaluation .
We do not claim that coherent paragraph summaries are better , but rather different ; each format has its use cases , and human evaluation should be done on each of the different formats separately .
The existing WikiHow datasets ( Koupaee and Wang , 2018 ; Ladhak et al. , 2020 ) can be considered the closest to WikiSum , as they originate from the same knowledge base .
However , while the existing WikiHow datasets split the article to generate the document and summary , WikiSum uses the entire article as the document and a summary specifically written by the article 's author ( called the Article Quick Summary ) .
The former uses the concatenation of the first line of each step , called the step header , as the list of highlights and the remainder of step text 's concatenation called " wraptext , " as the document 4 .
In addition to the different summary - form of the highlight - based WikiHow and WikiSum , the content of the summaries is significantly different , which can be illustrated by the low BLEU - 4 ( 0.06 5 ) between the two .
BigPatent ( Sharma et al. , 2019 ) , Arxiv and PubMed ( Cohan et al. , 2018 ) are recent summarization datasets with coherent paragraph summaries .
These datasets focus on the academic domain and are written for experts .
Like these datasets , Wik- iSum is composed of long documents and coherent paragraph summaries .
Nonetheless , it uses common everyday language and ranges over many domains ( see Figure 2 ) .
Finally , Table 1 compares WikiSum to common existing datasets .
Additional details on WikiSum are available in the appendix .
Measuring Text Difficulty
This section focuses on two crucial attributes : ease of readability and external knowledge required , shown ( in Section 4 ) to be important for easy and effective human evaluation .
For brevity , we focus on summarization datasets with coherent - paragraph summaries .
Readability Readability metrics attempt to indicate how difficult a passage in English is to read .
We used classical readability measures , including FKGL ( Farr et al. , 1951 ) , GFI ( Robert , 1968 ) , SMOG ( Mc Laughlin , 1969 ) , ARI ( Senter and Smith , 1967 ) , CLI ( Coleman and Liau , 1975 ) .
All these metrics are based on lexical features of the text , e.g. , number of words in a sentence or mean number of syllables per word .
They produce a score that is interpreted as the number of years of formal education required ( for a native English speaker ) to understand a piece of text 6 .
For each document , we measured readability scores 7 for the document and the ground truth summary .
The document is longer than the summary , so its readability is of higher importance .
We report the average readability score for all the samples in the dataset .
Readability scores for the documents are presented at the top of Table 2 .
The table shows that WikiSum is significantly easier to read than other documents from coherent - summary datasets ( arXiv , PubMed , BigPatent ) .
Similar results can be found for the readability scores for the summaries ( bottom of Table 2 ) .
To conclude , WikiSum is measured as drastically simpler to read than other coherentsummary datasets .
External Knowledge Existing datasets are composed of academic documents that are written for experts .
Often , to fully understand academic texts requires domain knowledge , which makes the annotator pool smaller , and 1 0 ,0 0 0 2 0 ,0 0 0 3 0 ,0 0 0 4 0 ,0 0 0 5 0 ,0 0 0 6 0 ,0 0 0 7 0 ,0 0 0 OpenSubtitles top-k 0 % 10 % 20 % 30 % 40 % 50 % Uncommon words ratio arxiv pubmed bigpatent wikisum thus , in most cases , more expensive .
Word frequency is a strong indicator of how familiar a word is ( Paetzold and Specia , 2016 ) , where rare words tend to be less familiar .
We used OpenSubtitles ( Lison and Tiedemann , 2016 ) , text corpora compiled from an extensive database of movie and TV subtitles to obtain word frequencies .
We hypothesize that movie and TV subtitles can roughly represent common knowledge among many people .
In Figure 3 , we show the percentage of non-frequent words in a document ( i.e. , words that cannot be found in the top-k words in OpenSubtitles ) as a function K , averaged over a random sample of 10 , 000 documents from each dataset .
This figure clearly shows that WikiSum is composed of significantly fewer words unpopular in TV shows and movies , requiring less specialized external knowledge .
Human Evaluation
We conducted a standard human evaluation on a summarization task , in addition to the automatic readability and the external knowledge metrics .
We gathered a pool of 6 annotators , without any prior knowledge of the project , all with a graduate degree ( M.sc. or Ph.D. ) and proficient English readinglevel .
We asked them to evaluate summaries generated by Pegasus ( Zhang et al. , 2020 )
Table 3 : Evaluation time per sample , evaluation difficulty / exhaustion rating , perceived qualification , and the ratio of unknown words in the document .
? denotes 95 % confidence interval according to student 's t distribution ( df=20 ) .
Difficulty , qualification , and tiring were marked on a 1 - 5 scale .
notation task followed Huang et al . ( 2020 ) and consisted of relevance , consistency , fluency , and coherency .
Due to resource limitations ( and the difficulty of annotating articles from the academic domain ) , we had to pick one coherent - paragraph dataset for comparison with WikiSum .
To avoid annotators ' domain bias , we selected articles from PubMed , which contains articles not in the area of expertise of any annotator , in addition to WikiSum .
We sampled random articles with 950 - 1050 words to avoid length bias , ensuring that article length is similar in both datasets .
All annotators allocated 1 hour , which amounted to 42 annotations , 21 for each dataset .
During the annotation task , we measured the evaluation time and asked the annotators to mark unfamiliar words .
In addition , we asked the annotators to rate the following aspects on a 1 - 5 scale : ( a) How difficult was the task ?
( b) How tiring was it ?
( c ) How qualified are you for this task ?
After each pair of PubMed and WikiHow samples were completed , the annotators selected which dataset they prefer to evaluate .
In Table 3 we show the annotators ' assessment of the tasks .
Compared to PubMed , a WikiSum annotation takes significantly less time , is less difficult , and less tiring .
Moreover , the annotators revealed that they were much more qualified to assess the WikiSum task summary .
Finally , in 90 % of the cases ( 19 out of 21 ) , the annotators revealed that they preferred a WikiSum annotation task .
This reinforces our findings that WikiSum is significantly easier to annotate than PubMed .
In the annotation task , we also asked the annotators to mark unfamiliar words in the article .
We found a strong correlation between the count of unfamiliar words and the task difficulty , evaluation time , and perceived required qualification ( Pearson correlation of 0.57 , 0.36 , ?0.48 8 , respectively , p < 0.05 ) .
Strong correlation was also found between the ARI readability metric ( Section 3.1 ) and the above-mentioned annotation metrics ( Pearson correlation of 0.69 , 0.49 , ?0.76 , p < 0.05 ) .
This demonstrates the effect of readability on the difficulty of an annotation task .
Finally , we found that unfamiliar words correspond to low-frequency OpenSubtitles words ( Section 3.2 ) .
The unfamiliar words on WikiSum and PubMed appear in the top 91 , 550 and 230 , 596 words on average , respectively , while familiar words appear in the top 16 , 935 and 59 , 244 words on average , respectively .
It also further validates Paetzold and Specia ( 2016 ) hypothesis about the strong correlation between word frequency and complexity .
Model Results and Discussion
To provide both abstractive and extractive baselines for WikiSum , we evaluate on PEGASUS LARGE ( Zhang et al. , 2020 ) , Tex -tRank ( Mihalcea and Tarau , 2004 ) , and the common LEAD - 3 that selects the first three sentences of the document as the summary .
We compare the results on WikiSum to the Arxiv , PubMed , and BigPatent Datasets results .
Table 4 reports the F1 scores of ROUGE -1 , 2 and L for all the models .
The results show that the models ' performance on WikiSum is not drastically different from the other datasets , making it an interesting dataset for benchmarking summarization systems .
The detailed evaluation setup can be found in the supplementary materials .
To conclude , this paper presents the WikiSum dataset , which is drastically simpler for human evaluation than existing summarization datasets where the summary appears as a coherent paragraph .
We showed WikiSum 's simplicity via various readability metrics and demonstrated that the text requires less external knowledge to be understood .
Finally , we validated our finding via a human evaluation task on WikiSum and PubMed .
themselves as less unqualified .
We remark that the quick summaries are indeed used by commercial voice assistants to answer howto questions .
As voice assistants gain popularity , so does the importance of such coherent - paragraph summaries .
A.3 Data Layout Raw data is available in the supplementary material , in a json format .
Each line consists of a single sample , with the following fields 1 . Link to the original article 2 . Article title 3 . Article text 4 . Quick summary 5 . Split fold ( train , dev , or test )
Finally , it also includes step headers : the first line in each step .
This is part of the article but might be considered more important , and therefore , it might find further uses by system designers .
A.4 Dataset Statistics
Most dataset statistics appear in Table 1 in the article 's main body and are repeated here for completeness .
The total number of samples in the WikiSum dataset is 39 , 775 .
On average , each summary consists of 101.2 words , while each article consists of 1 , 334.2 words .
The average compression ratio is 13.9 .
A.5 Evaluation details
We randomly split WikiSum into 35,775 ( document , summary ) training pairs , as well as 2,000 validation pairs and 2,000 test pairs .
The rest of the datasets were downloaded from the HuggingFace dataset repository 12 .
All the datasets were evaluated using TextRank 13 and Pegasus -large .
The ROUGE scores throughout the paper were calculated using rouge-score 14 .
We utilized TextRank to generate three summary sentences .
The Pegasus results on Arxiv , Pubmed , and Arxiv were taken from the Pegasus paper .
The results on WikiSum were computed by using the Github repository of the Pegasus paper 15 . Pegasus was trained on a single NVIDIA V100 Tensor Core such coatings can be applied to any implantable medical devices and are useful for a number of medical procedures including balloon angioplasty in cardiovascular stenting , ureteral stenting and catheterisation .
the calcium phosphate coatings can be applied to a substrate as one or more coatings by a sol - gel deposition process , an aerosol - gel deposition process , a biomimetic deposition process , a calcium phosphate cement deposition process , an electro - phoretic deposition process or an electrochemical deposition process .
the coating can contain and elude a drug in an engineered manner . "
The article is available at https : //patentscope.wipo.int/search/en/detail .
jsf?docId=WO2007147234 .
Figure 1 : 1 Figure 1 : Examples of how -to questions and their corresponding answer 's summarization in WikiSum .
Figure 3 : 3 Figure 3 : Ratio of uncommon words in the document , which cannot be found in the Top-K OpenSubtitles words , for different k values .
datasets and summaries 5000 # documents 1000 2000 3000 4000 0 Health Home Pets Food Education Personal Care Hobbies Finance uncategorized Arts Relationships Sports Youth Cars Computers Work World Family Life Philosophy Travel Holidays Figure 2 : Category distribution in WikiSum .
Table 2 : 2 Readability scores for the documents ( top ) and summaries ( bottom ) , measured in years of formal education required to read the text .
Smaller is simpler .
Dataset ARI FKGL GFI SMOG CLI Document WikiSum 7.4 arXiv 14.02 13.51 18.47 15.44 14.31 6.82 10.15 9.71 8.83 PubMed 16.74 16.27 20.64 17.03 15.01 BigPatent 13.46 13.32 17.47 14.68 11.68 Summary WikiSum 9.71 8.49 11.91 10.24 8.78 arXiv 16.44 16.1 20.5 16.8 15.23 PubMed 17.73 17.35 21.6 17.44 16.6 BigPatent 22.47 20.91 25.12 18.75 14.0
. The an- dataset time ( minutes ) difficulty ( rating ) exhausting ( rating ) qualified ( rating ) unknown ( % ) WikiSum 6.8?1.2 1.9?0.3 2.2?0.5 4.2?0.3 0.2?0.1 PubMed 10.0?1.2 3.7?0.3 3.9?0.4 2.2?0.4 3.7?1.4
https://www.wikihow.com 3 https://www.wikihow.com/Write-or-Edit-a-Quick-Summary-on-wikiHow
WikiHow author instructions ( wikihow.com , 2020 ) specifically states that the authors can use the wrap -text to describe why the step header is important .
This leads to many cases where the step headers are not a summary of the wrap- text .5
We used WikiSum as the reference , the results are very similar when WikiHow is used as a reference .
ROUGE -1 , 2 and L are 0.37 , 0.13 , and 0.23 , respectively .
Other readability metrics such as FRE ( Flesch , 1948 ) , LIX and RIX ( Bj?rnsson , 1968 ) , have a similar trend to the shown metrics , but require a translation to years of education , omitted from this paper for brevity .
7
https://github.com/mmautner/readability
www.scrapy.org 10 https://pypi.org/project/beautifulsoup4 11 https://www.wikihow.com/Write-or-Edit-a-Quick-Summary-on-wikiHow
https://huggingface.co/datasets 13 https://pypi.org/project/summa 14 https://pypi.org/project/rouge-score/ 15 https://github.com/google-research/pegasus
