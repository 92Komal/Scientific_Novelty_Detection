title
Video Paragraph Captioning as a Text Summarization Task
abstract
Video paragraph captioning aims to generate a set of coherent sentences to describe a video that contains several events .
Most previous methods simplify this task by using groundtruth event segments .
In this work , we propose a novel framework by taking this task as a text summarization task .
We first generate lots of sentence - level captions focusing on different video clips and then summarize these captions to obtain the final paragraph caption .
Our method does not depend on ground - truth event segments .
Experiments on two popular datasets ActivityNet Captions and YouCookII demonstrate the advantages of our new framework .
On the ActivityNet dataset , our method even outperforms some previous methods using ground - truth event segment labels .
Introduction Video captioning , the task of describing the content of a video in natural language , is a popular task both in computer vision and natural language processing .
In the beginning , researchers try to generate sentence - level captions for short video clips ( Venugopalan et al. , 2015 ) . Krishna et al. ( 2017 ) propose the task of dense video captioning .
The system needs to detect event segments first and then generate captions .
Park et al. ( 2019 ) propose the task of video paragraph captioning : they use ground - truth event segments and focus on generating coherent paragraphs .
Lei et al. ( 2020 ) follow the task setting and propose a recurrent transformer model that can generate more coherent and less repetitive paragraphs .
Considering the groundtruth event segments are often unavailable in practice , our goal is to generate paragraph captions without ground -truth segments .
The conventional framework of video paragraph captioning is shown in Figure 1a . Given an untrimmed video , an Event Detection module out - puts a set of non-redundant event segments .
The Event Captioning module generates captions for these segments .
The works of ( Park et al. , 2019 ; Lei et al. , 2020 ) use ground - truth event segments and focus on the Event Captioning module .
use extra humanannotated bounding boxes as supervision .
( Sah et al. , 2017 ; Zhou et al. , 2018 ; Mun et al. , 2019 ) use predicted event segments and generate captions based on them .
Sah et al. ( 2017 ) also summarizes these captions to generate a paragraph .
The above methods heavily depend on accurate event segments .
According to previous works ( Zhou et al. , 2018 ; Mun et al. , 2019 ) , the performance of the Event Detection module is not so good , making it a performance bottleneck .
To tackle this problem , we propose a novel framework VPCSum as shown in Figure 1 b .
For a given video , we first extract dense event segment candidates ( we call proposals ) , and a Proposal Captioning module is used to generate proposal captions .
Then we treat video paragraph captioning as a text summarization task to obtain the final summary ( paragraph caption ) .
In this work , we only consider extractive summarization , where the paragraph caption is composed by selecting from proposal captions .
We conduct experiments on two popular datasets ActivityNet Captions and YouCookII .
The results demonstrate the advantages of our framework .
On the Activ-ityNet Captions dataset , our method even outperforms some previous methods using ground - truth event segment labels .
Our VPCSum Method
As illustrated in Figure 1 b , our framework has three modules .
Proposal Extraction : it extracts dense proposals for a video ; Proposal Captioning : it generates captions for extracted proposals ; Caption Summarization : it summarizes the generated proposal captions to obtain the video paragraph caption .
We will introduce each module next .
Proposal Extraction
For proposal extraction , we use the BMN model ( Lin et al. , 2019 ) , a popular model for temporal action proposal generation .
It can extract complete and accurate proposals .
We extract the top 100 proposals for each video .
Proposal Captioning
For proposal captioning , we choose the TSRM - RNN model for ActivityNet Captions and VTransformer model ( Lei et al. , 2020 ) for YouCookII according to proposal captioning performance .
We believe that if we choose a better sentence - level captioning model , the performance can be further improved .
The caption summarization module summarizes proposal captions to generate the final video paragraph caption .
In this work , we focus on extractive summarization .
The architecture of our summarization model is illustrated in Figure 2 .
We first sort the proposal captions according to the proposal start time and add special [ CLS ] and [ SEP ] tokens to the beginning and end of each caption .
We use the summation of token embeddings , segment embeddings , and position embeddings to represent each word .
The input representations are fed into a pre-trained BERT model ( Devlin et al. , 2018 ) , after which we obtain the contextual token representations .
We use the contextual vectors of [ CLS ] s to represent each caption and feed them into stacked transformer layers ( Vaswani et al. , 2017 ) .
We use a sigmoid layer to compute the score of each caption :
Caption Summarization x i = ?( W h L i + b ) ( 1 ) where W and b are trainable parameters , h L i is the vector for caption i from the top transformer layer .
For extractive summarization , we need to annotate each sentence according to the gold summary as our training target .
Many researchers use a greedy algorithm ( Nallapati et al. , 2016 ) , sentences are selected one by one to maximize the ROUGE score against the gold summary .
The selected sentences are labeled 1 while others are labeled 0 ( hard- label ) .
In our task , we find a more effective soft-label annotation method .
We label caption c i with the max ROUGE score against gold captions and use binary cross-entropy as our loss function : y i = max g j ? gold ROU GE ( c i , g j ) ( 2 ) L = ?
i ( y i log x i + ( 1 ? y i ) log ( 1 ? x i ) ) ( 3 ) where g j is the j-th gold caption .
Leverage Visual Information
The above caption summarization module assigns each proposal caption a predicted score , indicating how likely it appears in the final paragraph caption .
The predicted score only depends on text information .
To leverage visual information , we need a " visual summarization " module , which gives a visually weighting score to each proposal .
The ESGN model ( Mun et al. , 2019 ) seems a good choice for us .
It uses a pointer network to select events from proposals and assigns a visually weighting score for each proposal .
We use this model to compute the visually weighting score .
Now we can extract the final paragraph caption .
The final score of each proposal caption is a weighted sum of the textually weighting score s txt and the visually weighting score s vis : score ( i ) = s txt , i + ?s vis , i ( 4 ) where ? is a hyper-parameter tuned on validation set .
We select captions according to score ( i ) and use Trigram Blocking to reduce redundancy , as in Liu and Lapata ( 2019 ) .
3 Experiments
Datasets
We conduct experiments on ActivityNet Captions ( Krishna et al. , 2017 ) and YouCookII ( Zhou et al. , 2017 ) .
ActivityNet Captions contains 10,009 videos in train set , 4,917 videos in val set .
Each video has 3.65 event segments on average .
Following ( Lei et al. , 2020 ) , the original val set is split into ae-val with 2,460 videos for validation and ae-test with 2,457 videos for test .
You CookII contains 1,333 videos in train set , 457 videos in val set .
Each video has 7.70 event segments on average .
Evaluation Metrics Following ( Lei et al. , 2020 ; Park et al. , 2019 ) , we evaluate the captioning performance at paragraph level .
We report standard caption metrics , including BLEU@4 ( Papineni et al. , 2002 ) , METEOR ( Denkowski and Lavie , 2014 ) , CIDEr ( Vedantam et al. , 2015 ) .
We also evaluate repetition using R@4 .
We use the scripts provided by ( Lei et al. , 2020 ) for evaluation 1 .
Implementation Details
For video preprocessing , we use appearance and optical flow features provided by Zhou et al . ( 2018 ) .
For BMN model and captioning models , we use the same hyperparameters suggested by the authors .
For ESGN model , we use a transformer encoder instead of an RNN encoder , with hidden size set to 512 , number of heads set to 8 , number of layers set to 3 .
For our caption summarization model , we use the base BERT model , 2 stacked transformer layers with hidden size set to 768 , number of heads set to 8 .
We set max input length to 1,700 , batch size to 10 , ? to 1 for ActivityNet Captions and max input length to 1,000 , batch size to 1 , ? to 1 for YouCookII .
Warmup steps are set to step num of 1 epoch .
We use Adam optimizer with an initial learning rate of 6e ? 4 . 1 https://github.com/jayleicn/ recurrent- transformer
Baselines and Results
We compare our VPCSum model with the following baselines .
Soft - NMS : it uses Soft-NMS ( Bodla et al. , 2017 ) to select event segments from BMN proposals , and uses the proposal captioning model to generate captions ; ESGN : similar to Soft - NMS , but it uses ESGN model ( Mun et al. , 2019 ) to select event segments from BMN proposals ; V- Trans : a Vanilla Transformer model , proposed by ( Zhou et al. , 2018 ) ; Trans- XL : a Transformer - XL model , proposed by ( Lei et al. , 2020 ) ; MART : a recurrent transformer model ( Lei et al. , 2020 ) ; COOT : it uses pretrained features to train MART model ( Ging et al. , 2020 ) .
Originally , the last four models deal with ground - truth event segments .
For fair comparison , we also test them with predicted event segments generated by ESGN model 2 . We report BLEU@4 ( B@4 ) , ME-TEOR ( M ) , CIDEr ( C ) , Repetition ( R@4 ) .
Models Tables 1 and 2 show the results on ActivityNet Captions and YouCookII .
We can observe that on the ActivityNet Captions , our model VPCSum within the new framework can generate better paragraph captions with higher Bleu@4 , METEOR , and CIDEr and lower repetition score R@4 , even outperforming V-trans * , Trans - XL * , MART * models using ground - truth event segments on every metric .
On the YouCookII dataset , our model outperforms the models in the same setting but is inferior to the models using ground - truth segments .
This may be because YouCookII has more segments ( 7.70 vs 3.65 ) than ActivityNet Captions .
Models B@4 M C R@4 ?
Ablation Study Table 3 shows the ablation study on ActivityNet Captions .
Compared to our full model ( Full ) , the traditional extractive summarization annotation method ( Hard- label ) is not suitable for our task .
If we set ? in Eq. ( 4 ) to 0 ( w/ o vis ) , the model loses useful visual information and performs not well .
If we remove Trigram Blocking ( w/ o tri-blk ) , the performance also degrades and repetition becomes a problem ( R@4 increases to 7.91 ) .
To verify the role of pretrained BERT model , we retrain our VPCSum without BERT pretrained weights ( w/ o pretrain ) .
We can see that BERT pretrained weights are not the major factor to the final performance .
We also replace our summarization model with unsupervised methods LexRank ( Erkan and Radev , 2004 ) and LSA ( Steinberger and Jezek , 2004 )
Qualitative Results Ground Truth : A girl jumps onto a balance beam .
She does a gymnastics routine on the balance beam .
She does a flip off the balance beam and lands on a mat .
MART : A gymnast is seen standing ready with her arms up and leads into her performing a gymnastics routine .
She continues performing several flips and tricks and tricks and ends with her jumping down and walking away .
She continues her routine and ends with her jumping down and jumping down and walking away .
MART * : A gymnast is seen standing before a beam and begins performing a gymnastics routine .
The girl then performs a routine on the beam and ends with her jumping down and jumping down and jumping .
The girl jumps off the beam and lands on the mat and jumps off the beam .
VPCSum : A gymnast is seen standing ready with her arms up and begins to do a routine .
She does a gymnastics routine on the beam .
She dismounts and lands on the mat .
We show an example in Figure 3 with paragraph captions generated by MART , MART * and our VPCSum model .
Compared to other models , our model can generate more clear and correct sentences with less redundancy .
The generated paragraph of our model can better describe the process of the whole event .
We also conduct a human evaluation on randomly sampled 50 videos from the ActivityNet Captions val set .
The annotators are asked to choose the better caption from two models in two aspects : relevance ( how related is the caption to the video content ) and diversity ( how diverse is the generated text ) .
We compare our VPCSum model with MART and MART * respectively .
We have 17 college students as our annotators .
Each video is judged by 3 annotators .
We show the results of the pairwise experiments in Table 4 .
Our VPCSum model performs better in relevance and diversity , and more people choose the caption of our model as the better one .
Human Evaluation
Ours
Conclusion
In this work , we view the task of video paragraph captioning as a text summarization task and propose a novel framework VPCSum .
It allows us to use text summarization techniques to handle this challenging task .
Experimental results on two popular datasets show the advantages of our model .
In the future , we will explore using abstractive summarization methods to generate better video paragraph captions .
Figure 1 : 1 Figure 1 : Comparison between conventional framework and ours .
