title
Considering Nested Tree Structure in Sentence Extractive Summarization with Pre-trained Transformer
abstract
Sentence extractive summarization shortens a document by selecting sentences for a summary while preserving its important contents .
However , constructing a coherent and informative summary is difficult using a pre-trained BERT - based encoder since it is not explicitly trained for representing the information of sentences in a document .
We propose a nested tree - based extractive summarization model on RoBERTa ( NeRoBERTa ) , where nested tree structures consist of syntactic and discourse trees in a given document .
Experimental results on the CNN / DailyMail dataset showed that NeRoBERTa outperforms baseline models in ROUGE .
Human evaluation results also showed that NeRoBERTa achieves significantly better scores than the baselines in terms of coherence and yields comparable scores to the state - of - the - art models .
Introduction Document summarization is a task of creating a concise summary from a given document while keeping the original content .
In general , sentence extraction methods , which select sentences in a document to create its summary , have the advantages of truthfulness compared with abstractive methods ( Cao et al. , 2018 ) and of fluency compared with word extraction methods ( Xu et al. , 2020 ) .
Neural networks have achieved great success in sentence extraction - based document summarization ( Cheng and Lapata , 2016 ; Zhou et al. , 2018 ) . Recently , Liu and Lapata ( 2019 ) proposed BERT - SUM , which utilizes BERT ( Devlin et al. , 2019 ) for sentence representations to create a summary .
Although the use of BERT resulted in significant performance improvement , this method decides the selection for each sentence independently .
Xu et al . ( 2020 ) proposed DISCOBERT by considering inter-sentence information through discourse graphs to construct a coherent summary .
Although they achieved remarkable scores in ROUGE , it was ? ? ! " ? ? !! ? !# ? ? # " ? ? #! ? ## ? # ? ? $ " ? ? $! ? $# ? # ? ! ? $ ? ? 0 1 0 ? Intra ? ! ? $ ( DiscoBERT ) Inter ( NeRoBERTa )
Nested ( 1 : select , 0 : delete ) Figure 1 : Different from the previous work , DIS - COBERT ( Xu et al. , 2020 ) , NeRoBERTa selects sentences by considering both intra-and inter-sentence relationships as a nested tree structure .
still difficult to construct a coherent summary compared to BERTSUM in human evaluation .
Zhong et al. ( 2020 ) attempted to change the paradigm by formulating summary - level extraction with a RoBERTa encoder and achieved the state - of- theart results on the CNN / DailyMail dataset .
In spite of the successful results of the above BERT - related methods , their sentence representations have room for improvement .
As reported , " [ CLS ] " , a pre-defined token for indicating sentence representations on BERT , is insufficient to express sentence information .
Even in RoBERTa , it is also a problem due to the lack of next sentence prediction in its pretraining step .
Therefore , for further improving summarization performance , we need to consider how to represent sentences in a BERT - related model and how to capture relationships between such sentence representations .
It is a key to create a coherent and informative summary with sentence extraction methods .
To tackle this problem , we propose a nested tree - based extractive summarization model on RoBERTa ( NeRoBERTa ) .
NeRoBERTa can extract coherent sentences for a summary of a given document by utilizing nested tree structures 1 of two different trees , syntactic and discourse dependency trees ( Zhao and Huang , 2017 ) .
Figure 1 shows the proposed NeRoBERTa to select sentences from a given document .
Different from the previous works that focused on inter-sentence information using discourse graphs ( Ishigaki et al. , 2019 ; Xu et al. , 2020 ) , NeRoBERTa considers both intra-and inter-sentence information ( syntactic and discourse graphs ) together as a nested tree .
The nested tree is encoded as a vector space representation through a graph attention network ( Veli?kovi ?
et al. , 2018 ) on a BERT - based encoder .
In this tree , we can explicitly represent sentence information at " root " words for each syntactic dependency tree without relying only on " [ CLS ] " tokens .
This representation is useful to extract informative and coherent sentences in that it can capture keywords in a sentence for considering textual coherence to other sentences .
Furthermore , based on the representation , we can also capture interactions between sentences through discourse dependency trees , succeeding in extracting coherent sentences .
It is also possible to consider even longdistance relationships as higher -order dependency relationships in this structure , such as relationships between children and their ancestors .
Thus , NeRoBERTa considers textual coherence through both syntactic and discourse trees to capture longdistance interactions between sentences .
Experimental results on the CNN / DailyMail dataset showed that our NeRoBERTa outperforms RoBERTa - based strong baselines in ROUGE .
Unlike the previous work ( Xu et al. , 2020 ) , NeR-oBERTa successfully constructs a coherent summary and is comparable to the state - of - the - art methods in human evaluation .
Nested Tree Structure
In this section , we describe how we construct two different types of graphs for a nested tree structure : a discourse graph and a syntactic graph .
We obtain discourse dependency relationships between sentences in a document through an RST parser .
A given document can be parsed into a tree format with the RST parser , where each leaf node is an EDU , a text span in the document .
Each text span has two types , nucleus and satellite .
While the nucleus spans contain semantically salient information , the satellite spans support and modify the nucleus ones .
sentences to construct a summary for a given document .
We use the recent state - of- the - art RST parser 2 ( Kobayashi et al. , 2020 ) to build an RST discourse tree ( RST - DT ) for all documents and convert it to an Inter-Sentential RST - DT ( ISRST - DT ) .
The ISRST - DT is first converted into a dependency - based discourse tree ( ISDEP - DT ) using the method described in ( Hirao et al. , 2013 ) .
Then , parent- child dependency relationships for each sentence can be formed .
We construct a directed graph for the discourse dependencies ( Ishigaki et al. , 2019 ) .
A dependency parser is used to build up the syntactic dependency relationships between words ( Manning et al. , 2014 ) .
We construct an undirected graph for the syntactic dependencies by following the previous settings ( Marcheggiani and Titov , 2017 ) .
Our Model Ishigaki et al. ( 2019 ) consider dependency information through hierarchical attention modules ( Kamigaito et al. , 2018 ) trained in supervised attention for dependency heads ( Kamigaito et al. , 2017 ) .
Unlike the previous work , our model uses constructed graph information through graph encoder layers that directly focus on the relationships between nodes defined by edges in the graph .
We explain the details of our model in this section .
Let w i be the i-th token in a document D = {w 1 , w 2 , ... , w n }.
Our model predicts p ( 1 | D , k ) , the probability of the k-th sentence in D being kept in a summary through the following modules .
Pre-trained Document Encoder
We append " [ CLS ] " and " [ SEP ] " tokens between sentences to encode a whole document ( Liu and Lapata , 2019 ) .
Then , BERT is used to build up a representation h i for each token w i as follows : {h 1 , h 2 , ... , h n } = BERT ( {w 1 , w 2 , ... , w n } ) .
Instead of BERT , we consider RoBERTa as well .
However , RoBERTa cannot be directly used in place of BERT for sentence - level extraction because RoBERTa does not consider the two types of tokens for the segment boundaries .
To address this issue , we use randomly initialized segment embeddings , W type ?
R 2,768 , instead of the original embeddings for keeping the same condition as BERT .
The number comes from the pre-trained segment embedding weights of the original BERT , which indicate the next sentence prediction step .
Then , the encoded hidden states , {h 1 , h 2 , ... , h n } , are fed into our graph encoders .
Graph Encoders Graph Notation : Let V d and V s be nodes for sentences and words , and E s and E d be edges between the nodes in V s and V d , respectively .
We denote constructed discourse and syntactic graphs as G d = ( V d , E d ) and G s = ( V s , E s ) , respectively .
We append undirected edges between " [ CLS ] " and " root " tokens in each sentence to E s because the parent of a " root " token would be a sentence representation .
GAT Networks :
We use Graph Attention Networks ( GAT ) ( Veli?kovi ?
et al. , 2018 ) to encode each graph G on hidden states of BERT as follows : f i = F 2 ( h i ) , h i ?
R d , n , ( 1 ) n i = N( drop ( f i ) + h i ) , ( 2 ) ? i , j = Softmax j ( L( F 1 [ W n n nl W n n nl ] ) ) , ( 3 ) h i = K k=1 T ( j?N i ? k i , j W k a h j ) , h i ?
R K?d , n , ( 4 ) h i = ReLU ( M( h i ) ) , h i ?
R d , n , ( 5 ) h G i = N( drop ( h i ) + n i ) , ( 6 ) where F i indicates i-th times stacked feed -forward networks .
N is layer normalization .
W n and W a are learnable weights .
L and T denote a non-linearity activation function , LeakyReLU , and a hyperbolic tangent , respectively .
?
i , j indicates normalized attention coefficients through a softmax function .
indicates concatenation , and n i represents connected nodes to node i in graph G. ReLU is an activation function .
M is a learnable weight .
After h i is fed into the graph encoder , we obtain h G i , which contains either syntactic or discourse graph information based on all tokens .
The syntactic and discourse graphs are independently encoded .
Then , they are concatenated as h root k = ReLU ( W ( h Gs r( k ) h G d r( k ) ) ) , where r( k ) indicates the position of a root in the k-th sentence .
For the final representations to predict labels , we use h root k to represent the k-th sentence .
Objective Function & Inference
We define p( 1|D , k ) = ?( W M ( h root k ) + b ) , where M is a two-stacked multi-head attention , ? is a sigmoid function , and W and b are weight parameters ( Liu and Lapata , 2019 ) .
Let y i ? { 1 , 0 } be an oracle label and Y = {y 1 , y 2 , ... , y n } be its set for a document .
We use ?
y k ?Y log(y k |x , k ) as our objective function .
In the inference step , we score the k-th sentence with p ( 1 | D , k ) and sort the sentences in descending order .
Then , we keep the top m sentences as a summary , where m is the number of sentences to be extracted .
Experiments
Experimental Settings Dataset :
We used the non-anonymized CNN / DailyMail dataset ( Hermann et al. , 2015 ) .
Based on the standard split , we divided the dataset into 287,226 , 13,368 and 11,490 articles for training , validation , and test datasets , respectively .
Parameter Settings :
We used PyTorch with the Torch Geometric ( Fey and Lenssen , 2019 ) to build up entire architectures with graph encoders .
The " bert- based - uncased " and " roberta- based " models in transformers 3 were used to encode maximum 768 tokens of each tokenized document .
The best model was selected based on the lowest " loss " score on the validation dataset .
A greedy search was used to construct the oracle summary by maximizing the sum of ROUGE -1 - F and ROUGE - 2 - F against the gold summary .
For the syntactic graph encoder , we stacked GAT Networks .
To track n-order dependency information , we simply added n-order nodes and edges to G d and G s .
The number of attention heads was set to 6 in each graph encoder .
To represent each word vector , we used a first sub-word vector .
We employed a traditional method of selecting top 3 sentences to construct a summary ( Liu and Lapata , 2019 ) .
Trigram blocking was used to reduce redundancy and to improve informativeness for all models ( Paulus et al. , 2018 ) . Compared Methods :
We compared our proposed methods with some baselines .
The proposed methods are as follows : NeRoBERTa considers our nested tree structure for both syntactic and discourse information .
SynRoBERTa and DiRoBERTa independently consider only either syntactic or discourse tree structure , respectively .
The baselines , which include state - of - the - art models , are as follows : BERTSUM introduces a method for learning a sentence boundary in a BERT - based model for the document summarization task ( Liu and Lapata , 2019 ) .
DISCOBERT constructs a summary based on EDU - level extraction , incorporating discourse and coreference information ( Xu et al. , 2020 ) .
MatchSum attempts to shift the paradigm from sentence - level to summary - level extraction during the extractive document summarization task ( Zhong et al. , 2020 ) .
RoBERTa encodes input documents using a " roberta- based " model .
Automatic Evaluation
We utilized ROUGE -metrics for the evaluation .
The experimental results on the CNN / DailyMail dataset are shown in Table 1 .
The first block contains Lead - 3 and Oracle scores .
The second block includes BERT - based previous studies including state- ofthe - art models .
The last block includes scores for our models and for re-implemented BERTSUM .
Our strong baseline RoBERTa outperformed BERTSUM .
The gain might be from using a bigger dataset with the dynamic masking pattern applied in the pre-trained RoBERTa .
SynRoBERTa and DiRoBERTa show that considering syntactic or discourse information was beneficial .
NeRoBERTa ( n s = { 1 , 2 } , n d = { 1 } ) ( in bold ) , that considers syntactic and discourse information simultaneously , further improved the performance .
It outperformed RoBERTa with a clear margin , specifically , 0.31 points in the R-1 - F score .
As can be seen in Figure 2 , RoBERTa can improve the prediction loss compared with BERT - SUM .
SynRoBERTa ( n s = { 1 , 2 } ) , which explicitly incorporates keywords information through syntactic information , can further improve the performance of RoBERTa .
This shows that considering keywords information through syntactic structures is beneficial to construct the sentence representations for considering textual coherence to other sentences .
Human Evaluation and Analysis Human evaluation was conducted for randomly sampled 100 documents from the test dataset .
" Amazon Mturk " was used for the experiments , and human evaluators graded scores from 1 to 5 ( 5 is the best ) in terms of four evaluation criteria .
5 Because summaries from DISCOBERT were worse than ones from BERTSUM in their human evaluation ( Xu et al. , 2020 ) , we evaluated only summaries from RoBERTa , NeRoBERTa ( n s = { 1 , 2 } , n d = { 1 } ) , and MatchSum .
Table 2 shows the results .
Coh , Infor , Read , and Redun indicate coherence , informativeness , readability , and redundancy , respectively .
As we expected , the proposed NeRoBERTa , which considers a nested tree structure , could capture coherence better than our strong baseline , RoBERTa .
In addition , NeRoBERTa was comparable to the current state - of- the - art model , MatchSum .
The informativeness score for Match - Sum was lower than RoBERTa and NeRoBERTa .
Model R-1-F R-2-F R-L-F Lead3
Table 3 shows example extracted sentences from a document and their discourse graph .
In this example , the discourse information alone was not enough in that S3 and S10 have the same discourse information , while S3 is more similar to the third sentence in the gold summary .
RoBERTa and DiRoBERTa constructed the same summary in - ( n s = { 1 , 2 } ) 43.63 20.51 40.02 DiRoBERTa ( n d = { 1 } ) 43.64 20.45 40.02 NeRoBERTa ( n s = { 1 } , n d = { 1 } ) 43.74 20.53 40.13 NeRoBERTa ( n s = { 1 , 2 } , n d = { 1 } ) 43.86 ? 20.64 ? 40.20 ? Table 1 : Experimental results on the CNN / DailyMail dataset .
n s and n d indicate the order of dependency relationships considered for syntactic and discourse graphs , respectively .
? indicates the improvement is significant with a 0.95 confidence interval estimated with the ROUGE script compared to RoBERTa .
Figure 2 : 2 Figure 2 : Validation losses for BERTSUM , RoBERTa , and SynRoBERTa ( n s = { 1 , 2 } ) . " [ CLS ] " and " [ ROOT ] " indicate the tokens of sentence representations for predicting labels .
Kikuchi et al. ( 2014 ) considered the nested tree structure in the traditional non-neural tree-trimming method .
Their method extracted words by tracking their parent words and
We used the RST - parser using the RoBERTa embeddings
https://github.com/huggingface/transformers
The paired - bootstrap- resampling ( Koehn , 2004 ) was used ( p < 0.05 ) .5 40 human evaluators who obtained both US high school and US bachelor degrees participated in the experiments .
