title
Dialogue State Tracking with a Language Model using Schema-Driven Prompting
abstract
Task - oriented conversational systems often use dialogue state tracking to represent the user 's intentions , which involves filling in values of pre-defined slots .
Many approaches have been proposed , often using task -specific architectures with special - purpose classifiers .
Recently , good results have been obtained using more general architectures based on pretrained language models .
Here , we introduce a new variation of the language modeling approach that uses schema-driven prompting to provide task - aware history encoding that is used for both categorical and non-categorical slots .
We further improve performance by augmenting the prompting with schema descriptions , a naturally occurring source of indomain knowledge .
Our purely generative system achieves state - of- the - art performance on MultiWOZ 2.2 and achieves competitive performance on two other benchmarks : Multi-WOZ 2.1 and M2M .
The data and code will be available at https://github.com/ chiahsuan156/DST -as-Prompting .
Introduction
In task - oriented dialogues , systems communicate with users through natural language to accomplish a wide range of tasks , such as food ordering , tech support , restaurant / hotel / travel booking , etc .
The backbone module of a typical system is dialogue state tracking ( DST ) , where the user goal is inferred from the dialogue history ( Henderson et al. , 2014 ; Shah et al. , 2018 ; Budzianowski et al. , 2018 ) .
User goals are represented in terms of values of pre-defined slots associated with a schema determined by the information needed to execute task -specific queries to the backend .
In other words , user goals are extracted progressively via slot filling based on the schema throughout the conversation .
In this paper , we focus on multi-domain DST where the dialogue state is encoded as a list of triplets in the form of ( domain , slot , value ) , e.g. ( " restaurant " , " area " , " centre " ) .
There are two broad paradigms of DST models , classification - based and generation - based models , where the major difference is how the slot value is inferred .
In classification - based models ( Ye et al. , 2021 ; , the prediction of a slot value is restricted to a fixed set for each slot , and non-categorical slots are constrained to values observed in the training data .
In contrast , generationbased models decode slot values sequentially ( token by token ) based on the dialogue context , with the potential of recovering unseen values .
Recently , generationbased DST built on large-scale pretrained neural language models ( LM ) achieve strong results without relying on domain-specific modules .
Among them , the autoregressive model ( Peng et al. , 2020a ; Hosseini - Asl et al. , 2020 ) uses a uni-directional encoder whereas the sequence - to- sequence model ( Lin et al. , 2020a ; Heck et al. , 2020 ) represents the dialogue context using a bi-directional encoder .
In this study , we follow a generation - based DST approach using a pre-trained sequence - to- sequence model , but with the new strategy of adding taskspecific prompts as input for sequence - to-sequence DST models , inspired by prompt- based fine-tuning ( Radford et al. , 2019 ; Brown et al. , 2020a ) .
Specifically , instead of generating domain and slot symbols in the decoder , we concatenate the dialogue context with domain and slot prompts as input to the encoder , where prompts are taken directly from the schema .
We hypothesize that jointly encoding dialogue context and schema-specific textual information can further benefit a sequence - to-sequence DST model .
This allows task - aware contextualization for more effectively guiding the decoder to generate slot values .
Although the domain and slot names typically have interpretable components , they often do not reflect standard written English , e.g. " arriveby " and " ref " .
Those custom meaning representations are typically abbreviated and / or under-specified , which creates a barrier for effectively utilizing the pretrained LMs .
To address this issue , we further incorporate natural language schema descriptions into prompting for DST , which include useful information to guide the decoder .
For example , the description of " ref " is " reference number of the hotel booking " ; the values of " has_internet " are " yes " , " no " , " free " , and " do n't care " .
In short , this work advances generation - based DST in two ways .
First , candidate schema labels are jointly encoded with the dialogue context , providing a task - aware contextualization for initializing the decoder .
Second , natural language descriptions of schema categories associated with database documentation are incorporated in encoding as prompts to the language model , allowing uniform handling of categorical and non-categorical slots .
When implemented using a strong pretrained text - to - text model , this simple approach achieves state - of- theart ( SOTA ) results on MultiWOZ 2.2 , and performance is on par with SOTA on MultiWOZ 2.1 and M2M .
In addition , our analyses provide empirical results that contribute towards understanding how schema description augmentation can effectively constrain the model prediction .
Related Work
Multi-Domain Dialogue State Tracking Task-oriented dialogue datasets ( Shah et al. , 2018 ; Henderson et al. , 2014 ) , have spurred the development of dialogue systems ( Zhong et al. , 2018 ; Chao and Lane , 2019 ) .
Recently , to further examine the generalization abilities , large scale cross-domain datasets have been proposed ( Budzianowski et al. , 2018 ; Zang et al. , 2020 ; Eric et al. , 2019 ; Rastogi et al. , 2020 b ) . Classification - based models ( Ye et al. , 2021 ; pick the candidate from the oracle list of possible slot values .
The assumption of the full access of the schema makes them have limited generalization abilities .
On the other hand , generation - based models Lin et al. , 2020a ) directly generate slot values token by token , making it possible to handle unseen domains and values .
Most of these models require task -specific modular designs .
Recently , generation - based models that are built on large-scale autoregressive pretrained language models ( Ham et al. , 2020 ; Hosseini - Asl et al. , 2020 ; Peng et al. , 2020a ) achieve promising state tracking results on MultiWOZ 2.0 and 2.1 when trained on additional supervision signals or dialogue corpus .
Both Ham et al. ( 2020 ) and Hosseini - Asl et al . ( 2020 ) require dialogue acts as inputs .
Both Hosseini - Asl et al. ( 2020 ) and Peng et al . ( 2020a ) require DB search results as inputs .
Peng et al. ( 2020a ) also leverages other dialogue corpora to finetune the language model .
Our work requires only the dialogue state labels and does not utilize any external dialogue datasets .
Language Models Large-scale pretrained language models have obtained state - of - the - art performance on diverse generation and understanding tasks including bidirectional encoder style language models ( Devlin et al. , 2019 ; Liu et al. , 2019 ) , auto-regressive language models ( Radford et al. , 2019 ; Brown et al. , 2020 b ) and more flexible sequence - to-sequence language models ( Raffel et al. , 2020 ) .
To adapt to dialogue tasks , variants of systems are finetuned on different dialogue corpora including chit- chat systems Adiwardana et al. , 2020 ; Roller et al. , 2020 ) and task - oriented dialogue systems ( Mehri et al. , 2019 ; Wu et al. , 2020 ; Henderson et al. , 2020 ; Peng et al. , 2020 b ) .
We leave it as future work to leverage domain- adapted language models .
Prompting Language Models
Extending a language model 's knowledge via prompts is an active line of research .
Radford et al. ( 2019 ) obtain empirical success by using prompts to guide zero shot generation without finetuning on any prompts .
Raffel et al. ( 2020 ) uses task -specific prompts in both finetuning and testing phase .
Recent studies have also tried to automatically discover prompts rather than writing them by humans ( Jiang et al. , 2020 ) .
Our proposed prompting method is largely inspired by this body of work .
Instead of prompt engineering / generation , we focus on using available natural language descriptions of schema categories associated with database documentation as task -specific promptings for DST .
Prompting Language Model for Dialogue State Tracking
In this section , we first set up the notations that are used throughout paper , and then review the generative DST with the sequence - to-sequence framework .
pretrained model .
Notation .
For task - oriented dialogues considered in this paper , a dialogue consists of a sequence of utterances alternating between two parties , 1 , 1 , ... , , , where and represent the user utterance and the system response , respectively .
In a turn , the user provides a new utterance and the system agent responds with utterance .
As shown in the bottom of Figure 1 , at turn , we denote the dialogue context as = { 1 , 1 , ? , ?1 , } , which excludes the latest system response .
In this work , we assume a multi-domain scenario , in which case the schema contains domains ? = { 1 , ? , } and slots ? = { 1 , ? , } to track as examples illustrated in Figure 1 . , the dialogue state at turn , is then defined as a mapping from a pair ( , ) into values .
Here , we define ( , ) = , if ( , ) is not in the current dialogue state .
In the given example of Figure 1 , the pair ( domain=hotel , slot=ref ) is not in the dialogue state , and the value " none " is assigned .
Generation - based DST with the Sequence-to-sequence Model
There are primarily two decoding strategies for generation - based DST in the literature for inferring the dialogue state at a particular turn - sequential ( a ) and independent ( b ) ( c ) - both of which are explored in the paper as illustrated in Figure 1 .
In the first case ( top system ( a ) in Figure 1 ) , the dialogue history is taken as input to the encoder , and domain-slot-value triplets ( , , ) are generated sequentially , where ( , ) ? .
This approach is adopted in many systems that leverage autoregressive LMs ( Peng et al. , 2020a ; Hosseini - Asl et al. , 2020 ) .
Despite being simple , this kind of sequential generation of multiple values is more likely to suffer from optimization issues with decoding long sequences resulting in lower performance .
However , given its wide adoption in the literature , we still include this type of generative DST with the same backbone pretrained encoderdecoder Transformer model in our experiments .
To partially address this issue , Lin et al . ( 2020 b ) propose a domain independent decoding where the decoder only have to generate a sequence of slot and value pairs within a specific given domain .
Although their model leverages the same backbone model as ours , we empirically find that this form of strategy is still of limited effectiveness .
In the second case ( middle two systems ( b ) ( c ) in Figure 1 ) , the values for each domain-slot pair are generated independently , potentially in parallel .
The domain and slot names ( embedded as continuous representations ) are either the initial hidden state of the decoder or the first input of the decoder .
Values are either generated for all possible domain-slot ( , ) pairs with a possible value of " none " and / or there is a separate gating mechanism for domain-slot combinations not currently active .
Since we are interested in enriching the input with task -specific information , we focus on extending the independent decoding modeling for our prompt- based DST .
Prompt- based DST
In this section , we formally present the flow of our prompt- based DST with an encoder-decoder architecture .
Here , we are interested in an encoderdecoder model with a bi-directional encoder ( Raffel et al. , 2020 ; Lewis et al. , 2020 ) , in contrast with the uni-directional encoder used in autoregressive LMs ( Radford et al. , 2019 ; Brown et al. , 2020a ) .
The input of the prompt- based DST is made up of a dialogue context and a task -specific prompt .
Here , we use two types of task -specific prompts , the domain-related prompt ( ) , and slot-related prompt ( ) , both of which are de-rived based on the given schema .
We leave the discussion of two specific realizations of taskspecific prompts to the later part of this section .
Specifically , all sub-sequences are concatenated with special segment tokens , i.e. , " [ user ] 1 [ system ] 1 . . . [ system ] ?1 [ user ] [ domain ] ( ) [ slot ] ( ) " , as input to the encoder , where [ user ] , [ system ] , [ domain ] , [ slot ] are special segment tokens for indicating the start of a specific user utterance , system utterance , domain-related prompt , and slotrelated prompt , respectively .
Given this prompt-augmented input , the bidirectional encoder then outputs = Encoder ( , ( ) , ( ) ) , ( 1 ) where ? ? ? is the hidden states of the encoder , is the input sequence length , and is the encoder hidden size .
Then , the decoder attends to the encoder hidden states and decodes the corresponding slot value ( , ) : ( , ) = Decoder ( ) .
( 2 )
The overall learning objective of this generation processing is maximizing the log-likelihood of ( , ) given , ( ) and ( ) , that is ? ( , ) log ( ( , ) | , ( ) , ( ) ) .
( 3 ) During inference , a greedy decoding procedure is directly applied , i.e. , only the most likely token in the given model vocabulary is predicted at each decoding step .
Schema-Based Prompt .
The first realization of task -specific prompt considered in this paper is based on the domain and slot names as defined in the task - dependent schema .
As shown in ( b ) of Figure 1 , given the domain name train and the slot name day , the specific prompt is in the form of " [ domain ] train [ slot ] day " .
Different from ( Lin et al. , 2020a ; where the taskspecific information is used in the decoder side , our symbol - based prompt as additional input to the bi-directional encoder can potentially achieve taskaware contextualizations .
Observing that users of - those domain / slot names contain limited information that can be utilized by pretrained LMs .
In other words , those symbols from the custom schema are typically under-specified and unlikely to appear in corpus for LM pretraining .
Fortunately , documentation is commonly available for real-world databases , and it is a rich resource for domain knowledge that allows dialogue systems to better understand the meanings of the abbreviated domain and slot names .
The documentation includes but is not limited to domain / slot descriptions and the list of possible values for categorical slots .
In this work , we experiment with a simple approach that augments the input by incorporating the domain description after the domain name and the slot description ( with the sequence of values , if any ) following the slot name , as illustrated in the system ( c ) in Figure 1 .
Backbone Sequence-to-sequence Model
Our prompt- based DST model is initialized with weights from a pretrained LM in an encoderdecoder fashion .
In this paper , we use the Text-to - Text Transformer ( T5 ) ( Raffel et al. , 2020 ) as our backbone model .
T5 is an encoder-decoder Transformer with relative position encodings ( Shaw et al. , 2018 ) .
We refer interested readers to the original paper for more details .
Experiments
Datasets
MultiWOZ 2.2 : Fully Annotated Natural Language Augmented Prompt
We present the evaluation results on MultiWOZ 2.2 in Table 2 .
The following baseline models are considered : TRADE , DS -DST ( Zhang et al. , 2019 ) and Seq2Seq-DU ( Feng et al. , 2020 ) .
Similar to ours , the decoding strategy of TRADE is independent .
However , the sum of domain and slot embeddings are the first input of the decoder , which makes their dialogue history representation not task - aware contextualized .
The sequential decoding strategy is worse than the independent decoding strategy by over 5 % with both T5 - small and T5 - base .
Even with T5 - small ( almost half the model size of BERT - base which is used in most previous benchmark models ) , our system achieves the SOTA performance using the independent decoding .
As expected , T5 - base systems outperform T5 - small systems .
With the augmentation of descriptions , we improve the overall JGA by over 1 % in both T5 - small and T5 - base .
MultiWOZ 2.1 : Partially Annotated Natural Language Augmented Prompt Different from MultiWOZ 2.2 studied in the previous section , MultiWOZ 2.1 only contains natural language descriptions for slots but not domains .
In addition , there is no possible slot value information .
The evaluation results on MultiWOZ 2.1 are shown in Table 3 , where we compare with TRADE , MinTL ( Lin et al. , 2020a ) , SST , TripPy ( Heck et al. , 2020 ) , Simple-TOD ( Hosseini - Asl et al. , 2020 ) , SOLOIST ( Peng et al. , 2020a ) and TripPy +SCORE finetune their models .
As expected , we observe that T5 - base models perform consistently better than T5 - small models .
Moreover , using descriptions consistently improves the performance of both models .
All our models outperform baselines that do not use extra dialogue data .
It is worth noting that comparing with MinTL ( T5 - small ) , our model is better by over 4 % even without descriptions .
Further , our T5 - small system is even better than MinTL built on BART - LARGE ( Lewis et al. , 2020 ) which has substantially more parameters .
Similar to ours , MinTL leverages a sequence-to-sequence LM .
One difference is that their domain information is fed only to the decoder while our approaches enables task - aware contextualization by prompting the LMs with domain and slot information on the encoder side .
Another difference is that they jointly learn DST together with dialogue response generation , which provides more supervision signals .
Therefore , the better performance of our systems implies that schema-driven prompting is effective .
Lastly , compared with MultiWOZ 2.2 , the performance gain brought by augmenting natural language descriptions is less pronounced which is likely caused by the reduced information available in MultiWOZ 2.1 descriptions .
M2M : Borrowed Natural Language Augmented Prompt Table 4 shows the evaluation results on M2M .
In this case , all natural language descriptions are directly borrowed from dialogue datasets that are an- notated in a different manner .
We achieve the SOTA performance on Sim-R and Sim -M+R while being comparable on Sim -M .
The improvements of descriptions are only evident on the restaurant domain .
The lack of improvement from slot descriptions for the movie domain may be because the slot descriptions do not add much beyond the slot name ( compared to " category " for the restaurant domain ) or that it has slots that generalize better across domains ( e.g. date , time , number of people ) .
Analysis
Breakdown Evaluation for MultiWOZ In Table 5 , we follow the categorization provided in ( Zang et al. , 2020 ) and show the breakdown evaluation of categorical and non-categorical slots on MultiWOZ 2.2 .
As we can see , the breakdown accuracy scores for both categorical and non-categorical slots are pretty consistent with the overall JGA .
For both T5 - small and T5 - base models , models with sequential decoding perform worse than the corresponding models with independent decoding for both categorical and non-categorical slots .
In particular , the independent decoding models achieve more pronounced improvement in categorical slots indicating that the task -specific prompt is very helpful for guiding the decoder to predict valid values .
When comparing models using natural language description with those not , we observe performance gains for both types of slots for T-base but only non-categorical slots for T5 - small .
It is likely that the smaller size of T5 has limited representation capability to effectively utilize the additional textual description information regarding types and possible values .
Ablation Study on Schema Descriptions
To understand what parts of the schema descriptions are most important , we experiment with three kinds of description combinations on MultiWOZ 2.2 using the T5 - small configuration : ( i ) excludes the list of possible values for categorical slots ( ii ) excludes slot descriptions ( iii ) excludes domain descriptions .
For ( i ) , there is an 0.4 % point drop in JGA , validating that value sets can successfully constrain the model output , as we illustrate in Table 6 . For ( ii ) , there is a 0.8 % point drop in JGA .
And for ( iii ) , there is a 0.1 % point drop in JGA .
This shows that slot descriptions are the most important part of the schema prompts and domain descriptions are relatively less effective .
This is probably due to the fact that there are 61 slots in MultiWOZ 2.2 but only 8 domains .
Also , the domain names are all self-contained single words .
The Effectiveness of Natural Language Augmented Prompt
In the dialogue history are partially matched to the slot description of arriveby making it easier for the description - augmented system to detect the mention of the correct slot .
For the second example , the type information in the description implicitly guides the model to focus on time -related information leading the correct output of the normalized time expression , 16:45 .
In contrast , the model without descriptions only generates the partial answer 4:45 , ignoring PM .
Lastly , " London Kings Street " is a typographical error in this case .
By utilizing the provided possible values included in the slot descriptions , the model is able to generate the correct slot value without spelling error , demonstrating that the natural language augmented prompt can successfully constrain the model output and potentially provides robustness to the dialogue state tracking system .
Error Analysis of Natural Language Augmented Prompt- based DST
Here , we further carry out error analyses into the natural language augmented prompt- based T5 - base model on MultiWOZ 2.2 .
As shown in Table 7 , we randomly sample 50 turns and categorize them into different types .
In summary , there are four types of errors : ( i ) The most common error type is annotation error in which the model prediction is actually correct , which is similar to the findings of ( Zhou and Small , 2019 ) . ( ii ) 20 % of the errors come from model failing to capture information provided by the system .
3 ( iii ) 16.66 % of the errors are caused by the model misses of at least one gold slot . ( iv ) 10 % of the errors are correct slot predictions with the wrong corresponding values .
In general , most errors are likely caused by the lack of explicit modeling of user-system interactions .
Conclusion
In this work , we propose a simple but effective task - oriented dialogue system based on large-scale pretrained LM .
We show that , by reformulating the dialogue state tracking task as prompting knowledge from LM , our model can benefit from the knowledge - rich sequence to sequence T5 model .
Based on our experiments , the proposed natural language augmented prompt- based DST model achieve SOTA on MultiWOZ 2.2 and comparable performance on MultiWOZ 2.1 and M2 M to recent SOTA models .
Moreover , our analyses provide evidence that the natural language prompt is effectively utilized to constrain the model prediction .
Figure 1 : Overview of generative DST approaches for multi-domain scenario .
The top three figures illustrate three different generative approaches considered in this paper and the bottom figure includes specific examples for dialogue history , domain names , slot names , natural language descriptions ( types , set of valid values , etc. ) for slots .
Sub-figure ( b ) ( c ) demonstrate two prompt- based DST models proposed , where method in ( c ) includes additional natural language description of slots considered for tracking .
Domain descriptions are omitted for brevity .
