title
A Three - Stage Learning Framework for Low-Resource Knowledge -Grounded Dialogue Generation
abstract
Neural conversation models have shown great potentials towards generating fluent and informative responses by introducing external background knowledge .
Nevertheless , it is laborious to construct such knowledge - grounded dialogues , and existing models usually perform poorly when transfer to new domains with limited training samples .
Therefore , building a knowledge - grounded dialogue system under the low-resource setting is a still crucial issue .
In this paper , we propose a novel threestage learning framework based on weakly supervised learning which benefits from large scale ungrounded dialogues and unstructured knowledge base .
To better cooperate with this framework , we devise a variant of Transformer with decoupled decoder which facilitates the disentangled learning of response generation and knowledge incorporation .
Evaluation results on two benchmarks indicate that our approach can outperform other state - of - the - art methods with less training data , and even in zero-resource scenario , our approach still performs well .
Introduction Neural dialogue systems have made rapid progress in recent years thanks to the advances in sequence generation technology ( Vinyals and Le , 2015 ; Vaswani et al. , 2017 ) .
Though such models in neural architectures are able to reply with plausible responses regarding to dialogue history , people can still feel a clear gap when they converse with the chatbots , compared with the conversation with humans .
To bridge the gap and generate fluent and informative responses , a number of approaches have been proposed by leveraging external knowledge .
Knowledge- grounded dialogue is a task of generating an informative response based on both dialogue history and a collection of external knowledge ( Dinan et al. , 2019 ) .
The forms of knowledge are diverse , and in this work , we only focus on knowledge in the form of unstructured documents .
Generally , it is difficult to construct large scale conversations that are naturally grounded on the documents for learning of a response generation model ( Zhao et al. , 2020a ) , and most of the previous methods Li et al. , 2019 ; Kim et al. , 2020 ; Dinan et al. , 2019 ) perform poorly when transfer into a new domain with limited training samples .
So there are growing appeals for lowresource dialogue response generation , which aims to leverage past experience to improve the performance with limited labeled training examples of target corpus .
To address this issue , we envisage to absorb useful information from other easily accessible heterogeneous datasets to enhance the performance of the knowledge - based dialogue model under lowresource setting .
Based on this assumption , we propose a novel Three - Stage Learning Framework ( TSLF ) .
TSLF attempts to divide the parameters of a model into dialogue -related and knowledge integration -related .
In the first stage , we use supervised learning to pre-train dialogue -related parameters on general dialogues ( e.g. , online forum comments ) , and perform domain- adaptive pre-training ( Gururangan et al. , 2020 ) to initialize knowledgerelated parameters on unlabeled knowledge base ( e.g. , items in Wikipedia ) .
In the second stage , inspired by the distant supervision in the relation extraction ( Mintz et al. , 2009 ) , we match a set of pseudo-knowledge for each ungrounded dialogue to construct a lower quality knowledge - grounded dialogue dataset , and further co-pretrain the above two groups of parameters on this dataset .
In the third stage , the trained model will be fine-tuned on the target low-resource dataset .
The flow of TSLF is shown in Figure 1 .
In order to better cooperate with the disentangled learning mechanism in TSLF , we devise Knowledge - Aware Transformer ( KAT ) , a vari - ant of vanilla Transformer ( Vaswani et al. , 2017 ) whose parameters are decoupled that facilitates the separate learning of dialogue generation and knowledge incorporation .
As shown in Figure 2 , besides dialogue history , KAT also accepts a set of knowledge as additional input .
KAT has a knowledgeaware decoder which could obtains information from the dialogue context and background documents through cross-attention and integrates them through a controller .
We conduct experiments on two knowledgegrounded dialogue generation benchmarks including Wizard- of-Wikipedia ( Dinan et al. , 2019 ) and CMU_DoG ( Zhou et al. , 2018 ) .
Evaluation results in terms of both automatic metrics and human judgment indicate that using only about 1/4 of the training data on Wizard ( 1/16 on CMU_DoG ) , the performance of our approach outperforms the competitive baselines which are learned from full crowdsourced training corpora .
Even without using any training data of the target dataset , our method still performs well .
The contributions in this work are summarized as follows : ( 1 ) We propose a novel three - stage learning framework that leverages weakly supervised learning to help build a low-resource knowledgegrounded dialogue generation model ; ( 2 ) We devise knowledge - aware Transformer , a knowledgegrounded neural conversation model with a novel dynamic knowledge selection mechanism , which can fully exploits the external knowledge to generate fluent and informative dialogue responses ; ( 3 ) Our KAT - TSLF achieves surprising performance under the scenarios of full data , low-resource and even zero-resource .
The source code is available at https : // github.com/neukg/KAT-TSLF .
Approach Low-resource knowledge- grounded dialogue generation is task that requires a method to learn from experience E , which consists of direct experience E d containing limited monolingual contextknowledge- response triples and indirect experience E i , to improve the performance in response generation measured by the evaluation metric P .
The direct experience E d refers to the training samples of target corpus D l = {( U i , K i , Y i ) } m 1 i=1 ( U i is dia - log history , Y i is response , and K i = { K j } s j=1 is a set of external knowledge documents of i-th sample ) which are under low-resource settings .
In this work , we consider E i as a large scale ungrounded dialogue dataset D d = {( U i , Y i ) } m 2 i=1 , a knowledge base D k = { K i } m 3 i=1 ( m 2 , m 3 m 1 ) and a pretrained language model which are easy to obtain .
In the following , we first introduce our KAT , and then show how to train it from coarse to fine under our TSLF .
Knowledge -Aware Transformer KAT accepts U and K = { K i } s i=1 as inputs , and generates a response ? .
It consists of three components : a dialogue context encoder ( DE ) to encode U , a knowledge encoder ( KE ) to encode K , and a decoder to incorporate dialog history , dynamically select knowledge and generate response .
The architecture of KAT is shown in Figure 2 .
Encoder
We define DE as a Transformer encoder , and the output is represented as U ?
R n?d , where n is the sequence length , and d is the hidden state dimension .
Similarly , KE is defined as another Transformer encoder , and it encode each document individually .
Following KE is a concatenation opera- tion that concatenates all document representations : K = [ K 1 ; ... ; K s ] ?
R sz?d , where K i ?
R z?d is output of i-th KE , and z is the sequence length of each document .
K and U will be used for the input of the decoder .
Knowledge -Aware Decoder Generally , not all knowledge in the K contributes to the generation of the response , so the model should have the ability to select knowledge .
Different from ( Dinan et al. , 2019 ; Kim et al. , 2020 ) ( Vaswani et al. , 2017 ) .
During decoding , KIB can dynamically select different knowledge according to dialogue context and the tokens that have been generated at current time step .
Controller
To control the knowledge and context contributions in each layer , we add a gate after the knowledge selection block .
Denote h k as output of KIB and h c as the residual from the previous block , the output of controller can be expressed by CT ( h k , h c ) = ? ? LN ( h k ) + ( 1 ? ? ) ? h c ? = ? ( w ? [ h k ; h c ] ) ( 1 ) where w ?
R 2d is a learnable parameter and ? denotes sigmoid function .
Three - Stage Learning Framework
For further discussion , we denote ?
d , ? k , and ?
a as the learnable parameters of the green , yellow and pink parts in Figure 2 respectively .
We can observe that ?
d is related to context encoding and response generation , ? k is related to knowledge representation and integration , and these two parts are disentangled .
In order to benefit from a wealth of heterogeneous corpora , we propose a three - stage learning framework .
In TSLF , we first initialize ?
d and ?
k in a decoupled scheme by training in ungrounded dialogues and unstructured knowledge documents respectively , and then co-optimize them with ?
a by weakly supervised learning and finally transfer KAT to target low-resource dataset .
The illustration of TSLF is shown in Figure 1 .
Stage I
We choose the state- of- the- art Transformer based encoder-decoder model BART ( Lewis et al. , 2020 ) as the the backbone , pre-training it on D d with dialogue response generation task : L d ( ? d ) = ? ( U, Y ) ?
D d t log p(y t |y <t , U ) ( 2 ) Besides , inspired by Gururangan et al . ( 2020 ) , we conduct domain- adaptive pre-training on unlabeled knowledge documents to improve knowledge representation ability .
Specifically , 15 % of tokens in a text K are replaced with < mask > or noise words , and another Transformer tries to rebuild it : L k ( ? + k ) = ?
K?D k t log p( k t |k <t , K ) ( 3 ) where K is the corrupt K .
We disentangle the encoder and the cross-attention block in each decoder layer from this Transformer ( ? + k ) and initialize ? k with them .
for i in { 1 , ... , o} do 7 : Sample K from D k ?
K randomly ; 8 : K ? K ? { K } ; 9 : end for 10 : D p ? D p ? {( U , K , Y ) } ; 11 : end if 12 : end for 13 : return D p ;
Stage II
In stage I , ? d and ?
k are trained separately , and the connection between knowledge and dialogue has not yet been established .
If KAT is fine- tuned directly on low-resource dataset D k , it may cause inconsistency problems , so we add a warm - up process to it .
Intuitively , responses from humans carry clues to relevance of the knowledge candidates ( Zhao et al. , 2020 b ) , so the knowledge document that promotes the flow of dialogue usually has a high textual similarity with the response .
Based on this assumption , we construct a set of pseudo-knowledge for some dialogues in D d to form a new weak supervision dataset D p according to Algorithm 1 .
I( query , documents ) means retrieve the document with the highest similarity ( e.g. , TF -IDF and BM25 ) .
Context-response pairs with low quality will be removed .
In the knowledge- grounded dialogue corpora , only less documents in knowledge pool are valuable , and others are noise .
The design of negative samples is to simulate this situation and make the distribution of knowledge in D p closer to the target data set .
We perform weakly supervised learning on D p to warmup KAT : L (?
d , ? k , ? a ) = ? ( U , K , Y ) ?
Dp log p( Y |K , U ) ( 4 )
Stage III
After warming up on D p , KAT will be fine-tuned on the target low-resource dataset : L (?
d , ? k , ? a ) = ? ( U , K , Y ) ?
D l log p( Y |K , U ) ( 5 ) If not fine-tuned , KAT can also be directly applied to zero-resource response generation .
Experiments
Datasets and Evaluation Methods
We conduct extensive experiments on two public English knowledge -grounded datasets : Wizard-of-Wikipedia ( Dinan et al. , 2019 ) and CMU_DoG ( Zhou et al. , 2018 ) .
Wizard- of-Wikipedia is a chitchatting dataset between two agents , and the two participants are not quite symmetric : one will play the role of a knowledgeable expert ( which we refer to as the wizard ) while the other is a curious learner ( the apprentice ) .
Each wizard turn is associated with ?60 sentences retrieved from the Wikipedia and each sentence contains ?30 words , and most of them are noise .
The test set is split into two subsets , test seen and test unseen .
The difference between the two is that the former contains some topics that overlap with the training set .
CMU_DoG also contains conversations between two workers who know the background documents and try to discuss the content in depth .
Following the common practice in evaluating open domain dialogue generation , we choose perplexity ( PPL ) , corpus-level BLEU ( Papineni et al. , 2002 ) , sentence - level ROUGE ( Lin , 2004 ) and corpus-level DISTINCT ( Li et al. , 2016 ) has a larger vocabulary that could express more information .
BLEU is computed with NLTK library ( Bird , 2006 ) and ROUGE is calculated with the code published with Kim et al . ( 2020 ) .
Besides quantitative evaluation , we also recruit three human annotators to do qualitative analysis on response quality .
For each dataset , we randomly sample 100 samples , and each sample contains the conversation history , response , and external knowledge set ( for Wizard - of- Wikipedia , we only provide ground - truth knowledge ) .
The annotators then judge the quality of the responses from three aspects , including context coherence , language fluency and knowledge relevance , and assign a score in { 0 , 1 , 2 } to each response for each aspect .
Each response receives 3 scores per aspect , and the agreement among the annotators is measured via Fleiss ' kappa ( Fleiss , 1971 ) .
Baselines
We compare our approach with the following baselines : ( 1 ) ITDD : an Transformer - based architecture which incrementally represents multi-turn dialogues and knowledge , and conducts response decoding in two passes ( Li et al. , 2019 ) ; ( 2 ) BART cat : A simple BART - based model that take the concatenation of dialogue context and all knowledge as the input of BART for response generation .
BART sets constraint on the maximum number of tokens it can handle , and we directly truncate the text that exceeds the length limit ; ( 2 ) BART skt : SKT is variational model that introduced BERT on the basis of and considered the knowledge selection history in multi-turn dialogue ( Kim et al. , 2020 ) .
We feed the knowledge candidate selected by SKT to BART for response generation .
It should be noted that training SKT requires human labels that indicate ground - truth knowledge which are crucial to the performance of the model .
For fair comparison , we use I to reselect the knowledge label ; ( 3 ) DRD :
Another low-resource dialogue model which devise a disentangled response decoder with copy mechanism ( See et al. , 2017 ) and use a two -stage framework to learn it ( Zhao et al. , 2020a ) .
DRD is not open source , so we ca n't make a very detailed comparison with it ; ( 4 ) ZRKGC : A double latent variable model that achieves the state - of - the - art performance in zeroresource knowledge - grounded dialogue generation ( Li et al. , 2020 ) . ZRKGC is based on UNILM
In order to save space , we merge the Wizard seen and unseen into one subfigure .
( Dong et al. , 2019 ) with 110M parameters whose performance is close to BART , so we will not replace the backbone of ZRKGC .
Implementation Details
The knowledge pool of target dataset is usually very large ( e.g. ?60 sentences in Wizard ) , in order to reduce the time overhead , following ( Kim et al. , 2020 ) , we only keep the first 40 sentences .
We use the base version of BART with 139M parameters in our work , and the number of parameters of KAT is 196M .
The batch size in stage I , II and III is 2048 , 128 and 16 respectively .
The max sequence length in source and target is 256 and 64 respectively .
All models are optimized with AdamW ( Loshchilov and Hutter , 2017 ) with learning rate 5e ? 5 in 3 epochs .
We employ beam search in response decoding ( the number of beams from 1 to 3 ) implemented by Wolf et al . ( 2020 ) .
Evaluation Results
Table 1 , 2 and 3 reports the evaluation results on automatic metrics , and we have the following observations : ( 1 ) In the full - data scenario , KAT achieves state - of - the - art performance without using any additional corpora , which means that KAT itself is an excellent dialogue model .
Besides , additional resources are unnecessary when there are enriched training datas , so TSLF has little effect in this setting ; ( 2 ) KAT - TSLF achieves the comparable performance with BART cat / skt even though the baselines have leveraged all training data , while our model is only learned with 1/4 training data on Wizard ( 1/16 on CMU _DoG ) .
We compare the low-resource performance with DRD , and the results are shown in Figure 3 .
For a fair comparison , we removed the pre-training language model and reduce the number of model parameters .
We can see that KAT - TSLF outperforms DRD ( especially in CMU_DoG ) .
The comparison with BART cat is supplemented in Figure 4 ; ( 3 ) Although our TSLF is mainly for low-resource scenarios , under the setting of zero resources ( i.e. , without stage III ) , the performance of KAT - TSLF also surpasses ZRKGC in most evaluation metrics ; ( 4 ) Responses generated by KAT have higher DIST -n , which means that our KAT can better obtain information from multiple knowledge and generate more diverse texts .
Table 4 reports the human evaluation results .
We observe that responses from our KAT - TSLF are more fluent and more contextually coherent than those from BART skt and ZRKGC .
Compared with our low-resource model , SKT has stronger knowledge relevance in the case of full data , thanks to its well - designed knowledge selection module .
Ablation Study
We conduct ablation experiments on Wizard and CMU_DoG , and the results are shown in Figure 4 .
So as to verify the effect of TSLF , we first removed stage I , stage II , and stage I II respectively .
Inserting a new module into an already well - trained large-scale pre-trained language model will cause inconsistency problems , which require a lot of data to reconcile , so after removing stage II or stage I II , the performance of our KAT in low-resource dropped sharply .
Although the quality of the automatically constructed warm - up dataset D p is lower than the target dataset D l , it also helps to establish the connection between the knowledge representation component and the dialogue component .
Besides , we tried not to pre-train ?
k on unlabeled documents , and the result has dropped slightly , which demonstrates that is still helpful to tailor a pretrained model to the domain of a target task .
In addition , replacing negative sampling with top-k retrieval will increase the inconsistency with the knowledge distribution of target dataset , leading to performance degradation .
Moreover , the controller also has an effect on the generalization of the model .
It can help KAT quickly adapt to new domains by adjusting the proportion of knowledge and context in the response .
In order to improve the generalization performance with limited training data , some works ( Chen and Shuai , 2021 ; Zhao et al. , 2020a ) fix most of the parameters during fine-tuning .
We also tried to frozen knowledge encoder and context encoder in stage III or stage II III , and the results show that the performance has not improved , indicating that with the help of stage II , our model can hardly fall into overfitting .
In order to verify the effect of our TSLF on other models , we try to combine BART cat with TSLF .
Since the parameters of BART are tightly coupled , we can only apply stage II to it .
Experimental results show that the performance is improved significantly under low-resource setting .
Discussions Case Study
Table 5 shows a case from Wizard , from which we can see that the response from our model with zero data not only smoothly catches the ground - truth knowledge ( highlighted in blue ) , but also expands the topic with proper pieces of other knowledge ( highlighted in yellow ) .
ZRKGC generated sentences that were inconsistent with the facts .
Although BART skt chose the correct knowledge , the narrative was too straightforward , and there is a repetition phenomenon .
We showed some other cases in the supplementary material .
Comparison with DRD
If we ignore the details , DRD is actually a special case of our method , which skips stage II .
During pre-training , DRD completely separates dialogue-related components and knowledge representation - related components , which makes it difficult to effectively promote the integration of dialogue and knowledge with only a small number of samples during fine-tuning .
So when the training data is extremely small , DRD can hardly work .
Besides , in order to prevent overfitting , DRD has to limit the number of parameters of the knowledge integration component and use fix other parameters when fine-tuning , which leads to limited performance of the model .
In addition , the complex model structure makes it difficult for DRD to use pre-trained language models .
KAT v.s. BART cat BART ( as well as most other pre-training language models ) has a limit on the maximum tokens of the input , so useful knowledge is likely to be truncated .
For example , there are about 60 external documents per sample in Wizard , and about 40 documents will be truncated .
In theory , KAT can accept an unlimited number of knowledge , so this should be one of the reasons why KAT 's performance is better than BAER cat .
Related Work Open domain end-to- end dialogue response generation is inspired by the success of applying neural sequence to sequence models on machine translation ( Sutskever et al. , 2014 ; Bahdanau et al. , 2015 ; Vaswani et al. , 2017 ) .
Very recently , in order to generate fluent , coherent and informative response , many approaches have been proposed by introducing external background documents ( Ghazvininejad et al. , 2018 ; Yavuz et al. , 2019 ; Li et al. , 2019 ; Lin et al. , 2020 ) . Besides documents ( Dinan et al. , 2019 ; Zhou et al. , 2018 ) , the are many forms of knowledge such as images ( Huber et al. , 2018 ) and triples in knowledge graph Tuan et al. , 2019 ) . Dinan et al. ( 2019 ) presents to divide knowledgegrounded dialogue into two steps : knowledge selection and dialogue generation .
PostKS , SKT ( Kim et al. , 2020 ) , PIPM ( Chen et al. , 2020 ) and SKT - KG ( Zhan et al. , 2021 ) use the prior and posterior distribution of knowledge to improve the accuracy of knowledge selection .
Zhao et al. ( 2020 b ) devise a reinforcement learning method to train a knowledge selector without ground - truth knowledge label .
DeepCopy ( Yavuz et al. , 2019 ) , ITDD ( Li et al. , 2019 ) and KIC ( Lin et al. , 2020 ) have improved the structure of the decoder so that it can better integrate knowledge .
Since knowledgeguided dialogue corpora need to be constructed through crowdsourcing , the size of datasets such as Wizard - of-Wikipedia ( Dinan et al. , 2019 ) are relatively small .
Zhao et al . ( 2020a ) and Li et al . ( 2020 ) proposed to conduct the knowledge - grounded conversation under the low-resource and zero-resource settings respectively .
We do not compare with Lin et al . ( 2020 ) ; Zhao et al. ( 2020 b ) since they did not release their entire source codes .
Our three - stage learning framework is inspired by Zhao et al . ( 2020a ) , which uses ungrounded dialogues and unstructured documents to train a knowledge - grounded dialogue model that can work in low-resource situations .
In addition , the design of stage II is inspired by distant supervision technology in relation extraction task ( Mintz et al. , 2009 ) .
The idea of KAT is also encouraged by disentangled decoder ( Raghu et al. , 2019 ) and the recent breakthrough in variants of Transformer ( Li et al. , 2019 ; Hashemi et al. , 2020 ; Izacard and Grave , 2020 ) .
Conclusion
We study knowledge- grounded dialogue generation under a low-resource setting by proposing a threestage learning framework and a knowledge - aware Transformer .
Evaluation results on two benchmarks indicate that our model achieves the stateof - the - art performance with less training data .
Besides , KAT -TSLF exhibits a good generalization ability on zero-resource scenario .
