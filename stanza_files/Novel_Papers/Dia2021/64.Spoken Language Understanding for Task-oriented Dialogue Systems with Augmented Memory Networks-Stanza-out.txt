title
Spoken Language Understanding for Task-oriented Dialogue Systems with Augmented Memory Networks
abstract
Spoken language understanding , usually including intent detection and slot filling , is a core component to build a spoken dialog system .
Recent research shows promising results by jointly learning of those two tasks based on the fact that slot filling and intent detection are sharing semantic knowledge .
Furthermore , attention mechanism boosts joint learning to achieve state - of - the - art results .
However , current joint learning models ignore the following important facts : 1 . Long-term slot context is not traced effectively , which is crucial for future slot filling .
2 . Slot tagging and intent detection could be mutually rewarding , but bidirectional interaction between slot filling and intent detection remains seldom explored .
In this paper , we propose a novel approach to model long-term slot context and to fully utilize the semantic correlation between slots and intents .
We adopt a key-value memory network to model slot context dynamically and to track more important slot tags decoded before , which are then fed into our decoder for slot tagging .
Furthermore , gated memory information is utilized to perform intent detection , mutually improving both tasks through global optimization .
Experiments on benchmark ATIS and Snips datasets show that our model achieves state - of - the - art performance and outperforms other methods , especially for the slot filling task .
Introduction
Task-oriented dialogue systems have attracted significant attention , which have been greatly advanced by deep learning techniques .
Traditionally , these dialog systems have been built as a pipeline , with modules including spoken language understanding ( SLU ) , dialog state tracking , action selection and language generation .
Among these problems , SLU , including intention detection and slot filling ( Tur and Mori , 2011 ) , is a key yet challenging problem to parse users ' utterances into se - mantic frames in order to capture a conversation 's core meaning .
Traditionally , intention detection is treated as a classification problem , whereas slot filling is usually defined as sequence labeling problem , where In - Out-Begin ( IOB ) format is applied for representing slot tags as illustrated in Table 1 . Given an utterance , SLU determines users ' intention and maps it into predefined semantic slots .
The input is a sequence of words , and the output is a sequence of predefined slot IDs .
A specific intent is assigned for the whole sentence .
In the traditional pipeline approach , intent detection and slot filling are implemented separately .
However , separate modeling of those two tasks is insufficient to take full advantage of all supervised signals , as they share semantic knowledge .
For example , if the intent of an utterance is " find_a_flight " , it is more likely to contain slots " de-parture_city " and " arrival_city " rather than " restau-rant_name " .
Another drawback of the pipeline method is that errors made in upper stream modules may propagate and be amplified in downstream components , which however could possibly be eased in joint model ( Zhang and Wang , 2016 ) .
Recently , joint model for intent detection and slot filling has been proposed and achieved promising results ( Liu and Lane , 2016 ; Goo et al. , 2018 ; .
Though achieving promising performance , their models suffer from two major issues : 1 ) Modeling of slot context .
Though the latent memory of RNNs can model history information , they are inherently unstable over long time sequences because the memories are the RNN hidden states .
( Weston et al. , 2014 ) observes that RNNs tend to focus more on short-term memories and forcefully compress historical records into one hidden state vector .
Thus , simple RNNs cannot preserve long-term slot context of the conversation , which is crucial to future slot tagging .
2 ) Bi-directional interaction between slot filling and intent detection .
The majority of joint modeling work has studied how to utilize intent information to improve slot filling performance .
However , the beneficial impact of slot information on intent detection is mostly ignored .
In fact , slots and intents are closely correlative , thus mutually reinforcing each other .
In this paper , we propose a new framework to jointly model intent detection and slot filling in order to achieve a deeper level of semantic modeling .
Specifically , our model is distinguished from previous work primarily in two ways .
?
Model the mutual interaction between intent detection and slot filling .
The fact that intent detection and slot filling are semantically related is well - observed and how to use intent information to boost slot filling is widely explored .
However , slot filling is beneficial to intent detection as well , and these benefits are yet to be explored .
We propose a gating mechanism between intents and slots based on KV - MNs in order to model the interaction between intent detection and slot filling .
Related Works
Since intent detection can be treated as an utterance classification problem , different classification methods , such as support vector machines ( SVM ) and RNNs ( Haffner et al. , 2003 ; Sarikaya et al. , 2011 ) , have proposed to solve it .
On the other hand , for slot filling , hidden markov models ( HMM ) and conditional random fields ( CRF ) ( Lee et al. , 1992 ; Ye - Yi Wang et al. , 2005 ; Raymond and Riccardi , 2007 ) were used to solve slot filling problem .
Later RNN based methods had become popular .
For example , Yao et al . ( 2013 ) ; Mesnil et al. ( 2015 ) employed RNNs for sequence labeling in order to perform slot filling .
Alternatively , intent detection and slot filling can be done jointly to overcome the error propagation .
Zhang and Wang ( 2016 ) first proposed joint work using RNNs for learning the correlation between intent and slots .
Hakkani -T?r et al. ( 2016 ) adopted a RNN for slot filling and the last hidden state of the RNN was used to predict the utterance intent .
Liu and Lane ( 2016 ) introduced an attention - based RNN encoder decoder model to jointly perform intent detection and slot filling .
An attention weighted sum of all encoded hidden states was used to predict the utterance intent .
All those models outperform the pipeline models via mutual enhancement between two tasks .
Most recently , some work tries to model the intent information for slot filling explicitly in the joint model .
Goo et al. ( 2018 ) ; proposed the gate mechanism to explore incorporating the intent information for slot filling .
However , as the sequence becomes longer , it is risky to simply rely on the gate function to sequentially summarize and compress all slots and context information in a single vector ( Cheng et al. , 2016 ) .
Wang et al. ( 2018 ) proposed the bi-model to consider the cross-impact between the intent and slots and achieve state - ofthe - art results .
Zhang et al. ( 2018 ) proposed a hierarchical capsule neural network to model the hierarchical relationship among word , slot , and intent in an utterance .
Niu et al. ( 2019 ) introduces a SF - ID network to establish the interrelated mechanism for slot filling and intent detection tasks .
Compared with their work , our method explicitly models the long-term slot context knowledge which is beneficial to both slot filling and intent detection .
Memory network provides a principled approach for modeling long- range dependency which has advanced many NLP tasks such as machine transla-tion and question answering ( Sukhbaatar et al. , 2015 ) .
The initial framework of memory networks was proposed by Weston et al . ( 2014 ) .
Following the idea , Sukhbaatar et al . ( 2015 ) proposed an end-to - end memory augmented model that significantly reduced the requirement of supervision during training .
Key-value memory network ( Miller et al. , 2016 ) encoded prior knowledge by introducing a key memory structure which storeed facts to address to the relevant memory value .
None of them is to model slot context information dynamically especially in single turn conversational systems .
In this paper , we demonstrate how memory networks can be used to model long-term slot context knowledge and the interaction between intent detection and slot filling .
Proposed Model Memory networks show promising results on learning long- range dependency , but they are insensitive to represent temporal dependencies between memories ( Wu et al. , 2018 ) . RNNs tend to be opposite .
Thus , it makes sense for us to combine those networks together to model long -term slot context information .
In this section , we present a specific key -value dynamic memory module to collect and remember slot clues in the dialog context .
Then context memory is used to enhance the Encoder - Decoder based model to perform slot filling and intent detection .
As illustrated in Figure 1 , our proposed model is composed of an Encoder-Decoder , and a Key-Value Memory Module including KEY - MEMORY , VALUE - MEMORY , a memory read unit , and a memory write unit .
Given a single-turn dialog , the Encoder transforms a word in user utterances into a dense vector by using a shared self-attentive encoder .
Then the memory network encodes longterm slot context information by incorporating historical slot tags through memory attention and WRITE operations of the memory network .
The slot decoder integrates short -term hidden state of self-attention encoder and the long-term slot context generated by attentively reading the VALUE - MEMORY to generate slot tagging at each timestamp .
Later , intent decoder performs token level intent detection , which is seen as a coarse-grained intent detection result .
Finally , a fine- grained intent detection is produced by gating memory modules .
Both intent detection and slot filling are optimized simultaneously via a joint learning scheme .
Self-Attentive Encoder Given an input utterance X = ( x 1 , x 2 , . . . , x T ) of T words , where each word is initially represented by a vector of dimension d , the BiLSTM ( Hochreiter and Schmidhuber , 1997 ) is applied to learn representations of each word by reading the input utterance forward and backward to produce context sensitive hidden states H = ( h 1 , h 2 , . . . , h T ) : h t = BiLSTM ( x t , h t?1 ) ( 1 )
Then , we use self-attention mechanism to capture the contextual information for each token .
We adopt the method proposed by ( Vaswani et al. , 2017 ) , where we first map the matrix of input vectors X ? R T ?d to queries ( Q ) , keys ( K ) and values ( ? ) matrices by using different linear projections and the self-attention output C ? R T ?d 1 is : C = softmax Q K ? d 2 ? ( 2 ) where d 1 and d 2 represents self-attention dimension and keys 'dimension .
We concatenate the output of self-attention and BiLSTM as the final encoding representation as shown in Qin et al . ( 2019 ) : E = H ? C ( 3 ) where E = ( e 1 , . . . , e T ) ? R T ?( d+d 1 ) and ? is a concatenation operation .
Slot Decoder
Our slot deocder consists of two components : 1 ) the key-value memory - augmented attention model which generates slot context representation of users ' utterance , and 2 ) the unidirectional LSTM decoder , which predicts the next slot tag step by step .
Dynamic Key Value Memory Network
To overcome the shortcomings of RNNs in capturing semantic clues over the long-term , we design a memory network that can preserve fine - grained semantic information of long-term slot context .
We adopt a key-value memory network , which memorizes information by using a large array of external memory slots .
The external memories enrich the representation capability compared with hidden vectors of RNNs and enable the KV - MNs to capture long-term data characteristics ( Liu and Perez , 2017 ) .
We aim to incorporate the knowledge contained in the historical slot tags into the memory slots .
The KV - MNs decompose slot semantics in an utterance into different slot categories and thus preserves more fine - grained information .
In KV - MNs , a memory slot is represented by a key vector and an associated value vector .
? KEY -MEMORY : The KEY-MEMORY K ? R d k ?n learns latent correlation between utterance words and slot tags , where n is the number of memory slots and d k is the dimension of each slot .
Each column vector , that is , i-th key vector k i ?
R d k is set to the ith column of the KEY - MEMORY K , which is shared by all conversation turns and fixed during the processing of word sequences .
? VALUE -MEMORY : Both the KEY-MEMORY and VALUE -MEMORY have the same number of memory slots .
Each value memory vector stores the value of slot tag mentioned in the utterance .
We form a value memory matrix V t ?
R dv?n by combining all n value slots .
Different from KEY-MEMORY K , VALUE-MEMORY
V t is word-specific and is continuously updated according to the input word sequence .
During the conversation , the value of a new slot tag may be added into the VALUE - MEMORY , and an old value can be erased .
In this way , we can adequately capture the slot context information on each mentioned slot .
Two types of operations , READ and WRITE , are designed to manipulate the value memories .
Memory -augmented Decoder
As shown in Figure 1 , the decoder uses the aligned BiLSTM hidden state h t as a query to address the KEY - MEMORY looking for an attention vector a t , and attentively reads the VALUE - MEMORY to generate slot context representation c t .
First , we use h t to address the KEY -MEMORY to find an accurate attention vector a t .
a t = Address ( h t , K ) ( 4 ) a t is subsequently used as the guidance for reading the VALUE - MEMORY V t?1 to get the slot context representation c t . c t = Read( a t , V t?1 ) ( 5 ) c t works together with the aligned encoder hidden state e t to generate the new decoder state at the decoding step t , h S t = LSTM h S t?1 , y S t?1 , e t ? c t ( 6 ) where h S t?1 is the previous slot decoder state and y S t?1 is the previous emitted slot lable distribution .
After that , we use the slot decoder hidden state h S t to update V t : V t = Write h S t , V t?1 ( 7 ) Finally , the decoder state h S t is utilized for slot filling : y S t = softmax W S h h S t ( 8 ) o S t = argmax y S t ( 9 ) where W S h are trainable parameters and o S t is the slot label of the word at timestamp t in the utterance .
Intent Detection Decoder Different than most existing work where intent information is used to do slot filling , our framework is directly leveraging the explicit slot context information to help intent detection .
Furthermore , a gated mechanism is used in order to effectively incorporate slot memory information into intent detection .
By performing gated intent detection , there are two advantages : 1 . Sharing slot context information with intent detection improves intent detection performance since those two tasks are related .
Furthermore , a gating mechanism which combines the intent detection information and slot context retrieved from key - value memory , regulates the degree of enhancement of intent detection to prevent information overload .
2 . Through shared key-value memory , the interaction between intent detection and slot filling can be effectively modeled and executed .
Plus , by jointly training those two tasks , not only can intent detection performance be improved by slot context knowledge , but also slot filling is enhanced by minimizing intent detection objective function .
In other words , by learning optimal parameters of shared key -value memory , slot filling and intent detection interact in a more effective and deeper way .
Intent Detection Decoder :
For intent detection , we use another uni-directional LSTM as the intent detection network .
At each decode step t , the decoder state h I t is generated by the previous decoder state h I t?1 , the previous emitted intent label distribution y I t?1 and the aligned encoder hidden e t . h I t = LSTM h I t?1 , y I t?1 , e t ( 10 )
Then the intent decoder state h I t together with the slot context c t is utilized for final intent detection .
Gated Memory :
We propose a gated mechanism to integrate slot context with intent detection .
The gate regulates the degree of slot context information to feed into the intent detection task and prevent information from overloading .
As shown in Figure 2 , the gate G is a trainable fully connected network with sigmoid activation .
I t = g t ? h I t + ( 1 ? g t ) ? c t ( 11 ) where g t = sigmoid W t [ h I t c t ] + b t .
Then , the output of gated decoder state h I t is utilized for intent detection : y I t = softmax W I h h I t ( 12 ) o I t = argmax(y I t ) ( 13 ) where y I t is the intent output distribution of the t-th token in the utterance , o I t represents the intent lable of t-th token and W I h are trainable parameters of the model .
The final utterance result O I is generated by voting from all token intent results as illustrated in Qin et al . ( 2019 ) .
Memory Access Operation
In this section , we detail how to access key -value memory at the decoding time step t. KEY-MEMORY Address : K ? R d k ?n denotes the KEY-MEMORY at decoding time step t.
The addressed attention vector is given by a t = Address ( h t , K ) ( 14 ) where a t ?
R n specifies the normalized weights assigned to the slots in K , with j-th slot being k j .
The attention weights a t , j are calculated based on the correlation between h t and k j : a t , j = exp( e t , j ) n i=1 exp( e t , i ) ( 15 ) where e t , j = k j ( W a h t + b a ) VALUE-MEMORY Read : V t ?
R dv?n denotes the VALUE -MEMORY at decoding time step t.
The output of reading the value memory V t is given by c t = n j=1 a t , j v t , j ( 16 ) VALUE-MEMORY Write : Similar to the attentive writing operation of neural turing machines ( Graves et al. , 2014 ) , we define two types of operation for updating the VALUE - MEMORY : FOR - GET and ADD .
FORGET determines the content to be removed from memory slots .
More specifically , the vector F t ?
R dv specifies the values to be forgotten or removed on each dimension in memory slots , which is then assigned to each memory slot through normalized weights a t .
We use the slot decoder hidden state h S t to update V t?1 .
Formally , the memory after FORGET operation is given by 2 , . . . , n ( 17 ) where ?t , i = v t?1 , i ( 1 ? a t , i ? F t ) , i = 1 , ? F t = ?( W F , h S t ) is parameterized with W F ?
R dv?d h , and ? stands for the Sigmoid activation function , and F t ?
R dv ; ?
a t ?
R n specifies the normalized weights assigned to the key memory slots in K , and a t, i represents the weight associated with the i-th memory slot .
ADD decides how much current information should be written to the memory as the added content : v t, i = ?t , i + a t , i ?
A t , i = 1 , 2 , . . . , n ( 18 ) where A t = ?( W A , h S t ) is parameterized with W A ?
R dv?d h and A t ?
R dv .
By learning the parameters of FORGET and ADD layers , our model can automatically determine which signal to weaken or strengthen based on input utterance words .
Joint Training
The loss function for intent detection is L 1 , and that for slot filling is L 2 , which are defined as cross entropy : L 1 ? m j=1 n I i=1 ? I , i j log y I , i j ( 19 ) and L 2 ? m j=1 n S i=1 ?S , i j log y S , i j ( 20 ) where ? I , i j and ?S , i j are the gold intent label and gold slot label respectively , m is the number of words in a word sequence , and n I and n S are the number of intent label types and the number of slot tag types , respectively .
Finally the joint objective is formulated as weighted - sum of these two loss functions using hyper-parameters ? and ? : L ? = ?L 1 + ?L 2 ( 21 )
Through joint training , the key - value memory shared by those two tasks can learn the shared representations and interactions between them , thus further promoting each other 's performance and easing the error propagation compared with pipeline models .
Experiments 4.1 Setup
To evaluate our proposed model , we conduct experiments on two widely used benchmark datasets , ATIS ( Airline Travel Information System ) and Snips .
Both datesets used in our paper follow the same format and partition as in Goo et al . ( 2018 ) .
ATIS dataset ( Hemphill et al. , 1990 ) contains audio recordings of people making flight reservations .
The training set has 4,478 utterances and the test set contains 893 utterances .
We use another 500 utterances for the development set .
There are 120 slot labels and 21 intent types in the training sets .
To justify the generalization of our proposed mode , we also execute our experiment on another NLU dataset collected by Snips ( Coucke et al. , 2018 ) 1 .
This data is collected from the Snips personal voice assistant , where the number of samples for each intent is approximately the same .
The training set contains 13,804 utterances and the test set contains 700 utterances .
We use another 700 utterances as the development set .
There are 72 slot labels and 7 intent types .
Compared to singledomain ATIS dataset , Snips is more complicated mainly due to the intent diversity and large vocabulary ( Goo et al. , 2018 ) .
For example , GetWeather and BookRestaurant in Snips are from different topics , resulting in a larger vocabulary .
On the other hand , intents in ATIS are all about flight information with similar vocabularies .
In our experiments , we set the dimension of word embedding to 256 for ATIS and 200 for Snips dataset .
L2 reularization used in our model is 1 ? 10 ?6 and dropout ratio is set to 0.4 for reducing overfit .
The number of memory columns is set to 20 for both datasets , and the dimensions of memory column vectors are set to 64 for ATIS , and to 200 for Snips .
The optimizer is Adam ( Kingma and Ba , 2014 ) .
During our experiments , we select the model which works the best on the development set , and then evaluate it on the test set .
We carefully choose some representative works , for example , Joint Seq .
( Hakkani - T? r et al. , 2016 ) , Attention BiRNN ( Liu and Lane , 2016 ) , Sloted - Gated ( Goo et al. , 2018 ) , CAPSULE -NLU ( Zhang et al. , 2019 ) , SF -ID Network ( Niu et al. , 2019 ) and Stack - Propagation ( Qin et al. , 2019 ) as our baselines .
When doing the comparison , we adopt the reported results from those papers directly .
Results
In order to have fair comparison with others ' work , we adopt the same metrics to evaluate our model .
That is , we evaluate slot filling using F1 score , intent prediction using accuracy , and sentence - level semantic frame parsing using whole frame accuracy .
Table 2 shows the experiment results of the proposed model on ATIS and Snips datasets .
From the table , we can see that our model outperforms all the baselines in all three aspects : slot filling ( F1 ) , intent detection ( Acc ) and setence accurancy ( Acc ) , demonstrating that explicitly modeling slot context and strong relationships between slots and intent can benefit SLU effectively from the key -value memory .
In the ATIS dataset , compared with the best prior joint work Stack - Propagation ( Qin et al. , 2019 ) , we achieve F1 score as 96.13 which is even slightly better than Stack - propagation 's F1 score ( 96.10 ) with BERT model .
This signifies that our key - value memory can not only capture long - term slot context , but also model correlation between slot filling and intent detection , which can be further optimized by joint training .
What 's more , in the Snips dataset , our model achieves good results in both slot filling and overall sentence .
Specifically , slot filling was improved by almost 1.0 % , and sentence accuracy by 1.4 % .
Generally , ATIS dataset is a simpler SLU task than Snips , and so the room to be improved is relatively small .
On the other hand , Snips is more complex so that it needs more complicated model to capture long-term context and share the knowledge across different topics .
Analysis From Section 4.2 , we can see good improvements on both datasets , but we want to know how each component impacts SLU performance .
Ablation Study
In this section , we explore how each component contributes to our full model .
Specifically , we ab- late three important scenarios and conduct them in this experiment .
Note that all the variants are based on joint learning .
?
Without key -value memory and gating architecture for integrating slot context information with intent detection .
This is the model similar to Qin et al . ( 2019 ) . ?
Only with key -value memory , but without sharing slot context information with intent detection .
?
With key - value memory and sharing , but without gating architecture , where only key - value memory is applied to model slot context and that information is directly fed into intent detection .
Table 3 shows the joint learning performance of our model on ATIS and Snips datasets by removing one component at one time .
First , if we remove key -value memory and gating architecture , the performance drops dramatically compared with our proposed model .
This is expected as it does not have any of our improvements .
Then we only consider key-value memory to model slot context .
From Table 3 , we can see that key -value memory does improve performance in a large scale .
The result can be interpreted as indicating that key -value memory learns long-term slot context representation effectively , which does compensate the weakness of RNN .
In the following , we apply key -value memory and also share it with intent detection without gating .
It is noticeable that SLU performance is enhanced further .
Sharing slot context information with intent detection not only improves intent accuracy , but also betters slot filling through joint optimization .
Finally , when we add gating mechanism , the performance improves further .
We attribute this to gating mechanism that regulates the degree of slot context information to feed into intent detection task and prevent information from overloading .
We also study how the number of memory slots and the dimension of memory slots impacts SLU performance .
Figure 3 shows the performance change with different hyper-parameters .
We found that the optimal size of memory slots for ATIS and Snips dataset is 20 , whereas the optimal dimension of memory slots is 64 for ATIS and 200 for Snips respectively .
Memory Attention
Analyzing the attention weights has been frequently used to show the memory read-out , since it is an intuitive way to understand the model dynamics .
Figure 4 shows the attention vector for each decoded slot , where each row represents attention vector a t .
Our model has a sharp distribution over the memory , which implies that it is able to select the most related memory slots from the value memory .
For example , when decoding " san " , our model selects memory slot 1 , 7 , 8,15 from the value memory to read context information , where memory slot 7 and 15 are representing word " from " and memory slot 1 representing word " flight " .
In other words , words " flight " and " from " contribute more than other previous words in order to decode " san " to B-fromloc.city_name .
Conclusion
In this paper , we propose a joint model to perform spoken language understanding with an augmented key - value memory to model slot context in order to capture long -term slot information .
In addition , we adopt a gating mechanism to incorporate slot context information for intent classification to improve intent detection performance .
Reciprocally , joint optimization promotes slot filling performance further by memory sharing between those two tasks .
Experiments on two public datasets show the effectiveness of our proposed model and achieve stateof - the - arts results .
Figure 1 : 1 Figure 1 : Framework of the proposed model
