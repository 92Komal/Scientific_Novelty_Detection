title
Generation and Extraction Combined Dialogue State Tracking with Hierarchical Ontology Integration
abstract
Recently , the focus of dialogue state tracking has expanded from single domain to multiple domains .
The task is characterized by the shared slots between domains .
As the scenario gets more complex , the out- of- vocabulary problem also becomes more severe .
Current models are not satisfactory for addressing the challenges of ontology integration between domains and out - of- vocabulary problems .
To address the problem , we explore the hierarchical semantics of the ontology and enhance the interrelation between slots with masked hierarchical attention .
In state value decoding stage , we address the out-of- vocabulary problem by combining generation method and extraction method together .
We evaluate the performance of our model on two representative datasets , MultiWOZ in English and CrossWOZ in Chinese .
The results show that our model yields a significant performance gain over current state - of - the - art state tracking model and it is more robust to out - of- vocabulary problem compared with other methods .
Introduction Dialogue state tracking ( DST ) is in charge of updating the belief state in task - oriented dialogue system ( Gao et al. , 2019a ) .
Traditional discriminative DST models assume that the task ontology is well defined in advance , that is to say , all states and their values are known to the model .
They usually rely on hand-crafted features or taskspecific lexicon ( Henderson et al. , 2014 ; Mrk?i? and Vuli ? , 2018 ) .
An inconvenience is that they are time - consuming and hard to expand to new tasks .
To overcome it , the open vocabulary - based models are proposed to decode the state value according to the dialogue context ( Xu and Hu , 2018 ; .
In recent years , the research frontier for taskoriented dialogue systems has expanded from single domain to multiple domains ( Budzianowski et Zhu et al. , 2020 ; Cheng et al. , 2020 ) .
There come new challenges demanding prompt solution .
Firstly , current model does not sufficiently consider the interrelation between slots in multi-domain scenario .
For example , user asks " I also want to find an attraction near the restaurant " , which implies that the hotel need to have the same area with the restaurant .
The implicit relation between the area slot of hotel and restaurant is the key to exactly track user 's intent .
Prior work simply used the summed or concatenated ( Zhang et al. , 2019 ; Kim et al. , 2020 ) embedding of the domain and slot as the states representation for the decoder .
Secondly , out - of- vocabulary ( OOV ) problem gets more severe since the user asking question with wider entities and more diverse words .
In this paper , we propose the generation and extraction combined method with hierarchical ontology integration , named GeeX , for dialogue state tracking .
First , we explore the hierarchical semantics of the ontology to enhance the representation of slots in multiple domains .
Inspired by Chen et al. ( 2019 ) , we adopt the directed acyclic graph to represent the ontology and enhance the slots interaction between domains with masked hierarchical attention .
We use the ontology of MultiWOZ ( Budzianowski et al. , 2018 ) to illustrate this mechanism .
As shown in figure 1 , the ontology has four First , it enhances the interrelation between slots in multiple domains .
Second , the compact structure is efficient for state representation which is appropriate to domain expansion since new domain often shares slots with old one ( Rastogi et al. , 2020 ) .
To address the OOV problem , we leverage generation and extraction by combining the two methods together .
We first predict the state operation policy to select the suitable decoding strategy .
Then , we enter into the corresponding decoder for value decoding according to the predicted policy .
The contributions of this paper are summarized as follows : ( i ) We adopt the masked hierarchical attention to represent the ontology to enhance the slots interrelation between domains .
( ii ) We combine generation and extraction to handle OOV problem in dialogue state tracking .
( iii ) Experiment results demonstrate that GeeX outperforms state - of - the - art baseline on two representative datasets .
Furthermore , GeeX also shows robustness in OOV testing .
Architecture
We use a four-stage model for state tracking , Figure 2 illustrates the architecture .
Masked Hierarchical Attention
We use a three - layer masked hierarchical attention to explicitly integrate the state information .
Assuming there are M states in total 1 . For the m - th 1 The full states in MultiWOZ and CrossWOZ are listed in Appendix A. state , we use a state-specific mask M m l ?
R | M l | to activate certain gate and only pass through their information to the next level to disentangle the layerwise information 2 .
The state is computed by , Sm l = ? | M l | M l Att ( Sm l?1 , ?l , ?l ) ?
R d h where , ?l = Att ( O l , O l , O l ) ? R | M l |?d h ( 1 ) where Att is the standard scaled dot-product attention ( Vaswani et al. , 2017 ) , l is the layer number , d h is the hidden dimension , | ? | denotes the length number .
O l represents the layer of Domain , Intent and Slot when l = 1 , 2 , 3 , respectively .
The dialogue state is the concatenation of all individual states , i.e. , S = S 1 ? ? ? ? ?
S M , where S m = Sm ?V m , V m is the value of Sm and ? denotes the concatenation operation .
Note that they are shareable among layers , so the hierarchical attention helps to implicitly model the interrelation between states .
Transformer Encoder
We represent the dialogue context as the concatenation of last turn system response D and current turn user utterance U 3 .
At t - th turn , the dialogue context is denoted as C t = D t?1 ? U t .
We use Transformer ( Vaswani et al. , 2017 ) to fuse the state information into dialogue context .
We concatenate last turn state S t?1 and current dialogue context C t as the input , i.e. , X t = [ CLS ] ? S t?1 ? [ SEP ] ?
C t , where [ CLS ] and [ SEP ] are the special token as in ( Devlin et al. , 2019 ) .
In output layer , we get the hidden representation for each of the input tokens .
Particularly , h S m corresponds to Sm representing the information of the m - th state .
Operation Gate
We predict the decoding operation o k ?
O = { CARRYOVER , GENERATE , P m op = Softmax ( W op h S m ) ( 2 ) where , W op ?
R | O|?d h is a learnable parameter .
State Value Decoder
We build two parallel decoders for state value prediction and selectively decode the value for states whose operation policy is GENERATE and EX - TRACT .
For each of the states , when its policy is GENERATE , we execute the generation decoding mode , and when its policy is EXTRACT , we enter into extraction decoding mode .
Generation Decoding
We use Gated Recurrent Units ( GRU ) ( Cho et al. , 2014 ) as the basic decoder and employ copy mechanism to calculate a probability over the dialogue context to encourage reusing words in the context .
We use the state representation h S m ( whose operation policy is GEN - ERATE ) to initialize the decoder hidden .
The final probability of decoding a certain word , e.g. , the ? - th token u( ? ) , is calculated by P m gen ( ? ) = P voc ( ? ) + P copy ( ? ) ( 3 ) where P voc ( ? ) is the probability computed from the decoder hidden over whole vocabulary , and P copy ( ? ) indicates the probability of copying words from the context .
Extraction Decoding
We treat the state value prediction as the extractive reading comprehension problem ( Gao et al. , 2019 b ( Gao et al. , , 2020 .
Specifically , we use the state representation h S m ( whose policy is EXTRACT ) as the query , the dialogue context H as background and the state value as the answer .
The extraction can be formalized as P m s , P m e = EXT ( h S m , H ) ( 4 ) where , P m s and P m e are the start index probability and the end index probability over the dialogue context , respectively .
In implementation , we use the extraction method EXT from .
Learning
We use cross-entropy to compute the operation policy loss and state value decoding loss : Lop = ?
M ? m=1 y m op logP m op Lgen = ? ? GEN ? ? y m gen ( ? ) logP m gen ( ? ) Lext = ? ?
EXT y m s logP m s + y m e logP m e ( 5 ) where y * * is the standard label for P * * .
We adopt multi-task learning to train the model .
The optimization objective is a combination of the three loss function , L = L op + L gen + L ext ( 6 ) 3 Experiment , 2015 ) .
The hidden size is set to 300 .
The learning rate is initialized to 10 ?3 and annealed in the range of [ 10 ?3 , 10 ?5 ] with a decay rate of 0.5 .
Benchmark
We compare GeeX with both discriminative methods i.e. , SUMBT and open vocabulary - based method , i.e. , D-STReader ( Gao et al. , 2019 b ) , TRADE , SOM ( Kim et al. , 2020 ) , TripPy ( Heck et al. , 2020 ) .
To further verify the effectiveness of hierarchical ontology integration and parallel decoding strategy , we also design three ablation models here , ( i ) - MHA .
The performance difference between vanilla extractive model ( i.e. , DSTReader , TripPy ) and Ge-eX mainly comes from the limitation that its decoding vocabulary is limited to the words that occurred in the dialogue history .
For examples , a user may find a cheap restaurant while described it as economical , the extractive model would lose efficacy to predict the right answer span .
GeeX also achieves higher score than the generation decoding models ( i.e. , TRADE , SOM ) .
After further observations , we find that most of the token can be directly extracted from context ( 82.0 % in MultiWOZ2.0 , 84.2 % in MultiWOZ2.1 and 83.7 % in CrossWOZ ) .
The extractive decoding models is more robust to decode longer sequence .
However , the generation decoding method helps to generate values not appearing in the context , so it is a perfect complement to extractive method .
Another performance gain comes from operation prediction .
As stated in ( Kim et al. , 2020 ) , a relatively larger amount of error originates from operation gate .
SOM uses CARRYOVER for states keeping unchanged while neglecting the difference between " none " and succeeding .
GeeX use CARRYOVER for state value succeeding from last turn and NULL for empty value , which help to explicitly take advantage of last turn belief states .
Ablation Study Ablation results are reported in bottom half of Table 1 , the degradation of - MHA , - EXT and - GEN validates the necessity of hierarchical ontology integration and parallel decoding approach .
- EXT outperforms SOM in generation decoding method and - GEN outperforms DSTReader in extraction decoding method , demonstrating that hierarchical ontology integration is effective to promote the slots interaction and lead to the performance boost .
decoding models , the improvement on - MHA further clarifies that the two parallel decoding approaches are complementary to each other .
Figure 3 : Result on OOV testing ( % ) .
We randomly mask the words in value with the probability of 0 % , 25 % , 50 % , 100 % , respectively .
OOV Testing
We simulate OOV instances by randomly masking the value token in dialogue context .
For example , we change ' I would like modern European food ' into ' I would like [ UNK ] European food ' .
Here , we take the three representative models , i.e. , SUMBT , DSTReader and SOM , for comparison .
As shown in Figure 3 , compared with SUMBT , DSTReader and SOM , GeeX still performs well in all OOV rates .
This is actually because the extraction decoder plays a crucial role for predicting OOV tokens , which is also reflected in the smaller performance drop of DSTReader .
In addition , the performance of SOM decreases more sharply as more instances set to be OOV , demonstrating that the copy -augmented model is inflexible to address multiple sequential unknown words .
The worst performance of SUMBT demonstrates that the discriminative model is ill-equipped to recognize unknown tokens .
Conclusion
In the paper , we explore the hierarchical structure of ontology and combine generation and extraction together for state value decoding .
With the domain expanding , supervised learning is not satisfactory for rapidly increasing requirements .
In future work , few -shot learning and knowledge fusion can be applied to further improve domain transferring performance . , only five domains ( restaurant , hotel , attraction , taxi , train ) in MultiWOZ are used in our experiment .
