title
Looking for Confirmations : An Effective and Human-Like Visual Dialogue Strategy
abstract
Generating goal-oriented questions in Visual Dialogue tasks is a challenging and longstanding problem .
State - Of - The - Art systems are shown to generate questions that , although grammatically correct , often lack an effective strategy and sound unnatural to humans .
Inspired by the cognitive literature on information search and cross-situational word learning , we design Confirm - it , a model based on a beam search re-ranking algorithm that guides an effective goal-oriented strategy by asking questions that confirm the model 's conjecture about the referent .
We take the GuessWhat ?!
game as a case-study .
We show that dialogues generated by Confirm - it are more natural and effective than beam search decoding without re-ranking .
Introduction Quite important progress has been made on multimodal conversational systems thanks to the introduction of the Encoder - Decoder framework ( Sutskever et al. , 2014 ) .
The success of these systems can be measured by evaluating them on task - oriented referential games .
Despite the high task -success achieved and the apparent linguistic well - formedness of the single questions , the quality of the generated dialogues , according to surfacelevel features , have been shown to be poor ; this holds for systems based on both greedy and beam search ( e.g. Shekhar et al. ( 2019 ) ; Zarrie ? and Schlangen ( 2018 ) ; Murahari et al. ( 2019 ) ) .
Testoni and Bernardi ( 2021a ) found that when taking these surface - level features as a proxy of linguistic quality , the latter does not correlate with task success , and the authors point to the importance of studying deeper features of dialogue structures .
We aim to develop a multimodal model able to generate dialogues that resemble human dialogue strategies .
Cognitive studies show that humans do not always act as " rational " agents .
When referring to ob- jects , they tend to be overspecific and prefer properties irrespectively of their utility for identifying the referent ( Gatt et al. , 2013 ) ; when searching for information or when learning a language , they tend to follow confirmation - driven strategies .
Modelling such behaviour in language learning , Medina et al . ( 2011 ) and Trueswell et al . ( 2013 ) propose a procedure in which a single hypothesized word-referent pair is maintained across learning instances , and it is abandoned only if the subsequent instance fails to confirm the pairing .
Inspired by these theories , we propose a model , Confirm - it , which generates questions driven by the agent 's confirmation bias .
Take the example of a referential guessing game in which an agent has to ask questions to guess an object in a given image .
Confirm - it will ask questions that reinforce its beliefs about which is the target object , till proven otherwise .
For instance , in Figure 1 , after learning that the target is a living entity ( turn 1 ) , the agent conjectures the target is the dog on the right of the picture ( though in principle , it could have been any of the candidates ) .
Hence , the decoder generates the question that would let it confirm such belief , " is it a dog ? " .
If its expectations are not met ( viz. , it receives a negative answer to such question - turn 2 b ) , it moves its attention to another candidate object .
We do not claim that our choice represents the optimal strategy to play the game , but we believe that it makes the generated dialogue more human-like .
To evaluate this strategy , we take as a test- bed GuessWhat ?!
( de Vries et al. , 2017 ) , a two -player game between a Questioner that has to guess the target , and an Oracle ( called " external Oracle " in the following ) who is aware of the target .
The widely used architecture of the Questioner , GDSE , jointly trains a Question Generator ( QGen ) and a Guesser ( Shekhar et al. , 2019 ) .
We augment this architecture with a module that simulates an internal Oracle .
Being an " internal " Oracle , at test time this agent does not know what the target object is : while at training time it learns to answer questions by receiving the gold standard datapoint ( the question , the actual target , and the human answer ) , at test time it assumes the target is the candidate object to which the Guesser assigns the highest probability .
Hence , the three modules of the Questioner straightforwardly cooperate one another .
The internal Oracle guides the QGen to ask questions that reinforce the Guesser 's beliefs .
Concretely , at training time , through Supervised Learning ( SL ) the QGen learns to ask human-like questions turn - by-turn , the internal Oracle to answer them , and the Guesser to guess the target object once the dialogue ends .
At test time , we implement a beam search re-ranking algorithm that simulates the single - conjecture learning strategy used by humans : among the questions the QGen generates via beam search , the algorithm promotes the questions whose answer ( obtained via the internal Oracle that receives the candidate with the highest probability as the target ) increases the most the model 's confidence in its hypothesis about the target .
We run both quantitative and qualitative analyses , and evaluate the effectiveness of the dialogue strategy by asking human annotators to guess the target object given the dialogues generated by Confirm - it .
We compare results giving the dialogue generated by Confirm - it when using the re-ranking algorithm and when generating the question proposed by the plain beam search .
We show that the task accuracy of both the conversational agent and human subjects increases when receiving the dialogues generated by the Confirm - it re-ranking algorithm .
Related Work
For open-ended language generation , Holtzman et al . ( 2020 ) claim that decoding strategies that op-timize for output with high probability ( like beam search ) lead to highly deteriorated texts , since the highest scores are often assigned to generic , incoherent , and repetitive sequences .
Several works propose reranking strategies on the set of hypotheses produced by the beam search following different criteria ( Du?ek and Jur?ek , 2016 ; Blain et al. , 2017 ; Agarwal et al. , 2018 ; Borgeaud and Emerson , 2020 ; Hargreaves et al. , 2021 ) to improve both the performance on a given task and the quality of the output .
In this work , we present a cognitivelyinspired reranking technique for a visual dialogue questioner agent .
In visual dialogue systems , the quality of the output has been improved mainly by aiming at reducing repetitions in the output .
This goal has been achieved through Reinforcement Learning by adding auxiliary objective functions ( Murahari et al. , 2019 ) , intermediate rewards ( Zhang et al. , 2018 , regularized information gain techniques ( Shukla et al. , 2019 ) , or intermediate probabilities with an attention mechanism ( Pang and Wang , 2020 ) .
Different from these works , we do not use the Reinforcement Learning paradigm and , instead of focusing on improving surface - level features , we indirectly operate on the dialogue structure .
Ruggeri and Lombrozo ( 2015 ) studied the way children and young adults search for information while asking yes -no questions given a set of candidate hypotheses .
The authors found that when prior knowledge favours some hypotheses over others , participants asked more hypothesis-scanning questions ( i.e. , questions that are tentative solutions , with a specific hypothesis that is directly tested ) .
This is in line with the observation in Baron ( 2000 ) that humans phrase questions to receive an affirmative answer that supports their theory , and with the broader finding in Wason ( 1960 ) that they tend to select the information that is in accord with their prior beliefs .
Inspired by these studies , we propose a new dialogue strategy for playing referential guessing games by exploiting the probabilities assigned by the Guesser module to different candidate objects .
Task and Dataset GuessWhat ?!
( de Vries et al. , 2017 ) is an asymmetric game involving two human participants who see a real-world image from MSCOCO ( Lin et al. , 2014 ) .
One of the participants ( the Oracle ) is secretly assigned a target object in the image , while the other participant ( the Questioner ) has to guess it by asking binary ( Yes / No ) questions to the Oracle .
The GuessWhat ?!
dataset consists of more than 150k human-human English dialogues containing on average 5.3 questions per dialogue .
Model and Re-ranking Strategy Our model , Confirm - it , builds on GDSE ( Shekhar et al. , 2019 ) .
In the latter , the hidden state representation produced by a multimodal encoder is used to jointly train the question generator ( QGen ) and the Guesser module .
The image is encoded with a ResNet - 152 network ( He et al. , 2016 ) and the dialogue history is encoded via an LSTM network .
QGen uses greedy search to generate questions .
To this multi-tasking setting , Confirm - it adds an internal Oracle trained to answer the question at each turn .
Moreover , it relies on beam search and , at inference time , it goes through a re-ranking phase which simulates the single - conjecture learning strategy .
The model architecture is provided in the Supplementary Material ( SM ) and the algorithm is spelled out below .
c h ? argmax ( ? p( c k 1:N ) ) 4 : q 1:B ? QGen ( H , I ) 5 : a * 1:B ? IO ( H + q 1:B , c h ) 6 : H * 1:B = H + ( q 1:B , a * 1:B ) 7 : p * ? Guesser ( H * 1:B , I , C 1:N ) 8 : Q ? q 1:B [ argmax ( p * ( c h ) ) ]
9 : Oracle provides an answer A to Q 10 : H ? H + Q + A 11 : end for Algorithm 1 describes the beam search reranking algorithm used by Confirm - it to promote the generation of an effective dialogue strategy .
Given an image , a set of candidate objects , a target object o t , and a beam size of B , at each dialogue turn the model predicts a probability distribution over the set of candidate objects given the current dialogue history .
The candidate that receives the highest probability is considered the model 's hy - pothesis c h .
The QGen outputs B questions , ordered by their probability .
Each of these questions is answered by the model 's internal Oracle that receives c h as the target object .
Among these B questions , Confirm - it selects the question Q that , paired with the answer provided by the internal Oracle , increases the most the model 's confidence over c h , measured as the probability assigned by the Guesser .
The external Oracle ( who is aware of the real target object o t ) answers Q , and this new question - answer pair is appended to the dialogue history .
In SM we provide a step-by-step example of how Confirm - it works .
None of the features of our case-study are crucial for the method to be applied to other tasks , e.g. it does need the questions to be polar , it does not need the questions to be visually grounded , it does not need the dialogue to be asymmetrical .
Implementation details 1 For the multi-task training , we adopt the modulo -n training proposed in Shekhar et al . ( 2019 ) , i.e. we train the Oracle and guesser modules every n ( =7 ) epochs of the QGen .
At inference time , we use a beam size of 3 and let the model generate dialogues of 5 turns .
Results
We study to what extent the re-ranking phase lets the model generate more effective and more natural dialogues .
To this end , we evaluate the Confirm - it task - accuracy with and without the ranking phase 2 and report qualitative analyses of the dialogues .
2 : Human
Accuracy refers to the task accuracy achieved by human annotators when receiving dialogues generated by the plain Beam Search , Confirm - it re-ranking , or the original dialogues produced by human players from the GuessWhat ?!
test set .
The other columns report relevant statistics of the dialogues : percentage of games with at least one repeated question verbatim , hallucination rate ( CHAIR -s ) , percentage of positive answers in the final turn ( % yes Last Turn ) , and percentage of consecutive questions not seen at training time ( lexical overlap , % novel q t?1 , q t per dialogue ) .
Task - accuracy Beam Search : is it food ? < no > is it a glass ? < no > is it a table ? < no > is it a chair ? < no > is it a glass ? < no > Confirm - it : is it food ? < no > is it a utensil ? < yes > is it on the left ? < yes > is it a fork ? < no > is it the knife ?
< yes >
Human :
Is it an utensil ?
< yes >
Is it the pizza cutter ?
< yes >
When the model undergoes the re-ranking phase , Confirm - it accuracy has an increase of + 4.35 % with respect to what it achieves when it outputs the question selected by the plain beam search , and an increase of + 4.8 % against greedy search .
Note that , instead , randomly re-ranking the set of questions lowers the performance .
This result shows that confirmation - driven strategies help generate more effective dialogues .
Interestingly , our re-ranking method does not require additional training compared to the SL paradigm .
More Effective Dialogues
To verify whether the improvement of Confirm - it is really due to the generation of more effective dialogues to solve the guessing task , we asked human subjects to guess the target given a dialogue .
We sampled 500 games from the GuessWhat ?!
test set containing less than 7 candidate objects .
Each participant played 150 games equally divided among dialogues generated by the model with the plain beam search , with our re-ranking strategy , and by the GuessWhat ?!
human players ( taken from the original test set ) .
We made sure no participant played the same game more than once .
In total , 10 English proficient volunteers within our organization joined the experiment .
As we can see from Table 2 , human annota-tors reach an accuracy of 70.8 % in identifying the target object when receiving dialogues generated by beam search and 77 % with Confirm - it , suggesting that the re-ranking phase let the model generate more effective dialogues .
The accuracy that the annotators achieve when playing the game with dialogues extracted from the original GuessWhat ?!
test set ( and thus generated by human players ) is much higher ( 96 % ) .
More Natural Dialogues
Figure 2 reports a sample game that illustrates the difference between a dialogue generated by human players , one generated by the plain beam search , and one by our re-ranking algorithm .
The dialogue generated by beam search contains a repetition ( " is it a glass ? " ) , it asks about entities not present in the image ( " chair " and " glasses " ) and it ends with a nonconclusive negatively answered question .
These features contribute to making the dialogues sound unnatural .
We check whether the re-ranking phase helps our model to get closer to human dialogues with respect to these features .
To this end , we compute the percentage of games with repeated questions and with the last turn containing a positively answered question .
Moreover , we employ CHAIRs to measure the percentage of hallucinated entities in a sequence , originally proposed in Rohrbach et al . ( 2018 ) for image captioning and recently applied also to GuessWhat ?!
( Testoni and Bernardi , 2021 b ) .
CHAIR -S is defined as the number of dialogues with at last one hallucinated entity divided by the total number of dialogues .
As we can see from Table 2 , dialogues generated by Confirm - it contain fewer games with at least one repeated question compared to the beam search setting ( - 8.14 % ) , fewer games with hallucinated entities ( - 2.61 % ) 3 , and more games with the last turn con-taining a positively answered question ( 71.87 % vs. 76.68 % ) .
The reduced number of hallucinations is a direct consequence of the Confirm - it strategy : following up on a single object through the dialogue , the model is less likely to engage in spurious exchanges on irrelevant objects .
Though this strategy continuously looks for confirmations , it is worth noting that it does not increase the number of repetitions , which instead are significantly reduced .
This is an interesting property emerging from the interplay between the internal Oracle and the re-ranking strategy , which suggests that asking the very same question more than once in a dialogue does not increase the model 's confidence in its hypothesis .
Qualitative Analysis of the Strategy
We also evaluated the strategy followed by Confirm - it by looking at the model 's decisions throughout the dialogue .
Interestingly , the model does not select only questions for which it expects a positive answer , though they are the majority ( 67 % ) .
See the SM , for a game in which the re-ranking promoted a question answered negatively by the internal Oracle .
Moreover , though the model looks for confirmations , it properly updates its beliefs when disconfirmed : when the model receives from its interlocutor an answer different from the one it expects ( based on its internal Oracle ) , in 70 % of the cases the Guesser changes the probabilities over the candidates accordingly , i.e. , it assigns the highest probability to a new candidate object .
Finally , the use of a human-like strategy does not imply having learned to simply mimic human dialogues from the training set : the re-ranking shows an absolute increase of + 12 % in the number of pairs of consecutive questions not seen during training ( see Table 2 ) .
Discussion and Conclusion
In this paper , we propose Confirm - it , a multimodal conversational model based on a decoding strategy inspired by cognitive studies of human behavior .
We show that , through the proposed beam search rereanking algorithm , our model generates dialogues that are more effective ( based on task - accuracy ) and more natural ( based on the dialogues features discussed above ) .
We believe further improvement could be obtained by increasing the performance of every single module .
Moreover , the structure of the generated dialogues remains to be analysed , total number of objects mentioned .
4.11 ( Human Dialogues ) . and we agree with van der Lee et al . ( 2021 ) that a proper evaluation should involve humans .
In future work , our method can be easily extended to other task - oriented dialogue tasks which involve a conversational agent as far as it has a module that generates questions and a module that performs a classification task .
Depending on the task at hand , different ways to take intermediate probabilities into account can be designed , but the core idea of the method would not change .
A Supplementary Material Section 4 of the paper describes the Confirm - it model and Figure 3 shows its architecture .
In section 5 ( Qualitative analysis of the strategy ) , we highlight that Confirm - it does not select only questions for which it expects a positive answer , as shown in Figure 5 .
In this case , given the dialogue history H , the model 's hypothesis c h ( the candidate that receives the highest probability according to the Guesser module ) , and a set of questions q 1 , q 2 , q 3 ordered by their probability according to beam search , the question that helps the most ( answered by the internal Oracle taking c h as the target ) is q 2 . Figure 6
We train the model using human dialogues , selecting only the games on which humans have succeed in finding the target object in at most 10 turns ( total number of dialogues used after this pre-processing step : 90 K in training and around 18 K both in validation and testing ) .
The dataset of human dialogues is available at https://guesswhat.ai/download.
H : Is it on the left ?
Yes c h : horse marked in yellow q 1 : Is it in front ?
Yes q 2 : Is it a person ?
No q 3 : Is it on the left ?
Yes
The question that helps the most in confirming c h is q 2 . Confirm - it selects q 2 ( which receives a negative answer according to the internal Oracle ) as the question that helps the most in confirming c h .
Figure 1 : 1 Figure 1 : At turn 2 , among the questions proposed by the beam search , Confirm - it chooses b since it is the most suitable one to confirm the current conjecture .
Algorithm 1 1 The Confirm - it algorithm Require : History H , Beam size B , Max turns T , Image I , Distractors D 1:N ?1 , target o t , Require : Candidates C 1:N ? D 1:N ?1 + o t Require : Internal Oracle IO Require : Target - aware external Oracle 1 : for turn = 1 : T do 2 : ? p( c k 1:N ) ? Guesser ( H , I , C 1:N ) 3 :
Figure 2 : 2 Figure 2 : Through re-ranking , dialogues become more effective and more natural .
The target object is highlighted in green .
Figure 3 : 3 Figure 3 : Model architecture of Confirm-it .
Figure 4 : 4 Figure 4 : Annotation carried out by human participants .
Figure 5 : 5 Figure5 : Given an image and the dialogue history H , Confirm - it assigns the highest probability to c h ( marked in yellow ) .
Beam search generates three questions for the follow - up turn ( ordered by their probability ) : thanks to its internal Oracle , the model anwers each of these questions by taking c h as the target .
Confirm - it selects q 2 ( which receives a negative answer according to the internal Oracle ) as the question that helps the most in confirming c h .
Table 1 : 1 Accuracy results of Confirm- it on the Guess - What ?!
test set when generating 5 questions per dialogue following different decoding strategies .
Confirmit refers to the accuracy achieved by our re-ranking algorithm compare to a random re-ranking of candidate questions , plain beam search , and greedy search .
Decoding Strategy Accuracy 5Q Confirm - it 51.38 Beam Search 47.03 Random Re-Rank 46.29 Greedy Search 46.58
Table Table 1 shows the task accuracy results of our model in the GuessWhat ?! game .
Human Accuracy ? % games with repetitions ?
CHAIR -s ? % Yes Last Turn ? % novel ( qt?1 , qt ) per dialogue ?
Beam Search 70.8 38.50 31.07 71.87 36.75 Confirm - it 77.0 30.36 28.46 76.68 48.25 Human Dialogues 96.0 0.01 7.45 86.64 -
automatically generated text : Current trends and best practice guidelines .
Comput .
Speech Lang. , 67:101151 .
Peter C. Wason . 1960 .
" on the failure to eliminate hypotheses in a conceptual task .
Quarterly Journal of Experimental Psychology , 12( 3):129-140 .
Sina Zarrie ? and David Schlangen .
2018 .
Decoding strategies for neural referring expression generation .
In Proceedings of the 11th International Conference on Natural Language Generation , Tilburg University , The Netherlands , November 5 - 8 , 2018 , pages 503 - 512 .
Association for Computational Linguistics .
Junjie Zhang , Qi Wu , Chunhua Shen , Jian Zhang , Jianfeng Lu , and Anton van den Hengel .
2018 .
Goaloriented visual question generation via intermediate rewards .
In Computer Vision - ECCV 2018 - 15th European Conference , Munich , Germany , September 8-14 , 2018 , Proceedings , Part V , volume 11209 of Lecture Notes in Computer Science , pages 189-204 .
Springer .
Our code is available at : https://github.com/ albertotestoni/ confirm_it 2 Remember that Confirm - it is based on the GDSE architecture .
For comparison , the accuracy reached by GDSE is 45.55 % with greedy search and 46.40 % using beam search decoding .
Rohrbach et al. ( 2018 ) propose another variant of the CHAIR metric called CHAIR - i ( per-instance ) , defined as the number of hallucinated objects in a sequence divided by the
