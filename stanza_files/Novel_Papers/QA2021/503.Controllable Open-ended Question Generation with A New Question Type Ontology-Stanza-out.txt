title
Controllable Open-ended Question Generation with A New Question Type Ontology
abstract
We investigate the less-explored task of generating open-ended questions that are typically answered by multiple sentences .
We first define a new question type ontology which differentiates the nuanced nature of questions better than widely used question words .
A new dataset with 4 , 959 questions is labeled based on the new ontology .
We then propose a novel question type-aware question generation framework , augmented by a semantic graph representation , to jointly predict question focuses and produce the question .
Based on this framework , we further use both exemplars and automatically generated templates to improve controllability and diversity .
Experiments on two newly collected large-scale datasets show that our model improves question quality over competitive comparisons based on automatic metrics .
Human judges also rate our model outputs highly in answerability , coverage of scope , and overall quality .
Finally , our model variants with templates can produce questions with enhanced controllability and diversity .
Introduction Question - asking has long served as an effective instrument for knowledge learning ( Andre , 1979 ; Tobin , 1990 ) and assessing learning progress ( Holme , 2003 ; Downing and Yudkowsky , 2009 ; Livingston , 2009 ) .
Compared to the widely studied task of generating factoid questions that inquire about " one bit " of information ( Du et al. , 2017 ; Duan et al. , 2017 ; Li et al. , 2019 ) , this work is interested in generating open-ended questions that require deep comprehension and long-form answers ( Labutov et al. , 2015 ) .
Such open-ended questions are valuable in education , e.g. , to facilitate complex knowledge acquisition ( Lai et al. , 2017 ) and nurture reasoning skills ( Shapley , 2000 ) , as well as in other applications like improving search engines ( Han Input :
It 's a difficult task to undertake .
Teenagers tend to identify gangs with " fitting " in .
Peer pressure plays a large part in it and sometimes teenagers have problems with their own identity being part of a gang deals with those issues .
It also provides a little bit of respect on the street ...
BART SAMPLING : - How do you stop a teen from joining a gang ?
( PROCEDURAL ) - How do you get teenagers to stop being in gangs ?
( PROCEDURAL ) - How do you get teens out of gangs ?
( PROCEDURAL ) BART + QWORD : - How do you get a teenager out of a gang ?
( PROCEDURAL )
- What is the best way to get teenagers out of gangs ?
( PROCEDURAL ) - Why do teenagers join gangs ?
( CAUSE ) TPLGEN : - How do I get [ NP ]
Figure 1 : Open-ended questions generated by different models after reading the same input : ( 1 ) BART decoded with nucleus sampling , ( 2 ) BART that considers different question words , and ( 3 ) our type - aware generator TPLGEN , that predicts focuses and operates with generated templates ( to the left of the arrows ) .
Questions generated by our model have diverse TYPEs .
et al. , 2019 ) and building open-domain dialogue systems ( Shum et al. , 2018 ) .
Significant progress has been made in generating factoid questions ( Zhang and Bansal , 2019 ; Zhou et al. , 2019 b ; Su et al. , 2020 ) , yet new challenges need to be addressed for open-ended questions .
First , specifying the question type is crucial for constructing meaningful questions ( Graesser et al. , 1992 ) .
Question words such as " why " and " when " are generally seen as being indicative of types ( Zhou et al. , 2019 b ) , but they underspecify the conceptual content of questions ( Olney et al. , 2012 ) .
Using Figure 1 as an example , different question words , i.e. , both " how " and " what " , can be used for inquiring about procedures .
It thus calls for a new question type ontology that can precisely capture the conceptual nature of questions .
Second , constructing questions from a text with multiple sentences needs to focus on its central concepts or phenomena that necessitate extensive descriptions .
New representations are needed to capture such content as question focus ( es ) , to go beyond existing methods that rely on entities and their neighboring words ( Du et al. , 2017 ; Sun et al. , 2018 ) even though they are effective for generating factoid questions .
Third , encouraging the diversity of generated questions ( Sultan et al. , 2020 ; Wang et al. , 2020 ) is less explored but critical for real world applications , e.g. , various questions should be proposed to gauge how well students grasp the knowledge of complex subjects .
In this work , we aim to address the challenges of generating open-ended questions from input consisting of multiple sentences .
We first introduce a new question type ontology , drawn upon researches in cognitive science and psychology ( Graesser et al. , 1992 ) , to capture deeper levels of cognition , such as causal reasoning and judgments .
Based on the new ontology , we collect and annotate a dataset of 4,959 questions to benefit research in both question generation and answering .
1 We then design a type-aware framework to jointly predict question focuses ( what to ask about ) and generate questions ( how to ask it ) .
Different from pipeline - based approaches ( e.g. , Sun et al . ( 2018 ) ) , our framework is built on large pre-trained BART ( Lewis et al. , 2020 ) , and uses shared representations to jointly conduct question focus prediction and question generation while learning taskspecific knowledge .
It is further augmented by a semantic graph that leverages both semantic roles and dependency relations , facilitating long text comprehension to pinpoint salient concepts .
Moreover , to achieve the goal of producing various types of questions from the same input , we investigate two model variants that use templates to improve controllability and generation diversity : one using pre-identified exemplars , the other employing generated templates to guide question writing , with sample outputs displayed in Figure 1 .
For experiments , we collect two new large-scale datasets consisting of open-ended questions with 1 Our data and code are available at : https:// shuyangcao.github.io/projects/ontology_ open_ended_question .
answers from ( 1 ) Yahoo Answers 2 L6 dataset and ( 2 ) popular question - asking communities on Reddit 3 , consisting of 291 K and 720K question - answer pairs , respectively .
Compared to existing popular QA datasets , such as SQuAD ( Rajpurkar et al. , 2016 ) and MS MARCO ( Bajaj et al. , 2016 ) ) , questions in our datasets ask about complex phenomena and perplexing social issues that seek solutions expressed in a long form .
Automatic metrics show that our type-aware question generation model outperforms competitive comparisons , highlighting the effectiveness of semantic graph-augmented representation and joint modeling of focus prediction and question generation .
Human judges also confirm that questions generated by our model have better overall quality .
Adding templates further promotes question diversity , as evaluated by both automatic evaluation and human assessment .
Related Work Question generation has long been studied to reduce human efforts in constructing questions for knowledge learning evaluation ( Mitkov and Ha , 2003 ; Brown et al. , 2005 ) .
Early work relies on syntactic transformation to convert declarative sentences to questions ( Heilman and Smith , 2010 ; Chali and Hasan , 2015 ) .
Recent advancements rely on sequence - to-sequence models to generate a question from a given sentence or paragraph by considering the focus , type , and general -specific relations of questions ( Sun et al. , 2018 ; Zhou et al. , 2019 b ; Krishna and Iyyer , 2019 ) .
In particular , question likelihoods and rewards are designed to steer them toward being addressed by the given answers ( Zhou et al. , 2019a ; Zhang and Bansal , 2019 ) .
Attempts are also made toward creating complex questions that require multi-hop reasoning over the given text , and graph - based representations have been an enabling tool to facilitate the access to both entities and relations ( Pan et al. , 2020 ; Su et al. , 2020 ) .
While our model also enhances the input with a semantic graph , it boasts a richer representation by including both dependency and semantic relations , with predicted question focuses highlighted via extra node embeddings .
Moreover , we create a separate layer of cross attentions that is dedicated to the semantic graph , while prior work uses the same set of attentions to attend to the concatenated text and graph representations .
Given the data-driven nature of question generation and answering tasks , recent studies take advantage of the availability of large-scale QA datasets , such as SQuAD ( Rajpurkar et al. , 2016 ) , MS MARCO ( Bajaj et al. , 2016 ) , HotpotQA ( Yang et al. , 2018 ) , DROP ( Dua et al. , 2019 ) , inter alia .
These corpora mainly contain factoid questions , while our newly collected datasets are not only larger in size but also comprise significantly more open-ended questions for querying reasons and procedures .
A dataset closer to ours is ELI5 , which also obtains open-ended questionanswer pairs from Reddit , while one of our datasets includes more Reddit communities and thus covers a wider range of topics .
Our work is more inline with generating deeper questions with responses that span over multiple sentences , where manually constructed templates are found effective ( Olney et al. , 2012 ) .
For example , Labutov et al. ( 2015 ) use crowdsourcing to collect question templates based on an ontology derived from Wikipedia and Freebase topics .
Different from the topic-based ontology , our question types are more aligned with cognitive levels .
Moreover , our templates are automatically learned from training data .
Recent work Daum ?
III , 2018 , 2019 ) focuses on asking clarification questions based on both retrieval and generation models .
As there has been no suitable framework for diverse types of questions , this work aims to fill the gap by introducing type-aware generation models which optionally leverage question templates for better controllability .
Generating diverse questions is much less studied , with existing approaches mainly focusing on entity replacement ( Cho et al. , 2019 ) , sampling decoding ( Sultan et al. , 2020 ; Wang et al. , 2020 ) , and post-filtering .
However , the produced diversity is driven by word choice and syntax variation , with little ability to control on question types , which is the focus of this work .
3 Data Collection and Question Type Annotation
Open-ended Question Datasets
To collect open-ended questions , we resort to online forums with active question - asking discussions .
Concretely , we gather and clean question - answer pairs from Reddit and Yahoo Answers , to train generators that construct questions by taking the corresponding answer as input .
Question Type Description ( asking for ...)
VERIFICATION the truthfulness of an event or a concept .
DISJUNCTIVE the true one given multiple events or concepts , where comparison among options is not needed .
CONCEPT a definition of an event or a concept .
EXTENT the extent or quantity of an event or a concept .
EXAMPLE example ( s ) or instance ( s ) of an event or a concept .
COMPARISON comparison among multiple events or concepts .
CAUSE the cause or reason for an event or a concept .
CONSEQUENCE the consequences or results of an event .
PROCEDURAL the procedures , tools , or methods by which a certain outcome is achieved .
JUDGMENTAL the opinions of the answerer 's own .
Table 1 : Our new question type ontology , which is adopted and modified from Olney et al . ( 2012 ) .
Types are sorted by levels of cognition ( lower to higher ) .
We choose five popular Reddit communities : r/ AskHistorians , r/ Ask Politics , r/ askscience , r/explainlikeimfive , and r/AskReddit , where open-ended questions are actively asked .
The original posts ( OPs ) are extracted , with their titles becoming questions .
We also keep the best answer with the highest karma ( i.e. , upvotes minus downvotes ) if it is greater than 1 .
A second dataset with question - answer pairs is collected from the Yahoo Answers L6 corpus 4 , which covers a broader range of topics than the Reddit data .
For each question , the best answer is rated by the user who raises the question .
Preprocessing .
To ensure both questions and answers are well - formed , human inspection is conducted in multiple iterations to design rules to filter out improper samples .
For instance , we discard samples whose answers have less than 15 content words to avoid the inclusion of factoid question .
More details are provided in Table 6 in Appendix A. Ultimately , 719,988 question - answer pairs are kept for Reddit , and 290,611 for Yahoo .
Each dataset is then divided into train , validation and test sets with a 90 % / 5 % / 5 % split .
The average lengths of questions and answers are 14.5 and 117.8 for Reddit , and 12.2 and 123.6 for Yahoo .
Question Type Ontology and Annotation Our question type ontology is adopted and modified from Olney et al . ( 2012 ) , where 18 categories are originally proposed for knowledge learning as -6427 sessment .
We recruited 6 native English speakers for three rounds of question type annotation .
Based on the annotators ' feedback after each round , we refine the definitions , merge ambiguous types , and delete inapplicable categories .
For example , an initial EXPECTATION type is merged into CAUSE due to their similarities in seeking causality .
Finally , 10 types are preserved ( Table 1 ) .
As can be seen , our ontology is designed to better capture the nature of questions than question words .
Annotating Questions with Types .
After the annotation guideline is finalized , we ask the same set of annotators to label 5,000 ( 2 ? 2,500 ) randomly sampled questions from both Reddit and Yahoo 's training sets .
Each question is labeled by two annotators , with disagreements resolved through discussions .
After removing samples without consensus , the final dataset consists of 4 , 959 questions .
EXAMPLE questions are most prevalent , comprising 23.4 % of samples , while only 2.6 % are CONSEQUENCE questions .
A Krippendorff's ? of 0.67 is obtained for all samples , indicating a reasonable agreement level .
The annotation guideline and examples for each question type are shown in Table 12 in Appendix A. Training Question Type Classifiers .
Since our type-aware question generation model requires a specified type as input , here we describe how to build two question type classifiers : ( 1 ) ? q , that labels a type by reading the question and is used to provide question type labels during training ; ( 2 ) ? a , that predicts a type for use by taking the answer as input and is used during test .
Both classifiers are based on RoBERTa ( Liu et al. , 2019 ) , where a prediction layer is built on top of the contextual representation of the [ BOS ] token to output question type probabilities .
?
q achieves a macro F1 score of 0.80 on a reserved test set , with data splits detailed in Appendix B .
To train ?
a , in addition to the annotated questions , we run ?
q on unlabeled questions in Reddit and Yahoo and include samples whose type prediction confidence score is above 0.9 .
We train one ? a for each dataset .
? a obtains macro F1 scores of 0.48 and 0.46 on the same reserved test set over all types after training on Yahoo and Reddit , respectively .
After running ?
q on both datasets , we find that Reddit has significantly more EXAMPLE questions ( 43.8 % of all samples ) .
Yahoo dataset is more balanced , with PROCEDURAL questions being the most frequent type ( 19.9 % of all samples ) .
Distri- 4 Type-aware Open-ended Question Generation
In this section , we present our type-aware question generation framework .
As shown in Figure 2 , our model takes in a multi-sentence text and a predicted question type .
Built on shared input representations , it first detects question focuses from a semantic graph , and then generates the question ( ? 4.1 ) .
We also propose two model variants that consider automatically extracted template exemplars or generated templates to achieve controllability ( ? 4.2 ) , enabling the generation of diverse questions .
Joint Focus Prediction and Question Generation ( JOINTGEN )
Our generator is built on top of BART ( Lewis et al. , 2020 ) .
To facilitate the detection of salient content ( i.e. , focuses ) to raise questions , we first augment the encoder with a semantic graph that consists of both dependency relations and semantic roles , capturing semantic relations over different scopes with varying granularities .
Question focuses are first detected based on the semantic graph , which then guide question generation via cross-attentions , as shown in Figure 2 .
Although the joint modeling of focus prediction and question generation has been studied before , our design differs by using shared representations consisting of the input text and semantic graph , and the prediction of focuses are included through gating mechanisms , whereas previous work , e.g. Pan et al . ( 2020 ) , simply employs multi-task learning .
Below , we first describe constructing the semantic graph- augmented encoder , followed by the joint modeling of two tasks .
Improving Long Text Comprehension with Semantic Graph .
To construct the semantic graph , for each sentence , we start with obtaining its dependency tree using Stanford CoreNLP ( Manning et al. , 2014 ) .
To better highlight core concepts , we discard less important relations , e.g. , auxiliaries .
The full list is included in Appendix C .
Since our goal is to detect central concepts that are well connected with many other words , we can remove relations on the edges to minimize the number of parameters to learn .
Moreover , as semantic roles can indicate main entities ( Mannem et al. , 2010 ) , we extract semantic roles and their relations with AllenNLP ( Shi and Lin , 2019 ) .
To merge the two sources of information , we add an edge in the dependency tree to connect the head word of the predicate and the head word of each semantic role .
To build a connected graph from the multi-sentence input , we add an edge between each sentence 's last token and the next sentence 's first token .
Finally , we merge nodes with the same surface forms or with corefered mentions .
To the best of our knowledge , this is the first time that both dependency and semantic relations are encoded in the same graph for question generation , and with enhanced connectivity of the constructed graph , our design can better signal content salience .
Joint Modeling with Cross-attentions .
Given a predicted question type t and a multi-sentence text x = {x 1 , ? ? ? , x n } , the BART encoder builds the contextual representation H = {h 0 , h 1 , ? ? ? , h n } at the last layer , where h 0 is for t .
To encode the semantic graph , we initialize the node representation for node v i by taking the average contextual representations of its tokens and appending four bits encoding the number of nodes ( capped at 10 ) that are merged into v i , to add frequency information .
This step yields new node representations v ( 0 ) i .
We then apply graph attention networks ( GATs ) ( Veli?kovi ?
et al. , 2018 ) of L layers to update the representations as follows : v ( l ) i = j?Ni a i , j W ( l ) v ( l?1 ) j ( 1 ) where W ( l ) is a learnable parameter for the l-th layer , and N i denotes the neighbors of v i .
The attention score a i , j is calculated as in GATs .
We use L = 2 for experiments .
To predict focuses , the final node representation v ( L ) i is fed into the following feedforward network , yielding the probability of v i being a focus as : p f ocus ( v i = 1 ) = ?( W 1 tanh ( W 2 v ( L ) i ) ) ( 2 ) where W 1 and W 2 are learnable parameters .
Bias terms are omitted for simplicity .
We construct ground - truth labels by treating a node as a focus if it contains words used in the question .
To generate the question , we use the gating mechanism to inform the focus prediction results , where new node representations after being weighted by the focus probability are : v ( L ) i = g i v ( L ) i g i = p f ocus ( v i = 1 ) ( 3 )
Our model benefits from both large pre-training and hybrid semantic graphs by adding a separate cross attention for node presentations in each BART decoder layer .
We then design separate cross attentions to attend ( 1 ) the output of the BART encoder , yielding z e , and ( 2 ) the node representations V ( L ) , producing z v , which are formulated as : z e = LN ( z s + Attn( z s , H ) ) ( 4 ) z v = LN ( z e + Attn ( z e , V ( L ) ) ) ( 5 ) z = LN ( z v + FFN ( z v ) ) ( 6 ) where z s denotes the output of self attentions for the current layer , and z is the output for the layer .
Attn( Template Extraction .
While collecting templates specific to a given type , we need to ensure they remain topic-independent to be generalizable to different domains .
To this end , we replace a word in the question with a template token that indicates its syntax function , e.g. , [ V ] for a verb , if it appears in the answer after lemmatization .
We further consider topically related words in the questions , by calculating word-level semantic similarities based on Numberbatch word embeddings ( Speer et al. , 2017 ) , which are found to perform better on our datasets than other embeddings .
Concretely , for each word in the answer , we replace the most similar word in the question with the template token .
This process is repeated until 80 % of content words in questions are replaced .
Finally , for each noun phrase , adjective phrase , and adverb phrase , if its head word has been replaced , the whole phrase is transformed into a phrase type token .
For instance , a question " What are the differences between global warming and climate change ? " becomes
" What are the differences between [ NP ] and [ NP ] ? "
Exemplars for Guidance ( EXPLGEN ) .
Our first model variant considers adding a template exemplar for the given type as additional input , which provide more specific information to control the type of generated questions .
Figure 2 shows one such example .
To identify exemplars , we use templates with frequencies above 20 on Yahoo and 50 on Reddit .
We then manually inspect these templates and remove the ones with topic-specific words , resulting in 66 exemplars for all types .
They are listed in Table 10 in Appendix D. During training , we choose the exemplar that has the lowest edit distance with the question , which is also used for training an exemplar selector based on RoBERTa .
During testing , the exemplar with the highest selector score is used .
The accuracy of the exemplar selector for each question type on the test set is reported in Table 11 in Appendix D.
Generated Templates for Guidance ( TPLGEN ) .
We further propose another model variant where we generate a new template and feed it ( instead of an exemplar template as in EXPLGEN ) as part of the question generation input .
Specifically , we reuse EXPLGEN to learn to generate a target template , as derived from the template extraction procedure .
During question realization , TPLGEN uses a BART - based generator that takes as input the question type , the input text , the generated template , and the words that are predicted as focuses .
We use separate cross attentions to attend the representations of the focused words , similar to how node representations are attended in JOINTGEN .
We recognize that having separate stages of exemplar selection and template generation introduces extra model training cost and potential errors in the pipeline .
This work , however , focuses on improving the controllability as well as diversity of question generation , and we will leave the building of more efficient models in the future work .
Experiment Results
Automatic Evaluation Comparisons and Metrics .
We compare with DEEPQG ( Pan et al. , 2020 ) , a model that uses dependency graphs for multi-hop question generation .
We also compare with BART models that are finetuned on the same datasets as in our models , by using inputs of ( 1 ) the answer ( BART ) , ( 2 ) the answer and a predicted question word ( BART + QWORD ) , and ( 3 ) the answer and a predicted question type ( BART + QTYPE ) .
For BART + QWORD , the question word is predicted by a RoBERTa classifier that considers the answer and is trained on our training sets .
We follow and use 9 categories of question words .
For both our models and BART + QTYPE , the most confident type predicted by the classifier ?
a ( described in ? 3.2 ) , which reads in the answer , is used as input .
To test the efficacy of semantic graphs , we further compare with a variant of JOINTGEN that only uses the flat Transformer for focus prediction and question generation , denoted as JOINTGEN w/o graph .
We evaluate the generated questions with BLEU ( Papineni et al. , 2002 ) , METEOR ( Lavie and Agarwal , 2007 ) , and ROUGE -L ( Lin , 2004 ) .
5 Results on both Yahoo and Reddit datasets are reported in Table 2 .
Our JOINTGEN outperforms all comparisons on both datasets over all automatic evaluation metrics except for METEOR on Reddit .
When taking out the semantic graphs , model performance degrades substantially , which suggests that Table 3 : Automatic evaluation on controllability and diversity by specifying 9 different question types .
We report type accuracy ( Acc ) , number of unique types ( UnT ) , and pairwise BLEU - 4 ( Pair ) .
Our EXPLGEN and TPLGEN achieve stronger controllability by respecting the given question types more , as well as show higher diversity than comparisons except for BART with nucleus sampling .
* : significantly better than all comparisons ( p < 0.005 ) .
having structured representation is useful for focus detection and the final question generation task .
We also observe a huge performance gap between DEEPQG and systems based on BART , signifying the importance of leveraging pre-trained models for open-ended question generation .
Meanwhile , adding question types helps BART generate more relevant questions than using question words , indicating the value of our new question type ontology .
Notably , our template - based generators , EX - PLGEN and TPLGEN , which are trained to comply with the given templates , still produce comparable scores .
This highlights the possibility to control the generated questions ' types and syntax as demonstrated by the templates , without performance loss .
Question Diversity Evaluation .
Next , we exam - 2 3 4 5 6 7 8 9 # of Given Types ine the controllability of models by specifying different question types as input .
The top 9 confident types 6 predicted by our type predictor ?
a are used as input to our models , producing 9 questions for evaluation .
For BART , we use nucleus sampling ( Holtzman et al. , 2020 ) with k = 10 and p = 0.7 to sample diverse questions .
To evaluate , we first calculate the question type accuracy by comparing whether the types of the generated questions match the specified ones , with types labeled by our classifier ? q ( ? 3.2 ) .
We then report the average numbers of unique question types in the 9 generated questions per sample , with higher number indicating better controllability .
Finally , we consider pairwise BLEU - 4 ( Cho et al. , 2019 ) by computing the BLEU - 4 between pairwise generated questions per sample , where lower values suggest higher content diversity .
First , our EXPLGEN and TPLGEN can generate questions with diverse types and content , as shown by the significantly higher numbers of unique types than all comparisons and lower pairwise BLEU scores than comparisons except for BART with nucleus sampling in Table 3 .
This implies stronger type control by template - based generators , compared to BART + QTYPE and JOINTGEN which only use the question type token as input .
Results on numbers of unique types by varying numbers of question types specified in the input are displayed in Figure 3 , where EXPLGEN and TPLGEN maintain steady controllability .
Second , our question type ontology provides a new perspective for question diversity evaluation .
Among the comparisons , although BART with nucleus sampling and BART + QWORD both have low pairwise BLEU , the types of questions they can generate are limited .
Human Evaluation Question Diversity .
We hire three annotators who have participated in our question type annotation study to evaluate 80 groups of questions generated by four selected models on each dataset .
For each group , we randomly sample an answer and indicate three most probably question types to each model , to generate three corresponding questions .
For each sample , the annotators are asked to rank the four models from 1 ( highest ) to 4 ( lowest ) on three aspects of diversities : type-whether the three generated questions have different types , syntax - whether they use different syntax , and answer content - whether the three questions need to be addressed with different answers .
Ties are allowed .
We find that human judges rate questions generated by our EXPLGEN and TPLGEN as having greater diversities over all aspects , except for syntax diversity on Reddit , as shown in Table 4 .
Among the two model variants , questions by TPLGEN yield more diverse answers .
Based on our observation , TPLGEN uses automatically generated templates to produce more focused questions with different answers , compared to EXPLGEN which employs exemplars .
This shows the promise of using automatically generated templates to create questions that need to be addressed with different answers .
Besides Figure 1 , we show more sample outputs in Figure 4 , where EXPLGEN and TPLGEN exhibit stronger controllability than JOINTGEN .
Question Content Quality .
We use the same set of human judges to evaluate another 80 groups of questions output by five selected models and the reference .
Three aspects are rated from 1 ( worst ) Answer :
My sister in law and her husband " genetically modified " their second child because the first has EB .
They eliminated that and had a baby that gets to live pain free .
Under the right circumstances , I 'm all for it ...
Figure 4 : Sample outputs of our models given different question types .
Spans that belong to the exemplars or the generated templates are colored with blue .
Generated questions that do not match the given type are marked by strikethrough .
to 5 ( best ) : appropriateness - whether the question is semantically correct , without considering the answer ; answerability - whether the question can be addressed by the given answer ; and scopewhether the question is related to a longer span of the answer ( global scope ) or focuses on local content ( e.g. , one phrase or one sentence ) .
We further ask the annotators to rank questions based on their overall quality and preferences , with ties allowed .
As shown in Table 5 , our JOINTGEN model produces questions with better answerability and that cover broader content in the answers .
It is also rated as the best in more than half of the evaluation instances on both datasets .
Between BART + QWORD and BART + QTYPE , human judges rate the system outputs that conditioned on our question types to have better overall quality .
Further Analyses
Does focus prediction correlate with question quality ?
We first investigate the relationship between focus prediction and question generation by using our joint model JOINTGEN .
As can be seen from Figure 5 , there is a strong correlation between F1 scores of focus prediction and BLEU - 4 as well We also show the F1 scores and BLEU - 4 for selected question types on the right of Figure 5 , again demonstrating the effect of focus detection on question quality .
When do our models fail to respect the given types ?
Next , we provide insights into which types of questions are challenging to generate by using our template - based models EXPLGEN and TPLGEN .
Both variants frequently fail to respect the given question type of VERIFICATION , in which cases they often produce JUDGEMENTAL questions .
They also tend to confuse EXAMPLE and EXTENT with CONCEPT questions .
After manually inspecting 50 generated questions for the aforementioned three types , we find that many of them can be labeled with both types , thus creating confusion for our classifier .
For instance , " What are the import restrictions in the US ? " can be considered as either
Conclusion
We present a new question type ontology which better captures the nuances of questions to support the study of open-ended question generation .
We further annotate a new dataset with 4,959 questions based on the proposed ontology .
We describe a joint question focus detection and question generation framework with a novel semantic graphaugmented representation , which is directly built on large pre-trained models .
Based on this framework , we also enhance the controllability and diversity of generated questions by employing template exemplars or automatically generated templates .
Experiments on two large datasets show that questions generated by our models have better quality and higher diversity than non-trivial comparisons , with similar results rated by human judges .
