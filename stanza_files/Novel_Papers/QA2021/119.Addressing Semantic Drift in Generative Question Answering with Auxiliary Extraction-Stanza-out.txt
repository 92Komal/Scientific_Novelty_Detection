title
Addressing Semantic Drift in Generative Question Answering with Auxiliary Extraction
abstract
Recently , question answering ( QA ) based on machine reading comprehension has become popular .
This work focuses on generative QA which aims to generate an abstractive answer to a given question instead of extracting an answer span from a provided passage .
Generative QA often suffers from two critical problems : ( 1 ) summarizing content irrelevant to a given question , ( 2 ) drifting away from a correct answer during generation .
In this paper , we address these problems by a novel Rationale - Enriched Answer Generator ( REAG ) , which incorporates an extractive mechanism into a generative model .
Specifically , we add an extraction task on the encoder to obtain the rationale for an answer , which is the most relevant piece of text in an input document to a given question .
Based on the extracted rationale and original input , the decoder is expected to generate an answer with high confidence .
We jointly train REAG on the MS MARCO QA + NLG task and the experimental results show that REAG improves the quality and semantic accuracy of answers over baseline models .
Introduction Question Answering ( QA ) has come a long way from answer sentence selection , relationship QA to machine reading comprehension ( MRC ) .
Recently , QA has become an essential problem in natural language understanding and a major milestone towards human-level machine intelligence .
Current mainstream approaches ( Chen et al. , 2017 ; treat MRC as a process of extracting a consecutive piece of text from a document to a given question .
Despite the great success in extractive MRC , in real-world applications , correct answers may span different Question does gameplay programmer need math skill
Passage
A good computer programmer is more of a problem solver and logical thinker than a math buff .
Besides , the industry is peppered with many computer programmers who do not really know much about mathematics .
Gold no , gameplay programmer does not need math skill .
PALM yes , gameplay programmer is a math buff .
REAG no , gameplay programmer does not need math skill .
Table 1 : An example of the " semantic drift " issue in generative reading comprehension from the MARCO dataset ( Nguyen et al. , 2016 ) .
The text span of words in blue is the rationale extracted by REAG .
passages or even not be literally present in the passages .
Directly extracting a consecutive answer span is often inadequate .
Therefore , the ability of generating an abstractive answer is needed , which requires a QA model to summarize the main content in a paragraph that is relevant to a given question .
Answering questions in natural language can be beneficial to a variety of QA applications , and has led to the development of smart devices such as Siri , Cortana and Alexa .
However , compared with answer extraction , answer generation for reading comprehension is more challenging , and has been less explored .
A major challenge in generative reading comprehension comes from out - of- control generation of abstractive answers .
Although much work has been done in neural language generation ( NLG ) , e.g. , KIGN ( Li et al. , 2018 ) for summarization , out - of- control generation remains an open question for generative QA which aims to produce correct and coherent answers .
Specifically , we observed that generative models often generate answers semantically drifting away from the given passage and question , known as the " semantic drift " problem .
As shown in Table 1 , the baseline generative model PALM ( Bi et al. , 2020 ) generates an answer that has almost contrary semantics with the gold answer .
In general , a generative model often suffers from two critical problems : ( 1 ) summarizing content irrelevant to a given question , and ( 2 ) drifting away from a correct answer during generation .
In this paper , we address these problems by a novel Rationale - Enriched Answer Generator ( REAG ) , which incorporates an extractive mechanism into a generative model in order to leverage relevant information to a given question in the contextual passage .
Specifically , we add an extraction task on the encoder to obtain the rationale for an answer , which is the most relevant piece of text in an input document to the given question .
On one hand , the introduction of the supervised extraction task enables the encoder to learn the relevance between a question and a passage ;
On the other hand , the extracted rationale can be further used to guide the answer generation .
Based on the extracted rationale and original input , the decoder is expected to summarize content relevant to a given question and generates an answer with high confidence .
Finally , we jointly train REAG on the MS MARCO QA + NLG task based on the common bottom layers .
The experimental results show that REAG improves the semantic accuracy of answers over the other state - of - the - art models .
2 Related Work
Machine Reading Comprehension
In recent years , machine reading comprehension has made great progress with the development of SQuAD ( Rajpurkar et al. , 2016 ) and MS MARCO ( Nguyen et al. , 2016 ) .
The current mainstream studies treat machine reading comprehension as answer span extraction from one passage ( Rajpurkar et al. , 2016 ( Rajpurkar et al. , , 2018 or multi-passages ( Nguyen et al. , 2016 ) , which is usually done by predicting the start and end position of an answer .
SLQA improved answer quality with a hierarchical attention fusion network , which conducted attention and fusion horizontally and vertically across layers between a passage and a question .
Recently , the BERT model Devlin et al . ( 2019 ) has proved effective for reading comprehension via unsupervised pre-training .
Generative Reading Comprehension Bi et al. ( 2019 ) proposed a Knowledge-Enriched Answer Generator ( KEAG ) to compose a natural answer by exploiting and aggregating evidence from all four information sources available : question , passage , vocabulary and knowledge .
Nishida et al. ( 2019a ) proposed a multi-style generative model to generate an abstractive summary from the given question , passages and multi-style .
Reliable Text Generation Compared with answer extraction , answer generation for reading comprehension is more challenging , and the major challenge in generative reading comprehension lies in out - of- control generation .
Recently , some studies have been carried out on increasing the reliability of generation in the encoder-decoder framework ( Liu et al. , 2018 ; Li et al. , 2018 ) . 3 Rationale-Enriched Answer Generation
Rationale Span Extraction
In a generative reading comprehension task , every answer has its corresponding rationale , an extractive span in the passage , which can be derived by matching the passage text with the answer .
The rationale can usually be located in a certain continuous area of the passage .
We use continuous text span as the rationale to minimize the difficulty of the extraction task .
Compared with the gold answer , the text span with the highest F1 - score in passage is identified as the rationale for training supervision .
Based on the identified rationale , we introduce a rationale extraction task into the encoder .
It enables the encoder to learn the relevance between the input question and the passage .
Specifically , the encoder predicts whether each token of the passage should be included in the rationale .
Every token in the rationale is labeled by 1 and the rest is labeled by 0 .
Given input question Q and passage P , we first concatenate them together into an input sequence X = {x 1 , x 2 , ... , x N }.
Then we use a shared word embedding layer to project each of the vectors into d-dimensional vectors , and add to each the corresponding position embedding .
The resulting vectors are then fed into the Transformer encoder to map the text into a sequence of encoder hidden states {h 1 , h 2 , ... , h N }.
The encoder hidden states can be used to predict whether each token of the passage should be included in the rationale .
Therefore , we add a fully connected layer with the sigmoid activation on top of the encoder , to compute the probability for each input word : p r i = sigmoid ( w 1 ? relu ( W 2 h i ) ) ( 1 ) where h i ?
R d is the output hidden state of the encoder for the i th token .
This gives the probability p r i that the i th token should be included in the rationale .
We then calculate the averaged cross entropy , similar to ( Ju et al. , 2019 ) , for the rationale extraction loss : L RE j = ?
1 N N i=1 ( y r ji log p r ji + ( 1? y r ji ) log ( 1 ? p r ji ) ) , ( 2 ) where N is the number of input tokens .
y r i is the rationale label for the i th token , and L RE j represents the rationale loss for the j th example in the training set .
Rationale-Enriched Answer Generation
This layer uses a stack of Transformer decoder blocks on top of the embeddings provided by the encoder 's word embedding layer .
The decoder is similar in structure to the encoder except that it includes standard attention mechanism after each self-attention layer that attends to the output of the encoder .
The rationale - aware hidden states output by the encoder are used for rationale extraction .
In calculating the decoder states s t , an cross attention is introduced into the decoder to attend to the rationale - aware encoder hidden states .
This results in the rationale - aware decoder hidden state s t : p(y t |y 1 , ... , y t?1 ) = softmax ( W e ( W v s t + b v ) + b e ) ( 3 ) During training , we minimize the negative loglikelihood of the answer word at each decoding time step .
Let y * t denote the target word in the decoding time step t.
The overall loss is then defined as : L GEN = ?
1 T T t=1 log p(y * t |y * 1 , ... , y * t?1 , x , ? ) ( 4 ) where T denotes the length of a gold answer .
Joint Training and Prediction
The rationale extraction task and the answer generation task are designed to share the same embedding and the encoder .
Therefore , we propose to train them together as multi-task learning .
The joint objective function is formulated as follows : L = L GEN + ?L RE ( 5 ) where ( Bi et al. , 2020 ) , an encoder-decoder generative language model pre-trained on a large corpus .
It consists of a 12 - layer encoder and 12layer decoder with 768 embedding / hidden size , 3072 feed - forward filter size and 12 attention heads .
REAG is trained with a dropout of 0.1 on all layers and attention weights .
During training and testing , we truncate the text to 512 tokens and limit the length of the answer to 50 tokens .
At test time , answers are generated using beam search with a beam size 5 . ?
Model Comparisons
Table 2 gives the comparison of other state- of- theart QA models on the MARCO Q&A + NLG dataset in ROUGE -L and BLEU -1 .
From this table , we observe that generative QA models ( e.g. , REAG , PALM ) are consistently superior to extractive models ( e.g. , BiDAF ) in answer quality .
Therefore , generative QA models establish a strong base architecture to be enhanced with the extra signals , which motivates this work .
Among the generative models , REAG outperforms all the other state- ofthe - art models with an improvement of over 2.8 % BLEU - 1 point and 1.1 % ROUGE -L.
Part of the results in the Table 2 are from ( Bi et al. , 2019 ) , which re-running other researchers ' code .
Ablation Study
We conduct ablation studies to assess the individual contribution of every component in REAG .
In addition , we ablate the linear-decay jointtraining which proves to be critical with over 0.5 % drops on the metrics after the ablation .
In order to exclude the influence of the pre-trained model , we ablate pre-training , retaining the rationale -span extraction .
This ablation leads to a drop from 70.98 to 69.54 on Rouge -L , which demonstrates the power of REAG in generating high-quality answers without pre-training .
Quantitative Analysis on Semantic Drift
For generative reading comprehension , it is difficult to make the answer completely correct , because even if the semantics are correct , there may be some expression differences from the gold answer .
Since neither ROUGE -L nor BLEU - 1 can measure it , we conduct a human evaluation of the semantic accuracy .
We randomly select 100 questions from the MARCO dev set , and manually evaluate whether the generated answers to these questions are semantically drifted .
Question can a child get a flu vaccine under 6 months ?
Gold Answer
No , a child under 6 months ca n't be given a flu vaccine .
PALM Answer Yes , a child can get a flu vaccine under 6 months .
REAG Answer
No , a child cannot get a flu vaccine under 6 months .
Question what is the sales tax in california
Gold Answer
The sales tax in California is 8 % .
PALM Answer
The sales tax in California is 7.625 %
REAG Answer
The sales tax in California is 8 % REAG in ROUGE -L and BLEU -1 .
As shown in Table 5 , the generated answers are strongly correlated with the rationales , demonstrating the effectiveness of leveraging the rationale signal .
Also , the fact that the gold answers have a lower agreement with the rationales indicates that a generative model , as opposed to an extractive one , is needed for the MARCO Q&A + NLG task .
Case Study
Table 6 gives two examples to show the answers generated by the REAG model and the PALM model .
In addition to the answers , we provide the rationales predicted by REAG 's encoder to demonstrate the effectiveness of rationale extraction .
In both examples , the rationale extraction module identifies the correct rationales , e.g. , Flu shots are not made for children under the age of 6 months .
and California ( 8 % ) .
In Example 1 , PALM is confused by the noise " Yes " in the beginning of the passage , which leads to the contrary semantics of its generated answer .
With the correctly extracted rationale , our REAG model generates an answer semantically consistent with the gold answer .
In Example 2 , PALM fails to identify a correct sales tax rate 8 % for California , so the response is incorrect and useless , even if it results in high ROUGE and BLEU scores against the gold answer .
In contrast , based on the extracted rationale California ( 8 % ) , our REAG generates a semantically correct answer .
Conclusion and Future Work
This paper presents a novel model REAG that is designed to incorporate an extractive mechanism into a generative QA model .
REAG introduces a new task on the encoder to extract rationales .
Based on these rationales and original input , a rationaleenriched decoder is proposed to generate an answer with high confidence .
The experimental results show that REAG significantly improves the quality and semantic accuracy of generated answers over state - of - the - art models .
is a hyper-parameter that controls the weight of the rationale extraction task .
During the training process , we use a linear decay schedule on the value of ? , in order to rely more on the rationale extraction task for addressing the semantic drift problem at the early stage , following by more focus on the target generation task subsequently .
4 Experiments 4.1 Experiment Configuration Dataset and Evaluation Metric .
Given our objec- tive of generating natural answers by document reading , the MARCO dataset 1 ( Nguyen et al. , 2016 ) released by Microsoft is a good fit for bench - marking REAG and other answer generation meth - ods .
We use the latest MARCO V2.1 dataset and focus on the " QA + Natural Language Generation " task in the evaluation .
The data has been split into a training set ( 150 k QA pairs ) , a dev set ( 12 k QA pairs ) and a test set ( 110 k questions ) .
Since true answers are not available in the test set and the task is retired now , we hold out the dev set for evalua - tion in our experiments , and test models for each question on its associated passages by concatenat - ing them all together .
Following Bi et al. ( 2019 ) , we tune the hyper-parameters by cross-validation on the training set .
Implementation Details .
Our REAG is based on PALM
