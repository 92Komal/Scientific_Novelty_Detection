title
Simple Entity -Centric Questions Challenge Dense Retrievers
abstract
Open-domain question answering has exploded in popularity recently due to the success of dense retrieval models , which have surpassed sparse models using only a few supervised training examples .
However , in this paper , we demonstrate current dense models are not yet the holy grail of retrieval .
We first construct Entity Questions , a set of simple , entityrich questions based on facts from Wikidata ( e.g. , " Where was Arve Furset born ? " ) , and observe that dense retrievers drastically underperform sparse methods .
We investigate this issue and uncover that dense retrievers can only generalize to common entities unless the question pattern is explicitly observed during training .
We discuss two simple solutions towards addressing this critical problem .
First , we demonstrate that data augmentation is unable to fix the generalization problem .
Second , we argue a more robust passage encoder helps facilitate better question adaptation using specialized question encoders .
We hope our work can shed light on the challenges in creating a robust , universal dense retriever that works well across different input distributions .
1 * The first two authors contributed equally .
1 Our dataset and code are publicly available at https : // github.com/princeton-nlp/EntityQuestions .
Introduction Recent dense passage retrievers outperform traditional sparse retrieval methods like TF -IDF and BM25 ( Robertson and Zaragoza , 2009 ) by a large margin on popular question answering datasets , Guu et al.
2020 , Karpukhin et al. 2020 , Xiong et al. 2021 .
These dense models are trained using supervised datasets and the dense passage retriever ( DPR ) model ( Karpukhin et al. , 2020 ) demonstrates that only training 1,000 supervised examples on top of BERT 1 : Top - 20 retrieval accuracy for dense and sparse retrieval models on Natural Questions ( Kwiatkowski et al. , 2019 ) and our Entity Questions along with a set of sampled questions ( full results in Appendix A ) .
We test two DPR models : ( 1 ) trained on NQ only ; ( 2 ) trained on 4 datasets ( NQ , TQA , WebQ , TREC ) combined .
2
In this work , we argue that dense retrieval models are not yet robust enough to replace sparse methods , and investigate some of the key shortcomings dense retrievers still face .
We first construct Entity Questions , an evaluation benchmark of simple , entity -centric questions like " Where was Arve Furset born ? " , and show dense retrieval methods generalize very poorly .
As shown in Table 1 , a DPR model trained on either a single dataset Natural Questions ( NQ ) ( Kwiatkowski et al. , 2019 ) or a combination of common QA datasets drastically underperforms the sparse BM25 baseline ( 49.7 % vs 71.2 % on average ) , with the gap on some question patterns reaching 60 % absolute !
Based on these results , we perform a deep dive into why a single dense model performs so poorly on these simple questions .
We decouple the two distinct aspects of these questions : the entities and the question pattern , and identify what about these questions gives dense models such a hard time .
We discover the dense model is only able to successfully answer questions based on common entities , quickly degrading on rarer entities .
We also observe that dense models can generalize to unseen entities only when the question pattern is explicitly observed during training .
We end with two investigations of practical solutions towards addressing this crucial problem .
First , we consider data augmentation and analyze the trade- off between single - and multi-task finetuning .
Second , we consider a single fixed passage index and fine-tune specialized question encoders , leading to memory - efficient transfer to new questions .
We find that data augmentation , while able to close gaps on a single domain , is unable to consistently improve performance on unseen domains .
We also find that building a robust passage encoder is crucial in order to successfully adapt to new domains .
We view this study as one important step towards building universal dense retrieval models .
Background and Related Work Sparse retrieval
Before the emergence of dense retrievers , traditional sparse retrievers such as TF - IDF or BM25 were the de facto method in opendomain question - answering systems ( Chen et al. , 2017 ; Yang et al. , 2019 ) .
These sparse models measure similarity using weighted term-matching between questions and passages and do not train on a particular data distribution .
It is well - known that sparse models are great at lexical matching , but fail to capture synonyms and paraphrases .
Dense retrieval
On the contrary , dense models Karpukhin et al. , 2020 ; Guu et al. , 2020 ) measure similarity using learned representations from supervised QA datasets , leveraging pre-trained language models like BERT .
In this paper , we use the popular dense passage retriever ( DPR ) model ( Karpukhin et al. , 2020 ) as our main evaluation , 3 and we also report the evaluation of REALM ( Guu et al. , 2020 ) in Appendix A. DPR models the retrieval problem using two encoders , namely the question and the passage encoders , initialized using BERT .
DPR uses a contrastive objective during training , with in - batch negatives and hard negatives mined from BM25 .
During inference , a pre-defined large set of passages ( e.g. , 21million passages in English Wikipedia ) are encoded and pre-indexed - for any test question , the top passages with the highest similarity scores are returned .
Recently , other advances have been made in improving dense retrieval , including incorporating better hard negatives ( Xiong et al. , 2021 ; Qu et al. , 2021 ) , or fine- grained phrase retrieval ( Lee et al. , 2021 ) .
We leave them for future investigation .
Generalization problem
Despite the impressive in- domain performance of dense retrievers , their capability of generalizing to unseen questions still remains relatively under-explored .
Recently , Lewis et al. ( 2021a ) discover that there is a large overlap between training and testing sets on popular QA benchmarks , concluding that current models tend to memorize training questions and perform significantly worse on non-overlapping questions .
AmbER test sets are designed to study the entity disambiguation capacities of passage retrievers and entity linkers .
They find models perform much worse on rare entities compared to common entities .
Similar to this work , our results show dense retrieval models generalize poorly , especially on rare entities .
We further conduct a series of analyses to dissect the problem and investigate potential approaches for learning robust dense retrieval models .
Finally , another concurrent work ( Thakur et al. , 2021 ) introduces the BEIR benchmark for zero-shot evaluation of retrieval models and shows that dense retrieval models underperform BM25 on most of their datasets .
Entity Questions
In this section , we build a new benchmark Enti-ty Questions , a set of simple , entity -centric questions and compare dense and sparse retrievers .
Dataset collection
We select 24 common relations from Wikidata ( Vrande ?i? and Kr?tzsch , 2014 ) and convert fact ( subject , relation , object ) triples into natural language questions using manually defined templates ( Appendix A ) .
To ensure the converted natural language questions are answerable from Wikipedia , we sample triples from the T-REx dataset ( Elsahar et al. , 2018 ) , where triples are aligned with a sentence as evidence in Wikipedia .
We select relations following the criteria : ( 1 ) there are enough triples ( > 2 k ) in the T-REx ; ( 2 ) it is easy enough to formulate clear questions for the relation ; ( 3 ) we do not select relations with only a few answer candidates ( e.g. , gender ) , which may cause too many false negatives when we evaluate the retriever ; ( 4 ) we include both person-related relations ( e.g. , place -of- birth ) and non-person relations ( e.g. , headquarter ) .
For each relation , we randomly sample up to 1,000 facts to form the evaluation set .
We report the averaged accuracy over all relations of Entity Questions .
Results
We evaluate DPR and BM25 on the En-tity Questions dataset and report results in Table 1 ( see full results and examples in Appendix A ) .
DPR trained on NQ significantly underperforms BM25 on almost all sets of questions .
For example , on the question " Where was [ E ] born ? " , BM25 outperforms DPR by 49.8 % absolute using top - 20 retrieval accuracy .
4 Although training DPR on multiple datasets can improve the performance ( i.e. , from 49.7 % to 56.7 % on average ) , it still clearly pales in comparison to BM25 .
We note the gaps are especially large on questions about person entities .
In order to test the generality of our findings , we also evaluate the retrieval performance of REALM ( Guu et al. , 2020 ) on Entity Questions .
Compared to DPR , REALM adopts a pre-training task called salient span masking ( SSM ) , along with an inverse cloze task from .
We include the evaluation results in Appendix A. 5 We find that REALM still scores much lower than BM25 over all relations ( 19.6 % on average ) .
This suggests that incorporating pre-training tasks such as SSM still does not solve the generalization problem on these simple entity -centric questions .
In this section , we investigate why dense retrievers do not perform well on these questions .
Specifically , we want to understand whether the poor generalization should be attributed to ( a ) novel entities , or ( b ) unseen question patterns .
To do this , we study DPR trained on the NQ dataset and evaluate on three representative question templates : placeof - birth , headquarter , and creator .
6
Dense retrievers exhibit popularity bias
We first determine how the entity [ E ] in the question affects DPR 's ability to retrieve relevant passages .
To do this , we consider all triples in Wikidata that are associated with a particular relation , and order them based on frequency of the subject entity in Wikipedia .
In our analysis , we use the Wikipedia hyperlink count as a proxy for an entity 's frequency .
Next , we group the triples into 8 buckets such that each bucket has approximately the same cumulative frequency .
Using these buckets , we consider two new evaluation sets for each relation .
The first ( denoted " rand ent " ) randomly samples at most 1,000 triples from each bucket .
The second ( denoted " train ent " ) selects all triples within each bucket that have subject entities observed in questions within the NQ training set , as identified by ELQ ( Li et al. , 2020 ) .
We evaluate DPR and BM25 on these evaluation sets and plot the top -20 accuracy in Figure 1 . DPR performs well on the most common entities but quickly degrades on rarer entities , while BM25 is less sensitive to entity frequency .
It is also notable that DPR performs generally better on entities seen during NQ training than on randomly selected entities .
This suggests that DPR representations are much better at representing the most common entities as well as entities observed during training .
Observing questions helps generalization
We next investigate whether DPR generalizes to unseen entities when trained on the question pattern .
For each relation considered , we build a training set with at most 8 , 000 triples .
We ensure no tokens from training triples overlap with tokens from triples in the corresponding test set .
In addition to using the question template used during evaluation to generate training questions , we also build a training set based on a syntactically different but semantically equal question template .
7
We fine- tune DPR models on the training set for each relation and test on the evaluation set of Entity Questions for the particular relation and report results in Table 2 . Clearly , observing the question pattern during training allows DPR to generalize well on unseen entities .
On all three relations , DPR can match or even outperform BM25 in terms of retrieval accuracy .
Training on the equivalent question pattern achieves comparable performance to the exact pattern , showing dense models do not rely on specific phrasing of the question .
We also attempt fine-tuning the question encoder and passage encoder separately .
As shown in Table 2 , surprisingly , there is a significant discrepancy between only training the passage encoder ( OnlyP ) and only training the question encoder ( OnlyQ ) : for example , on place - of- birth , DPR achieves 72.8 % accuracy with the fine-tuned passage encoder , while it 7 place - of- birth : " What is the birthplace of [ E ] ? " ; headquarter : " Where is [ E ] headquartered ? " ; creator : " Who is the creator of [ E ] ? " .
achieves 45.4 % if only the question encoder is finetuned .
This suggests that passage representations might be the culprit for model generalization .
To understand what passage representations have learned from fine-tuning , we visualize the DPR passage space before and after fine-tuning using t-SNE ( Van der Maaten and Hinton , 2008 ) .
We plot the representations of positive passages sampled from NQ and place - of- birth in Figure 2 .
Before fine-tuning , positive passages for place - of- birth questions are clustered together .
Discriminating passages in this clustered space is more difficult using an inner product , which explains why only finetuning the question encoder yields minimal gains .
After fine-tuning , the passages are distributed more sparsely , making differentiation much easier .
Towards Robust Dense Retrieval Equipped with a clear understanding of the issues , we explore some simple techniques aimed at fixing the generalization problem .
Data augmentation
We first explore whether fine-tuning on questions from a single Entity Questions relation can help generalize on the full set of Entity Questions as well as other QA datasets such as NQ .
We construct a training set of questions for a single relation and consider two training regimes : one where we fine-tune on relation questions alone ; and a second where we fine-tune on both relation questions and NQ in a multi-task fashion .
We perform this analysis for three relations and report top - 20 retrieval accuracy in Table 3 .
We find that fine-tuning only on a single relation improves Entity Questions meaningfully , but degrades performance on NQ and still largely falls behind BM25 on average .
When fine- tuning on both relation questions and NQ together , most of the performance on NQ is retained , but the gains on Entity Questions are much more muted .
Clearly , fine - tuning on one type of entity -centric question does not necessarily fix the generalization problem for other relations .
This trade- off between accuracy on the original distribution and improvement on the new questions presents an interesting tension for universal dense encoders to grapple with .
Specialized question encoders
While it is challenging to have one retrieval model for all unseen question distributions , we consider an alternative approach of having a single passage index and adapting specialized question encoders .
Since the passage index is fixed across different question patterns and cannot be adapted using fine-tuning , having a robust passage encoder is crucial .
We compare two DPR passage encoders : one based on NQ and the other on the PAQ dataset ( Lewis et al. , 2021 b ) . 8
We expect a question encoder trained on PAQ is more robust because ( a ) 10 M passages are sampled in PAQ , which is arguably more varied than NQ , and ( b ) all the plausible answer spans are identified using automatic tools .
We fine- tune a question encoder for each relation in Entity Questions , keeping the passage encoder fixed .
As shown in Table 4 , 9 fine-tuning the encoder trained on PAQ improves performance over fine-tuning the encoder trained on NQ .
This suggests the DPR - PAQ encoder is more robust and adaptable , nearly closing the gap with BM25 using a single passage index .
We believe constructing a robust passage index is an encouraging avenue for future work towards a more general retriever .
Conclusion
In this study , we show that DPR significantly underperforms BM25 on Entity Questions , a dataset of simple questions based on facts mined from Wikidata .
We derive key insights about why DPR performs so poorly on this dataset .
We learn that DPR remembers robust representations for common entities , but has trouble differentiating rarer entities without explicitly observing the question pattern during training .
We suggest future work in incorporating explicit entity memory into dense retrievers to help differentiate rare entities .
Numerous recent works Li et al. 2020 ; Cao et al. 2021 ) demonstrate retrievers can easily learn dense representations for a large number of Wikipedia entities .
DPR could also leverage entity - aware embedding models like EaE ( F?vry et al. , 2020 ) or LUKE ( Yamada et al. , 2020 ) to better recall long-tail entities .
