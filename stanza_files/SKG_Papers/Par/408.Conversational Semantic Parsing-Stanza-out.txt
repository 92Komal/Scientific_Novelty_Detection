title
Conversational Semantic Parsing
abstract
The structured representation for semantic parsing in task - oriented assistant systems is geared towards simple understanding of one-turn queries .
Due to the limitations of the representation , the session - based properties such as coreference resolution and context carryover are processed downstream in a pipelined system .
In this paper , we propose a semantic representation for such task - oriented conversational systems that can represent concepts such as co-reference and context carryover , enabling comprehensive understanding of queries in a session .
We release a new session - based , compositional taskoriented parsing dataset of 20 k sessions consisting of 60 k utterances .
Unlike Dialog State Tracking Challenges , the queries in the dataset have compositional forms .
We propose a new family of Seq2Seq models for the session - based parsing above , which achieve better or comparable performance to the current state - of- the - art on ATIS , SNIPS , TOP and DSTC2 .
Notably , we improve the best known results on DSTC2 by up to 5 points for slot-carryover .
Introduction
At the core of conversational assistants lies the semantic representation , which provides a structured description of tasks supported by the assistant .
Traditional dialog systems operate through a flat representation , usually composed of a single intent and a list of slots with non-overlapping content from the utterance ( Bapna et al. , 2017 ; .
Although flat representations are trivial to model with standard intent / slot tagging models , the semantic representation is fundamentally limiting .
explored the limitations of flat representations and proposed a compositional generalization which allowed slots to contain nested intents while allowing easy modeling through neural shift- reduce parsers such as RNNG ( Dyer et al. , 2016 ) .
Our contributions are the following : ?
We explore the limitations of this compositional form and propose an extension which overcomes these limitations that we call decoupled representation .
?
To parse this more complicated representation , we propose a family of Seq2Seq models based off the Pointer - Generator architecture that set state of the art in multiple semantic parsing and dialog tasks ( See et al. , 2017 ) . ?
To further advance session based task oriented semantic parsing , we release a publicly available set with 60 k utterances constituting roughly 20 k sessions .
Semantic Representation
The compositional extension proposed by overcame the limitation of classical intent-slot frameworks by allowing nested intents in slots .
But to maintain an easily model- able structure the following constraint was introduced : the in-order traversal of the compositional semantic representation must reconstruct the utterance .
Following this constraint it is possible to use discriminative neural shift reduce parsers such as RNNG to parse into this form ( Dyer et al. , 2016 ) .
Although at face value this constraint seems reasonable , it has non-trivial implications for both the semantic parsing component ( NLU ) and downstream components in conversational assistants .
Surpassing Utterance Level Limitations with Decoupled Form First we 'll take a look at the space of utterances that can be covered by the compositional representation .
One fundamental problem with the in-order constraint is that it disallows long-distance dependencies within the semantic representation .
For example , the utterance On Monday , set an alarm for 8 am . would optimally have a single date - time slot : [ SL DATETIME 8 am on Monday ] .
But , because 8 am and on Monday are at opposite ends of the utterance , there is no way to construct a semantic parse tree with a single datetime slot .
mentioned this problem , but had some empirical data showing that utterances with long-distance dependencies are rare in English .
Although this might be true , having fundamental limitations on what type of utterances can be supported even with a complete ontology is concerning .
In English , discontinuities are restricted in occurrence , despite emerging naturally within certain patterns , because English is a configurational language , which uses strongly marked word order to impart some level of semantic information ( Chomsky , 1981 ) .
Beyond English , however , there are numerous world languages that are non-configurational and have much freer or potentially completely free word order .
Non-configurational languages may often present the same semantic information through the use of Case Markers , Declensions , or other systems .
The relatively free word order this allows creates much less emphasis on the collocation of a semantic unit's to -kens .
Therefore , as conversational assistants progress toward multiple languages it 's important to consider that constraints that are acceptable if only English is considered will not analogously scale to other languages .
A simple solution is to convert a standard compositional intent-slot parse into a logical form containing two label types ( slot and intent ) , with no constraints over intent spans .
This is trivially accomplished by removing all text in the compositional semantic parse that does not appear in a leaf slot .
We call this form of semantic parse the decoupled semantic representation , due to the semantic representation not being tightly coupled with the original utterance .
Figure 1 shows a side by side example of compositional and decoupled semantic representations for the utterance Please remind me to call John .
Session Based Limitations
Because traditional conversational systems historically have had a clear separation between utterance level semantic parsing and dialog systems ( which stitch together utterance level information into sessions ) , semantic representations have not focused on sessionbased representations .
Integrating session information into semantic parsers has been limited to refinement - based approaches .
Figure 2 shows an example of refinement and informationally complete based approaches to semantic parsing .
The refinement approach delegates responsibility of sessionbased semantic parsing to a separate dialog component .
Consequently , refinement approaches tend to have a very limited ontology due to the semantic parser operating over a fixed input ( non-session utterances ) .
Predicting what slot to use for refining works for flat semantic representations , but it is non-trivial to extend to compositional or decoupled .
The position of a slot in a flat semantic representation is not meaningful , thus it is sufficient to only predict the slot without specifying its position in the parse .
But both compositional and decoupled extensions to intent-slot parsing vary semantically by the position of the slot ( or nested intent ) .
We present an example in Figure 3 .
Given the followup utterance remind me to call , a classical system would need to carry over the whole CONTACT slot , but the question is to where ?
The semantic parse is not flat .
The slot could be carried over to the CREATE REMINDER intent or the nested GET CONTACT intent .
So , if we were to extend classical slot carryover , we not only would need to predict what slot to carry over from the conversation , but what intent within the current semantic parse to place it under .
We propose a new paradigm that does joint classical semantic parsing with co-reference resolution and slot-carryover .
Session Based Semantic Parsing
We present a simple extension to the decoupled paradigm of intent-slot semantic parsing by introduction of a new reference ( REF ) label type .
The REF label type contains two elements in its set to represent co-references and slot-carryover as separate operations .
Coreferences can be seen as an explicit reference , namely a reference conditioned on an explicit word , while slot- carryover is treated as an implicit reference ( conditioned by relevant contextual information ) .
As an example , refer to the sample session with decoupled semantic parses in Figure 4 3 Model
Sequence-to-Sequence Architecture
The decoupled semantic parsing model is an extension of the very common sequence - tosequence learning approach ( Sutskever et al. , 2014 ) , with the source sequence being the utterance and the target sequence being a linearized version of the target tree .
Trees are linearized by bracketing them , using the same approach as Vinyals et al . ( 2015 ) .
The decoupled tree in Fig. 1 b , for example , would be linearized to the following target sequence : [ IN : CREATE REMINDER , [ SL : PERSON REMINDED , me , ] , [ SL : TODO , ... , ] .
After tokenization , an encoder processes the source tokens w i and produces corresponding encoder hidden states : where the encoder , in our experiments , is either a standard bidirectional LSTM or a transformer .
In spite of its drawbacks , the rigid structure of the compositional semantic trees ( Fig. 1a ) has the advantage of readily mapping to the RNNG formalism and its inductive biases .
The decoupled semantic representation , being more flexible , does not have such an easily exploitable form - but we can still exploit whatever structure exists .
The tokens of the linearized decoupled representation ( the target sequence ) can always be divided into two classes : utterance tokens that are already present in the source sequence - which form the leaves of the tree - and ontology symbols .
Taking again the example tree of Fig. 1 b , me , call , and John are all tokens from the utterance , while [ IN : CREATE REMINDER , [ SL : PERSON REMINDED , ] , etc. , are ontology symbols .
This partition is reflected in the structure of the decoder : at every decoding step , the model can either generate an element from the ontology , or copy a token from the source sequence via a mechanism analogous to the pointer - generator network of See et al . ( 2017 ) .
At decoding time step t , the decoder is fed with the encoder 's outputs and produces a vector of features x t , which is used to compute an ontology generation distribution p g t : x t = Decoder ( e 1 , ... , e t ; d t?1 ; s t?1 ) , p g t = softmax Linear g [ x t ] , where d t?1 is the previous output of the decoder , s t?1 is the decoder 's incremental state , and Linear ? [ x ] is short - hand for an affine transformation with parameters ? , i.e. W ? x + b ? .
The decoder 's features are also used to calculate the attention distribution - using multi-head attention ( Vaswani et al. , 2017 ) - which then serves to produce the utterance copy distribution p c t : p c t , ?
t = MhAttention ( e 1 , ... , e t ; Linear c [ x t ] ) , p ? t = ?
( Linear ? [ x t ? t ] ) , where ?( x ) = 1 1+e ?x the standard sigmoid function , indicates concatenation , and MhAttention indicates attention which returns , respectively , the attention distribution and its weights .
Finally , the extended probability distribution is computed as a mixture of the ontology generation and utterance copy distributions : p t = p ? t ? p g t + ( 1 ? p ? t ) ?
p c t .
Encoder and Decoder
We experiment with two main variants of the decoupled model : one based on recurrent neural networks , and one based on the transformer architecture ( Vaswani et al. , 2017 ) . RNN
Our base model uses two distinct stacked bidirectional LSTMs as the encoder and stacked unidirectional LSTMs as the decoder .
Both consist of two layers of size 512 , with randomly initialized embeddings of size 300 .
The base model is optimized with LAMB while others are optimized with Adam , using parameters ?
1 = 0.9 , ? 2 = 0.999 , = 10 ?8 , and L2 penalty 10 ?5 ( Kingma and Ba , 2014 ) .
The learning rate is found separately for each experiment via hyperparameter search .
We also use stochastic weight averaging ( Izmailov et al. , 2018 ) , and exponential learning rate decay .
For an extended version of this model , we also try incorporating contextualized word vectors , by augmenting the input with ELMo embeddings ( Peters et al. , 2018 ) .
Transformer
We also experiment with two further variants of the model , that replace encoder and decoder with transformers .
In the first variant , the encoder is initialized with RoBERTa ( Liu et al. , 2019 ) , a pretrained language model .
The decoder is a randomly initialized 3 - layer transformer , with hidden size 512 and 4 attention heads .
In the second variant , we initialise both encoder and decoder with BART ( Lewis et al. , 2019 ) , a sequence -tosequence pretained model .
Both encoder and decoder consist of 12 layers with hidden size 1024 .
We train these with stochastic weight averaging ( Izmailov et al. , 2018 ) , and determine optimal hypermarameters on the validation sets .
Experiments
Session Based Task Oriented Parsing
To incentivize further research into session based semantic parsing through the decoupled intent-slot paradigm we are releasing 20 thousand annotated sessions in 4 domains : calling , weather , music and reminder .
We also allow for mixtures of domains within a session .
The data was collected in two stages .
First we asked crowdsourced workers to write sessions ( both from the users perspective as well as the Assistant 's output ) tied to certain domains .
Once we vetted the sessions , we asked a second group of annotators to annotate the user input per session .
Each session was given to three separate annotators .
We used majority voting to automatically resolve the correct parse when possible .
In the cases where there was no agreement , we selected the maximum informative parse which abode by the labeling representations semantic constraints .
The annotator agreement rate was 55 % , while our final chosen semantic parses were correct 94 % of the time .
The large delta between the two numbers is due to multiple correct semantic parses existing for the same session .
We open source SB - TOP in the following link : http://www.dl.fbaipublicfiles.
com/sbtop/SBTOP.zip .
More information about the dataset can be found in the Table ? ? in the Appendix .
Semantic Parsing
We evaluate the decoupled model on five semantic parsing datasets , four public and one internal .
All but two are annotated with compositional semantic representations and the other with the standard flat intent-slot representation .
In order to apply the decoupled models to them , we follow a mechanical procedure to transform the annotations to decoupled representations : all utterance tokens which are not part of a slot are stripped .
This procedure effectively turns the tree of Fig. 1a into the tree of Fig. 1 b .
We note that this procedure for all compositional and flat data available is therefore we can convert from decoupled back to source representation .
The first public dataset is TOP , which consists of over 31 k training utterances covering the navigation , events , and navigation to events domains .
The first internal dataset we use contains over 170k training utterances annotated with flat representations , covering over 140 distinct intents from a variety of domains including weather , communication , music , weather , and device control .
The second internal dataset contains over 67 k training utterances with fully hierarchical representations , and covers over 60 intents all in the communication domain .
The second and third public datasets are SNIPS Natural Language Understanding benchmark1 ( SNIPS - NLU ) and the Airline Travel Information Systems ( ATIS ) dataset ( Hemphill et al. , 1990 ) .
We follow the same procedure that was mentioned above for preparing the decoupled data for both of these datasets .
As can be seen from Table 1 b , our proposed approach outperforms the previous state- ofthe - art results on the ATIS , comparable to state - of- the - art on SNIPS , and TOP semantic parsing task , which had been obtained with the Seq2SeqPtr model by Rongali et al . ( 2020 ) .
Comparing the decoupled model to RNNGs , we note that a single decoupled model , using either biLSTMs or transformers ( with RoBERTa or BART pretraining ) is able to outperform the RNNG .
In fact , the decoupled model even outperforms an ensemble of seven RNNGs .
The decoupled biLSTM extended with ELMo inputs is able to outperform the transformer model initialised with RoBERTa pretraining .
However , the best performance is achieved by using the transformer model with BART - large pretraining , with the decoupled model fine - tuned jointly on top of it ( Lewis et al. , 2019 ) .
In order to understand how much of these gains are due to the semantic representation , we perform an ablation study by evaluating the biLSTM and RoBERTa - based models on TOP data using the standard logical form representation , and find a drop in frame accuracy of 0.32 and 0.55 respectively .
The TOP dataset contains to the order of 30k examples in its training set .
In order to further tease out the differences between the biLSTM and transformer approaches , and to see how they compare when more training data is available , we also evaluate these models on our two larger internal datasets .
Table 1 c shows that the RoBERTa - based model does indeed benefit from the extra training data , being able to outperform the biLSTMbased model on the two datasets .
In both cases , the decoupled model with BART pretraining achieves the top performance .
The same procedure was used over our SB - TOP dataset , with the only variant being we concatenated SB - TOP and TOP and jointly trained over both datasets .
Table 2 shows the test results over
Slot carryover
To evaluate the ability of the decoupled models to work on session - based data , we evaluate them on a task which requires drawing information for multiple utterances .
The DSTC2 dataset ( Henderson et al. , 2014 ) contains a number of dialogues annotated with dialogue state - slightly over 2 k sessions in the training set .
They involve users searching for restaurants , by specifying constraints such as cuisine type and price range .
Given that users will often take multiple turns to specify all constraints , determining the correct dialogue state requires the model to consider all past turns too .
Consider the example of the two -turn DSTC2 session shown in Figure 5 : the [ SL : AREA south ] slot , introduced in the first session , is said to carry over to the second session as it still applies to the dialogue state , despite not being explicitly mentioned .
1
To make previous utterances available to the model , we use a simple approach : all utterances are concatenated , with a separator token , and are fed to the encoder .
The decoupled models are evaluated on frame accuracy and slot carryover - the fraction of slots correctly carried over from one turn to the next .
Carryover figures are split by slot distance : how many turns prior to the current one the slot under consideration first appeared .
As shown in Table 3 , the RoBERTa decoupled model outperforms the biLSTM model on frame accuracy , while the biLSTM model takes the lead in terms of raw slot carryover performance .
BART outperforms both , achieving the best overall performance .
For informative purposes , we also include results from standard dialogue state tracking models .
The results show that the decoupled models , despite not being specifically designed for the task of dialogue state tracking , compare favorably to other approaches in the literature .
While our models outperform them on most metrics , it should be noted that they 2018 ) or a fixed length dialogue representation .
It is interesting to note that the decoupled models perform better on distant slots : this suggests that the models may be paying more attention to the beginning of the sentences , which may be an artifact of their pretraining .
Related Work Traditional work on semantic parsing , either for the purposes of question answering or taskoriented request understanding , has focused on mapping utterances to logical form representations ( Zelle and Mooney , 1996 ; Zettlemoyer and Collins , 2005 ; Kwiatkowksi et al. , 2010 ; Liang , 2016 ; van Noord et al. , 2018 ) .
Logical forms , while very expressive , are also complex .
Highly trained annotators are required for the creation of training data , and as a result there is a lack of large scale datasets that make use of these formalisms .
Intent-slot representations such as those used for the ATIS dataset ( Price , 1990 ) or the datasets released as part of the DSTC challenges ( Henderson et al. , 2014 ; Rastogi et al. , 2019 ) have less expressive power , but have the major advantage of being simple enough to enable the creation of large-scale datasets .
introduce a hierarchical intent-slot representation , and show that it is expressive enough to capture the majority of user-generated queries in two domains .
Recent approaches to semantic parsing have focused on using techniques such as RN - NGs , RNNGs augmented with ensembling and re-ranking techniques or contextual embeddings ( Einolghozati et al. , 2018 ) , sequence - to-sequence recurrent neural networks augmented with pointer mechanisms ( Jia and Liang , 2016 ) , capsule networks ( Zhang et al. , 2019 ) , and Transformer - based architectures ( Rongali et al. , 2020 ) .
Conclusions
We started this paper by exploring the limitations of compositional intent-slot representations for semantic parsing .
Due to the constraints it imposes , it cannot represent certain utterances with long-term dependencies , and it is unsuitable for semantic parsing at the session ( multi-utterance ) level .
To overcome these limitations we propose an extension of this representation , the decoupled representation .
We propose a family of sequenceto-sequence models based on the pointergenerator architecture - using both recurrent neural network and transformer architectures - and show that they achieve top performance on several semantic parsing tasks .
Further , to advance session - based task - oriented semantic parsing , we release to the public a new dataset of roughly 20 k sessions ( over 60 k utterances ) .
Figure 1 : 1 Figure 1 : Compositional and decoupled semantic representations for the single utterance " Please remind me to call John " .
Figure 2 : 2 Figure 2 : Refinement and Complete session based semantic representations for the utterance " call " .
Figure 3 : 3 Figure 3 : Sample session with complex slot-carryover : " Is mom available ? "
-" Remind me to call "
Figure 5 : 5 Figure 5 : Example DSTC2 session , annotated for the decoupled model .
Table 1 : 1 Frame accuracy of the decoupled models on semantic parsing tasks .
? indicates results Hakkani -T?r et al. ( 2016 ) ; from Goo et al. ( 2018 ) ; * , from Zhang et al. ( 2018 ) ; ? , from Chen et al . ( 2019a ) .
( a) Accuracy on TOP .
( b) Accuracy on ATIS and SNIPS .
( c ) Accuracy on internal datasets .
Model Acc. Model ATIS SNIPS Model Acc. RNNG 80.86 Joint biRNN ? 80.7 73.2 Multi-domain ( 170 k ) RNNG + Ensembling RNNG + ELMo Decoupled biLSTM Decoupled transformer 64.50 83.84 83.93 79.51 Decoupled ELMo 84.85 Decoupled RoBERTa 84.52 Decoupled BART 87.10
Slot gated ?
CapsuleNLU * Joint BERT ? Joint BERT CRF ? 88.6 82.2 83.4 88.2 Decoupled BART 89.25 91.00 75.5 80.9 92.8 92.6 Best Seq2SeqPtr 87.12 87.14 Decoupled ELMo Decoupled RoBERTa 87.32 86.03 88.29 Decoupled BART Single-domain ( 67 k ) Decoupled ELMo 90.52 Decoupled RoBERTa 91.51 Best Seq2SeqPtr 86.67 Decoupled BART 92.16
Table 2 : 2 Decoupled model architecture results over the SB - TOP dataset .
FA is exact match between canonicalized predicted and tree structures .
Ref Only FA does not distinguish between implicit / explicit references .
Intent accuracy is accuracy over top level intents while Inner Parse Accuracy is FA not considering top level intent .
Model Oracle@Beam FA Ref-only FA Intent Acc. Inner Parse Acc. Humans 1 55.04 57.4 84.32 60.12 Decoupled biLSTM 1 48.48 49.19 78.60 52.74 5 60.24 69.88 93.71 72.01 Decoupled ELMo 1 51.22 52.03 80.93 55.07 5 62.58 70.08 94.73 72.11 Decoupled BART 1 53.45 54.18 82.46 56.84 5 65.19 72.78 96.67 76.45
Table 3 : 3 Performance of the decoupled models on a state tracking task ( DSTC2 ) .
Model Accuracy Slot distance 1 2 ? 3 LSTM - based ( Naik et al. , 2018 ) - 91.11 91.34 87.99
Pointer network decoder ( Chen et al. , 2019 b ) - 92.70 92.04 92.90 91.39
Transformer decoder ( Chen et al. , 2019 b ) - 93.00 92.69 92.80 89.49 GLAD ( Zhong et al. , 2018 ) 74.5 - - - - Decoupled biLSTM 88.3 93.34 94.73 95.28 95.73 Decoupled RoBERTa 89.8 91.98 92.94 93.58 94.28 Decoupled BART 90.2 94.21 95.47 95.90 97.05 are very different in nature : the decoupled models attend over all utterances leading up to and including the current turn , while state tracking models generally only have access to the current utterance and the previous system actions - in the case of Zhong et al .
(
The image shows the tree form to which we converted the DSTC2 native state tracking annotations , to make them easily linearizable and thus treatable by the decoupled models .
