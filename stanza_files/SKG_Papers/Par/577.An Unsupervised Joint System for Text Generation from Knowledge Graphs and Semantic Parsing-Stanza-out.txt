title
An Unsupervised Joint System for Text Generation from Knowledge Graphs and Semantic Parsing
abstract
Knowledge graphs ( KGs ) can vary greatly from one domain to another .
Therefore supervised approaches to both graph - to - text generation and text - to - graph knowledge extraction ( semantic parsing ) will always suffer from a shortage of domain-specific parallel graphtext data ; at the same time , adapting a model trained on a different domain is often impossible due to little or no overlap in entities and relations .
This situation calls for an approach that ( 1 ) does not need large amounts of annotated data and thus ( 2 ) does not need to rely on domain adaptation techniques to work well in different domains .
To this end , we present the first approach to unsupervised text generation from KGs and show simultaneously how it can be used for unsupervised semantic parsing .
We evaluate our approach on WebNLG v2.1 and a new benchmark leveraging scene graphs from Visual Genome .
Our system outperforms strong baselines for both text ?
graph conversion tasks without any manual adaptation from one dataset to the other .
In additional experiments , we investigate the impact of using different unsupervised objectives .
1
Introduction Knowledge graphs ( KGs ) are a general - purpose approach for storing information in a structured , machine - accessible way ( Van Harmelen et al. , 2008 ) .
They are used in various fields and domains to model knowledge about topics as different as lexical semantics ( Fellbaum , 2005 ; van Assem et al. , 2006 ) , common sense ( Speer et al. , 2017 ; Sap et al. , 2019 ) , biomedical research ( Wishart et al. , 2018 ) and visual relations in images .
This ubiquity of KGs necessitates interpretability because diverse users - both experts and nonexperts - work with them .
Even though , in prin-ciple , a KG is human- interpretable , non-experts may have difficulty making sense of it .
Thus , there is a need for methods , such as automatic natural language generation ( " graph?text " ) , that support them .
Semantic parsing , i.e. , the conversion of a text to a formal meaning representation , such as a KG , ( " text? graph " ) is equally important because it makes information that only exists in text form accessible to machines , thus assisting knowledge base engineers in KG creation and completion .
As KGs are so flexible in expressing various kinds of knowledge , separately created KGs vary a lot .
This unavoidably leads to a shortage of training data for both graph ?
text tasks .
We therefore propose an unsupervised model that ( 1 ) easily adapts to new KG domains and ( 2 ) only requires unlabeled ( i.e. , non-parallel ) texts and graphs from the target domain , together with a few fact extraction heuristics , but no manual annotation .
To show the effectiveness of our approach , we conduct experiments on the latest release ( v2.1 ) of the WebNLG corpus ( Shimorina and Gardent , 2018 ) and on a new benchmark we derive from Visual Genome .
While both of these datasets contain enough annotations to train supervised models , we evaluate our unsupervised approach by ignoring these annotations .
The datasets are particularly well - suited for our evaluation as both graphs and texts are completely humangenerated .
Thus for both our tasks , models are evaluated with natural , i.e. , human- generated targets .
Concretely , we make the following contributions : ( 1 ) We present the first unsupervised non-template approach to text generation from KGs ( graph?text ) .
( 2 ) We jointly develop a new unsupervised approach to semantic parsing that automatically adjusts to a target KG schema ( text? graph ) .
( 3 ) In contrast to prior unsupervised graph ?
text and text ?
graph work , our model does not re-quire manual adaptation to new domains or graph schemas .
( 4 ) We provide a thorough analysis of the impact of different unsupervised objectives , especially the ones we newly introduce for text ?
graph conversion .
( 5 ) We create a new large-scale dataset for text ?
graph transformation tasks in the visual domain .
2 Related Work graph ? text .
Our work is the first attempt at fully unsupervised text generation from KGs .
In this respect it is only comparable to traditional rule- or template - based approaches ( Kukich , 1983 ; McRoy et al. , 2000 ) .
However , in contrast to these approaches , which need to be manually adapted to new domains and KG schemas , our method is generally applicable to all kinds of data without modification .
There is a large body of literature about supervised text generation from structured data , notably about the creation of sports game summaries from statistical records ( Robin , 1995 ; Tanaka - Ishii et al. , 1998 ) .
Recent efforts make use of neural encoderdecoder mechanisms ( Wiseman et al. , 2017 ; Puduppully et al. , 2019 ) .
Although text creation from relational databases is related and our unsupervised method is , in principle , also applicable to this domain , in our work we specifically address text creation from graph - like structures such as KGs .
One recent work on supervised text creation from KGs is ( Bhowmik and de Melo , 2018 ) .
They generate a short description of an entity , i.e. , a single KG node , based on a set of facts about the entity .
We , however , generate a description of the whole KG , which involves multiple entities and their relations .
Koncel- Kedziorski et al. ( 2019 ) generate texts from whole KGs .
They , however , do not evaluate on human-generated KGs but automatically generated ones from the scientific information extraction tool SciIE ( Luan et al. , 2018 ) .
Their supervised model is based on message passing through the topology of the incidence graph of the KG input .
Such graph neural networks ( Kipf and Welling , 2017 ; Veli?kovi ?
et al. , 2018 ) have been widely adopted in supervised graph - to - text tasks ( Beck et al. , 2018 ; Damonte and Cohen , 2019 ; Ribeiro et al. , 2019 Ribeiro et al. , , 2020 .
Even though Marcheggiani and Perez-Beltrachini ( 2018 ) report that graph neural networks can make better use of graph input than RNNs for supervised learning , for our unsuper-vised approach we follow the line of research that uses RNN - based sequence - to- sequence models Sutskever et al. , 2014 ) operating on serialized triple sets ( Gardent et al. , 2017 b ; Trisedya et al. , 2018 ; Gehrmann et al. , 2018 ; Castro Ferreira et al. , 2019 ; Fan et al. , 2019 ) .
We make this choice because learning a common semantic space for both texts and graphs by means of a shared encoder and decoder is a central component of our model .
It is a nontrivial , separate research question whether and how encoder-decoder parameters can effectively be shared for models working on both sequential and non-sequential data .
We thus leave the adaptation of our approach to graph neural networks for future work .
text ? graph .
Converting a text into a KG representation , our method is an alternative to prior work on open information extraction ( Niklaus et al. , 2018 ) with the advantage that the extractions , though trained without labeled data , automatically adjust to the KGs used for training .
It is therefore also related to relation extraction in the unsupervised ( Yao et al. , 2011 ; Marcheggiani and Titov , 2016 ; Simon et al. , 2019 ) and distantly supervised setting ( Riedel et al. , 2010 ; Parikh et al. , 2015 ) .
However , these systems merely predict a single relation between two given entities in a single sentence , while we translate a whole text into a KG with potentially multiple facts .
Our text ?
graph task is therefore most closely related to semantic parsing ( Kamath and Das , 2019 ) , but we convert statements into KG facts whereas semantic parsing typically converts a question into a KG or database query .
Poon and Domingos ( 2009 ) proposed the first unsupervised approach .
They , however , still need an additional KG alignment step , i.e. , are not able to directly adjust to the target KG .
Other approaches overcome this limitation but only in exchange for the inflexibility of manually created domain-specific lexicons ( Popescu et al. , 2004 ; Goldwasser et al. , 2011 ) . Poon ( 2013 ) 's approach is more flexible but still relies on preprocessing by a dependency parser , which generally means that language -specific annotations to train such a parser are needed .
Our approach is endto-end , i.e. , does not need any language -specific preprocessing during inference and only depends on a POS tagger used in the rule- based text ?
graph system to bootstrap training .
Unsupervised sequence generation .
Our unsu-pervised training regime for both text ?
graph tasks is inspired by ( Lample et al. , 2018 b ) .
They used self-supervised pretraining and backtranslation for unsupervised translation from one language to another .
We adapt these principles and their noise model to our tasks , and introduce two new noise functions specific to text ?
graph conversion .
Preliminaries
Data structure
We formalize a KG as a labeled directed multigraph ( V , E , s , t , l ) where entities are nodes V and edges E represent relations between entities .
The lookup functions s , t : E ?
V assign to each edge its source and target node .
The labeling function l assigns labels to nodes and edges where node labels are entity names and edge labels come from a predefined set R of relation types .
An equivalent representation of a KG is the set of its facts .
A fact is a triple consisting of an edge 's source node ( the subject ) , the edge itself ( the predicate ) , and its target node ( the object ) .
So the set of facts F of a KG can be obtained from its edges : F := { ( s( e ) , e , t ( e ) ) | e ? E } .
Applying l to all triple elements and writing out F in an arbitrary order generates a serialization that makes the KG accessible to sequence models otherwise used only for text .
This has the advantage that we can train a sequence encoder to embed text and KGs in the same semantic space .
Specifically , we serialize a KG by writing out its facts separated with end-of-fact symbols ( EOF ) and elements of each fact with special SEP symbols .
We thus define our task as a sequence-to-sequence ( seq2seq ) task .
Scene Graphs
The Visual Genome ( VG ) repository is a large collection of images with associated manually annotated scene graphs ; see Fig. 1 . A scene graph formally describes image objects with their attributes , e.g. , ( hydrant , attr , yellow ) , and their relations to other image objects , e.g. , ( woman , in , shorts ) .
Each scene graph is organized into smaller subgraphs , known as region graphs , representing a subpart of a more complex larger picture that is interesting on its own .
Each region graph is associated with an English text , the region description .
Texts and graphs were not automatically produced from each other , but were collected from crowdworkers who were presented an image region and then generated text and graph .
So although the graphs were not specifically designed to closely resemble the texts , they describe the same image region .
This semantic correspondence makes scene graph ?
text conversion an interesting and challenging problem because text and graph are not simple translations of each other .
Scene graphs are formalized in the same way as other KGs : V here contains image objects and their attributes , and R contains all types of visual relationships and the special label attr for edges between attribute and non-attribute nodes .
Fig. 2 shows an example .
VG scene graphs have been used before for traditional KG tasks , such as KG completion ( Wan et al. , 2018 ) , but we are the first to use them for a text ?
graph conversion dataset .
Approaches
Rule- based systems
We propose a rule- based system as unsupervised baseline for each of the text ?
graph tasks .
Note that they both assume that the texts are in English .
R graph ?
text .
From a KG serialization , we remove SEP symbols and replace EOF symbols by the word and .
The special label attr is mapped to is .
This corresponds to a template - based enumeration of all KG facts .
See Table 5 for an example .
R text ?
graph .
After preprocessing a text with NLTK 's default POS tagger ( Loper and Bird , 2004 ) and removing stop words , we apply two simple heuristics to extract facts : ( 1 ) Each verb becomes a predicate ; is creates facts with predicate attr .
The content words directly before and after such a predicate word become subject and object .
( 2 ) Adjectives a form attributes , i.e. , build facts of the form ( X , attr , a ) where X is filled with the first noun after a .
These heuristics are similar in nature to a rudimentary parser .
See Table 8 for an example .
Neural seq2seq systems
Our main system is a neural seq2seq architecture .
We equip the standard encoder-decoder model with attention and copy mechanism ( Gu et al. , 2016 ) .
Allowing the model to directly copy from the source to the target side is beneficial in data to text generation ( Wiseman et al. , 2017 ; Puduppully et al. , 2019 ) .
The encoder ( resp. decoder ) is a bidirectional ( resp. unidirectional ) LSTM ( Hochreiter and Schmidhuber , 1997 ) .
Dropout ( Hinton et al. , 2012 ) is applied at the input of both encoder and decoder ( Britz et al. , 2017 ) .
We combine this model with the following concepts : Multi-task model .
In unsupervised machine translation , systems are trained for both translation directions ( Lample et al. , 2018 b ) .
In the same way , we train our system for both conversion tasks text ?
graph , sharing encoder and decoder .
To tell the decoder which type of output should be produced ( text or graph ) , we initialize the cell state of the decoder with an embedding of the desired output type .
The hidden state of the decoder is initialized with the last state of the encoder as usual .
Noisy source samples .
Lample et al. ( 2018a ) introduced denoising auto-encoding as pretraining and auxiliary task to train the decoder to produce well - formed output and make the encoder robust to noisy input .
The training examples for this task consist of a noisy version of a sentence as source and the original sentence as target .
We adapt this idea and propose the following noise functions for the domains of graphs and texts : swap , drop , blank , repeat , rule .
Table 1 describes their behavior .
swap , drop and blank are adapted from ( Lample et al. , 2018a ) with facts in graphs taking the role of words in text .
As order should be irrelevant in a set of facts , we drop the locality constraint in the swap permutation for graphs by setting k = +?.
Denoising samples generated by repeat requires the model to learn to remove redundant information in a set of facts .
In the case of text , repeat mimics a behavior often observed with insufficiently trained neural models , i.e. , repeating words considered important .
Unlike the other noise functions , rule does not " perturb " its input , but rather noisily backtranslates it .
We will see in Section 7 that bootstrapping with these noisy translations is essential .
We consider two fundamentally different noise injection regimes : ( 1 ) The composed noise setting is an adaptation of Lample et al . ( 2018a ) 's noise model ( blank?drop?swap ) where our newly introduced noise functions rule and repeat are added to the start and end of the pipeline , i.e. , all data samples are treated equally with the same noise function C comp := repeat?blank?drop?swap?rule .
Figure 3 shows an example .
( 2 ) In the sampled noise setting , we do not use all noise functions at once but sample a single one per data instance .
Training regimes
We denote the sets of graphs and corresponding texts by G and T .
The set of available supervised examples ( x , y ) ?
G ? T is called S ? G ? T . P g and P t are probabilistic models that generate , conditioned on any input , a graph ( g ) or a text ( t ) .
Unsupervised training .
We first obtain a language model for both graphs and text by training one epoch with the denoising auto-encoder objective : L denoise = E x?G [? log P g ( x|C ( x ) ) ] + E y?T [? log P t ( y | C ( y ) ) ] where C ?
C comp for composed noise and C ? { swap , blank , drop , repeat , rule} for sampled noise .
In this pretraining epoch only , we use all possible noise functions individually on all available data .
As sampled noise incorporates five different noise functions and composed noise only one , this results in five times more pretraining samples for sampled noise than for composed noise .
In subsequent epochs , we additionally consider L back as training signal : L back = E x?G [? log P g ( x|z * ( x ) ) ] + E y?T [? log P t ( y|w * ( y ) ) ] z * ( x ) = arg max z P t ( z| x ) w * ( y ) = arg max w P g ( w|y )
This means that , in each iteration , we apply the current model to backtranslate a text ( graph ) to obtain a potentially imperfect graph ( text ) that we can use as noisy source with the clean original input being the target .
This gives us a pseudo-parallel training instance for the next iteration - recall that we address unsupervised generation , i.e. , without access to parallel data .
The total loss in these epochs is L back + L denoise , where now L denoise only samples one possible type of noise independently for each data instance .
Supervised training .
Our intended application is an unsupervised scenario .
For our two datasets , however , we have labeled data ( i.e. , a " parallel corpus " ) and so can also compare our model to its supervised variant .
Although supervised performance is generally better , it serves as a reference point and gives us an idea of the impact of supervision as opposed to factors like model architecture and hyperparameters .
The supervised loss is simply defined as follows : L sup = E ( x,y ) ? S [? log P t ( y|x ) ? log P g ( x|y ) ]
5 Experiments
Data
For our experiments , we randomly split the VG images 80/10/10 into train / val / test .
We then remove all graphs from train that also occur in one of the images in val or test .
Finally , we unify graph serialization duplicates with different texts to single instances with multiple references for graph ?
text and proceed analogously with text duplicates for text ?
graph .
For WebNLG v2.1 , we use the data splits as provided .
Following ( Gardent et al. , 2017a ) , we resolve the camel case of relation names and remove underscores from entity names in a preprocessing step .
For both datasets , the order of facts in graph serializations corresponds to the order of triples in the original dataset .
Because of VG 's enormous size and limited computation power , we additionally create a closed - domain ball ( Lin et al. , 2018 ) ; the CHRF ++ script is from ( Popovi ? , 2017 b ) . sports subset of VG , called VG ball , which we can use to quickly conduct additional experiments ( see Section 7 ) .
We identify all images where at least one region graph contains at least one fact that mentions an object ending with ball and take all regions from them ( keeping data splits the same ) .
In contrast to alternatives like random subsampling , we consider this domain-focused construction more realistic .
Table 2 shows relevant statistics for all datasets .
While VG and WebNLG have similar statistics , VG is around 70 times larger than WebNLG , which makes it an interesting benchmark for future research , both supervised and unsupervised .
Apart from size , there are two important differences : ( 1 ) The VG graph schema has been freely defined by crowd workers and thus features a large variety of different relations .
( 2 ) The percentage of graph tokens occurring in the text , a measure important for the text ?
graph task , is lower for VG than for WebNLG .
Thus , VG graphs contain more details than their corresponding texts , which is a characteristic feature of the domain of image captions : they mainly describe the salient image parts .
Training details
We train all models with the Adam optimizer ( Kingma and Ba , 2015 ) for maximally 30 epochs .
We stop supervised models early when L sup does not decrease on val for 10 epochs .
Unsupervised models are stopped after 5 iterations on VG because of its big size and limited computational resources .
All hyperparameters and more details are described in Appendices A and B .
Our implementation is based on AllenNLP ( Gardner et al. , 2017 ) .
In unsupervised training , input graphs and texts are the same as in supervised training - only the gold target sides are ignored .
While it is an artificial setup to split paired data and treat them as V 100 is a 100 - size random sample from val .
All results are computed with scripts from ( Lin et al. , 2018 ) . unpaired , this not only makes the supervised and unsupervised settings more directly comparable , but also ensures that the text data resemble the evaluation texts in style and domain .
For the purpose of experiments on a benchmark , this seems appropriate to us .
For a concrete use case , it would be an important first step to find adequate texts that showcase the desired language style and that are about a similar topic as the KGs that are to be textualized .
As KGs are rarely the only means of storing information , e.g. , in an industrial context , such texts should not be hard to come by in practice .
4 shows how performance of our unsupervised model changes at every backtranslation iteration , measured in BLEU ( Papineni et al. , 2002 ) , a common metric for natural language generation .
For model selection , we adopt the two methods proposed by Lample et al . ( 2018 b ) , i.e. , a small validation set ( we take a 100 - size random subset of val , called V 100 ) and a fully unsupervised criterion ( U ) where BLEU compares an unlabeled sample with its back - and - forth translation .
We confirm their finding that U is not reliable for neural text generation models whereas V 100 correlates better with performance on the larger test sets .
We use V 100 for model selection in the rest of this paper .
Quantitative evaluation .
Table 3 shows BLEU , METEOR ( Banerjee and Lavie , 2005 ) and CHRF ++ ( Popovi ? , 2017a ) for our unsupervised models and the rule baseline R graph ?
text , which is in many cases , i.e. , if parallel graph - text data are scarce , the only alternative .
First , we observe that R graph ?
text performs much better on WebNLG than VG , indicating that our new benchmark poses a tougher challenge .
Second , our unsupervised models consistently outperform this baseline on all metrics and on both datasets , showing that our method produces textual descriptions much closer to human- generated ones .
Third , noise composition , the general default in unsupervised machine translation , does not always perform better than noise sampling .
Thus , it is worthwhile to try different noise settings for new tasks or datasets .
Results and Discussion Surprisingly , supervised and unsupervised models perform nearly on par .
Real supervision does not seem to give much better guidance in training than our unsupervised regime , as measured by our three metrics on two different datasets .
Some metric-dataset combinations even favor one of the unsupervised models .
Our qualitative observations provide a possible explanation for that .
Qualitative observations .
Taking a look at example generations ( Table 5 ) , we also see qualitatively how much easier it is to grasp the content of our natural language summarization than reading through a simple enumeration of KG facts .
We find that the unsupervised model ( c ) seems to output the KG information in a more complete manner than its supervised counterpart ( d ) .
The supervision probably introduces a bias present in the training data that image captions focus on salient image parts and therefore the supervised model is encouraged to omit information .
As it never sees a corresponding text - graph pair together , the unsupervised model cannot draw such a conclusion .
Graph extraction from texts
We evaluate semantic parsing ( text? graph ) performance by computing the micro-averaged F1 score of extracted facts .
If there are multiple reference graphs ( cf. Section 5.1 ) , an extracted fact is considered correct if it occurs in at least one reference graph .
For the ground truth number of facts to be extracted from a given text , we take the maximum number of facts of all its reference graphs .
Model selection .
Table 6 shows that ( compared to text generation quality ) U is more reliable for text ?
graph performance .
For sampled noise , it correctly identifies the best iteration , whereas for composed noise it chooses second best .
In both noise settings , V 100 perfectly chooses the best model .
Quantitative observations .
Table 7 shows a comparison of our unsupervised models with two rule- based systems , our R text ?
graph and the highly domain-specific Stanford Scene Graph Parser ( SSGP ) by Schuster et al . ( 2015 ) .
We choose these two baselines to adequately represent the state of the art in the unsupervised setting .
Recall from Section 2 that the only previous unsupervised works either cannot adapt to a target graph schema ( open information extraction ) , which means their precision and recall of retrieved facts is always 0 , or have been created for SQL query generation from natural language questions ( Poon , 2013 ) , a related task that is yet so different that an adaptation to triple set generation from natural language statements is nontrivial .
While rule- based systems do not automatically adapt to new graph schemas either , R text ?
graph and SSGP were at least designed with the scene graph domain in mind .
Although SSGP was not optimized to match the scene graphs from VG , its rules were still engineered to cover typical idiosyncrasies of textual image descriptions and corresponding scene graphs .
Besides , we evaluate it with lemmatized reference graphs because it only predicts lemmata as predicates .
All this gives it a major advantage over the other presented systems but it is nonetheless outperformed by our best unsupervised model - even on VG .
This shows that our automatic method can beat even hand -crafted domain-specific rules .
Both R text ? graph and SSGP fail to predict any fact from WebNLG .
The DBpedia facts from WebNLG often contain multi-token entities while R text ?
graph only picks single tokens from the text .
Likewise , SSGP models multi-token entities as two nodes with an attr relation .
This illustrates the importance of automatic adaptation to the target KG .
Although our system uses R text ?
graph during unsupervised training and is similarly not adapted to the WebNLG dataset , it performs significantly better .
Supervision helps more on WebNLG than on VG .
The poor performance of R text ? graph on WebNLG is probably a handicap for unsupervised learning .
Qualitative observations .
Table 8 shows example facts extracted by different systems .
R text ? graph and SSGP are both fooled by the proximity of the noun pants and the verb play whereas our model correctly identifies man as the subject .
It , however , fails to identify shirt as an entity and associates the two attributes colorful and white to pants .
Only the supervised model produces perfect output .
Noise and translation completeness Sampled noise only creates training pairs that either are complete rule- based translations or reconstruction pairs from a noisy graph to a complete graph or a noisy text to a complete text .
In contrast , composed noise can introduce translations from a noisy text to a complete graph or vice versa and thus encourage a system to omit input information ( cf. Fig. 3 ) .
This difference is mirrored nicely in the results of our unsupervised systems for both tasks : composed noise performs better on VG where omit-ted information in an image caption is common and sampled noise works better on WebNLG where the texts describe their graphs completely .
Noise Ablation Study
Our unsupervised objectives are defined by different types of noise models .
Hence , we examine their impact in a noise ablation study .
Table 9 shows results for text ?
graph and graph ?
text on the validation splits of VG ball and WebNLG .
For both datasets and tasks , introducing variation via noise functions is crucial for the success of unsupervised learning .
The model without noise ( i.e. , C ( x ) = x ) fails completely as do all models lacking rule as type of noise , the only exception being the only - drop system on WebNLG .
Even though drop seems to work equally well in this one case , the simple translations delivered by our rulebased systems clearly provide the most useful information for the unsupervised models - notably in combination with the other noise functions : removing rule and keeping all other types of noise ( cf. " sample all but rule " and " comp. all but rule " ) performs much worse than leaving out drop .
We hypothesize that our two rule systems provide two important pieces of information : ( 1 ) R graph ?
text helps distinguish data format tokens from text tokens and ( 2 ) R text ?
graph helps find probable candidate words in a text that form facts for the data output .
As opposed to machine translation , where usually every word in a sentence is translated into a fluent sentence in the target language , identifying words that probably form a fact is more important in data-to / from -text generation .
We moreover observe that our unsupervised models always improve on the rule- based systems even when rule is the only type of noise : graph ?
text BLEU increases from 6.2/18.3 to 19.5/37.4 on VG ball / WebNLG and text ?
graph F1 from 14.4/0.0 to 18.5/31.0 .
Finally , our ablation study makes clear that there is no best noise model for all datasets and tasks .
We therefore recommend experimenting with both different sets of noise functions and noise injection regimes ( sampled vs. composed ) for new data .
Conclusion
We presented the first fully unsupervised approach to text generation from KGs and a novel approach to unsupervised semantic parsing that automatically adapts to a target KG .
We showed the effectiveness of our approach on two datasets , WebNLG v2.1 and a new text ?
graph benchmark in the visual domain , derived from Visual Genome .
We quantitatively and qualitatively analyzed our method on text ?
graph conversion .
We explored the impact of different unsupervised objectives in an ablation study and found that our newly introduced unsupervised objective using rule- based translations is essential for the success of unsupervised learning .
Table 10 : BLEU scores on WebNLG for our unsupervised models evaluated for graph ?
text at different iterations .
U is calculated on all unlabeled data used for training .
V 100 is a 100 - size random sample from val .
All results are computed with scripts from ( Lin et al. , 2018 ) .
Table 11 : F1 scores on WebNLG for our unsupervised models evaluated for text ?
graph at different iterations .
U is calculated on all unlabeled data used for training .
V 100 is a 100 - size random sample from val .
Figure 1 : Figure 2 : 12 Figure1 : Region graphs and textual region descriptions in Visual Genome ( VG ) .
Image regions serve as common reference for text and graph creation but are disregarded in our work .
We solely focus on the pairs of corresponding texts and graphs .
Illustration adapted from .
