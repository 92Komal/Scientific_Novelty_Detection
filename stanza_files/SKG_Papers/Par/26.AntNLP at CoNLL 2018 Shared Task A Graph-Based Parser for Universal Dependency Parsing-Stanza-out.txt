title
AntNLP at CoNLL 2018 Shared Task : A Graph- based Parser for Universal Dependency Parsing
abstract
We describe the graph- based dependency parser in our system ( AntNLP ) submitted to the CoNLL 2018 UD Shared Task .
We use bidirectional lstm to get the word representation , then a bi-affine pointer networks to compute scores of candidate dependency edges and the MST algorithm to get the final dependency tree .
From the official testing results , our system gets 70.90 LAS F1 score ( rank 9/26 ) , 55.92 MLAS ( 10/26 ) and 60.91 BLEX ( 8/26 ) .
Introduction
The focus of the CoNLL 2018 UD Shared Task is learning syntactic dependency parsers that can work over many typologically different languages , even low-resource languages for which there is little or no training data .
The Universal Dependencies ( Nivre et al. , 2017 a , b) treebank collection has 82 treebanks over 57 kinds of languages .
In this paper we describe our system ( AntNLP ) submitted to the CoNLL 2018 UD Shared Task .
Our system is based on the deep biaffine neural dependency parser ( Dozat and Manning , 2016 ) .
The system contains a BiLSTM feature extractor for getting context - aware word representation and two biaffine classifiers to predict the head token of each word and the label between a head and its dependent .
There are three main metrics for this task , LAS ( labeled attachment score ) , MLAS ( morphologyaware labeled attachment score ) and BLEX ( bilexical dependency score ) .
From the official testing results , our system gets 70.90 LAS F1 score ( rank 9/26 ) , 55.92 MLAS ( 10/26 ) and 60.91 BLEX ( 8/26 ) .
In a word , Our system is ranked top 10 according to the three metrics described above .
Additionally , in the categories of small treebanks , our system obtains the sixth place with a MLAS score of 63.73 .
Besides that , our system ranked tenth in the EPE 2018 campaign with a 55.71 F1 score .
The rest of this paper is organized as follows .
Section 2 gives a brief description of our overall system , including the system framework and parser architecture .
In Section 3 , 4 we describe our monolingual model and multilingual model .
In Section 5 , we briefly list our experimental results .
System Overview
The CoNLL 2018 UD
Shared
Task aims to construct dependency trees based on raw texts , which means that the participants should not only build the parsing model , but also preprocess systems of the sentence segmentation , tokenization , POStagging and morphological analysis .
We use preprocessors from the official UDPipe tool in our submission .
The structure of the entire system is shown in Figure 1 .
Our main focus is on building a graph - based parser .
We implement a graph- based bi-affine parser following Dozat and Manning ( 2016 ) .
The parser architecture is shown in Figure 2 , which consists of the following components : ?
Token representation , which produces the context independent representation of each token in the sentence .
? Deep Bi-LSTM
Encoder , which produces the context - aware representation of each token in the sentence based on context .
?
Bi-affine Pointer Networks , which assign probabilities to all possible candidate edges .
We describe the three sub-modules in the following sections in detail .
Token representation Recent studies on dependency parsing show that densely embedded word representation could help to improve empirical parser performance .
For example : Chen and Manning ( 2014 ) map words and POS tags to a d-dimensional vector space .
Dozat and Manning ( 2016 ) use the pre-trained GloVe embeddings as an extra representation of the word .
Ma et al. ( 2018 ) use Convolutional Neural Networks ( CNNs ) to encode character - level information of a word .
The token representation module of our parser also uses dense embedding representations .
Details on token embeddings are given in the following .
?
Word e w i :
The word embedding is randomly initialized from the normal distribution N ( 0 , 1 ) ( e w i ? R 100 ) . ? Lemma e l i :
The lemma embedding is randomly initialized from the normal distribution N ( 0 , 1 ) ( e l i ? R 100 ) . ? Pre-trained Word e pw i : The FastText pretrained word embedding ( e pw i ? R 300 ) .
We will not update e pw i during the training process .
?
UPOS e u i : The UPOS - tag embedding is randomly initialized from the normal distribution N ( 0 , 1 ) ( e u i ? R 100 ) . ? XPOS e x i : The XPOS - tag embedding is randomly initialized from the normal distribution N ( 0 , 1 ) ( e x i ? R 100 ) . ?
Char e c i :
The character - level embedding is obtained by the character - level CNNs ( e c i ? R 64 ) .
Our parser uses two kinds of token representations , one is a lexicalized representation of the monolingual model , another one is the delexicalized representation of the multilingual model .
The lexicalized representation x l i of token w i is defined as : x l i = [ e w i + e l i ; word e u i + e x i P OS ; e pw i ; e c i ] ( 1 ) and the delexicalized representation x d i of token w i is defined as : x d i = [ e u i ; e x i ; e c i ] ( 2 ) In the following sections , we uses x i to represent x l i or x d i when the context is clear .
Deep Bi-LSTM
Encoder Generally , the token embeddings defined above are context independent , which means that the sentence - level information is ignored .
In recent years , some work shows that the deep BiLSTM can effectively capture the contextual information of words ( Dyer et al. , 2015 ; Kiperwasser and Goldberg , 2016 ; Dozat and Manning , 2016 ; Ma et al. , 2018 ) .
In order to encode context features , we use a 3layer sentence level BiLSTM on top of x 1:n : h t = LSTM ( h t?1 , x i , ? ) h t = LSTM ( h t+1 , x i , ? ) v i = h i ? h i ? are the model parameters of the forward hidden sequence h. ? are the model parameters of the backward hidden sequence h .
The vector v i is our final vector representation of ith token in s , which takes into account both the entire history h i and the entire future h i by concatenating h i and h i .
Biaffine Pointer Networks
How to determine the probability of each dependency edge is an important part of the graphbased parser .
The work of Dozat and Manning ( 2016 ) shows that the biaffine pointer ( attention ) networks can calculate the probability of each dependency edge well .
Here we used a similar biaffine pointer network structure .
In order to better represent the direction of the dependency edges , we use multi-layer perceptron ( MLP ) networks to learn each word as the representation of head and dependent words , rather than simply exchanging feature vectors .
And we also separate the predictions of dependent edges and their labels .
First , for each v i , we use two MLPs to define two pointers h arc i and s arc i , which is the representation of v i with respect to whether it is seen as a head or a modifier of an candidate edge .
h arc i = MLP ( arc ) head ( v i ) s arc i = MLP ( arc ) dep ( v i )
Similarly , we use h rel i and s rel to describe x i when determine the relation label of a candidate edge .
h rel i = MLP ( rel ) head ( v i ) s rel i = MLP ( rel ) dep ( v i )
We first use the arc-biaffine pointer networks to predict the probability of a dependency edge between any two words .
For any two words w i and w j in a sentence , the probability p ( arc ) i? j that they form a dependency edge w i ?
w j is as follows : a ( arc ) i? j = h arc i ?
W ( arc ) ? s arc j + u ( arc ) ? s arc j p ( arc ) i? j = softmax ( a ( arc ) i? * ) [ j ] where ? arc = { W ( arc ) , u ( arc ) } are the model parameters , a w i ? w j ?
T is then computed .
The definition of p ( rel ) i r ? ?j is as follows : a ( rel ) i * ? ?j = h rel i ?
W ( rel ) ? s rel j + V ( rel ) ? h rel i + U ( rel ) ? s rel j p ( rel ) i r ? ?j = softmax ( a ( rel ) i * ? ?j ) [ r ] where ? rel = { W ( rel ) , V ( rel ) , U ( rel ) } are the model parameters , W ( rel ) is a 3 - dimensional tensor .
a
Training Details
We train our model by minimizing the negative log likelihood of the gold standard ( w i r( w i ) ? ? ? w h( w i ) ) arcs in all training sentences : J ( arc ) = ? 1 |? | S? N S i=1 log p ( arc ) i?h( w i ) J ( rel ) = ? 1 |? | S? N S i=1 log p ( rel ) i r( w i ) ? ? ?h( w i ) J = J ( arc ) + J ( rel ) where ? is the training set , h( w i ) and r(w i ) is w i 's gold standard head and relation within sentence S , and N s is the number of words in S .
Monolingual Model
There are 82 treebanks in the CoNLL 2018 UD Shared Task , including 61 big treebanks , 5 PUD treebanks ( additional parallel test sets ) , 7 small treebanks and 9 low-resource language treebanks .
There are several languages in which there are many treebanks , such as en ewt , en gum and en lines in English .
We combine training sets and development sets for multiple treebanks of the same language .
And then just train a model for the language and make predictions on its different treebanks .
For each language of UD version 2.2 sets ( Nivre et al. , 2018 ; Zeman et al. , 2018 ) with both a training set and a development set , we train a parser using lexicalized token representation and only using its monolingual training set ( no cross-lingual features ) 1 . The architecture of the monolingual model is shown in Figure 3 .
Multilingual Model
For 7 languages without a development set , we divide them into two classes based on the size of their training set , which can be fine-tuned ( ga , sme ) and can not be fine-tuned ( bxr , hsb , hy , kk , kmr ) .
For each language of UD version 2.2 sets ( Nivre
Experimental Results
We trained our system based on a Nvidia GeForce GTX Titan X .
We used the official TIRA ( Potthast et al. , 2014 ) to evaluate the system .
We used Dynet neural network library to build our system ( Neubig et al. , 2017 ) .
The hyperparameters of the final system used for all the reported experiments are detailed in Table 5 .
Overall Results
The main official evaluation results are given in Table 4 . And the ( arc ) 500 Hidden units in M LP ( rel ) 100 Learning rate 0.002 Optimization algorithm Adam Table 5 : Hyper-parameter values used in shared task .
of 26 teams .
Compared to the baseline obtained with UDPipe1.2 ( Straka et al. , 2016 ) , our system gained 5.10 LAS improvement on average .
Our system shows better results on 7 small treebanks .
Performance improvement are more obvious when considering only small treebanks ( for example , our system ranked fourth best on ru taiga and sl sst ) .
Besides that , our system ranked tenth in the EPE 3 2018 campaign with a 55.71 F1 score .
Discussion on Multilingual Model
As described in section 4 , we trained 46 crosslanguage models and selected the corresponding cross-language model for 7 languages that did not have a development set .
Generally , cross- language models are trained in the language of the same family .
However , apart from grammatical similarity , the language family division also considers the linguistic history , geographical location and other factors .
We want to select a language 's crosslanguage model to consider only grammatical similarity .
So we use a cross-language model to predict the results in this language as a basis for selection .
In table 3 , the experimental results show that the Cross- language model with the best performance in hsb , hy , kk , and kmr languages comes from the same language family , while the Cross- language model with the best performance in bxr , ga , and sme is not from the same language family .
Therefore , constructing a cross-language model according to the language of the same family is only applicable to some languages , not all of them .
We 've only chosen the best performing cross-language model at the moment .
In the future , we will try to select the top-k cross -language model .
Conclusions
In this paper , we present a graph- based dependency parsing system for the CoNLL 2018 UD Shared Task , which composed of a BiLSTMs feature extractor and a bi-affine pointer networks .
The results suggests that a deep BiLSTM extractor and a bi-affine pointer networks is a way to achieve competitive parsing performances .
We will continue to improve our system in our future work .
Corpus AntNLP Figure 1 : 1 Figure 1 : The structure of the entire system .
Figure 2 : 2 Figure 2 : The architecture of our parser system .
i? j is the computed score of the dependency edge .
a ( arc ) i? * is a vector , and the k th dimension is the score of the dependency edge a ( arc ) i?k . p ( arc ) i? j is the j th dimension of normalization of the vector a ( arc ) i? * , meaning the probability of dependency edge w i ? w j .We obtain a dependency tree representation T of a complete graph p( arc ) * ? * using the maximum spanning tree ( MST ) algorithm .
The probability p ( rel ) i r ? ?j of the relation r of each dependency edge
, and the k th dimension is the score of the dependency edge w i k ? ? w j .
Figure 3 : 3 Figure 3 : The architecture of our parser system .
Table 1 : 1 Table6 shows the per-treebank LAS F1 results .
Our system achieved 70.90 F1 ( LAS ) on the overall 82 tree banks , ranked 9 th out Corpus listed above are languages that do n't have development set and the training set size is too small to be fine-tuned .
" # Train " means the number of sentences .
" Cross Language " means the language with the highest LAS score for corresponding origin language in our delexicalized cross-language model .
Language # Train Cross language LAS Buryat ( bxr ) 19 Uyghur ( ug ) 27.45 Upper Sorbian ( hsb ) 23 Croatian ( hr ) 39.35 Armenian ( hy ) 50 Latvian ( lv ) 29.35 Kazakh ( kk ) 9 Turkish ( tr ) 23.44 Kurmanji ( kmr ) 19 Persian ( fa ) 26.03 Corpus # Total # Train # Dev Cross language LAS Fine-tune Irish ( ga ) 566 476 90 Hebrew ( he ) 36.13 68.49 North Sami ( sme ) 2464 1948 516 Swedish ( sv ) 36.13 63.00
Table 2 : 2 Corpus listed above are languages that do n't have development set .
Because the training set size is much bigger , we decide to divide the training set into two parts , one for training set and the other for development set .
Origin language language family Cross Language Language family Buryat ( bxr ) Mongolic Uyghur ( ug ) Turkic Southeastern Upper Sorbian ( hsb ) IE Slavic Croatian ( hr ) IE Slavic Armenian ( hy ) IE Armenian Latvian ( lv ) IE Baltic Kazakh ( kk ) Turkic Northwestern Turkish ( tr ) Turkic Southwestern Kurmanji ( kmr ) IE Iranian Persian ( fa ) IE Iranian Irish ( ga ) IE Celtic Hebrew ( he ) Afro-Asiatic Semitic North Sami ( sme ) Uralic Sami Swedish ( sv ) IE Germanic
Table 3 : 3 language families and genera for origin language and cross language ( IE = Indo- European ) .
2 Corpus FLAS Baseline Rank MLAS Baseline Rank BLEX Baseline Rank All treebanks ( 82 ) 70.90 65.80 9 55.92 52.42 10 60.91 55.80 8 Big treebanks ( 61 ) 79.61 74.14 12 65.43 61.27 11 70.34 64.67 9 PUD treebanks ( 5 ) 68.87 66.63 11 53.47 51.75 10 57.71 54.87 8 Small treebanks ( 7 ) 63.73 55.01 6 42.24 38.80 7 48.31 41.06 6 Low resource ( 9 ) 18.59 17.17 10 3.43 2.82 9 8.61 7.63 8
Table 4 : 4 Official experiment results with rank .
( number ) : number of corpora .
FLAS means F1 score of LAS .
word / lemma dropout 0.33 upos / xpos tag dropout 0.33 char-CNN dropout 0.33 BiLSTM layers 3 BiLSTM hidden layer dimensions 400 Hidden units in M LP
Table 6 : 6 Official experiment results on each treebank .
The results in table are F1 ( LAS ) .
* : some corpus ' name too long to display completely , using * to indicate omission .
Rank Best Baseline Corpus AntNLP Rank Best Baseline af afr * 82.63 11 85.47 77.88 hy arm * 25.09 10 37.01 21.79 ar pad * 70.75 12 77.06 66.41 id gsd 76.86 16 80.05 74.37 bg btb 87.24 13 91.22 84.91 it isd * 89.14 12 92.00 86.26 br keb 10.06 16 38.64 10.25 it pos * 72.30 9 79.39 66.81 bxr bdt 19.53 1 1 12.61 ja gsd 72.82 17 83.11 72.32 ca anc* 89.28 13 91.61 85.61 ja mod * 12.94 22 28.33 22.71 cs cac 90.47 6 91.61 83.72 kk ktb 19.26 18 31.93 24.21 cs fic* 90.14 7 92.02 82.49 kmr mg 23.20 16 30.41 23.92 cs pdt 89.41 9 91.68 83.94 ko gsd 80.15 12 85.14 61.40 cs pud 84.76 6 86.13 80.08 ko kai *
85.01 11 86.91 70.25 cu pro* 68.23 13 75.73 65.46 la itt* 83.14 12 87.08 75.95 da ddt 80.56 10 86.28 75.43 la per * 60.99 5 72.63 47.61 de gsd 76.88 10 80.36 70.85 la pro* 66.24 12 73.61 59.66 el gdt 85.76 12 89.65 82.11 lv lvt* 75.56 12 83.97 69.43 en ewt 80.74 12 84.57 77.56 nl alp * 84.69 11 89.56 77.60 en gum 79.70 11 85.05 74.20 nl las * 82.04 8 86.84 74.56 en lin* 79.25 5 81.97 73.10 no bok *
89.19 7 91.23 83.47 en pud 84.60 9 87.89 79.56 no nyn *
88.26 9 90.99 82.13 es anc * 88.84 11 90.93 84.43 no nyn * 66.26 4 70.34 48.95 et edt 81.37 11 85.35 75.02 pcm nsc 18.30 6 30.07 12.18 eu bdt 79.01 11 84.22 70.13 pl lfg 91.16 13 94.86 87.53 fa ser * 83.98 11 88.11 79.10 pl sz 85.03 14 92.23 81.90 fi ftb 83.72 11 88.53 75.64 pt bos * 86.71 8 87.81 82.07 fi pud 85.50 10 90.23 80.15 ro rrt 84.92 8 86.87 80.27 fi tdt 83.18 11 88.73 76.45 ru syn * 90.20 10 92.48 84.59 fo oft 20.13 21 49.43 25.19 ru tai * 68.99 4 74.24 55.51 fr gsd 84.84 10 86.89 81.05 sk snk 81.14 12 88.85 75.41 fr seq * 85.32 10 89.89 81.12 sl ssj 83.26 12 91.47 77.33 fr spo* 70.96 8 75.78 65.56 sl sst 56.30 4 61.39 46.95 fro src* 83.13 12 87.12 79.27 sme gie * 57.15 13 69.87 56.98 ga idt 64.38 11 70.88 62.93 sr set 85.77 10 88.66 82.07 gl ctg 81.12 8 82.76 76.10 sv lin * 80.01 10 84.08 74.06 gl tre * 72.03 8 74.25 66.16 sv pud 76.54 10 80.35 70.63 got pro* 62.97 14 69.55 62.16 sv tal * 83.41 12 88.63 77.91 grc per * 70.76 9 79.39 57.75 th pud 0.36 18 13.70 0.70 grc pro* 73.82 9 79.25 67.57 tr ims* 59.68 12 66.44 54.04 he htb 61.43 12 76.09 57.86 ug udt 61.42 10 67.05 56.26 hi hdt * 90.44 11 92.41 87.15 uk iu 79.91 12 88.43 74.91 hr set 83.48 12 87.36 78.61 ur udt* 79.85 14 83.39 77.29 hsb ufa* 31.36 6 46.42 23.64 vi vtb 42.65 11 55.22 39.63 hu sze * 73.19 12 82.66 66.76 zh gsd 62.83 13 76.77 57.91
In total , we trained 46 monolingual models .
Zeman et al. , 2018 ) with both a training set and a development set , we train a parser using delexicalized token representation as a crosslanguage model .
The architecture of the multilingual model is shown in Figure3 .
The training set of these 5 languages are then used as a development set to validate the performance of each cross - language model ( see Table1 ) .
We select the best performance model as a cross-language model for the corresponding language .
For both ga and sme , we manually divide the development set from the training set and fine - tune the crosslanguage model .
Prediction and fine-tuning results are shown in the Table2 .
Information from http://universaldependencies.org.
3 http://epe.nlpl.eu
