title
Evaluating Dependency Parsing : Robust and Heuristics - Free Cross-Annotation Evaluation
abstract
Methods for evaluating dependency parsing using attachment scores are highly sensitive to representational variation between dependency treebanks , making cross-experimental evaluation opaque .
This paper develops a robust procedure for cross-experimental evaluation , based on deterministic unificationbased operations for harmonizing different representations and a refined notion of tree edit distance for evaluating parse hypotheses relative to multiple gold standards .
We demonstrate that , for different conversions of the Penn Treebank into dependencies , performance trends that are observed for parsing results in isolation change or dissolve completely when parse hypotheses are normalized and brought into the same common ground .
Introduction Data-driven dependency parsing has seen a considerable surge of interest in recent years .
Dependency parsers have been tested on parsing sentences in English ( Yamada and Matsumoto , 2003 ; Nivre and Scholz , 2004 ; McDonald et al. , 2005 ) as well as many other languages ( Nivre et al. , 2007 a ) .
The evaluation metric traditionally associated with dependency parsing is based on scoring labeled or unlabeled attachment decisions , whereby each correctly identified pair of head- dependent words is counted towards the success of the parser ( Buchholz and Marsi , 2006 ) .
As it turns out , however , such evaluation procedures are sensitive to the annotation choices in the data on which the parser was trained .
Different annotation schemes often make different assumptions with respect to how linguistic content is represented in a treebank ( Rambow , 2010 ) .
The consequence of such annotation discrepancies is that when we compare parsing results across different experiments , even ones that use the same parser and the same set of sentences , the gap between results in different experiments may not reflect a true gap in performance , but rather a difference in the annotation decisions made in the respective treebanks .
Different methods have been proposed for making dependency parsing results comparable across experiments .
These methods include picking a single gold standard for all experiments to which the parser output should be converted ( Carroll et al. , 1998 ; Cer et al. , 2010 ) , evaluating parsers by comparing their performance in an embedding task ( Miyao et al. , 2008 ; Buyko and Hahn , 2010 ) , or neutralizing the arc direction in the native representation of dependency trees ( Schwartz et al. , 2011 ) .
Each of these methods has its own drawbacks .
Picking a single gold standard skews the results in favor of parsers which were trained on it .
Transforming dependency trees to a set of pre-defined labeled dependencies , or into task - based features , requires the use of heuristic rules that run the risk of distorting correct information and introducing noise of their own .
Neutralizing the direction of arcs is limited to unlabeled evaluation and local context , and thus may not cover all possible discrepancies .
This paper proposes a new three -step protocol for cross-experiment parser evaluation , and in particular for comparing parsing results across data sets that adhere to different annotation schemes .
In the first step all structures are brought into a single formal space of events that neutralizes representation peculiarities ( for instance , arc directionality ) .
The second step formally computes , for each sentence in the data , the common denominator of the different gold standards , containing all and only linguistic content that is shared between the different schemes .
The last step computes the normalized distance from this common denominator to parse hypotheses , minus the cost of distances that reflect mere annotation idiosyncrasies .
The procedure that implements this protocol is fully deterministic and heuristics -free .
We use the proposed procedure to compare dependency parsing results trained on Penn Treebank trees converted into dependency trees according to five different sets of linguistic assumptions .
We show that when starting off with the same set of sentences and the same parser , training on different conversion schemes yields apparently significant performance gaps .
When results across schemes are normalized and compared against the shared linguistic content , these performance gaps decrease or dissolve completely .
This effect is robust across parsing algorithms .
We conclude that it is imperative that cross-experiment parse evaluation be a well thoughtthrough endeavor , and suggest ways to extend the protocol to additional evaluation scenarios .
The Challenge : Treebank Theories Dependency treebanks contain information about the grammatically meaningful elements in the utterance and the grammatical relations between them .
Even if the formal representation in a dependency treebank is well - defined according to current standards ( K? bler et al. , 2009 ) , there are different ways in which the trees can be used to express syntactic content ( Rambow , 2010 ) .
Consider , for instance , algorithms for converting the phrase-structure trees in the Penn Treebank ( Marcus et al. , 1993 ) into dependency structures .
Different conversion algorithms implicitly make different assumptions about how to represent linguistic content in the data .
When multiple conversion algorithms are applied to the same data , we end up with different dependency trees for the same sentences ( Johansson and Nugues , 2007 ; Choi and Palmer , 2010 ; de Marneffe et al. , 2006 ) .
Some common cases of discrepancies are as follows .
Lexical vs. Functional Head Choice .
In linguistics , there is a distinction between lexical heads and functional heads .
A lexical head carries the semantic gist of a phrase while a functional one marks its relation to other parts of the sentence .
The two kinds of heads may or may not coincide in a single word form ( Zwicky , 1993 ) .
Common examples refer to prepositional phrases , such as the phrase " on Sunday " .
This phrase has two possible analyses , one selects a lexical head ( 1a ) and the other selects a functional one ( 1 b ) , as depicted below .
( 1a ) Sunday on ( 1 b ) on Sunday Similar choices are found in phrases which contain functional elements such as determiners , coordination markers , subordinating elements , and so on .
Multi-Headed Constructions .
Some phrases are considered to have multiple lexical heads , for instance , coordinated structures .
Since dependencybased formalisms require us to represent all content as binary relations , there are different ways we could represent such constructions .
Let us consider the coordination of nominals below .
We can choose between a functional head ( 1a ) and a lexical head ( 2 b , 2c ) .
We can further choose between a flat representation in which the first conjunct is a single head ( 2 b ) , or a nested structure where each conjunct / marker is the head of the following element ( 2 c ) .
All three alternatives empirically exist .
Example ( 2a ) reflects the structures in the CoNLL 2007 shared task data ( Nivre et al. , 2007a ) . Johansson and Nugues ( 2007 ) use structures like ( 2 b ) .
Example ( 2 c ) reflects the analysis of Mel'?uk ( 1988 ) . Periphrastic Marking .
When a phrase includes periphrastic marking - such as the tense and modal marking in the phrase " would have worked " below - there are different ways to consider its division into phrases .
One way to analyze this phrase would be to choose auxiliaries as heads , as in ( 3a ) .
Another alternative would be to choose the final verb as the In standard settings , an experiment that uses a data set which adheres to a certain annotation scheme reports results that are compared against the annotation standard that the parser was trained on .
But if parsers were trained on different annotation standards , the empirical results are not comparable across experiments .
Consider , for instance , the example in Figure 1 .
If parse1 and parse2 are compared against gold2 using labeled attachment scores ( LAS ) , then parse1 results are lower than the results of parse2 , even though both parsers produced linguistically correct and perfectly useful output .
Existing methods for making parsing results comparable across experiments include heuristics for converting outputs into dependency trees of a predefined standard ( Briscoe et al. , 2002 ; Cer et al. , 2010 ) or evaluating the performance of a parser within an embedding task ( Miyao et al. , 2008 ; Buyko and Hahn , 2010 ) .
However , heuristic rules for crossannotation conversion are typically hand written and error prone , and may not cover all possible discrepancies .
Task - based evaluation may be sensitive to the particular implementation of the embedding task and the procedures that extract specific task - related features from the different parses .
Beyond that , conversion heuristics and task - based procedures are currently developed almost exclusively for English .
Other languages typically lack such resources .
A recent study by Schwartz et al . ( 2011 ) takes a different approach towards cross-annotation evaluation .
They consider different directions of head-dependent relations ( such as on ?
Sunday and Sunday?on ) and different parent-child and grandparent - child relations in a chain ( such as arrive ?
on and arrive ?
sunday in " arrive on sunday " ) as equivalent .
They then score arcs that fall within corresponding equivalence sets .
Using these new scores Schwartz et al . ( 2011 ) neutralize certain annotation discrepancies that distort parse comparison .
However , their treatment is limited to local context and does not treat structures larger than two sequential arcs .
Additionally , since arcs in different directions are typically labeled differently , this method only applies for unlabeled dependencies .
What we need is a fully deterministic and formally precise procedure for comparing any set of labeled or unlabeled dependency trees , by consolidating the shared linguistic content of the complete dependency trees in different annotation schemes , and comparing parse hypotheses through sound metrics that can take into account multiple gold standards .
The Proposal : Cross-Annotation Evaluation in Three Simple Steps
We propose a new protocol for cross-experiment parse evaluation , consisting of three fundamental components : ( i ) abstracting away from annotation peculiarities , ( ii ) generalizing theory -specific structures into a single linguistically coherent gold standard that contains all and only consistent information from all sources , and ( iii ) defining a sound metric that takes into account the different gold standards that are being considered in the experiments .
In this section we first define functional trees as the common space of formal objects and define a deterministic conversion procedure from dependency trees to functional trees .
Next we define a set of formal operations on functional trees that compute , for every pair of corresponding trees of the same yield , a single gold tree that resolves inconsistencies among gold standard alternatives and combines the information that they share .
Finally , we define scores based on tree edit distance , refined to consider the distance from parses to the overall gold tree as well as the different annotation alternatives .
Preliminaries .
Let T be a finite set of terminal symbols and let L be a set of grammatical relation labels .
A dependency graph d is a directed graph which consists of nodes V d and arcs A d ?
V d ? V d .
We assume that all nodes in V d are labeled by terminal symbols via a function label V : V d ?
T .
A well - formed dependency graph d = ( V d , A d ) for a sentence S = t 1 , t 2 , ... , t n is any dependency graph that is a directed tree originating out of a node v 0 labeled t 0 = ROOT , and spans all terminals in the sentence , that is , for every t i ?
S there exists v j ?
V d labeled label V ( v j ) = t i .
For simplicity we assume that every node v j is indexed according to the position of the terminal label , i.e. , that for each t i labeling v j , i always equals j .
In a labeled dependency tree , arcs in A d are labeled by elements of L via a function label A : A d ?
L that encodes the grammatical relation between the terminals labeling the connected nodes .
We define two auxiliary functions on nodes in dependency trees .
The function subtree : Step 1 : Functional Representation
Our first goal is to define a representation format that keeps all functional relationships that are represented in the dependency trees intact , but remains neutral with respect to the directionality of the head- dependent relations .
To do so we define functional trees - linearly - ordered labeled trees which , instead of head - to - head binary relations , represent the complete functional structure of a sentence .
Assuming the same sets of terminal symbols T and grammatical relation labels L , and assuming extended sets of nodes V and arcs A ? V ? V , a functional tree ? = ( V , A ) is a directed tree originating from a single root v 0 ?
V where all non-terminal nodes in ? are labeled with grammatical relation labels that signify the grammatical function of the chunk they dominate inside the tree via label NT : V ? L. All terminal nodes in ? are labeled with terminal symbols via a label T : V ? T function .
The function span : V ? P( V ) now picks out the set of terminal labels of the terminal nodes accessible by a node v ?
V via A .
We obtain functional trees from dependency trees using the following procedure : V d ? P( V d ) assigns to every node v ?
V d the ?
Initialize the set of nodes and arcs in the tree .
V := V d A := A d ?
Label each node v ?
V with the label of its incoming arc. label NT ( v ) = label A ( u , v ) ?
In case | span ( v ) | > 1 add a new node u as a daughter designating the lexical head , labeled with the wildcard symbol * : V := V ? {u} A := A ? {( v , u ) } label NT ( u ) = * ?
For each node v such that | span ( v ) | = 1 , add a new node u as a daughter , labeled with its own terminal : V := V ? {u} A := A ? {( v , u ) } if ( label NT ( v ) = * ) label T ( u ) := label V ( v ) else label T ( u ) := label V ( parent ( v ) )
That is to say , we label all nodes with spans greater than 1 with the grammatical function of their head , and for each node we add a new daughter u designating the head word , labeled with its grammatical function .
Wildcard labels are compatible with any , more specific , grammatical function of the word inside the phrase .
This gives us a constituencylike representation of dependency trees labeled with functional information , which retains the linguistic assumptions reflected in the dependency trees .
When applying this procedure , examples ( 1 ) - ( 3 ) get transformed into ( 4 ) -( 6 ) respectively .
Considering the functional trees resulting from our procedure , it is easy to see that for tree pairs ( 4 a ) -( 4 b ) and ( 5a ) - ( 5 b ) the respective functional trees are identical modulo wildcards , while tree pairs ( 5 b ) -( 5 c ) and ( 6a ) - ( 6 b ) end up with different tree structures that realize different assumptions concerning the internal structure of the tree .
In order to compare , combine or detect inconsistencies in the information inherent in different functional trees , we define a set of formal operations that are inspired by familiar notions from unification - based formalisms ( Shieber ( 1986 ) and references therein ) .
Step 2 : Formal Operations on Trees
The intuition behind the formal operations we define is simple .
A completely flat tree over a span is the most general structural description that can be given to it .
The more nodes dominate a span , the more linguistic assumptions are made with respect to its structure .
If an arc structure in one tree merely elaborates an existing flat span in another tree , the theories underlying the schemes are compatible , and their information can be combined .
Otherwise , there exists a conflict in the linguistic assumptions , and we need to relax some of the assumptions , i.e. , remove functional nodes , in order to obtain a coherent structure that contains the information on which they agree .
Let ? 1 , ? 2 be functional trees over the same yield t 1 , .. , t n .
Let the function span ( v ) pick out the terminals labeling terminal nodes that are accessible via a node v ?
V in the functional tree through the relation A .
We define first the tree subsumption relation for comparing the amount of information inherent in the arc-structure of two trees .
2 T- Subsumption , denoted t , is a relation between trees which indicates that a tree ?
1 is consistent with and more general than tree ?
2 . Formally : ? 1 t ?
2 iff for every node n ? ?
1 there exists a node m ? ? 2 such that span ( n ) = span ( m ) and label ( n ) = label ( m ) .
Looking at the functional trees of ( 4 a ) -( 4 b ) we see that their unlabeled skeletons mutually subsume each other .
In their labeled versions , however , each tree contains labeling information that is lacking in the other .
In the functional trees ( 5 b ) - ( 5 c ) a flat structure over a span in ( 5 b ) is more elaborated in ( 5 c ) .
In order to combine information in trees with compatible arc structures , we define tree unification .
T-Unification , denoted t , is the operation that returns the most general tree structure ?
3 that is subsumed by both ?
1 , ? 2 if such exists , and fails otherwise .
Formally : ? 1 t ? 2 = ? 3 iff ? 1 t ? 3 and ?
2 t ? 3 , and for all ?
4 such that ?
1 t ? 4 and ?
2 t ? 4 it holds that ?
3 t ? 4 .
Tree unification collects the information from two trees into a single result if they are consistent , and detects an inconsistency otherwise .
In case of an inconsistency , as is the case in the functional trees ( 6a ) and ( 6 b ) , we cannot unify the structures due to a conflict concerning the internal division of an expression into phrases .
However , we still want to generalize these two trees into one tree that contains all and only the information that they share .
For that we define the tree generalization operation .
T-Generalization , denoted t , is the operation that returns the most specific tree that is more general than both trees .
Formally , ? 1 t ? 2 = ? 3 iff ? 3 t ? 1 and ?
3 t ? 2 , and for every ?
4 such that ?
4 t ? 1 and ?
4 t ?
2 it holds that ?
4 t ? 3 . Unlike unification , generalization can never fail .
For every pair of trees there exists a tree that is more general than both : in the extreme case , pick the completely flat structure over the yield , which is more general than any other structure .
For ( 6a ) - ( 6 b ) , for instance , we get that ( 6a ) t ( 6 b ) is a flat tree over pre-terminals where " would " and " have " are labeled with ' vg ' and " worked " is the head , labeled with '*' .
The generalization of two functional trees provides us with one structure that reflects the common and consistent content of the two trees .
These structures thus provide us with a formally well - defined gold standard for cross-treebank evaluation .
Step 3 : Measuring Distances .
Our functional trees superficially look like constituency - based trees , so a simple proposal would be to use Parseval measures ( Black et al. , 1991 ) for comparing the parsed trees against the new generalized gold trees .
Parseval scores , however , have two significant drawbacks .
First , they are known to be too restrictive with respect to some errors and too permissive with respect to others ( Carroll et al. , 1998 ; K?bler and Telljohann , 2002 ; Roark , 2002 ; Rehbein and van Genabith , 2007 ) .
Secondly , F 1 scores would still penalize structures that are correct with respect to the original gold , but are not there in the generalized structure .
Here we propose to adopt measures that are based on tree edit distance ( TED ) instead .
TEDbased measures are , in fact , an extension of attachment scores for dependency trees .
Consider , for instance , the following operations on dependency arcs .
reattach - arc remove arc ( u , v ) ?
A d and add an arc A d ? { ( w , v ) }. relabel - arc relabel arc l 1 ( u , v ) as l 2 ( u , v) Assuming that each operation is assigned a cost , the attachment score of comparing two dependency trees is simply the cost of all edit operations that are required to turn a parse tree into its gold standard , normalized with respect to the overall size of the dependency tree and subtracted from a unity .
3
Here we apply the idea of defining scores by TED costs normalized relative to the size of the tree and substracted from a unity , and extend it from fixed - size dependency trees to ordered trees of arbitrary size .
Our formalization follows closely the formulation of the T-Dice measure of Emms ( 2008 ) , building on his thorough investigation of the formal and empirical differences between TED - based measures and Parseval .
We first define for any ordered and labeled tree ? the following operations .
relabel - node change the label of node v in ? delete-node delete a non-root node v in ? with parent u , making the children of v the children of u , inserted in the place of v as a subsequence in the left-to - right order of the children of u. insert-node insert a node v as a child of u in ? making it the parent of a consecutive subsequence of the children of u .
An edit script ES ( ?
1 , ? 2 ) = {e 0 , e 1 ....e k } between ?
1 and ?
2 is a set of edit operations required for turning ?
1 into ?
2 . Now , assume that we are given a cost function defined for each edit operation .
The cost of ES ( ?
1 , ? 2 ) is the sum of the costs of the operations in the script .
An optimal edit script is an edit script between ?
1 and ?
2 of minimum cost .
ES * ( ?
1 , ? 2 ) = argmin ES ( ? 1 , ? 2 ) e?ES (? 1 , ? 2 ) cost ( e )
The tree edit distance problem is defined to be the problem of finding the optimal edit script and computing the corresponding distance ( Bille , 2005 ) .
A simple way to calculate the error ? of a parse would be to define it as the edit distance between the parse hypothesis ?
1 and the gold standard ?
2 . ?(?
1 , ? 2 ) = cost ( ES * ( ? 1 , ? 2 ) )
However , in such cases the parser may still get penalized for recovering nodes that are lacking in the generalization .
To solve this , we refine the distance between a parse tree and the generalized gold tree to discard edit operations on nodes that are there in the native gold tree but are eliminated through generalization .
We compute the intersection of the edit script turning the parse tree into the generalize gold with the edit script turning the native gold tree into the generalized gold , and discard its cost .
That is , if parse1 and parse2 are compared against gold1 and gold2 respectively , and if we set gold3 to be the result of gold1 t gold2 , then ? new is defined as : Now , if gold1 and gold3 are identical , then ES * ( gold1 , gold3 ) =?
and we fall back on the simple tree edit distance score ? new ( parse1 , gold1 , gold3 ) =?( parse1 , gold3 ) .
When parse1 and gold1 are identical , i.e. , the parser produced perfect output with respect to its own scheme , then ? new ( parse1 , gold1 , gold3 ) =? new ( gold1 , gold1 , gold3 ) =?( gold1 , gold3 ) ? cost ( ES * ( gold1 , gold3 ) ) =0 , and the parser does not get penalized for recovering a correct structure in gold1 that is lacking in gold3 .
In order to turn distances into accuracy measures we have to normalize distances relative to the maximal number of operations that is conceivable .
In the worst case , we would have to remove all the internal nodes in the parse tree and add all the internal nodes of the generalized gold , so our normalization factor ? is defined as follows , where |?| is the size 4 of ?.
?( parse1 , gold3 ) = | parse1 | + | gold3 |
We now define the score of parse1 as follows : 5 1 ? ? new ( parse1 , gold1 , gold3 ) ?( parse1 , gold3 )
Figure 2 summarizes the steps in the evaluation procedure we defined so far .
We start off with two versions of the treebank , TB1 and TB2 , which are parsed separately and provide their own gold standards and parse hypotheses in a labeled dependencies format .
All dependency trees are then converted into functional trees , and we compute the generalization of each pair of gold trees for each sentence in the data .
This provides the generalized gold standard for all experiments , here marked as gold3 .
6
We finally compute the distances ? new ( parse1 , gold1 , gold3 ) and ? new ( parse2 , gold2 , gold3 ) using the different tree edit distances that are now available , and we repeat the procedure for each sentence in the test set .
To normalize the scores for an entire test set of size n we can take the arithmetic mean of the scores .
| test-set | i=1 score( parse1 i , gold1 i , gold3 i ) | test- set |
Alternatively we can globally average of all edit distance costs , normalized by the maximally possible edits on parse trees turned into generalized trees .
1 ? | test-set | i=1 ? new ( parse1 i , gold1 i , gold3 i ) | test-set | i=1 ?( parse1 i , gold3 i )
The latter score , global averaging over the entire test set , is the metric we use in our evaluation procedure .
Experiments
We demonstrate the application of our procedure to comparing dependency parsing results on different versions of the Penn Treebank ( Marcus et al. , 1993 ) .
The Data
We use data from the PTB , converted into dependency structures using the LTH software , a general purpose tool for constituency - todependency conversion ( Johansson and Nugues , 2007 ) .
We use LTH to implement the five different annotation standards detailed in Table 3 .
The Default , OldLTH and CoNLL schemes mainly differ in their coordination structure , and the Functional and Lexical schemes differ in their selection of a functional and a lexical head , respectively .
All schemes use the same inventory of labels .
7
The LTH parameter settings for the different schemes are elaborated in the supplementary material .
The Setup
We use two different parsers : ( i ) Malt - Parser ( Nivre et al. , 2007 b ) with the arc eager algorithm as optimized for English in ( Nivre et al. , 2010 ) and ( ii ) MSTParser with the second-order projective model of McDonald and Pereira ( 2006 ) .
Both parsers were trained on the different instances of sections 2 - 21 of the PTB obeying the different annotation schemes in Table 3 . Each trained model was used to parse section 23 .
All non-projective dependencies in the training and gold sets were projectivized prior to training and parsing using the algorithm of Nivre and Nilsson ( 2005 ) .
A more principled treatment of non-projective dependency trees is an important topic for future research .
We evaluated the parses using labeled and unlabeled attachment scores , and using our TEDEVAL software package .
Evaluation Our TEDEVAL software package implements the pipeline described in Section 3 .
We convert all parse and gold trees into functional trees using the algorithm defined in Section 3 , and for each pair of parsing experiments we calculate a shared gold standard using generalization determined through a chart- based greedy algorithm .
8 Our scoring procedure uses the TED algorithm defined by Zhang and Shasha ( 1989 ) .
9
The unlabeled score is obtained by assigning cost ( e ) = 0 for every e relabeling operation .
To calculate pairwise statistical significance we use a shuffling test with 10,000 iterations ( Cohen , 1995 ) .
A sample of all files in the evaluation pipeline for a subset of 10 PTB sentences is available in the supplementary materials .
10 7 In case the labels are not taken from the same inventory , e.g. , subjects in one scheme are marked as SUB and in the other marked as SBJ , it is possible define a a set of zero-cost operation types - in such case , to the operation relabel ( SUB , SBJ ) - in order not to penalize string label discrepancies .
8
Our algorithm has space and runtime complexity of O( n 2 ) .
2 reports the evaluation of the parses produced by MSTParser for the same experimental setup .
Our goal here is not to compare the parsers , but to verify that the effects of switching from LAS to TEDEVAL are robust across parsing algorithms .
In each of the tables , the top three groups of four rows compare results of parsed dependency trees trained on a particular scheme against gold trees of the same and the other schemes .
The next three groups of two rows report the results for comparing pairwise sets of experiments against a generalized gold using our proposed procedure .
In the last group of two rows we compare all parsing results against a single gold obtained through a three - way generalization .
As expected , every parser appears to perform at its best when evaluated against the scheme it was trained on .
This is the case for both LAS and TEDE - VAL measures and the performance gaps are statistically significant .
When moving to pairwise evaluation against a single generalized gold , for instance , when comparing CoNLL07 to the Default settings , there is still a gap in performance , e.g. , between OldLTH and CoNLL07 , and between OldLTH and Default .
This gap is however a lot smaller and is not always statistically significant .
In fact , when evaluating the effect of linguistically disparate annotation variations such as Lexical and Functional on the performance of MaltParser , Table 1 shows that when using TEDEVAL and a generalized gold the performance gaps are small and statistically insignificant .
Moreover , observed performance trends when evaluating individual experiments on their original training scheme may change when compared against a generalized gold .
The Default scheme , for Malt - Parser , appears better than OldLTH when both are evaluated against their training schemes .
But looking at the pairwise - evaluated experiments , it is the other way round ( the difference is smaller , but statistically significant ) .
In evaluating against a three - way generalization , all the results obtained for different training schemes are on a par with one another , with minor gaps in performance , rarely statistically significant .
This suggests that apparent performance trends between experiments when evaluating with respect to the training schemes may be misleading .
These observations are robust across parsing algorithms .
In each of the tables , results obtained against the training schemes show significant differences whereas applying our cross-experimental procedure shows small to no gaps in performance across different schemes .
Annotation variants which seem to have crucial effects have a relatively small influence when parsed structures are brought into the same formal and theoretical common ground for comparison .
Of course , it may be the case that one parser is better trained on one scheme while the other utilizes better another scheme , but objective performance gaps can only be observed when they are compared against shared linguistic content .
Discussion and Extensions
This paper addresses the problem of crossexperiment evaluation .
As it turns out , this problem arises in NLP in different shapes and forms ; when evaluating a parser against different annotation schemes , when evaluating parsing performance across parsers and different formalisms , and when comparing parser performance across languages .
We consider our contribution successful if after reading it the reader develops a healthy suspicion to blunt comparison of numbers across experiments , or better yet , across different papers .
Cross-experiment comparison should be a careful and well thoughtthrough endeavor , in which we retain as much information as we can from the parsed structures , avoid lossy conversions , and focus on an object of evaluation which is agreed upon by all variants .
Our proposal introduces one way of doing so in a streamlined , efficient and formally worked out way .
While individual components may be further refined or improved , the proposed setup and implementation can be straightforwardly applied to crossparser and cross-framework evaluation .
In the future we plan to use this procedure for comparing constituency and dependency parsers .
A conversion from constituency - based trees into functional trees is straightforward to define : simply replace the node labels with the grammatical function of their dominating arc - and the rest of the pipeline follows .
A pre-condition for cross-framework evaluation is that all representations encode the same set of grammatical relations by , e.g. , annotating arcs in dependency trees or decorating nodes in constituency trees .
For some treebanks this is already the case ( Nivre and Megyesi , 2007 ; Skut et al. , 1997 ; Hinrichs et al. , 2004 ) while for others this is still lacking .
Recent studies ( Briscoe et al. , 2002 ; de Marneffe et al. , 2006 ) suggest that evaluation through a single set of grammatical relations as the common denominator is a linguistically sound and practically useful way to go .
To guarantee extensions for crossframework evaluation it would be fruitful to make sure that resources use the same set of grammatical relation labels across different formal representation types .
Moreover , we further aim to inquire whether we can find a single set of grammatical relation labels that can be used across treebanks for multiple languages .
This would then pave the way for the development of cross-language evaluation procedures .
Conclusion
We propose an end-to- end procedure for comparing dependency parsing results across experiments based on three steps : ( i ) converting dependency trees to functional trees , ( ii ) generalizing functional trees to harmonize information from different sources , and ( iii ) using distance - based metrics that take the different sources into account .
When applied to parsing results of different dependency schemes , dramatic gaps observed when comparing parsing results obtained in isolation decrease or dissolve completely when using our proposed pipeline .
Figure 1 : 1 Figure 1 : Calculating cross-experiment LAS results
