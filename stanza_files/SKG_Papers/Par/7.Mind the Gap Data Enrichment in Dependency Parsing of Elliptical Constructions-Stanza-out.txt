title
Mind the Gap : Data Enrichment in Dependency Parsing of Elliptical Constructions
abstract
In this paper , we focus on parsing rare and non-trivial constructions , in particular ellipsis .
We report on several experiments in enrichment of training data for this specific construction , evaluated on five languages : Czech , English , Finnish , Russian and Slovak .
These data enrichment methods draw upon self-training and tri-training , combined with a stratified sampling method mimicking the structural complexity of the original treebank .
In addition , using these same methods , we also demonstrate small improvements over the CoNLL - 17 parsing shared task winning system for four of the five languages , not only restricted to the elliptical constructions .
Introduction
Dependency parsing of natural language text may seem like a solved problem , at least for resourcerich languages and domains , where state - of - theart parsers attack or surpass 90 % labeled attachment score ( LAS ) .
However , certain syntactic phenomena such as coordination and ellipsis are notoriously hard and even stateof - the - art parsers could benefit from better models of these constructions .
Our work focuses on one such construction that combines both coordination and ellipsis : gapping , an omission of a repeated predicate which can be understood from context ( Coppock , 2001 ) .
For example , in Mary won gold and Peter bronze , the second instance of the verb is omitted , as the meaning is evident from the context .
In dependency parsing this creates a situation where the parent node is missing ( omitted verb won ) while its dependents are still present ( Peter and bronze ) .
In the Universal Dependencies annotation scheme ( Nivre et al. , 2016 ) gapping constructions are analyzed by promoting one of the orphaned dependents to the position of its missing parent , and connecting all remaining core arguments to that promoted one with the orphan relation ( see Figure 1 ) .
Therefore the dependency parser must learn to predict relations between words that should not usually be connected .
Gapping has been studied extensively in theoretical works ( Johnson , 2009 ( Johnson , , 2014 Lakoff and Ross , 1970 ; Sag , 1976 ) .
However , it received almost no attention in NLP works , neither concerned with parsing nor with corpora creation .
Among the recent papers , Kummerfeld and Klein ( 2017 ) proposed a one- endpoint - crossing graph parser able to recover a range of null elements and trace types , and Schuster ( Schuster et al. , 2018 ) proposed two methods to recover elided predicates in sentences with gapping .
The aforementioned lack of corpora that would pay attention to gapping , as well as natural relative rarity of gapping , leads to its underrepresentation in training corpora : they do not provide enough examples for the parser to learn gapping .
Therefore we investigate methods of enriching the training data with new material from large raw corpora .
The present work consist of two parts .
In the first part , we experiment on enriching data in general , without a specific focus on gapping constructions .
This part builds upon self-training and tritraining related work known from the literature , but also develops and tests a stratified approach for selecting a structurally balanced subcorpus .
In the second part , we focus on elliptical sentences , comparing general enrichment of training data with enrichment using elliptical sentences artificially constructed by removal of a coordinated element .
Data
Languages and treebanks
For the parsing experiments we selected five treebanks from the Universal Dependencies ( UD ) col- lection ( Nivre et al. , 2016 ) .
We experiment with the following treebanks : UD Czech , UD English , UD Finnish , UD Russian - SynTagRus , and UD Slovak .
With the exception of UD Russian - SynTagRus , all our experiments are based on UD release 2.0 .
This UD release was used in the CoNLL -17 Shared Task on Multilingual Parsing from Raw Text to Universal Dependencies , giving us a point of comparison to the state - of - the - art .
For UD Russian - SynTagRus , we use UD release 2.1 , which has a considerably improved annotation of elliptic sentences .
For English , which has only a few elliptical sentences in the original treebank , we also utilize in testing a set of elliptical sentences gathered by Schuster et al . ( 2018 ) .
This selection of data strives to maximize the amount of elliptical constructions present in the treebanks , while also covering different modern languages and providing variation .
Decisions are based on the work by who collected statistics on elliptical constructions that are explicitly marked with orphan relation within the UD treebanks .
Relatively high number of elliptical constructions within chosen treebanks is the property of the treebanks rather than the languages .
Additional material Automatic parses
As an additional data source in our parsing experiments , we use the multilingual raw text collection by .
This collection includes web crawl data for 45 languages automatically parsed using the UDPipe parser ( Straka and Strakov ? , 2017 ) trained on the UD version 2.0 treebanks .
For Russian , where we use newer version of the treebank , we reparsed the raw data with UDPipe model trained on the corresponding treebank version to agree with the treebank data in use .
As our goal is to use the web crawled data to enrich the official training data in the parsing experiments , we want to ensure the quality of the automatically parsed data .
To achieve this , we apply a method that stands between the standard self-training and tri-training techniques .
In selftraining , the labeled training data ( L ) is iteratively enriched with unlabeled data ( U ) automatically labeled with the same learning system ( L = L+U l ) , whereas in tri-training ( Zhou and Li , 2005 ) there are three different learning systems , A , B and C , and the labeled data for the system A is enriched with instances from U on which the two other systems agree , therefore L a = L + ( U b ? U c ) .
Different variations of these methods have been successfully applied in dependency parsing , for example ( McClosky et al. , 2006 ; S?gaard and Rish ? j , 2010 ; Li et al. , 2014 ; Weiss et al. , 2015 ) .
In this work we use two parsers ( A and B ) to process the unlabeled crawl data , and then the sentences where these two parsers fully agree are used to enrich the training data for the system A , i.e .
L a = L + ( U a ? U b ) .
Therefore the method can be seen as a form of expanded self-training or limited tri-training .
A similar technique is successfully used for example by Sagae and Tsujii ( 2007 ) in parser domain adaptation and Bj?rkelund et al . ( 2014 ) in general parsing .
In our experiments the main parser used in final experiments as well as labeling the crawl data , is the neural graph - based Stanford parser ( Dozat et al. , 2017 ) , the winning and state - of- the - art system from the CoNLL -17 Shared Task .
The secondary parser for labeling the crawl data is UDPipe , a neural transition - based parser , as these parses are already provided together with the crawl data .
Both of these parsers include their own part- of-speech tagger , which is trained together ( but not jointly ) with the dependency parser in all our experiments .
In the final self-training web crawl datasets we then keep only deduplicated sentences with identical partof-speech and dependency analyses .
All results reported in this paper are measured on gold tokenization , and the parser hyperparameters are those used for these systems in the CoNLL -17 Shared Task .
Artificial treebanks on elliptical constructions
For specifically experimenting on elliptical constructions , we additionally include data from the semi-automatically constructed artificial treebanks by Droganova et al . ( 2018 ) .
These treebanks simulate gapping by removing words in particular coordination constructions , providing data for experimenting with the otherwise very rare construction .
For English and Finnish the given datasets are manually curated for grammaticality and fluency , whereas for Czech the quality relies on the rules developed for the process .
For Russian and Slovak , which are not part of the original artificial treebank release , we create automatically constructed artificial datasets by running the pipeline developed for the Czech language .
Size of the artificial data is shown in Table 1 .
Token Sentence
Experiments
First , we set out to evaluate the overall quality of the trees in the raw enrichment dataset produced by our self-training variant by parsing and filtering web crawl data .
In our baseline experiments we train parsers ( Dozat et al. , 2017 ) using purely the new self-training data .
From the full self-training dataset we sample datasets comparable to the sizes of the original treebanks to train parsers .
These parsers are then evaluated using the original test set of the corresponding treebank .
This gives us an overall estimate of the self-training data quality compared to the original treebanks .
Tree sampling Predictably , our automatically selected selftraining data is biased towards short , simple sentences where the parsers are more likely to agree .
Long sentences are in turn often composed of simple coordinated item lists .
To rectify this bias , we employ a sampling method which aims to more closely follow the distribution of the original treebank compared to randomly sampling sentences from the full self-training data .
We base the sampling on two features of every tree : the number of tokens , and the number of unique dependency relation types divided by the number of tokens .
The latter accounts for tree complexity , as it penalizes trees where the same relation type is repeated too many times , and it specifically allows us to downsample the long coordinated item lists where the ratio drops much lower than average .
We of course take into account that a relation type can naturally occur more than once in a sentence , and that it is not ideal to force the ratio close to 1.0 .
However , as the sampling method tries to mimic the distribution from the original treebank , it should to pick the correct variance while discarding the extremes .
The sampling procedure proceeds as follows :
First , we divide the space of the two features , length and complexity , into buckets and estimate from the treebank training data the target distribution , and the expected number of trees to be sampled in each bucket .
Then we select from the full self-training dataset the appropriate number of trees into each bucket .
Since the web crawl data is heavily skewed , it is not possible to obtain a sufficient number of sampled trees in the exact desired distribution , because many rare lengthcomplexity combinations are heavily underrepresented in the data .
We therefore run the sampling procedure in several iterations , until the desired number of trees have been obtained .
This results in a distribution closer to , although not necessarily fully matching , the original treebank .
To evaluate the impact of this sampling procedure , we compare it to two baselines .
RandomS randomly selects the exact same number of sentences as the above-mentioned Identical sampling procedure .
This results in a dataset which is considerably smaller in terms of tokens , because the web crawl data ( on which the two parsers agree ) is heavily biased towards short trees .
To make sure our evaluation is not affected by simply using less data in terms of tokens , we also provide the Ran-domT baseline , where trees are randomly selected until the same number of tokens is reached as in the Identical sample .
Here we are able to evaluate the quality of the sampled data , not its bulk .
In Table 2 we see that , as expected , when sampling the same amount of sentences as in the training section of the original treebank , the RandomS sampling produces datasets considerably smaller in terms of tokens , whereas RandomT results in datasets considerably larger in terms of trees when the same amount of tokens as in the RandomS dataset is sampled .
This confirms the assumption that parsers tend to agree on shorter sentences in the web crawl data , introducing the bias towards them .
On the other hand , when the same number of sentences is selected as in the RandomS sampling and the original treebank , the Identical sampling strategy results in dataset much closer to the original treebank in terms of tokens .
Parsing results for the different sampling strategies are shown in Table 3 . Except for Slovak , the results follow an intuitively expectable pattern : the sample with the least tokens results in the worst score , and of the two samples with the same number of tokens , the one which follows the treebank distribution receives the better score .
Surprisingly , for Slovak the sampling strategy which mimics the treebank distribution receives a score almost 3 pp lower than the one with random sampling of the same amount of tokens .
A possible explanation is given in the description of the Slovak treebank which mentions that it consists of sentences on which two annotators agreed , and is biased towards short and simple sentences .
The data is thus not representative of the language use , possibly causing the effect .
Lacking a better explanation for the time being , we also add the RandomT sampling dataset into our experiments for Slovak .
Overall , the parsing results on the automatically selected data are surprisingly good , lagging only several percent points behind parsers trained on the manually annotated treebanks .
Enrichment
In this section , we test the overall suitability of the sampled trees as an additional data for parsing .
We produce training data composed of the original treebank training section , and a progressively increasing number of sampled trees : 20 % , 100 % , and 200 % ( relative to the treebank training data size , i.e. + 100 % sample doubles the total amount of training data ) .
The parsing results are shown in Table 4 .
Positively , for all languages except Czech , we can improve the overall parsing accuracy , for Slovak by as much as 2.7 pp , which is a rather non-trivial improvement .
In general , the smaller the treebank , the larger the benefit .
With the exception of Slovak , the improvements are relatively modest , in the less than halfa-percent range .
Nevertheless , since our baseline is the winning parser of the CoNLL -17 Shared Task , these constitute improvements over the current state - of - the- art .
Based on these experiments , we can conclude that self-training data extracted from web crawl seem to be suitable material for enriching the training data for parsing , and in next section we continue to test whether the same data and methods can be used to increase occurrences of a rare linguistic construction to make it more learnable for parsers .
Ellipsis
Our special focus point is that of parsing elliptic constructions .
We therefore test whether increasing the number of elliptical sentences in the training data improves the parsing accuracy of these constructions , without sacrificing the overall parsing accuracy .
We follow the same data enrichment methods as used above in general domain and proceed to select elliptical sentences ( recognized through the orphan relation ) from the same selftraining data automatically produced from web crawl ( Section 2.2 ) .
We then train parsers using a combination of the ellipsis subset and the original training section for each language .
We enrich Czech , Russian and Slovak training data with elliptical sentences , progressively increasing their size by 5 % , 10 % and 15 % .
For Finnish , only 5 % of elliptical sentences was available in the filtered web crawl data , and for English not a single sentence .
The experiments showed mixed results ( Table 5 ) .
For Russian and Slovak the accuracy of the dependencies involved in gapping is improved by web crawl enrichment , whereas the results for Czech remained largely the same and Finnish slightly decreased ( column Web crawl ) .
Unfortunately , for Slovak and Finnish , we cannot draw firm conclusions due to the small number of orphan relations in the test set .
For English , even the treebank results are very low : the parser predicts only very few orphan relations ( recall 1.71 % ) and the web crawl data contains no orphans on which the two parsers could agree , thus making it impossible to enrich the data using this method .
Clearly , English requires a different strategy , and we will return to it shortly .
Positively , none of the languages substantially suffered in terms of overall LAS when adding extra elliptical sentences into the training data .
For Slovak , we can even see a significant improvement in overall parsing accuracy , in line with the experiments in Section 3.1 .
Increasing the proportion of orphan sentences in the training data has the predictable effect of in-creasing the orphan F-score and decreasing the overall LAS of the parser .
These differences are nevertheless only very minor and can only be observed for Czech and Russian which have sufficient number of orphan relation examples in the test set .
For Slovak , with 18 examples , we cannot draw any conclusions , and for English and Finnish , there is not a sufficient number of orphan examples in the filtered web crawl data to allow us to vary the proportion .
For all languages , we also experiment with the artificial elliptic sentence dataset of Droganova et al . ( 2018 ) , described earlier in Section 2.2 .
For Czech , English and Finnish , the dataset contains semi-automatically produced , and in the case of English and Finnish , also manually validated instances of elliptic sentences .
For Slovak and Russian , we replicate the procedure of Droganova et al. , sans the manual validation , obtaining artificial orphan datasets for all the five languages under study .
Subsequently , we train parsers using a combination of sentences from the artificial treebank and the original training set .
The results of this experiments are in Table 5 , column Artificial .
Compared to web crawl , the artificial data results in a lower performance on orphans for Czech , Slovak and Russian , and higher for Finnish , but once again keeping in mind the small size of Finnish and Slovak test set , it is difficult to come to a firm conclusion .
Clearly , though , the web crawl data does not perform substantially worse than the artificial data , even though it is gathered fully automatically .
A very substantial improvement is achieved on English , where the web crawl data fails to deliver even a single orphan example , whereas the artificial data gains recall of 9.62 % .
This offers us an opportunity to once again try to obtain orphan examples for English from the web crawl data , since this time we can train the parsers on the combination of the original treebank and the artificial data , hopefully resulting in parsers which are in fact able to predict at least some orphan relations , which in turn can result in new elliptic sentences from the web crawl data .
As seen from Table 5 , the artificial data increases the orphan F-score from 3.36 % to 17.18 % relative to training only on the treebank , and we are therefore able to obtain a parser which is at least by the order of magnitude comparable to the other four languages in parsing accuracy of elliptic constructions .
We observe no loss in terms of the over - Treebank : original treebank ( baseline experiment ) ;
Web crawl : Enriching the original treebank with the elliptical sentences extracted from the automatically parsed web crawl data ; Artificial : Enriching the original treebank with the artificial ellipsis treebank ; LAS , % : overall parsing accuracy ; O Prec ( orphan precision ) : number of correct orphan nodes divided by the number of all predicted orphan nodes ; O Rec ( orphan recall ) : number of correct orphan nodes divided by the number of gold-standard orphan nodes ; O F ( Orphan F-score ) :
Fmeasure restricted to the nodes that are labeled as orphan : 2 PR / ( P+ R ) .
For English , the orphan P/R / F scores are evaluated on a dataset of the two orphan relations in the original test section , combined with 466 English elliptic sentences of Schuster et al . ( 2018 ) .
The extra sentences are not used in the LAS column , so as to preserve comparability of overall LAS scores across the various runs .
all LAS , demonstrating that it is in fact possible to achieve a substantial improvement in parsing of a rare , non-trivial construction without sacrificing the overall performance .
Using the web data self-training filtering procedure with two parsers trained on the tree-bank + artificial data , we can now repeat the experiment with enriching parser training data with orphan relations , results of which are shown in Table 6 .
We test the following models : ? original UD English v.2.0 treebank ; ? original UD English v.2.0 treebank combined with the artificial sentences ; ? original UD English v.2.0 treebank combined with the artificial sentences and web crawl dataset ; size progressively increased by 5 % , 10 % and 15 % .
Here we use the original UD English v.2.0 treebank extended with the artificial sentences to train the models ( Section 2.2 ) that produce the web crawl data for English .
The best orphan F-score of 36 % , more than ten times higher compared to using the original treebank , is obtained by enriching the training data with 15 % elliptic sentences from the artificial and filtered web data .
The orphan F-score of 36 % is on par with the other languages and , positively , the overall LAS of the parser remains essentially unchanged - the parser does not sacrifice anything : F-measure restricted to the nodes that are labeled as orphan : 2 PR / ( P+ R ) .
For English , the orphan P/R / F scores are evaluated on a dataset of the two orphan relations in the original test set , combined with 466 English elliptic sentences of Schuster et al . ( 2018 ) .
The extra sentences are not used in the LAS column , so as to preserve comparability of overall LAS scores across the various runs .
This is necessary since elliptic sentences are typically syntactically more complex and would therefore skew overall parser performance evaluation .
in order to gain the improvement on orphan relations .
These English results therefore not only explore the influence of the number of elliptical sentences on the parsing accuracy , but also test a method applicable in the case where the treebank does not contain almost any elliptical constructions and results in parsers that only generate the relation very rarely .
Conclusions
We have explored several methods of enriching training data for dependency parsers , with a specific focus on rare phenomena such as ellipsis ( gapping ) .
This focused enrichment leads to mixed results .
On one hand , for several languages we did not obtain a significant improvement of the parsing accuracy of ellipsis , possibly in part owing to the small number of testing examples .
On the other hand , though , we have demonstrated that for English ellipsis parsing accuracy can be improved from single digit numbers to performance on par with the other languages .
We have also validated the method of constructing artificial elliptical examples as a mean to enrich parser training data .
Additionally , we have shown that useful training data can be obtained using web crawl data and a self-training or tri-training style method , even though the two parsers in question differ substantially in their overall performance .
Finally , we have shown that this parser training data enrichment can lead to improvements of general parser accuracy , improving upon the state of the art for all but one language .
The improvement was especially notable for Slovak .
Czech was the only treebank not benefiting from this additional data , likely owing to the fact that is is an already very large , and homogenous treebank .
As part of these experiments , we have introduced and demonstrated the effectiveness of a stratified sampling method which corrects for the skewed distribution of sentences selected in the web filtering experiments .
Figure 1 : 1 Figure 1 : UD representation of a sentence with repeated verb ( a ) , and with an omitted verb in a gapping construction ( b ) .
Table 1 : 1 The size of the artificial data Czech 50 K 2876 English 7.3 K 421 Finnish 13 K 1000 Russian 87 K 5000 Slovak 7.1 564
Table 2 : 2 Training data sizes after each sampling strategy compared to the original treebank training section ( TB ) , sentences / tokens .
Table 3 : 3 Results of the baseline parsing experiments , using only automatically collected data , reported in terms of LAS % .
Random T : random sample , same amount of tokens as in the Random S samples ; Random S : random sample , same amount of sentences as in the original treebanks ; Identical : identical sample , imitates the distribution of trees in the original treebanks .
For comparison , the TB column shows the LAS of a parser trained on the original treebank training data .
Language TB + 20 % + 100 % + 200 % Czech 91.20 % 91.13 % 90.98 % 90.72 % English 86.94 % 87.32 % 87.43 % 87.29 % Finnish 87.89 % 87.83 % 88.24 % 88.32 % Russian 93.35 % 93.38 % 93.22 % 93.08 % Slovak 86.04 % 87.89 % 88.36 % 88.36 % Slovak T 86.04 % 88.14 % 88.57 % 88.77 % Table 4 : Enriching treebank data with identical sam- ple from automatic data , LAS % .
TB : original tree - bank ( baseline experiment ; the scores are better than re- ported in the CoNLL -17 Shared Task because we eval - uate on gold segmentation while the shared task sys- tems are evaluated on predicted segmentation ) ; + 20 % -+200 % : size of the identical sample used to enrich the treebank data ( with respect to the original treebank size ) .
Slovak T : enriching Slovak treebank with ran- dom tokens sample instead of identical .
Table 5 : 5 Enriching treebank data with elliptical sentences .
All : number of orphan labels in the test data ;
Table 6 6 : Enriching the English treebank data with elliptical sentences .
LAS , % : overall parsing accu- racy ; O Precision ( orphan precision ) : number of cor- rect orphan labels divided by the number of all pre- dicted orphan nodes ; O Recall ( orphan recall ) : num - ber of correct orphan labels divided by the number of gold -standard orphan nodes ; O F-score ( orphan F- score )
