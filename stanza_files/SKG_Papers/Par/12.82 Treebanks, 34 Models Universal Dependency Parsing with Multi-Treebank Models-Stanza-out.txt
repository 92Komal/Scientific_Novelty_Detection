title
82 Treebanks , 34 Models : Universal Dependency Parsing with Multi-Treebank Models
abstract
We present the Uppsala system for the CoNLL 2018 Shared Task on universal dependency parsing .
Our system is a pipeline consisting of three components : the first performs joint word and sentence segmentation ; the second predicts part-ofspeech tags and morphological features ; the third predicts dependency trees from words and tags .
Instead of training a single parsing model for each treebank , we trained models with multiple treebanks for one language or closely related languages , greatly reducing the number of models .
On the official test run , we ranked 7th of 27 teams for the LAS and MLAS metrics .
Our system obtained the best scores overall for word segmentation , universal POS tagging , and morphological features .
Introduction
The CoNLL 2018 Shared Task on Multilingual Parsing from Raw Text to Universal Dependencies ( Zeman et al. , 2018 ) requires participants to build systems that take as input raw text , without any linguistic annotation , and output full labelled dependency trees for 82 test treebanks covering 46 different languages .
Besides the labeled attachment score ( LAS ) used to evaluate systems in the 2017 edition of the Shared Task ( Zeman et al. , 2017 ) , this year 's task introduces two new metrics : morphology - aware labeled attachment score ( MLAS ) and bi-lexical dependency score ( BLEX ) .
The Uppsala system focuses exclusively on LAS and MLAS , and consists of a three -step pipeline .
The first step is a model for joint sentence and word segmentation which uses the BiRNN - CRF framework of Shao et al . ( , 2018 to predict sentence and word boundaries in the raw input and simultaneously marks multiword tokens that need non-segmental analysis .
The second component is a part-of-speech ( POS ) tagger based on Bohnet et al . ( 2018 ) , which employs a sentence - based character model and also predicts morphological features .
The final stage is a greedy transitionbased dependency parser that takes segmented words and their predicted POS tags as input and produces full dependency trees .
While the segmenter and tagger models are trained on a single treebank , the parser uses multi-treebank learning to boost performance and reduce the number of models .
After evaluation on the official test sets , which was run on the TIRA server ( Potthast et al. , 2014 ) , the Uppsala system ranked 7th of 27 systems with respect to LAS , with a macro-average F1 of 72.37 , and 7th of 27 systems with respect to MLAS , with a macro-average F1 of 59.20 .
It also reached the highest average score for word segmentation ( 98.18 ) , universal POS ( UPOS ) tagging ( 90.91 ) , and morphological features ( 87.59 ) .
Corrigendum :
After the test phase was over , we discovered that we had used a non-permitted resource when developing the UPOS tagger for Thai PUD ( see Section 4 ) .
Setting our LAS , MLAS and UPOS scores to 0.00 for Thai PUD gives the corrected scores : LAS 72.31 , MLAS 59.17 , UPOS 90.50 .
This does not affect the ranking for any of the three scores , as confirmed by the shared task organizers .
Resources
All three components of our system were trained principally on the training sets of Universal Dependencies v2.2 released to coincide with the shared task .
The tagger and parser also make use of the pre-trained word em-beddings provided by the organisers , as well as Facebook word embeddings ( Bojanowski et al. , 2017 ) , and both word and character embeddings trained on Wikipedia text 1 with word2vec ( Mikolov et al. , 2013 ) .
For languages with no training data , we also used external resources in the form of Wikipedia text , parallel data from OPUS ( Tiedemann , 2012 ) , the Moses statistical machine translation system ( Koehn et al. , 2007 ) , and the Apertium morphological transducer for Breton .
2
Sentence and Word Segmentation
We employ the model of Shao et al . ( 2018 ) for joint sentence segmentation and word segmentation .
Given the input character sequence , we model the prediction of word boundary tags as a sequence labelling problem using a BiRNN - CRF framework ( Huang et al. , 2015 ; .
This is complemented with an attentionbased LSTM model ( Bahdanau et al. , 2014 ) for transducing non-segmental multiword tokens .
To enable joint sentence segmentation , we add extra boundary tags as in de Lhoneux et al . ( 2017 a ) .
We use the default parameter settings introduced by Shao et al . ( 2018 ) and train a segmentation model for all treebanks with at least 50 sentences of training data .
For treebanks with less or no training data ( except Thai discussed below ) , we substitute a model for another treebank / language : ? For Japanese Modern , Czech PUD , English PUD and Swedish PUD , we use the model trained on the largest treebank from the same language ( Japanese GSD , Czech PDT , English EWT and Swedish Talbanken ) .
?
For Finnish PUD , we use Finnish TDT rather than the slightly larger Finnish FTB , because the latter does not contain raw text suitable for training a segmenter .
? For Naija NSC , we use English EWT .
?
For other test sets with little or no training data , we select models based on the size of the intersection of the character sets measured on Wikipedia data ( see Table 2 for details ) .
3 Thai Segmentation of Thai was a particularly difficult case :
Thai uses a unique script , with no spaces between words , and there was no training data available .
Spaces in
Thai text can function as sentence boundaries , but are also used equivalently to commas in English .
For
Thai sentence segmentation , we exploited the fact that four other datasets are parallel , i.e. , there is a one- to- one correspondence between sentences in Thai and in Czech PUD , English PUD , Finnish PUD and Swedish PUD .
4 First , we split the Thai text by white space and treat the obtained character strings as potential sentences or sub-sentences .
We then align them to the segmented sentences of the four parallel datasets using the Gale - Church algorithm ( Gale and Church , 1993 ) .
Finally , we compare the sentence boundaries obtained from different parallel datasets and adopt the ones that are shared within at least three parallel datasets .
For word segmentation , we use a trie-based segmenter with a word list derived from the Facebook word embeddings .
5
The segmenter retrieves words by greedy forward maximum matching ( Wong and Chan , 1996 ) .
This method requires no training but gave us the highest word segmentation score of 69.93 % for Thai , compared to the baseline score of 8.56 % .
Tagging and Morphological Analysis
We use two separate instantiations of the tagger 6 described in Bohnet et al . ( 2018 ) to predict UPOS tags and morphological features , respectively .
The tagger uses a Meta-BiLSTM over the output of a sentence - based character model and a word model .
There are two features that mainly distinguishes the tagger from previous work .
The character BiLSTMs use the full context of the sentence in contrast to most other taggers which use words only as context for the character model .
This character model is combined with the word model in the Meta-BiLSTM relatively late , after two layers of BiLSTMs .
For both the word and character models , we use two layers of BiLSTMs with 300 LSTM cells per layer .
We employ batches with 8000 words and 20000 characters .
We keep all other hyperparameters as defined in Bohnet et al . ( 2018 ) .
From the training schema described in the above paper , we deviate slightly in that we perform early stopping on the word , character and meta-model independently .
We apply early stopping due to the performance of the development set ( or training set when no development set is available ) and stop when no improvement occurs in 1000 training steps .
We use the same settings for UPOS tagging and morphological features .
To deal with languages that have little or no training data , we adopt three different strategies : ?
For the PUD treebanks ( except Thai ) , Japanese Modern and Naija NSC , we use the same model substitutions as for segmentation ( see Table 2 ) . ?
For Faroese we used the model for Norwegian Nynorsk , as we believe this to be the most closely related language .
?
For treebanks with small training sets we use only the provided training sets for training .
Since these treebanks do not have development sets , we use the training sets for early stopping as well .
?
For Breton and Thai , which have no training sets and no suitable substitution models , we use a bootstrapping approach to train taggers as described below .
Bootstrapping
We first annotate an unlabeled corpus using an external morphological analyzer .
We then create a ( fuzzy and context- independent ) mapping from the morphological analysis to universal POS tags and features , which allows us to relabel the annotated corpus and train taggers using the same settings as for other languages .
For Breton , we annotated about 60,000 sentences from Breton OfisPublik , which is part of OPUS , 7 using the Apertium morphological analyzer .
The Apertium tags could be mapped to universal POS tags and a few morphological features like person , number and gender .
For Thai , we annotated about 33,000 sentences from Wikipedia using PyThaiNLP 8 and mapped only to UPOS tags ( no features ) .
Unfortunately , we realized only after the test phase that PyThaiNLP was not a permitted resource , which invalidates our UPOS tagging scores for Thai , as well as the LAS and MLAS scores which depend on the tagger .
Note , however , that the score for morphological features is not affected , as we did not predict features at all for Thai .
The same goes for sentence and word segmentation , which do not depend on the tagger .
Lemmas
Due to time constraints we chose not to focus on the BLEX metric in this shared task .
In order to avoid zero scores , however , we simply copied a lowercased version of the raw token into the lemma column .
Dependency Parsing
We use a greedy transition - based parser ( Nivre , 2008 ) based on the framework of Kiperwasser and Goldberg ( 2016 b ) where BiLSTMs ( Hochreiter and Schmidhuber , 1997 ; Graves , 2008 ) learn representations of tokens in context , and are trained together with a multi-layer perceptron that predicts transitions and arc labels based on a few BiLSTM vectors .
Our parser is extended with a SWAP transition to allow the construction of nonprojective dependency trees ( Nivre , 2009 ) .
We also introduce a static-dynamic oracle to allow the parser to learn from non-optimal configurations at training time in order to recover better from mistakes at test time ( de Lhoneux et al. , 2017 b ) .
In our parser , the vector representation x i of a word type w i before it is passed to the BiLSTM feature extractors is given by : x i = e( w i ) ? e( p i ) ? BiLSTM ( ch 1 : m ) .
Here , e( w i ) represents the word embedding and e( p i ) the POS tag embedding ( Chen and Manning , 2014 ) ; these are concatenated to a character - based vector , obtained by running a BiLSTM over the characters ch 1 : m of w i .
With the aim of training multi-treebank models , we additionally created a variant of the parser which adds a treebank embedding e( tb i ) to input vectors in a spirit similar to the language embeddings of Ammar et al . ( 2016 ) and de Lhoneux et al . ( 2017 a ) : x i = e( w i ) ? e( p i ) ? BiLSTM ( ch 1 : m ) ? e( tb i ) .
We have previously shown that treebank embeddings provide an effective way to combine multiple monolingual heterogeneous treebanks and applied them to lowresource languages ( de Lhoneux et al. , 2017a ) .
In this shared task , the treebank embedding model was used both monolingually , to combine several treebanks for a single language , and multilingually , mainly for closely related languages , both for languages with no or small treebanks , and for languages with medium and large treebanks , as described in Section 6 .
During training , a word embedding for each word type in the training data is initialized using the pre-trained embeddings provided by the organizers where available .
For the remaining languages , we use different strategies : ? For Afrikaans , Armenian , Buryat , Gothic , Kurmanji , North Sami , Serbian and Upper Sorbian , we carry out our own pre-training on the Wikipedia dumps of these languages , tokenising them with the baseline UDPipe models and running the implementation of word2vec in the Gensim Python library 9 with 30 iterations and a minimum count of 1 . ?
For Breton and Thai , we use specially - trained multilingual embeddings ( see Section 6 ) .
?
For Naija and Old French , we substitute English and French embeddings , respectively .
?
For Faroese , we do not use pre-trained embeddings .
While it is possible to train such embeddings on Wikipedia data , as there is no UD training data for Faroese we choose instead to rely on its similarity to other Scandinavian languages ( see Section 6 ) .
Word types in the training data that are not found amongst the pre-trained embeddings are initialized randomly using Glorot initialization ( Glorot and Bengio , 2010 ) , as are all POS tag and treebank embeddings .
Character vectors are also initialized randomly , except for Chinese , Japanese and Korean , in which case we pre-train character vectors using word2vec on the Wikipedia dumps of these languages .
At test time , we first look for out - of- vocabulary ( OOV ) words and characters ( i.e. , those that are not found in the treebank training data ) amongst the pre-trained embeddings and otherwise assign them a trained OOV vector .
10 A variant of word dropout is applied to the word embeddings , as described in Kiperwasser and Goldberg ( 2016 a ) , and we apply dropout also to the character vectors .
We use the extended feature set of Kiperwasser and Goldberg ( 2016 b ) ( top 3 items on the stack together with their rightmost and leftmost depen- dents plus first item on the buffer with its leftmost dependent ) .
We train all models for 30 epochs with hyper-parameter settings shown in Table 1 .
Note our unusually large character embedding sizes ; we have previously found these to be effective , especially for morphologically rich languages .
Our code is publicly available .
We release the version used here as UU - Parser 2.3 .
11 Using Morphological Features
Having a strong morphological analyzer , we were interested in finding out whether or not we can improve parsing accuracy using predicted morphological information .
We conducted several experiments on the development sets for a subset of treebanks .
However , no experiment gave us any improvement in terms of LAS and we decided not to use this technique for the shared task .
What we tried was to create an embedding representing either the full set of morphological features or a subset of potentially useful features , for example case ( which has been shown to be useful for parsing by Kapociute - Dzikiene et al . ( 2013 ) and Eryigit et al . ( 2008 ) ) , verb form and a few others .
That embedding was concatenated to the word embedding at the input of the BiLSTM .
We varied the embedding size ( 10 , 20 , 30 , 40 ) , tried different subsets of morphological features , and tried with and without using dropout on that embedding .
We also tried creating an embedding of a concatenation of the universal POS tag and the Case feature and replace the POS embedding with this one .
We are currently unsure why none of these experiments were successful and plan to investigate this in the future .
It would be interesting to find out whether or not this information is captured somewhere else .
A way to test this would be to use diagnostic classifiers on vector representations , as is done for example in Hupkes et al . ( 2018 ) or in Adi et al . ( 2017 ) .
Multi-Treebank Models
One of our main goals was to leverage information across treebanks to improve performance and reduce the number of parsing models .
We use two different types of models :
1 . Single models , where we train one model per treebank ( 17 models applied to 18 treebanks , including special models for Breton KEB and Thai PUD ) .
Multi-treebank models ?
Monolingual models , based on multiple treebanks for one language ( 4 models , trained on 10 treebanks , applied to 11 treebanks ) .
?
Multilingual models , based on treebanks from several ( mostly ) closely related languages ( 12 models , trained on 48 treebanks , applied to 52 treebanks ; plus a special model for Naija NSC ) .
When a multi-treebank model is applied to a test set from a treebank with training data , we naturally use the treebank embedding of that treebank also for the test sentences .
However , when parsing a test set with no corresponding training data , we have to use one of the other treebank embeddings .
In the following , we refer to the treebank selected for this purpose as the proxy treebank ( or simply proxy ) .
In order to keep the training times and language balance in each model reasonable , we cap the number of sentences used from each treebank to 15,000 , with a new random sample selected at each epoch .
This only affects a small number of treebanks , since most training sets are smaller than 15,000 sentences .
For all our multi-treebank models , we apply the treebank embeddings described in Section 5 .
Where two or more treebanks in a multilingual model come from the same language , we use separate treebank embeddings for each of them .
We have previously shown that multi-treebank models can boost LAS in many cases , especially for small treebanks , when applied monolingually , and ap-plied it to low-resource languages ( de Lhoneux et al. , 2017a ) .
In this paper , we add POS tags and pre-trained embeddings to that framework , and extend it to also cover multilingual parsing for languages with varying amounts of training data .
Treebanks sharing a single model are grouped together in Table 2 .
To decide which languages to combine in our multilingual models , we use two sources : knowledge about language families and language relatedness , and clusterings of treebank embeddings from training our parser with all available languages .
We created clusterings by training single parser models with treebank embeddings for all treebanks with training data , capping the maximum number of sentences per treebank to 800 .
We then used Ward 's method to perform a hierarchical cluster analysis .
We found that the most stable clusters were for closely related languages .
There was also a tendency for treebanks containing old languages ( i.e. , Ancient Greek , Gothic , Latin and Old Church Slavonic ) to cluster together .
One reason for these languages parsing well together could be that several of the 7 treebanks come from the same annotation projects , four from PROIEL , and two from Perseus , containing consistently annotated and at least partially parallel data , e.g. , from the Bible .
For the multi-treebank models , we performed preliminary experiments on development data investigating the effect of different groupings of languages .
The main tendency we found was that it was better to use smaller groups of closely related languages rather than larger groups of slightly less related languages .
For example , using multilingual models only for Galician - Portuguese and Spanish - Catalan was better than combining all Romance languages in a larger model , and combining Dutch - German - Afrikaans was better than also including English .
A case where we use less related languages is for languages with very little training data ( 31 sentences or less ) , believing that it may be beneficial in this special case .
We implemented this for Buryat , Uyghur and Kazakh , which are trained with Turkish , and Kurmanji , which is trained with Persian , even though these languages are not so closely related .
For Armenian , which has only 50 training sentences , we could not find a close enough language , and instead train a single model on the available data .
For the four languages that are not in a multilingual cluster but have more than one available treebank , we use monolingual multitreebank models ( English , French , Italian and Korean ) .
For the nine treebanks that have no training data we use different strategies : ? For Japanese Modern , we apply the monotreebank Japanese GSD model . ?
For the four PUD treebanks , we apply the multi-treebank models trained using the other treebanks from that language , with the largest available treebank as proxy ( except for Finnish , where we prefer Finnish TDT over FTB ; cf. Section 3 and Stymne et al . ( 2018 ) ) .
?
For Faroese , we apply the model for the Scandinavian languages , which are closely related , with Norwegian Nynorsk as proxy ( cf. Section 4 ) .
In addition , we map the Faroese characters { ?} , which do not occur in the other Scandinavian languages , to { Iyud} .
?
For Naija , an English - based creole , whose treebank according to the README file contains spoken language data , we train a special multilingual model on English EWT and the three small spoken treebanks for French , Norwegian , and Slovenian , and usd English EWT as proxy .
12 ? For Thai and Breton , we create multilingual models trained with word and POS embeddings only ( i.e. , no character models or treebank embeddings ) on Chinese and Irish , respectively .
These models make use of multilingual word embeddings provided with Facebook 's MUSE multilingual embeddings , 13 as described in more detail below .
For all multi-treebank models , we choose the model from the epoch that has the best mean LAS score among the treebanks that have available development data .
This means that treebanks without development data rely on a model that is good for other languages in the group .
In the cases of the mono-treebank Armenian and Irish models , where there is no development data , we choose the 12 We had found this combination to be useful in preliminary experiments where we tried to parse French Spoken without any French training data .
13 https://github.com/facebookresearch/
MUSE model from the final training epoch .
This also applies to the Breton model trained on Irish data .
Thai - Chinese
For the Thai model trained on Chinese , we were able to map Facebook 's monolingual embeddings for each language to English using MUSE , thus creating multilingual Thai- Chinese embeddings .
We then trained a monolingual parser model using the mapped Chinese embeddings to initialize all word embeddings , and ensuring that these were not updated during training ( unlike in the standard parser setup described in Section 5 ) .
At test time , we look up all OOV word types , which are the great majority , in the mapped
Thai embeddings first , otherwise assign them to a learned OOV vector .
Note that in this case , we had to increase the word embedding dimension in our parser to 300 to accomodate the larger Facebook embeddings .
Breton -Irish For Breton and Irish , the Facebook software does not come with the necessary resources to map these languages into English .
Here we instead created a small dictionary by using all available parallel data from OPUS ( Ubuntu , KDE and Gnome , a total of 350 K text snippets ) , and training a statistical machine translation model using Moses ( Koehn et al. , 2007 ) .
From the lexical word- to - word correspondences created , we kept all cases where the translation probabilities in both directions were at least 0.4 and the words were not identical ( in order to exclude a lot of English noise in the data ) , resulting in a word list of 6027 words .
We then trained monolingual embeddings for Breton using word2vec on Wikipedia data , and mapped them directly to Irish using MUSE .
A parser model was then trained , similarly to the Thai- Chinese case , using Irish embeddings as initialization , turning off updates to the word embeddings , and applying the mapped Breton embeddings at test time .
Results and Discussion Table 2 shows selected test results for the Uppsala system , including the two main metrics LAS and MLAS ( plus a mono-treebank baseline for LAS ) , 14 the sentence and word segmentation accuracy , and the accuracy of UPOS tagging and morphological features ( UFEATS ) .
To make the table more readable , we have added a simple color | < ? < | < ?SE < ? < ?+SE < | < ?+?
< | . coding .
Scores that are significantly higher / lower than the mean score of the 21 systems that successfully parsed all test sets are marked with two shades of green / red .
The lighter shade marks differences that are outside the interval defined by the standard error of the mean ( ? ? SE , SE = ?/ ? N ) but within one standard deviation ( std dev ) from the mean .
The darker shade marks differences that are more than one std dev above / below the mean ( ? ? ? ) .
Finally , scores that are no longer valid because of the Thai UPOS tagger are crossed out in yellow cells , and corrected scores are added where relevant .
Looking first at the LAS scores , we see that our results are significantly above the mean for all aggregate sets of treebanks ( ALL , BIG , PUD , SMALL , LOW - RESOURCE ) with an especially strong result for the low-resource group ( even after setting the Thai score to 0.00 ) .
If we look at specific languages , we do particularly well on low-resource languages like Breton , Buryat , Kazakh and Kurmanji , but also on languages like Arabic , Hebrew , Japanese and Chinese , where we benefit from having better word segmentation than most other systems .
Our results are significantly worse than the mean only for Afrikaans AfriBooms , Old French SRCMF , Galician CTG , Latin PROIEL , and Portuguese Bosque .
For Galician and Portuguese , this may be the effect of lower word segmentation and tagging accuracy .
To find out whether our multi-treebank and multi-lingual models were in fact beneficial for parsing accuracy , we ran a post-evaluation experiment with one model per test set , each trained only on a single treebank .
We refer to this as the mono-treebank baseline , and the LAS scores can be found in the second ( uncolored ) LAS column in Table 2 .
The results show that merging treebanks and languages did in fact improve parsing accuracy in a remarkably consistent fashion .
For the 64 test sets that were parsed with a multi-treebank model , only four had a ( marginally ) higher score with the mono-treebank baseline model : Estonian EDT , Russian SynTagRus , Slovenian SSJ , and Turkish IMST .
Looking at the aggregate sets , we see that , as expected , the pooling of resources helps most for LOW - RESOURCE ( 25.33 vs. 17.72 ) and SMALL ( 63.60 vs. 60.06 ) , but even for BIG there is some improvement ( 80.21 vs. 79.61 ) .
We find these results very encouraging , as they indicate that our treebank embedding method is a reli-able method for pooling training data both within and across languages .
It is also worth noting that this method is easy to use and does not require extra external resources used in most work on multilingual parsing , like multilingual word embeddings ( Ammar et al. , 2016 ) or linguistic re-write rules ( Aufrant et al. , 2016 ) to achieve good results .
Turning to the MLAS scores , we see a very similar picture , but our results are relatively speaking stronger also for PUD and SMALL .
There are a few striking reversals , where we do significantly better than the mean for LAS but significantly worse for MLAS , including Buryat BDT , Hebrew HTB and Ukrainian IU .
Buryat and Ukrainian are languages for which we use a multilingual model for parsing , but not for UPOS tagging and morphological features , so it may be due to sparse data for tags and morphology , since these languages have very little training data .
This is supported by the observation that low-resource languages in general have a larger drop from LAS to MLAS than other languages .
For sentence segmentation , the Uppsala system achieved the second best scores overall , and results are significantly above the mean for all aggregates except SMALL , which perhaps indicates a sensitivity to data sparseness for the data-driven joint sentence and word segmenter ( we see the same pattern for word segmentation ) .
However , there is a much larger variance in results than for the parsing scores , with altogether 23 treebanks having scores significantly below the mean .
For word segmentation , we obtained the best results overall , strongly outperforming the mean for all groups except SMALL .
We know from previous work ( Shao et al. , 2018 ) that our word segmenter performs well on more challenging languages like Arabic , Hebrew , Japanese , and Chinese ( although we were beaten by the Stanford team for the former two and by the HIT - SCIR team for the latter two ) .
By contrast , it sometimes falls below the mean for the easier languages , but typically only by a very small fraction ( for example 99.99 vs. 100.00 for 3 treebanks ) .
Finally , it is worth noting that the maximum-matching segmenter developed specifically for Thai achieved a score of 69.93 , which was more than 5 points better than any other system .
Our results for UPOS tagging indicate that this may be the strongest component of the system , although it is clearly helped by getting its input from a highly accurate word segmenter .
The Uppsala system ranks first overall with scores more than one std dev above the mean for all aggregates .
There is also much less variance than in the segmentation results , and scores are significantly below the mean only for five treebanks : Galician CTG , Gothic PROIEL , Hebrew HTB , Upper Sorbian UFAL , and Portuguese Bosque .
For Galician and Upper Sorbian , the result can at least partly be explained by a lower - than - average word segmentation accuracy .
The results for morphological features are similar to the ones for UPOS tagging , with the best overall score but with less substantial improvements over the mean .
The four treebanks where scores are significantly below the mean are all languages with little or no training data : Upper Sorbian UFAL , Hungarian Szeged , Naija NSC and Ukrainian IU .
All in all , the 2018 edition of the Uppsala parser can be characterized as a system that is strong on segmentation ( especially word segmentation ) and prediction of UPOS tags and morphological features , and where the dependency parsing component performs well in low-resource scenarios thanks to the use of multi-treebank models , both within and across languages .
For what it is worth , we also seem to have the highest ranking singleparser transition - based system in a task that is otherwise dominated by graph - based models , in particular variants of the winning Stanford system from 2017 ( Dozat et al. , 2017 ) .
Extrinsic Parser Evaluation
In addition to the official shared task evaluation , we also participated in the 2018 edition of the Extrinsic Parser Evaluation Initiative ( EPE ) ( Fares et al. , 2018 ) , where parsers developed for the CoNLL 2018 shared task were evaluated with respect to their contribution to three downstream systems : biological event extraction , fine - grained opinion analysis , and negation resolution .
The downstream systems are available for English only , and we participated with our English model trained on English EWT , English LinES and English GUM , using English EWT as the proxy .
In the extrinsic evaluation , the Uppsala system ranked second for event extraction , first for opinion analysis , and 16th ( out of 16 systems ) for negation resolution .
Our results for the first two tasks are better than expected , given that our system ranks in the middle with respect to intrinsic evaluation on English ( 9th for LAS , 6th for UPOS ) .
By contrast , our performance is very low on the negation resolution task , which we suspect is due to the fact that our system only predicts universal part- of-speech tags ( UPOS ) and not the language specific PTB tags ( XPOS ) , since the three systems that only predict UPOS are all ranked at the bottom of the list .
Conclusion
We have described the Uppsala submission to the CoNLL 2018 shared task , consisting of a segmenter that jointly extracts words and sentences from a raw text , a tagger that provides UPOS tags and morphological features , and a parser that builds a dependency tree given the words and tags of each sentence .
For the parser we applied multi-treebank models both monolingually and multilingually , resulting in only 34 models for 82 treebanks as well as significant improvements in parsing accuracy especially for low-resource languages .
We ranked 7th for the official LAS and MLAS scores , and first for the unofficial scores on word segmentation , UPOS tagging and morphological features .
Table 1 : 1 Hyper-parameter values for parsing .
Character embedding dimension 500 Character BiLSTM layers 1 Character BiLSTM output dimension 200 Word embedding dimension 100 POS embedding dimension 20 Treebank embedding dimension 12 Word BiLSTM layers 2 Word BiLSTM hidden / output dimension 250 Hidden units in MLP 100 Word dropout 0.33 ? ( for OOV vector training ) 0.25 Character dropout 0.33 p agg ( for exploration training ) 0.1
