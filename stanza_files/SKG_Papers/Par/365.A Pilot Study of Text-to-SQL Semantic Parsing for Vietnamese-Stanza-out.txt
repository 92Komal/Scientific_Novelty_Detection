title
A Pilot Study of Text-to- SQL Semantic Parsing for Vietnamese
abstract
Semantic parsing is an important NLP task .
However , Vietnamese is a low-resource language in this research area .
In this paper , we present the first public large-scale Text-to - SQL semantic parsing dataset for Vietnamese .
We extend and evaluate two strong semantic parsing baselines EditSQL and IRNet ( Guo et al. , 2019 ) on our dataset .
We compare the two baselines with key configurations and find that : automatic Vietnamese word segmentation improves the parsing results of both baselines ; the normalized pointwise mutual information ( NPMI ) score ( Bouma , 2009 ) is useful for schema linking ; latent syntactic features extracted from a neural dependency parser for Vietnamese also improve the results ; and the monolingual language model PhoBERT for Vietnamese ( Nguyen and Nguyen , 2020 ) helps produce higher performances than the recent best multilingual language model XLM -R ( Conneau et al. , 2020 ) .
Introduction Semantic parsing is the task of converting natural language sentences into meaning representations such as logical forms or standard SQL database queries ( Mooney , 2007 ) , which serves as an important component in many NLP systems such as Question answering and Task-oriented dialogue ( Androutsopoulos et al. , 1995 ; Moldovan et al. , 2003 ; Guo et al. , 2018 ) .
The significant availability of the world 's knowledge stored in relational databases leads to the creation of large-scale Text-to - SQL datasets , such as WikiSQL ( Zhong et al. , 2017 ) and Spider ( Yu et al. , 2018 ) , which help boost the development of various state- ofthe - art sequence - to-sequence ( seq2seq ) semantic parsers ( Bogin et al. , 2019 ; ; Guo *
Work done during internship at VinAI Research .
et al. , 2019 ) .
Compared to WikiSQL , the Spider dataset presents challenges not only in handling complex questions but also in generalizing to unseen databases during evaluation .
Most SQL semantic parsing benchmarks , such as WikiSQL and Spider , are exclusively for English .
Thus the development of semantic parsers has largely been limited to the English language .
As SQL is a database interface and universal semantic representation , it is worth investigating the Text-to - SQL semantic parsing task for languages other than English .
Especially , the difference in linguistic characteristics could add difficulties in applying seq2seq semantic parsing models to the non-English languages ( Min et al. , 2019 ) .
For example , about 85 % of word types in Vietnamese are composed of at least two syllables ( Thang et al. , 2008 ) .
Unlike English , in addition to marking word boundaries , white space is also used to separate syllables that constitute words in Vietnamese written texts .
For example , an 8- syllable written text " C? bao nhi ? u qu?c gia ? ch?u ?u "
( How many countries in Europe ) forms 5 words " C? bao_nhi?u
How many qu?c_gia country ? in ch?u_?u Europe " .
Thus it is interesting to study the influence of word segmentation in Vietnamese on its SQL parsing , i.e. syllable level vs. word level .
In terms of Vietnamese semantic parsing , previous approaches construct rule templates to convert single database - driven questions into meaning representations Nguyen et al. , 2009
Nguyen et al. , , 2012
Tung et al. , 2015 ; Nguyen et al. , 2017 ) .
Recently , Vuong et al. ( 2019 ) formulate the Textto - SQL semantic parsing task for Vietnamese as a sequence labeling - based slot filling problem , and then solve it by using a conventional CRF model with handcrafted features , due to the simple structure of the input questions they deal with .
Note that seq2seq - based semantic parsers have not yet been explored in any previous work w.r.t. Vietnamese .
Semantic parsing datasets for Vietnamese include a corpus of 5460 sentences for assigning semantic roles ( Phuong et al. , 2017 ) and a small Textto - SQL dataset of 1258 simple structured questions over 3 databases ( Vuong et al. , 2019 ) .
However , these two datasets are not publicly available for research community .
In this paper , we introduce the first public largescale Text-to - SQL dataset for the Vietnamese semantic parsing task .
In particular , we create this dataset by manually translating the Spider dataset into Vietnamese .
We empirically evaluate strong seq2seq baseline parsers EditSQL and IRNet ( Guo et al. , 2019 ) on our dataset .
Extending the baselines , we extensively investigate key configurations and find that : ( 1 ) Our human- translated dataset is far more reliable than a dataset consisting of machine - translated questions , and the overall result obtained for Vietnamese is comparable to that for English .
( 2 ) Automatic Vietnamese word segmentation improves the performances of the baselines .
( 3 ) The NPMI score ( Bouma , 2009 ) is useful for linking a cell value mentioned in a question to a column in the database schema .
( 4 ) Latent syntactic features , which are dumped from a neural dependency parser pre-trained for Vietnamese ( Nguyen and Verspoor , 2018 ) , also help improve the performances .
( 5 ) Highest improvements are accounted for the use of pre-trained language models , where PhoBERT ( Nguyen and Nguyen , 2020 ) helps produce higher results than XLM -R ( Conneau et al. , 2020 ) .
We hope that our dataset can serve as a starting point for future Vietnamese semantic parsing research and applications .
We publicly release our dataset at : https://github.com/ VinAIResearch / ViText2SQL .
Our Dataset
We manually translate all English questions and the database schema ( i.e. Table 1 : Statistics of our human-translated dataset . " # Qu. " , " # SQL " and " # DB " denote the numbers of questions , SQL queries and databases , respectively .
" # T/ D " abbreviates the average number of tables per database .
" # Easy " , " # Med . " , " # Hard " and " # ExH " denote the numbers of questions categorized by their SQL queries ' hardness levels of " easy " , " medium " , " hard " and " extra hard " , respectively ( as defined by Yu et al . ) .
7.0 + ) .
Every question and SQL query pair from the same database is first translated by one student and then cross-checked and corrected by the second student ; and finally the NLP researcher verifies the original and corrected versions and makes further revisions if needed .
Note that in case we have literal translation for a question , we stick to the style of the original English question as much as possible .
Otherwise , for complex questions , we will rephrase them based on the semantic meaning of the corresponding SQL queries to obtain the most natural language questions in Vietnamese .
Following Yu et al. ( 2018 ) and Min et al . ( 2019 ) , we split our dataset into training , development and test sets such that no database overlaps between them , as detailed in Table 1 . Examples of question and SQL query pairs from our dataset are presented in Table 2 .
Note that translated question and SQL query pairs in our dataset are written at the syllable level .
To obtain a word-level version of the dataset , we apply RDRSegmenter from VnCoreNLP to perform automatic Vietnamese word segmentation .
3 Baseline Models and Extensions
Original ( Easy question - involving one table in one database
Baselines Recent state - of- the - art results on the Spider dataset are reported for RYANSQL ( Choi et al. , 2020 ) and RAT -SQL ( Wang et al. , 2020 ) , which are based on the seq2seq encoder-decoder architectures .
However , their implementations are not published at the time of our empirical investigation .
1
Thus we select seq2seq based models EditSQL and IRNet ( Guo et al. , 2019 ) with publicly available implementations as our baselines , which produce near state - of - the - art scores on Spider .
We briefly describe the baselines EditSQL and IRNet as follows : ? ?
IRNet first performs an n-gram matching - based schema linking to identify the columns and the tables mentioned in a question .
Then it takes the question , a database schema and the schema linking results as input to synthesize a tree-structured SemQL query - an intermediate representation bridging the input question and a target SQL query .
This synthesizing process is performed by using a BiLSTM - based question encoder and an attention - based schema encoder together with a grammar- based LSTM decoder ( Yin and Neubig , 2017 ) .
Finally , IRNet deterministically uses the synthesized SemQL query to infer the SQL query with domain knowledge .
See and Guo et al . ( 2019 ) for more details of EditSQL and IRNet , respectively .
Our Extensions NPMI for schema linking : IRNet essentially relies on the large-scale knowledge graph ConceptNet ( Speer et al. , 2017 ) to link a cell value mentioned in a question to a column in the database schema , based on two ConceptNet categories ' is a type of ' and ' related terms ' .
However , these two Concept - Net categories are not available for Vietnamese .
Thus we propose a novel use of the NPMI collocation score ( Bouma , 2009 ) for the schema linking in IRNet , which ranks the NPMI scores between the cell values and column names to match a cell value to its column .
Latent syntactic features : Previous works have shown that syntactic features help improve semantic parsing ( Monroe and Wang , 2014 ; Jie and Lu , 2018 ) .
Unlike these works that use handcrafted syntactic features extracted from dependency parse trees , and inspired by Zhang et al . ( 2017 ) 's relation extraction work , we investigate whether latent syntactic features , extracted from the BiLSTM - based dependency parser jPTDP ( Nguyen and Verspoor , 2018 ) pre-trained for Vietnamese , would help improve Vietnamese Text-to - SQL parsing .
In particular , our approach is that we dump latent feature representations from jPTDP 's BiLSTM encoder given our word- level inputs , and directly use them as part of input embeddings of EditSQL and IRNet .
Pre-trained language models :
Zhang et al. ( 2019 ) and Guo et al . ( 2019 ) make use of BERT ( Devlin et al. , 2019 ) to improve their model performances .
Thus we also extend EditSQL and IRNet with the use of pre-trained language models XLM - R- base ( Conneau et al. , 2020 ) and PhoBERT - base ( Nguyen and Nguyen , 2020 ) for the syllable - and word - level settings , respectively .
XLM -R is the recent best multi-lingual model , based on RoBERTa , pre-trained on a 2.5TB multilingual corpus which contains 137GB of syllable - level Vietnamese texts .
PhoBERT is a monolingual variant of RoBERTa for Vietnamese , pre-trained on a 20 GB of word- level Vietnamese texts .
Experiments
Experimental Setup
We conduct experiments to study a quantitative comparison between our human-translated dataset and a machine - translated dataset , 2 the influence of Vietnamese word segmentation ( i.e. syllable level and word level ) , and the usefulness of the latent syntactic features , the pre-trained language models and the NPMI - based approach for schema linking .
For both baselines EditSQL and IRNet which require input pre-trained embeddings for syllables [ MT ] denotes accuracy results with the machinetranslated questions .
The subscript " DeP " refers to the use of the latent syntactic features .
Other subscripts denote the use of the pre-trained language models .
" En " denotes our results on the English Spider dataset but under our training / development / test split w.r.t. the total 9691 public available questions .
and words , we pre-train a set of 300 - dimensional syllable embeddings and another set of 300 dimensional word embeddings using the Word2 Vec skip gram model ( Mikolov et al. , 2013 ) on syllableand word-level corpora of 20 GB Vietnamese texts ( Nguyen and Nguyen , 2020 ) .
In addition , we also use these 20 GB syllable - and word- level Vietnamese corpora as our external datasets to compute the NPMI score ( with a window size of 20 ) for schema linking in IRNet .
Our hyperparameters for EditSQL and IRNet are taken from and Guo et al . ( 2019 ) , respectively .
The pre-trained syllable and word embeddings are fixed , while the pre-trained language models XLM -R and PhoBERT are finetuned during training .
Following Yu et al. ( 2018 ) , we use two commonly used metrics for evaluation .
The first one is the exact matching accuracy , which reports the percentage of input questions that have exactly the same SQL output as its gold reference .
The second one is the component matching F 1 , which reports F 1 scores for SELECT , WHERE , ORDER BY , GROUP BY and all other keywords .
We run for 10 training epochs and evaluate the exact matching accuracy after each epoch on the development set , and then select the best model checkpoint to report the final result on the test set .
Main Results
Table 3 shows the overall exact matching results of EditSQL and IRNet on the development and test sets .
Clearly , IRNet does better than EditSQL , which is consistent with results obtained on the original English Spider dataset .
We find that our human-translated dataset is far more reliable than a dataset consisting of machinetranslated questions .
In particular , at the word level , compared to the machine - translated dataset , our dataset obtains about 30.2- 17.4 ? 13 % and 43.6- 21.6 = 22 % absolute improvements in accuracies of EditSQL and IRNet , respectively ( i.e. 75 % - 100 % relative improvements ) .
In addition , the word- based Text- to - SQL parsing obtains about 5 + % absolute higher accuracies than the syllable - based Text-to - SQL parsing ( EditSQL : 24.1%?30.2 % ; IRNet : 38.2%?43.6 % ) , i.e. automatic Vietnamese word segmentation improves the accuracy results .
Furthermore , latent syntactic features dumped from the pre-trained dependency parser jPTDP for Vietnamese help improve the performances of the baselines ( EditSQL : 30.2%?42.2 % ; IRNet : 43.6%?47.1 % ) .
Also , biggest improvements are accounted for the use of pre-trained language models .
In particular , PhoBERT helps produce higher results than XLM -R ( EditSQL : 52.6 % vs. 51.3 % ; IRNet : 53.2 % vs. 52.8 % ) .
We also retrain EditSQL and IRNet on the English Spider dataset with the use of the strong pretrained language model RoBERTa instead of BERT , but under our dataset split .
We find that the overall results for Vietnamese are smaller but comparable to the English results .
Therefore , Text-to - SQL semantic parsing for Vietnamese might not be significantly more challenging than that for English .
Table 4 shows the exact matching accuracies of EditSQL and IRNet w.r.t. different hardness levels of SQL queries and the F 1 scores w.r.t. different SQL components on the test set .
Clearly , in most cases , the pre-trained language models PhoBERT and XLM -R help produce substantially higher results than the latent syntactic features , especially for the WHERE component .
NPMI - based schema linking :
We also investigate the contribution of our NPMI - based extension approach for schema linking in applying IRNet for Vietnamese .
Without using NPMI for schema linking , 3 we observe 6 + % absolute decrease in the exact matching accuracies of IRNet on both development and test sets , thus showing the usefulness of our NPMI - based approach for schema linking .
Error Analysis
To understand the source of errors , we perform an error analysis on the development set which consists of 954 questions .
Using IRNet PhoBERT which produces the best result , we identify several causes of errors from 382/954 failed examples .
For 121/382 cases ( 32 % ) , IRNet PhoBERT makes incorrect predictions on the column names which are not mentioned or only partially mentioned in the questions .
For example , given the question " Hi?n th? t?n v? n?m ph?t h?nh c?a nh?ng b?i h?t thu ? c v? ca s? tr? tu? i nh ? t " ( Show the name and the release year of the song by the youngest singer ) , 4 the model produces an incorrect column name prediction of " t?n " ( name ) instead of the correct one " t?n b?i h?t " ( song name ) .
Errors related to column name predictions can either be missing the entire column names or inserting random column names into the WHERE component of the predicted SQL queries .
About 12 % of failed examples ( 47/382 ) in fact have an equivalent implementation of their intent with a different SQL syntax .
For example , the model produces a ' failed ' SQL output " SELECT MAX [ s? c ch?a ] FROM [ s?n v?n ?ng ] " which is equivalent to the gold SQL query of " SELECT [ s? c ch?a ] FROM [ s?n v?n ?ng ] ORDER BY [ s? c ch?a ]
DESC LIMIT 1 " , i.e. the SQL output would be valid if we measure an execution accuracy .
About 22 % of failed examples ( 84/382 ) are caused by nested and complex SQL queries which mostly belong to the Extra Hard category .
With 18 % of failed examples ( 70/382 ) , incorrectly predicting operators is another common type of errors .
For example , given the phrases " gi? nh?t " ( oldest ) and " tr? nh?t " ( youngest ) in the question , the model fails to predict the correct operators max and min , respectively .
The remaining 60/382 cases ( 16 % ) are accounted for an incorrect prediction of table names in a FROM clause .
Conclusion
In this paper , we have presented the first public large-scale dataset for Vietnamese Text-to - SQL semantic parsing .
We also extensively experiment with key research configurations using two strong baseline models on our dataset and find that : the input representations , the NPMI - based approach for schema linking , the latent syntactic features and the pre-trained language models all have the influence on this Vietnamese -specific task .
We hope that our dataset can serve as the starting point for further research and applications in Vietnamese question answering and dialogue systems . ) :
What is the number of cars with more than 4 cylinders ?
SELECT count ( * )
FROM CARS_DATA WHERE Cylinders > 4 Translated : Cho bi?t s? l?ng nh?ng chi?c xe c? nhi? u h?n 4 xi lanh .
SELECT count ( * ) FROM [ d? li?u xe ] WHERE [ s? l?ng xi lanh ] > 4 Original ( Hard question - with a nested SQL query ) :
Which countries in europe have at least 3 car manufacturers ?
SELECT T1 .
CountryName FROM COUNTRIES AS T1 JOIN CONTINENTS AS T2 ON T1 .
Continent = T2 .
ContId JOIN CAR_MAKERS AS T3 ON T1 .
CountryId = T3 .
Country WHERE T2 . Continent = " europe " GROUP BY T1 .
CountryName HAVING count ( * ) >=
3 Translated : Nh?ng qu?c gia n?o ? ch?u ?u c? ?t nh?t 3 nh ? s?n xu?t xe h?i? SELECT T1 . [ t? n qu?c gia ] FROM [ qu? c gia ] AS T1 JOIN [ l ? c ?a] AS T2 ON T1 . [ l?c ?a ] = T2 . [ id l?c ?a ] JOIN [ nh ? s?n xu?t xe h?i ] AS T3 ON T1 . [ id qu?c gia ] = T3 . [ qu?c gia ] WHERE T2 . [ l?c ?a] = " ch? u ?u " GROUP BY T1 . [ t? n qu?c gia ] HAVING count ( * ) >= 3
