title
AMR Parsing as Graph Prediction with Latent Alignment
abstract
meaning representations ( AMRs ) are broad-coverage sentence - level semantic representations .
AMRs represent sentences as rooted labeled directed acyclic graphs .
AMR parsing is challenging partly due to the lack of annotated alignments between nodes in the graphs and words in the corresponding sentences .
We introduce a neural parser which treats alignments as latent variables within a joint probabilistic model of concepts , relations and alignments .
As exact inference requires marginalizing over alignments and is infeasible , we use the variational autoencoding framework and a continuous relaxation of the discrete alignments .
We show that joint modeling is preferable to using a pipeline of align and parse .
The parser achieves the best reported results on the standard benchmark ( 74.4 % on LDC2016E25 ) .
Introduction Abstract meaning representations ( AMRs ) ( Banarescu et al. , 2013 ) are broad-coverage sentencelevel semantic representations .
AMR encodes , among others , information about semantic relations , named entities , co-reference , negation and modality .
The semantic representations can be regarded as rooted labeled directed acyclic graphs ( see Figure 1 ) .
As AMR abstracts away from details of surface realization , it is potentially beneficial in many semantic related NLP tasks , including text summarization ( Liu et al. , 2015 ; Dohare and Karnick , 2017 ) , machine translation ( Jones et al. , 2012 ) and question answering ( Mitra and Baral , 2016 ) .
AMR parsing has recently received a lot of attention ( e.g. , ( Flanigan et al. , 2014 ; Artzi et al. , 2015 ; Konstas et al. , 2017 ) ) .
One distinctive aspect of AMR annotation is the lack of explicit alignments between nodes in the graph ( concepts ) and words in the sentences .
Though this arguably simplified the annotation process ( Banarescu et al. , 2013 ) , it is not straightforward to produce an effective parser without relying on an alignment .
Most AMR parsers ( Damonte et al. , 2017 ; Flanigan et al. , 2016 ; Werling et al. , 2015 ; Foland and Martin , 2017 ) use a pipeline where the aligner training stage precedes training a parser .
The aligners are not directly informed by the AMR parsing objective and may produce alignments suboptimal for this task .
The boys must not go In this work , we demonstrate that the alignments can be treated as latent variables in a joint probabilistic model and induced in such a way as to be beneficial for AMR parsing .
Intuitively , in our probabilistic model , every node in a graph is assumed to be aligned to a word in a sentence : each concept is predicted based on the corresponding RNN state .
Similarly , graph edges ( i.e. relations ) are predicted based on representations of concepts and aligned words ( see Figure 2 ) .
As alignments are latent , exact inference requires marginalizing over latent alignments , which is in-feasible .
Instead we use variational inference , specifically the variational autoencoding framework of Kingma and Welling ( 2014 ) .
Using discrete latent variables in deep learning has proven to be challenging ( Mnih and Gregor , 2014 ; Bornschein and Bengio , 2015 ) .
We use a continuous relaxation of the alignment problem , relying on the recently introduced Gumbel - Sinkhorn construction ( Mena et al. , 2018 ) .
This yields a computationally - efficient approximate method for estimating our joint probabilistic model of concepts , relations and alignments .
We assume injective alignments from concepts to words : every node in the graph is aligned to a single word in the sentence and every word is aligned to at most one node in the graph .
This is necessary for two reasons .
First , it lets us treat concept identification as sequence tagging at test time .
For every word we would simply predict the corresponding concept or predict NULL to signify that no concept should be generated at this position .
Secondly , Gumbel - Sinkhorn can only work under this assumption .
This constraint , though often appropriate , is problematic for certain AMR constructions ( e.g. , named entities ) .
In order to deal with these cases , we re-categorized AMR concepts .
Similar recategorization strategies have been used in previous work ( Foland and Martin , 2017 ; Peng et al. , 2017 ) .
The resulting parser achieves 74.4 % Smatch score on the standard test set when using LDC2016E25 training set , 1 an improvement of 3.4 % over the previous best result ( van Noord and Bos , 2017 ) .
We also demonstrate that inducing alignments within the joint model is indeed beneficial .
When , instead of inducing alignments , we follow the standard approach and produce them on preprocessing , the performance drops by 0.9 % Smatch .
Our main contributions can be summarized as follows : ? we introduce a joint probabilistic model for alignment , concept and relation identification ; ? we demonstrate that a continuous relaxation can be used to effectively estimate the model ; ? the model achieves the best reported results .
2 1 The standard deviation across multiple training runs was 0.16 % .
2
The code can be accessed from https://github.
com/ChunchuanLv/AMR_AS_GRAPH_PREDICTION
Probabilistic Model
In this section we describe our probabilistic model and the estimation technique .
In section 3 , we describe preprocessing and post-processing ( including concept re-categorization , sense disambiguation , wikification and root selection ) .
Notation and setting We will use the following notation throughout the paper .
We refer to words in the sentences as w = ( w 1 , . . . , w n ) , where n is sentence length , w k ?
V for k ? { 1 . . . , n} .
The concepts ( i.e. labeled nodes ) are c = ( c 1 , . . . , c m ) , where m is the number of concepts and c i ?
C for i ?
{ 1 . . . , m} .
For example , in Figure 1 , c = ( obligate , go , boy , - ) .
3 Note that senses are predicted at post-processing , as discussed in Section 3.2 ( i.e. go is labeled as go - 02 ) .
A relation between ' predicate concept ' i and ' argument concept ' j is denoted by r ij ?
R ; it is set to NULL if j is not an argument of i .
In our example , r 2,3 = ARG0 and r 1,3 = NULL .
We will use R to denote all relations in the graph .
To represent alignments , we will use a = { a 1 , . . . , a m } , where a i ? { 1 , . . . , n} returns the index of a word aligned to concept i .
In our example , a 1 = 3 .
All three model components rely on bidirectional LSTM encoders ( Schuster and Paliwal , 1997 ) .
We denote states of BiLSTM ( i.e. concatenation of forward and backward LSTM states ) as h k ?
R d ( k ? { 1 , . . . , n} ) .
The sentence encoder takes pre-trained fixed word embeddings , randomly initialized lemma embeddings , part-ofspeech and named -entity tag embeddings .
Method overview
We believe that using discrete alignments , rather than attention - based models ( Bahdanau et al. , 2015 ) is crucial for AMR parsing .
AMR banks are a lot smaller than parallel corpora used in machine translation ( MT ) and hence it is important to inject a useful inductive bias .
We constrain our alignments from concepts to words to be injective .
First , it encodes the observation that concepts are mostly triggered by single words ( especially , after re-categorization , Section 3.1 ) .
Second , it implies that each word corresponds to at most one concept ( if any ) .
This encourages competition : alignments are mutually - repulsive .
In our example , obligate is not lexically similar to the word must and may be hard to align .
However , given that other concepts are easy to predict , alignment candidates other than must and the will be immediately ruled out .
We believe that these are the key reasons for why attention - based neural models do not achieve competitive results on AMR ( Konstas et al. , 2017 ) and why state - of - the - art models rely on aligners .
Our goal is to combine best of two worlds : to use alignments ( as in state - of - the- art AMR methods ) and to induce them while optimizing for the end goal ( similarly to the attention component of encoder-decoder models ) .
Our model consists of three parts : ( 1 ) the concept identification model P ? ( c|a , w ) ; ( 2 ) the relation identification model P ? ( R|a , w , c ) and ( 3 ) the alignment model Q ? ( a|c , R , w ) .
4 Formally , ( 1 ) and ( 2 ) together with the uniform prior over alignments P ( a ) form the generative model of AMR graphs .
In contrast , the alignment model Q ? ( a|c , R , w ) , as will be explained below , is approximating the intractable posterior P ? , ? ( a|c , R , w ) within that probabilistic model .
In other words , we assume the following model for generating the AMR graph : P ? , ? ( c , R|w ) = a P ( a ) P ? ( c|a , w ) P ?
( R|a , w , c ) = a P ( a ) m i=1 P ( c i |h a i ) m i , j=1 P ( r ij |h a i , c i , h a j , c j ) 4 ? , ? and ? denote all parameters of the models .
AMR concepts are assumed to be generated conditional independently relying on the BiLSTM states and surface forms of the aligned words .
Similarly , relations are predicted based only on AMR concept embeddings and LSTM states corresponding to words aligned to the involved concepts .
Their combined representations are fed into a bi-affine classifier ( Dozat and Manning , 2017 ) ( see Fig- ure 2 ) .
The expression involves intractable marginalization over all valid alignments .
As standard in variational autoencoders , VAEs ( Kingma and Welling , 2014 ) , we lower - bound the loglikelihood as log P ? , ? ( c , R|w ) ? E Q [ log P ? ( c|a , w ) P ? ( R|a , w , c ) ] ? D KL ( Q ? ( a|c , R , w ) | | P ( a ) ) , ( 1 ) where Q ? ( a|c , R , w ) is the variational posterior ( aka the inference network ) , E Q [. . .] refers to the expectation under Q ? ( a|c , R , w ) and D KL is the Kullback - Liebler divergence .
In VAEs , the lower bound is maximized both with respect to model parameters ( ? and ? in our case ) and the parameters of the inference network ( ? ) .
Unfortunately , gradient - based optimization with discrete latent variables is challenging .
We use a continuous relaxation of our optimization problem , where realvalued vectors ? i ?
R n ( for every concept i ) approximate discrete alignment variables a i .
This relaxation results in low-variance estimates of the gradient using the parameterization trick ( Kingma and Welling , 2014 ) , and ensures fast and stable training .
We will describe the model components and the relaxed inference procedure in detail in sections 2.6 and 2.7 .
Though the estimation procedure requires the use of the relaxation , the learned parser is straightforward to use .
Given our assumptions about the alignments , we can independently choose for each word w k ( k = 1 , . . . , m ) the most probably concept according to P ? ( c|h k ) .
If the highest scoring option is NULL , no concept is introduced .
The relations could then be predicted relying on P ? ( R|a , w , c ) .
This would have led to generating inconsistent AMR graphs , so instead we search for the highest scoring valid graph ( see Section 3.2 ) .
Note that the alignment model Q ? is not used at test time and only necessary to train accurate concept and relation identification models .
Concept identification model
The concept identification model chooses a concept c ( i.e. a labeled node ) conditioned on the aligned word k or decides that no concept should be introduced ( i.e. returns NULL ) .
Though it can be modeled with a softmax classifier , it would not be effective in handling rare or unseen words .
First , we split the decision into estimating the probability of concept category ? ( c ) ? T ( e.g. ' number ' , ' frame ' ) and estimating the probability of the specific concept within the chosen category .
Second , based on a lemmatizer and training data 5 we prepare one candidate concept e k for each word k in vocabulary ( e.g. , it would propose want if the word is wants ) .
Similar to Luong et al. ( 2015 ) , our model can then either copy the candidate e k or rely on the softmax over potential concepts of category ? .
Formally , the concept prediction model is defined as P ? ( c|h k , w k ) = P ( ? ( c ) | h k , w k ) ? [ [ e k = c ] ] ? exp( v T copy h k ) + exp( v T c h k ) Z( h k , ? ) , where the first multiplicative term is a softmax classifier over categories ( including NULL ) ; v copy , v c ?
R d ( for c ? C ) are model parameters ; [ [ . . .] ] denotes the indicator function and equals 1 if its argument is true and 0 , otherwise ; Z( h , ? ) is the partition function ensuring that the scores sum to 1 .
Relation identification model
We use the following arc-factored relation identification model : P ? ( R|a , w , c ) = m i , j=1 P ( r ij |h a i , c i , h a j , c j ) ( 2 ) Each term is modeled in exactly the same way : 1 . for both endpoints , embedding of the concept c is concatenated with the RNN state h ; 2 . they are linearly projected to a lower dimension separately through M h ( h a i ? c i ) ?
R d f and M d ( h a j ? c j ) ?
R d f , where ? denotes concatenation ; 3 . a log-linear model with bilinear scores M h ( h a i ? c i ) T C r M d ( h a j ? c j ) , C r ?
R d f ?d f is used to compute the probabilities .
5 See supplementary materials .
In the above discussion , we assumed that BiL-STM encodes a sentence once and the BiLSTM states are then used to predict concepts and relations .
In semantic role labeling , the task closely related to the relation identification stage of AMR parsing , a slight modification of this approach was shown more effective ( Zhou and Xu , 2015 ; .
In that previous work , the sentence was encoded by a BiLSTM once per each predicate ( i.e. verb ) and the encoding was in turn used to identify arguments of that predicate .
The only difference across the re-encoding passes was a binary flag used as input to the BiL -STM encoder at each word position .
The flag was set to 1 for the word corresponding to the predicate and to 0 for all other words .
In that way , BiLSTM was encoding the sentence specifically for predicting arguments of a given predicate .
Inspired by this approach , when predicting label r ij for j ? { 1 , . . . m} , we input binary flags p 1 , . . . p n to the BiLSTM encoder which are set to 1 for the word indexed by a i ( p a i = 1 ) and to 0 for other words ( p j = 0 , for j = a i ) .
This also means that BiLSTM encoders for predicting relations and concepts end up being distinct .
We use this multi-pass approach in our experiments .
6
Alignment model Recall that the alignment model is only used at training , and hence it can rely both on input ( states h 1 , . . . , h n ) and on the list of concepts c 1 , . . . , c m .
Formally , we add ( m?n ) NULL concepts to the list .
7 Aligning a word to any NULL , would correspond to saying that the word is not aligned to any ' real ' concept .
Note that each one- to- one alignment ( i.e. permutation ) between n such concepts and n words implies a valid injective alignment of n words to m ' real ' concepts .
This reduction to permutations will come handy when we turn to the Gumbel - Sinkhorn relaxation in the next section .
Given this reduction , from now on , we will assume that m = n.
As with sentences , we use a BiLSTM model to encode concepts c , where g i ?
R dg , i ? { 1 , . . . , n} .
We use a globally - normalized align-ment model : Q ? ( a|c , R , w ) = exp ( n i=1 ?( g i , h a i ) ) Z ? ( c , w ) , where Z ? ( c , w ) is the intractable partition function and the terms ?( g i , h a i ) score each alignment link according to a bilinear form ?( g i , h a i ) = g T i Bh a i , ( 3 ) where B ?
R dg?d is a parameter matrix .
Estimating model with Gumbel-Sinkhorn Recall that our learning objective ( 1 ) involves expectation under the alignment model .
The partition function of the alignment model Z ? ( c , w ) is intractable , and it is tricky even to draw samples from the distribution .
Luckily , the recently proposed relaxation ( Mena et al. , 2018 ) lets us circumvent this issue .
First , note that exact samples from a categorical distribution can be obtained using the perturb- and - max technique ( Papandreou and Yuille , 2011 ) .
For our alignment model , it would correspond to adding independent noise to the score for every possible alignment and choosing the highest scoring one : a = argmax a?P n i=1 ?( g i , h a i ) + a , ( 4 ) where P is the set of all permutations of n elements , a is a noise drawn independently for each a from the fixed Gumbel distribution ( G ( 0 , 1 ) ) .
Unfortunately , this is also intractable , as there are n! permutations .
Instead , in perturband - max an approximate schema is used where noise is assumed factorizable .
In other words , first noisy scores are computed as ?( g i , h a i ) = ?( g i , h a i ) + i , a i , where i , a i ? G ( 0 , 1 ) and an approximate sample is obtained by a = argmax a n i=1 ?( g i , h a i ) , Such sampling procedure is still intractable in our case and also non-differentiable .
The main contribution of Mena et al . ( 2018 ) is approximating this argmax with a simple differentiable computation ? = S t ( ? , ? ) which yields an approximate ( i.e. relaxed ) permutation .
We use ? and ? to denote the n ?
n matrices of alignment scores ?( g i , h k ) and noise variables ik , respectively .
Instead of returning index a i for every concept i , it would return a ( peaky ) distribution over words ? i .
The peakiness is controlled by the temperature parameter t of Gumbel - Sinkhorn which balances smoothness ( ' differentiability ' ) vs. bias of the estimator .
For further details and the derivation , we refer the reader to the original paper ( Mena et al. , 2018 ) .
Note that ? is a function of the alignment model Q ? , so we will write ? ? in what follows .
The variational bound ( 1 ) can now be approximated as E ?G ( 0 , 1 ) [ log P ? ( c| S t ( ? ? , ? ) , w ) + log P ? ( R| S t ( ? ? , ? ) , w , c ) ] ? D KL ( ? ? + ? t || ? t 0 ) ( 5 ) Following Mena et al. ( 2018 ) , the original KL term from equation ( 1 ) is approximated by the KL term between two n ? n matrices of i.i.d .
Gumbel distributions with different temperature and mean .
The parameter t 0 is the ' prior temperature ' .
Using the Gumbel - Sinkhorn construction unfortunately does not guarantee that i ? ij = 1 .
To encourage this equality to hold , and equivalently to discourage overlapping alignments , we add another regularizer to the objective ( 5 ) : ?(? , ?) = ? j max ( i ( ? ij ) ? 1 , 0 ) . ( 6 ) Our final objective is fully differentiable with respect to all parameters ( i.e. ? , ? and ? ) and has low variance as sampling is performed from the fixed non-parameterized distribution , as in standard VAEs .
Relaxing concept and relation identification
One remaining question is how to use the soft input ? = S t ( ? ? , ? ) in the concept and relation identification models in equation ( 5 ) .
In other words , we need to define how we compute P ? ( c| S t ( ? ? , ? ) , w ) and P ? ( R| S t ( ? ? , ? ) , w , c ) .
The standard technique would be to pass to the models expectations under the relaxed variables n k=1 ? ik h k , instead of the vectors h a i ( Maddison et al. , 2017 ; Jang et al. , 2017 ) .
This is what we do for the relation identification model .
We use this approach also to relax the one- hot encoding of the predicate position ( p , see Section 2.4 ) .
However , the concept prediction model log P ? ( c| S t ( ? ? , ? ) , w ) relies on the pointing mechanism , i.e. directly exploits the words w rather than relies only on biLSTM states h k .
So instead we treat ? i as a prior in a hierarchical model : logP ? ( c i |? i , w ) ? log n k=1 ?ik P ? ( c i | a i = k , w ) ( 7 ) As we will show in our experiments , a softer version of the loss is even more effective : logP ? ( c i |?
i , w ) ? log n k=1 ( ? ik P ? ( c i | a i = k , w ) ) ? , ( 8 ) where we set the parameter ? = 0.5 .
We believe that using this loss encourages the model to more actively explore the alignment space .
Geometrically , the loss surface shaped as a ball in the 0.5 norm space would push the model away from the corners , thus encouraging exploration .
3 Pre-and post-pocessing 2017 ) ) ; this transformation is reversed at the post-processing stage .
Our approach is very similar to the Factored Concept Label system of , with one important difference that we unpack our concepts before the relation identification stage , so the relations are predicted between original concepts ( all nodes in each group share the same alignment distributions to the RNN states ) .
Intuitively , the goal is to ensure that concepts rarely lexically triggered ( e.g. , thing in Figure 3 ) get grouped together with lexically triggered nodes .
Such ' primary ' concepts get encoded in the category of the concept ( the set of categories is ? , see also section 2.3 ) .
In Figure 3 , the re-categorized concept thing ( opinion ) is produced from thing and opine - 01 .
We use concept as the dummy category type .
There are 8 templates in our system which extract re-categorizations for fixed phrases ( e.g. thing ( opinion ) ) , and a deterministic system for grouping lexically flexible , but structurally stable sub-graphs ( e.g. , named entities , have - rel-role - 91 and have - org- role - 91 concepts ) .
Details of the re-categorization procedure and other pre-processing are provided in appendix .
Post-processing
For post-processing , we handle sensedisambiguation , wikification and ensure legitimacy of the produced AMR graph .
For sense disambiguation we pick the most frequent sense for that particular concept ( ' - 01 ' , if unseen ) .
For wikification we again look - up in the training set and default to " - " .
There is certainly room for improvement in both stages .
Our probability model predicts edges conditional independently and thus cannot guarantee the connectivity of AMR graph , also there are additional constraints which are useful to impose .
We enforce three constraints : ( 1 ) specific concepts can have only one neighbor ( e.g. , ' number ' and ' string ' ; see appendix for details ) ; ( 2 ) each predicate concept can have at most one argument for each relation r ? R ; ( 3 ) the graph should be connected .
Constraint ( 1 ) is addressed by keeping only the highest scoring neighbor .
In order to satisfy the last two constraints we use a simple greedy procedure .
First , for each edge , we pick - up the highest scoring relation and edge ( possibly NULL ) .
If the constraint ( 2 ) is violated , we simply keep the highest scoring edge among the duplicates and drop the rest .
If the graph is not connected ( i.e. constraint ( 3 ) is violated ) , we greedily choose edges linking the connected components until the graph gets connected ( MSCG in Flanigan et al . ( 2014 ) ) .
Finally , we need to select a root node .
Similarly to relation identification , for each candidate concept c i , we concatenate its embedding with the corresponding LSTM state ( h a i ) and use these scores in a softmax classifier over all the concepts .
Model Data Smatch JAMR ( Flanigan et al. , 2016 ) R1 67.0 AMREager ( Damonte et al. , 2017 ) R1 64.0 CAMR ( Wang et al. , 2016 ) R1 66.5 SEQ2SEQ + 20M ( Konstas et al. , 2017 ) R1 62.1 Mul-BiLSTM ( Foland and Martin , 2017
We used the development set to perform model selection and hyperparameter tuning .
The hyperparameters , as well as information about embeddings and pre-processing , are presented in the supplementary materials .
We used Adam ( Kingma and Ba , 2014 ) to optimize the loss ( 5 ) and to train the root classifier .
Our best model is trained fully jointly , and we do early stopping on the development set scores .
Training takes approximately 6 hours on a single GeForce GTX 1080 Ti with Intel Xeon CPU E5-2620 v4 .
Experiments and discussion
We start by comparing our parser to previous work ( see Table 1 ) .
Our model substantially outperforms all the previous models on both datasets .
Specifically , it achieves 74.4 % Smatch score on LDC2016E25 ( R2 ) , which is an improvement of 3.4 % over character seq2seq model relying on silver data ( van Noord and Bos , 2017 ) . For LDC2015E86 ( R1 ) , we obtain 73.7 % Smatch score , which is an improvement of 3.0 % over 8 Annotation in R2 has also been slightly revised .
Models
A the previous best model , multi-BiLSTM parser of Foland and Martin ( 2017 ) .
In order to disentangle individual phenomena , we use the AMR - evaluation tools ( Damonte et al. , 2017 ) and compare to systems which reported these scores ( Table 2 ) .
We obtain the highest scores on most subtasks .
The exception is negation detection .
However , this is not too surprising as many negations are encoded with morphology , and character models , unlike our word - level model , are able to capture predictive morphological features ( e.g. , detect prefixes such as " un - " or " im - " ) .
Now , we turn to ablation tests ( see Table 3 ) .
First , we would like to see if our latent alignment framework is beneficial .
In order to test this , we create a baseline version of our system ( ' prealign ' ) which relies on the JAMR aligner ( Flani - Formally , the true posterior under the conceptonly model in ' 2 stages ' assigns exactly the same probability to both configurations , and the alignment model Q ? will be forced to mimic it ( even though it relies on an LSTM model of the graph ) .
The spurious ambiguity will have a detrimental effect on the relation identification stage .
It is interesting to see the contribution of other modeling decisions we made when modeling and relaxing alignments .
First , instead of using Gumbel - Sinkhorn , which encourages mutuallyrepulsive alignments , we now use a factorized alignment model .
Note that this model ( ' No Sinkhorn ' in Table 5 ) still relies on ( relaxed ) discrete alignments ( using Gumbel softmax ) but does not constrain the alignments to be injective .
A substantial drop in performance indicates that the prior knowledge about the nature of alignments appears beneficial .
Second , we remove the additional regularizer for Gumbel - Sinkhorn approximation ( equation ( 6 ) ) .
The performance drop in Smatch score ( ' No Sinkhorn reg ' ) is only moderate .
Finally , we show that using the simple hierarchical relaxation ( equation ( 7 ) ) rather than our softer version of the loss ( equation ( 8 ) ) results in a substantial drop in performance ( ' No soft loss ' , - 0.7 % Smatch ) .
We hypothesize that the softer relaxation favors exploration of alignments and helps to discover better configurations .
Additional Related Work Alignment performance has been previously identified as a potential bottleneck affecting AMR parsing ( Damonte et al. , 2017 ; Foland and Martin , 2017 ) .
Some recent work has focused on building aligners specifically for training their parsers ( Werling et al. , 2015 ; .
However , those aligners are trained independently of concept and relation identification and only used at pre-processing .
Treating alignment as discrete variables has been successful in some sequence transduction tasks with neural models ( Yu et al. , 2017 ( Yu et al. , , 2016 .
Our work is similar in that we also train discrete alignments jointly but the tasks , the inference framework and the decoders are very different .
The discrete alignment modeling framework has been developed in the context of traditional ( i.e. non-neural ) statistical machine translation ( Brown et al. , 1993 ) .
Such translation models have also been successfully applied to semantic parsing tasks ( e.g. , ( Andreas et al. , 2013 ) ) , where they rivaled specialized semantic parsers from that period .
However , they are considerably less accurate than current state - of - the - art parsers applied to the same datasets ( e.g. , ( Dong and Lapata , 2016 ) ) .
For AMR parsing , another way to avoid using pre-trained aligners is to use seq2seq models ( Konstas et al. , 2017 ; van Noord and Bos , 2017 ) .
In particular , van Noord and Bos ( 2017 ) used character level seq2seq model and achieved the previous state - of - the - art result .
However , their model is very data demanding as they needed to train it on additional 100K sentences parsed by other parsers .
This may be due to two reasons .
First , seq2seq models are often not as strong on smaller datasets .
Second , recurrent decoders may struggle with predicting the linearized AMRs , as many statistical dependencies are highly non-local .
Conclusions
We introduced a neural AMR parser trained by jointly modeling alignments , concepts and relations .
We make such joint modeling computationally feasible by using the variational autoencoding framework and continuous relaxations .
The parser achieves state - of - the - art results and ablation tests show that joint modeling is indeed beneficial .
We believe that the proposed approach may be extended to other parsing tasks where alignments are latent ( e.g. , parsing to logical form ( Liang , 2016 ) ) .
Another promising direction is integrating character seq2seq to substitute the copy function .
This should also improve the handling of negation and rare words .
Though our parsing model does not use any linearization of the graph , we relied on LSTMs and somewhat arbitrary linearization ( depth-first traversal ) to encode the AMR graph in our alignment model .
A better alternative would be to use graph convolutional networks Kipf and Welling , 2017 ) : neighborhoods in the graph are likely to be more informative for predicting alignments than the neighborhoods in the graph traversal .
Figure 1 : 1 Figure 1 : An example of AMR , the dashed lines denote latent alignments , obligate - 01 is the root .
Numbers indicate depth-first traversal order .
Figure 2 : 2 Figure 2 : Relation identification : predicting a relation between boy and go -02 relying on the two concepts and corresponding RNN states .
Figure 3 : 3 Figure 3 : An example of re-categorized AMR .
AMR graph at the top , re-categorized concepts in the middle , and the sentence is at the bottom .
Figure 4 : 4 Figure4 : When modeling concepts alone , the posterior probability of the correct ( green ) and wrong ( red ) alignment links will be the same .
The probabilistic model is invariant to the ordering of concepts , though the order affects the inference algorithm ( see Section 2.5 ) .
We use depth-first traversal of the graph to generate the ordering .
Using the vanilla one- pass model from equation ( 2 ) results in 1.4 % drop in Smatch score .7
After re-categorization ( Section 3.1 ) , m ?
n holds for most cases .
For exceptions , we append NULL to the sentence .
