title
On the Role of Supervision in Unsupervised Constituency Parsing
abstract
We analyze several recent unsupervised constituency parsing models , which are tuned with respect to the parsing F 1 score on the Wall Street Journal ( WSJ ) development set ( 1,700 sentences ) .
We introduce strong baselines for them , by training an existing supervised parsing model ( Kitaev and Klein , 2018 ) on the same labeled examples they access .
When training on the 1,700 examples , or even when using only 50 examples for training and 5 for development , such a few-shot parsing approach can outperform all the unsupervised parsing methods by a significant margin .
Fewshot parsing can be further improved by a simple data augmentation method and selftraining .
This suggests that , in order to arrive at fair conclusions , we should carefully consider the amount of labeled data used for model development .
We propose two protocols for future work on unsupervised parsing : ( i ) use fully unsupervised criteria for hyperparameter tuning and model selection ; ( ii ) use as few labeled examples as possible for model development , and compare to few -shot parsing trained on the same labeled examples .
1
Introduction Recent work has considered neural unsupervised constituency parsing ( Shen et al. , 2018a ; Drozdov et al. , 2019 ; Kim et al. , 2019 b , inter alia ) , showing that it can achieve much better performance than trivial baselines .
However , many of these approaches use the gold parse trees of all sentences in a development set for either early stopping ( Shen et al. , 2018a
( Shen et al. , , 2019 Drozdov et al. , 2019 , inter alia ) or hyperparameter tuning ( Kim et al. , 2019a ) .
In contrast , models trained and tuned without any labeled data ( Kim et al. , 2019 b ; Peng et al. , 2019 ) are much less competitive .
1 Project page : https://ttic.uchicago.edu/ ?freda/project/rsucp/
Are the labeled examples important in order to obtain decent unsupervised parsing performance ?
How well can we do if we train on these labeled examples rather than merely using them for tuning ?
In this work , we consider training a supervised constituency parsing model ( Kitaev and Klein , 2018 ) with very few examples as a strong baseline for unsupervised parsing tuned on labeled examples .
We empirically characterize unsupervised and few-shot parsing across the spectrum of labeled data availability , finding that ( i ) tuning based on a few ( as few as 15 ) labeled examples is sufficient to improve unsupervised parsers over fully unsupervised criteria by a significant margin ; ( ii ) unsupervised parsing with supervised tuning does outperform few -shot parsing with fewer than 15 labeled examples , but few -shot parsing quickly dominates once there are more than 55 examples ; and ( iii ) when few -shot parsing is combined with a simple data augmentation method and self-training ( Steedman et al. , 2003 ; Reichart and Rappoport , 2007 ; McClosky et al. , 2006 , inter alia ) , only 15 examples are needed for few-shot parsing to begin to dominate .
Based on these results , we propose the following two protocols for future work on unsupervised parsing :
1 . Derive and use fully unsupervised criteria for hyperparameter tuning and model selection .
2 . Use as few labeled examples as possible for model development and tuning , and compare to few -shot parsing models trained on the used examples as a strong baseline .
We suggest future work to tune and compare models under each protocol separately .
In addition , we present two side findings on unsupervised parsing : ( i ) the vocabulary size in unsupervised parsing , which has not been widely considered as a hyperparameter and varies across prior work , greatly affects the performance of all unsupervised parsing models tested ; and ( ii ) selftraining can help improve all investigated unsupervised parsing ( Shen et al. , 2018a
( Shen et al. , , 2019 Drozdov et al. , 2019 ; Kim et al. , 2019a ) and few-shot parsing models , and thus can be considered as a post-processing step in future work .
Related Work Unsupervised parsing .
During the past two decades , there has been a lot of work on both unsupervised constituency parsing ( Klein and Manning , 2002 Manning , , 2004 Bod , 2006 a , b ; Seginer , 2007 ; Snyder et al. , 2009 , inter alia ) and unsupervised dependency parsing ( Klein and Manning , 2004 ; Smith and Eisner , 2006 ; Spitkovsky et al. , 2011 Spitkovsky et al. , , 2013 .
Recent work has proposed several effective models for unsupervised or distantly supervised constituency parsing , optimizing either a language modeling objective ( Shen et al. , 2018a
( Shen et al. , , 2019 Kim et al. , 2019 b , a , inter alia ) or other downstream semantic objectives ( Li et al. , 2019 ; Shi et al. , 2019 ) .
Some of them are tuned with labeled examples in the WSJ development set ( Shen et al. , 2018a
( Shen et al. , , 2019
Htut et al. , 2018 ; Drozdov et al. , 2019 ; Kim et al. , 2019a ; Wang et al. , 2019 ) or other labeled examples ( Jin et al. , 2018 ( Jin et al. , , 2019 . Data augmentation .
Data augmentation is a strategy for automatically increasing the amount and variety of data for training models , without actually collecting any new data .
Such methods have been found helpful on many NLP tasks , including text classification ( Kobayashi , 2018 ; Samanta et al. , 2019 ) , relation classification ( Xu et al. , 2016 ) , and part- of-speech tagging ( S ? ahin and Steedman , 2018 ) .
Part of our approach also falls into the category of data augmentation , applied specifically to constituency parsing from very few examples .
Few - shot parsing .
Sagae et al. ( 2008 ) show that a supervised dependency parsing model trained on 100 examples can work surprisingly well .
Recent work has demonstrated the potential of few-shot dependency parsing on multiple languages ( Aufrant et al. , 2018 ; Meechan -Maddon and Nivre , 2019 ; Vania et al. , 2019 , inter alia ) .
Our approach ( ?3 ) can be viewed as few-shot constituency parsing .
Few - Shot Constituency Parsing
We apply Benepar ( ?3.1 ; Kitaev and Klein , 2018 ) as the base model for few-shot parsing .
We present a simple data augmentation method ( ?3.2 ) and an iterative self-training strategy ( ?3.3 ) to further improve the performance .
We suggest that such an approach should serve as a strong baseline for unsupervised parsing with supervised tuning .
Parsing Model
The Benepar parsing model consists of ( i ) word embeddings , ( ii ) transformer - based ( Vaswani et al. , 2017 ) word -span embeddings , and ( iii ) a multilayer perceptron to compute a score for each labeled span .
2
The score of an arbitrary tree is defined as the sum of all of its internal span scores .
Given a sentence and its ground - truth parse tree T * , the model is trained to satisfy score ( T * ) ? score ( T ) + ?( T * , T ) for any tree T ( T = T * ) , where ? denotes the Hamming loss on labeled spans .
The label - aware CKY algorithm is used to obtain the tree with the highest score .
More details can be found in Kitaev and Klein ( 2018 ) .
Data Augmentation
We introduce a data augmentation method , subtree substitution ( SUB ; Figure 1 ) , to automatically improve the diversity of data in the few-shot setting .
We start with a set of sentences with N unlabeled parse trees S = { s i , T i } N i=1 ; s i = w i1 , w i2 , . . . , w iL i denotes a sentence with L i words , where w ik denotes a word ; T i = { b ij , e ij } C i j=1 denotes the unlabeled parse tree of s i with C i nonterminal nodes ; b ij and e ij denotes the beginning and ending index of a constituent .
The augmented dataset S is initialized to S .
At each step , we draw a sentence s i and its parse tree T i uniformly from S , and draw a constituent b ij , e ij ?
T i uniformly from T i .
After that , we replace b ij , e ij with a random b kh , e kh ?
t k ; that is , we replace a constituent with another one from the training set .
We let s i and T i denote modified sentence and its parse tree , assign S ? S ? {( s i , T i ) } , and repeat the above procedure until S reaches the desired size .
et al. ( 2003 ) , Reichart and Rappoport ( 2007 ) and McClosky et al. ( 2006 ) have shown that
Self- Training
Steedman
Experiments
Dataset and Training Details
We use the WSJ portion of the Penn Treebank corpus ( Marcus et al. , 1993 ) to train and evaluate the models , replace all number tokens with a special token , and split standard train / dev/ test sets following Kim et al . ( 2019 b ) .
3
For each criterion , we tune the hyperparameters of each model with respect to its performance on the development set .
To solve the problem of vocabulary sparsity in the few-shot parsing setting ( ?3 ) , we initialize the word embeddings of Benepar ( Kitaev and Klein , 2018 ) with the word embeddings from an LSTM - based ( Hochreiter and Schmidhuber , 1997 ) language model trained on the WSJ training set .
During training , models are able to access all sentences ( without parse trees ) in the WSJ training set ; for few -shot parsing or unsupervised parsing with supervised tuning , some unlabeled parse trees in the WSJ development set are available as well .
We augment the training set to 10,000 examples for few-shot parsing with SUB , and apply 5 - step self -training when applicable .
We evaluate the unlabeled F 1 score of all models using evalb , 4 discarding punctuation .
More details can be found in the supplementary material .
Models and Tuning Criteria
We investigate four recently proposed models : PRPN ( Shen et al. , 2018a ) , ON -LSTM ( Shen et al. , 2019 ) , DIORA ( Drozdov et al. , 2019 ) , and Compound PCFG ( Kim et al. , 2019a ) . PRPN and ON - LSTM are left-to- right neural language models , where syntactic distance ( Shen et al. , 2018 b ) between consecutive words is computed from the model output and used to infer the constituency parse tree .
DIORA learns text -span representations and span-level scores by optimizing a masked language modeling objective .
The compound PCFG uses a neural parameterization of a PCFG , as well as a per-sentence latent vector which introduces context sensitivity .
Both DIORA and the Compound PCFG use the CKY algorithm to infer the parse tree of a given sentence .
As fully unsupervised tuning criteria , we use perplexity on the development set for PRPN and ON - LSTM , and the upper bound of perplexity for the Compound PCFG , following Shen et al . ( 2018a
Shen et al. ( , 2019 and Kim et al . ( 2019a ) respectively .
For DIORA , we use its reconstruction loss on the development set .
5
Comparison between Unsupervised Parsing and Few - Shot Parsing
We compare unsupervised parsing against few-shot parsing ( Table 1 and Figure
On the other hand , we find that a few labeled examples are consistently helpful for most models to achieve better results than fully unsupervised parsing .
In addition , models tuned on a very small number ( e.g. , 15 ) of labeled examples can achieve similar performance to those tuned on 1,700 labeled examples ; that is , we need far fewer labeled examples than existing unsupervised parsing approaches have used to obtain very similar results .
To test if SUB can also help improve unsupervised parsing models , we generate 10 K sentences from the 1,700 sentences in the WSJ development set with SUB ( Figure 1 ) , and add them to the 40 Ksentence WSJ training set .
We compare unsupervised parsing models trained on the original WSJ training set and the augmented one ( Table 2 ) .
We find that SUB can sometimes help , but not by a large margin , and all numbers in Table 2 are far below the performance of few-shot parsing with the same data availability ( 82.6 ; Table 1 ) .
Few - shot parsing with data augmentation is a strong baseline for unsupervised parsing with data augmentation .
The Importance of Vocabulary Size
We notice that the result of the Compound PCFG in et al . ( 2019a ) .
6
The only major difference between their approach and ours is the vocabulary size : instead of keeping all words , they keep the most frequent 10 K words in the WSJ corpus and replace others with a special token .
To analyze the importance of this choice , we compare the performance of the models with vocabulary size 35 K vs. 10 K ( Figure 2 ) , tuning models separately in the two settings .
We find that the vocabulary size , which has not been widely considered a hyperparameter and varies across prior work , greatly affects the performance of all models tested .
One possible reason is that a large portion ( 79.9 % ) of the low-frequency ( i.e. , outside the 10K vocabulary ) word tokens are nouns or adjectives - some models ( e.g. , PRPN and Compound PCFG ) may benefit from collapsing these tokens to a single form , as it may be a beneficial kind of word clustering .
This suggests that we should consider tuning the vocabulary size as a hyperparameter , or fix the vocabulary size for fair comparison in future work .
Self - Training Improves all Models Inspired by the fact that self-training boosts the performance of few-shot parsing ( Table 1 ) , we apply iterative self-training to the unsupervised parsing models as well , and find that it improves all models ( Table 3 ) .
7
It is worth noting that 5 - step self - training is better than 1 - step self - training for all base models we experimented with .
Our results suggest that iterative ( e.g. , 5 - step ) self - training may be considered as a standard post-hoc processing step for unsupervised parsing .
Discussion
While many state - of- the - art unsupervised parsing models are tuned on all labeled examples in a development set ( Drozdov et al. , 2019 ; Kim et al. , 2019 b ; Wang et al. , 2019 , inter alia ) , we have demonstrated that , given the same data , few -shot parsing with simple data augmentation and self-training can consistently outperform all of these models by a large margin .
We suggest that one possibility for future work is to focus on fully unsupervised criteria , such as language model perplexity ( Shen et al. , 2018a
( Shen et al. , , 2019 Kim et al. , 2019 b ; Peng et al. , 2019 ; Li et al. , 2020 ) and model stability across different random seeds ( Shi et al. , 2019 ) , for model selection , as discussed in unsupervised learning work Eisner , 2005 , 2006 ; Spitkovsky et al. , 2010 a , b , inter alia ) .
An alternative is to use as few labeled examples in the development set as possible , and compare to few -shot parsing trained on the used examples as a strong baseline .
In addition , we find that self-training is a useful post-processing step for unsupervised parsing .
7 A similar idea and similar results have been presented by Kim et al . ( 2019a ) , where they train an RNNG ( Dyer et al. , 2016 ) to fit the prediction of unsupervised parsing models .
Our work does not necessarily imply that unsupervised parsers produce poor parses ; they may be producing good parses that clash with the conventions of treebanks ( Klein , 2005 ) .
If this is the case , then extrinsic evaluation of parsers in downstream tasks ( Shi et al. , 2018 ) , e.g. , machine translation ( DeNero and Uszkoreit , 2011 ; Neubig et al. , 2012 ; Gimpel and Smith , 2014 ) , may better show the potential of unsupervised methods .
# hyperparameter search trials = 3 ? 3 = 9 .
We follow Shen et al. ( 2019 ) to use a 3 - layer ON - LSTM , and used the master gates in the second layer to compute the syntactic distance ( Shen et al. , 2018 b ) . DIORA ( Drozdov et al. , 2019 ) . 13
Hyperparameter Considered Values Architecture TreeLSTM ( Tai et al. , 2015 ; Zhu et al. , 2015 ) MLP , MLP - shared learning rate 1 ? 10 ?3 , 5 ? 10 ?4 , 1 ? 10
A.4 Run Time
We report the running time and number of epochs within the allowed running time ( i.e. , 96 h ) of each best-performing model in Table 6 .
A.5 Hyperparameters for Self-Training
We use the following hyperparameters for all selftraining experiments .
Other possible hyperparameters are identical to the default ones .
Please see the code attached for details .
Hyperparameter Value learning rate 5 ? 10 ?5 hidden layer size 1024 number of hidden layers 2
B Evaluation parameters
We report the used evalb parameters in Table 7 .
We modify the original COLLINS .
prm to let it evaluate unlabeled F 1 score , 16 ignoring punctuation when testing .
17 across 5 runs .
Due to space limitation , we do not include the standard deviation plot .
We show the full plot in Figure 3 .
All the standard deviations are less than 3 .
Figure 1 : 1 Figure1 : Illustration of the proposed data augmentation approach for improving few-shot parsing : we create new sentences by subtree substitution ( e.g. , substituting the subtree in the solid box by the ones in the dotted or dashed box ) , whether the created sentences are grammatical or not .
NT denotes nonterminal nodes .
Table 2 : 2 Table 1 is much worse than that reported by Kim Unlabeled F 1 scores on the standard WSJ test set .
WSJ train denotes models trained with the 40K sentences in the WSJ training set , and + WSJ dev SUB denotes models trained with the union of WSJ training sentences and 10 K sentences augmented from 1,700 WSJ development sentences .
The best number in each row is bolded .
| V|=35 K F1 | V|=10 K PRPN Few-shot ON -LSTM C-PCFG DIORA 60 70 50 40 0 number of labaled examples 15 25 55 105 0 number of labeled examples 15 25 55 105 Figure 2 : Performance of models with vocabulary size 35 K ( left ) and 10 K ( right ) on WSJ Section 24 .
C-PCFG denotes the Compound PCFG .
The F 1 scores are aver - aged over 5 runs with the same hyperparameters , dif- ferent random seeds , and different sets of labeled ex - amples when applicable .
Model WSJtrain + WSJ dev SUB PRPN 44.9 46.1 DIORA 48.0 48.2 Compound PCFG 39.2 42.2 ON -LSTM 52.0 48.2
Table 3 : 3 F 1 score on WSJ Section 24 of different models , where the base models are those used to report results in Table1with | D label | = 15 .
Model # ST -steps 0 1 5 PRPN 44.7 44.7 45.1 DIORA 46.7 48.7 49.1 Compound PCFG 41.1 41.8 42.2 ON -LSTM 50.2 51.3 52.1 Few - Shot 44.3 44.5 45.0 Few- Shot + SUB 53.3 55.5 56.6
Table 4 : 4 Details of standard split for the WSJ portion of the Penn Treebank ( Marcus et al. , 1993 ) .
All sentences are in English .
The rest split is intended left not to use in the standard supervised parsing process , and may be used for other purposes .
We use Section 24 for our analysis .
Benepar ( Kitaev and Klein , 2018 ) .
10 Hyperparameter Considered Values learning rate 1 ? 10 ?3 , 5 ? 10 ?4 , 1 ? 10 ?4 , 5 ? 10 ?5 hidden layer size 256 , 512 , 1024 number of hidden layers 2 , 4 , 8 # hyperparameter search trials = 4 ? 3 ? 3 = 36 . PRPN
( Shen et al. , 2018a ) .
11 Hyperparameter Considered Values learning rate 1 ? 10 ?3 , 5 ? 10 ?4 , 1 ? 10 ?4 hidden layer size 100 , 200 , 400 # hyperparameter search trials = 3 ? 3 = 9 . ON -LSTM
( Shen et al. , 2019 ) .
12 Hyperparameter Considered Values learning rate 1 , 10 , 30 hidden layer size 100 , 200 , 400
Table 5 : 5
The best performing sets of hyperparameters with respect to each investigated criterion , as well as corresponding validation performance ( on the corresponding development set , i.e. , the first several or no labeled examples in WSJ Section 22 ) .
The hyperparameter values are given by the order mentioned in A.2 .
# Available Ex. denotes the number of available ( labeled ) examples .
# Params denotes number of ( trainable ) model parameters , estimated in the setting that the vocabulary size | V | = 35K .
The performance without parenthesis is in terms of F 1 score ; ppl. denotes language model perplexity ; recons .
loss denotes the reconstruction loss .
C Standard Deviation w.r.t. Different Small Development Sets
In the main content of the paper ( Figure 2 ) ,
We show the average WSJ Section 24 performance ( in terms of F 1 score ) of models with different random seeds and different development set ( if applicable ) ,
Table 6 : 6
The number of epoch and estimated run time of each model .
In this work , there are only two labels : ( i ) NT denotes a constituent and ( ii ) ? denotes non-constituent .
The label ?
enables the parser to output non-binary trees ; details can be found inKitaev and Klein ( 2018 ) .
Almost all existing unsupervised parsing models do not use the nonterminal categories in the development set , so we propose to train such unlabeled constituency parsing models as their baselines .
For analysis purposes ( ?4.5 and Figure2 ) , we use WSJ Section 24 , instead of the standard development set ( Section 22 ) as we train few-shot parsing on part of it .
We do not use the standard test split ( Section 23 ) to avoid tuning on the test set , hence our analysis numbers are not directly comparable with those reported in original papers .
https://nlp.cs.nyu.edu/evalb/
5 Drozdov et al. ( 2019 ) did not evaluate any unsupervised tuning criteria for DIORA .
We choose reconstruction loss because it is what DIORA minimizes during training .
Our DIORA result also differs from that reported by Drozdov et al . ( 2019 ) ; however , our number is not directly comparable to theirs due to different data settings - they use a different training set and apply ELMo ( Peters et al. , 2018 ) for model initialization .
https://catalog.ldc.upenn.edu/ LDC99T429
Shen et al. ( 2019 ) and previous work have shown that SGD leads to much better performance than Adam , in terms of language model perplexity .
Practically , some settings share the same best hyperparameters , so we can save all the intermediate checkpoints and do post-hoc model selection efficiently .
16 https://nlp.cs.nyu.edu/evalb/EVALB.
tgz17
We keep the punctuation during training .
