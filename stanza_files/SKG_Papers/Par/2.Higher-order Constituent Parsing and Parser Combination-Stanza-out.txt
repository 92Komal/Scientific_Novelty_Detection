title
Higher -Order Constituent Parsing and Parser Combination *
abstract
This paper presents a higher - order model for constituent parsing aimed at utilizing more local structural context to decide the score of a grammar rule instance in a parse tree .
Experiments on English and Chinese treebanks confirm its advantage over its first-order version .
It achieves its best F1 scores of 91.86 % and 85.58 % on the two languages , respectively , and further pushes them to 92.80 % and 85.60 % via combination with other highperformance parsers .
Introduction Factorization is crucial to discriminative parsing .
Previous discriminative parsing models usually factor a parse tree into a set of parts .
Each part is scored separately to ensure tractability .
In dependency parsing ( DP ) , the number of dependencies in a part is called the order of a DP model ( Koo and Collins , 2010 ) .
Accordingly , existing graph - based DP models can be categorized into tree groups , namely , the first-order ( Eisner , 1996 ; McDonald et al. , 2005a ; McDonald et al. , 2005 b ) , second-order ( McDonald and Pereira , 2006 ; Carreras , 2007 ) and third -order ( Koo and Collins , 2010 ) models .
Similarly , we can define the order of constituent parsing in terms of the number of grammar rules in a part .
Then , the previous discriminative constituent parsing models ( Johnson , 2001 ; Henderson , 2004 ; Taskar et al. , 2004 ; Petrov and Klein , 2008a ; Petrov and Klein , 2008 b ; Finkel et al. , 2008 ) are the first-order ones , because there is only one grammar rule in a part .
The discriminative re-scoring models ( Collins , 2000 ; Collins and Duffy , 2002 ; Charniak and Johnson , 2005 ; Huang , 2008 ) can be viewed as previous attempts to higher - order constituent parsing , using some parts containing more than one grammar rule as non-local features .
In this paper , we present a higher - order constituent parsing model 1 based on these previous works .
It allows multiple adjacent grammar rules in each part of a parse tree , so as to utilize more local structural context to decide the plausibility of a grammar rule instance .
Evaluated on the PTB WSJ and Chinese Treebank , it achieves its best F1 scores of 91.86 % and 85.58 % , respectively .
Combined with other high- performance parsers under the framework of constituent recombination ( Sagae and Lavie , 2006 ; Fossum and Knight , 2009 ) , this model further enhances the F1 scores to 92.80 % and 85.60 % , the highest ones achieved so far on these two data sets .
Higher -order Constituent Parsing Discriminative parsing is aimed to learn a function f : S ?
T from a set of sentences S to a set of valid parses T according to a given CFG , which maps an input sentence s ?
S to a set of candidate parses T ( s ) .
The function takes the following discriminative form : where g( t , s ) is a scoring function to evaluate the event that t is the parse of s. Following Collins ( 2002 ) , this scoring function is formulated in the linear form f ( s ) = arg max t?T ( s ) g( t , s ) ( 1 ) g( t , s ) = ? ? ?( t , s ) , ( 2 ) where ?( t , s ) is a vector of features and ? the vector of their associated weights .
To ensure tractability , this model is factorized as g(t , s ) = r?t g ( Q ( r ) , s ) = r?t ? ? ?( Q( r ) , s ) , ( 3 ) where g ( Q ( r ) , s ) scores Q ( r ) , a part centered at grammar rule instance r in t , and ?( Q ( r ) , s ) is the vector of features for Q ( r ) .
Each Q ( r ) makes its own contribution to g(t , s ) .
A part in a parse tree is illustrated in Figure 1 .
It consists of the center grammar rule instance NP ? NP VP and a set of immediate neighbors , i.e. , its parent PP ? IN NP , its children NP ? DT QP and VP ? VBN PP , and its sibling IN ? of .
This set of neighboring rule instances forms a local structural context to provide useful information to determine the plausibility of the center rule instance .
Feature
The feature vector ?( Q ( r ) , s ) consists of a series of features {? i ( Q ( r ) , s ) ) | i ? 0 } . The first feature ? 0 ( Q( r ) , s ) is calculated with a PCFG - based generative parsing model ( Petrov and Klein , 2007 ) , as defined in ( 4 ) below , where r is the grammar rule instance A ?
B C that covers the span from the b-th to the e-th word , splitting at the m-th word , x , y and z are latent variables in the PCFG - based model , and I ( ? ) and O ( ? ) are the inside and outside probabilities , respectively .
All other features ? i ( Q ( r ) , s ) are binary functions that indicate whether a configuration exists in Q ( r ) and s .
These features are by their own nature in two categories , namely , lexical and structural .
All features extracted from the part in Figure 1 are demonstrated in Table 1 .
Some back - off structural features are used for smoothing , which cannot be presented due to limited space .
With only lexical features in a part , this parsing model backs off to a first-order one similar to those in the previous works .
Adding structural features , each involving a least a neighboring rule instance , makes it a higher - order parsing model .
Decoding
The factorization of the parsing model allows us to develop an exact decoding algorithm for it .
Following Huang ( 2008 ) , this algorithm traverses a parse forest in a bottom - up manner .
However , it determines and keeps the best derivation for every grammar rule instance instead of for each node .
Because all structures above the current rule instance is not determined yet , the computation of its nonlocal structural features , e.g. , parent and sibling features , has to be delayed until it joins an upper level structure .
For example , when computing the score of a derivation under the center rule NP ? NP VP in Figure 1 , the algorithm will extract child features from its children NP ? DT QP and VP ? VBN PP .
The parent and sibling features of the two child rules can also be extracted from the current derivation and used to calculate the score of this derivation .
But parent and sibling features for the center rule will not be computed until the decoding process reaches the rule above , i.e. , PP ? IN NP .
This algorithm is more complex than the approximate decoding algorithm of Huang ( 2008 ) .
However , its efficiency heavily depends on the size of the parse forest it has to handle .
Forest pruning ( Char - Child ? 0 ( Q( r ) , s ) = NP ? DT QP & VP ? VBN PP & NP ? NP VP NP ? DT QP & NP ? NP VP VP ? VBN PP & NP ? NP VP Sibling Left & IN ? of & NP ? NP VP
Constituent Recombination Following Fossum and Knight ( 2009 ) , our constituent weighting scheme for parser combination uses multiple outputs of independent parsers .
Suppose each parser generates a k-best parse list for an input sentence , the weight of a candidate constituent c is defined as ?( c ) = i k ? i ?( c , t i , k ) f ( t i , k ) , ( 5 ) where i is the index of an individual parser , ? i the weight indicating the confidence of a parser , ?( c , t i , k ) a binary function indicating whether c is contained in t i , k , the k-th parse output from the ith parser , and f ( t i , k ) the score of the k-th parse assigned by the i-th parser , as defined in Fossum and Knight ( 2009 ) .
The weight of a recombined parse is defined as the sum of weights of all constituents in the parse .
However , this definition has a systematic bias towards selecting a parse with as many constituents as possible for the highest weight .
A pruning threshold ? , similar to the one in Sagae and Lavie ( 2006 ) , is therefore needed to restrain the number of constituents in a recombined parse .
The parameters ?
i and ? are tuned by the Powell 's method ( Powell , 1964 ) on a development set , using the F1 score of PARSEVAL ( Black et al. , 1991 ) as objective .
Experiment
Our parsing models are evaluated on both English and Chinese treebanks , i.e. , the WSJ section of Penn Treebank 3.0 ( LDC99T42 ) and the Chinese Treebank 5.1 ( LDC2005T01U01 ) .
In order to compare with previous works , we opt for the same split as in Petrov and Klein ( 2007 ) , as listed in Table 2 .
For parser combination , we follow the setting of Fossum and Knight ( 2009 ) , using Section 24 instead of Section 22 of WSJ treebank as development set .
In this work , the lexical model of Chen and Kit ( 2011 ) is combined with our syntactic model under the framework of product- of-experts ( Hinton , 2002 ) .
A factor ? is introduced to balance the two models .
It is tuned on a development set using the gold sec - ( 2003 ) 90.70 Carreras et al . ( 2008 ) 91.1 Re-scoring Collins ( 2000 ) 89.70 Charniak and Johnson ( 2005 ) 91.02
The parser of Charniak and Johnson 91.40 43.54 Huang ( 2008 ) 91.69 43.5 Combination Fossum and Knight ( 2009 ) 92.4
Zhang et al. ( 2009 ) 92.3 Petrov ( 2010 ) 91.85 41.9 Self-training Zhang et al. ( 2009 ) tion search algorithm ( Kiefer , 1953 ) .
The parameters ? of each parsing model are estimated from a training set using an averaged perceptron algorithm , following Collins ( 2002 ) and Huang ( 2008 ) .
The performance of our firstand higher - order parsing models on all sentences of the two test sets is presented in Table 3 , where ? indicates a tuned balance factor .
This parser is also combined with the parser of Charniak and Johnson ( 2005 ) 2 and the Stanford .
parser 3
The best combination results in Table 3 are achieved with k=70 for English and k=100 for Chinese for selecting the k-best parses .
Our results are compared with the best previous ones on the same test sets in Tables 4 and 5
Conclusion
This paper has presented a higher - order model for constituent parsing that factorizes a parse tree into larger parts than before , in hopes of increasing its power of discriminating the true parse from the others without losing tractability .
A performance gain of 0.3 % - 0.4 % demonstrates its advantage over its first-order version .
Including a PCFG - based model as its basic feature , this model achieves a better performance than previous single and re-scoring parsers , and its combination with other parsers performs even better ( by about 1 % ) .
More importantly , it extends the existing works into a more general framework of constituent parsing to utilize more lexical and structural context and incorporate more strength of various parsing techniques .
However , higher - order constituent parsing inevitably leads to a high computational complexity .
We intend to deal with the efficiency problem of our model with some advanced parallel computing technologies in our future works .
Figure 1 : 1 Figure 1 : A part of a parse tree centered at NP ? NP VP
