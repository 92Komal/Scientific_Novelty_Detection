title
Bi-directional Attention with Agreement for Dependency Parsing
abstract
We develop a novel bi-directional attention model for dependency parsing , which learns to agree on headword predictions from the forward and backward parsing directions .
The parsing procedure for each direction is formulated as sequentially querying the memory component that stores continuous headword embeddings .
The proposed parser makes use of soft headword embeddings , allowing the model to implicitly capture high-order parsing history without dramatically increasing the computational complexity .
We conduct experiments on English , Chinese , and 12 other languages from the CoNLL 2006 shared task , showing that the proposed model achieves state - of - the - art unlabeled attachment scores on 6 languages .
1
Introduction Recently , several neural network models have been developed for efficiently accessing long-term memory and discovering dependencies in sequential data .
The memory network framework has been studied in the context of question answering and language modeling Sukhbaatar et al. , 2015 ) , whereas the neural attention model under the encoder-decoder framework has been applied to machine translation ( Bahdanau et al. , 2015 ) and constituency parsing ( Vinyals et al. , 2015 b ) .
Both frameworks learn the latent alignment between the source and target sequences , and the mechanism of attention over the encoder can be viewed as a soft operation on the memory .
Although already used in the encoder for capturing global context information ( Bahdanau et al. , 2015 ) , the bi-directional recurrent neural network ( RNN ) has yet to be employed in the decoder .
Bi-directional decoding is expected to be advantageous over the previously developed uni-directional counterpart , because the former exploits richer contextual information .
Intuitively , we can use two separate uni-directional RNNs where each one constructs its respective attended encoder context vectors for computing RNN hidden states .
However , the drawback of this approach is that the decoder would often produce different alignments resulting in discrepancies for the forward and backward directions .
In this paper , we design a training objective function to enforce attention agreement between both directions , inspired by the alignmentby - agreement idea from Liang et al . ( 2006 ) .
Specifically , we develop a dependency parser ( BiAtt - DP ) using a bi-directional attention model based on the memory network .
Given that the golden alignment is observed for dependency parsing in the training stage , we further derive a simple and interpretable approximation for the agreement objective , which makes a natural connection between the latent and observed alignment cases .
The proposed BiAtt - DP parses a sentence in a linear order via sequentially querying the memory component that stores continuous embeddings for all headwords .
In other words , we consider all possible arcs during the parsing .
This formulation is adopted by graph - based parsers such as the MST - Parser ( McDonald et al. , 2005 ) .
The consideration of all possible arcs makes the proposed BiAtt - DP different from many recently developed neural dependency parsers ( Chen and Manning , 2014 ; , which use a transitionbased algorithm by modeling the parsing procedure as a sequence of actions on buffers .
Moreover , unlike most graph - based parsers which may suffer from high computational complexity when utilizing high-order parsing history ( McDonald and Pereira , 2006 ) , the proposed BiAtt - DP can implicitly inject such information into the model while keeping the computational complexity in the order of O(n 2 ) for a sentence with n words .
This is achieved by feeding the RNN in the query component with a soft headword embedding , which is computed as the probability - weighted sum of all headword embeddings in the memory component .
To the best of our knowledge , this is the first attempt to apply memory network models to graphbased dependency parsing .
Moreover , it is the first extension of neural attention models from unidirection to multi-direction by enforcing agreement on alignments .
Experiments on English , Chinese , and 12 languages from the CoNLL 2006 shared task show the BiAtt - DP can achieve competitive parsing accuracy with several state - of - the - art parsers .
Furthermore , our model achieves the highest unlabeled attachment score ( UAS ) on Chinese , Czech , Dutch , German , Spanish and Turkish .
A MemNet-based Dependency Parser
The proposed parser first encodes each word in a sentence by continuous embeddings using a bidirectional RNN , and then performs two types of operations , i.e. 1 ) headword predictions based on bidirectional parsing history and 2 ) the relation prediction conditioned on the current modifier and its predicted headword both in the embedding space .
In the following , we first present how the token embeddings are constructed .
Then , the key components of the proposed parser , i.e. the memory component and the query component , are discussed in detail .
Lastly , we describe the parsing algorithm using a bidirectional attention model with agreement .
Token Embeddings
In the proposed BiAtt - DP , the memory and query components share the same token embeddings .
We use the notion of additive token embedding as in ( Botha and Blunsom , 2014 ) to utilize the available information about the token , e.g. , its word form , lemma , part- of-speech ( POS ) tag , and morphological features .
Specifically , the token embedding is computed as E form e form i + E pos e pos i + E lemma e lemma i + ? ? ? , where e i 's are one- hot encoding vectors for the ith word , and E's are parameters to be learned that store the continuous embeddings for corresponding feature .
Note those one- hot encoding vectors have different dimensions , depending on individual vocabulary sizes , and all E's have the same first dimension but different second dimension .
The additive token embeddings allow us to easily integrate a variety of information .
Moreover , we only need to make a single decision on the dimensionality of the token embedding , rather than a combination of decisions on word embeddings and POS tag embeddings as in concatenated token embeddings used by Chen and Manning ( 2014 ) , and .
It reduces the number of model parameters to be tuned , especially when lots of different features are used .
In our experiments , the word form and fine- grained POS tag are always used , whereas other features are used depending on their availability in the dataset .
All singleton words , lemmas , and POS tags are replaced by special tokens .
The additive token embeddings are transformed into another space before they are used by the memory and query components , i.e. x i = LReL P E form e form i + ? ? ? , where P is the projection matrix and is shared by the memory and query components as well .
The activation function of this projection layer is the leaky rectified linear ( LReL ) function ( Mass et al. , 2013 ) with 0.1 as the slope of the negative part .
In the remaining part of the paper , we refer to x i ?
R p as the token embedding for word at position i .
Note the subscript i is substituted by j and t for the memory and query components , respectively .
Components
As shown in Figure 1 , the proposed BiAtt - DP has three components , i.e. a memory component , a leftto-right query component , and a right - to - left query component .
Given a sentence of length n , the parser first uses a bi-directional RNN to construct n + 1 headword embeddings , m 0 , m 1 , . . . , m n ?
R e , with m 0 reserved for the ROOT symbol .
Each query component is an uni-directional attention model .
In a query component , a sequence of n modifier embeddings q 1 , . . . , q n ?
R d are constructed recursively by conditioning on all headword embeddings .
To address the vanishing gradient issue in RNNs , we use the gated recurrent unit ( GRU ) proposed by Cho et al . ( 2014 ) , where an update gate and a reset gate are employed to control the information flow .
We replace the hyperbolic tangent function in GRU with the LReL function , which is faster to compute and achieves better parsing accuracy in our preliminary studies .
In the following , we refer to headword and modifier embeddings as memory and query vectors , respectively .
Memory Component :
The proposed BiAtt -DP uses a bi-directional RNN to obtain the memory vectors .
At time step j , the current hidden state vector h l j ?
R e/2 ( or h r j ? R e/2 ) is computed as a non-linear transformation based on the current input vector x j and the previous hidden state vector h l j?1 ( or h r j +1 ) , i.e. h l j = GRU ( h l j?1 , x j ) ( or h r j = GRU ( h r j+1 , x j ) ) .
Ideally , the recursive nature of the RNN allows it to capture all context information from one-side , and a bi-directional RNN can thus capture context information from both sides .
We concatenate the hidden layers of the left-to- right RNN and the right - to - left RNN for the word at position j as the memory vector m j = h l j ; h r j .
These memory vectors are expected to encode the words and their context information in the headword space .
Query Component :
For each query component , we use a single-directional RNN with GRU to obtain the query vectors q j 's , which are the hidden state vectors of the RNN .
Each q t is used to query the memory component , returning association scores s t , j 's between the word at position t and the head - t and q r t . word at position j for j ?
{ 0 , ? ? ? , n} , i.e. s t , j = v T ? ( Cm j + Dq t ) , ( 1 ) where ?(? ) is the element- wise hyperbolic tangent function , and C ? R h?e , D ? R h?d and v ?
R h are model parameters .
Then , we can obtain probabilities ( aka attention weights ) , a t,0 , ? ? ? , a t , n , over all headwords in the sentence by normalizing s t , j 's , using a softmax function a t = softmax ( s t ) .
( 2 )
The soft headword embedding is then defined as mt = n j=1 a t , j m j .
At each time step t , the RNN takes the soft headword embedding ml t?1 or mr t+1 as the input , in addition to the token embedding x t .
Formally , for the forward case , the q t can be computed as q t = GRU ( q t?1 , [ mt ; x t ] ) .
Although the RNN is able to capture long-span context information to some extent , the local context may very easily dominate the hidden state .
Therefore , this additional soft headword embedding allows the model to access long-span context information in a different channel .
On the other hand , by recursively feeding both the query vector and the soft headword embedding into the RNN , the model implicitly captures high-order parsing history information , which can potentially improve the parsing accuracy ( Yamada and Matsumoto , 2003 ; McDonald and Pereira , 2006 ) .
However , for a graph- based dependency parser , utilizing parsing history features is computationally expensive .
For example , an k-th order MSTParser ( McDonald and Pereira , 2006 ) has O( n k +1 ) complexity for a sentence of n words .
In contrast , the BiAtt -DP implicitly captures high-order parsing history while keeping the complexity in the order of O( n 2 ) , i.e. for each direction .
we compute n ( n + 1 ) pair-wise probabilities a t , j for t = 1 , ? ? ? , n and j = 0 , ? ? ? , n. In this paper , we choose to use soft headword embeddings rather than making hard decisions on headwords .
In the latter case , beam search may potentially improve the parsing accuracy at the cost of higher computational complexity , i.e. O( Bn 2 ) with a beam width of B .
When using soft headword embeddings , there is no need to perform beam search .
Moreover , it is straightforward to incorporate parsing history from both directions by using two query components at the cost of O ( 2n 2 ) , which cannot be easily achieved when using beam search .
The parsing decision can be made directly based on attention weights from the two query components or further rescored by the maximum spanning tree ( MST ) search algorithm .
Parsing by Attention with Agreement
For the bi-directional attention model , the underlying probability distributions a l t and a r t may not agree with each other .
In order to encourage the agreement , we use the mathematically convenient metric , i.e. the squared Hellinger distance H 2 a l t | | a r t , for quantifying the distance between these two distri-butions .
For dependency parsing , when the golden alignment is known during training , we can derive an upper bound on the latent agreement objective as H 2 ( a l t , a r t ) ?
2 D( g t || a l t ) + D( g t || a r t ) , where D (?||? ) is the KL - divergence .
The complete derivation is provided in the Appendix A. During optimization , we can safely drop the constant scaler and the square root operation in the upper bound , leading to the following loss function D( g t || a l t ) + D( g t || a r t ) = 2D ( g t ||a l t a r t ) , ( 3 ) where indicates element -wise multiplication .
The resulting loss function is equivalent to the crossentropy loss , which is widely adopted for training neural networks .
As we can see , the loss function ( 3 ) tries to minimize the distance between the golden alignment and the intersection of the two directional attention alignments at every time step .
Therefore , during inference , the headword prediction for the word at time step t can be obtained as argmax j log a l t , j + log a r t , j , seeking for agreement between both query components .
This parsing procedure is also similar to the exhaustive left-to- right modifier -first search algorithm described in ( Covington , 2001 ) , but it is enhanced by an additional right - to - left search with the agreement enforcement .
Alternatively , we can treat ( log a l t , j + log a r t , j ) as a score of the corresponding arc and then search for the MST to form a dependency parse tree , as proposed in ( McDonald et al. , 2005 ) .
The MST search is achieved via the Chu-Liu-Edmonds algorithm ( Chu and Liu , 1965 ; Edmonds , 1967 ) , which can be implemented in O(n 2 ) for dense graphs according to Tarjan ( 1977 ) .
In practice , the MST search slows down the parsing speed by 6 - 10 % .
However , it forces the parser to produce a valid tree , and we observe a slight improvement on parsing accuracy in most cases .
After obtaining each modifier and its soft header embeddings , we use a single - layer perceptron to predict the head-modifier relation , i.e. y t = softmax U ml t ; mr t + W q l t ; q r where y t,1 , ? ? ? , y t , m are the probabilities of m possible relations , and U ? R m?2e and W ? R m?2d are model parameters .
Model Learning
For the t-th word ( modifier ) w t in a sentence of length n , let H l t and H r t denote random variables representing the predicted headword from forward ( left-to- right ) and backward ( right - to- left ) parsing directions , respectively .
Also let R t denote the random variable representing the dependency relation for w t .
The joint probability of headword and relation predictions can be written as P ( R 1:n , H l 1:n , H r 1:n |w 1:n ) = n t=1 P ( R t |w 1:n ) P ( H l t |w 1:n ) P
( H r t |w 1:n ) = n t=1 y l t, Rt ?
a l t, H l t ?
a r t,H r t ( 5 ) where at each time step we assume head-modifier relations and headwords from both directions are independent with each other when conditioned on the global knowledge of the whole sentence .
Note that the long-span context and high-order parsing history information are injected when we model P ( H l t |w 1:n ) , P ( H r t |w 1:n ) and P ( R t |w 1:n ) , as discussed in Section 2.2 .
As discussed in Section 2.3 , the model can be trained by encouraging attention agreement between two query components .
From ( 5 ) , we observe that it is equivalent to maximizing the log-likelihood of the golden dependency tree ( or minimizing the crossentropy ) for each training sentence , i.e. n t=1 log y t, relationt + log a l t, headt + log a r t, headt , where a t , j and y t ,r are defined in ( 2 ) and ( 4 ) , respectively , and relation t and head t are golden relation and headword labels , respectively .
The gradients are computed via the back - propagation algorithm ( Rumelhart et al. , 1986 ) .
Errors of y t come from the arc labels , whereas there are two source of errors for a t , one from the headword labels and the other back - propagated from errors of y t .
We use stochastic gradient descent with the Adam algorithm proposed in ( Kingma and Ba , 2015 ) .
The learning rate is halved at each iteration once the loglikelihood of the dev set decreases .
The whole training procedure terminates when the log-likelihood decreases for the second time .
All learning parameters except bias terms are initialized randomly according to the Gaussian distribution N ( 0 , 10 ?2 ) .
In our experiments , we tune the initial learning rate with a step size of 0.0002 , and choose the best one based on the log-likelihood on the dev set at the first epoch .
Empirically , the selected initial learning rates fall in the range of [ 0.0004 , 0.0010 ] for hidden layer size [ 128 , 320 ] , and tend to be larger when using a smaller hidden layer size , i.e. [ 0.0016 , 0.0034 ] for hidden layer size around 80 .
The training data are randomly shuffled at every epoch .
Experiments
In this section , we present the parsing accuracy of the proposed BiAtt - DP on 14 languages .
We report both UAS and labeled attachment score ( LAS ) , obtained by the CoNLL -X eval .pl script 2 which ignores punctuation symbols .
The headword predictions are made through the MST search , which slightly improves both UAS and LAS ( less than 0.3 % absolutely ) .
Overall , the proposed BiAtt - DP achieves competitive parsing accuracy on all languages as state - of - the - art parsers , and obtains better UAS in 6 languages .
We also show the impact of using POS tags and pre-trained word embeddings .
Moreover , different variants of the full model are compared in this section .
Data
We work on the English Treebank - 3 ( PTB ) dataset ( Marcus et al. , 1999 ) , the Chinese Treebank - 5.1 ( CTB ) dataset ( Palmer et al. , 2005 ) , and 12 other languages from the CoNLL 2006 shared task ( Buchholz and Marsi , 2006 ) .
For PTB and CTB datasets , we use exactly the same setup as in ( Chen and Manning , 2014 ; .
Specifically , we convert the English and Chinese data using the Stanford parser v3.3.0 ( de Marneffe et al. , 2006 ) and the Penn2 Malt tool ( Zhang and Clark , 2008 ) , respectively .
For English , POS tags are obtained using the Stanford POS tagger v3.3.0 ( Toutanova et al. , 2003 ) , whereas for Chinese , we use gold segmentation and POS tags .
When constructing the token embeddings for English and Chinese , both the word form and the POS tag are used .
We also initialize E form by pretrained word embeddings 3 . For the 12 other languages , we randomly hold out 5 % of the training data as the dev set .
In addition to the word form and find- grained POS tags , we use extra features such as lemmas , coarse- grained POS tags , and morphemes when they are available in the dataset .
No pre-trained word embeddings are used for these 12 languages .
Model Configurations
The hidden layer size is kept the same across all RNNs in the proposed BiAtt-DP .
We also require the dimension of the token embeddings to be the same as the hidden layer size .
Note that we concatenate the hidden layers of two RNNs for constructing m j , and thus we have e = 2d .
The weight matrices C and D respectively project vectors m j and q t to the same dimension h , which is equivalent to d .
For English and Chinese , since the dimension of pretrained word embeddings are 300 , we use 300 ? h as the dimension of embedding parameters E's .
For the 12 other languages , we use square matrices for the embedding parameters E's .
For all languages ,
We tune the hidden layer size and choose one according to UAS on the dev set .
The selected hidden layer sizes for these languages are : 368 ( English ) , 114 ( Chinese ) , 128 ( Arabic ) , 160 ( Bulgarian ) , 224 ( Czech ) , 176 ( Danish ) , 220 ( Dutch ) , 200 ( German ) , 128 ( Japanese ) , 168 ( Portuguese ) , 128 ( Slovene ) , 144 ( Spanish ) , 176 ( Swedish ) , and 128 ( Turkish ) .
Results
We first compare our parser with state - of - the - art neural transition - based dependency parsers on PTB and CTB .
For English , we also compare with stateof - the - art graph - based dependency parsers .
The results are shown in Table 1 and Table 2 the transition - based parsers , it achieves better accuracy than Chen and Manning ( 2014 ) , which uses a feed-forward neural network , and , which uses three stack LSTM networks .
Compared with the integrated parsing and tagging models , the BiAtt - DP outperforms Bohnet and Nivre ( 2012 ) but has a small gap to .
On CTB , it achieves best UAS and similar LAS .
This may be caused by that the relation vocabulary size is relatively smaller than the average sentence length , which biases the joint objective to be more sensitive to UAS .
The parsing speed is around 50 - 60 sents / sec measured on a desktop with Intel Core i7 CPU @ 3.33 GHz using single thread .
Next , in Table 3 we show the parsing accuracy of the proposed BiAtt - DP on 12 languages in the CoNLL 2006 shared task , including comparison with state - of - the - art parsers .
Specifically , we show UAS of the 3rd- order RBGParser as reported in since it also uses low-dimensional continuous embeddings .
However , there are several major differences between the RBGParser and the BiAtt- DP .
First , in ( Buchholz and Marsi , 2006 ) .
We also report corresponding LAS in squared brackets .
The results of the 3rd- order RBGParser are reported in .
Best published results on the same dataset in terms of UAS among ( Pitler and McDonald , 2015 ) , ( Zhang and McDonald , 2014 ) , ( Zhang et al. , 2013 ) , ( Zhang and McDonald , 2012 ) , ( Rush and Petrov , 2012 ) , ( Martins et al. , 2013 ) , ( Martins et al. , 2010 ) , and ( Koo et al. , 2010 ) .
To study the effectiveness of the parser in dealing with non-projectivity , we follow ( Pitler and McDonald , 2015 ) , to compute the recall of crossed and uncrossed arcs in the gold tree , as well as the percentage of crossed arcs .
from low-rank tensors .
Second , the RBGParser uses combined scoring of arcs by including traditional features from the MSTParser ( McDonald and Pereira , 2006 ) / TurboParser ( Martins et al. , 2013 ) .
Third , the RBGParser employs a third - order parsing algorithm based on , although it also implements a first-order parsing algorithm , which achieves lower UAS in general .
In Table 3 , we show that the proposed BiAtt -DP outperforms the RBGParser in most languages except Japanese , Slovene , and Swedish .
It can be observed from Table 3 that the BiAtt - DP has highly competitive parsing accuracy as stateof - the - art parsers .
Moreover , it achieves best UAS for 5 out of 12 languages .
For the remaining seven languages , the UAS gaps between the BiAtt-DP and state - of - the - art parsers are within 1.0 % , except Swedish .
An arguably fair comparison for the BiAtt -DP is the MSTParser ( McDonald and Pereira , 2006 ) , since the BiAtt - DP replaces the scoring function for arcs but uses exactly the same search algorithm .
Due to the space limit , we refer readers to for results of the MSTParsers ( also shown in Appendix B ) .
The BiAtt - DP consistently outperforms both parser by up to 5 % absolute UAS score .
Finally , following ( Pitler and McDonald , 2015 ) , we also analyze the performance of the BiAtt - DP on both crossed and uncrossed arcs .
Since the BiAtt-DP uses a graph- based non-projective parsing algorithm , it is interesting to evaluate the performance on crossed arcs , which result in the non-projectivity of the dependency tree .
The last three columns of Table 3 show the recall of crossed arcs , that of uncrossed arcs , and the percentage of crossed arcs in the test set .
Pitler and McDonald ( 2015 ) reported numbers on the same data for Dutch , German , Portuguese , and Slovene as in this paper .
For these four languages , the BiAtt -DP achieves better UAS than that reported in ( Pitler and McDonald , 2015 ) .
More importantly , we observe that the improvement on recall of crossed arcs ( around 10 - 18 % absolutely ) is much more significant than that of uncrossed arcs ( around 1 - 3 % absolutely ) , which indicates the effectiveness of the BiAtt - DP in parsing languages with non-projective trees .
Ablative Study
Here we try to study the impact of using pre-trained word embeddings , POS tags , as well as the bidirectional query components on our model .
First of all , we start from our best model ( Model 1 in Table 4 ) on English , which uses 300 as the token embedding dimension and 368 as the hidden layer size .
We keep those model parameter dimensions unchanged and analyze different factors by comparing the parsing accuracy on PTB dev set .
The results are summarized in Table 4 . Comparing Models 1 - 3 , it can be observed that without using pre-trained word embeddings , both UAS and LAS drop by 0.6 % , and without using POS tags in token embeddings , the numbers further drop by 1.6 % in UAS and around 2.6 % in LAS .
In terms of query components , using single query component ( Models 4 - 5 ) degrades UAS by 0.7-0.9 % and LAS by around 1.0 % , compared with Model 2 .
For Model 6 , the soft headword embedding is only used for arc label predictions but not fed into the next hidden state , which is around 0.3 % worse than Model 2 .
This supports the hypothesis about the usefulness of the parsing history information .
We also implement a variant of Model 6 which produces one a t instead two by using both q l t and q r t in ( 1 ) .
It gets 92.44 % UAS and 89.26 % LAS , indicating that naively applying a bi-directional RNN may not be enough .
Related Work Neural Dependency Parsing : Recently developed neural dependency parsers are mostly transition - based models , which read words sequentially from a buffer into a stack and incrementally build a parse tree by predicting a sequence of transitions ( Yamada and Matsumoto , 2003 ; Nivre , 2003 ; Nivre , 2004 ) .
A feed-forward neural network is used in ( Chen and Manning , 2014 ) , where they represent the current state with 18 selected elements such as the top words on the stack and buffer .
Each element is encoded by concatenated embeddings of words , POS tags , and arc labels .
Their dependency parser achieves improvement on both accuracy and parsing speed .
improve the parser using semi-supervised structured learning and unlabeled data .
The model is extended to integrate parsing and tagging in .
On the other hand , develop the stack LSTM architecture , which uses three LSTMs to respectively model the sequences of buffer states , stack states , and actions .
Unlike the transition - based formulation , the proposed BiAtt - DP directly predicts the headword and the dependency relation at each time step .
Specifically , there is no explicit representation of actions or headwords in our model .
The model learns to retrieve the most relevant information from the input memory to make decisions on headwords and head-modifier relations .
Graph - based Dependency Parsing :
In addition to the transition - based parsers , another line of research in dependency parsing uses graph - based models .
Graph - based parser usually build a dependency tree from a directed graph and learns to scoring the possible arcs .
Due to this nature , nonprojective parsing can be done straightforwardly by most graph - based dependency parsers .
The MST - Parser ( McDonald et al. , 2005 ) and the TurboParser ( Martins et al. , 2010 ) are two examples of graphbased parsers .
The MSTParser formulates the parsing as searching for the MST , whereas the Tur-boParser performs approximate variational inference over a factor graph .
The RBGParser proposed in can also be viewed as a graph - based parser , which scores arcs using low-dimensional continuous features derived from low-rank tensors as well as features used by MST - Parser / TurboParser .
It also employs a sampler- based algorithm for parsing .
Neural Attention Model :
The proposed BiAtt - DP is closely related to the memory network ( Sukhbaatar et al. , 2015 ) for question answering , as well as the neural attention models for machine translation ( Bahdanau et al. , 2015 ) and constituency parsing ( Vinyals et al. , 2015 b ) .
The way we query the memory component and obtain the soft headword embeddings is essentially the attention mechanism .
However , different from the above studies where the alignment information is latent , in dependency parsing , the arc between the modifier and headword is known during training .
Thus , we can utilize these labels for attention weights .
The similar idea is employed by the pointer network in ( Vinyals et al. , 2015a ) , which is used to solve three different combinatorial optimization problems .
Conclusion
In this paper , we develop a bi-directional attention model by encouraging agreement between the latent attention alignments .
Through a simple and interpretable approximation , we make the connection between latent and observed alignments for training the model .
We apply the bi-directional attention model incorporating the agreement objective during training to the proposed memory - network - based dependency parser .
The resulting parser is able to implicitly capture the high-order parsing history without suffering from issue of high computational complexity for graph - based dependency parsing .
We have carried out empirical studies over 14 languages .
The parsing accuracy of the proposed model is highly competitive with state - of - the - art dependency parsers .
For English , the proposed BiAtt - DP outperforms all graph - based parsers .
It also achieves state - of - the - art performance in 6 languages in terms of UAS , demonstrating the effectiveness of the proposed mechanism of bi-directional attention with agreement and its use in dependency parsing .
A Upper Bound on H 2 ( p , q ) Here , we use the following definition of squared Hellinger distance for countable space H 2 ( p , q ) = 1 2 i ( ? p i ? ? q i ) 2 where p , q ? ? k are two k-simplexes .
Introducing g ? ? k , the squared Hellinger distance can be upper bounded as H 2 ( p , q ) ? ? 2H ( p , q ) ( 6 ) ? ? 2 [ H ( p , g ) + H(q , g ) ] ( 7 ) ? 2 H 2 ( p , g ) + H 2 ( q , g ) ( 8 ) where ( 6 ) , ( 7 ) and ( 8 ) follow the inequalities between the 1 - norm and the 2 - norm , the triangle inequality defined for a metric , and the Cauchy - Schwarz 's inequality , respectively .
Using the relationship between the KL - divergence and the squared Hellinger distance , ( 8 ) can be further bounded by 2 D ( g | | p ) + D ( g | |q ) .
( Buchholz and Marsi , 2006 ) .
We use the numbers reported in .
