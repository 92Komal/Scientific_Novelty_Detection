title
Turbo Parsers : Dependency Parsing by Approximate Variational Inference
abstract
We present a unified view of two state - of- theart non-projective dependency parsers , both approximate : the loopy belief propagation parser of Smith and Eisner ( 2008 ) and the relaxed linear program of Martins et al . ( 2009 ) .
By representing the model assumptions with a factor graph , we shed light on the optimization problems tackled in each method .
We also propose a new aggressive online algorithm to learn the model parameters , which makes use of the underlying variational representation .
The algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions , including CRFs and structured SVMs .
Experiments show state - of - the - art performance for 14 languages .
Introduction Feature- rich discriminative models that break locality / independence assumptions can boost a parser 's performance ( McDonald et al. , 2006 ; Huang , 2008 ; Finkel et al. , 2008 ; Smith and Eisner , 2008 ; Martins et al. , 2009 ; . Often , inference with such models becomes computationally intractable , causing a demand for understanding and improving approximate parsing algorithms .
In this paper , we show a formal connection between two recently - proposed approximate inference techniques for non-projective dependency parsing : loopy belief propagation ( Smith and Eisner , 2008 ) and linear programming relaxation ( Martins et al. , 2009 ) .
While those two parsers are differently motivated , we show that both correspond to inference in a factor graph , and both optimize objective functions over local approximations of the marginal polytope .
The connection is made clear by writing the explicit declarative optimization problem underlying Smith and Eisner ( 2008 ) and by showing the factor graph underlying Martins et al . ( 2009 ) .
The success of both approaches parallels similar approximations in other fields , such as statistical image processing and error-correcting coding .
Throughtout , we call these turbo parsers .
1
Our contributions are not limited to dependency parsing : we present a general method for inference in factor graphs with hard constraints ( ?2 ) , which extends some combinatorial factors considered by Smith and Eisner ( 2008 ) .
After presenting a geometric view of the variational approximations underlying message - passing algorithms ( ?3 ) , and closing the gap between the two aforementioned parsers ( ?4 ) , we consider the problem of learning the model parameters ( ?5 ) .
To this end , we propose an aggressive online algorithm that generalizes MIRA ( Crammer et al. , 2006 ) to arbitrary loss functions .
We adopt a family of losses subsuming CRFs ( Lafferty et al. , 2001 ) and structured SVMs ( Taskar et al. , 2003 ; Tsochantaridis et al. , 2004 ) .
Finally , we present a technique for including features not attested in the training data , allowing for richer models without substantial runtime costs .
Our experiments ( ?6 ) show state - of - the - art performance on dependency parsing benchmarks .
Structured Inference and Factor Graphs Denote by X a set of input objects from which we want to infer some hidden structure conveyed in an output set Y. Each input x ? X ( e.g. , a sentence ) is associated with a set of candidate outputs Y( x ) ? Y ( e.g. , parse trees ) ; we are interested in the case where Y( x ) is a large structured set .
Choices about the representation of elements of Y( x ) play a major role in algorithm design .
In many problems , the elements of Y( x ) can be represented as discrete- valued vectors of the form y = y 1 , . . . , y I , each y i taking values in a label set Y i .
For example , in unlabeled dependency parsing , I is the number of candidate dependency arcs ( quadratic in the sentence length ) , and each Y i = { 0 , 1 } .
Of course , the y i are highly interdependent .
Factor Graphs .
Probabilistic models like CRFs ( Lafferty et al. , 2001 ) assume a factorization of the conditional distribution of Y , Pr( Y = y | X = x ) ? C?C ? C ( x , y C ) , ( 1 ) where each C ? { 1 , . . . , I} is a factor , C is the set of factors , each y C y i i?C denotes a partial output assignment , and each ?
C is a nonnegative potential function that depends on the output only via its restriction to C .
A factor graph ( Kschischang et al. , 2001 ) is a convenient representation for the factorization in Eq. 1 : it is a bipartite graph G x comprised of variable nodes { 1 , . . . , I} and factor nodes C ? C , with an edge connecting the ith variable node and a factor node C iff i ?
C. Hence , the factor graph G x makes explicit the direct dependencies among the variables {y 1 , . . . , y I }. Factor graphs have been used for several NLP tasks , such as dependency parsing , segmentation , and co-reference resolution ( Sutton et al. , 2007 ; Smith and Eisner , 2008 ; McCallum et al. , 2009 ) . Hard and Soft Constraint Factors .
It may be the case that valid outputs are a proper subset of Y 1 ? ? ? ? ?
Y I - for example , in dependency parsing , the entries of the output vector y must jointly define a spanning tree .
This requires hard constraint factors that rule out forbidden partial assignments by mapping them to zero potential values .
See Table 1 for an inventory of hard constraint factors used in this paper .
Factors that are not of this special kind are called soft factors , and have strictly positive potentials .
We thus have a partition C = C hard ?
C soft .
We let the soft factor potentials take the form ? C ( x , y C ) exp (? ? C ( x , y C ) ) , where ? ?
R d is a vector of parameters ( shared across factors ) and ?
C ( x , y C ) is a local feature vector .
The conditional distribution of Y ( Eq. 1 ) thus becomes log-linear : Pr ? ( y|x ) = Z x ( ? ) ?1 exp ( ? ?( x , y ) ) , ( 2 ) where Z x ( ? ) y ?Y( x ) exp (? ?( x , y ) ) is the partition function , and the features decompose as : ?( x , y ) C?C soft ? C ( x , y C ) .
( 3 ) Dependency Parsing .
Smith and Eisner ( 2008 ) proposed a factor graph representation for dependency parsing ( Fig. 1 ) .
The graph has O(n 2 ) variable nodes ( n is the sentence length ) , one per candidate arc a h , m linking a head h and modifier m .
Outputs are binary , with y a = 1 iff arc a belongs to the dependency tree .
There is a hard factor TREE connected to all variables , that constrains the overall arc configurations to form a spanning tree .
There is a unary soft factor per arc , whose log-potential reflects the score of that arc .
There are also O(n 3 ) pairwise factors ; their log-potentials reflect the scores of sibling and grandparent arcs .
These factors create loops , thus calling for approximate inference .
Without them , the model is arc-factored , and exact inference in it is well studied : finding the most probable parse tree takes O(n 3 ) time with the Chu-Liu-Edmonds algorithm ( McDonald et al. , 2005 ) , 2 and computing posterior marginals for all arcs takes O(n 3 ) time via the matrix-tree theorem ( Smith and Smith , 2007 ; Koo et al. , 2007 ) .
Message - passing algorithms .
In general factor graphs , both inference problemsobtaining the most probable output ( the MAP ) argmax y?Y ( x ) Pr ? ( y|x ) , and computing the marginals Pr ? ( Y i = y i | x ) - can be addressed with the belief propagation ( BP ) algorithm ( Pearl , 1988 ) , which iteratively passes messages between variables and factors reflecting their local " beliefs . "
A general binary factor : ? C ( v 1 , . . . , vn ) = ?
1 v 1 , . . . , vn ?
S C 0 otherwise , where S C ? { 0 , 1 } n . ? Message-induced distribution : ? m j?C j=1 , ... , n ? Partition function : Z C ( ? ) P v 1 , ... , vn ?S C Q n i=1 m v i i?C ? Marginals : MARG i ( ? )
Pr?{ V i = 1 | V 1 , . . . , Vn ? S C } ? Max-marginals : MAX -MARG i , b ( ? ) max v?S C Pr?( v|v i = b ) ? Sum-prod. : m C?i = m ?1 i?C ? MARG i ( ? ) / ( 1 ? MARG i ( ? ) ) ? Max-prod. : m C?i = m ?1 i?C ? MAX -MARG i,1 ( ?) / MAX -MARG i,0 ( ? ) ? Local agreem .
constr. : z ? conv S C , where z = ? i ( 1 ) n i=1 ?
Entropy : H C = log Z C ( ? ) ? P n i=1 MARG i ( ? ) log m i?C TREE ?TREE ( ya a?A ) = ?
1 y ? Ytree ( i.e. , { a ?
A | ya = 1 } is a directed spanning tree ) 0 otherwise , where A is the set of candidate arcs .
? Partition function Ztree ( ? ) and marginals MARGa ( ? ) a?A computed via the matrix-tree theorem , with ?
ma?TREE a?A ? Sum-prod. : mTREE ?a = m ?1 a?TREE ? MARG a ( ? ) / ( 1 ? MARGa ( ? ) ) ? Max-prod. : mTREE ?a = m ?1 a?TREE ? MAX - MARG a,1 ( v i = 1 0 otherwise .
? Sum-prod. : m XOR ? i = "
P j =i m j?XOR " ?1 ? Max-prod. : m XOR ? i = `max j =i m j?XOR ?1 ? Local agreem .
constr . :
P i z i = 1 , z i ? [ 0 , 1 ] , ?i ? HXOR = ? P i ( m i?XOR / P j m j?XOR ) log ( m i?XOR / P j m j?XOR ) OR ?OR ( v 1 , . . . , vn ) = ?
1 P n i=1 v i ?
1 0 otherwise .
? Sum-prod. : m OR ? i = " 1 ? Q j =i ( 1 + m j?OR ) ?1 " ?1 ? Max-prod. : m OR ? i = max {1 , min j =i m ?1 j?OR } ? Local agreem .
constr . : P i z i ? 1 , z i ? [ 0 , 1 ] , ?i OR -WITH - OUTPUT ?OR - OUT ( v 1 , . . . , vn ) = ?
1 vn = W n?1 i=1 v i 0 otherwise .
? Sum-prod. : m OR - OUT ? i = ( " 1 ? ( 1 ? m ?1 n?OR - OUT ) Q j =i , n ( 1 + m j?OR - OUT ) ?1 " ?1 i < n Q j =n ( 1 + m j?OR - OUT ) ?
1 i = n. ? Max-prod. : m OR - OUT ? i = ( min n mn ?OR - OUT
Q j =i , n max {1 , m j?OR - OUT } , max { 1 , min j =i , n m ?1 j?OR - OUT } o i < n Q j =n max { 1 , m j?OR - OUT } min{1 , max j =n m j?OR - OUT } i = n. Table 1 : Hard constraint factors , their potentials , messages , and entropies .
The top row shows expressions for a general binary factor : each outgoing message is computed from incoming marginals ( in the sum-product case ) , or max-marginals ( in the max-product case ) ; the entropy of the factor ( see ?3 ) is computed from these marginals and the partition function ; the local agreement constraints ( ?4 ) involve the convex hull of the set S C of allowed configurations ( see footnote 5 ) .
The TREE , XOR , OR and OR -WITH - OUTPUT factors allow tractable computation of all these quantities ( rows 2 - 5 ) .
Two of these factors ( TREE and XOR ) had been proposed by Smith and Eisner ( 2008 ) ; we provide further information ( max - product messages , entropies , and local agreement constraints ) .
Factors OR and OR -WITH - OUTPUT are novel to the best of our knowledge .
This inventory covers many cases , since the above formulae can be extended to the case where some inputs are negated : just replace the corresponding messages by their reciprocal , v i by 1 ? v i , etc .
This allows building factors NAND ( an OR factor with negated inputs ) , IMPLY ( a 2 - input OR with the first input negated ) , and XOR-WITH - OUTPUT ( an XOR factor with the last input negated ) .
In sum-product BP , the messages take the form : 3 M i?C ( y i ) ? D =C M D?i ( y i ) ( 4 ) M C?i ( y i ) ? y C ?y i ? C (y C ) j =i M j?C ( y j ) .
( 5 ) In max -product BP , the summation in Eq. 5 is replaced by a maximization .
Upon convergence , variable and factor beliefs are computed as : ? i ( y i ) ? C M C?i ( y i ) ( 6 ) ? C ( y C ) ? ? C ( y C ) i M i?C ( y i ) . ( 7 ) BP is exact when the factor graph is a tree : in the sum-product case , the beliefs in Eqs. 6 - 7 correspond TREE 1 ARC ( h , m ? ) SIB ( h , m? , m? ) 1 2 SIB ( h , m? , m? ) 1 3 SIB ( h , m? , m? ) 2 3 GRAND ( g , h , m ? ) 1 2 ARC ( h , m ? ) 3 ARC ( h , m ? ) ARC ( g , h ) Figure 1 : Factor graph corresponding to the dependency parsing model of Smith and Eisner ( 2008 ) with sibling and grandparent features .
Circles denote variable nodes , and squares denote factor nodes .
Note the loops created by the inclusion of pairwise factors ( GRAND and SIB ) .
In Table 1 we present closed - form expressions for the factor-to- variable message ratios m C?i M C?i ( 1 ) / M C?i ( 0 ) in terms of their variable - tofactor counterparts m i?C M i?C ( 1 ) / M i?C ( 0 ) ; these ratios are all that is necessary when the variables are binary .
Detailed derivations are presented in an extended version of this paper ( Martins et al. , 2010 b ) .
Variational Representations Let P x { Pr ? ( .|x ) | ? ?
R d } be the family of all distributions of the form in Eq. 2 .
We next present an alternative parametrization for the distributions in P x in terms of factor marginals .
We will see that each distribution can be seen as a point in the socalled marginal polytope ( Wainwright and Jordan , 2008 ) ; this will pave the way for the variational representations to be derived next .
Parts and Output Indicators .
A part is a pair C , y C , where C is a soft factor and y C a partial output assignment .
We let R = { C , y C | C ?
C soft , y C ? i?C
Y i } be the set of all parts .
Given an output y ? Y ( x ) , a part C , y C is said to be ac- tive if it locally matches the output , i.e. , if y C = y C .
Any output y ? Y ( x ) can be mapped to a | R |- dimensional binary vector ?( y ) indicating which parts are active , i.e. , [?( y ) ]
C , y C = 1 if y C = y C Pr{?C ( YC ) = 1 | Yi = yi} and MC?i( yi ) ? max ? C ( y C ) =1 Pr{YC = yC | Yi = yi} , respectively for the sum-product and max-product cases ; these probabilities are induced by the messages in Eq. 4 : for an event A ? Q i?C Yi , Pr{YC ? A} P y C I( yC ? A ) Q i?C Mi?C ( yi ) .
and 0 otherwise ; ?(y ) is called the output indicator vector .
This mapping allows decoupling the feature vector in Eq. 3 as the product of an input matrix and an output vector : ?( x , y ) = C?C soft ? C ( x , y C ) = F ( x ) ?( y ) , ( 8 ) where F ( x ) is a d-by - |R | matrix whose columns contain the part-local feature vectors ? C ( x , y C ) .
Observe , however , that not every vector in { 0 , 1 } | R | corresponds necessarily to a valid output in Y ( x ) .
Marginal Polytope .
Moving to vector representations of outputs leads naturally to a geometric view of the problem .
The marginal polytope is the convex hull 5 of all the " valid " output indicator vectors : M( G x ) conv {?( y ) | y ? Y ( x ) } .
Note that M( G x ) only depends on the factor graph G x and the hard constraints ( i.e. , it is independent of the parameters ? ) .
The importance of the marginal polytope stems from two facts : ( i ) each vertex of M( G x ) corresponds to an output in Y ( x ) ; ( ii ) each point in M( G x ) corresponds to a vector of marginal probabilities that is realizable by some distribution ( not necessarily in P x ) that factors according to G x . Variational Representations .
We now describe formally how the points in M( G x ) are linked to the distributions in P x .
We extend the " canonical overcomplete parametrization " case , studied by Wainwright and Jordan ( 2008 ) ( .|x ) .
The component of ? ? M( G x ) indexed by part C , y C is denoted ?
C ( y C ) .
Proposition 1 . There is a map coupling each distri- bution Pr ? ( .|x ) ?
P x to a unique ? ? M( G x ) such that E ? [? ( Y ) ] = ?. Define H ( ? ) H( Pr ? ( .|x ) ) if some Pr ? ( .|x ) is coupled to ? , and H ( ? ) = ? if no such Pr ? ( .|x ) exists .
Then : 1 . The following variational representation for the log-partition function ( mentioned in Eq. 2 ) holds : log Z x ( ? ) = max ?M( Gx ) ? F ( x ) ? + H ( ? ) .
( 9 ) 5 The convex hull of { z1 , . . . , z k } is the set of points that can be written as P k i=1 ?izi , where 2 .
The problem in Eq. 9 is convex and its solution is attained at the factor marginals , i.e. , there is a maximizer ? s.t. ?C (y P k i=1 ?i = C ) = Pr ? ( Y C = y C | x ) for each C ? C .
The gradient of the log-partition function is ? log Z x ( ? ) = F ( x ) ?.
3 . The MAP ? argmax y?Y ( x ) Pr ? ( y|x ) can be obtained by solving the linear program ? ?(? ) = argmax ?M( Gx ) ? F ( x ) ?. ( 10 ) A proof of this proposition can be found in Martins et al . ( 2010a ) .
Fig. 2 provides an illustration of the dual parametrization implied by Prop .
1 .
Approximate Inference & Turbo Parsing
We now show how the variational just described relates to message - passing algorithms and provides a common framework for analyzing two recent dependency parsers .
Later ( ?5 ) , Prop .
1 is used constructively for learning the model parameters .
Loopy BP as a Variational Approximation
For general factor graphs with loops , the marginal polytope M( G x ) cannot be compactly specified and the entropy term H ( ? ) lacks a closed form , rendering exact optimizations in Eqs. 9 - 10 intractable .
A popular approximate algorithm for marginal inference is sum-product loopy BP , which passes messages as described in ?2 and , upon convergence , computes beliefs via Eqs. 6 - 7 .
Were loopy BP exact , these beliefs would be the true marginals and hence a point in the marginal polytope M( G x ) .
However , this need not be the case , as elucidated by Yedidia et al . ( 2001 ) and others , who first analyzed loopy BP from a variational perspective .
The following two approximations underlie loopy BP : ?
The marginal polytope M( G x ) is approximated by the local polytope L( G x ) .
This is an outer bound ; its name derives from the fact that it only imposes local agreement constraints ?i , y i ?
Y i , C ? C : y i ? i ( y i ) = 1 , y C ?y i ? C (y C ) = ? i ( y i ) . ( 11 ) Namely , it is characterized by L( G x ) {? ? R | R | + | Eq. 11 holds ?i , y i ?
Y i , C ? C}.
The elements of L( G x ) are called pseudo-marginals .
Clearly , the true marginals satisfy Eq. 11 , and therefore M( G x ) ? L( G x ) . ?
The entropy H is replaced by its Bethe approximation H Bethe ( ? ) I i=1 ( 1 ? d i ) H ( ? i ) + C?C H ( ? C ) , where d i = |{ C | i ?
C}| is the number of factors connected to the ith variable , H ( ?
i ) ? y i ? i ( y i ) log ? i ( y i ) and H ( ?
C ) ? y C ? C (y C ) log ? C ( y C ) .
Any stationary point of sum-product BP is a local optimum of the variational problem in Eq. 9 with M( G x ) replaced by L( G x ) and H replaced by H Bethe ( Yedidia et al. , 2001 ) .
Note however that multiple optima may exist , since H Bethe is not necessarily concave , and that BP may not converge .
Table 1 shows closed form expressions for the local agreement constraints and entropies of some hard - constraint factors , obtained by invoking Eq. 7 and observing that ?
C (y C ) must be zero if configuration y C is forbidden .
See Martins et al . ( 2010 b ) .
Two Dependency Turbo Parsers
We next present our main contribution : a formal connection between two recent approximate dependency parsers , which at first sight appear unrelated .
Recall that ( i ) Smith and Eisner ( 2008 ) proposed a factor graph ( Fig. 1 ) in which they run loopy BP , and that ( ii ) Martins et al . ( 2009 ) approximate parsing as the solution of a linear program .
Here , we fill the blanks in the two approaches : we derive explicitly the variational problem addressed in ( i ) and we provide the underlying factor graph in ( ii ) .
This puts the two approaches side-by-side as approximate methods for marginal and MAP inference .
Since both rely on " local " approximations ( in the sense of Eq. 11 ) that ignore the loops in their graphical models , we dub them turbo parsers by analogy with error-correcting turbo decoders ( see footnote 1 ) .
Turbo Parser # 1 : Sum-Product Loopy BP .
The factor graph depicted in Fig. 1 - call it G x - includes pairwise soft factors connecting sibling and grandparent arcs .
6
We next characterize the local polytope L( G x ) and the Bethe approximation H Bethe inherent in Smith and Eisner 's loopy BP algorithm .
Let A be the set of candidate arcs , and P ?
A 2 the set of pairs of arcs that have factors .
Let ? = ?
A , ?
P with ?
A = ? a a?A and ?
P = ? ab a , b ?P .
Since all variables are binary , we may write , for each a ?
A , ? a ( 1 ) = z a and ?
a ( 0 ) = 1 ? z a , where z a is a variable constrained to [ 0 , 1 ] .
Let z A z a a?A ; the local agreement constraints at the TREE factor ( see Table 1 ) are written as z A ? Z tree ( x ) , where Z tree ( x ) is the arborescence polytope , i.e. , the convex hull of all incidence vectors of dependency trees ( Martins et al. , 2009 ) .
It is straightforward to write a contingency table and obtain the following local agreement constraints at the pairwise factors : ? ab ( 1 , 1 ) = z ab , ? ab ( 0 , 0 ) = 1 ? z a ? z b + z ab ? ab ( 1 , 0 ) = z a ?
z ab , ? ab ( 0 , 1 ) = z b ?
z ab .
Noting that all these pseudo-marginals are constrained to the unit interval , one can get rid of all variables ? ab and write everything as z a ? [ 0 , 1 ] , z b ? [ 0 , 1 ] , z ab ? [ 0 , 1 ] , z ab ?
z a , z ab ?
z b , z ab ?
z a + z b ? 1 , ( 12 ) inequalities which , along with z A ? Z tree ( x ) , define the local polytope L( G x ) .
As for the factor entropies , start by noting that the TREE - factor entropy H tree can be obtained in closed form by computing the marginals zA and the partition function Z x ( ? ) ( via the matrix - tree theorem ) and recalling the variational representation in Eq. 9 , yielding H tree = log Z x ( ? ) ? ? F ( x ) z A . Some algebra allows writing the overall Bethe entropy approximation as : H Bethe ( ? ) = H tree ( z A ) ? a , b ?P
I a;b ( z a , z b , z ab ) , ( 13 ) where we introduced the mutual information associated with each pairwise factor , 1 . I a;b ( z a , z b , z ab ) = FLOW ( 0 , m , k ) FLOW ( n , m , k ) PATH ( m , k ) PATH - BUILDER ( m , k ) FLOW ( h,1 , k ) FLOW ( h, n , k ) PATH ( h , k ) FLOW - DELTA ( h , k ) ARC ( h , m ) FLOW ( h , m , k ) FLOW -IMPLIES - ARC ( h , ya , y b ? ab ( y a , y b ) log ?
ab ( ya , y b ) ?a( ya ) ?
b ( y b ) .
The approximate variational expression becomes log Z x ( ? ) ? max z ? F ( x ) z + H tree ( z A ) ? a , b ?P
I a;b ( z a , z b , z ab ) s.t. z ab ?
z a , z ab ?
z b , z ab ?
z a + z b ?
1 , ? a , b ? P , z A ?
Z tree , ( 14 ) whose maximizer corresponds to the beliefs returned by the Smith and Eisner 's loopy BP algorithm ( if it converges ) .
Turbo Parser # 2 : LP - Relaxed MAP .
We now turn to the concise integer LP formulation of Martins et al . ( 2009 ) .
The formulation is exact but NPhard , and so an LP relaxation is made there by dropping the integer constraints .
We next construct a factor graph G x and show that the LP relaxation corresponds to an optimization of the form in Eq. 10 , with the marginal polytope M( G x ) replaced by L( G x ) .
G x includes the following auxiliary variable nodes : path variables p ij i=0 , ... , n , j=1 , ... , n , which indicate whether word j descends from i in the dependency tree , and flow variables f k a a ?A , k=1 , ... , n , which evaluate to 1 iff arc a " carries flow " to k , i.e. , iff there is a path from the root to k that passes through a .
We need to seed these variables imposing p 0 k = p kk = 1 , ?k , f h h , m = 0 , ?h , m ; ( 15 ) i.e. , any word descends from the root and from itself , and arcs leaving a word carry no flow to that word .
This can be done with unary hard constraint factors .
We then replace the TREE factor in Fig. 1 by the factors shown in Fig. 3 : ? O( n ) XOR factors , each connecting all arc variables of the form { h , m } h=0 , ... , n .
These ensure that each word has exactly one parent .
Each factor yields a local agreement constraint ( see Table 1 ) : n h=0 z h , m = 1 , m ? { 1 , . . . , n} ( 16 ) ? O( n 3 ) IMPLY factors , each expressing that if an arc carries flow , then that arc must be active .
Such factors are OR factors with the first input negated , hence , the local agreement constraints are : f k a ?
z a , a ?
A , k ? { 1 , . . . , n}. ( 17 ) ? O(n
In sum , although the approaches of Smith and Eisner ( 2008 ) and Martins et al . ( 2009 ) look very different , in reality both are variational approximations emanating from Prop .
1 , respectively for marginal and MAP inference .
However , they operate on distinct factor graphs , respectively Figs .
1 and 3 . 9 p mk = n h=0 f k h , m , m , k ? { 1 , . . . , n} ( 18 ) ? O( n 2 ) XOR -
Online Learning Our learning algorithm is presented in Alg .
1 . It is a generalized online learner that tackles 2 - regularized empirical risk minimization of the form min ? R d ? 2 ? 2 + 1 m m i=1 L ( ? ; x i , y i ) , ( 21 ) where each x i , y i is a training example , ? ? 0 is the regularization constant , and L ( ? ; x , y ) is a nonnegative convex loss .
Examples include the logistic loss used in CRFs ( ? log Pr ? ( y| x ) ) and the hinge loss of structured SVMs ( max y ?Y( x ) ? ( ?( x , y ) ? ?( x , y ) ) + ( y , y ) for some cost function ) .
These are both special cases of the family defined in Fig. 4 , which also includes the structured perceptron 's loss ( ? ? ? , ? = 0 ) and the softmax -margin loss of Gimpel and Smith ( 2010 ; ? = ? = 1 ) .
Alg.
1 is closely related to stochastic or online gradient descent methods , but with the key advantage of not needing a learning rate hyperparameter .
We sketch the derivation of Alg. 1 ; full details can be found in Martins et al . ( 2010 a ) .
On the tth round , one example x t , y t is considered .
We seek to solve min ? , ? ?m 2 ? ? ? t 2 + ? s.t. L ( ? ; x t , y t ) ? ? , ? ? 0 , ( 23 ) 9 Given what was just exposed , it seems appealing to try max -product loopy BP on the factor graph of Fig. 1 , or sumproduct loopy BP on the one in Fig.
3 . Both attempts present serious challenges : the former requires computing messages sent by the tree factor , which requires O(n 2 ) calls to the Chu-Liu-Edmonds algorithm and hence O( n 5 ) time .
No obvious strategy seems to exist for simultaneous computation of all messages , unlike in the sum-product case .
The latter is even more challenging , as standard sum-product loopy BP has serious issues in the factor graph of Fig. 3 ; we construct in Martins et al . ( 2010 b ) a simple example with a very poor Bethe approximation .
This might be fixed by using other variants of sum-product BP , e.g. , ones in which the entropy approximation is concave .
L ? , ? ( ? ; x , y ) 1 ? log y ?Y( x ) exp ? ? ?( x , y ) ? ?( x , y ) + ? ( y , y ) ( 22 ) Figure 4 : A family of loss functions including as particular cases the ones used in CRFs , structured SVMs , and the structured perceptron .
The hyperparameter ? is the analogue of the inverse temperature in a Gibbs distribution , while ? scales the cost .
For any choice of ? > 0 and ? ? 0 , the resulting loss function is convex in ? , since , up to a scale factor , it is the composition of the ( convex ) log-sum - exp function with an affine map .
Algorithm 1 Aggressive Online Learning 1 : Input : { x i , y i } m i=1 , ? , number of epochs K 2 : Initialize ? 1 ? 0 ; set T = mK 3 : for t = 1 to T do 4 : Receive instance x t , y t and set ?
t = ?( y t ) 5 : Solve Eq. 24 to obtain ?t and L ? , ? (?
t , x t , y t ) 6 : Compute ?L ? , ? (?
t , x t , y t ) = F( x t ) ( ?t ? t ) 7 : Compute ? t = min 1 ?m , L ? , ? ( ? t;xt , yt ) ?L ? , ? ( ? t;xt , yt ) 2 8 : Return ?
t+1 = ? t ? ? t ?L ? , ? (? t ; x t , y t ) 9 : end for 10 : Return the averaged model ? ? 1 T T t=1 ? t . which trades off conservativeness ( stay close to the most recent solution ? t ) and correctness ( keep the loss small ) .
Alg. 1's lines 7 - 8 are the result of taking the first-order Taylor approximation of L around ?
t , which yields the lower bound L ( ? ; x t , y t ) ? L (? t ; x t , y t ) + ( ? ? ? t ) ?L ( ?
t ; x t , y t ) , and plugging that linear approximation into the constraint of Eq. 23 , which gives a simple Euclidean projection problem ( with slack ) with a closed - form solution .
The online updating requires evaluating the loss and computing its gradient .
Both quantities can be computed using the variational expression in Prop.
1 , for any loss L ? , ? (? ; x , y ) in Fig. 4 . 10
Our only assumption is that the cost function ( y , y ) can be written as a sum over factor-local costs ; letting ? = ?( y ) and ? = ?( y ) , this implies ( y , y ) = p ? + q for some p and q which are constant with respect to ? . 11 Under this assumption , L ? , ? (? ; x , y ) becomes expressible in terms of the log-partition function of a distribution whose log-potentials are set to ?( F ( x ) ? + ?p ) .
From Eq. 9 and after some algebra , we finally obtain L ? , ? (? ; x , y ) = 10 Our description also applies to the ( non - differentiable ) hinge loss case , when ? ? ? , if we replace all instances of " the gradient " in the text by " a subgradient . "
11 For the Hamming cost , this holds with p = 1 ? 2 ? and q = 1 ?. See Taskar et al. ( 2006 ) for other examples .
max ? ?M( Gx ) ? F ( x ) ( ? ? ) + 1 ? H ( ? ) +?( p ? +q ) . ( 24 ) Let ? be a maximizer in Eq. 24 ; from the second statement of Prop .
1 we obtain ?L ? , ? (? ; x , y ) = F ( x ) ( ? ) .
When the inference problem in Eq. 24 is intractable , approximate message - passing algorithms like loopy BP still allow us to obtain approximations of the loss L ? , ? and its gradient .
For the hinge loss , we arrive precisely at the maxloss variant of 1 - best MIRA ( Crammer et al. , 2006 ) .
For the logistic loss , we arrive at a new online learning algorithm for CRFs that resembles stochastic gradient descent but with an automatic step size that follows from our variational representation .
Unsupported Features .
As datasets grow , so do the sets of features , creating further computational challenges .
Often only " supported " features - those observed in the training data- are included , and even those are commonly eliminated when their frequencies fall below a threshold .
Important information may be lost as a result of these expedient choices .
Formally , the supported feature set is F supp m i=1 supp ?( x i , y i ) , where supp u {j | u j = 0 } denotes the support of vector u .
F supp is a subset of the complete feature set , comprised of those features that occur in some candidate output , F comp m i=1 y i ?Y( x i ) supp ?( x i , y i ) .
Features in F comp \ F supp are called unsupported .
Sha and Pereira ( 2003 ) have shown that training a CRF - based shallow parser with the complete feature set may improve performance ( over the supported one ) , at the cost of 4.6 times more features .
Dependency parsing has a much higher ratio ( around 20 for bilexical word- word features , as estimated in the Penn Treebank ) , due to the quadratic or faster growth of the number of parts , of which only a few are active in a legal output .
We propose a simple strategy for handling F comp efficiently , which can be applied for those losses in Fig.
4 where ? = ?. ( e.g. , the structured SVM and perceptron ) .
Our procedure is the following : keep an active set F contain - ing all features that have been instantiated in Alg .
1 . At each round , run lines 4 - 5 as usual , using only features in F.
Since the other features have not been used before , they have a zero weight , hence can be ignored .
When ? = ? , the variational problem in Eq. 24 consists of a MAP computation and the solution corresponds to one output ?t ? Y( x t ) .
Only the parts that are active in ?t but not in y t , or vice-versa , will have features that might receive a nonzero update .
Those parts are reexamined for new features and the active set F is updated accordingly .
Experiments
We trained non-projective dependency parsers for 14 languages , using datasets from the CoNLL -X shared task ( Buchholz and Marsi , 2006 ) and two datasets for English : one from the CoNLL - 2008 shared task ( Surdeanu et al. , 2008 ) , which contains non-projective arcs , and another derived from the Penn Treebank applying the standard head rules of Yamada and Matsumoto ( 2003 ) , in which all parse trees are projective .
12 We implemented Alg. 1 , 12
We used the provided train / test splits for all datasets .
For English , we used the standard test partitions ( section 23 of the Wall Street Journal ) .
We did not exploit the fact that some datasets only contain projective trees and have unique roots .
which handles any loss function L ? , ? . 13
When ? < ? , Turbo Parser # 1 and the loopy BP algorithm of Smith and Eisner ( 2008 ) is used ; otherwise , Turbo Parser # 2 is used and the LP relaxation is solved with CPLEX .
In both cases , we employed the same pruning strategy as Martins et al . ( 2009 ) .
Two different feature configurations were first tried : an arc-factored model and a model with second-order features ( siblings and grandparents ) .
We used the same arc-factored features as McDonald et al . ( 2005 ) and second-order features that conjoin words and lemmas ( at most two ) , parts - ofspeech tags , and ( if available ) morphological information ; this was the same set of features as in Martins et al . ( 2009 ) .
Table 2 shows the results obtained in both configurations , for CRF and SVM loss functions .
While in the arc-factored case performance is similar , in second-order models there seems to be a consistent gain when the SVM loss is used .
There are two possible reasons : first , SVMs take the cost function into consideration ; second , Turbo Parser # 2 is less approximate than Turbo Parser # 1 , since only the marginal polytope is approximated ( the entropy function is not involved ) .
The loopy BP algorithm managed to converge for nearly all sentences ( with message damping ) .
The last three columns show the beneficial effect of unsupported features for the SVM case ( with a more powerful model with non-projectivity features ) .
For most languages , unsupported features convey helpful information , which can be used with little extra cost ( on average , 2.5 times more features are instantiated ) .
A combination of the techniques discussed here yields parsers that are in line with very strong competitors - for example , the parser of , which is exact , third - order , and constrains the outputs to be projective , does not outperform ours on the projective English dataset .
14 Finally ,
Table 3 shows results obtained for different settings of ? and ?.
Interestingly , we observe that higher scores are obtained for loss functions that are " between " SVMs and CRFs .
Related Work
There has been recent work studying efficient computation of messages in combinatorial factors : bipartite matchings ( Duchi et al. , 2007 ) , projective and non-projective arborescences ( Smith and Eisner , 2008 ) , as well as high order factors with countbased potentials ( Tarlow et al. , 2010 ) , among others .
Some of our combinatorial factors ( OR , OR -WITH - OUTPUT ) and the analogous entropy computations were never considered , to the best of our knowledge .
Prop .
1 appears in Wainwright and Jordan ( 2008 ) for canonical overcomplete models ; we adapt it here for models with shared features .
We rely on the variational interpretation of loopy BP , due to Yedidia et al . ( 2001 ) , to derive the objective being optimized by Smith and Eisner 's loopy BP parser .
Independently of our work , 14
This might be due to the fact that Koo and Collins ( 2010 ) trained with the perceptron algorithm and did not use unsupported features .
Experiments plugging the perceptron loss ( ? ? ? , ? ? 0 ) into Alg. 1 yielded worse performance than with the hinge loss .
recently proposed an efficient dual decomposition method to solve an LP problem similar ( but not equal ) to the one in Eq. 20 , 15 with excellent parsing performance .
Their parser is also an instance of a turbo parser since it relies on a local approximation of a marginal polytope .
While one can also use dual decomposition to address our MAP problem , the fact that our model does not decompose as nicely as the one in would likely result in slower convergence .
Conclusion
We presented a unified view of two recent approximate dependency parsers , by stating their underlying factor graphs and by deriving the variational problems that they address .
We introduced new hard constraint factors , along with formulae for their messages , local belief constraints , and entropies .
We provided an aggressive online algorithm for training the models with a broad family of losses .
There are several possible directions for future work .
Recent progress in message - passing algorithms yield " convexified "
Bethe approximations that can be used for marginal inference ( Wainwright et al. , 2005 ) , and provably convergent max-product variants that solve the relaxed LP ( Globerson and Jaakkola , 2008 ) .
Other parsing formalisms can be handled with the inventory of factors shown hereamong them , phrase-structure parsing .
Figure 2 : 2 Figure2 : Dual parametrization of the distributions in P x .
Our parameter space ( left ) is first linearly mapped to the space of factor log-potentials ( middle ) .
The latter is mapped to the marginal polytope M( G x ) ( right ) .
In general only a subset of M( G x ) is reachable from our parameter space .
Any distribution in P x can be parametrized by a vector ? ?
R d or by a point ? ? M( G x ) .
