title
SParse : Koc Â¸ University Graph - Based Parsing System for the CoNLL 2018 Shared Task
abstract
We present SParse , our Graph - Based Parsing model submitted for the CoNLL 2018 Shared Task : Multilingual Parsing from Raw Text to Universal Dependencies ( Zeman et al. , 2018 ) .
Our model extends the state- of - the - art biaffine parser ( Dozat and Manning , 2016 ) with a structural meta-learning module , SMeta , that combines local and global label predictions .
Our parser has been trained and run on Universal Dependencies datasets
Introduction End-to- end learning with neural networks has proven to be effective in parsing natural language ( Kiperwasser and Goldberg , 2016 ) .
Graph - based dependency parsers ( McDonald et al. , 2005 ) represent dependency scores between words as a matrix representing a weighted fully connected graph , from which a spanning tree algorithm extracts the best parse tree .
This setting is very compatible with neural network models that are good at producing matrices of continuous numbers .
Compared to transition - based parsing ( K?rnap et al. , 2017 ; Kiperwasser and Goldberg , 2016 ) , which was the basis of our university 's last year entry , graph - based parsers have the disadvantage of producing n 2 entries for parsing an n-word sentence .
Furthermore , algorithms used to parse these entries can be even more complex than O( n 2 ) .
However , graph- based parsers allow easyto- parallelize static architectures rather than sequential decision mechanisms and are able to parse non-projective sentences .
Non- projective graph - based parsing is the core of last year 's winning entry ( Dozat et al. , 2017 ) .
Neural graph - based parsers can be divided into two components : encoder and decoder .
The encoder is responsible for representing the sentence as a sequence of continuous feature vectors .
The decoder receives this sequence and produces the parse tree , by first creating a graph representation and then extracting the maximum spanning tree ( MST ) .
We use a bidirectional RNN ( bi- RNN ) to produce a contextual vector for each word in a sentence .
Use of bi-RNNs is the defacto standard in dependency parsing , as it allows representing each word conditioned on the whole sentence .
Our main contribution in the encoder part is to the word embeddings feeding the bi-RNN .
We use word vectors coming from a language model pretrained on very large language corpora , similar to K?rnap et al . ( 2017 ) .
We extend word embeddings with learnable embeddings for UPOS tags , XPOS tags and FEATs where applicable .
Our decoder can be viewed as a more structured version of the state - of - the - art biaffine decoder of Dozat et al . ( 2017 ) , where we attempt to condition the label - seeking units to a parsetree instead of simple local predictions .
We propose a meta-learning module that allows structured and unstructured predictions to be combined as a weighted sum .
This additional computational complexity is paid off by our simple word -level model in the encoder part .
We call it that we call structured meta-biaffine decoder or shortly SMeta .
We implemented our model using Knet deep learning framework ( Yuret , 2016 ) in Julia language ( Bezanson et al. , 2017 ) .
Our code will be made available publicly .
We could only get official results for two corpora due to an unexpected software bug .
Therefore , we present unofficial results obtained after the submission deadline as well .
2 Related work Kiperwasser and Goldberg ( 2016 ) use trainable BiLSTMs to represent features of each word , instead of defining the features manually .
They formulated the structured prediction using hinge loss based on the gold parse tree and parsed scores .
Dozat and Manning ( 2016 ) propose deep biaffine attention combined with the parsing model of Kiperwasser and Goldberg ( 2016 ) , which simplifies the architecture by allowing implementation with a single layer instead of two linear layers .
Stanford 's Graph - based Neural Dependency Parser ( Dozat et al. , 2017 ) at the CoNLL 2017 Shared Task ( Zeman et al. , 2017 ) is implemented with four ReLU layers , two layers for finding heads and dependents of each word , and two layers for finding the dependency relations for each head - dependent pair .
The outputs are then fed into two biaffine layers , one for determining the head of the word , and another for determining the dependency relation of head- dependent pair .
We propose a dependency parsing model based on the graph - based parser by Dozat and Manning ( 2016 ) .
We are adding a meta-biaffine decoder layer , similar to the tagging model proposed by Bohnet et al . ( 2018 ) , for computing the arc labels based on the full tree constructed from the unlabeled arc scores instead of computing them independently .
Our parsing model uses pretrained word embeddings from K?rnap et al . ( 2017 ) .
Our parser uses the same language model with K?rnap et al . ( 2017 ) , in which graph based - parsing algorithms are applied .
However , a transition - based parsing model is given in K?rnap et al . ( 2017 ) .
Therefore , some adaptations are made on the features proposed by K?rnap et al . ( 2017 ) in order to use them in a graph based parsing model .
We did not use contextual features coming from the language model or features related to words in stack and buffer .
Instead , we trained a three - layer BiLSTM from scratch to encode contextual features .
Model
In this section , we depict important aspects of our architecture which is shown in Figure 1 .
We discuss encoder and decoder separately and then give the model hyper-parameters used .
Encoder Word Model
We used four main features to represent each word in a sentence : a pre-trained word embedding , UPOS tag embedding , XPOS tag embedding and FEAT embedding .
Pre-trained words come from the language model in K?rnap et al . ( 2017 ) .
This model represents each word using a character - level LSTM , which is a suitable setting for morphologically rich languages , as shown in Dozat et al . ( 2017 ) .
We use the word vectors without further training .
UPOS and XPOS tag embeddings are represented by vectors randomly initialized using unit Gaussian distribution .
Morphological features , also called FEATs , are different in the sense that there are zero or more FEATs for each word .
We follow a simple strategy : we represent each FEAT using a randomly initialized vector and add all FEAT embeddings for each word .
We simply used zero for word vectors without any morphological features .
For practical reasons , we also needed to represent ROOT word of a sentence .
We do so by randomly initializing a word embedding and setting all other embeddings to zero .
At test time , we used tags and morphological features produced by MorphNet ( Dayan ? k et al. , 2018 ) .
For languages where this model is not available , we directly used UDPipe results ( Straka et al. , 2016 ) .
Sentence Model
We used a three - layer bidirectional LSTM to represent a sentence .
We used the hidden size of 200 for both forward and backward LSTMs .
Dropout ( Srivastava et al. , 2014 ) is performed at the input of each LSTM layer , including the first layer .
Our LSTM simply use nth hidden state for nth word , different from the language model in K?rnap et al . ( 2017 ) .
The language model discussed in the previous section also provides context embeddings .
We performed experiments for combining our own contextual representation with this representation using various concatenation and addition strategies , but we observed poorer performance in terms of generalization .
Also , using the language model directly as a feature extractor led to unsatisfactory performance , different from last years ' transitionbased entry of our institution ( K?rnap et al. , 2017 ) .
Decoder Structured Meta-Biaffine Decoder ( SMeta )
Deep biaffine decoder ( Dozat and Manning , 2016 ) is the core of last year 's winning entry ( Dozat et al. , 2017 ) , so we used this module as our starting point .
Biaffine architecture is computationally efficient and can be used with easy - to-train softmax objective , different from harder - to-optimize hinge loss objectives as in Kiperwasser and Goldberg ( 2016 ) .
Similar to ( Dozat et al. , 2017 ) , we produce four different hidden vectors , two for arcs and two for relations ( or labels ) .
Formally ( 1 ) h ( arc?dep ) i = MLP ( arc?dep ) ( h i ) ( 2 ) h ( arc?head ) i = MLP ( arc?head ) ( h i ) ( 3 ) h ( rel?dep ) i = MLP ( rel?dep ) ( h i ) ( 4 ) h ( rel?head ) i = MLP ( rel?head ) ( h i ) where h i represents ith hidden state of the bi-LSTM embedding .
The vectors correspond to arcs seeking their dependents , arcs seeking their heads , and corresponding relations .
MLP can be any neural network module .
Here , we simply use dense layers followed by ReLU activations , as in ( Dozat et al. , 2017 ) .
Now , we perform the biaffine transformation to compute the score matrix representing the graph , ( 5 ) s ( arc ) i = H ( arc?head ) W ( arc ) h i + H ( arc?head ) b T( arc ) where H ( arc?head ) represents matrix of h ( arc?head ) i vectors , W ( arc ) and b ( arc ) are learnable weights .
Up to this point , our decoder is identical to the one in ( Dozat et al. , 2017 ) .
The difference is in the computation of predicted arcs .
We compute two different predictions : ( 6 ) y l( arc ) i = arg max j s ( arc ) ij ( 7 ) p = parse ( S ( arc ) ) ( 8 ) y s( arc ) i = p i Here S ( arc ) is the matrix of arc scores and parse is a spanning tree algorithm that computes the indices of the predicted arcs .
Now , we compute label scores using these two predictions .
First , we compute coefficient vector k using the bi-RNN encodings , ( 9 ) h = n i=1 h i n ( 10 ) k = W ( meta ) h + b ( meta ) where n is the number of words in the sentence , W and b are learned parameters .
Averaging over time is inspired by the global average pooling operator in the vision literature ( Lin et al. , 2013 ) , transforming temporal representation to a global one .
We now compute the weighted sum of label predictions using coefficient vector k. s l( rel ) i = h T( rel?head ) y l( arc ) i U ( rel ) h ( rel?dep ) i + W ( rel ) cat ( h ( rel?dep ) i , h ( rel?head ) y l( arc ) i ) + b ( rel ) ( 11 ) s s(rel ) i = h T( rel?head ) y s( arc ) i U ( rel ) h ( rel?dep ) i + W ( rel ) cat ( h ( rel?dep ) i , h ( rel?head ) y s(arc ) i ) + b ( rel ) ( 12 ) ( 13 ) s ( rel ) i = k 1 s l( rel ) i + k 2 s s(rel ) i ( 14 ) y ( rel ) i = arg max j s ( rel ) ij where U ( rel ) , W ( rel ) and b ( rel ) are learned parameters .
Our model is trained using sum of softmax losses similar to ( Dozat et al. , 2017 ) .
Parsing algorithms
In our parsing model , Chu-Liu-Edmonds algorithm ( Chu , 1965 ; Edmonds , 1967 ) and Eisner ( 1996 ) 's algorithm are used interchangeably , during both the training of parser models and parsing phase of test datasets .
On the languages whose training dataset consists of more than 250,000 words , Chu-Liu-Edmonds algorithm is used for parsing since it has a complexity of O(n 2 ) , where n is the number of words .
This approach allows us to train our models on relatively larger datasets in less amount of time , compared to the Eisner 's algorithm whose time complexity is O( n 3 ) .
On training datasets having at most 250,000 words , Eisner 's algorithm is used during both training and parsing phase .
Eisners algorithm produces only projective trees and Chu-Liu-Edmonds algorithm produces both projective and non-projective trees .
This means the number of possible trees Eisner 's algorithm can generate is fewer compared to Chu-Liu-Edmonds algorithm , so even though Eisner 's algorithm has higher time complexity than Chu-Liu-Edmonds algorithm , parsing models are trained faster when Eisner 's algorithm is used .
Hyperparameters
We used a 150 - dimensional tag and feature embeddings and 350 - dimensional word embeddings for the word model .
Bi-RNN sentence model has the hidden size of 200 for both forward and backward RNNs , producing 400 - dimensional feature context vectors .
We used the hidden size of 400 for arc MLPs and 100 for relation MLPs .
Training
We used Adam optimizer ( Kingma and Ba , 2014 ) with its standard parameters .
Based on dataset size , we trained the model for 25 to 100 epochs and selected the model based on its validation labeled attachment accuracy .
We sampled sentences with identical number of words in a minibatch .
In training corpora that are sufficiently large , we sampled minibatches so that approximately 500 tokens exist in a single minibatch .
We reduced this size to 250 for relatively small corpora .
For very small corpora , we simply sample a constant number of sentences as a minibatch .
( Potthast et al. , 2014 ) machine allocated for the task .
Dataset
We saved best models with corresponding configurations , scores and optimization states during training for recovery .
We then re-create the model files for the best models of each corpus by removing the optimization states .
MorphNet is the morphological analysis and disambiguation tool proposed by Dayan ? k et al. ( 2018 ) , which we used while training our parsing model .
During training of the parser , CoNLL -U formatted training dataset files , which are produced by UDPipe ( Straka et al. , 2016 ) , are given to MorphNet as input .
Then , MorphNet applies its own morphological analysis and disambiguation , and new CoNLL -U formatted files produced by MorphNet are used by our parser .
Even though we used lemmatized and tagged outputs generated by MorphNet while training our parser , we run our parser on outputs generated by UDPipe , due to time constrains during parsing .
Results and Discussion
Short after the testing period ended , our parser obtained results on 64 treebank test sets out of 82 , which are shown in Table 1 .
According to the results announced including the unofficial runs , we had an average LAS score of 57 % on the 64 test sets on which our model is run and ranked 24th among the best runs of 27 teams .
The MLAS score of our model is 46.40 % and our model is ranked 22nd out of the submissions of 27 models .
And , the BLEX score of our model is 49.17 % and our model is ranked 21st out of the best BLEX results of all 27 models including unofficial runs .
1 According to the results , our model performs better at datasets with comparably larger training data .
For instance , our model has around 90 % LAS score on Catalan , Indian , Italian , Polish and Russian languages which have higher number of tokens in training data .
Furthermore , our model performs relatively well in some low-resource languages like Turkish and Hungarian .
However , on the datasets with very small or no training data , such as Japanese Modern , Russian Taiga and Irish IDT , we get lower scores .
Hence , our model benefits from large amount of data during training process , but prediction with low resources remains as an issue for our model .
1 Best results of each team including unofficial runs are announced in http://universaldependencies.org/conll18/ results-best-all.html
Our results and rankings announced in the paper are taken from the CoNLL 2018 best results webpage in September 2 , 2018 and may change with the inclusion of new results of participated teams later .
Contributions
In this work , we proposed a new decoding mechanism , called SMeta , for graph - based neural dependency parsing .
This architecture attempts to combine structured and unstructured prediction methods using meta-learning .
We coupled this architecture with custom training methods and algorithms to evaluate its performance .
Figure 1 : 1 Figure 1 : Overall model architecture .
Table 1 : 1 Our unofficial F1 scores .
Tests are done in TIRA LAS MLAS BLEX Dataset LAS MLAS BLEX ar padt 67.57 % 56.22 % 58.84 % hu szeged 73.84 % 56.03 % 63.31 % bg btb 85.85 % 76.28 % 74.73 % id gsd 76.88 % 65.34 % 64.89 % ca ancora 87.60 % 79.05 % 79.49 % it isdt 87.51 % 78.81 % 78.75 % cs cac 86.05 % 73.54 % 80.14 % it postwita 69.48 % 56.20 % 56.73 % cs fictree 84.21 % 71 % 76.86 % ja modern 22.91 % 8.47 % 9.73 % cs pdt 86.07 % 76.54 % 81.39 % ko gsd 75.52 % 69.15 % 63.18 % cs pud 81.80 % 68.19 % 75.10 % ko kaist 81.64 % 74.46 % 68.86 % cu proiel 67.80 % 56.51 % 60.93 % la ittb 77.66 % 68.09 % 73.91 % da ddt 77.90 % 68.14 % 68.64 % la perseus 47.45 % 29.99 % 32.47 % de gsd 70.65 % 34.50 % 60.40 % la proiel 64.29 % 51.80 % 58.05 % el gdt 83.65 % 66.76 % 70.14 % lv lvtb 71.52 % 57.26 % 60.20 % en ewt 77.87 % 68.57 % 70.83 % nl alpino 78.28 % 63.38 % 66.08 % en gum 75.51 % 63.95 % 63.52 % nl lassysmall 78.10 % 65.73 % 66.77 % en lines 73.97 % 65.15 % 66.06 % pl lfg 88.21 % 75.40 % 79.25 % en pud 81.31 % 69.85 % 73.18 % pl sz 83.01 % 65.10 % 73.34 % es ancora 86.94 % 79.14 % 79.58 % pt bosque 84.32 % 70.27 % 75.29 % et edt 77.45 % 69.58 % 66.05 % ro rrt 82.74 % 74.19 % 74.60 % eu bdt 73.84 % 60.49 % 66.38 % ru syntagrus 88.22 % 80.16 % 81.05 % fa seraji 81.74 % 75.15 % 71.93 % ru taiga 40.01 % 23.92 % 25.44 % fi ftb 76.62 % 66.28 % 62.88 % sk snk 77.81 % 56.11 % 62.23 % fi tdt 79.30 % 71.07 % 64.37 % sl ssj 78.46 % 64.93 % 70.50 % fr gsd 82.32 % 72.91 % 74.97 % sl sst 43.63 % 31.14 % 35.41 % fr sequoia 82.60 % 73.07 % 76.07 % sv lines 74.87 % 59.32 % 67.25 % fr spoken 64.62 % 53.15 % 54.18 % sv pud 71.66 % 43.65 % 55.60 % ga idt 35.84 % 13.07 % 16.77 % sv talbanken 79.06 % 70.55 % 71.16 % gl ctg 80.31 % 67.77 % 70.72 % tr imst 60.19 % 49.57 % 51.02 % got proiel 62.40 % 49.19 % 55.15 % ug udt 58.48 % 38.18 % 45.94 % grc perseus 61.82 % 34.17 % 41.16 % uk iu 75.93 % 57.81 % 64.84 % grc proiel 68.03 % 49.40 % 55.62 % ur udtb 78.41 % 51.28 % 64.94 % he htb 60.09 % 46.32 % 49.19 % vi vtb 41.27 % 34.73 % 36.95 % hi hdtb 87.66 % 70 % 80.47 % zh gsd 59.51 % 49.33 % 54.33 % hr set 80.86 % 60.65 % 72.36 %
