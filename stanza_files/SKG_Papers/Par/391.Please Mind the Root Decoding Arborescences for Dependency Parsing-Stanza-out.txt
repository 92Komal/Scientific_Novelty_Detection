title
Please Mind the Root : Decoding Arborescences for Dependency Parsing
abstract
The connection between dependency trees and spanning trees is exploited by the NLP community to train and to decode graph - based dependency parsers .
However , the NLP literature has missed an important difference between the two structures : only one edge may emanate from the root in a dependency tree .
We analyzed the output of state - of - the - art parsers on many languages from the Universal Dependency Treebank : although these parsers are often able to learn that trees which violate the constraint should be assigned lower probabilities , their ability to do so unsurprisingly degrades as the size of the training set decreases .
In fact , the worst constraint -violation rate we observe is 24 % .
Prior work has proposed an inefficient algorithm to enforce the constraint , which adds a factor of n to the decoding runtime .
We adapt an algorithm due to Gabow and Tarjan ( 1984 ) to dependency parsing , which satisfies the constraint without compromising the original runtime .
1
Introduction
Developing probabilistic models of dependency trees requires efficient exploration over a set of possible dependency trees , which grows exponentially with the length of the input sentence n. Under an edge-factored model ( McDonald et al. , 2005 ; Ma and Hovy , 2017 ; , finding the maximum-a- posteriori dependency tree is equivalent to finding the maximum weight spanning tree in a weighted directed graph .
More precisely , spanning trees in directed graphs are known as arborescences .
The maximum-weight arborescence can be found in O( n 2 ) ( Tarjan , 1977 ; Camerini et al. , 1979
However , an oversight in the relationship between dependency trees and arborescences has gone largely unnoticed in the dependency parsing literature .
Most dependency annotation standards enforce a root constraint :
Exactly one edge may emanate from the root node .
3 For example , the Universal Dependency Treebank ( UD ; Nivre et al . ( 2018 ) ) , a large-scale multilingual syntactic annotation effort , states in their documentation ( UD Contributors ) :
There should be just one node with the root dependency relation in every tree .
This oversight implies that parsers may return malformed dependency trees .
Indeed , we examined the output of a state - of - the - art parser ( Qi et al. , 2020 ) for 63 UD treebanks .
We saw that decoding without a root constraint resulted in 1.80 % ( on average ) of the decoded dependency trees being malformed .
This increased to 6.21 % on languages that contain less than one thousand training instances with the worst case of 24 % on Kurmanji .
The NLP literature has proposed two solutions to enforce the root constraint : ( 1 ) Allow invalid dependency trees-hoping that the model can learn to assign them low probabilities and decode singly rooted trees , or ( 2 ) return the best of n runs of the CLE each with a fixed edge emanating from the root .
4
The first solution is clearly problematic as it may allow parsers to predict malformed dependency trees .
This issue is further swept under the rug with " forgiving " evaluation metrics , such as attachment scores , which give ( 2005 ) ) opt for the simpler CLE algorithm ( Chu and Liu , 1965 ; Bock , 1971 ; Edmonds , 1967 ) , which has a worst- case bound of O(n 3 ) , but is often fast in practice .
3 A notable exception is the Prague Dependency Treebank ( Bej?ek et al. , 2013 ) , which allows for multi-rooted trees .
root Someplace that is like $ 30 an entree Figure 1 : A malformed dependency tree from our experiment .
Shown are the incorrect ( highlighted ) and correct ( highlighted ) dependency relations for token 8 . partial credit for malformed output .
5
The second solution , while correct , adds an unnecessary factor of n to the runtime of root-constrained decoding .
In this paper , we identify a much more efficient solution than ( 2 ) .
We do so by unearthing an O(n 2 ) algorithm due to Gabow and Tarjan ( 1984 ) from the theoretical computer science literature .
This algorithm appears to have gone unnoticed in NLP literature ; 6 we adapt the algorithm to correctly and efficiently handle the root constraint during decoding in edge-factored non-projective dependency parsing .
7
Approach
In this section , the marker indicates that a recently introduced concept is illustrated the worked example in Fig.
2 . Let G = ( ? , V , E ) be a rooted weighted directed graph where V is a set of nodes , E is a set of weighted edges , E ? {( i w ? A j) | i , j ?
V , w ?
R} , 8 and ? ?
V is a designated root node with no incoming edges .
In terms of dependency parsing , each non -?
node corresponds to a token in the sentence , and ? represents the special root token that is not a token in the sentence .
Edges represent possible dependency relations between tokens .
The edge weights are scores from a model ( e.g. , linear ( McDonald et al. , 2005 ) , or neural network ) .
Fig. 1 shows an example .
We allow G to be a multigraph , i.e. , we allow multiple edges between pairs of nodes .
Multi-graphs are a natural encoding of labeled dependency relations where possible labels between words are captured by multiple edges be - 5
We note exact match metrics , which consider the entire arborescence , do penalize root constraint violations 6
There is one exception : Corro et al . ( 2016 ) mention Gabow and Tarjan ( 1984 ) 's algorithm in a footnote .
7
Much like this paper , efficient root-constrained marginal inference is also possible without picking up an extra factor of n , but it requires some attention to detail ( Koo et al. , 2007 ; Zmigrod et al. , 2020 ) . 8
When there is no ambiguity , we may abuse notation using G to refer to either its node or edge set , e.g. , we may write ( i ? A j ) ?
G to mean ( i ? A j ) ?
E , and i ?
G to mean i ?
V . tween nodes in the graph .
Multi-graphs pose no difficulty as only the highest - weight edge between two nodes may be selected in the returned tree .
An arborescence of G is a subgraph A = ( ? , V , E ) where E ?
E such that : ( C2 ) A has no cycles .
A dependency tree of G is an arborescence that additionally satisfies ( C3 ) |{(? ? A ) ? E }| = 1 In words , ( C3 ) says A contains exactly one out-edge from ?. Let A ( G ) and A ? ( G ) denote the sets of arborescences and dependency trees , respectively .
The weight of a graph or subgraph is defined as w ( G ) def = ( i w ? Aj ) ?G w ( 1 ) In ?2.1 , we describe an efficient algorithm for finding the best ( highest- weight ) arborescence G * = argmax A?A ( G ) w ( A ) ( 2 ) and , in ?2.2 , the best dependency tree .
9 G ? = argmax A?A ? ( G ) w ( A ) ( 3 )
Finding the best arborescence A first stab at finding G * would be to select the best ( non-self-loop ) incoming edge for each node .
Although , this satisfies ( C1 ) , it does not ( necessarily ) satisfy ( C2 ) .
We call this subgraph the greedy graph , denoted ?
A G . Clearly , w ( ? A G ) ? w( G * ) since it is subject to fewer restrictions .
Further - more , if ?
A G happens to be acyclic , it is clearly equal to G * .
What are we to do in the event of a cycle ?
That answer has two parts .
Part 1 : We call any cycle C in ?
A G a critical cycle .
Naturally , ( C2 ) implies that critical cycles can never be part of an arborescence .
However , they help us identify optimal arborescences for certain subproblems .
Specifically , if we were to " break " the cycle at any node j ?
C by removing its ( unique ) incoming edge , we would have an opti- ) and so we get ( G / C ) ? ( highlighted ) .
Step ( d ) stitches ( G / C ) ? C ( 3 ) yielding G ? ( highlighted ) .
mal arborescence rooted at j for the subgraph over the nodes in C. Let C ( j ) be a subgraph of C rooted at j that denotes the broken cycle at j. Let G j ) ) .
Akin to dynamic programming , this choice edge weight ( due to Georgiadis ( 2003 ) ) gives the best " cost- to - go " for breaking the cycle at j. ? A j) ? G ? enter : if i / ? C , j ? C , then ( i w ? A c ) ?
G / C where w = w + w( C ( ? exit : if i ?
C , j / ? C , then ( c w ? A j ) ? G /C ? external : if i / ? C , j / ? C , then ( i w ? A j ) ?
G /C ? dead : if i ?
C , j ?
C , then no edge related to ( i w ? A j ) is in G /C .
This is because such an edge ( c ? A c ) would be a self-cycle , which can never be part of an arborescence .
Additionally , we define a bookkeeping function , ? , which maps the nodes and edges of G / C to their counterparts in G .
We overload ?( G ) to apply point-wise to the constituent nodes and edges .
By ( C1 ) , we have that for any A C ? A ( G / C ) , there exists exactly one incoming edge ( i ? A c ) to the cycle node c.
We can use ? to infer where the cycle was broken with ?( i ? A c ) = ( i ? A j ) .
We call j the entrance site of A C .
Consequently , we can stitch together an arborescence as ?( A C ) ? C ( j ) .
We use the shorthand A C C ( j ) for this operation due to its visual similarity to unraveling a cycle .
G / C may also have a critical cycle , so we have to apply this reasoning recursively .
This is captured by Karp ( 1971 )
Theorem 1 suggests a recursive strategy for finding G * , which is the basis of many efficient algorithms ( Tarjan , 1977 ; Camerini et al. , 1979 ; Georgiadis , 2003 ; Chu and Liu , 1965 ; Bock , 1971 ; Edmonds , 1967 ) .
We detail one such algorithm in Alg 1 .
Alg 1 can be made to run in O(n 2 ) time for dense with the appropriate implementation choices , such as Union - Find ( Hopcroft and Ullman , 1973 ) to maintain membership of nodes to contracted nodes , as well as radix sort ( Knuth , 1973 ) to sort incoming edges to contracted nodes ; using a regular sort would add a factor of log n to the runtime .
Algorithm 1 j) where j is the entrance site of ( G / C ) ? . 1 : def opt ( G ) : Find G * ? A ( G ) or G ? ? A ? ( G ) 2 : if ?
A G has G ? = ( G /C ) ? C ( Theorem 2 suggests a recursive strategy constrain ( Alg 1 ) for finding G ? given G * . Gabow and Tarjan ( 1984 , Theorem 7.1 ) prove that such a strategy will execute in O(n 2 ) and so when combined with opt ( G ) ( Alg 1 ) leads to a O(n 2 ) runtime for finding G ? given a graph G .
The efficiency of the algorithm amounts to requiring a bound of O(n ) calls to constrain that will lead to the reduction case in order to obtain any number optimization cases .
Each recursive call does a linear amount of work to search for the edge to remove and to stitch together the results of recursion .
Rather than computing the greedy graph from scratch , implementations should exploit that each edge removal will only change one element of the greedy graph .
Thus , we can find w( ? ? A G\ \e ) in constant time .
Experiment
How often do state - of - the - art parsers generate malformed dependency trees ?
We examined 63 Universal Dependency Treebanks ( Nivre et al. , 2018 ) and computed the rate of malformed trees when decoding using edge weights generated by pre-trained models supplied by Qi et al . ( 2020 ) .
On average , we observed that 1.80 % of trees are malformed .
We were surprised to see that - although the edgefactored model used is not expressive enough to capture the root constraint exactly - there are useful correlates of the root constraint in the surface form of the sentence , which the model appears to use to workaround this limitation .
This becomes further evident when we examine the relative change 12 in UAS ( 0.0083 % ) and exact match scores ( 0.60 % ) when using the constrained algorithm as opposed to the unconstrained algorithm .
Nevertheless , given less data , it is harder to learn to exploit the surface correlates ; thus , we see an increasing average rate of violation , 6.21 % , when examining languages with training set sizes of less than 1 , 000 sentences .
Similarly , the relative change in UAS and exact match score increases to 0.0368 % and 2.91 % respectively .
Indeed , the worst violation rate was 24 % was seen for Kurmanji which only contains 20 sentences in the training set .
Kurmanji consequently had the largest relative changes to both UAS and exact match scores of 0.41 % and 22.22 % .
We break down the malformed rate and accuracy changes by training size in Tab .
1 . Furthermore , the correlation between training size and malformed tree rate can be seen in Fig .
3 while the correlation between training size and relative accuracy change can be seen in Fig.
4 . We provide a full table of the results in App. C .
Conclusion
In this paper , we have bridged the gap between the graph - theory and dependency parsing literature .
We presented an efficient O(n 2 ) for finding the maximum arborescence of a graph .
Furthermore , we highlighted an important distinction between dependency trees and arborescences , namely that dependency trees are arborescences subject to a root constraint .
Previous work uses inefficient algorithms to enforce this constraint .
We provide a solution which runs in O( n 2 ) .
Our hope is that this paper will remind future research in dependency parsing to please mind the root .
