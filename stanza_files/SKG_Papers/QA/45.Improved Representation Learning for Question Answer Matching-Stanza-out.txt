title
Improved Representation Learning for Question Answer Matching
abstract
Passage - level question answer matching is a challenging task since it requires effective representations that capture the complex semantic relations between questions and answers .
In this work , we propose a series of deep learning models to address passage answer selection .
To match passage answers to questions accommodating their complex semantic relations , unlike most previous work that utilizes a single deep learning structure , we develop hybrid models that process the text using both convolutional and recurrent neural networks , combining the merits on extracting linguistic information from both structures .
Additionally , we also develop a simple but effective attention mechanism for the purpose of constructing better answer representations according to the input question , which is imperative for better modeling long answer sequences .
The results on two public benchmark datasets , InsuranceQA and TREC - QA , show that our proposed models outperform a variety of strong baselines .
Introduction Passage - level answer selection is one of the essential components in typical question answering ( QA ) systems .
It can be defined as follows :
Given a question and a pool of candidate passages , select the passages that contain the correct answer .
The performance of the passage selection task is not only crucial to non-factoid QA systems , where a question is expected to be answered with a sequence of descriptive text ( e.g. the question in Table 1 ) , but also very important to factoid QA systems , where the answer passage selection step is Question : Does Medicare cover my spouse ?
Ground - truth answer :
If your spouse has worked and paid Medicare taxes for the entire required 40 quarters , or is eligible for Medicare by virtue of being disabled or some other reason , your spouse can receive his / her own medicare benefits .
If your spouse has not met those qualifications , if you have met them , and if your spouse is age 65 , he / she can receive Medicare based on your eligibility .
Another candidate answer :
If you were married to a Medicare eligible spouse for at least 10 years , you may qualify for Medicare .
If you are widowed , and have not remarried , and you were married to your spouse at least 9 months before your spouse 's death , you may be eligible for Medicare benefits under a spouse provision .
Table 1 : An example of a question with the ground -truth answer and a negative answer extracted from the InsuranceQA dataset .
also known as passage scoring .
In factoid QA , if the sentences selected by the passage scorer module do not contain the answer , it will definitely lead to an incorrect response from the QA system .
One central challenge of this task lies in the complex and versatile semantic relations observed between questions and passage answers .
For example , while the task of supporting passage selection for factoid QA may be largely cast as a textual entailment problem , what makes an answer better than another in the real world for non-factoid QA often depends on many factors .
Specifically , different from many other pairmatching NLP tasks , the linguistic similarities between questions and answers may or may not be indicative for our task .
This is because , depending on what the question is looking for , a good answer may come in different forms : sometimes a correct answer completes the question precisely with the missing information , and in other scenarios , good answers need to elaborate part of the question to rationalize it , and so on .
For instance , the question in Table 1 only contains five words , while the best answer uses 60 words for elaboration .
On the other hand , the best answers from a pool can also be noisy and include extraneous information irrelevant to the question .
Additionally , while a good answer must relate to the question , they often do not share common lexical units .
For instance , in the example question , " cover " is not directly mentioned in the answer .
This issue may confuse simple word-matching systems .
These challenges consequently make handcrafting features much less desirable compared to deep learning based methods .
Furthermore , they also require our systems to learn how to distinguish useful pieces from irrelevant ones , and further , to focus more on the former .
Finally , the system should be capable of capturing the nuances between the best answer and an acceptable one .
For example , the second answer in Table 1 is suitable for a questioner , whose spouse is Medicare eligible , asking about his / her own coverage , while the example question is more likely asked by a person , who is Medicare eligible , asking about his / her spouse ' coverage .
Clearly , the first answer is more appropriate for the question , although the second one implicitly answers it .
A good system should reflect this preference .
While this task is usually approached as a pairwise - ranking problem , the best strategy to capture the association between the questions and answers is still an open problem .
Established approaches normally suffer from two weaknesses at this point .
First , prior work , such as ( Feng et al. , 2015 ; Wang and Nyberg , 2015 ) , resort to either convolutional neural network ( CNN ) or recurrent neural network ( RNN ) respectively .
However , each structure describes only one semantic perspective of the text .
CNN emphasizes the local interaction within n-gram , while RNN is designed to capture long range information and forget unimportant local information .
How to combine the merits from both has not been sufficiently explored .
Secondly , previous approaches are usually based on independently generated question and answer embeddings ; the quality of such representations , however , usually degrades as the answer sequences grow longer .
In this work , we propose a series of deep learning models in order to address such weaknesses .
We start with the basic discriminative framework for answer selection .
We first propose two independent models , Convolutional - pooling LSTM and Convolution - based LSTM , which are designed to benefit from both of the two popular deep learning structures to distinguish better between useful and irrelevant pieces presented in questions and answers .
Next , by breaking the independence assumption of the question and answer embedding , we introduce an effective attention mechanism to generate answer representations according to the question , such that the embeddings do not overlook informative parts of the answers .
We report experimental results for two answer selection datasets : ( 1 ) InsuranceQA ( Feng et al. , 2015 ) 1 , a recently released large-scale nonfactoid QA dataset from the insurance domain , and ( 2 ) TREC - QA 2 , which was created by Wang et al . ( 2007 ) based on Text REtrieval Conference ( TREC ) QA track data .
The contribution of this paper is hence threefold : 1 ) We propose hybrid neural networks , which learn better representations for both questions and answers by combining merits of both RNN and CNN .
2 ) We prove the effectiveness of attention on the answer selection task , which has not been sufficiently explored in prior work .
3 ) We achieve the state- of- the - art results on both TREC - QA and InsuranceQA datasets .
The rest of the paper is organized as follows :
Section 2 describes the related work for answer selection ; Section 3 provides the details of the proposed models ; Experimental settings and results are discussed in Section 4 and 5 ; Finally , we draw conclusions in Section 6 .
Related work Previous work on answer selection normally used feature engineering , linguistic tools , or external resources .
For example , semantic features were constructed based on WordNet in ( Yih et al. , 2013 ) .
This model pairs semantically related words based on word semantic relations .
In ( Wang and Manning , 2010 ; Wang et al. , 2007 ) , the answer selection problem was transformed to a syntacti-cal matching between the question / answer parse trees .
Some work tried to fulfill the matching using minimal edit sequences between dependency parse trees ( Heilman and Smith , 2010 ; Yao et al. , 2013 ) .
Discriminative tree-edit feature extraction and engineering over parsing trees were automated in ( Severyn and Moschitti , 2013 ) .
Such methods might suffer from the availability of additional resources , the effort of feature engineering and the systematic complexity introduced by the linguistic tools , such as parse trees and dependency trees .
Some recent work has used deep learning methods for the passage - level answer selection task .
The approaches normally pursue the solution on the following directions .
First , a joint feature vector is constructed based on both the question and the answer , and then the task can be converted into a classification or ranking problem ( Wang and Nyberg , 2015 ; Hu et al. , 2014 ) .
Second , recently proposed models for text generation can intrinsically be used for answer selection and generation ( Bahdanau et al. , 2015 ; Vinyals and Le , 2015 ) .
Finally , the question and answer representations can be learned and then matched by certain similarity metrics ( Feng et al. , 2015 ;
Yu et al. , 2014 ; dos Santos et al. , 2015 ; Qiu and Huang , 2015 ) .
Fundamentally , our proposed models belong to the last category .
Meanwhile , attention - based systems have shown very promising results on a variety of NLP tasks , such as machine translation ( Bahdanau et al. , 2015 ; Sutskever et al. , 2014 ) , machine reading comprehension ( Hermann et al. , 2015 ) , text summarization ( Rush et al. , 2015 ) and text entailment ( Rockt ?
schel et al. , 2016 ) .
Such models learn to focus their attention to specific parts of their input and most of them are based on a one - way attention , in which the attention is basically performed merely over one type of input based on another ( e.g. over target languages based on the source languages for machine translation , or over documents according to queries for reading comprehension ) .
Most recently , several two -way attention mechanisms are proposed , where the information from the two input items can influence the computation of each others representations .
Rockt? schel et al. ( 2016 ) develop a two -way attention mechanism including another one - way attention over the premise conditioned on the hypothesis , in addition to the one over hypothesis conditioned on premise .
dos Santos et al. ( 2016 ) and Yin et al . ( 2015 ) generate interactive attention weights on both inputs by assignment matrices .
Yin et al. ( 2015 ) use a simple Euclidean distance to compute the interdependence between the two input texts , while dos Santos et al . ( 2016 ) resort to attentive parameter matrices .
Approaches
In this section , we first present our basic discriminative framework for answer selection based on long short - term memory ( LSTM ) , which we call QA - LSTM .
Next , we detail the proposed hybrid and attentive neural networks that are built on top of the QA - LSTM framework .
LSTM for Answer Selection Our LSTM implementation is similar to the one in ( Graves et al. , 2013 ) with minor modifications .
Given an input sequence X = { x ( 1 ) , x( 2 ) , ? ? ? , x( n ) } , where x( t ) is an Edimension word vector in this paper , the hidden vector h( t ) ( with size H ) at the time step t is updated as follows .
i t = ?( W i x( t ) + U i h( t ? 1 ) + b i ) ( 1 ) f t = ?( W f x( t ) + U f h( t ? 1 ) + b f ) ( 2 ) o t = ?( W o x( t ) + U o h( t ? 1 ) + b o ) ( 3 ) Ct = tanh ( W c x( t ) + U c h( t ? 1 ) + b c ) ( 4 ) C t = i t * Ct + f t * C t?1 ( 5 ) h t = o t * tanh ( C t ) ( 6 )
There are three gates ( input i , forget f and output o ) , and a cell memory vector C t . ? is the sigmoid function .
W ? R H?E , U ? R H?H and b ?
R H?1 are the network parameters .
Single-direction LSTMs suffer from the weakness of not making use of the contextual information from the future tokens .
Bidirectional LSTMs ( biLSTMs ) use both the previous and future context by processing the sequence in two directions , and generate two sequences of output vectors .
The output for each token is the concatenation of the two vectors from both directions , i.e. h t = ? ? h t ? ? h t . QA - LSTM : Our basic answer selection framework is shown in Figure 1 . Given an input pair ( q , a ) , where q is a question and a is a candidate answer , first we retrieve the word embeddings ( WEs ) of both q and a .
Then , we separately apply a biLSTM over the two sequences of WEs .
Next , we generate a fixed -sized distributed vector representations using one of the following three approaches : ( 1 ) the concatenation of the last vectors on both directions of the biLSTM ; ( 2 ) average pooling over all the output vectors of the biLSTM ; ( 3 ) max pooling over all the output vectors .
Finally , we use cosine similarity sim(q , a ) to score the input ( q , a ) pair .
It is important to note that the same biLSTM is applied to both q and a .
Similar to ( Feng et al. , 2015 ; Weston et al. , 2014 ; Hu et al. , 2014 ) , we define the training objective as a hinge loss .
L = max{0 , M ?sim(q , a + ) + sim(q , a ? ) } ( 7 ) where a + is a ground truth answer , a ? is an incorrect answer randomly chosen from the entire answer space , and M is a margin .
We treat any question with more than one ground truth as multiple training examples .
During training , for each question we randomly sample K negative answers , but only use the one with the highest L to update the model .
Finally , dropout operation is performed on the representations before cosine similarity matching .
The same scoring function , loss function and negative sampling procedure is also used in the NN architectures presented in what follows .
Convolutional LSTMs
The pooling strategies used in QA - LSTM suffer from the incapability of filtering important local information , especially when dealing with long answer sequences .
Also , it is well known that LSTM models successfully keep the useful information from longrange dependency .
But the strength has a tradeoff effect of ignoring the local n-gram coherence .
This can be partially alleviated with bidirectional architectures .
Meanwhile , the convolutional structures have been widely used in the question answering tasks , Figure 1 : Basic Model : QA -LSTM such as ( Yu et al. , 2014 ; Feng et al. , 2015 ; Hu et al. , 2014 ) .
Classical convolutional layers usually emphasize the local lexical connections of the n-gram .
However , the local pieces are associated with each other only at the pooling step .
No longrange dependencies are taken into account during the formulation of convolution vectors .
Fundamentally , recurrent and convolutional neural networks have their own pros and cons , due to their different topologies .
How to keep both merits motivates our studies of the following two hybrid models .
Convolutional -pooling LSTMs
In Figure 2 we detail the convolutional - pooling LSTM architecture .
In this NN architecture , we replace the simple pooling layers ( average / maxpooling ) by a convolutional layer , which allows to capture richer local information by applying a convolution over sequences of LSTM output vectors .
The number of output vectors k ( context window size ) considered by the convolution is a hyper-parameter of the model .
The convolution structure adopted in this work is as follows : Z ? R k|h |?L is a matrix where the m-th column is the concatenation of k hidden vectors generated from biLSTM centralized in the m-th word of the sequence , L is the length of the sequence after wide convolution ( Kalchbrenner et al. , 2014 ) .
The output of the convolution with c filters is , C = tanh ( W cp Z ) ( 8 ) where W cp are network parameters , and C ? R c?L .
The j-th element of the representation vectors ( o q and o a ) is computed as follows , [ o j ] = max 1 < l < L [ C j,l ] ( 9 ) Figure 2 : Convolutional - pooling LSTM
Convolution - based LSTMs
In Figure 3 , we detail our second hybrid NN architecture .
The aim of this approach is to capture the local n-gram interaction at the lower level using a convolution .
At the higher level , we build bidirectional LSTMs , which extract the long range dependency based on convoluted n-gram .
Combining convolutional and recurrent structures have been investigated in prior work other than question answering ( Donahue et al. , 2015 ; Zuo et al. , 2015 ; Sainath et al. , 2015 ) .
As shown in Figure 3 , the model first retrieves word vectors for each token in the sequence .
Next , we compose the matrix D ? R kE?L , where each column l in D consists of the concatenation of k word vectors of size E centered at the l-th word .
The matrix X ? R c?L , which is the output of the convolution with c filters is computed as follows : X = tanh ( W cb D ) ( 10 )
The matrix X is the input to the biLSTM structure in Eqs. 1 - 6 .
After the biLSTM step , we use maxpooling over the biLSTM output vectors to obtain the representations of both q and a .
Attentive LSTMs
In the previous subsections , the two most popular deep learning architectures are integrated to generate semantic representations for questions and answers from both the long-range sequential and local n-gram perspectives .
QA - LSTM and the two proposed hybrid models are basically siamese networks ( Chopra et al. , 2005 ) .
These structures overlook another potential issue .
The answers might be extremely long and contain lots of words that are not related to the question at hand .
No matter what advanced neural networks are exploited at the answer side , the resulting representation might still be distracted by non-useful information .
A typical example is the Figure 3 : Convolution - based LSTM second candidate answer in Table 1 .
If the construction of the answer representation is not aware of the input question , the representation might be strongly influenced by n-grams such as " are widowed " and " your spouse 's death " , which are informative if we only look at the candidate answer , but are not so important for the input question .
We address this problem by developing a simple attention model for the answer vector generation , in order to alleviate this weakness by dynamically aligning the more informative parts of answers to the questions .
Inspired by the work in ( Hermann et al. , 2015 ) , we develop a very simple but efficient word-level attention on the basic model .
In Figure 4 , we detail our Attentive LSTM architecture .
Prior to the average or mean pooling , each biLSTM output vector is multiplied by a softmax weight , which is determined by the question representation from biL - STM .
Specifically , given the output vector of biL - STM on the answer side at time step t , h a ( t ) , and the question representation , o q , the updated vector h a ( t ) for each answer token are formulated below .
m a , q ( t ) = W am h a ( t ) + W qm o q ( 11 ) s a , q ( t ) ? exp ( w T ms tanh ( m a , q ( t ) ) ) ( 12 ) h a ( t ) = h a ( t ) s a , q ( t ) ( 13 ) where W am , W qm and w ms are attention parameters .
Conceptually , the attention mechanism gives more weight to certain words of the candidate answer , where the weights are computed by taking into consideration information from the question .
The expectation is that words in the candidate answer that are more important with regard to the input question should receive larger weights .
The attention mechanism in this paper is conceptually analogous to the one used in one - layer memory network ( Sukhbaatar et al. , 2015 ) .
The fundamental difference is that the transformed question vector and answer unit vectors are combined in an inner-product pattern in order to generate attentive weights in memory network , whereas this work adopts a summation operation ( Eq. 11 ) .
InsuranceQA Experiments
The first dataset we use to evaluate the proposed approaches is the InsuranceQA , which has been recently proposed by Feng et al . ( 2015 ) .
We use the first version of this dataset .
This dataset contains question and answer pairs from the insurance domain and is already divided into a training set , a validation set , and two test sets .
We do not see any obvious categorical differentiation between two tests ' questions .
We list the numbers of questions and answers of the dataset in Table 2 .
We refer the reader to ( Feng et al. , 2015 ) , for more details regarding the InsuranceQA data .
In this dataset , a question may have multiple correct answers , and normally the questions are much shorter than answers .
The average length of questions in tokens is 7 , while the average length of answers is 94 .
Such difference posts additional challenges for the answer selection task .
This corpus contains 24981 unique answers in total .
For the development and test sets , the InsuranceQA also includes an answer pool of 500 candidate answers for each question .
These answer pools were constructed by including the correct answer (s ) and randomly selected candidates from the complete set of unique answers .
The top - 1 accuracy of the answer selection is reported .
Setup
The proposed models are implemented with Theano ( Bastien et al. , 2012 ) and all experiments are conducted in a GPU cluster .
We use the accuracy on validation set to select the best epoch and best hyper-parameter settings for testing .
The word embeddings are pre-trained , using word2vec ( Mikolov et al. , 2013 ) 3 .
The training data for the word embeddings is a Wikipedia cor-pus of 164 million tokens combined with the questions and answers in the InsuranceQA training set .
The word vector size is set to 100 .
Word embeddings are also part of the parameters and are optimized during the training .
Stochastic Gradient Descent ( SGD ) is the optimization strategy .
The learning rate ? is 1.1 .
We get the best performances when the negative answer count K = 50 .
We also tried different margins in the hing loss function , and finally fixed the margin as M = 0.2 .
We train our models in mini-batches ( with batch size as 20 ) , and the maximum length L of questions and answers is 200 .
Any tokens out of this range are discarded .
In order to get more obvious comparison between the proposed models and the basic framework , with respect to the ground - truth answer length in Fig. 5 , we also provide the results of K = 1 .
In this case , we set M = 0.1 , ? = 0.1 and mini-batches as 100 to get the best performance on the validation set .
Also , the dimension of LSTM output vectors is 141x2 for bidirectional LSTM in QA - LSTM , Attentive LSTM and Convolutional - pooling LSTM , such that biLSTM has a comparable number of parameters with a single-direction LSTM with 200 dimensions .
For Convolution - based LSTM , since LSTM structure is built on the top of CNN , we fixed the CNN output as 282 dimensions and tune the biLSTM hidden vector size in the experiments .
Because the sequences within a mini-batch have different lengths , we use a mask matrix to indicate the real length of each sequence .
Baselines
For comparison , we report the performances of four baselines in the top group in Table 3 : two state - of - the - art non -DL approaches and two variations of a strong DL approach based on CNN .
Bag-of-word :
The idf-weighted sum of word vectors is used as a feature vector .
The candidates are ranked by the cosine similarity to the question .
Metzler-Bendersky IR model : A state- of- theart weighted dependency model ( Bendersky et al. , 2010 ; Bendersky et al. , 2011 ) , which employs a weighted combination of term-based and term proximity - based features to score each candidate .
Architecture -II in ( Feng et al. , 2015 ) : A CNN model is employed to learn distributed representations of questions and answers .
Cosine similarity is used to rank answers .
Model Validation Test1 Test2 Bag-of-word 31.9 32.1 32.2 Metzler-Bendersky IR model 52.7 55.1 50.8 CNN ( Feng et al. , 2015 ) 61.8 62.8 59.2 CNN with GESD ( Feng et al. , 2015 ) 65 Architecture -II with Geometricmean of Euclidean and Sigmoid Dot product ( GESD ) :
Cosine similarity is replaced by GESD , which got the best performance in ( Feng et al. , 2015 ) .
Results and discussions
In this section , we provide detailed analysis on the experimental results .
Table 3 summarizes the results of our models on InsuranceQA .
From Row ( A ) to ( D ) , we list QA - LSTM without either CNN structure or attention .
They vary on the pooling method used .
We can see that by concatenating the last vectors from both directions , ( A ) performs the worst .
We see that using max-pooling ( C ) is much better than average pooling ( B ) .
The potential reason may be that the max-pooling extracts more local values for each dimension .
Compared to ( C ) , ( D ) is better , showing the need of multiple negative answers in training .
Row ( E ) to ( I ) show the results of Convolutional - pooling LSTMs and Convolutionbased LSTMs with different filter sizes c , biLSTM hidden sizes | h| and negative answer pool size K.
Increasing the negative answer pool size , we are allowed to use less filter counts ( F vs E ) .
Larger filter counts help on the test accuracies ( G vs F ) for Convolutional - pooling LSTMs .
We have the same observation with larger biLSTM hidden vector size for Convolution - based LSTMs .
Both convolutional models outperform the plain QA - LSTM ( D ) by about 1.0 % on test1 , and 0.7 % on test2 .
Rows ( L- N ) correspond to QA - LSTM with the attention model , with either max-pooling or average pooling .
We observe that max-pooling is better than avg-pooling , which is consistent with QA - LSTMs .
In comparison to Model ( D ) , Model ( N ) shows over 2 % improvement on both validation and Test1 sets .
And ( N ) gets improvements over the best baseline in Table 3 by 3.5 % , 3.7 % and 3.8 % on the validation , Test1 and Test2 sets , respectively .
Compared to Architecture II in ( Feng et al. , 2015 ) , which involved a large number of CNN filters , ( N ) model also has fewer parameters .
We also test the proposed attention mechanism on convolutional networks .
( J ) replaces the LSTM in QA - LSTM with a convolutional layer .
We set the filter size c = 400 and window size k = 3 according to the validation accuracy .
( K ) performs the similar attention on the convolutional output of the answers .
Similar to biLSTM , the attention on the convolutional layer gives over 2 % accuracy improvement on both test sets , which proves the attention 's efficiency on both CNN and RNN structures .
Finally , we investigate the proposed models on how they perform with respect to long answers .
To better illustrate the performance difference , we Models MAP MRR ( Yao et al. , 2013 ) 0.631 0.748 ( Severyn and Moschitti , 2013 ) 0.678 0.736 ( Yih et al. , 2013 )- BDT 0.694 0.789 ( Yih et al. , 2013 )- LCLR 0.709 0.770 ( Wang and Nyberg , 2015 ) 0.713 0.791
Architecture -II ( Feng et al. , 2015 ) 0.711 0.800 ( Severyn and Moschitti , 2015 ) 0.671 0.728 w/o additional features ( Severyn and Moschitti , 2015 ) 0
The test set results on TREC - QA compare the models with K = 1 ( i.e. the models C , E , L ) .
We divide the questions of Test1 and Test2 sets into eleven buckets , according to the average length of their ground truth answers .
As shown in Figure 5 , QA - LSTM gets better or similar performance compared to the proposed models on buckets with shorter answers ( L ? 50 , 50 < L ?55 , 55 < L ?60 ) .
As the answer lengths increase , the gap between QA - LSTM and other models becomes more obvious .
It suggests the effectiveness of Convolutional - pooling LSTM and Attentive LSTM for long -answer questions .
In ( Feng et al. , 2015 ) , GESD outperforms cosine similarity in their models .
However , the proposed models with GESD as similarity scores do not provide any improvement on the accuracy .
TREC - QA Experiments
In this section we detail our experimental setup and results using the TREC - QA dataset .
Data , metrics and baselines
We test the models on TREC - QA dataset , created based on Text REtrieval Conference ( TREC ) QA track ( 8- 13 ) data .
More detail of the generation steps for this data can be found in ( Wang et al. , 2007 ) .
We follow the exact approach of train / dev/ test questions selection in ( Wang and Nyberg , 2015 ) , in which all questions with only positive or negative answers are removed .
Finally , we have 1162 training , 65 development and 68 test questions .
Similar to previous work , we use Mean Average Precision ( MAP ) and Mean Reciprocal Rank ( MRR ) as evaluation metrics , which are evaluated using the official scripts .
In the top part of Table 4 , we list the performance of recent prior work on this dataset .
We implemented the Architecture II in ( Feng et al. , 2015 ) from scratch .
The CNN structure in ( Severyn and Moschitti , 2015 ) combined with additional human-designed features achieved the best MAP and MRR .
Setup
We keep the configurations same as those in Insur-anceQA in section 4.1 , except the following differences : 1 ) Following Wang and Nyberg ( 2015 ) , we use 300 - dimensional vectors that were trained and provided by word2vec ( Mikolov et al. , 2013 ) using a part of the Google News dataset 4 . 2 ) Since the word vectors of TREC - QA have a greater dimension than InsuranceQA , we accordingly have larger biLSTM hidden vectors and CNN filters , in order not to lose information from word vectors .
Here we set both of them as 600 .
3 ) We use the models from the epoch with the best MAP on the validation set .
4 ) We also observe that because of the smaller data size , we need a decayed learning rate ? in order to stablize the models ' training .
Specifically , we set the initial ? 0 = 1.1 , and decrease it for each epoch T > 1 as ?
T = ? 0 /T . 5 ) We fix the negative answer size K = 50 during training .
Results
The bottom part of Table 4 shows the performance of the proposed models .
For the comparison purpose , we replace biLSTM with a convolution in Model ( A ) , and also use max-pooling to get question and answer embeddings , and call this model QA - CNN .
QA - LSTM ( B ) improves MAP and MRR in more than 1 % when compared to QA - CNN ( A ) .
Compared to ( B ) , convolutionalpooling ( C ) performs better on MAP by 0.9 % , and convolution - based models on MAP by 0.4 % and MRR by 0.8 % .
Attentive LSTM is the best proposed model , and outperforms the best baseline ( Severyn and Moschitti , 2015 ) by 0.7 % on MAP and 2.2 % on MRR .
Note that the best result in ( Severyn and Moschitti , 2015 ) was obtained by combining CNN - based features with additional human-defined features .
In contrast , our attentive LSTM model achieves higher performance without using any human-defined features .
Conclusion
In this paper , we address the following problem for the answer passage selection : how can we construct the embeddings for questions and candidate answers , in order to better distinguish the correct answers from other candidates ?
We propose three independent models in two directions .
First , we develop two hybrid models which combine the strength of both recurrent and convolutional neural networks .
Second , we introduce a simple oneway attention mechanism , in order to generate answer embeddings influenced by the question context .
Such attention fixes the issue of independent generation of the question and answer embeddings in previous work .
All proposed models are departed from a basic architecture , built on bidirectional LSTMs .
We conduct experiments on Insur-anceQA and TREC - QA datasets , and the experimental results demonstrate that the proposed models outperform a variety of strong baselines .
Potential future work include : 1 ) Evaluating the proposed approaches for different tasks , such as community QA and textual entailment ;
2 ) Including the sentential attention mechanism ;
3 ) Integrating the hybrid and the attentive mechanisms into a single framework .
Figure Figure 4 : Attentive LSTM
