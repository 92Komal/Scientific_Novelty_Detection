title
Identifying
Where to Focus in Reading Comprehension for Neural Question Generation
abstract
A first step in the task of automatically generating questions for testing reading comprehension is to identify questionworthy sentences , i.e. sentences in a text passage that humans find it worthwhile to ask questions about .
We propose a hierarchical neural sentence - level sequence tagging model for this task , which existing approaches to question generation have ignored .
The approach is fully data-driven - with no sophisticated NLP pipelines or any hand -crafted rules / features - and compares favorably to a number of baselines when evaluated on the SQuAD data set .
When incorporated into an existing neural question generation system , the resulting end-to - end system achieves stateof - the - art performance for paragraph - level question generation for reading comprehension .
Introduction and Related Work Automatically generating questions for testing reading comprehension is a challenging task ( Mannem et al. , 2010 ; Rus et al. , 2010 ) .
First and foremost , the question generation system must determine which concepts in the associated text passage are important , i.e. are worth asking a question about .
The little previous work that exists in this area currently circumvents this critical step in passagelevel question generation by assuming that such sentences have already been identified .
In particular , prior work focuses almost exclusively on sentence - level question generation : given a text passage , assume that all sentences contain a question - worthy concept and generate one or more questions for each ( Heilman and Smith , 2010 ; Du et al. , 2017 ; Zhou et al. , 2017 ) .
In contrast , we study the task of passage - level question generation ( QG ) .
Inspired by the large body of research in text summarization on identifying sentences that contain " summary - worthy " content ( e.g. Mihalcea ( 2005 ) , Berg- Kirkpatrick et al. ( 2011 ) , ) , we develop a method to identify the question - worthy sentences in each paragraph of a reading comprehension passage .
Inspired further by the success of neural sequence models for many natural language processing tasks ( e.g. named entity recognition ( Collobert et al. , 2011 ) , sentiment classification ( Socher et al. , 2013 ) , machine translation ( Sutskever et al. , 2014 ) , dependency parsing ( Chen and Manning , 2014 ) ) , including very recently document- level text summarization ( Cheng and Lapata , 2016 ) , we propose a hierarchical neural sentence - level sequence tagging model for question - worthy sentence identification .
We employ the SQuAD reading comprehension data set ( Rajpurkar et al. , 2016 ) for evaluation and show that our sentence selection approach compares favorably to a number of baselines including the feature - rich sentence selection model of Cheng and Lapata ( 2016 ) proposed in the context of extract - based summarization , and the convolutional neural network model of Kim ( 2014 ) that achieves state - of - the - art results on a variety of sentence classification tasks .
We also incorporate our sentence selection component into the neural question generation system of Du et al . ( 2017 ) and show , again using SQuAD , that our resulting end-to - end system achieves state - of - the - art performance for the challenging task of paragraph - level question generation for reading comprehension .
Problem Formulation
In this section , we define the tasks of important ( i.e. question - worthy ) sentence selection and sentence - level question generation ( QG ) .
Our full paragraph - level QG system includes both of these components .
For the sentence selection task , given a paragraph D consisting of a sequence of sentences {s 1 , ... , s m } , we aim to select a subset of k question - worthy sentences ( k < m ) .
The goal is defined as finding y = {y 1 , ... , y m } , such that , y = arg max y log P 1 ( y|D ) = arg max y | y | t=1 log P 1 ( y t | D ) ( 1 ) where log P ( y|D ) is the conditional loglikelihood of the label sequence y ; and y i = 1 means sentence i is question - worthy ( contains at least one answer ) , otherwise y i = 0 .
For sentence - level QG , the goal is to find the best word sequence z ( a question of arbitrary length ) that maximizes the conditional likelihood given the input sentence x and satisfies : z = arg max z log P 2 ( z| x ) = arg max z | z| t=1 log P 2 ( z t |x , z < t ) ( 2 ) where P 2 ( z| x ) is modeled with a global attention mechanism ( Section 3 ) .
Model Important Sentence Selection
Our general idea for the hierarchical neural network architecture is illustrated in Figure 1 . First , we perform the encoding using sum operation or convolu-tion + maximum pooling operation ( Kim , 2014 ; dos Santos and Zadrozny , 2014 ) over the word vectors comprising each sentence in the input paragraph .
For simplicity and consistency , we denote the sentence encoding process as ENC .
Given the t th sentence x = {x 1 , ... , x n } in the paragraph , we have its encoding : s t = ENC ( [ x 1 , ... , x n ] ) ( 3 )
Then we use a bidirectional LSTM ( Hochreiter and Schmidhuber , 1997 ) to encode the paragraph , ? ? ? ? ? ? " ? # ? $ ? % ? & ? $ ? % ? & ? # ? "
Sentence Encoder Figure 1 : Hierarchical neural network architecture for sentence - level sequence labeling .
The input is a paragraph consisting of sentences , whose encoded representation is fed into each hidden unit .
? ? h t = ? ? ? LSTM s t , ? ? h t?1 ? ? h t = ? ? ? LSTM s t , ? ? h t+1
We use the concatenation of the two , namely , [ ? ? h t ; ? ? h t ] , as the hidden state h t at time stamp t , and feed it to the upper layers to get the probability distribution of y t ( ? { 0 , 1 } ) , P 1 ( y t |D ; ? ) = softmax MLP tanh [ ? ? h t ; ? ? h t ] where MLP is multi-layer neural network and tanh is the activation function .
Question Generation Similar to Du et al. ( 2017 ) , we implement the sentence - level question generator with an attention - based sequence - to-sequence learning framework ( Sutskever et al. , 2014 ; Bahdanau et al. , 2015 ) , to map a sentence in the reading comprehension article to natural questions .
It consists of an LSTM encoder and decoder .
The encoder is a bi-directional LSTM network ; it encodes the input sentence x into a sequence of hidden states q 1 , q 2 , ..
The decoder is another LSTM that uses global attention over the encoder hidden states .
The entire encoder-decoder structure learns the probability of generating a question given a sentence , as indicated by equation 2 .
To be more specific , P 2 ( z t |x , z < t ) = softmax ( W s tanh ( W t [ h t ; c t ] ) ) where W s , W t are parameter matrices ; h t is the hidden state of the decoder LSTM ; and c t is the context vector created dynamically by the encoder LSTM - the weighted sum of the hidden states computed for the source sentence : c t = i=1 , .. , |x| a i, t q i
The attention weights a i,t are calculated via a bilinear scoring function and softmax normalization : a i , t = exp ( h T t W b q i ) j exp( h T t W b q j )
Apart from the bilinear score , alternative options for computing the attention can also be used ( e.g. dot product ) .
Readers can refer to Luong et al . ( 2015 ) for more details .
During inference , beam search is used to predict the question .
The decoded UNK token at time step t , is replaced with the token in the input sentence with the highest attention score , the index of which is arg max i a i , t .
Henceforth , we will refer to our sentence - level Neural Question Generation system as NQG .
Note that generating answer-specific questions would be easy for this architecture - we can append answer location features to the vectors of tokens in the sentence .
To better mimic the real life case ( where questions are generated with no prior knowledge of the desired answers ) , we do not use such location features in our experiments .
Experimental Setup and Results
Dataset and Implementation Details
We use the SQuAD dataset ( Rajpurkar et al. , 2016 ) for training and evaluation for both important sentence selection and sentence - level NQG .
The dataset contains 536 curated Wikipedia articles with over 100k questions posed about the articles .
The authors employ Amazon Mechanical Turk crowd-workers to generate questions based on the article paragraphs and to annotate the corresponding answer spans in the text .
Later , to make the evaluation of the dataset more robust , other crowd-workers are employed to provide additional answers to the questions .
We split the public portion of the dataset into training ( ? 80 % ) , validation ( ? 10 % ) and test ( ? 10 % ) sets at the paragraph level .
For the sentence selection task , we treat sentences that contain at least one answer span ( question - worthy sentences ) as positive examples ( y = 1 ) ; all remaining sentences are considered negative ( y = 0 ) .
Not surprisingly , the training set is unbalanced : 52332 ( ? 60 % ) sentences contain answers , while 29693 sentences do not .
Because of the variabil - 2008 ) to augment the set of known summaryworthy sentences .
In contrast , we adopt a conservative approach rather than predict too many sentences as being question - worthy : we pair up source sentences with their corresponding questions , and use just these sentence -question pairs to training the encoder-decoder model .
We use the glove .
840B.300d pre-trained embeddings ( Pennington et al. , 2014 ) for initialization of the embedding layer for our sentence selection model and the full NQG model .
glove .6B.100d embeddings are used for calculating sentence similarity feature of the baseline linear model ( LREG ) .
Tokens outside the vocabulary list are replaced by the UNK symbol .
Hyperparameters for all models are tuned on the validation set and results are reported on the test set .
Sentence Selection Results
We compare to a number of baselines .
The Random baseline assigns a random label to each sentence .
The Majority baseline assumes that all sentences are question -worthy .
The convolutional neural networks ( CNN ) sentence classification model ( Kim , 2014 ) has similar structure to our CNN sentence encoder , but the classification is done only at the sentence - level rather than jointly at paragraph -level .
LREG w/ BOW is the logistic regression model with bag-of-words features .
LREG w/ para .- level is the feature - rich LREG model designed by Cheng and Lapata ( 2016 ) ; the features include : sentence length , position of sentence , number of named entities in the sentence , number of sentences in the paragraph , sentence - tosentence cohesion , and sentence - to - paragraph relevance .
Sentence -to-sentence cohesion is obtained conservative eval .
liberal eval .
a a a a a a a a a a System Output Gold Data w/ Q w/o Q w/ Q w/o Q w/ Q matching zero matching full w/o Q zero - zero - Table 3 : For a source sentence in SQuAD , given the prediction from the sentence selection system and the corresponding NQG output , we provide conservative and liberal evaluations .
by calculating the embedding space similarity between it and every other sentence in the paragraph ( similar for sentence - to- paragraph relevance ) .
In document summarization , graph - based extractive summarization models ( e.g. TGRAPH Parveen et al. ( 2015 ) and URANK Wan ( 2010 ) ) focus on global optimization and extract sentences contributing to topical coherent summaries .
Because this does not really fit our task - a summaryworthy sentence might not necessarily contain enough information for generating a good question - we do not include these as comparisons .
Results are displayed in Table 1 .
Our models with sum or CNN as the sentence encoder significantly outperform the feature - rich LREG as well as the other baselines in terms of F-measure .
Evaluation of the full QG system
To evaluate the full systems for paragraph - level QG , we introduce in Table 3 the " conservative " and " liberal " evaluation strategies .
Given an input source sentence , there will be in total four possibilities : if both the gold standard data and prediction include the sentence , then we use its n-gram matching score ( by BLEU ( Papineni et al. , 2002 ) and METEOR ( Denkowski and Lavie , 2014 ) ) ; if neither the gold data nor prediction include the sentence , then the sentence is discarded from the evaluation ; if the gold data includes the sentence while the prediction does not , we assign a score of 0 for it ; and if gold data does not include the sentence while prediction does , the generated question gets a 0 for conservative , while it gets full Wikipedia paragraph : arnold schwarzenegger has been involved with the special olympics for many years after they were founded by his ex-mother - in- law , eunice kennedy shriver . after they were founded by his ex-mother - in- law , eunice kennedy shriver . in 2007 , schwarzenegger was the official spokesperson for the special olympics which were held in shanghai , china . :::::::::::: schwarzenegger ::: was :: the ::::: : official :::::::::: spokesperson ::: for :: the ::::: : special :::::: : olympics :::: : which ::: : were ::: held :: in ::::: : : shanghai , ::: : : china : . schwarzenegger believes that quality school opportunities should be made available to children who might not normally be able to access them .
in 1995 , he founded the inner city games foundation - lrb-icg - rrb- which provides cultural , : in ::: : 1995 : , :: he :::::: : founded :: the :::: : inner :: : city :::: : games ::::::: : foundation :::: - lrb- : : icg ::: : -rrb-::: : : which ::::: : : provides ::::: : cultural , :::::::: educational ::: and :::::::: : community ::::::::: enrichment :::::::::: programming :: to ::: : youth : . icg is active in 15 cities around the country and serves over 250,000 children in over 400 schools countrywide . :: icg :: is :::: : active : in :: : 15 ::: : cities :::: : around ::: the ::::: : country ::: and :::: : serves ::: over ::::: : : 250,000 ::::: : children :: in ::: : over :: : 400 ::::: : schools ::::::::: countrywide : . he has also been involved with after-school all-stars , and founded the los angeles branch in 2002 . asas is an after school program provider , educating youth about health , fitness and nutrition .
Our questions : Q1 : who founded the special olympics ?
Q2 : who was the official adviser for the special olympics ? Q3 : when was the inner city games foundation founded ?
Q4 : how many schools does icg have ?
Gold questions : Q1 : schwarzenegger was the spokesperson for the special olympic games held in what city in china ? Q2 : what nonprofit did schwarzenegger found in 1995 ? Q3 : about how many schools across the country is icg active in ?
Figure 2 : Sample output from our full NQG system , the four questions correspond to the four highlighted sentences in the paragraph in the same order .
Darkness indicates sentence importance , the score for deciding the darkness is obtained from the softmax results .
Wave - lined sentences bear label y = 1 , and 0 otherwise .
The three gold questions also correspond to the wave - lined sentences in the same order .
Please refer to the appendix for sample output on more Wikipedia articles .
score for liberal evaluation .
Table 2 shows that the QG system incorporating our best performing sentence extractor outperforms its LREG counterpart across metrics .
Note that to calculate the score for the matching case , similar to our earlier work ( Du et al. , 2017 ) , we adapt the image captioning evaluation scripts of Chen et al . ( 2015 ) since there can be several gold standard questions for a single input sentence .
In Figure 2 , we provide questions generated by the full NQG system ( Q1 - 4 ) and according to the gold standard ( Q1 - 3 ) for the selected Wikipedia paragraph .
The sentences they were drawn from are shown with wavy lines ( gold standard ) and via highlighting ( our system ) .
Darkness of the highlighting is proportional to the softmax score provided by the sentence extractor .
Conclusion
In this work we introduced the task of identifying important sentences - good sentences to ask a question about - in the reading comprehension setting .
We proposed a hierarchical neural sentence labeling model and investigated encoding sentences with sum and convolution operations .
The question generation system that uses our sentence selection model consistently outperforms previous approaches and achieves state- ofthe - art paragraph - level question generation performance on the SQUAD data set .
In future work , we would like to investigate approaches to identify question - worth concepts rather than question - worthy sentences .
It would also be interesting to see if the generated questions can be used to help improve question answering systems .
Table 1 : 1 . , q | x| . Automatic evaluation results for important sentence selection .
The best performing system in each column is highlighted in boldface .
Paragraph - level accuracies are calculated as the proportion of paragraphs in which all of the sentences are predicted correctly .
We show two -tailed t-test results on F-measure for our best performing method compared to the other baselines .
( Statistical significance is indicated with Model Precision Recall F-measure Acc. Paragraph -level Acc. RANDOM 63.45 50.29 56.11 50.27 11.69 Majority Baseline 63.21 100.00 77.46 63.21 32.30 CNN ( Kim , 2014 ) 68.35 90.13 77.74 67.38 24.73 LREG ( w/ BOW ) 68.52 86.55 76.49 66.37 31.36 LREG ( w/ para.- level ) ( Cheng and Lapata , 2016 ) 70.49 89.08 78.70 69.52 33.95 Ours SUM ( no pre-trained ) 73.02 89.23 80.32 72.36 36.46 Ours SUM ( w/ pre-trained ) 73.85 87.65 80.16 72.58 36.30 Ours CNN ( no pre-trained ) 73.15 89.29 80.42 * 72.52 35.93 Ours CNN ( w/ pre-trained ) 74.35 86.11 79.80 72.44 36.87 * ( p < 0.005 ) . )
