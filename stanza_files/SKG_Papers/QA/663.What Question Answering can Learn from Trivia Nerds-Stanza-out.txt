title
What Question Answering can Learn from Trivia Nerds
abstract
Question answering ( QA ) is not just building systems ; this NLP subfield also creates and curates challenging question datasets that reveal the best systems .
We argue that QA datasets - and QA leaderboards - closely resemble trivia tournaments : the questions agents - humans or machines - answer reveals a " winner " .
However , the research community has ignored the lessons from decades of the trivia community creating vibrant , fair , and effective QA competitions .
After detailing problems with existing QA datasets , we outline several lessons that transfer to QA research : removing ambiguity , identifying better QA agents , and adjudicating disputes .
Introduction
This paper takes an unconventional analysis to answer " where we 've been and where we 're going " in question answering ( QA ) .
Instead of approaching the question only as computer scientists , we apply the best practices of trivia tournaments to QA datasets .
The QA community is obsessed with evaluation .
Schools , companies , and newspapers hail new SOTAs and topping leaderboards , giving rise to troubling claims ( Lipton and Steinhardt , 2019 ) that an " AI model tops humans " ( Najberg , 2018 ) because it ' won ' some leaderboard , putting " millions of jobs at risk " ( Cuthbertson , 2018 ) .
But what is a leaderboard ?
A leaderboard is a statistic about QA accuracy that induces a ranking over participants .
Newsflash : this is the same as a trivia tournament .
The trivia community has been doing this for decades ( Jennings , 2006 ) ; Section 2 details this overlap between the qualities of a first-class QA dataset ( and its requisite leaderboard ) .
The experts running these tournaments are imperfect , but they 've learned from their past mistakes ( see Appendix A for a brief historical perspective ) and cre-ated a community that reliably identifies those best at question answering .
Beyond the format of the competition , trivia norms ensure individual questions are clear , unambiguous , and reward knowledge ( Section 3 ) .
We are not saying that academic QA should surrender to trivia questions or the community - far from it !
The trivia community does not understand the real world information seeking needs of users or what questions challenge computers .
However , they have well -tested protocols to declare that someone is better at answering questions than another .
This collection of tradecraft and principles can nonetheless help the QA community .
Beyond these general concepts that QA can learn from , Section 4 reviews how the " gold standard " of trivia formats , Quizbowl can improve traditional QA .
We then briefly discuss how research that uses fun , fair , and good trivia questions can benefit from the expertise , pedantry , and passion of the trivia community ( Section 5 ) .
2 Surprise , this is a Trivia Tournament !
" My research is n't a silly trivia tournament , " you say .
That may be , but let us first tell you a little about what running a tournament is like , and perhaps you might see similarities .
First , the questions .
Either you write them yourself or you pay someone to write questions by a particular date ( sometimes people on the Internet ) .
Then , you advertise .
You talk about your questions : who is writing them , what subjects are covered , and why people should try to answer them .
Next , you have the tournament .
You keep your questions secure until test time , collect answers from all participants , and declare a winner .
Afterward , people use the questions to train for future tournaments .
These have natural analogs to crowd sourcing questions , writing the paper , advertising , and running a leaderboard .
Trivia nerds cannot help you form hypotheses or write your paper , but they can tell you how to run a fun , well - calibrated , and discriminative tournament .
Such tournaments are designed to effectively find a winner , which matches the scientific goal of knowing which model best answers questions .
Our goal is not to encourage the QA community to adopt the quirks and gimmicks of trivia games .
Instead , it 's to encourage experiments and datasets that consistently and efficiently find the systems that best answer questions .
Are we having fun ?
Many authors use crowdworkers to establish human accuracy ( Rajpurkar et al. , 2016 ; Choi et al. , 2018 ) .
However , they are not the only humans who should answer a dataset 's questions .
So should the dataset 's creators .
In the trivia world , this is called a play test : get in the shoes of someone answering the questions .
If you find them boring , repetitive , or uninteresting , so will crowdworkers .
If you can find shortcuts to answer questions ( Rondeau and Hazen , 2018 ; Kaushik and Lipton , 2018 ) , so will a computer .
Concretely , Weissenborn et al. ( 2017 ) catalog artifacts in SQuAD ( Rajpurkar et al. , 2018 ) , the most popular QA leaderboard .
If you see a list like " Along with Canada and the United Kingdom , what country .
. . " , you can ignore the rest of the question and just type Ctrl + F ( Yuan et al. , 2019 ; Russell , 2020 ) to find the third country -Australia in this case - that appears with " Canada and the UK " .
Other times , a SQuAD playtest would reveal frustrating questions that are i ) answerable given the information but not with a direct span , 1 ii ) answerable only given facts beyond the given paragraph , 2 iii ) unintentionally embedded in a discourse , resulting in arbitrary correct answers , 3 iv ) or non-questions .
1 A source paragraph says " In [ Commonwealth countries ] .
. . the term is generally restricted to .
. . Private education in North America covers the whole gamut .
. . " ; thus , " What is the term private school restricted to in the US ? " has the information needed but not as a span .
2 A source paragraph says " Sculptors [ in the collection include ]
Nicholas Stone , Caius Gabriel Cibber , [... ] , Thomas Brock , Alfred Gilbert , [...] and Eric Gill " , i.e. , a list of names ; thus , the question " Which British sculptor whose work includes the Queen Victoria memorial in front of Buckingham Palace is included in the V&A collection ? " should be unanswerable in SQuAD .
3 A question " Who else did Luther use violent rhetoric towards ? " has the gold answer " writings condemning the Jews and in diatribes against Turks " .
SearchQA ( Dunn et al. , 2017 ) , derived from Jeopardy ! , asks " An article that he wrote about his riverboat days was eventually expanded into Life on the Mississippi . "
The apprentice and newspaper writer who wrote the article is named Samuel Langhorne Clemens ; however , the reference answer is his later pen name , Mark Twain .
Most QA evaluation metrics would count Samuel Clemens as incorrect .
In a real game of Jeopardy ! , this would not be an issue ( Section 3.1 ) .
Of course , fun is relative , and any dataset is bound to contain errors .
However , playtesting is an easy way to find systematic problems : unfair , unfun playtests make for ineffective leaderboards .
Eating your own dog food can help diagnose artifacts , scoring issues , or other shortcomings early in the process .
The deeper issues when creating a QA task are : i ) have you designed a task that is internally consistent , ii ) supported by a scoring metric that matches your goals , iii ) using gold annotations that reward those who do the task well ?
Imagine someone who loves answering the questions your task poses : would they have fun on your task ?
This is the foundation of Gamification ( von Ahn , 2006 ) , which can create quality data from users motivated by fun rather than pay .
Even if you pay crowdworkers , unfun questions may undermine your dataset goals .
Am I measuring what I care about ?
Answering questions requires multiple skills : identifying answer mentions ( Hermann et al. , 2015 ) , naming the answer ( Yih et al. , 2015 ) , abstaining when necessary ( Rajpurkar et al. , 2018 ) , and justifying an answer ( Thorne et al. , 2018 ) .
In QA , the emphasis on SOTA and leaderboards has focused attention on single automatically computable metrics -systems tend to be compared by their ' SQuAD score ' or their ' NQ score ' , as if this were all there is to say about their relative capabilities .
Like QA leaderboards , trivia tournaments need to decide on a single winner , but they explicitly recognize that there are more interesting comparisons .
A tournament may recognize different background / resources - high school , small school , undergraduates ( Hentzel , 2018 ) .
Similarly , more practical leaderboards would reflect training time or resource requirements ( see Dodge et al. , 2019 ) including ' constrained ' or ' unconstrained ' training ( Bojar et al. , 2014 ) .
Tournaments also give specific awards ( e.g. , highest score without incorrect answers ) .
Again , there are obvious leaderboard analogs that would go beyond a single number .
In SQuAD 2.0 ( Rajpurkar et al. , 2018 ) , abstaining contributes the same to the overall F 1 as a fully correct answer , obscuring whether a system is more precise or an effective abstainer .
If the task recognizes both abilities as important , reporting a single score risks implicitly prioritizing one balance of the two .
Do my questions separate the best ?
Assume that you have picked a metric ( or a set of metrics ) that captures what you care about .
A leaderboard based on this metric can rack up citations as people chase the top spot .
But your leaderboard is only useful if it is discriminative : the best system reliably wins .
There are many ways questions might not be discriminative .
If every system gets a question right ( e.g. , abstain on non-questions like " asdf " or correctly answer " What is the capital of Poland ? " ) , the dataset does not separate participants .
Similarly , if every system flubs " what is the oldest north- facing kosher restaurant " , it is not discriminative .
Sugawara et al. ( 2018 ) call these questions " easy " and " hard " ; we instead argue for a three - way distinction .
In between easy questions ( system answers correctly with probability 1.0 ) and hard ( probability 0.0 ) , questions with probabilities nearer to 0.5 are more interesting .
Taking a cue from Vygotsky 's proximal development theory of human learning ( Chaiklin , 2003 ) , these discriminative questions - rather than the easy or the hard ones - should most improve QA systems .
These Goldilocks 4 questions ( not random noise ) decide who tops the leaderboard .
Unfortunately , existing datasets have many easy questions .
Sugawara et al. ( 2020 ) find that ablations like shuffling word order ( Feng et al. , 2018 ) , shuffling sentences , or only offering the most similar sentence do not impair systems .
Newer datasets such as DROP ( Dua et al. , 2019 ) and HellaSwag ( Zellers et al. , 2019 ) are harder for today 's systems ; because Goldilocks is a moving target , we propose annual evaluations in Section 5 .
Why so few Goldilocks questions ?
This is a common problem in trivia tournaments , particularly pub quizzes ( Diamond , 2009 ) , where challenging questions can scare off patrons .
Many quiz masters prefer popularity with players and thus write easier questions .
Sometimes there are fewer Goldilocks questions not by choice , but by chance : a dataset becomes less discriminative through annotation error .
All datasets have some annotation error ; if this annotation error is concentrated on the Goldilocks questions , the dataset will be less useful .
As we write this in 2020 , humans and computers sometimes struggle on the same questions .
Figure 1 shows two datasets of the same size with the same annotation error .
However , they have different difficulty distributions and correlation of annotation error and difficulty .
The dataset that has more discriminative questions and consistent annotator error has fewer questions that do not discriminate the winner of the leaderboard .
We call this the effective dataset proportion ? ( higher is better ) .
Figure 2 shows the test set size required to reliably discriminate systems for different ? , based on a simulation ( Appendix B ) .
At this point , you may despair about how big a dataset you need .
5
The same terror besets trivia tournament organizers .
Instead of writing more questions , they use pyramidality ( Section 4 ) to make every question count .
The Craft of Question Writing Trivia enthusiasts agree that questions need to be well written ( despite other disagreements ) .
Asking " good questions " requires sophisticated pragmatic reasoning ( Hawkins et al. , 2015 ) , and pedagogy explicitly acknowledges the complexity of writing effective questions for assessing student performance ( Haladyna , 2004 , focusing on multiple choice questions ) .
QA datasets , however , are often collected from the wild or written by untrained crowdworkers .
Crowdworkers lack experience in crafting questions and may introduce idiosyncrasies that shortcut machine learning ( Geva et al. , 2019 ) .
Similarly , data collected from the wild such as Natural Questions or Ama-zonQA ( Gupta et al. , 2019 ) by design have vast variations in quality .
In the previous section , we focused on how datasets as a whole should be structured .
Now , we focus on how specific questions should be structured to make the dataset as valuable as possible .
Avoiding ambiguity and assumptions Ambiguity in questions not only frustrates answerers who resolve the ambiguity ' incorrectly ' .
Ambiguity also frustrates the goal of using questions to assess knowledge .
Thus , the US Department of Transportation explicitly bans ambiguous questions from exams for flight instructors ( Flight Standards Service , 2008 ) ; and the trivia community has likewise developed rules and norms that prevent ambiguity .
While this is true in many contexts , examples are rife in format called Quizbowl ( Boyd - Graber et al. , 2012 ) , whose very long questions 6 showcase trivia writers ' tactics .
For example , Quizbowl author Zhu Ying ( writing for the 2005 PARFAIT tournament ) asks participants to identify a fictional character while warning against possible confusion [ emphasis added ] :
In contrast , QA datasets often contain ambiguous and under-specified questions .
While this sometimes reflects real world complexities such as actual under-specified or ill-formed search queries ( Faruqui and Das , 2018 ; , ignoring this ambiguity is problematic .
As a concrete example , Natural Questions answers " what year did the us hockey team win the Olympics " with 1960 and 1980 , ignoring the US women 's team , which won in 1998 and 2018 , and further assuming the query is about ice rather than field hockey ( also an Olympic event ) .
Natural Questions associates a page about the United States men's national ice hockey team , arbitrarily removing the ambiguity post hoc .
However , this does not resolve the ambiguity , which persists in the original question : information retrieval arbitrarily provides one of many interpretations .
True to their name , Natural Questions are often under-specified when users ask a question online .
The problem is neither that such questions exist nor that machine reading QA considers questions given an associated context .
The problem is that tasks do not explicitly acknowledge the original ambiguity and gloss over the implicit assumptions in the data .
This introduces potential noise and bias ( i.e. , giving a bonus to systems that make the same assumptions as the dataset ) in leaderboard rankings .
At best , these will become part of the measurement error of datasets ( no dataset is perfect ) .
At worst , they will recapitulate the biases that went into the creation of the datasets .
Then , the community will implicitly equate the biases with correctness : you get high scores if you adopt this set of assumptions .
These enter into real-world systems , further perpetuating the bias .
Playtesting can reveal these issues ( Section 2.1 ) , as implicit assumptions can rob a player of correctly answered questions .
If you wanted to answer 2014 to " when did Michigan last win the championship " - when the Michigan State Spartans won the Women's Cross Country championship - and you cannot because you chose the wrong school , the wrong sport , and the wrong gender , you would complain as a player ; researchers instead discover latent assumptions that creep into the data .
7
It is worth emphasizing that this is not a purely hypothetical problem .
For example , Open Domain Retrieval Question Answering deliberately avoids providing a reference context for the question in its framing but , in re-purposing data such as Natural Questions , opaquely relies on it for the gold answers .
Avoiding superficial evaluations
A related issue is that , in the words of Voorhees and Tice ( 2000 ) , " there is no such thing as a question with an obvious answer " .
As a consequence , trivia question authors delineate acceptable and unacceptable answers .
For example , in writing for the trivia tournament Harvard Fall XI , Robert Chu uses a mental model of an answerer to explicitly delineate the range of acceptable correct answers :
In Newtonian gravity , this quantity satisfies Poisson 's equation .
[. . . ]
For a dipole , this quantity is given by negative the dipole moment dotted with the electric field .
[. . . ]
For 10 points , name this form of energy contrasted with kinetic .
ANSWER : potential energy ( prompt on energy ; accept specific types like electrical potential energy or gravitational potential energy ; do not accept or prompt on just " potential " )
Likewise , the style guides for writing questions stipulate that you must give the answer type clearly and early on .
These mentions specify whether you want a book , a collection , a movement , etc .
It also signals the level of specificity requested .
For example , a question about a date must state " day and month required " ( September 11 , " month and year required " ( April 1968 ) , or " day , month , and year required " ( September 1 , 1939 ) .
This is true for other answers as well : city and team , party and country , or more generally " two answers required " .
Despite these conventions , no pre-defined set of answers is perfect , and every worthwhile trivia competition has a process for adjudicating answers .
In high school and college national competitions and game shows , if low-level staff cannot resolve the issue by throwing out a single question or accepting minor variations ( America instead of USA ) , the low-level staff contacts the tournament director .
The tournament director , who has a deeper knowledge of rules and questions , often decide the issue .
If not , the protest goes through an adjudication process designed to minimize bias : 8 write the summary of the dispute , get all parties to agree to the summary , and then hand the decision off to mutually agreed experts from the tournament 's phone tree .
The substance of the disagreement is communicated ( without identities ) , and the experts apply the rules and decide .
Consider what happened when a particularly inept Jeopardy !
contestant 9 did not answer laproscope to " Your surgeon could choose to take a look inside you with this type of fiber-optic instrument " .
Since the van Doren scandal ( Freedman , 1997 ) , every television trivia contestant has an advocate assigned from an auditing company .
In this case , the advocate initiated a process that went to a panel of judges who then ruled that endoscope ( a more general term ) was also correct .
The need for a similar process seems to have been well - recognized in the earliest days of QA system bake - offs such as TREC - QA , and Voorhees ( 2008 ) notes that [ d ] ifferent QA runs very seldom return exactly the same [ answer ] , and it is quite difficult to determine automatically whether the difference [. . . ] is significant .
In stark contrast to this , QA datasets typically only provide a single string or , if one is lucky , several strings .
A correct answer means exactly matching these strings or at least having a high token overlap F 1 , and failure to agree with the pre-recorded admissible answers will put you at an uncontestable disadvantage on the leaderboard ( Section 2.2 ) .
To illustrate how current evaluations fall short of meaningful discrimination , we qualitatively analyze two near -SOTA systems on SQuAD V1.1 : the original XLNet ( Yang et al. , 2019 ) and a subsequent iteration called Despite XLNet - 123 's margin of almost four absolute F 1 ( 94 vs 98 ) on development data , a manual inspection of a sample of 100 of XLNet - 123 's wins indicate that around two -thirds are ' spurious ' : 56 % are likely to be considered not only equally good but essentially identical ; 7 % are cases where the answer set omits a correct alternative ; and 5 % of cases are ' bad ' questions .
11
Our goal is not to dwell on the exact proportions , to minimize the achievements of these strong systems , or to minimize the usefulness of quantitative evaluations .
We merely want to raise the limitation of blind automation for distinguishing between systems on a leaderboard .
Taking our cue from the trivia community , we present an alternative for MRQA .
Blind test sets are created for a specific time ; all systems are submitted simultaneously .
Then , all questions and answers are revealed .
System authors can protest correctness rulings on questions , directly addressing the issues above .
After agreement is reached , quantitative metrics are computed for comparison purposes - despite their inherent limitations they at least can be trusted .
Adopting this for MRQA would require creating a new , smaller test set every year .
However , this would gradually refine the annotations and process .
This suggestion is not novel : Voorhees and Tice ( 2000 ) accept automatic evaluations " for experiments internal to an organization where the benefits of a reusable test collection are most significant ( and the limitations are likely to be understood ) " ( our emphasis ) but that " satisfactory techniques for [ automatically ] evaluating new runs " have not been found yet .
We are not aware of any change on this front - if anything , we seem to have become more insensitive as a community to just how limited our current evaluations are .
Focus on the bubble
While every question should be perfect , time and resources are limited .
Thus , authors and editors of tournaments " focus on the bubble " , where the " bubble " are the questions most likely to discriminate between top teams at the tournament .
These questions are thoroughly playtested , vetted , and edited .
Only after these questions have been perfected will the other questions undergo the same level of polish .
For computers , the same logic applies .
Authors should ensure that these discriminative questions are correct , free of ambiguity , and unimpeachable .
However , as far as we can tell , the authors of QA datasets do not give any special attention to these questions .
Unlike a human trivia tournament , howeverwith finite patience of the participants - this does not mean that you should necessarily remove all of the easy or hard questions from your dataset .
This could inadvertently lead to systems unable to answer simple questions like " who is buried in Grant 's tomb ? "
( Dwan , 2000 , Chapter 7 ) . Instead , focus more resources on the bubble .
Why Quizbowl is the Gold Standard
We now focus our thus far wide - ranging QA discussion to a specific format : Quizbowl , which has many of the desirable properties outlined above .
We have no delusion that mainstream QA will universally adopt this format ( indeed , a monoculture would be bad ) .
However , given the community 's emphasis on fair evaluation , computer QA can borrow aspects from the gold standard of human QA .
We have shown example of Quizbowl questions , but we have not explained how the format works ; see for more .
You might be scared off by how long the questions are .
However , in real Quizbowl trivia tournaments , they are not finished because the questions are interruptible .
Interruptible A moderator reads a question .
Once someone knows the answer , they use a signaling device to " buzz in " .
If the player who buzzed is right , they get points .
Otherwise , they lose points and the question continues for the other team .
Not all trivia games with buzzers have this property , however .
For example , take Jeopardy ! , the subject of Watson 's tour de force ( Ferrucci et al. , 2010 ) .
While Jeopardy !
also uses signaling devices , these only work once the question has been read in its entirety ; Ken Jennings , one of the top Jeopardy !
players ( and also a Quizbowler ) explains it on a Planet Money interview ( Malone , 2019 ) : Jennings :
The buzzer is not live until Alex finishes reading the question .
And if you buzz in before your buzzer goes live , you actually lock yourself out for a fraction of a second .
So the big mistake on the show is people who are all adrenalized and are buzzing too quickly , too eagerly .
Malone : OK .
To some degree , Jeopardy !
is kind of a video game , and a crappy video game where it 's , like , light goes on , press button - that 's it .
Jennings : ( Laughter )
Yeah .
Jeopardy !'s buzzers are a gimmick to ensure good television ; however , Quizbowl buzzers discriminate knowledge ( Section 2.3 ) .
Similarly , while TriviaQA ( Joshi et al. , 2017 ) is written by knowledgeable writers , the questions are not pyramidal .
Pyramidal Recall that effective datasets discriminate the best from the rest - the higher the proportion of effective questions ? , the better .
Quizbowl's ? is nearly 1.0 because discrimination happens within a question : after every word , an answerer must decide if they know enough to answer .
Quizbowl questions are arranged so that questions are maximally pyramidal : questions begin with hard clues-ones that require deep understandingto more accessible clues that are well known .
Well - Edited Quizbowl questions are created in phases .
First , the author selects the answer and assembles ( pyramidal ) clues .
A subject editor then removes ambiguity , adjusts acceptable answers , and tweaks clues to optimize discrimination .
Finally , a packetizer ensures the overall set is diverse , has uniform difficulty , and is without repeats .
Unnatural Trivia questions are fake : the asker already knows the answer .
But they 're no more fake than a course 's final exam , which - like leaderboards - are designed to test knowledge .
Experts know when questions are ambiguous ( Section 3.1 ) ; while " what play has a character whose father is dead " could be Hamlet , Antigone , or Proof , a good writer 's knowledge avoids the ambiguity .
When authors omit these cues , the question is derided as a " hose " ( Eltinge , 2013 ) , which robs the tournament of fun ( Section 2.1 ) .
One of the benefits of contrived formats is a focus on specific phenomena .
Dua et al. ( 2019 ) exclude questions an existing MRQA system could answer to focus on challenging quantitative reasoning .
One of the trivia experts consulted in Wallace et al . ( 2019 ) crafted a question that tripped up neural QA by embedding the phrase " this author opens Crime and Punishment " into a question ; the top system confidently answers Fyodor Dostoyevski .
However , that phrase was in a longer question " The narrator in Cogwheels by this author opens Crime and Punishment to find it has become The Brothers Karamazov " .
Again , this shows the inventiveness and linguistic dexterity of the trivia community .
A counterargument is that real- life questionse.g. , on Yahoo !
Questions ( Szpektor and Dror , 2013 ) , Quora ( Iyer et al. , 2017 ) or web search - ignore the craft of question writing .
Real humans react to unclear questions with confusion or divergent answers , explicitly answering with how they interpreted the original question ( " I assume you meant .
. . " ) .
Given real world applications will have to deal with the inherent noise and ambiguity of unclear questions , our systems must cope with it .
However , addressing the real world cannot happen by glossing over its complexity .
Complicated Quizbowl is more complex than other datasets .
Unlike other datasets where you just need to decide what to answer , in Quizbowl you also need to choose when to answer the question .
12
While this improves the dataset 's discrimination , it can hurt popularity because you cannot copy / paste code from other QA tasks .
The cumbersome pyramidal structure complicates some questions ( e.g. , what is log base four of sixty - four ) .
A Call to Action
You may disagree with the superiority of Quizbowl as a QA framework ( de gustibus non est disputandum ) .
In this final section , we hope to distill our advice into a call to action regardless of your question format or source .
Here are our recommendations if you want to have an effective leaderboard .
Talk to Trivia Nerds
You should talk to trivia nerds because they have useful information ( not just about the election of 1876 ) .
Trivia is not just the accumulation of information but also connecting disparate facts ( Jennings , 2006 ) .
These skills are exactly those we want computers to develop .
Trivia nerds are writing questions anyway ; we can save money and time if we pool resources .
13 Computer scientists benefit if the trivia community writes questions that are n't trivial for computers to solve ( e.g. , avoiding quotes and named entities ) .
The trivia community benefits from tools that make their job easier : show related questions , link to Wikipedia , or predict where humans will answer .
Likewise , the broader public has unique knowledge and skills .
In contrast to low-paid crowdworkers , public platforms for question answering and citizen science ( Bowser et al. , 2013 ) are brimming with free expertise if you can engage the relevant communities .
For example , the Quora query " Is there a nuclear control room on nuclear aircraft carriers ? " is purportedly answered by someone who worked in such a room ( Humphries , 2017 ) .
As machine learning algorithms improve , the " good enough " crowdsourcing that got us this far may not be enough for continued progress .
Eat Your Own Dog Food
As you develop new question answering tasks , you should feel comfortable playing the task as a human .
Importantly , this is not just to replicate what crowdworkers are doing ( also important ) but to remove hidden assumptions , institute fair metrics , and define the task well .
For this to feel real , you will need to keep score ; have all of your coauthors participate and compare scores .
Again , we emphasize that human and computer skills are not identical , but this is a benefit : humans ' natural aversion to unfairness will help you create a better task , while computers will blindly optimize an objective function ( Bostrom , 2003 ) .
As you go through the process of playing on your question - answer dataset , you can see where you might have fallen short on the goals we outline in Section 3 .
Wo n't Somebody Look at the Data ?
After QA datasets are released , there should also be deeper , more frequent discussion of actual questions within the NLP community .
Part of every post-mortem of trivia tournaments is a detailed discussion of the questions , where good questions are praised and bad questions are excoriated .
This is not meant to shame the writers but rather to help build and reinforce cultural norms : questions should be well - written , precise , and fulfill the creator 's goals .
Just like trivia tournaments , QA datasets resemble a product for sale .
Creators want people to invest time and sometimes money ( e.g. , GPU hours ) in using their data and submitting to their leaderboards .
It is " good business " to build a reputation for quality questions and discussing individual questions .
Similarly , discussing and comparing the actual predictions made by the competing systems should be part of any competition culture - without it , it is hard to tell what a couple of points on some leaderboard mean .
To make this possible , we recommend that leaderboards include an easy way for anyone to download a system 's development predictions for qualitative analyses .
Make Questions Discriminative
We argue that questions should be discriminative ( Section 2.3 ) , and while Quizbowl is one solution ( Section 4 ) , not everyone is crazy enough to adopt this ( beautiful ) format .
For more traditional QA tasks , you can maximize the usefulness of your dataset by ensuring as many questions as possible are challenging ( but not impossible ) for today 's QA systems .
But you can use some Quizbowl intuitions to improve discrimination .
In visual QA , you can offer increasing resolutions of the image .
For other settings , create pyramidality by adding metadata : coreference , disambiguation , or alignment to a knowledge base .
In short , consider multiple versions / views of your data that progress from difficult to easy .
This not only makes more of your dataset discriminative but also reveals what makes a question answerable .
Embrace Multiple Answers or Specify Specificity
As QA moves to more complicated formats and answer candidates , what constitutes a correct answer becomes more complicated .
Fully automatic evaluations are valuable for both training and quick -turnaround evaluation .
In the case annotators disagree , the question should explicitly state what level of specificity is required ( e.g. , September 1 , 1939 vs. 1939 .
Or , if not all questions have a single answer , link answers to a knowledge base with multiple surface forms or explicitly enumerate which answers are acceptable .
Appreciate Ambiguity
If your intended QA application has to handle ambiguous questions , do justice to the ambiguity by making it part of your task - for example , recognize the original ambigu-ity and resolve it ( " did you mean .
. . " ) instead of giving credit for happening to ' fit the data ' .
To ensure that our datasets properly " isolate the property that motivated [ the dataset ] in the first place " ( Zaenen , 2006 ) , we need to explicitly appreciate the unavoidable ambiguity instead of silently glossing over it .
14
This is already an active area of research , with conversational QA being a new setting actively explored by several datasets ( Reddy et al. , 2018 ; Choi et al. , 2018 ) ; and other work explicitly focusing on identifying useful clarification questions ( Rao and Daum ? III ) , thematically linked questions ( Elgohary et al. , 2018 ) or resolving ambiguities that arise from coreference or pragmatic constraints by rewriting underspecified question strings ( Elgohary et al. , 2019 ; Min et al. , 2020 ) . Revel in Spectacle
However , with more complicated systems and evaluations , a return to the yearly evaluations of TRECQA may be the best option .
This improves not only the quality of evaluation ( we can have real- time human judging ) but also lets the test set reflect the build it / break it cycle ( Ruef et al. , 2016 ) , as attempted by the 2019 iteration of FEVER ( Thorne et al. , 2019 ) .
Moreover , another lesson the QA community could learn from trivia games is to turn it into a spectacle : exciting games with a telegenic host .
This has a benefit to the public , who see how QA systems fail on difficult questions and to QA researchers , who have a spoonful of fun sugar to inspect their systems ' output and their competitors ' .
In between full automation and expensive humans in the loop are automatic metrics that mimic the flexibility of human raters , inspired by machine translation evaluations ( Papineni et al. , 2002 ; Specia and Farzindar , 2010 ) or summarization ( Lin , 2004 ) .
However , we should not forget that these metrics were introduced as ' understudies ' - good enough when quick evaluations are needed for system building but no substitute for a proper evaluation .
In machine translation , Laubli et al . ( 2020 ) reveal that crowdworkers cannot spot the errors that neural MT systems make -fortunately , trivia nerds are cheaper than professional translators .
Be Honest in Crowning QA Champions Leaderboards are a ranking over entrants based on a ranking over numbers .
This can be problematic for several reasons .
The first is that single numbers have some variance ; it 's better to communicate estimates with error bars .
While - particularly for leaderboards - it is tempting to turn everything into a single number , there are often different sub-tasks and systems who deserve recognition .
A simple model that requires less training data or runs in under ten milliseconds may be objectively more useful than a bloated , brittle monster of a system that has a slightly higher F 1 ( Dodge et al. , 2019 ) .
While you may only rank by a single metric ( this is what trivia tournaments do too ) , you may want to recognize the highestscoring model that was built by undergrads , took no more than one second per example , was trained only on Wikipedia , etc .
Finally , if you want to make human-computer comparisons , pick the right humans .
Paraphrasing a participant of the 2019 MRQA workshop ( Fisch et al. , 2019 ) , a system better than the average human at brain surgery does not imply superhuman performance in brain surgery .
Likewise , beating a distracted crowdworker on QA is not QA 's endgame .
If your task is realistic , fun , and challenging , you will find experts to play against your computer .
Not only will this give you human baselines worth reporting - they can also tell you how to fix your QA dataset .
. . after all , they 've been at it longer than you have .
In the United States , modern trivia exploded immediately after World War II via countless game shows including College Bowl ( Baber , 2015 ) , the precursor to Quizbowl .
The craze spread to the United Kingdom in a bootlegged version of Quizbowl called University Challenge ( now licensed by ITV ) and pub quizzes ( Taylor et al. , 2012 ) .
The initial explosion , however , was not without controversy .
A string of cheating scandals , most notably the Van Doren ( Freedman , 1997 ) scandal ( the subject of the film Quiz Show ) , and the 1977 entry of Quizbowl into intercollegiate competition forced trivia to " grow up " .
Professional organizations and more " grownup " game shows like Jeopardy !
( the " all responses in the form of a question " gimmick grew out of how some game shows gave contestants the answers ) helped created formalized structures for trivia .
As the generation that grew up with formalized trivia reached adulthood , they sought to make the outcomes of trivia competitions more rigorous , eschewing the randomness that makes for good television .
Organizations like National Academic Quiz Tournaments and the Academic Competition Federation created routes for the best players to help direct how trivia competitions would be run .
In 2019 , these organizations have institutionalized the best practices of " good trivia " described here .
B Simulating the Test Set Needed
We simulate a head- to - head trivia competition where System A and System B have an accuracy a ( probability of getting a question right ) separated by some difference : a A ? a B ? ?.
We then simulate this on a test set of size N -scaled by the effective dataset proportion ?- via draws from two Binomial distributions with success probabilities of a A and a B : R a ? Binomial ( ? N , a A R b ? Binomial ( ? N , a B ) ( 1 ) and see the minimum test set questions ( using an experiment size of 5000 ) needed to detect the better system 95 % of the time ( i.e. , the minimum N such that R a > R b from Equation 1 in 0.95 of the experiments ) .
Our emphasis , however is ? : the smaller the percentage of discriminative questions ( either because of difficulty or because of annotation error ) , the larger your test set must be .
a majority of all MEPs is as good an answer as majority , yet its Exact Match score is 0 .
The problem is not merely one of picking a soft metric ; even its Token - F1 score is merely 0.4 , effectively penalizing a system for giving a more complete answer .
The limitations of Token - F1 become even clearer in light of the following significant span difference : QUESTION :
What measure of a computational problem broadly defines the inherent difficulty of the solution ?
CONTEXT : A problem is regarded as inherently difficult if its solution requires significant resources , whatever the algorithm used .
We agree with the automatic evaluation that a system answering significant resources to this question should not be given full ( and possibly no ) credit as it fails to mention relevant context .
Nevertheless , the Token - F1 of this answer is 0.57 , i.e. , larger than for the insignificant span difference just discussed .
C.2 Missing Gold Answers
We also observed 7 ( out of 100 ) cases of missing gold answers .
As an example , consider QUESTION :
What would someone who is civilly disobedient do in court ?
CONTEXT : Steven Barkan writes that if defendants plead not guilty , " they must decide whether their primary goal will be to win an acquittal and avoid imprisonment or a fine , or to use the proceedings as a forum to inform the jury and the public of the political circumstances surrounding the case and their reasons for breaking the law via civil disobedience . "
[. . . ]
In countries such as the United States whose laws guarantee the right to a jury trial but do not excuse lawbreaking for political purposes , some civil disobedients seek jury nullification .
While annotators did mark two distinct spans as gold answers , they ignored jury nullification which is a fine answer to the question and should be rewarded .
Reasonable people can disagree whether this is a missing answer or if it is excluded by a subtlety in the question 's phrasing .
This is precisely the point-relying on a pre-collected answer strings without a process for adjudicating disagreements in official comparisons does not do justice to the complexity of question answering .
C.3 Bad Questions
We also observed 5 cases of genuinely bad questions .
Consider QUESTION : What library contains the Selmur Productions catalogue ?
CONTEXT : Also part of the library is the aforementioned Selznick library , the Cinerama Productions / Palomar theatrical library and the Selmur Productions catalog that the network acquired some years back
This is an annotation error - the correct answer to the question is not available from the paragraph and would have to be ( the American Broadcast Company 's ) Programming Library .
While we have to live with annotation errors as part of reality , it is not clear that we ought to accept them for official evaluations - any human taking a closer look at the paragraph , as part of an adjudication process , would concede that the question is problematic .
Other cases of ' annotation ' error are more subtle , involving meaning - changing typos , for example : QUESTION : Which French kind [ sic ] issued this declaration ?
CONTEXT :
They retained the religious provisions of the Edict of Nantes until the rule of Louis XIV , who progressively increased persecution of them until he issued the Edict of Fontainebleau ( 1685 ) , which abolished all legal recognition of Protestantism in France
While one could debate whether or not systems ought to be able to do ' charitable ' reinterpretations of the question text , this is part of the point-cases like these warrant discussion and should not be silently glossed over when ' computing the score ' .
Figure 2 : 2 Figure2 : How much test data do you need to discriminate two systems with 95 % confidence ?
This depends on both the difference in accuracy between the systems ( x axis ) and the average accuracy of the systems ( closer to 50 % is harder ) .
Test set creators do not have much control over those .
They do have control , however , over how many questions are discriminative .
If all questions are discriminative ( right ) , you only need 2500 questions , but if three quarters of your questions are too easy , too hard , or have annotation errors ( left ) , you 'll need 15000 .
He 's not Sherlock Holmes , but his address is 221B .
He 's not the Janitor on Scrubs , but his father is played by R. Lee Ermy .
[. . . ]
For ten points , name this misanthropic , crippled , Vicodindependent central character of a FOX medical drama .
ANSWER : Gregory House , MD
15 C Qualitative Analysis Examples
We provide some concrete examples for the classes into which we classified the XLNet - 123 wins over XLNet .
We indicate gold answer spans ( provided by the human annotators ) by underlining ( there may be , the XLNet answer span by bold face , and the XLNet - 123 answer span by italics , combining for tokens shared between spans as is appropriate .
What type of vote must the Parliament have to either block or suggest changes to the Commission 's proposals ?
CONTEXT :
The essence is there are three readings , starting with a Commission proposal , where the Parliament must vote by a majority of all MEPs ( not just those present ) to block or suggest changes C.1 Insignificant and significant span differences QUESTION :
In a British folktale first recorded by Robert Southey , the character Goldilocks finds three beds : one too hard , one not hard enough , and one " just right " .
Using a more sophisticated simulation approach , the TREC 2002 QA test set ( Voorhees , 2003 ) could not discriminate systems with less than a seven absolute score point difference .
Like Jeopardy ! , they are not syntactically questions but still are designed to elicit knowledge - based responses ; for consistency , we still call them questions .
Where to draw the line is a matter of judgment ; computers - which lack common sense-might find questions ambiguous where humans would not .
https://www.naqt.com/rules/#protest
http://www.j-archive.com/showgame.php?game_id=6112
We could not find a paper describing XLNet - 123 , the submission is by http://tia.today.11 Examples in Appendix C.
This complex methodology can be an advantage .
The underlying mechanisms of systems that can play Quizbowl ( e.g. , reinforcement learning ) share properties with other tasks , such as simultaneous translation ( Grissom II et al. , 2014 ; Ma et al. , 2019 ) , human incremental processing ( Levy et al. , 2008 ; Levy , 2011 ) , and opponent modeling ( He et al. , 2016 ) .
Many question answering datasets benefit from the efforts of the trivia community .
Ethically using the data , however , requires acknowledging their contributions and using their input to create datasets ( Jo and Gebru , 2020 , Consent and Inclusivity criterion ) .
Not surprisingly , ' inherent ' ambiguity is not limited to QA ; Pavlick and Kwiatkowski ( 2019 ) show natural language inference has ' inherent disagreements ' between humans and advocate for recovering the full range of accepted inferences .
Disclaimer :
This should be only one of many considerations in deciding on the size of your test set .
Other factors may include balancing for demographic properties , covering linguistic variation , or capturing task -specific phenomena .
