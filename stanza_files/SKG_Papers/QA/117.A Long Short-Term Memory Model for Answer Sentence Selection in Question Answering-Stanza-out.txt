title
A Long Short -Term Memory Model for Answer Sentence Selection in Question Answering
abstract
In this paper , we present an approach that address the answer sentence selection problem for question answering .
The proposed method uses a stacked bidirectional Long- Short Term Memory ( BLSTM ) network to sequentially read words from question and answer sentences , and then outputs their relevance scores .
Unlike prior work , this approach does not require any syntactic parsing or external knowledge resources such as WordNet which may not be available in some domains or languages .
The full system is based on a combination of the stacked BLSTM relevance model and keywords matching .
The results of our experiments on a public benchmark dataset from TREC show that our system outperforms previous work which requires syntactic features and external knowledge resources .
Introduction
A typical architecture of open-domain question answering ( QA ) systems is composed of three high level major steps : a ) question analysis and retrieval of candidate passages ; b ) ranking and selecting of passages which contain the answer ; and optionally c ) extracting and verifying the answer ( Prager , 2006 ; Ferrucci , 2012 ) .
In this paper , we focus on the answer sentence selection .
Being considered as a key subtask of QA , the selection is to identify the answer - bearing sentences from all candidate sentences .
The selected sentences should be relevant to and answer the input questions .
The nature of this task is to match not only the words but also the meaning between question and answer sentences .
For instance , although both of the following sentences contain keywords " Capriati " and " play " , only the first sentence answers the question : " What sport does Jennifer Capriati play ? "
Positive Sentence : " Capriati , 19 , who has not played competitive tennis since November 1994 , has been given a wild card to take part in the Paris tournament which starts on February 13 . "
Negative Sentence : " Capriati also was playing in the U.S. Open semifinals in ' 91 , one year before Davenport won the junior title on those same courts . "
Besides its application in the automated factoid QA system , another benefit of the answer sentence selection is that it can be potentially used to predict answer quality in community QA sites .
The techniques developed from this task might also be beneficial to the emerging real- time user-oriented QA tasks such as TREC LiveQA .
However , usergenerated content can be noisy and hard to parse with off - the-shelf NLP tools .
Therefore , methods that requires less syntactic features are desirable .
Recently , neural network - based distributed sentence modeling has been found successful in many natural language processing tasks such as word sense disambiguation ( McCarthy et al. , 2004 ) , discourse parsing ( Li et al. , 2014 ) , machine translation ( Sutskever et al. , 2014 ; Cho et al. , 2014 ) , and paraphrase detection ( Socher et al. , 2011 ) .
In this paper , we present an approach that leverages the power of deep neural network to address the answer sentence selection problem for question answering .
Our method employs stacked bidirectional Long Short - Term Memory ( BLSTM ) to sequentially read the words from question and answer sentences , and then output their relevance scores .
The full system , when combined with keywords matching , outperforms previous approaches without using any syntactic parsing or external knowledge resources .
Related Work Prior to this work there were other approaches to address the sentence selection task .
The majority of previous approaches focused on syntactic matching between questions and answers .
Punyakanok et al. ( 2004 ) and Cui et al . ( 2005 ) were among the earliest to propose the general tree matching methods based on tree - edit distance .
Subsequent to these two papers , the approach in ( Wang et al. , 2007 ) use quasi-synchronous grammar to match each pair of question and sentence by their dependency trees .
Later , tree kernel function together with a logistic regression model ( Heilman and Smith , 2010 ) or Conditional Random Fields models ( Wang and Manning , 2010 ; Yao et al. , 2013 ) with extracted feature were adopted to learn the associations between question and answer .
Recently , discriminative tree -edit features extraction and engineering over parsing trees are automated in ( Severyn and Moschitti , 2013 ) .
Besides syntactic approaches , lexical semantic model ( Yih et al. , 2013 ) is also used to select answer sentences .
This model is to pair semantically related words based on word relations including synonymy / antonymy , hypernymy / hyponymy and general semantic word similarity .
There were also prior efforts in deep learning neural networks to question answering .
Yih et al. ( 2014 ) focused on answering single-relation factual questions by a semantic similarity model using convolutional neural networks .
Bordes et al. ( 2014 ) jointly embedded words and knowledge base constituents into same vector space to measure the relevance of question and answer sentences in that space .
Iyyer et al. ( 2014 ) worked on the quiz bowl task , which is an application of recursive neural networks for factoid question answering over paragraphs .
The correct answers are identified from a relatively small fixed set of candidate answers which are in the form of entities instead of sentences .
Approach
The goal of this system is to reduce as much as possible the dependency on syntactic features and external resources by leveraging the power of deep recurrent neural network architecture .
The proposed network architecture is trained directly on the word sequences of question and answer passages , and is actually not limited to sentences .
Network Architecture Recurrent Neural Network RNN is an extension of conventional feed -forward neural network , used to deal with variable - length sequence input .
It uses a recurrent hidden state whose activation is dependent on that of the one immediate before .
More formally , given an input sequence x = ( x 1 , x 2 , . . . , x T ) , a conventional RNN updates the hidden vector sequence h = ( h 1 , h 2 , . . . , h T ) and output vector sequence y = ( y 1 , y 2 , . . . , y T ) from t = 1 to T as follows : h t = H( W xh x t + W hh h t?1 + b h ) ( 1 ) y t = W hy h t + b y ( 2 ) where the W denotes weight matrices , the b denotes bias vectors and H ( ? ) is the recurrent hidden layer function .
Long Short -Term Memory ( LSTM )
Due to the gradient vanishing problem , conventional RNNs is found difficult to be trained to exploit long- range dependencies .
In order to mitigate this weak point in conventional RNNs , specially designed activation functions have been introduced .
LSTM is one of the earliest attempts and still a popular option to tackle this problem .
LSTM cell was originally proposed by Hochreiter and Schmidhuber ( 1997 ) .
Several minor modifications have been made to the original LSTM cell since then .
In our approach , we adopted a slightly modified implementation of LSTM in ( Graves , 2013 ) .
In the LSTM architecture , there are three gates ( input i , forget f and output o ) , and a cell memory activation vector c .
The vector formulas for recurrent hidden layer function H in this version of LSTM network are implemented as following : i t = ?( W xi x t + W hi h t?1 + b i ) ( 3 ) f t = ?( W xf x t + W hf h t?1 + b f ) ( 4 ) c t = f t c t?1 + i t ?
( W xc x t + W hc h t?1 + b c ) ( 5 ) o t = ?( W xo x t + W ho h t?1 + b o ) ( 6 ) h t = o t ?( c t ) ( 7 ) where , ? and ? are the cell input and cell output non-linear activation functions which are stated as tanh in this paper .
LSTM uses input and output gates to control the flow of information through the cell .
The input gate should be kept sufficiently active to allow the signals in .
Same rule applies to the output gate .
The forget gate is used to reset the cell 's own state .
In ( Graves , 2013 ) , peephole connections are usually used to connect gates to the cell in tasks requiring precise timing and counting of the internal states .
In our approach , we do n't use peephole connections because the precise timing does not seem to be required .
Bidirectional RNNs
Another weak point of conventional RNNs is their utilization of only previous context with no exploitation of future context .
Unlike conventional RNNs , bidirectional RNNs utilize both the previous and future context , by processing the data from two directions with two separate hidden layers .
One layer processes the input sequence in the forward direction , while the other processes the input in the reverse direction .
The output of current time step is then generated by combining both layers ' hidden vector ? ? h t and ? ? h t by : y t = W? ? h y ? ? h t + W? ? h y ? ? h t + b y .
Stacked RNNs
In a stacked RNN , the output h t from the lower layer becomes the input of the upper layer .
Through the multi-layer stacked network , it is possible to achieve different levels of abstraction from multiple network layers .
There are theoretical supports indicating that a deep , hierarchical model can be more efficient in representing some functions than a shallow one ( Bengio , 2009 ) .
Empirical performance improvement is also observed in LSTM network compared with the shallow network .
Answer Sentence Selection with Stacked BLSTM
As per analysis in section 3.1 , we adopt multilayer stacked bidirectional LSTM RNNs ( rather than conventional RNNs ) to model the answer sentence selection problem as illustrated in Figure 2 .
The words of input sentences are first converted to vector representations learned from word2vec tool ( Mikolov et al. , 2013 ) .
In order to differentiate question q and answer a sentences , we insert a special symbol , < S > , after the question sequence .
Then , the question and answer sentences word vectors are sequentially read by BLSTM from both directions .
In this way , the contextual information across words in both question and answer sentences is modeled by employing temporal recurrence in BLSTM .
Since the LSTM in each direction carries a cell memory while reading the input sequence , it is capable of aggregating the context information and storing it into cell memory vector .
For each time step in the BLSTM layer , the hidden vector or the output vector is generated by combining the cell memory vectors from two LSTM of both sides .
In other words , all the contextual information across the entire sequence ( both question and answer sentences ) has been taken into consideration .
The final output of each time step is the label indicating whether the candidate answer sentence should be selected as the correct answer sentence for the input question .
This objective encourages the BLSTMs to learn a weight matrix that outputs a positive label if there is overlapping context information between two LSTM cell memories .
Mean pooling is applied to all time step outputs during the training .
During the test phase , we collect mean , sum and max poolings as features .
Incorporating Keywords Matching
In order to identify the correct candidate answer sentences , it is crucial to match the cardinal numbers and proper nouns with those occurred in the question .
However , many cardinal numbers and proper nouns are out of the vocabulary ( OOV ) of our word embeddings .
In addition , some proper nouns ' embeddings may bring noise to the matching process .
For example , " Japan " and " China " are two words very close in the embedding space .
It is critical to discriminate these two proper nouns when matching question and answer sentences .
In order to mitigate this weak point of the distributed representations , our full system combined the stacked BLSTM relevance model and exact keywords overlapping baseline by gradient boosted regression tree ( GBDT ) method ( Friedman , 2001 ) .
Experiments Dataset
The answer sentence selection dataset used in this paper was created by Wang et al . ( 2007 ) based on Text REtrieval Conference ( TREC ) QA track ( 8- 13 ) data .
1 Candidate answer sentences were automatically retrieved for each question which is on average associated with 33 candidate sentences .
There are two sets of data provided for training .
One is the full training set containing 1229 questions that are automatically labeled by matching answer keys ' regular expressions .
2
However , the generated labels are noisy and sometimes erroneously mark unrelated sentences as the correct answers solely because those sentences contain answer keys .
Wang et al. ( 2007 ) also provided one small training set contains 94 questions , which were manually corrected for errors .
In our experiments , we use the full training set because it provides significantly more question and answer sentences for learning , even though some of its labels are noisy .
The development and test data sets have 82 and 100 questions , respectively .
Following ( Wang et al. , 2007 ) , candidate answer sentences with over 40 words and questions with only positive or negative candidate answer sentences are removed from 1 http://nlp.stanford.edu/mengqiu/data/ qg-emnlp07-data.tgz
2
Because the original full training dataset is no longer available from the website of the lead author of ( Wang et al. , 2007 ) , we obtained this data re-released from Yao et al . ( 2013 ) : http://cs.jhu.edu/ ?xuchen / packages / jacana-qa-naacl2013-data-results.tar.bz2 evaluation .
3 Evaluation Metric Following previous works on this task , we also use Mean Average Precision ( MAP ) and Mean Reciprocal Rank ( MRR ) as evaluation metrics , which are calculated using the official trec eval evaluation scripts .
Keywords Matching Baseline ( BM25 )
As noted by Yih et al . ( 2013 ) , counting overlapped keywords , especially when re-weighted by idf value of the question word , is a fairly competitive baseline .
Following ( Yih et al. , 2013 ) , our keywords matching baseline also counts the words that occurred in both questions and answer sentences , after excluding stop words and lowering the case .
But , instead of the tf ? idf formula used in ( Yih et al. , 2013 ) , word counts are re-weighted by its idf value using the Okapi BM25 ( Robertson and Walker , 1997 ) formula ( with constants values K 1 = 1.2 and B = 0.75 ) .
Network Setup
The network weights are randomly initialized using a Gaussian distribution ( ? = 0 and ? = 0.1 ) , and the network is trained with the stochastic gradient descent ( SGD ) with momentum 0.9 .
We experimented single - layer unidirectional LSTM , single - layer BLSTM , and three - layer stacked BLSTM .
Each layer of LSTM and BLSTM has a memory size of 500 .
We use 300 - dimensional vectors that were trained and provided by word2vec tool ( Mikolov et al. , 2013 ) using a part of the Google News dataset 4 ( around 100 billion tokens ) .
Results
Table 1 surveys prior results on this task , and places our models in the context of the current state - of - the - art results .
BM25 ) , and pooling values of single - layer unidirectional LSTM ( Single - Layer LSTM ) , single - Layer bidirectional LSTM ( Single - Layer BLSTM ) and three - Layer stacked BLSTM 's ( Three - Layer BLSTM ) outputs .
Gradient boosted regression tree ( GBDT ) method is used to combine features .
LSTM , and performs comparably to previous systems .
In order to mitigate the weak point of the distributed representations previously discussed in section 3.3 , we combine the stacked BLSTM outputs with a keywords matching baseline ( BM25 ) .
Our combined system 's results are statistically significantly better than the keywords matching baseline ( using the Student 's t-test with p < 0.05 ) and outperforms previous state - of - art results .
Conclusion
In this paper , we presented an approach to address the answer sentence selection problem for question answering , by a combination of the stacked bidirectional LSTM model and keywords matching .
The experiments provide strong evidence that distributed and symbolic representations encode complementary types of knowledge , which are all helpful in identifying answer sentences .
Based on the experiment results , we found that our model not only performs better than previous work but most importantly does not require any syntactic features or external resources .
In the future , we would like to further evaluate the models presented in this paper for different tasks , such as answer quality prediction in Community QA , recognizing textual entailment , and machine comprehension of text .
Figure 1 : 1 Figure 1 : An illustration of a stacked bidirectional LSTM network
