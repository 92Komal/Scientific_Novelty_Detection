title
Summarizing Chinese Medical Answer with Graph Convolution Networks and Question - focused Dual Attention
abstract
Online search engines are a popular source of medical information for users , where users can enter questions and obtain relevant answers .
It is desirable to generate answer summaries for online search engines , particularly summaries that can reveal direct answers to questions .
Moreover , answer summaries are expected to reveal the most relevant information in response to questions ; hence , the summaries should be generated with a focus on the question , which is a challenging topic-focused summarization task .
In this paper , we propose an approach that utilizes graph convolution networks and question - focused dual attention for Chinese medical answer summarization .
We first organize the original long answer text into a medical concept graph with graph convolution networks to better understand the internal structure of the text and the correlation between medical concepts .
Then , we introduce a question - focused dual attention mechanism to generate summaries relevant to questions .
Experimental results demonstrate that the proposed model can generate more coherent and informative summaries compared with baseline models .
Introduction Online search engines ( e.g. , Google , Bing ) have a wealth of fresh health - related information , which is appealing for users with medical questions .
Users can enter questions to obtain relevant answers .
However , most answers generated by domain experts are incredibly long , and some are even more than 512 words .
It is intuitive to generate answer summaries , which will benefit both users and search engines .
Such abstract resources are valuable to attract users ' attention and encourage clicking and reading .
Moreover , answer summaries are expected to reveal the most relevant information in response to questions ; hence , the summaries should be focused on the question , which is a challenging topic-focused summarization task , as shown in Table 1 .
( Zhou et al. , 2006 ) first introduces answer summarization as an application of extractive summarization .
designs a questionenhanced pointer - generator network that exploits the correlation information between questionanswer pairs to focus on the essential information when generating answer summaries .
However , those approaches are trained and tested mainly on generic domain datasets , which are not straightforwardly applicable to the medical scenarios .
Moreover , there are still several nontrivial challenges for answer summarization in the medical domain as follows : ?
The original answers can be extremely long , which makes it intractable for vanilla sequence - to-sequence models .
?
The most important parts of the answer not only rely on the keywords of the answer but should also be relative to the question .
For example , for the question listed in Table 1 , note that " ? " ( treat ) is more important than " ? " ( heart ) although the latter occurs more times in the answer .
?
The answer focuses on different concepts of the same question , which makes the summaries quite diverse .
For instance , a summary can consist of multiple plots , such as " ? ? " ( mild patient ) and " ? " ( serious patient ) .
Although the answer summarization task is not new , studies and corpus for the Chinese medical domain are still limited .
To this end , we propose a graph convolution network with question - focused dual attention ( Q- GCN ) model to generate summaries .
Our motivation is that graph - based structure can better represent the correlation between diverse concepts in the answer and capture the plot of the whole text .
Specifically , we decompose the long answer text into several entities / keywords centered clusters of texts and represent the answer with a medical concept graph .
Each vertex of the graph is formed with concept clusters regarding the entities / keywords .
We calculate the edge between vertices via semantic relations between the vertices .
Moreover , to enhance the relevance of the summaries regarding questions , we propose a question - focused dual attention mechanism to extract the primary information from the answer .
We highlight our contributions as follows : ?
We represent the long medical answer with a medical concept graph that explicitly organizes the text into concept-centered vertices .
?
We propose a novel graph convolutional network with question - focused dual attention to generate summaries based on the medical concept graph .
?
Experimental results on our collected largescale Chinese question - answer -summary corpus ( ChMedQA ) and WikiHowQA demonstrate the efficacy of our approach .
Related Work Text Summarization .
Text summarization techniques can be classified into two categories : extractive and abstractive summarization .
Extractive approaches regard summarization as a sentence classification ( Nallapati et al. , 2017 ) or a sequence labeling task ( Cheng and Lapata , 2016 ) to select sentences from the article to form the summary , while abstractive approaches generally employ attention - based encoder-decoder models ( Nallapati et al. , 2016 ; See et al. , 2017 ; Ye et al. , 2020 ) to generate abstractive summaries .
Our method is an abstract approach that can generate more fluent and coherent summaries .
Answer summarization is first introduced by ( Zhou et al. , 2006 ) as an application of summarization .
Subsequently , studies on answer summarization are still regarded as a separate summarization module in QA pipeline ( Song et al. , 2017 ) .
Moreover , query - based summarization methods ( Singh et al. , 2018 ) can also serve as a good solution for this task .
designs a question - enhanced pointer generator network to generate answer summaries .
There are few previous studies ( Kogilavani and Balasubramanie , 2009 ) on medical answer summarization .
As domain knowledge is helpful for generating coherent and informative summaries , previous approaches usually leverage ontologies ( Kogilavani and Balasubramanie , 2009 ) , concepts ( Morales et al. , 2008 ; Schulze and Neves , 2016 ) to summarize answers .
Graph Convolution Networks .
Recently , graph convolution network ( GCN ) models have increasingly attracted attention ( Zhang et al. , 2019 ) , which is beneficial for graph data modeling ( Yin et al. , 2019 ) .
Some existing literature such as SQLto - Text ( Xu et al. , 2018 ) , AMR - to- Text ( Beck et al. , 2018 ; Song et al. , 2018 ; use GCN for generating text .
However , these approaches utilize the graph that already exists , and the input text is very short .
We are faced with extreme long text .
Recently , proposes to model a news article with a topic graph and utilizes the GCN to generate comments automatically .
( Wang et al. , 2020 ) presents a heterogeneous graph - based neural network for extractive summarization .
Different from their approaches , we focus on the medical domain , and the generated summaries should be relevant to the input questions .
To the best of our knowledge , we are the first to apply GCNs to the medical answer summarization task .
Methodology
Problem Definition
Let A denote an answer containing several sentences [ s 1 , s 2 , s 3 , s 4 , ? ? ? , s m ] , where s i is the i-th sentence in the answer and Q denotes the input question .
Our task is to generate an abstractive summary of A that is most relevant to the input question Q.
Framework
Our approach is shown in Figure 1 as an encoderdecoder framework .
Specifically , our encoder aims to convert the original answer text to a medical concept graph .
We propose question - focused dual attention to generate the summary sequence based on the graph and the encoded question .
Medical Concept Graph Construction
We construct our medical concept graph with the medical answer , as shown in Algorithm 1 .
For this paper , we define the medical concepts as phrases / words of medical entities or keywords that are vital components of the text .
Note that the answers from online platforms have a considerable amount of noise .
Some sentences in the answer are even irrelevant to the main question , for example , " ? " ( Thanks for inviting . ) .
Thus , given an input question Q and an answer A , we first perform word segmentation and then medical named entity recognition ( NER ) for the text with a pretrained BERT - CRF ( Devlin et al. , 2018 ) if s i contains ? ? ? then 5 : Assign s i to vectex v k 6 : else 7 : Assign s i to vertex v empty 8 : for vertex v i and v j do 9 : Obtain edge weight : w i , j = ?( v i , v j ) that , we obtain the concepts of the answer , and we associate each sentence in the answer to its corresponding concepts .
Specifically , we assign the sentence to the concept ? if ? appears in the sentence .
Thus , a single sentence will be connected with more than one concept , which may implicitly indicate the correlation between concepts .
We assign sentences that do not contain any of the concepts with an " empty " vertex .
The sentences and the concept ? ? ? consist of the vertex v k in the medical concept graph .
We represent each vertex by the concatenation of the concept and sentence words in the answer .
The edges between vertices denoted as ? in Algorithm 1 can be constructed via a range of approaches .
Whereas , the more sentences mention two concepts together , the closer those two con-cepts are .
To this end , we adopt a structure - based method in this paper .
Specifically , if vertices v i and v j share at least one sentence , we then add an edge e i , j between them , and its weight is obtained with the number of shared sentences .
It is also convenient to utilize content - based approaches , such as TF - IDF , to calculate the similarity .
Node Initialization
We encode the vertex in the medical concept graph with vector u i .
First , we utilize a multi-head selfattention based vertex encoder .
This vertex encoder consists of two modules , namely the embedding module and the self-attention module .
We adopt the regular word embedding of both words and concepts via a sharing embedding lookup table to represent word information .
The regular words refer to words other than concept words .
We also add absolute and relative positional embedding p absolute i , p relative i to represent the position information .
p absolute i aims to encoder the absolute locations of the words and concepts in the answer .
To better learn relative position embedding , we put the concept ? in front of the word sequence .
In this way , the relative position embedding of the concept has the same embedding p 0 .
We add the word embedding w i and position embedding p absolute i , p relative i to get the final embedding u i , formally : u i = w i + p absolute i + p relative i ( 1 ) After that , we feed u i into the self-attention module to obtain the hidden representation a i of each word .
The self-attention can explicitly model the interactions among words to capture the context of the vertex .
We calculate the hidden representation of self-attention layer using Equation 2 to Equation 4 , where Q , K , and V represent the query , key , and value vectors , respectively .
Attention ( Q , K , V ) = softmax QK T V ( 2 ) M ultiHead ( Q , K , V ) = [ head 1 ; ? ? ? ; head h ] W o ( 3 ) head i = Attention QW Qs i , QW Ks i , QW Vs i ( 4 ) Whereas the concept ? is the vertex 's vital information , we adopt the representation of the concept a 0 to represent the entire vertex .
Graph Convolution Networks
We feed the vertex v i into a graph encoder after obtaining the hidden vectors , which explicitly models the graph structure of the constructed medical concept graph .
We use an implementation of the GCN model following ( Kipf and Welling , 2016 ) .
To be specific , we denote the adjacency matrix of the interaction graph as A ? R N ?N , where A ij = w ij ( defined in ? 3.3 ) , and D is a diagonal matrix .
H l+1 = ? D? 1 2 ? D? 1 2 H l W l ( 5 ) ? = A + I N ( 6 ) where I N is the identity matrix , D? 1 2 ?
D is the normalized adjacency matrix , and W l is a learnable weight matrix .
We also add residual connections between layers to avoid over-smoothing .
g l+1 = H l+1 + H l ( 7 ) g out = tanh W o g K ( 8 ) g K is the output of the last layer of GCN .
We add one feed forward layer to the final output of the GCN .
Question - focused Dual Attention
Because the question is a crucial signal , we propose a question - focused dual attention mechanism to emphasize those important vertex and de-emphasize irrelevant vertex .
We utilize the transformer to generate the hidden output of the question q and calculate the first attention weights as : ? j = exp ( ? ( q , g j ) ) . exp ( ? ( q , g k ) ) ( 9 ) where ? is the attention function , q is the hidden representation of question , and g i is the final representation of vertex i .
We utilize the recurrent neural network with attention .
Given the output of the GCN v 0 , v 1 , ? ? ? , v n , and the initial state t 0 , the decoder is able to generate a sequence of summery tokens y 1 , y 2 , ? ? ? , y m .
We calculate the second attention weights as : t i = RN N ( t i?1 , c i?1 ) ( 10 ) ? j = exp ( ?
( t i , g j ) exp ( ? ( t i , g k ) ) ( 11 ) where ? is the attention function , t i is the hidden representation of state i , and g i is the final representation of vertex i .
We combine ?
i and ?
i with the following formula to obtain the final attention weight of each state : ?
i = softmax ( ? i + ( 1 ? ? ) ?
i ) = exp ( ? i + ( 1 ? ? ) ? i ) k? [ 1 , n ] exp ( ? k + ( 1 ? ? ) ? k ) ( 12 )
Here , ? i denotes the final attention weight towards the graph vertex i , and ? ? [ 0 , 1 ] is a soft switch to adjust the importance of two attention weights , ? i and ? i .
There are multiple ways to set the parameter ?.
The simplest one is to treat ? as a hyper-parameter and manually adjust it to obtain the best performance .
Alternatively , ? can also be learned by a neural network automatically .
We select the latter approach because it adaptively assigns different values to ? on different scenarios and achieves better experimental results .
We calculate ? by using the following formula : ? = ? w T [ ? ; ?] + b ( 13 ) where vectors w and scalars b are learnable rameters , and ? is the sigmoid function .
Ultimately , the final attention weights are employed to calculate a weighted sum of the state vectors , resulting in a semantic vector that represents the context : c i = ? j v j ( 14 )
Because the concepts v may appear in the summarization , which is vital information for the long answer , we use the copy mechanism following ( Gu et al. , 2016 ) by summing the predicted word token probability distribution with the attention distribution .
The probability p copy is dynamically calculated using context vector c i and decoding hidden state t i .
y i = softmax ( W o ( tanh ( W ( [ t i ; c i ] ) + b ) ) ) ( 15 ) p copy = ?
( W copy [ t i ; c i ] ) ( 16 )
Experiments
We conduct three kinds of experiments : 1 ) automatic and manual evaluation with ablation study for Chinese medical answer summarization ; 2 ) further experiments on WikiHowQA ; 3 ) model analysis regarding question length , question - focused dual attention , and error analysis .
Dataset and Settings
We collect question and answer pairs from a popular Chinese search engine and split them into train / dev/ test sets with a ratio of 8:1:1 .
We annotate 70 % of the training set by a pretrained sentence ranking model 1 and the rest ( train , dev , test ) by crowdsourcing .
We observe that the medical answer length is excessively long , which is challenging to the sequence - to-sequence model .
To further analyze our approach 's generalization , we conduct experiments on WikiHowQA 2 dataset that has extreme long answers .
WikiHowQA is constructed based on the WikiHow dataset by via filtering out those questions without answers or summaries and those answers with punctuation only .
We detail the average length concerning the answer and the number of samples in both datasets in Table 2 .
We utilize the 100 - dimension pre-trained GloVe embeddings .
The performance ( F1 ) of medical NER and keyword extraction is 0.91 and 0.89 , respectively .
We utilize Stanford CoreNLP 3 and Tex-tRank ( Mihalcea and Tarau , 2004 ) for the Wiki-HowQA dataset .
We only utilize one layer GCN to ease the over-smoothing problem .
We use a dropout rate of 0.2 .
We utilize Adam optimizer to train the parameters with the initial learning rate of 0.0005 .
We train our approach with four epochs .
Baselines and Metrics
We compare the proposed method with the following baselines , including four extractive methods ( Lead3 , TextRank ( Mihalcea and Tarau , 2004 ) , NeuralSum ( Cheng and Lapata , 2016 ) , and NeuSum ( Zhou et al. , 2018 ) ) ; two abstractive methods ( Seq2Seq ( Nallapati et al. , 2016 ) and PGN ( See et al. , 2017 ) ) ; and five query - based methods ( BERT ( Devlin et al. , 2018 ) , XLNet ( Yang et al. , 2019 ) , PGN ( See et al. , 2017 ) , SD2 ( Nema et al. , 2017 ) ) , biASBLSTM ( Singh et al. , 2018 ) , and ASAS .
For BERT / XLNet 4 , we utilize the abstractive summarization schema as the encoder part is replaced with the BERT / XLNet encoder ( question&answer ) and the decoder is trained from scratch .
We also compare variations of our approach : w/o position is the approach without position embedding ; w/o question is the approach without question - focused dual attention ; w/ o GCN is the approach without GCN .
We run each experiment five times and calculate the average performance .
We use ROUGE F1 scores to evaluate the summarization methods .
Main Evaluation Results Main results .
The summarization results are listed in Table 3 .
We notice that XLNet achieves a higher ROUGE score than BERT , which may because XL - Net is an autoregressive approach , while BERT is a denoising autoencoder approach that is not suitable for the generation .
PGN outperforms XLNet , which may because there exist severe OOV problems in the medical domain , while PGN can copy words from the source text .
We also observe that the question - enhanced approaches outperform all the state - of - the - art methods , which demonstrates the effectiveness of incorporating question information .
Besides , by organizing the answer text into the concept graph , our approach further improves the results by a noticeable margin .
Ablation Study Results .
We observe that the approach without position embedding has a slight performance decay , which demonstrates that position information is necessary .
We also notice a severe performance drop when removing questionfocused dual attention , which demonstrates that the question can not be ignored when summarizing answers .
Besides , we observe a performance decay without GCN , which illustrates that graph - based structure can better represent the long text .
Human Evaluation .
We conduct human evaluation to evaluate the generated answer summaries in four aspects : ( 1 ) Informativity :
How well does the summary capture the key information from the original answer ?
( 2 ) Conciseness :
How concise is the summary ?
( 3 ) Readability :
How fluent and coherent is the summary ?
( 4 ) Correlatedness :
How correlated are the summary and the given question ?
We randomly sample 50 answers and generate their summaries by using five methods , namely NeuralSum , Question - enhanced BERT , Questionenhanced XLNet , Question - enhanced PGN , and the proposed approach .
Three data annotators are requested to score each generated summary on a scale of 1 to 5 ( higher the better ) .
Table 4 lists the human evaluation results , which shows that our approach consistently outperforms the other methods in all aspects .
BERT and XLNet achieve relatively low scores in informativity and conciseness , which may be due to the failure of modeling long input text .
However , BERT and XLNet generate more fluent summaries with higher readability scores , which may take advantage of the pre-trained language model .
To intuitively observe the advantage of the proposed method , we randomly select one example to show the results of the answer summary generation .
As shown in Figure 5 , the extractive method Question ??
How to treat premature heartbeat ?
NeuralSum ? ?
Generally , mild patients do not require treatment and can a placebo ; serious patients can take medication or radiofrequency ablation to relieve symptoms .
Question - enhanced PGN ?
The patient should take medication or radiofrequency ablation .
Q-GCN ? ?
Mild patients do not require treatment ; serious patients should take medication or radiofrequency ablation .
( e.g. , NeuralSum ) selects essential sentences from the original answer to form the answer summary , which still contains much insignificant or redundant information .
The abstractive method ( e.g. , PGN ) generates the answer summary from the vocabulary and the original answer , which may omit some concepts and essential information .
Besides , we observe that some baseline models tend to generate general summaries such as " ? " ( the patient should ) when encountering long-tail concepts , which is similar to the dull response problem in dialogue ( Du and Black , 2019 ) .
It significantly affects the performance scores of conciseness and correlatedness .
To address these defects , our approach accounts for the information provided by the question and critical component from the medical concept graph with GCN , which is able to understand the main point of the answer rather than generating high - frequency phrases that are irrelevant or even useless to the given question .
Noticeably , our model learns well to generate answer summaries that are highly related to the given questions , so there is a substantial improvement in terms of informativity , conciseness , and correlatedness .
However , we also notice that our approach receives a slightly lower readability score .
We assume that this is because there exists a similar structure between different models in the decoder .
We observe that our model can not distinguish between similar characters and repeatedly generates the same tokens sometimes .
These phenomena are common in the natural language generation , which reveals the deficiency of understanding world knowledge .
We leave this for future work .
Evaluation on WikiHowQA From Table 6 , we observe : 1 ) our approach still performs better than all baselines , which demonstrates that our approach can apply to the general domain ; 2 ) we notice that the performance improvements are relatively smaller .
We think this may because in the general domain , in addition to entities and keywords , there also exist some verb phrases which may reveal the critical point in the answers .
From the Table 7 we observe : 1 ) our approach performs better than all baselines in human evaluation except the informativity , which may be caused by the negation of some context in the answers ; 2 ) we notice the significant performance improvement in conciseness and correlatedness , which further proves that the graph-structure can better understand the main point of the answer .
Analysis Length of Answer .
To validate the effectiveness of the proposed method on long-sentence answer summarization , we sample the test set according to the length of the answer .
As shown in Figure 2 , we compare our approach with two baseline methods , SEQ2SEQ , and NEUSUM , by measuring the ROUGE -L .
We observe that our approach is more efficient , especially for long answers .
For answers that are shorter than 100 words , SEQ2SEQ and NEUSUM are marginally better than our approach , which indicates that the summary may have lost some information for short answers .
However , the performance of these two methods deteriorates with an increase in the answer length , whereas our approach maintains excellent stability .
In summary , explicitly organizing the text into a graph-structure can better represent long text .
Question - Focused Dual Attention .
To evaluate whether our question guides the procedure of answer summary generation , we deliberately change the question with the same answer and obtain different summarization results , as shown in Table 8 .
We observe that our model can control the summarization of answers with different questions , indicating the efficacy of question - focused dual attention .
For example , by changing the question from " ? " ( pay attention ) to " ? ? " ( what fruits to eat ) , we generate results which directly address the question .
However , when changing the original question to a question that cannot be summarized ( cannot find an answer regarding the question ) , our approach fails to generate concise summaries .
We also observe that our approach without question - focused dual attention generates trivial summaries , which include redundant information and miss the key points relevant to the question .
Those observations demonstrate that question - focused dual attention can enhance generating summaries relevant to questions .
Error Analysis .
We conduct an error analysis of our approach .
We first random sample 100 test instances with wrong entities / keywords .
Surprisingly , we observe that 80 % of them generate coherent and informative summaries , which shows that incorrect entities / keywords have little influences on the quality of summarization .
We further analyze the wrong instances and divide them into five categories .
First , our model can generate fluency summaries with significantly long sentences but may fail to generate well with some short answers .
Second , our model cannot handle time and numbers .
For example , when summarizing the answer " ? ?... "
( Normally , do not need to take medications and will begin to swell on its own in about three days ... ) with the question " ? ? "
( How many days can I recover if stung by a bee ) , our model cannot provide reasonable summaries because it does not understand what " ? " ( how many days ) is .
Third , our model is vulnerable , to some extent , to adversarial attacking , such as adding a negative modifier " ? " ( not ) in the question ; our model fails to understand the true meaning and yields poor results .
Finally , we find that our model is sensitive to typos and some extreme long-tail terminologies , such as " ? " ( stoma chache ) and " ? " ( vaginal B-ultrasound ) .
Conclusion and Future Work
In this paper , we propose an approach of graph convolution network with question - focused dual attention to generate Chinese answer summaries .
Experimental results indicate that our model can summarize more coherently and informatively , thereby showing that organizing long text with a graph structure is beneficial and question - focused dual attention further improves the informativeness and correlation .
In the future , we plan to 1 ) exploit knowledge such as commonsense to generate logical summaries ; 2 ) investigate efficient methodologies to model the correlation between concepts ; 3 ) apply our approach to similar applications such multiple document summarization .
Figure 2 : 2 Figure 2 : Model performance # answer length .
