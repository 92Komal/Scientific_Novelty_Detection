title
QUASE : Question - Answer Driven Sentence Encoding
abstract
Question - answering ( QA ) data often encodes essential information in many facets .
This paper studies a natural question : Can we get supervision from QA data for other tasks ( typically , non - QA ones ) ?
For example , can we use QAMR to improve named entity recognition ?
We suggest that simply further pre-training BERT is often not the best option , and propose the questionanswer driven sentence encoding ( QUASE ) framework .
QUASE learns representations from QA data , using BERT or other state- ofthe - art contextual language models .
In particular , we observe the need to distinguish between two types of sentence encodings , depending on whether the target task is a single - or multisentence input ; in both cases , the resulting encoding is shown to be an easy - to - use plugin for many downstream tasks .
This work may point out an alternative way to supervise NLP tasks .
1
Introduction
It is labor-intensive to acquire human annotations for NLP tasks which require research expertise .
For instance , one needs to know thousands of semantic frames in order to provide semantic role labelings ( SRL ) ( Palmer et al. , 2010 ) .
It is thus an important research direction to investigate how to get supervision signals from indirect data and improve one 's target task .
This paper studies the case of learning from question - answering ( QA ) data for other tasks ( typically not QA ) .
We choose QA because ( 1 ) a growing interest of QA has led to many large-scale QA datasets available to the community ; ( 2 ) a QA task often requires comprehensive understanding of language and may encode rich information that is useful for other tasks ; ( 3 ) it is much easier to answer questions relative to a sentence than to annotate linguistics phenomena in it , making this a plausible supervision signal ( Roth , 2017 ) .
There has been work showing that QA data for task A can help another QA task T , conceptually by further pre-training the same model on A ( an often larger ) before training on T ( a smaller ) ( Talmor and Berant , 2019 ; Sun et al. , 2019 ) .
However , it remains unclear how to use these QA data when the target task does not share the same model as the QA task , which is often the case when the target task is not QA .
For instance , QA - SRL ( He et al. , 2015 ) , which uses QA pairs to represent those predicateargument structures in SRL , should be intuitively helpful for SRL parsing , but the significant difference in their surface forms prevents us from using the same model in both tasks .
The success of modern language modeling techniques , e.g. , ELMo ( Peters et al. , 2018 ) , BERT ( Devlin et al. , 2019 ) , and many others , has pointed out an alternative solution to this problem .
That is , to further pre-train 2 a neural language model ( LM ) on these QA data in certain ways , obtain a sentence encoder , and use the sentence encoder for the target task , either by fine- tuning or as additional feature vectors .
We call this general framework question - answer driven sentence encoding ( QUASE ) .
A straightforward implementation of QUASE is to first further pre-train BERT ( or other LMs ) on the QA data in the standard way , as if this QA task is the target , and then fine - tune it on the real target task .
This implementation is technically similar to STILTS ( Phang et al. , 2018 ) , except that STILTS is mainly further pre-trained on textual entailment ( TE ) data .
However , similar to the observations made in STILTS and their follow - up works ( Wang et al. , 2019 ) , we find that additional QA data does not necessarily help the target task using the implementation above .
While it is unclear how to predict this behaviour , we do find that this happens a lot for tasks whose input is a single sentence , e.g. , SRL and named entity recognition ( NER ) , instead of a sentence pair , e.g. , TE .
This might be because QA is itself a paired - sentence task , and the implementation above ( i.e. , to further pre-train BERT on QA data ) may learn certain attention patterns that can transfer to another paired - sentence task more easily than to a single-sentence task .
Therefore , we argue that , for single-sentence target tasks , QUASE should restrict the interaction between the two sentence inputs when it further pre-trains on QA data .
We propose a new neural structure for this and name the resulting implementation s-QUASE , where " s " stands for " single ; " in contrast , we name the straightforward implementation mentioned above p-QUASE for " paired . "
Results show that s-QUASE outperforms p-QUASE significantly on 3 single-sentence tasks - SRL , NER , and semantic dependency parsing ( SDP ) - indicating the importance of this distinction .
Let QUASE
A be the QUASE further pre-trained on QA data A .
We extensively compare 6 different choices of A : TriviaQA ( Joshi et al. , 2017 ) , NewsQA ( Trischler et al. , 2017 ) , SQuAD ( Rajpurkar et al. , 2016 ) , relation extraction ( RE ) dataset in QA format ( QA - RE for short ) ( Levy et al. , 2017 ) , Large QA -SRL ( FitzGerald et al. , 2018 ) , and QAMR .
Interestingly , we find that if we use s-QUASE for singlesentence tasks and p-QUASE for paired - sentence tasks , then QUASE QAMR improves all 7 tasks 3 in low resource settings , with an average error reduction rate of 7.1 % compared to BERT .
4
While the set of tasks we experimented with here is nonexhaustive , we think that QUASE QAMR has the potential of improving on a wide range of tasks .
This work has three important implications .
First , it provides supporting evidence to an important alternative to supervising NLP tasks : using QA to annotate language , which has been discussed in works such as QA - SRL , QAMR , and QA - RE .
If it is difficult to teach annotators the formalism of a certain task , perhaps we can instead collect QA data that query the target phenomena and thus get supervision from QA for the original task ( and possibly more ) .
Second , the distinction between s-QUASE and p-QUASE suggests that sentence encoders should consider some properties of the target task ( e.g. , this work distinguishes between single - and multi-sentence tasks ) .
Third , the good performance of QUASE QAM R suggests that predicate - argument identification is an important capability that many tasks rely on ; in contrast , many prior works observed that only language modeling would improve target tasks generally .
QA Driven Sentence Encoding
This work aims to find an effective way to use readily available QA data to improve a target task that is typically not QA .
A natural choice nowadaysgiven the success of language models - is to further pre-train sentence encoders , e.g. BERT , on QA data in certain ways , and then use the new encoder in a target task .
This general framework is called QUASE in this work , and the assumption is that the sentence encoders learned from QA data have useful information for the target task .
A straightforward implementation of QUASE is to further pre-train BERT on QA data in the standard way , i.e. , fine - tune BERT as if this QA dataset is the target task , and then fine - tune BERT on the real target task .
However , we find that this straightforward implementation is less effective or even negatively impacts target tasks with single -sentence input ; similar observations were also made in STILTS ( Phang et al. , 2018 ) and its follow-ups ( Wang et al. , 2019 ) :
They further pretrain sentence encoders , e.g. , ELMo , BERT , and GPT ( Radford et al. , 2018 ) , on TE data and find that it is not effective for the syntax - oriented CoLA task and the SST sentiment task in GLUE , which are both single -sentence tasks ( Wang et al. , 2018 ) .
One plausible reason is that the step of further pre-training on QA data does not take into account some properties of the target task , for instance , the number of input sentences .
QA is inherently a paired - sentence task ; a typical setup is , given a context sentence and a question sentence , predict the answer span .
Further pre-training BERT on QA data will inevitably learn how to attend to the context given the question .
This is preferable when the target task is also taking a pair of sentences as input , while it may be irrelevant or harmful for single -sentence tasks .
It points out that we may need two types of sentence encodings when further pre-training BERT on QA data , depending on the type of the target task .
The following subsection discusses this issue in detail .
Two Types of Sentence Encodings Standard sentence encoding is the problem of converting a sentence S=[w 1 , w 2 , ? ? ? , w n ] to a se- quence of vectors h( S ) = [ h 1 , h 2 , ? ? ? , h n ] ( e. g. , skipthoughts ( Kiros et al. , 2015 ) ) .
Ideally , h( S ) should encode all the information in S , so that it is taskagnostic : given a target task , one can simply probe h( S ) and retrieve relevant information .
In practice , however , only the information relevant to the training task of h( S ) is kept .
For instance , when we have a task with multi-sentence input ( e.g. , QA and TE ) , the attention pattern
A among these sentences will affect the final sentence encoding , which we call h A ( S ) ; in comparison , we denote the sentence encoding learned from single - sentence tasks by h( S ) , since there is no cross-sentence attention A .
In a perfect world , the standard sentence encoding h( S ) expresses also the conditional sentence encoding h A ( S ) .
However , we believe that there is a trade- off between the quality and the quantity of semantic information a model can encode .
Our empirical results corroborate this conclusion and more details can be found in Appendix A.2 .
The distinction between the sentence encodings types may explain the negative impact of using QA data for some single -sentence tasks :
Further pre-training BERT on QA data essentially produces a sentence encoding with cross-sentence attentions h A ( S ) , while the single-sentence tasks expect h ( S ) .
These two sentence encodings may be very different :
One view is from the theory of information bottleneck ( Tishby et al. , 1999 ; Tishby and Zaslavsky , 2015 ) , which argues that training a neural network on a certain task is extracting an approximate minimal sufficient statistic of the input sentences with regard to the target task ; information irrelevant to the target task is maximally compressed .
In our case , this corresponds to the process where the conditional sentence encoding compresses the information irrelevant to the relation , which will enhance the quality but reduce the quantity of the sentence information .
Two Implementations of QUASE
In order to fix this issue , we need to know how to learn h( S ) from QA data .
However , since QA is a paired - sentence task , the attention pattern between the context sentence and the question sentence is important for successful further pre-training on QA .
Therefore , we propose that if the target task is single -sentence input , then fur-ther pre-training on QA data should also focus on single-sentence encodings in the initial layers ; the context sentence should not interact with the question sentence until the very last few layers .
This change is expected to hurt the capability to solve the auxiliary QA task , but it is later proved to transfer better to the target task .
This new treatment is called s-QUASE with " s " representing " singlesentence , " while the straightforward implementation mentioned above is called p-QUASE where " p " means " paired - sentence . "
The specific structures are shown in Fig. 1 .
s-QUASE
The architecture of s-QUASE is shown in Fig. 1 ( a ) .
When further pre-training it on QA data , the context sentence and the question sentence are fed into two pipelines .
We use the same Sentence2 Question and Question2 Sentence attention as used in BiDAF .
Above that , " Sentence Modeling , " " Question Modeling , " and " Interaction Layer " are all bidirectional transformers ( Vaswani et al. , 2017 ) with 2 layers , 2 layers , and 1 layer , respectively .
Finally , we use the same classification layer as BERT , which is needed for training on QA data .
Overall , this implementation restricts interactions between the paired - sentence input , especially from the question to the context , because when serving the target task , this attention will not be available .
Using s-QUASE in target tasks .
Given a sentence S , s-QUASE can provide a sequence of hidden vectors h( S ) , i.e. , the output of the " Sentence Modeling " layer in Fig. 1 ( a ) .
Although h( S ) does not rely on the question sentence , h( S ) is optimized so that upper layers can use it to handle those questions in the QA training data , so h ( S ) indeed captures information related to the phenomena queried by those QA pairs .
For single- sentence tasks , we use h( S ) from s-QUASE as additional features , and concatenate it to the word embeddings in the input layer of any specific neural model .
5
p-QUASE
The architecture of p-QUASE is shown in Fig. 1 ( b ) , which is the standard way of pre-training BERT .
That is , when further pre-training it on QA data , the context sentence and the question sentence form a single sequence ( separated by special tokens ) and are fed into BERT .
Using p-QUASE in target tasks .
Given a sentence pair S ( concatenated ) , p-QUASE produces h A ( S ) , i.e. , the output of the BERT module in Fig. 1 ( b ) .
One can of course continue fine- tuning p-QUASE on the target task , but we find that adding p-QUASE to an existing model for the target task is empirically better ( although not very significant ) ; specifically , we try to add h A ( S ) to the final layer before the classification layer , and we also allow p-QUASE to be updated when training on the target task , although it is conceivable that other usages may lead to even stronger results .
For instance , when the target task is token classification , e.g. , MRC , we can simply concatenate the vectors of h A ( S ) at each timestamp to any existing model ; when the target task is sentence classification , e.g. , TE , we apply max-pooling and average - pooling on h A ( S ) , respectively , and concatenate the two resulting vectors to any existing model before the final classification layer .
Related Work on Sentence Encoding Modern LMs are essentially sentence encoders pretrained on unlabeled data and they outperform early sentence encoders such as skip-thoughts ( Kiros et al. , 2015 ) .
While an LM like BERT can handle lexical and syntactic variations quite well , it still needs to learn from some annotations to acquire the " definition " of many tasks , especially those requiring complex semantics ( Tenney et al. , 2019 ) .
Although we extensively use BERT here , we think that the specific choice of LM is orthogonal to our proposal of learning from QA data .
Stronger LMs , e.g. , RoBERTa ( Liu et al. , 2019 ) or XLNet ( Yang et al. , 2019 ) , may only strengthen the proposal here .
This is because a stronger LM represents unlabeled data better , while the proposed work is about how to represent labeled data better .
CoVe ( McCann et al. , 2017 ) is another attempt to learn from indirect data , translation data specifically .
However , it does not outperform ELMo or BERT in many NLP tasks ( Peters et al. , 2018 ) and probing analysis ( Tenney et al. , 2019 ) .
In contrast , our QUASE will show stronger experimental results than BERT on multiple tasks .
In addition , we think QA data is generally cheaper to collect than translation data .
The proposed work is highly relevant to Phang et al . ( 2018 ) and their follow - up works ( Wang et al. , 2019 ) to improve another target task .
The key differences are as follows :
First , we distinguish two types of sentence encodings , which provide explanation to their puzzle that sentence - pair tasks seem to benefit more from further pre-training than single -sentence tasks do .
Second , they only focus on fine-tuning based methods which cannot be easily plugged in many single -sentence tasks such as SRL and Coref , while we analyze both fine-tuning based and feature - based approaches .
Third , they mainly use TE signals for further pre-training , and evaluate their models on GLUE ( Wang et al. , 2018 ) which is a suite of tasks very similar to TE .
Our work instead makes use of QA data to help tasks that are typically not QA .
Fourth , from their suite of further pre-training tasks , they observe that only further pre-training on language modeling tasks has the power to improve a target task in general , while we find that QAMR may also have this potential , indicating the universality of predicate - argument structures in NLP tasks .
Our work is also related to Sentence -BERT ( Reimers and Gurevych , 2019 ) in terms of providing a better sentence representation .
However , their focus was deriving semantically meaningful sentence embeddings that can be compared using cosine-similarity , which reduces the computational cost of finding the most similar pairs .
In contrast , QUASE provides a better sentence encoder in the same format as BERT ( a sequence of word embeddings ) to better support tasks that require complex semantics .
Applications of QUASE
In this section , we conduct thorough experiments to show that QUASE is a good framework to get supervision from QA data for other tasks .
We first give an overview of the datasets and models used in these experiments before diving into the details of each experiment .
Specifically , we use PropBank ( Kingsbury and Palmer , 2002 ) ( SRL ) , the dataset from the SemEval ' 15 shared task ( Oepen et al. , 2015 ) with DELPH -IN MRS - Derived Semantic Dependencies target representation ( SDP ) , CoNLL'03 ( Tjong Kim Sang and De Meulder , 2003 ) ( NER ) , the dataset in SemEval '10 Task 8 ( Hendrickx et al. , 2009 )
For single -sentence tasks , we use both simple baselines ( e.g. , BiLSTM and CNN ; see Appendix B.1 ) and near-state - of - the - art models published in recent years .
As in ELMo , we use the deep neural model in for SRL , the model in Peters et al . ( 2018 ) for NER , and the end-to - end neural model in for Coref .
We also use the biaffine network in Dozat and Manning ( 2018 ) for SDP but we removed part- of-speech tags from its input , and the attention - based BiLSTM in Zhou et al . ( 2016 ) is the strong baseline for RE .
In addition , we replace the original word embeddings in these models ( e.g. , GloVe ( Pennington et al. , 2014 ) ) by BERT .
Throughout this paper , we use the pre-trained case-insensitive BERT - base implementation .
More details on our experimental setting can be found in Appendix B , including the details of simple models in B.1 , some common experimental settings of QUASE in B.2 , and s-QUASE combined with other SOTA embeddings ( ELMo and Flair ( Akbik et al. , 2018 ) ) in B.3 .
Necessity of Two Representations
We first consider a straightforward method to use QA data for other tasks - to further pre-train BERT on these QA data .
We compare BERT further pre-trained on QAMR ( denoted by BERT QAM R ) with BERT on two single-sentence tasks ( SRL and RE ) and two paired - sentence tasks ( TE and MRC ) .
We use a feature - based approach for singlesentence tasks and a fine-tuning approach for paired - sentence tasks .
The reason is two -fold .
On the one hand , current SOTAs of all singlesentence tasks considered in this paper are still 2 : Probing results of the sentence encoders from s-QUASE and p-QUASE .
In all tasks , we fix the model QUASE and use the sentence encodings as input feature vectors for the model of each task .
In order to keep the model structure as simple as possible , we use BiLSTM for SRL , NER , and TE , Biaffine for SDP , and BiDAF for MRC .
We compare on 10 % and 100 % of the data in all tasks except TE , where we use 30 % to save run-time .
feature - based .
How to efficiently use sentence encoders ( e.g. BERT ) in a fine-tuning approach for some complicated tasks ( e.g. SRL and SDP ) is unclear .
On the other hand , the fine-tuning approach shows great advantage over feature - based on many paired - sentence tasks ( e.g. TE and MRC ) .
Similar to Phang et al. ( 2018 ) , we find in Table 1 that the two single-sentence tasks benefit less than the two paired - sentence tasks from BERT QAM R , which indicates that simply " further pre-training BERT " is not enough .
We then compare s-QUASE QAM R and p-QUASE QAM R on three single-sentence tasks ( SRL , SDP and NER ) and two paired - sentence tasks ( TE and MRC ) to show that it is important to distinguish two types of sentence representations .
Rather than concatenating two embeddings as proposed in Sec. 2.2 , here we replace BERT embeddings with QUASE embeddings for convenience .
The results are shown in Table 2 .
We find that s-QUASE has a great advantage over p-QUASE on single-sentence tasks and p-QUASE is better than s-QUASE on paired - sentence tasks .
The proposal of two types of sentence encoders tackles the problem one may encounter when there is only further pre-training BERT on QAMR for single -sentence tasks .
In summary , it is necessary to distinguish two types of sentence representations for singlesentence tasks and paired - sentence tasks .
Sample Complexity of QUASE
Data Choice for Further Pre-training
We compare BERT with QUASE further pretrained with the same numbre of QA pairs on 6 different QA datasets ( TriviaQA ( Joshi et al. , 2017 ) , NewsQA ( Trischler et al. , 2017 ) , SQuAD , QA -RE ( Levy et al. , 2017 ) , Large QA -SRL ( FitzGerald et al. , 2018 ) , and QAMR ) .
s-QUASE further pretrained on different QA datasets are evaluated on four single -sentence tasks in a feature - based approach : SRL , SDP , NER and RE .
p-QUASE further pre-trained on different QA datasets is evaluated on one task ( TE ) in a fine-tuning approach .
In Table 3 , we find that the best options are quite different across different target tasks , which is expected because a task usually benefits more from a more similar QA dataset .
However , we also find that QAMR is generally a good furtherpre -training choice for QUASE .
This is consistent with our intuition : First , QAMR has a simpler concept class than other paragraph - level QA datasets , such as TriviaQA , NewsQA and SQuAD .
It is easier for QUASE to learn a good representation with QAMR to help sentence - level tasks .
Second , QAMR is more general than other sentence - level QA datasets , such as QA - RE and Large QA - SRL .
7
Therefore , we think that the capability to identify predicate - argument structures can generally help many sentence - level tasks , as we discuss next .
The Effectiveness of QUASE
Here we compare QUASE QAM R with BERT on 5 single-sentence tasks and 2 paired - sentence tasks , where QUASE QAM R is further pre-trained on the training set ( 51 K QA pairs ) of the QAMR dataset .
As shown in Table 4 , we find that QUASE QAM R 7 Although the average performance of QUASEQAMR on five tasks is slightly below QUASELarge QA ?
SRL , for which the benefit mostly comes from SRL .
QUASE is mainly designed to improve a lot of tasks , so QAMR is a better choice in our setup , but in practice , we do not limit QUASE to any specific QA dataset and one can use the best one for corresponding target tasks .
has a better performance than BERT on both singlesentence tasks and paired - sentence tasks , especially in the low-resource setting 8 , indicating that QUASE QAM R can provide extra features compared to BERT .
Admittedly , the improvement in the " Full " setting is not significantly large , but we think that this is expected because large direct training data are available ( such as SRL with 278 K training examples in OntoNotes ) .
However , it is still promising that 51 K indirect QA pairs can improve downstream tasks in the low-resource setting ( i.e. several thousands direct training examples ) .
That is because they help the scalability of machine learning methods , especially for some specific domains or some low-resource languages where direct training data do not exist in large scale .
Discussion
In this section we discuss a few issues pertaining to improving QUASE by using additional QA datasets and the comparison of QUASE with related symbolic representations .
Further
Comparison with Symbolic Meaning Representations Traditional ( symbolic ) shallow meaning representations such as SRL and AMR , suffer from having a fixed set of relations one has to commit to .
Moreover , inducing these representations requires costly annotation by experts .
Proposals such as QA - SRL , QAMR , semantic proto-roles ( Reisinger et al. , 2015 ) , and universal dependencies ( White et al. , 2016 ) avoid some of these issues by using natural language annotations , but it is unclear how other tasks can take advantage of them .
QUASE is proposed to facilitate inducing distributed representations instead of symbolic representations from QA signals ; it benefits from cheaper annotation and flexibility , and can also be easily used in downstream tasks .
The following probing analysis , based on the Xinhua subset in the AMR dataset , shows that s-QUASE QAM R embeddings encode more semantics related to AMR than BERT embeddings .
Specifically , we use the same edge probing model as Tenney et al . ( 2019 ) , and find that the probing accuracy ( 73.59 ) of s-QUASE QAM
R embeddings is higher than that ( 71.58 ) of BERT .
At the same time , we find that p-QUASE QAM R can achieve 76.91 F1 on the PTB set of QA - SRL , indicating that p-QUASE QAM R can capture enough information related to SRL to have a good zero-shot SRL performance .
More details can be found in Appendix C.1 .
Another fact worth noting is that AMR can be used to improve downstream tasks , such as MRC ( Sachan and Xing , 2016 ) , TE ( Lien and Kouylekov , 2015 ) , RE ( Garg et al. , 2019 ) and SRL ( Song et al. , 2018 ) .
The benefits of QUASE QAM R on downstream tasks show that we can take advantage of AMR by learning from much cheaper QA signals dedicated to it .
Difficulties in Learning Symbolic Representations from QA Signals QUASE is designed to learn distributed representations from QA signals to help down-stream tasks .
We further show the difficulties of learning two types of corresponding symbolic representations from QA signals , which indicates that the two other possible methods are not as tractable as ours .
One option of symbolic representation is the QAMR graph .
show that question generation for QAMR representations can only achieve a precision of 28 % , and a recall of 24 % , even with fuzzy matching ( multi-BLEU 10 > 0.8 ) .
Furthermore , it is still unclear how to use the complex QAMR graph in downstream tasks .
These results indicate that learning a QAMR parser for down-stream tasks is mainly hindered by question generation , and how to use the full information of QAMR for downstream tasks is still unclear .
Another choice of symbolic representation is AMR , since QAMR is proposed to replace AMR .
We consider a simpler setting , learning an SRL parser from Large QA - SRL .
We propose three models in different perspectives , but the best performance of them is only 54.10 F1 , even with fuzzy matching ( Intersection / Union ? 0.5 ) .
More details can be found in Appendix C.2 .
Although a lot of methods ( Khashabi et al. , 2018 ; Marcheggiani and Titov , 2017 ; Strubell et al. , 2018 ) can be adopted to use SRL / AMR in downstream tasks , the difficulty of learning a good SRL / AMR parser from QA signals hinders this direction .
The difficulties of learning the two types of symbolic representations from QA signals indicate that our proposal of learning distributed representations from QA signals is a better way of making use of the latent semantic information in QA pairs for down-stream tasks .
Conclusion
In this paper , we investigate an important problem in NLP : Can we make use of low-cost signals , such as QA data , to help related tasks ?
We retrieve signals from sentence - level QA pairs to help NLP tasks via two types of sentence encoding approaches .
For tasks with a single-sentence input , such as SRL and NER , we propose s-QUASE that provides latent sentence - level representations ; for tasks with a sentence pair input , such as TE and MRC we propose p-QUASE , that generates latent 10 An average of BLEU1 - BLEU4 scores .
representations related to attentions .
Experiments on a wide range of tasks show that the distinction of s-QUASE and p-QUASE is highly effective , and QUASE QAM R has the potential to improve on many tasks , especially in the low-resource setting .
