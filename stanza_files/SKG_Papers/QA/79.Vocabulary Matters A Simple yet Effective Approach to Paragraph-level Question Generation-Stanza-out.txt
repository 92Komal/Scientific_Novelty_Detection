title
Vocabulary Matters : A Simple yet Effective Approach to Paragraph-level Question Generation
abstract
Question generation ( QG ) has recently attracted considerable attention .
Most of the current neural models take as input only one or two sentences and perform poorly when multiple sentences or complete paragraphs are given as input .
However , in real-world scenarios , it is very important to be able to generate high-quality questions from complete paragraphs .
In this paper , we present a simple yet effective technique for answer -aware question generation from paragraphs .
We augment a basic sequence - to-sequence QG model with dynamic , paragraph -specific dictionary and copy attention that is persistent across the corpus , without requiring features generated by sophisticated NLP pipelines or handcrafted rules .
Our evaluation on SQuAD shows that our model significantly outperforms current state - of - theart systems in question generation from paragraphs in both automatic and human evaluation .
We achieve a 6 - point improvement over the best system on from 16.38 to 22.62 .
Introduction and Related work Automatic question generation ( QG ) from text aims to generate meaningful , relevant , and answerable questions from a given textual input .
Owing to its applicability in conversational systems such as Cortana , Siri , chatbots , and automated tutoring systems , QG has attracted considerable interest in both academia and industry .
Recent neural network - based approaches ( Du et al. , 2017 ; Kumar et al. , 2018a , b ; Du and Cardie , 2018 ; Zhao et al. , 2018 ; Song et al. , 2018 ; Subramanian et al. , 2018 ; Tang et al. , 2017 ; Wang et al. , 2017 ) represent the state - of - the - art in question generation .
Most of these techniques learn to generate questions from short text , i.e. , one or two sentences ( Du et al. , 2017 ; Kumar et al. , 2018a , b ; Du and Cardie , 2018 ) .
On the other hand , the ability to generate high-quality questions from longer text such as from multiple sentences or from a paragraph in its entirety , is more useful in real-world settings .
However , given that a paragraph contains a longer context and more information than a sentence , it is a significantly more challenging problem to generate questions around a longer context .
In figure 1 we present one motivating example demonstrating why the model needs information more than just a single sentence for generating question a meaningful and relevant question .
As we can see in figure 1 , question 2 , question generated by our model use multiple sentences as context .
Du et al. ( 2017 ) recently observed that 20 % of the questions in the SQuAD dataset ( Rajpurkar et al. , 2016 ) require paragraphlevel information to answer them .
For the same reason , it is intuitive to conclude that the ability to consider the complete context ; however long it may be , is critical for generating high-quality questions .
Legislative power in Warsaw is vested in a unicameral Warsaw City Council ( Rada Miasta ) , which comprises 06 members .
Council members are elected directly every four years .
Like most legislative bodies , the City Council divides itself into committees which have the oversight of various functions of the city government .
Bills passed by a simple majority are sent to the mayor ( the President of Warsaw ) , who may sign them into law .
If the mayor vetoes a bill , the Council has 30 days to override the veto by a two -thirds majority vote .
Human Generated :
How many days does the Council have to override the mayor 's veto ?
Our Model :
How long does it take to override the veto ?
Figure 1 : Examples of ground -truth questions and questions generated by our model from the same paragraph .
Each question and its corresponding answer are highlighted using the same color .
Zhao et al. ( 2018 ) very recently proposed a technique ( referred to MPGSN here ) for paragraph - level question generation using a max out pointer mechanism and a gated self-attention encoder .
Their best model achieves BLEU -4 of 16.38 on SQuAD with paragraphs as input .
Compared to ( Zhao et al. , 2018 ) , our model has less number of parameters ( making it more computationally efficient ) , is relatively easy to train and is somewhat deterministically biased toward the generation of important words in the input paragraph .
In this paper , we propose a simple yet effective paragraph - level question generation technique .
We augment the standard sequence - to-sequence model based on bidirectional LSTM with two components : ( 1 ) a dynamic , paragraph -specific dictionary and ( 2 ) a copy attention mechanism that is persistent across paragraphs .
Our evaluation on SQuAD shows significant improvement over MPGSN in automatic evaluation .
We achieve a 6 - point increase with respect to BLEU - 4 ( from 16.38 to 22.62 ) over MPGSN 's best system .
We perform the human evaluation of our model with and without copy attention , and we observe that we obtain 27 % more relevant questions when the copy attention is incorporated .
For a given paragraph as input , we depict in Figure 1 , the ground - truth questions as well as the questions generated along with the answers highlighted in the paragraph .
As can be seen from the example , while generating the second question( highlighted in green color ) , our model uses information not only from the sentence containing the answer , but also relevant context from the complete paragraph .
Problem Formulation & Approach Given a paragraph ' P ' and answer ' A ' , a question generation model iteratively samples question word q t ?
V Q at every time step ' t ' from the probability distribution given by : Pr ( Q | P , A ; ? ) = | Q| t=1 Pr( q t | P , A ; ? ) ( 1 )
Where V Q is the question vocabulary , ? is the set of parameters , and A is the answer .
Our question generation model consists of a two -layer paragraph encoder and a one-layer question decoder , equipped with a dynamic dictionary and copy attention .
In Figure 2 , we illustrate the overall architecture of our paragraph level question generation model .
The dynamic dictionary allows every training instance ( paragraph ) to have its own vocabulary instead of relying on the preprocessed global vocabulary .
Copy attention enables the model to predict question words from the extended vocabulary ( complete vocabulary + paragraph vocabulary ) .
Copy attention operates over the union of words in vocabulary and paragraph words .
Paragraph encoder
We use a two -layer bidirectional long short -term memory ( Bi - LSTM ) network stack as the paragraph encoder .
The paragraph encoder takes an answer-tagged paragraph as input and outputs a representation of the paragraph .
Note that the Bi-LSTM network processes the input paragraph in both the forward and backward directions : ? ? h t = LST M ( e t , ? ? ? h t?1 ) and ? ? h t = LST M ( e t , ? ? ? h t +1 ) , where ? ? h t ( resp. ? ? h t ) is the forward ( resp. backward ) hidden state at time step t and e t is the vector representation of current input x t at time step t.
The final hidden state for the current word input is the concatenation of the forward and backward hidden state vectors : h t = [ ? ? h t , ? ? h t ] .
Dynamic , shared dictionary
In the traditional approach , a new / unknown word is typically replaced with the " < unk > " token .
The copy mechanism ( Gu et al. , 2016 ) then unfortunately learns to copy this " < unk > " token instead of the actual ( unknown ) word from the source paragraph .
Instead , we use a separate dynamic dictionary unique to each source paragraph , which includes all and only words that occur in the paragraph .
This allows our model to copy source words that may not be in the target dictionary into the target ( question ) .
Using a dynamic dictionary consisting of the preprocessed vocabulary instead of a static one enables the copy mechanism to copy the exact words directly into the question , even if they are rare and unknown .
Given a source paragraph p , we denote its dynamic vocabulary by V p .
Our copy attention mechanism takes into account V p and the global vocabulary V to determine whether to copy a word from V p or to predict a word from question vocabulary V Q .
As our model 's source as well as target are in the same language , we work with a shared source and target vocabulary , though we learn different language models for the paragraph and the question .
Sharing source and target vocabulary also decreases the memory requirement resulting from matrix multiplication ( thus making faster training through larger batch size ) possible .
It also enables efficient question decoding , thus reducing the time for inference on the test data .
Question decoder
Our question decoder is another Bi-LSTM that takes as input the last hidden state and context representation from the encoder and generates question words sequentially based on the previously generated words .
The decoder hidden state ( s t = [ ? ? s t , ? ? s t ] ) at time step t is the concatenation of the forward and backward hidden state representations : ? ? s t = LST M ( o t , ? ? ? s t?1 ) and ? ? s t = LST M ( o t , ? ? ? s t +1 ) , where o t is the vector representation of decoder input ( y t ) at time step t.
During training time the vector representation of words from the ground - truth question is fed as decoder input , and during test time the vector representation of the vocabulary word with maximum probability is fed as input .
We feed EOS symbol as input to decoder from both forward and backward dircetion at time t 0 . Bidirectional decoder factorizes the conditional decoding probabilities in both directions ( left- to - right and right - to- left ) into summation as : P y t | [ y m ] m =t = ? ? ? logp y t | Y [ 1:t?1 ] + ? ? ? logP y t | Y [ t + 1:Ty ] ( 2 )
The probability distribution over words in the vocabulary is calculated as : Pr(q t ) = sof tmax ( W g ?( W s [ s t , h t ] + b s ) + b g ) ( 3 ) where W g , W s , b s and b g are trainable model parameters .
Probability distribution P ( q t ) uses the standard softmax over the question vocabulary V Q .
This is used to sample word with maximum probability while decoding a question .
Copy attention
We know that a good question should be relevant to ( answerable from ) the paragraph .
So we learn a probabilistic mixture model over the question vocabulary V Q and the current paragraph vocabulary V P .
The current paragraph vocabulary is generated by a dynamic dictionary module .
Our copy attention calculates two values : cs : a binary - valued variable which acts a switch between copying a word from the paragraph 's dynamic vocabulary V P or generating from the question vocabulary V Q Pr .|V P : probability of copying a particular word from paragraph vocabulary V P .
Therefore , the final probability distribution from which a word will be sampled while generating a question is calculated over the extended vocabulary V Q ? V p .
Given a word from the extended vocabulary w ?
V Q ?V P , its probability Pr( w ) is computed as : Pr( w ) = Pr( cs = 1 ) Pr w|V P + Pr( cs = 0 ) Pr w|V Q ( 4 ) The switch probability Pr(cs ) is determined using the decoder hidden states as : Pr( cs = 1 ) = ?( W cs s t + b cs ) ( 5 ) where W cs and b cs are trainable model parameters .
Pr w|V Q is the probability of predicting a word from complete vocabulary V Q .
The copy attention weight a t is computed as : e t i = v T tanh ( W h h i + W s s t + b attn ) ( 6 ) a t = sparsemax ( e t ) ( 7 )
Where v , W h , W s and b attn are trainable model parameters .
The probability of copying a word from the paragraph vocabulary V P is estimated as : Pr w|V P = ?( W a a t + b a ) ( 8 ) where W a and b a are trainable model parameters .
Experimental Setup
We report the experimental result of our model ( referred to as NQG dd ) and compare it with the current state of the art MPGSN ( Zhao et al. , 2018 ) .
We employ the widely - used metrics BLEU ( Papineni et al. , 2002 ) , ROUGE -L and METEOR for automatic evaluation .
We use evaluation script provided by ( Chen et al. , 2015 ) . Similar to ( Kumar et al. , 2018a ) we also report qualitative assessment on the syntax , semantics and relevance of the questions generated by our model .
All experiments are performed on the SQuAD dataset ( Rajpurkar et al. , 2016 ) , where complete paragraphs are taken as input instead of just one or two sentences .
We reformat the SQuAD dataset such that during training time , each source instance is a ( paragraph , question ) pair annotated with the gold answers , and the target is a question .
Following the exact setup from MPGSN ( Zhao et al. , 2018 ) , we split the SQuAD train set into train and validation set containing 77,526 and 9,995 instances respectively , and take the separate SQuAD dev set containing 10,556 instances as our test set .
Results and Analysis Table 1 summarizes results of the automatic evaluation of the test set .
As can be seen , our model significantly outperforms the state - of- the - art MPGSN on all metrics .
The improvements on BLEU are especially substantial , the BLEU - 4 score of MPGSN is 16.38 , and ours ( with copy incorporated ) is 22.62 , an improvement of 6.24 , or 38 % .
This large performance difference demonstrates the effectiveness of our dynamic dictionary .
In Table 2 we present human evaluation results .
We evaluate the quality of questions generated in terms on syntactic correctness , semantic correctness and relevance to the paragraph .
The evaluation is performed on a randomly selected subset of 100 sentences from the test set .
Each of the three evaluators are presented the 100 paragraph -question pairs for two variants of our model ( with and without copy ) and asked for a binary responses for all three parameters .
We averaged responses received by all three evaluators to compute the final scores .
As can be seen , the incorporation of the copy attention improves performance , especially on relevance .
We also measure the inter-rater agreement using Randolph 's free-marginal multirater kappa ( Randolph , 2005 ) .
It can be observed that our quality metrics for both our models are rated as substantial agreement ( Viera et al. , 2005 ) .
To explain how our model attends to different words in the source paragraph we visualize attention weights in Figure 3 , which shows attention weights between question 2 generated by our model and the corresponding paragraph in Figure 1 .
We observe that the attention weight is high for words near the answer and the model attends to all relevant context rather that just the sentence containing the answer .
Table 2 : Human evaluation results ( columns " Score " ) as well as inter-rater agreement ( columns " Kappa " ) for each of our two models on 100 questions from the test set .
The scores are between 0 ( worst ) and 100 ( best ) .
Best results for each metric ( column ) are in bold .
We also note that our training is faster atleast by a factor of 2 .
We expected this since we replace a slightly expensive self-attention mechanism in the decoder of ( Zhao et al. , 2018 ) with a simpler dynamic dictionary and reusable copy attention .
Conclusion Paragraph - level question generation ( QG ) is an important but challenging problem , mainly due to the challenge in effectively handling a longer context .
We present a simple yet effective approach for automatic question generation from paragraphs .
Besides using a standard global source dictionary , our RNN - based model incorporates a dynamic , paragraph -specific dictionary , and learns to switch between copying from the combined Figure 2 : 2 Figure 2 : Overall architecture of our paragraph - level question generation model .
