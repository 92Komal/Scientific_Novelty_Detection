title
TWEETQA : A Social Media Focused Question Answering Dataset
abstract
With social media becoming increasingly popular on which lots of news and real-time events are reported , developing automated question answering systems is critical to the effectiveness of many applications that rely on realtime knowledge .
While previous datasets have concentrated on question answering ( QA ) for formal text like news and Wikipedia , we present the first large-scale dataset for QA over social media data .
To ensure that the tweets we collected are useful , we only gather tweets used by journalists to write news articles .
We then ask human annotators to write questions and answers upon these tweets .
Unlike other QA datasets like SQuAD in which the answers are extractive , we allow the answers to be abstractive .
We show that two recently proposed neural models that perform well on formal texts are limited in their performance when applied to our dataset .
In addition , even the finetuned BERT model is still lagging behind human performance with a large margin .
Our results thus point to the need of improved QA systems targeting social media text .
1
Introduction Social media is now becoming an important realtime information source , especially during natural disasters and emergencies .
It is now very common for traditional news media to frequently probe users and resort to social media platforms to obtain real-time developments of events .
According to a recent survey by Pew Research Center 2 , in 2017 , more than two -thirds of Americans read some of their news on social media .
Even for American people who are 50 or older , 55 % of them report getting news from social media , Passage :
Oh man just read about Paul Walkers death .
So young .
Ugggh makes me sick especially when it 's caused by an accident .
God bless his soul .
- Jay Sean ( @jaysean ) December 1 , 2013 Q : why is sean torn over the actor 's death ?
A : walker was young Table 1 : An example showing challenges of TWEETQA .
Note the highly informal nature of the text and the presence of social media specific text like usernames which need to be comprehended to accurately answer the question .
which is 10 % points higher than the number in 2016 .
Among all major social media sites , Twitter is most frequently used as a news source , with 74 % of its users obtaining their news from Twitter .
All these statistical facts suggest that understanding user- generated noisy social media text from Twitter is a significant task .
In recent years , while several tools for core natural language understanding tasks involving syntactic and semantic analysis have been developed for noisy social media text ( Gimpel et al. , 2011 ; Ritter et al. , 2011 ; Wang et al. , 2014 ) , there is little work on question answering or reading comprehension over social media , with the primary bottleneck being the lack of available datasets .
We observe that recently proposed QA datasets usually focus on formal domains , e.g. CNN / DAILYMAIL ( Hermann et al. , 2015 ) and NewsQA ( Trischler et al. , 2016 ) on news articles ; SQuAD ( Rajpurkar et al. , 2016 ) and WIKI - MOVIES ( Miller et al. , 2016 ) that use Wikipedia .
In this paper , we propose the first large-scale dataset for QA over social media data .
Rather than naively obtaining tweets from Twitter using the Twitter API 3 which can yield irrelevant tweets with no valuable information , we restrict ourselves only to tweets which have been used by journalists in news articles thus implicitly implying that such tweets contain useful and relevant information .
To obtain such relevant tweets , we crawled thousands of news articles that include tweet quotations and then employed crowd-sourcing to elicit questions and answers based on these event-aligned tweets .
Table 1 gives an example from our TWEETQA dataset .
It shows that QA over tweets raises challenges not only because of the informal nature of oral-style texts ( e.g. inferring the answer from multiple short sentences , like the phrase " so young " that forms an independent sentence in the example ) , but also from tweet -specific expressions ( such as inferring that it is " Jay Sean " feeling sad about Paul 's death because he posted the tweet ) .
Furthermore , we show the distinctive nature of TWEETQA by comparing the collected data with traditional QA datasets collected primarily from formal domains .
In particular , we demonstrate empirically that three strong neural models which achieve good performance on formal data do not generalize well to social media data , bringing out challenges to developing QA systems that work well on social media domains .
In summary , our contributions are : ?
We present the first question answering dataset , TWEETQA , that focuses on social media context ; ?
We conduct extensive analysis of questions and answer tuples derived from social media text and distinguish it from standard question answering datasets constructed from formaltext domains ; ?
Finally , we show the challenges of question answering on social media text by quantifying the performance gap between human readers and recently proposed neural models , and also provide insights on the difficulties by analyzing the decomposed performance over different question types .
Related Work Tweet NLP Traditional core NLP research typically focuses on English newswire datasets such as the Penn Treebank ( Marcus et al. , 1993 ) .
In recent Hill et al. , 2015 ) or quiz-style problems ( Richardson et al. , 2013 ; Lai et al. , 2017 ) .
The former one aims to generate single - token answers from automatically constructed pseudo-questions while the latter requires choosing from multiple answer candidates .
However , such unnatural settings make them fail to serve as the standard QA benchmarks .
Instead , researchers started to ask human annotators to create questions and answers given passages in a crowdsourced way .
Such efforts give the rise of large-scale human- annotated RC datasets , many of which are quite popular in the community such as SQuAD ( Rajpurkar et al. , 2016 ) , MS MARCO ( Nguyen et al. , 2016 ) , NewsQA ( Trischler et al. , 2016 ) .
More recently , researchers propose even challenging datasets that require QA within dialogue or conversational context ( Reddy et al. , 2018 ; .
According to the difference of the answer format , these datasets can be further divided to two major categories : extractive and abstractive .
In the first category , the answers are in text spans of the given passages , while in the latter case , the answers may not appear in the passages .
It is worth mentioning that in almost all previously developed datasets , the passages are from Wikipedia , news articles or fiction stories , which are considered as the formal language .
Yet , there is little effort on RC over informal one like tweets .
TweetQA
In this section , we first describe the three -step data collection process of TWEETQA : tweet crawling , question - answer writing and answer validation .
Next , we define the specific task of TWEETQA and discuss several evaluation metrics .
To better understand the characteristics of the TWEETQA task , we also include our analysis on the answer and question characteristics using a subset of QA pairs from the development set .
Data Collection Tweet Crawling
One major challenge of building a QA dataset on tweets is the sparsity of informative tweets .
Many users write tweets to express their feelings or emotions about their personal lives .
These tweets are generally uninformative and also very difficult to ask questions about .
Given the linguistic variance of tweets , it is generally hard to directly distinguish those tweets from informative ones .
In terms of this , rather than starting from Twitter API Search , we look into the archived snapshots 4 of two major news websites ( CNN , NBC ) , and then extract the tweet blocks that are embedded in the news articles .
In order to get enough data , we first extract the URLs of all section pages ( e.g. World , Politics , Money , Tech ) from the snapshot of each home page and then crawl all articles with tweets from these section pages .
Note that another possible way to collect informative tweets is to download the tweets that are posted by the official Twitter accounts of news media .
However , these tweets are often just the summaries of news articles , which are written in formal text .
As our focus is to develop a dataset for QA on informal social media text , we do not consider this approach .
After we extracted tweets from archived news articles , we observed that there is still a portion of tweets that have very simple semantic structures and thus are very difficult to raise meaningful questions .
An example of such tweets can be like : Figure 1 : An example we use to guide the crowdworkers when eliciting question answer pairs .
We elicit question that are neither too specific nor too general , do not require background knowledge .
" Wanted to share this today - @IAmSteveHarvey " .
This tweet is actually talking about an image attached to this tweet .
Some other tweets with simple text structures may talk about an inserted link or even videos .
To filter out these tweets that heavily rely on attached media to convey information , we utilize a state - of - the - art semantic role labeling model trained on CoNLL - 2005 ( He et al. , 2017 to analyze the predicate - argument structure of the tweets collected from news articles and keep only the tweets with more than two labeled arguments .
This filtering process also automatically filters out most of the short tweets .
For the tweets collected from CNN , 22.8 % of them were filtered via semantic role labeling .
For tweets from NBC , 24.1 % of the tweets were filtered .
Question - Answer Writing
We then use Amazon Mechanical Turk to collect question - answer pairs for the filtered tweets .
For each Human Intelligence Task ( HIT ) , we ask the worker to read three tweets and write two question - answer pairs for each tweet .
To ensure the quality , we require the workers to be located in major English speaking countries ( i.e. Canada , US , and UK ) and have an acceptance rate larger than 95 % .
Since we use tweets as context , lots of important information are contained in hashtags or even emojis .
Instead of only showing the text to the workers , we use javascript to directly embed the whole tweet into each HIT .
This gives workers the same experience as reading tweets via web browsers and help them to better compose questions .
To avoid trivial questions that can be simply answered by superficial text matching methods or too challenging questions that require background knowledge .
We explicitly state the following items in the HIT instructions for question writing : ?
No Yes - no questions should be asked . ?
The question should have at least five words .
?
Videos , images or inserted links should not be considered . ?
No background knowledge should be required to answer the question .
To help the workers better follow the instructions , we also include a representative example showing both good and bad questions or answers in our instructions .
Figure 1 shows the example we use to guide the workers .
As for the answers , since the context we consider is relatively shorter than the context of previous datasets , we do not restrict the answers to be in the tweet , otherwise , the task may potentially be simplified as a classification problem .
The workers are allowed to write their answers in their own words .
We just require the answers to be brief and can be directly inferred from the tweets .
After we retrieve the QA pairs from all HITs , we conduct further post-filtering to filter out the pairs from workers that obviously do not follow instructions .
We remove QA pairs with yes / no answers .
Questions with less than five words are also filtered out .
This process filtered 13 % of the QA pairs .
The dataset now includes 10,898 articles , 17,794 tweets , and 13,757 crowdsourced question - answer pairs .
The collected QA pairs will be directly available to the public , and we will provide a script to download the original tweets and detailed documentation on how we build our dataset .
Also note that since we keep the original news article and news titles for each tweet , our dataset can also be used to explore more challenging generation tasks .
Table 2 shows the statistics of our current collection , and the frequency of different types of questions is shown in Answer Validation
For the purposes of human performance evaluation and inter-annotator agreement checking , we launch a different set of HITs to ask workers to answer questions in the test and development set .
The workers are shown with the tweet blocks as well as the questions collected in the previous step .
At this step , workers are allowed to label the questions as " NA " if they think the questions are not answerable .
We find that 3.1 % of the questions are labeled as unanswerable by the workers ( for SQuAD , the ratio is 2.6 % ) .
Since the answers collected at this step and previous step are written by different workers , the answers can be written in different text forms even they are semantically equal to each other .
For example , one answer can be " Hillary Clinton " while the other is " @HillaryClinton " .
As it is not straightforward to automatically calculate the overall agreement , we manually check the agreement on a subset of 200 random samples from the development set and ask an independent human moderator to verify the result .
It turns out that 90 % of the answers pairs are semantically equivalent , 2 % of them are partially equivalent ( one of them is incomplete ) and 8 % are totally inconsistent .
The answers collected at this step are also used to measure the human performance .
We have 59 individual workers participated in this process .
Task and Evaluation
As described in the question - answer writing process , the answers in our dataset are different from those in some existing extractive datasets .
Thus we consider the task of answer generation for TWEETQA and we use several standard metrics for natural language generation to evaluate QA systems on our dataset , namely we consider BLEU - 1 5 ( Papineni et al. , 2002 ) , Meteor ( Denkowski and Lavie , 2011 ) and Rouge -L ( Lin , 2004 ) in this paper .
To evaluate machine systems , we compute the scores using both the original answer and validation answer as references .
For human performance , we use the validation answers as generated ones and the original answers as references to calculate the scores .
Analysis
In this section , we analyze our dataset and outline the key properties that distinguish it from standard QA datasets like SQuAD ( Rajpurkar et al. , 2016 ) .
First , our dataset is derived from social media text which can be quite informal and user-centric as opposed to SQuAD which is derived from Wikipedia and hence more formal in nature .
We observe that the shared vocabulary between SQuAD and TWEETQA is only 10.79 % , suggesting a significant difference in their lexical content .
Figure 2 shows the 1000 most distinctive words in each domain as extracted from SQuAD and TWEETQA .
Note the stark differences in the words seen in the TWEETQA dataset , which include a large number of user accounts with a heavy tail .
Examples include @realdonaldtrump , @jdsutter , @justinkirkland and # cnnworldcup , # goldenglobes .
In contrast , the SQuAD dataset rarely has usernames or hashtags that are used to signify events or refer to the authors .
It is also worth noting that the data collected from social media can not only capture events and developments in real -time but also capture individual opinions and thus requires reasoning related to the authorship of the content as is illustrated in Table 1 .
In addition , while SQuAD requires all answers to be spans from the given passage , we do not enforce any such restriction and answers can be free-form text .
In fact , we observed that 43 % of our QA pairs consists of answers which do not have an exact substring matching with their corresponding passages .
All of the above distinguishing factors have implications to existing models which we analyze in upcoming sections .
We conduct analysis on a subset of TWEETQA to get a better understanding of the kind of reasoning skills that are required to answer these questions .
We sample 150 questions from the development set , then manually label their reasoning categories .
Table 4 shows the analysis results .
We use some of the categories in SQuAD ( Rajpurkar et al. , 2016 ) and also proposes some tweet -specific reasoning types .
Our first observation is that almost half of the questions only require the ability to identify paraphrases .
Although most of the " paraphrasing only " questions are considered as fairly easy questions , we find that a significant amount ( about 3/4 ) of these questions are asked about event-related topics , such as information about " who did what to whom , when and where " .
This is actually consistent with our motivation to create TWEETQA , as we expect this dataset could be used to develop systems that automatically collect information about real-time events .
Apart from these questions , there are also a group of questions that require understanding common sense , deep semantics ( i.e. the answers cannot be derived from the literal meanings of the tweets ) , and relations of sentences 6 ( including coreference resolution ) , which are also appeared in other RC datasets ( Rajpurkar et al. , 2016 ) .
On the other hand , the TWEETQA also has its unique properties .
Specifically , a significant amount of questions require certain reasoning skills that are specific to social media data : ?
Understanding authorship :
Since tweets are highly personal , it is critical to understand how questions / tweets related to the authors .
?
Oral English & Tweet English :
Tweets are often oral and informal .
QA over tweets requires the understanding of common oral English .
Our TWEETQA also requires understanding some tweet -specific English , like conversation - style English .
? Understanding of user IDs & hashtags :
Tweets often contains user IDs and hashtags , which are single special tokens .
Understanding these special tokens is important to answer person- or event-related questions .
For example , the second example requires both the understanding of sentences relations and tweet language habits to answer the question ; and the third example requires both the understanding of sentences relations and authorship .
Experiments
To show the challenge of TweetQA for existing approaches , we consider four representative methods as baselines .
For data processing , we first remove the URLs in the tweets and then tokenize the QA pairs and tweets using NLTK .
7
This process is consistent for all baselines .
Query Matching Baseline
We first consider a simple query matching baseline similar to the IR baseline in Kocisk ?
et al. ( 2017 ) .
But instead of only considering several genres of spans as potential answers , we try to match the question with all possible spans in the tweet context and choose the span with the highest BLEU - 1 score as the final answer , which follows the method and implementation 8 of answer span selection for open-domain QA .
We include this baseline to show that TWEETQA is a nontrivial task which cannot be easily solved with superficial text matching .
Neural Baselines
We then explore three typical neural models that perform well on existing formal - text datasets .
One takes a generative perspective and learns to decode the answer conditioned on the question and context , while the others learns to extract a text span from the context that best answers the question .
Generative QA RNN - based encoder-decoder models have been widely used for natural language generation tasks .
Here we consider a recently pro-posed generative model ( Song et al. , 2017 ) that first encodes the context and question into a multi-perspective memory via four different neural matching layers , then decodes the answer using an attention - based model equipped with both copy and coverage mechanisms .
The model is trained on our dataset for 15 epochs and we choose the model parameters that achieve the best BLEU - 1 score on the development set .
BiDAF
Unlike the aforementioned generative model , the Bi-Directional Attention Flow ( BiDAF ) ( Seo et al. , 2016 ) network learns to directly predict the answer span in the context .
BiDAF first utilizes multi-level embedding layers to encode both the question and context , then uses bi-directional attention flow to get a query - aware context representation , which is further modeled by an RNN layer to make the span predictions .
Since our TWEETQA does not have labeled answer spans as in SQuAD , we need to use the human-written answers to retrieve the answerspan labels for training .
To get the approximate answer spans , we consider the same matching approach as in the query matching baseline .
But instead of using questions to do matching , we use the human-written answers to get the spans that achieve the best BLEU - 1 scores .
Fine-Tuning BERT
This is another extractive RC model that benefits from the recent advance in pretrained general language encoders ( Peters et al. , 2018 ; Devlin et al. , 2018 ) .
In our work , we select the BERT model ( Devlin et al. , 2018 ) which has achieved the best performance on SQuAD .
In our experiments , we use the PyTorch reimple - mentation 9 of the uncased base model .
The batch size is set as 12 and we fine - tune the model for 2 epochs with learning rate 3e - 5 .
Evaluation
Overall Performance
We test the performance of all baseline systems using the three generative metrics mentioned in Section 3.2 .
As shown in Table 5 , there is a large performance gap between human performance and all baseline methods , including BERT , which has achieved superhuman performance on SQuAD .
This confirms than TWEETQA is more challenging than formal-test RC tasks .
We also show the upper bound of the extractive models ( denoted as EXTRACT - UPPER ) .
In the upper bound method , the answers are defined as n-grams from the tweets that maximize the BLEU-1/ METEOR / ROUGE -L compared to the annotated groundtruth .
From the results , we can see that the BERT model still lags behind the upper bound significantly , showing great potential for future research .
It is also interesting to see that the HUMAN performance is slightly worse compared to the upper bound .
This indicates ( 1 ) the difficulty of our problem also exists for humanbeings and ( 2 ) for the answer verification process , the workers tend to also extract texts from tweets as answers .
According to the comparison between the two non-pretraining baselines , our generative baseline yields better results than BiDAF .
We believe this is largely due to the abstractive nature of our dataset , since the workers can sometimes write the answers using their own words .
Performance Analysis over Human-Labeled Question Types
To better understand the difficulty of the TWEETQA task for current neural models , we analyze the decomposed model performance on the different kinds of questions that require different types of reasoning ( we tested on the subset which has been used for the analysis in Table 4 ) .
Table 6 shows the results of the best performed non-pretraining and pretraining approach , i.e. , the generative QA baseline and the fine- tuned BERT .
Our full comparison including the BiDAF performance and evaluation on more metrics can be found in Appendix A. Following previous RC research , we also include analysis on automaticallylabeled question types in Appendix B .
As indicated by the results on METEOR and ROUGE -L ( also indicated by a third metric , BLEU - 1 , as shown in Appendix A ) , both baselines perform worse on questions that require the understanding deep semantics and userID&hashtags .
The former kind of questions also appear in other benchmarks and is known to be challenging for many current models .
The second kind of questions is tweet -specific and is related to specific properties of social media data .
Since both models are designed for formal - text passages and there is no special treatment for understanding user IDs and hashtags , the performance is severely limited on the questions requiring such reasoning abilities .
We believe that good segmentation , disambiguation and linking tools developed by the social media community for processing the userIDs and hashtags will significantly help these question types .
On non-pretraining model Besides the easy questions requiring mainly paraphrasing skill , we also find that the questions requiring the understanding of authorship and oral / tweet English habits are not very difficult .
We think this is due to the reason that , except for these tweet -specific tokens , the rest parts of the questions are rather simple , which may require only simple reasoning skill ( e.g. paraphrasing ) .
On pretraining model Although BERT was demonstrated to be a powerful tool for reading comprehension , this is the first time a detailed analysis has been done on its reasoning skills .
From the results , the huge improvement of BERT mainly comes from two types .
The first is paraphrasing , which is not surprising because that a well pretrained language model is expected to be able to better encode sentences .
Thus the derived embedding space could work better for sentence comparison .
The second type is commonsense , which is consistent with the good performance of BERT ( Devlin et al. , 2018 ) on SWAG ( Zellers et al. , 2018 ) .
We believe that this provides further evidence about the connection between largescaled deep neural language model and certain kinds of commonsense .
Conclusion
We present the first dataset for QA on social media data by leveraging news media and crowdsourcing .
The proposed dataset informs us of the distinctiveness of social media from formal domains in the context of QA .
Specifically , we find that QA on social media requires systems to comprehend social media specific linguistic patterns like informality , hashtags , usernames , and authorship .
These distinguishing linguistic factors bring up important problems for the research of QA that currently focuses on formal text .
We see our dataset as a first step towards enabling not only a deeper understanding of natural language in social media but also rich applications that can extract essential real - time knowledge from social media .
A Full results of Performance Analysis over Human-Labeled Question Types
Table 7 gives our full evaluation on human annotated question types .
Compared with the BiDAF model , one interesting observation is that the generative baseline gets much worse results on ambiguous questions .
We conjecture that although these questions are meaningless , they still have many words that overlapped with the contexts .
This can give BiDAF potential advantage over the generative baseline .
