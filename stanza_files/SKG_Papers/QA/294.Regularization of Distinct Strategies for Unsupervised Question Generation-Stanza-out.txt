title
Regularization of Distinct Strategies for Unsupervised Question Generation
abstract
Unsupervised question answering ( UQA ) has been proposed to avoid the high cost of creating high-quality datasets for QA .
One approach to UQA is to train a QA model with questions generated automatically .
However , the generated questions are either too similar to a word sequence in the context or too drifted from the semantics of the context , thereby making it difficult to train a robust QA model .
We propose a novel regularization method based on teacher -student architecture to avoid bias toward a particular question generation strategy and modulate the process of generating individual words when a question is generated .
Our experiments demonstrate that we have achieved the goal of generating higher -quality questions for UQA across diverse QA datasets and tasks .
We also show that this method can be useful for creating a QA model with few-shot learning .
Introduction Machine Reading for Question Answering ( MRQA ) is the task of answering questions from a context that contains the answer .
This field has seen remarkable progress in recent years , with QA models outperforming humans on a question answering ( QA ) benchmark like SQuAD ( Rajpurkar et al. , 2016 ) .
Training a QA model requires a large amount of data , and constructing such a dataset is usually laborious or sometimes even impossible for some domains and languages .
Because of this , Lewis et al . ( 2019 ) explored unsupervised question answering ( UQA ) , a setting where manually constructed triples , ( context , question , answer ) , are not available for training .
They approached the problem with unsupervised question generation * Equal contribution .
Context ... Level 1 of DDM Architecture was formally published in 1986 .
...
Generated Question
LM -type Copy-type
When did the first level 1 of DDM Architecture come out ?
When level 1 of DDM Architecture was formally published ?
Figure 1 : An example of LM-type ( left ) and copy-type ( right ) questions generated from a context .
Newly generated tokens are in italics and copied tokens in bold .
( UQG ) , where answers are first extracted from contexts , and then questions are generated from ( contexts , answers ) pairs .
Such questions are combined into ( contexts , questions , answers ) triples to form a training dataset for a QA model .
Another approach to UQG is based on language models ( LM ) .
Radford et al. ( 2019 ) proposed an LM approach , GPT - 2 , that shook the natural language processing ( NLP ) research community with its remarkable capability of automatically generating high-quality text .
Naturally , GPT - 2 was shown to generate high-quality questions from contexts ( Klein and Nabi , 2019 ; Puri et al. , 2020 ) .
Despite the recent progress made by these efforts , UQG remains to be an open problem .
For example , the method proposed by Lewis et al . ( 2019 ) generates a question that is often so similar to a word sequence in the given context that QA models are trained with only straightforward questions and thus hampered from solving more challenging problems .
We refer these questions to copy-type .
The drawback of copy-type questions can be mitigated by generative LM like GPT - 2 as they can generate tokens that are not in the context but semantically plausible .
However , they can generate questions semantically drifted from the context which may not be answerable even by humans .
Figure 1 shows an example for LM -type and copy -type questions .
Inspired by the knowledge distillation approach with teacher -student interactions ( Hinton et al. , 2015 ) , we propose a pipeline architecture where a student module learns from a teacher how to generate higher quality questions that mitigate the drawbacks of both strategies of questions .
The teacher employs two QG models 1 ( assistants ) , LM -type and copy-type , and adopts a semantic -level regularization process newly designed to avoid a bias toward a particular question generation strategy ( i.e. , either copy - type or LM-type ) .
The student module learns how to make a balance between the two extreme types of questions generated by the assistants .
The regularization helps suppress the copying behavior and semantic drifts of the existing QG models and generate more versatile questions that are compatible with the given contexts .
Our contributions are i ) a novel generation method based on the teacher -student architecture that regularizes two generation models in an unsupervised setting .
ii ) adopting this method for unsupervised question generation to create a higherquality QA dataset 2 compared to existing UQG models without manual labeling .
iii ) using this QA dataset to train a QA model and demonstrate the robustness and effectiveness of our generation method thorough experiments in low QA data regimes .
Preliminary Study
In order to ensure that using the two somewhat conflicting types of questions can result in a QA improvement , we set out to run a preliminary experiment .
We simply combine the two datasets , copy-type and LM - type ( question creation details are provided in Section 4.1 ) to train a QG model to see if the regularization idea would be beneficial at all .
We posit that a positive result would not only justify a more sophisticated regularization process but also serve as a baseline for the experiments .
Since the dataset used for training the QG model contains two types of questions , we use the following batch loss : L batch = ?L LM + ( 1 ? ?) L copy ( 1 ) where the hyperparameter ?
controls the loss incurred by each question type in the batch .
Assuming that learning the patterns of copy-type questions would be easier than LM - type questions , due to the simplicity of the former , we enforce that reducing the total loss is influenced more by the LM - type rather than the copy-type .
Model EM F1 Copy-type QG 40.1 49.4 LM-type QG 42.0 50.9 QG trained on copy and LM 44.4 53.9 Table 1 : A result of the preliminary experiment with QA on the SQuAD 1.1 dev set .
To test the quality of this naive QG approach , we have trained three QA models based on three different QG models : i ) a QG model trained on 10 K copy- type questions , ii ) a QG model trained on 10K LM - type questions , iii ) a QG model trained on the combination of 5 K copy -type questions and 5 K LM - type questions .
To ensure a fair comparison , the three models use the same contexts and answers .
As shown in Table 1 , the combination of the two types of questions yields better performance .
This Figure 3 : Overall structure of the proposed approach .
At time step t , the pre-trained LM - type and copy -type assistants produce each probability distribution over vocabulary for the token q t .
Then , the regularization module selects the probability distribution P ( w ) for the token q t such that the generated question is not biased to either type , and P ( w ) is used as the soft target to train the student module .
positive result sets the stage for the main thrust of this work : design and application of token - level regularization for training a QG model to overcome the drawbacks of two styles of questions .
Proposed Approach
In an unsupervised setting , it is difficult to solve the problems related to the LM-type and copy - type QG models , primarily because the gold standard questions are not provided .
However , the preliminary study ( Section 2 ) shows there is evidence that using two QG models , LM - type and copy -type , in an interleaving way has the potential to solve the problems of each model .
Instead of simply merging two datasets ( as in the Preliminary Study ) , we propose a finer- grained approach of using token - level regularization rather than instance - level that selects full questions .
Figure 2 illustrates the difference .
We posit that generated questions should not be easily classified into either LM - type or copy - type if the two styles were to be inter-mixed in a balanced way .
In other words , the style of the questions should not be highly biased toward either side .
In short , our unsupervised QG problem is reduced to devise a token - level regularization method that generates questions indistinguishable between the two question types and eventually mitigates their drawbacks .
Problem Formulation
The goal of the QG model is to generate the most probable question Q = ( q 1 , ... , q | Q| ) given a context C = ( c 1 , ... , c | C | ) and an answer A = ( a 1 , ... , a | A | ) , which is a subspan of C , ( i.e. , a 1 = c i and a | A | = c j where 1 ? i ? j ? | C| ) .
Q = arg max Q P ( Q|C , A ) ( 2 ) As discussed in Section 2 , Q should be discouraged from being biased toward either LM - type or copy-type .
To enforce this property , we define a function F that returns the probability of the question following a specific type ( LM for LM - type or CP for copy-type ) , given the context and the generated question tokens up to t. F : Q ? C ? type ? ? [ 0 , 1 ] ( 3 ) where Q and C represent the infinite set of possible questions and contexts , respectively .
Question tokens are generated sequentially by considering the sub-sequence of up to t ?
1 tokens at time step t ( denoted as q < t ) that have been generated so far .
With the binary function defined above , we can constrain the next token qt to satisfy the following condition : if the question generated up to t ?
1 is of LM-type , qt should maximize the score of the copy -type , or vice versa .
qt = ? ? ? ? ? arg max w F( q <t : w , LM ) , if F( q <t , LM ) < F( q <t , CP ) arg max w F( q <t : w , CP ) , otherwise ( 4 ) where w is a token over the vocabulary V and " : " represents the concatenation operation .
Overall Architecture
As a way to implement the aforementioned idea of generating each token qt sequentially based on the nature of the sub-question up to t ?
1 , we take a teacher -student structure as a pipeline inspired by Hinton et al . ( 2015 ) .
The teacher is composed of two types of " assistants " ( LM - type QG and copytype QG models ) , and a regularization module , whereas the student is a single QG model that receives the knowledge transferred from the teacher .
In this way , the student model can learn the regularization process of the teacher and generalize it even without golden labels .
The pipeline system works as follows .
Given a context as the only input , it first randomly selects a named-entity as the answer and predicts wh-word to input to the two assistants ( i.e. , the two QG models ) .
This wh-word prediction is needed for a performance boost as shown in ( Kang et al. , 2019 ) .
Each of the two assistants predicts a new token , and the regularization module subsequently selects the one to accept .
The selected token is input again to the two assistants with the previously generated tokens to generate the next ones and so on until a question mark is generated .
At each time step t , we also store the probability distribution over all the tokens in the vocabulary , from the selected assistant .
These distributions , rather than the tokens , help generalize the knowledge of the teacher and hence allow the student to learn the probabilities of selecting particular tokens at individual time steps in a more reliable way .
This entire procedure is illustrated in Figure 3 , as well as in Algorithm 1 in Appendix A.3 .
Question Generation Module
One of the benefits of our architecture is that the modules are not bounded by any specific model .
For the current work , we employ the QG model proposed by Chan and Fan ( 2019 ) for the two assistants of the teacher and for the student , taking advantage of BERT .
The essence of this QG model is to input a context , an answer , generated question tokens at each time step , and a [ MASK ] token at the end .
The embedding of the last token ( i.e. , [ MASK ] ) at the output layer is used to predict the next generated token , qt ?
V , where V is the vocabulary .
The QG model works as follows .
First , the context C and the answer A , which is a sub-span of the context , are integrated into C with the special tokens [ HL ] to signal the start and end of the answer .
C = c 1 , c 2 , ... , [ HL ] , a 1 , ... , a | A | , [ HL ] , ... , c | C | ( 5 )
Then the question tokens generated so far ( prior to the current time step t ) are added to complete the input to BERT , which is used to generate H : X t?1 = ( [ CLS ] , C , [ SEP ] , q1 , ... , qt ?1 , [ M ASK ] ) ( 6 ) H = BERT ( X t?1 ) ( 7 ) where H ? R | Xt |?h is the matrix of BERT token embeddings and h is the hidden size of a BERT token embedding .
In order to generate a token from the BERT output , the embedding of the [ MASK ] token , H [ M ASK ] , is transformed into the vocabulary space using the linear layer W ? R h?|V | .
This gives a probability distribution over the vocabulary given the input : P ( w| X t?1 ) = sof tmax ( H [ M ASK ] ? W + b ) ( 8 )
The next token , qt , is the word , w ?
V , with the highest probability : qt = arg max w P ( w| X t?1 ) ( 9 )
Teacher Module
The teacher module consisting of two assistant modules and the regularization module provides soft targets to the student module .
The assistant modules are pre-trained with cross-entropy to implement the question generators and serve as the source of knowledge to the regularization module .
Inspired by GAN ( Goodfellow et al. , 2014 ) , we set the goal of the regularization module to preventing the generated question from being easily detected as either LM type or copy type .
By making a generated question indistinguishable between the LM and copy types , we attempt to make the student mitigate the drawbacks of either type .
We implement the regularization module as a discriminator , D , that takes as input the context and the list of generated tokens up to t ?
1 , and decides which of the LM-type , ? LM , and copy-type , ? CP , assistants should generate the next token .
More formally , the next token is selected as follows : D : C ? Q ? ? { LM , CP } ( 10 ) qt = arg max w P ( w| X t?1 , ? LM ) , if D( C , q <t ) = LM arg max w P ( w| X t?1 , ? CP ) , if D( C , q <t ) = CP ( 11 )
This discriminator is implemented using BERT .
Its input , X t?1 , is the concatenation of the question tokens with the context : X t?1 = ( [ CLS ] , q1 , ... , qt ?1 , [ SEP ] , C , [ SEP ] ) ( 12 )
The BERT embedding of the token [ CLS ] is then input to the binary classifier W D ?
R h?2 to select the type .
H = BERT ( X t?1 ) ( 13 ) type = arg max (?( H [ CLS ] ? W D + b D ) ) ) ( 14 ) where ? is the sigmoid function .
The training dataset for the regularization module consists of a list of ( context , question from either LM - type or copy - type assistant ) pairs serving as input and class , either LM - type or copy -type , as a label .
Each question is truncated to a random length to simulate the use case of sequentially generating question tokens .
This module achieves a regularization effect because it predicts the class of a question : if the class of the generated question up to some time step t is predicted to be an LM - type , the next token of the question is generated from the copy -type assistant .
This regularization process will be reflected in the questions used to train the student , which in turn learns how to generate questions that are regularized between the LM style and the copy style .
Student Module Following Hinton et al. ( 2015 ) , the student is trained using the probability distributions of generating tokens by either one of the assistants as soft targets .
This allows the student to learn more information from the teacher than using hard targets ( i.e. , tokens ) because it can learn even a small probability of generating a token .
Since we regularize the probability distribution to be given to the student module at each time step , the student ends up learning how to generalize the regularization process .
We use KL - divergence loss to minimize the difference between the teacher 's ( i.e. from one of the assistants ) and student 's probability distributions as follows .
L student = D KL ( P ( ?| X t?1 ; ? student ) || ( 15 ) P ( w| X t?1 ; ? teacher ) )
It is worth noting that the training with soft targets is possible because the same architecture and thus , the same vocabulary is used for the assistants and the student .
To avoid repeated tokens , the penalized sampling technique as in Keskar et al . ( 2019 ) is applied for the calculation of the probability distribution .
This technique is also applied to prevent from generating special tokens and the answer , which should not be included in a question .
Details of the penalized tokens are provided in Appendix A.2 .
Experiments
The primary goal of our experiments is to demonstrate that the method of regularizing copy-type and LM - type questions yields questions that overcome the drawbacks of the two types .
By measuring the QA performance on several QA datasets , we establish the generalizability of the proposed approach .
Dataset Creation
All our models use contexts from the dataset of Lewis et al . ( 2019 ) 3 .
In the case of the copy - type dataset , it also uses the questions and answers from the original dataset to train a QG model to create the copy -type questions .
On the other hand , the QG - type dataset uses Stanza ( Qi et al. , 2020 ) Implementation
The hyperparameter ? used in Eq. ( 1 ) is set to 0.8 .
The QG models are based on BERT - HLSQG ( Chan and Fan , 2019 ) , with the hyperparameters provided by the authors and minor modifications to apply penalized sampling as in Keskar et al . ( 2019 ) .
Experimental Setting
The discriminator model and the QA model for the evaluation are based on the BERT - base and BERT - large , respectively , implemented by Hugging Face ( Wolf et al. , 2019 ) .
Their default hyperparameters are used without dropout .
A detailed description of the hyperparameters is in Appendix A.4 .
Training
The copy-type and LM - type assistants are pre-trained on 10 K instances .
The discriminator model is trained on a fully balanced training set of 70 K instances .
The student QG model is trained on 10 K instances generated by the teacher module .
The QA models for the experiments are trained with 10 K instances .
4
The contexts for the training datasets are randomly sampled without replacement .
Evaluation
The in- domain MRQA shared task ( Fisch et al. , 2019 ) 5 including modified SQuAD 1.1 ( Rajpurkar et al. , 2016 ) , NewsQA ( Trischler et al. , 2017 ) , TriviaQA ( Joshi et al. , 2017 ) , SearchQA ( Dunn et al. , 2017 ) , HotpotQA ( Yang et al. , 2018 ) , and Natural Questions ( Kwiatkowski et al. , 2019 ) is used for the experiments in Sections 4.2 , 4.3 , and 4.5 .
The SQuAD 1.1 dev set 6 is used for the experimental results in Section 4.4 .
Overall Performance
For a fair and extensive comparison with the previous work from Lewis et al . ( 2019 ) on MRQA datasets , a QA model was trained using the dataset provided by Lewis et al . ( 2019 ) .
In addition , to understand the effect of each module in the entire system , we trained the QA models with the datasets generated by the two assistants , the teacher module , and the student module .
As can be seen in Table 2 , our model significantly outperforms the baselines on all the QA datasets but SearchQA .
This result validates the proposed approach , suggesting it alleviates the drawback of the copy -type questions generated by ( Lewis et al. , 2019 ) as well as those generated by the single QG models .
The proposed approach clearly creates a more challenging and yet semantically less deviating QA dataset that ends up training a more robust and reliable QA model .
The Exact Match ( EM ) scores are provided in the appendix A.5 .
The reason for the low performance of our approach on SearchQA appears to lie in the nature of this dataset .
Considering that the LM- type QG is significantly interior to the copy - type QG on the dataset only , it is clear that copy -type questions are much more useful for SearchQA than the LM-type .
Given that our models ( " Teacher " and " Student " ) attempt to deviate from the copy - type questions by design , the generated questions end up making the QA model less effective in handling copy -type questions .
The student model performs better than the teacher module due to the student 's capability to generalize the discriminator , which is based on our heuristic assumption ( Eq. 11 ) .
Module
Roles of the Modules Since the teacher module can generate questions by itself , we show its performance in Table 2 .
Across most of the datasets , it outperforms the baselines including the two QG models in the assistants , showing the clear value of the regularization module that effectively combines them .
By comparing its performance on SQuAD against the result in the preliminary study ( Table 1 ) , we confirm that the token - level regularization ( 58.2 in F1 ) is more effective than the instance - level regularization ( 53.9 ) implemented by the " QG trained on copy and LM " model , as suggested in Section 3 .
Table 2 also witnesses the value of the student module or the teacher -student architecture as it helps improve the performance over the teacher module ; the student learned and generalized the regularization process from the teacher .
Effect of Regularization Module
The discriminator model was trained on 35 K LMtype questions and 35 K Copy-type questions ( 70 K in total , as mentioned in section 4.1 ) .
After the training , the discriminator model achieves an accuracy of 95.53 % on a 10 K dev set ( 5 K for each type ) .
To further study the effect of the discriminator model in regularizing the two styles of questions , we replace it with simpler models and analyze the performance of the generated questions .
The simpler models are i ) random , i.e. , at each time step , an assistant is selected randomly , and ii ) frequency - based , i.e. , the probability of selecting an assistant is proportional to the number of times the other assistant has been selected .
As a result , three different sets of questions are used for training a QA model and evaluated on the dev set of SQuAD 1.1 .
As shown in Table 3 , the discriminator model outperforms the other two models .
This indicates that the context information and the previously generated question tokens are essential to correctly determining the question type , which in turn affects the quality of the regularization .
This result is coherent with the intuition that the context must be essential to deciding whether or not a question is a copy from itself .
Also noteworthy is that the frequency - based model performs better than the random model , implying that frequency plays a role in the regularization task .
It appears that striking a balance between the two styles in the generating tokens at each step helps in generating higher quality questions .
The random model is likely to generate a less balanced list of tokens , e.g. , the first half of the tokens biased towards one type .
The result reaffirms that the generated question should not be heavily biased toward a question type .
Potential for Few-shot Learning
Although our main goal is to improve unsupervised QA through a dataset generated in an unsupervised way , it is instructive to consider a few-shot learning setting , in which a limited number of pre-labeled training instances are used instead of the entire set .
To investigate the potential of the proposed method for improved QA performance with few-shot learning , we pre-train the BERT - large QA model with a synthetic QA dataset generated from the student module .
The QA model is then fine-tuned with increasing numbers of pre-labeled instances , starting from 0 ( zero-shot learning ) to all the available training instances .
Figure 5 shows the result on the in- domain MRQA shared task .
As can be seen , UQG is remarkably useful in the no-data or small - data regimes .
The proposed QG method added to the BERT - large QA model enjoys a performance boost of about 10 to 20 points , depending on the datasets , with an addition of only about 100 pre-labeled instances .
On the other hand , it is sufficient to add only 1 K pre-labeled instances to reach the performance level of the supervised models , where its performance begins to level off .
While BERT - large alone can reach the levels with slightly more instances , this analysis gives a hope that UQA can serve as a strong method with no or a very small number of pre-labeled instances for comparable effectiveness .
Error Analysis
The analysis result in Table 4 is intended to give an insight into where our method fails .
The first example shows that the question deviates from the context , perhaps due to the heavy use of LM -type .
The second example suggests that our model suffers from the problem of handling pronouns .
In the last example , we can see that the end of the question is not natural , indicating that our model should improve on learning how to end a question .
A common problem we observe from these cases is the overuse of " you " when it is unnecessary .
Context Generated Question
The final was a one-sided affair , with Suburbs proving too strong for the southerners .
Where did you get the final of this round in adelaide ?
... Later he went to New College , Oxford , where he completed an M.A. and D.Phil. in Indian history ...
What was he doing in new college of oxford and did you study history ?
In general case , the HJB equation does not have a classical ( smooth ) solution .
What is general case of the equation in general case : " if you say that Table 4 : Examples of incorrectly generated questions using our student module .
Answers in italics .
5 Related Work Lewis et al. ( 2019 ) proposed the task of UQA and modeled it with an unsupervised question generation ( UQG ) task .
They make use of named entities and noun phrases as answers and create " fill - in- theblank " cloze questions as a way to achieve UQG .
Their scheme is to identify a sentence that contains an answer , which is then masked to create a cloze question .
Since these questions do not look natural , they use back -translation to convert cloze questions into more natural - looking ones .
We argue that the resulting questions are so similar to the contexts that it is hard to train a robust QA model .
Puri et al. ( 2020 ) propose to create questions by training GPT - 2 on the SQuAD training set ( Rajpurkar et al. , 2016 ) and show there is a huge performance gap between a trained QG model based on GPT - 2 and the non-trained version .
Klein and Nabi ( 2019 ) argue that since GPT - 2 is trained for general text prediction , the result is not appropriate for the QG task .
Consequently , the questions generated as such are not guaranteed to be answerable .
To overcome this problem , they propose to leverage the connection between QA and QG so that GPT - 2 is trained for QG , resulting in a QG - optimized GPT - 2 model .
Unlike these two approaches , our work uses a pre-trained version of GPT - 2 , which is not trained on a QG dataset , and demonstrates that we can improve the quality of questions in a completely unsupervised way and narrow the performance gap between the supervised and unsupervised QA .
Hinton et al. ( 2015 ) propose the teacher -student architecture for knowledge distillation .
This method allows compressing the knowledge of a large model , the teacher , into a smaller model , the student , which is also able to generalize the knowledge of the teacher .
Unlike the original purpose of this architecture , we devise a teacher module that regularizes two question generation models , while the role of the student is to generalize this regularization process .
Conclusion and Future Work
We have proposed a novel method for unsupervised question generation ( QG ) , where the questions are generated with a teacher -student architecture .
The teacher is composed of two distinctive QG models as assistants and a regularization module that attempts to stay unbiased between the two styles of QG .
As the main thrust of this structure , the regularization scheme takes a novel approach of selecting the next tokens in a probabilistic way when a question is generated .
This knowledge is implicitly transferred to the student module so that it can mitigate the drawbacks of the two QG models in the teacher module and generate higher quality questions .
To encourage further development of unsupervised question answering , we release the QA dataset generated by our student model .
With a series of experiments across the indomain MRQA shared tasks , we demonstrate the effectiveness of the proposed method as well as its generalizability .
We also provide an insight as to how the proposed method can help progress toward zero-shot and few-shot learning .
As reflected in the qualitative analysis , the current method still generates unreasonable questions that , for example , deviate too much from the context , end unnaturally , and fail to handle pronouns appropriately .
To handle those cases , we need to look into what other types of QG can serve as new teachers and how the regularization needs to evolve .
We leave it as future work in addition to the generalization of this approach to other generative models besides QG .
