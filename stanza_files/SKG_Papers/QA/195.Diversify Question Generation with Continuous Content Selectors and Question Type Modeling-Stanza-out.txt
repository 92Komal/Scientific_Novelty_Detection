title
Diversify Question Generation with Continuous Content Selectors and Question Type Modeling
abstract
Generating questions based on answers and relevant contexts is a challenging task .
Recent work mainly pays attention to the quality of a single generated question .
However , question generation is actually a one- to-many problem , as it is possible to raise questions with different focuses on contexts and various means of expression .
In this paper , we explore the diversity of question generation and come up with methods from these two aspects .
Specifically , we relate contextual focuses with content selectors , which are modeled by a continuous latent variable with the technique of conditional variational auto-encoder ( CVAE ) .
In the realization of CVAE , a multimodal prior distribution is adopted to allow for more diverse content selectors .
To take into account various means of expression , question types are explicitly modeled and a diversity - promoting algorithm is proposed further .
Experimental results on public datasets show that our proposed method can significantly improve the diversity of generated questions , especially from the perspective of using different question types .
Overall , our proposed method achieves a better trade - off between generation quality and diversity compared with existing approaches .
Introduction
As a reverse task of question answering ( QA ) , question generation ( QG ) aims to generate questions from a given answer and its relevant context .
The task holds the potential value of educational purpose to generate questions for reading comprehension materials ( Heilman and Smith , 2010 ) .
It can also be deployed as chatbot components ( Li et al. , 2017 ) for evaluating or improving mental health ( Colby , 1975 ) .
Moreover , QG can be applied to extend the question - answer pairs ( Du and Cardie , 2018 ) for QA systems .
Traditional methods for QG mainly use rigid heuristic rules to transform a sentence into related Source context : the network was engineered and operated by mci telecommunications under a cooperative agreement with the nsf .
Target question : who operated the vbsn network ?
Focus 1 : the network was engineered and operated by mci telecommunications under a cooperative agreement with the nsf . ( Ours ) who operates the network with nsf ?
Focus 2 : the network was engineered and operated by mci telecommunications under a cooperative agreement with the nsf . ( Ours ) who operated the network under a cooperative agreement with the nsf ?
Focus 3 : the network was engineered and operated by mci telecommunications under a cooperative agreement with the nsf . ( Ours ) in what company was the network engineered and operated by the nsf ?
Figure 1 : Diversified questions generated by our method for the given passage - answer pair ( answer is underlined ) .
Different questions can be raised according to distinct focuses on the context ( colored ) and various means of expression ( italic ) .
questions ( Heilman , 2011 ) .
However , these approaches heavily rely on manually crafted features , which cannot be easily generalized .
In recent years , neural techniques are applied to this task and have achieved significant progress ( Zhou et al. , 2017 ; Du et al. , 2017 ) .
Most of these methods follow the one- to - one encoder- decoder paradigm and focus on improving the quality of a single generated question ( Zhao et al. , 2018 ; .
However , given an answer and its associated context , it is possible to raise multiple questions with different focuses on the context and various means of expression .
Figure 1 shows some different questions that can be generated from a given source context .
The characteristic of diversity is inherent in QG and has the potential to enhance the value of this task .
However , the diversity is not fully explored with existing methods .
and Fan et al. ( 2018 b ) noticed this problem and modeled the variety with latent variable models .
However , the introduced latent variable was regarded as a holistic attribute , whose meaning was opaque and weakly related to the origin of diversity .
More recently , Cho et al . ( 2019 ) proposed a mixture content selection model for generation , whose diversity is determined by a fixed number of selectors .
However , the discrete property confines its variety to a large extent .
In this paper , we use a more flexible continuous latent variable for content selection to deal with different focuses on a context .
Moreover , question types are explicitly incorporated to consider different ways of expression .
With these components , a question can be generated in three steps .
Firstly , a content selector in the form of a continuous latent variable is sampled conditioning on the source context .
Secondly , a question type is predicted based on the context as well as the content selector .
Lastly , the content of a question is generated with above information about contextual focuses and means of expression .
Considering the variety of content selectors and question types , the diversity of generated questions can be ensured .
Overall , the main contributions of this paper are as follows : ?
We explicitly consider the content selection process of QG and model content selectors as a continuous latent variable for different focuses on contexts .
CVAE is utilized and the multimodal prior technique is adopted for more diverse selectors .
?
We consider various means of expression through the incorporation of question type modeling .
A diversity - promoting algorithm concerning the use of distinct question types among generations is proposed further .
?
We conduct experiments on the public datasets SQuAD and NewsQA , whose results demonstrate a better trade - off between generation quality and diversity compared with previous methods .
Further analysis demonstrates the effectiveness of our proposed components .
Related Work Automatic question generation has attracted an increasing attention from the natural language gen-eration community in recent years , which is reflected in newly published datasets ( Zhou et al. , 2017 ; Chen et al. , 2018 ) and sophisticated techniques ( Du et al. , 2017 ; Liu et al. , 2019 ) .
Traditional methods are mainly rule-based , where they first transform the source information into syntactic representation and then use templates to generate related questions ( Heilman , 2011 ) .
These methods largely depend on rigid heuristic rules and cannot be easily generalized .
In contrast to rule- based methods , neural networks have the potential to learn implicit patterns from labeled data , thus become more prevalent in question generation .
Du et al. ( 2017 ) and Zhou et al . ( 2017 ) followed the paradigm of sequenceto-sequence and showed promising results when combining rich features and attention mechanism .
and Zhou et al . ( 2019 ) incorporated answer-focused information to improve the relevance between answers and questions .
Liu et al. ( 2019 ) and Chen et al . ( 2020 ) introduced graph networks to estimate significant contents in the source context .
Most of previous work regarded question generation as a one- to - one problem and focused on improving the quality of a single generated question .
Some work noticed the diversity inherent in QG and came up with methods to consider this characteristic .
Yao et al . ( 2018 ) used a latent variable to model the holistic attributes in questions .
Similar ideas could also been found in some related work ( Jain et al. , 2017 ; Fan et al. , 2018 b ) .
However , the meaning of the holistic features is only opaque and cannot be strongly connected with diversity .
More recently , Cho et al . ( 2019 ) proposed a mixture content selection model for generation .
The diversity was determined by a fixed number of content selectors .
Different from their work , we model the latent variable of content selectors in a continuous space , which holds the potential of capturing more variety inherent in content selection .
Besides above related work , other techniques plugged into the general encoder-decoder framework can also be utilized to promote diversity ( Li et al. , 2016 ; Shen et al. , 2019 ) .
However , the particular characteristics of question generation are not fully considered in these approaches .
Method Question generation aims to model the probability of a question q given an answer a and its context c , which can be combined as the source information x = {c , a} .
To diversify generated questions , we incorporate a continuous multi-dimensional latent variable z for content selection and explicitly model question types to deal with means of expression .
Generation can be factorized into three stages .
Firstly , a content selector z is sampled conditioning on the input x .
This is used to indicate which parts of the source information should be focused on .
Secondly , a question type q t is predicted considering the specific content selector z and the input x .
Lastly , the relevant question content q c is generated with selected contents and predicted question type .
The final question q can be composed as ( q t , q c ) .
The factorization can be formulated as follows : p ? ( q|x ) = E z?p ? ( z|x ) [ p ? ( q|x , z ) ] = E z?p ? ( z| x ) [ p ? ( q t |x , z ) p ? ( q c |x , z , q t ) ] ( 1 )
The choice of a continuous latent variable as content selectors leads to more variety compared with its discrete counterpart .
CVAE ( Sohn et al. , 2015 ) is adopted to make training more tractable .
Then the objective function turns out be the evidence lower bound ( ELBO ) of logp ? ( q|x ) : L ( ? , ? ; x , q ) = E z?p ? ( z| x , q ) [ logp ? ( q|x , z ) + logp ? ( z|x ) ? logp ? ( z|x , q ) ] ( 2 ) where p ? ( z|x , q ) is incorporated to approximate the the posterior distribution p ? ( z|x , q ) . L ( ? , ? ; x , q ) can be approximated using Monte Carlo estimate and learning can be conducted with re-parameterization trick ( Kingma and Welling , 2014 ) on p ? ( z|x , q ) and p ? ( z| x ) : z ? p ? ( z|x , q ) L ( ? , ? ; x , q ) = logp ? ( q t |x , z ) + logp ?
( q c |x , z , q t ) + logp ? ( z|x ) ? logp ? ( z|x , q ) ( 3 )
The first two components in L denote the reconstruction error that forces the sampled content selector to be informative of what to focus on .
The last two components constitute a kind of regularization that drive the posterior to match the prior .
The overall architecture is illustrated in Figure 2 .
In the following subsections , we will elaborate the details of each stage .
Content Selector
In our framework , the content selector is modeled as a continuous multi-dimensional latent variable z , which is used to focus on relevant contextual information .
Following CVAE , a recognition network p ? ( z|x , q ) is defined to approximate the true posterior distribution .
As shown in the form of p ? ( z|x , q ) , it is conditioned on the source information x as well as the target question q.
As for the source information , we decompose the context c as a sequence of words { x i } n i=1 . Following Zhou et al. ( 2017 ) , we exploit lexical features to enrich word embeddings as x = { x i } n i=1 .
Then a bidirectional recurrent neural network ( Bi - RNN ) is used to produce a sequence of hidden states { h i } n i=1 .
At last , condensed source information s is aggregated with a self-attention operation : ?
i = softmax ( u T h tanh ( W h h i + b h ) ) s = n i=1 ? i h i ( 4 ) We assume the target question has content words {y t } m t=1 .
Then , the target information t can be calculated with a similar process as Equation 4 .
To model the continuous property of the latent variable z , we assume p ? ( z|x , q ) follows multivariate Gaussian distribution with a diagonal covariance matrix , hence the recognition network can be calculated as : p ? ( z|x , q ) ? N ( ? , ? 2 I ) ? log ( ?
2 ) = W r s t + b r ( 5 ) Given Equation 3 , we also need to define the prior distribution p ? ( z|x ) of the latent variable z.
Traditional methods often represent the prior as another Gaussian distribution for the sake of tractable calculation .
To enrich the model with more variety and prevent the variational posterior to be over-regularized , we adopt a multimodal prior distribution .
Gaussian mixture distribution has the potential to fit more diverse multi-dimensional data , which are suitable to enlarge the divergence between content selectors with different focuses .
Instead of introducing transformation matrices to mean and variance for each mode , we adopt the multimodal prior technique of VampPrior ( Tomczak and Welling , 2018 ) , where only marginal additive parameters are needed and overfitting can be alleviated .
More specifically , the multimodal prior distribution can be formulated as follows : p ? ( z|x ) ? 1 K K k=1 N (? k , ?
2 k I ) ? k log ( ?
2 k ) = W r s t k + b r ( 6 ) where tk denotes a pseudo-input , which is a learnable vector with the same dimension as t. K is a hyper-parameter denoting the number of modes .
Given above recognition and prior networks , we can use re-parametrization trick to obtain samples of z from p ? ( z|x , q ) ( training ) or p ? ( z|x ) ( testing ) .
With the sampled latent variable z , we can calculate what to focus on the context c : o i = sigmoid ( u
T z tanh ( W z [ h i ; z ; E [ q t ] ] + b z ) ) ( 7 ) where [ ; ] means vector concatenation .
E[ q t ] denotes the word embedding of question type q t , which will be elaborated in subsection 3.2 .
We use o to represent {o i } n i=1 for simplicity .
Question Type Predictor Given source information s and sampled content selector z , question type predictor produces a probability distribution to indicate how likely the selected contents can be inquired by different question types .
In this paper , we categorized question types according to the interrogative words commonly used in general questions .
Specifically , they are classified into 8 types - what , who , how , when , which , where , why and other ( Zhou et al. , 2019 ) .
We combine the contextual information s and the selector representation z as the input .
Two fully connected layers followed by a softmax layer are Algorithm 1 Pseudo-code for diversity - promoting question type selection algorithm .
P ? N ?
L is the question type distributions of N different samples with L types .
? inf represents the negative infinity .
decay is a hyper-parameter controlling the degree of diversity and tuned by the development set .
The algorithm returns q i t for each sample , which means its predicted question type .
1 . procedure QUESTIONTYPESELECT ( P , N , L ) 2 . for t ? { 1 , 2 , ...N } do 3 . i , j = argmax i , j { P i , j } 4 . q i t = j 5 .
P ij = ? inf j ? 1 ? L 6 . P i j ?= decay i ?
1 ? N 7 . end for 8 . return {q i t } N i=1 9 . end procedure used to estimate the final question type distribution for a relevant question .
The loss corresponds to the first item in Equation 3 : logp ?
( q t |x , z ) = log softmax ( W t 1 tanh ( W t 2 [ s ; z ] ) )
( 8 ) Given the question type predictor , we propose a diversity - promoting algorithm in the inference phase .
In Algorithm 1 , we utilize decay to explicitly control the degree of diversity for multiple generations .
Specifically , given multiple samples with their question type distributions as a whole , we iteratively pick the highest probability and assign its type to the corresponding sample .
Then , the probability of choosing the same question type for other samples will be restrained by decay .
Therefore , it is more likely to allocate different types to the rest , thus the degree of diversity in question types can be explicitly promoted .
Controlled Generator
We utilize focused encoder and decoder to make the generation process aware of the selected contents and the predicted question type .
Focused Encoder
The selected contents can be regarded as a clue indicator feature ( Liu et al. , 2019 ) , which assigns a binary value to each word to signify its importance .
To stabilize training , we use the soft version of this indicator feature , whose weight is given by o in Equation 7 .
In the inference phase , we discrete this indicator by setting a threshold ( Cho et al. , 2019 ) .
Specifically , this feature is transformed into another embedding as follows : E[ o i ] = o i E 1 + ( 1 ? o i )E 0 for training I ( o i )E 1 + ( 1 ? I ( o i ) ) E 0 for inference ( 9 ) where E 1 and E 0 correspond to the trainable embeddings for the two values of this clue indicator .
I ( o i ) represents the discreteness of the content selection probability o i .
This embedding is appended to the word embedding x i introduced in subsection 3.1 .
The resulting embeddings are denoted as { x i } n i=1 .
Then another Bi-RNN is utilized to obtain focused contextual representations as h = {h i } n i=1 .
Focused Decoder
We assume that the contextual representations h , the content selection indicator o and the question type q t should be combined to generate relevant question content q c = {y t } m t=1 , which is the remaining part of a question other than its type .
Following the traditional paradigm , a unidirectional Gated Recurrent Unit ( GRU ) ( Cho et al. , 2014 ) is employed to form the decoder .
It takes the question type q t as the initial input word y 0 and refers to representations h for attention mechanism ( Bahdanau et al. , 2015 ) .
More details can be found in the implementation of NQG ++
( Zhou et al. , 2017 ) .
Traditional methods calculate attention weights using the correlation between the hidden states of the encoder and the decoder , which is defined at the word level .
In our method , the content selector z decides what to focus on before generation , thus has the ability to provide attention at the sentence level .
This is similar to the idea used in data- to - text generation ( Mei et al. , 2016 ) .
Therefore , we combine the content selection probability o to refine the attention weights ?
t , i at position t : ? t , i = ? t , i o i n i=1 ? t , i o i ( 10 )
Note that incorporating content selection in this way is an independent operation , which can be plugged into any standard attention method .
As for generation distribution , we adopt copygenerator ( See et al. , 2017 ) to deal with the out-of- vocabulary problem .
Then , the loss function exerted on the question content , which corresponds to the second term of Equation 3 , can be calculated as follows : logp ? ( q c |x , z , q t ) = m t=1 logp ? ( y t |y <t , h , o ) ( 11 )
Training
As the selected contents play an important role in our model , we assume they are consistent with the final generation .
Although this behavior can be learned with Equation 11 in an end-to - end manner , we add an auxiliary loss function to facilitate it .
Formally , we set the gold label of content selection g i to 1 if the source token x i appears in the target question q and 0 otherwise .
Without annotations of real focuses , above labels serve as proxies to ease learning .
The loss function is thus defined as : L sel ( ? , ? ; x , q ) = n i=1 [ g i logo i + ( 1? g i ) log ( 1 ? o i ) ] ( 12 )
It is well known that a vanilla CVAE with RNN decoder has the risk of failing to encoding meaningful information in the latent variable ( Bowman et al. , 2016 ) .
Inspired by the same concern in the previous work ( Zhao et al. , 2017 ) , we also adopt the bag-of - word loss L bow ( ? , ? ; x , q ) as an auxiliary loss , which requires the latent variable to predict the words shown in the target question .
Moreover , the technique of KL cost annealing ( Bowman et al. , 2016 ) is also incorporated to let the divergence of p ? ( z|x , q ) and p ? ( z|x ) gradually influence the learning procedure .
Therefore , the overall loss function of the whole framework is defined as : L ( ? , ? ; x , q ) = L ( ? , ? ; x , q ) + L sel ( ? , ? ; x , q ) + L bow ( ? , ? ; x , q ) ( 13 ) which can be optimized by stochastic gradient descent .
Experiments
Experiment Settings Dataset
We conduct experiments on two public datasets SQuAD ( Rajpurkar et al. , 2016 ) and N ewsQA ( Trischler et al. , 2017 ) .
As for SQuAD , we follow the same corpus split by Zhou et al . ( 2017 ) and directly utilize their provided lexical features 1 . There are 86635 , 8965 and 8964 sentence - answer -question triples in the training , development and testing set respectively .
As for N ewsQA , we follow the original split of this dataset , resulting in 92549 , 5166 and 5126 triples for training , development and testing .
Implementation Details
The vocabulary is set to contain the most frequent 20000 words in each training set .
We set the dimension of word embedding to 300 and hidden size to 512 .
The representations of lexical features and focus indicator are randomly initialized as 16 - dimensional vectors .
The dimension of the latent variable z and the hidden size of the question type predictor are set to 128 .
The number of layers for RNN is set to 1 in both the encoder and the decoder .
We update the model parameters using Adam optimizer ( Kingma and Ba , 2014 ) with learning rate of 0.001 , momentum parameters ?
1 = 0.9 and ? 1 = 0.999 .
Batch size is set to 64 during training .
The development set is used to find the best model and hyper-parameters .
Our model is implemented with Pytorch 1.0.0 .
Baselines and Metrics
We compare our method with recent diversified generation methods including Truncated Sampling ( Fan et al. , 2018a ) , Diverse Beam Search ( Vijayakumar et al. , 2018 , Mixture Decoder ( Shen et al. , 2019 ) and Mixture Content Selection ( Cho et al. , 2019 ) .
The implementations and naming conventions of above baselines follow those by Cho et al . ( 2019 ) .
As for our method , to get N generations for each passage - answer pair , we sample N content selectors from the multimodal prior defined by Equation 6 . Given these content selectors , question types are promoted to be distinct with Algorithm 1 and greedy search is conducted for a fair comparison .
Note that there is no restriction on the number of prior modes ( K ) to get N samples .
However , it is a natural choice to set K = N and get a sample from each mode .
We name this model as N - M. Prior .
In further analysis , we will also show the influence of setting different values to K .
We use metrics 2 adopted by Cho et al . ( 2019 )
evaluate generation quality and diversity : Top - 1 metric ( ? )
This measures the top - 1 accuracy ( BLEU - 4 ) among the N - best generations .
Oracle metric ( ? )
This measures the upper bound of top - 1 accuracy ( Oracle BLEU - 4 ) by comparing the best hypothesis among the top -N generations with the target question .
The metric reflects the overall quality of top -N generations .
Pairwise metric ( ? )
This measures the withindistribution similarity .
The metric computes the average of sentence - level metrics ( Self BLEU - 4 ) between one sentence and the rest in a generated collection .
Low pairwise metric indicates high diversity .
Given these metrics , we come up with a comprehensive measurement to balance generation quality and diversity .
Overall metric ( ? )
This measures the overall performance concerning both quality and diversity : Top - 1 metric ?
Oracle metric ?
Pairwise metric Also , we introduce other two metrics regarding with the diversity of generated question types .
Type coverage metric ( ? )
This measures the percentage that the question type of the target question is covered by top -N generations .
Type diversity metric ( ? )
This measures the average number of distinct question types in top -N generations .
Results and Analysis Results compared with baselines
The experimental results on SQuAD are displayed in Table 1 .
The table shows that the quality of generated questions with our method ( N - M. Prior ) scores comparable BLEU - 4 to the state - of - the - art , which is much superior compared with methods based on beam search and sampling .
Moreover , from the perspective of diversity , our method performs evidently better than other mixture models , resulting in the best trade - off between diversity and quality as shown by the overall metric .
Furthermore , focusing on the measurements concerning question types , we can find that our model demonstrates significant improvements from both the coverage and the diversity , which are caused by the explicit modeling and diversifying of question types .
We can observe the similar phenomenon that our method performs better with regard to the diversity metrics from the performance on N ewsQA in Table 2 .
We also conduct human evaluation comparing Table 5 : Experiments on SQuAD with different numbers of prior modes ( K = 1 , 3 , 5 ) when generating multiple samples ( N = 3 , 5 ) .
the diversity of the generated questions from our model 3 - M. Prior with other mixture model baselines in Table 3 .
The table shows that our method outperforms its counterparts in terms of diversity with statistical significance .
Diversifying question types
As described in Algorithm 1 , the diversity of question types can be explicitly controlled by setting different values of decay .
The influence is clearly shown in the Figure 3 ( a ) .
As decay gradually increases , the diversity of question types increases as well as their coverage of the golden type .
Also , from the Figure 3 ( b ) , we can see that , a small value of decay results in better generation quality metrics .
The reason is that the incorporation of more diverse question types may lead to more possibilities of raising good questions .
As its value continues to grow , the diversity keeps on increasing at the risk of inappropriate question types used , which results in a slight degradation of the generation quality .
We can select an appropriate decay value according to the overall metric .
Ablation Analysis
To show the effects of important components in our model , we conduct an ablation study on SQuAD .
As shown in Table 4 , the proposed diversity - promoting algorithm can clearly improve the generation diversity with nearly no negative impact on the quality , which can also be shown in Figure 3 when decay is small .
As for content selection , incorporating its influence in the encoder-decoder architecture improves the overall metric obviously .
Also , we observe that the auxiliary loss function on selected contents can make a big difference , demonstrating its necessity to make content selectors focus on diverse and valid text pieces .
Moreover , learning tricks about CVAE contribute to a more informative latent variable and improve the diversity evidently .
Influence of multimodal prior distribution
The continuous property of content selectors make it possible to generate N questions even given a standard gaussian prior .
However , the introduction of multimodal prior can enrich content selectors with more variety and lead to more diverse generations .
As shown in Table 5 , the number of prior modes ( K = 1 , 3 , 5 ) has an effect on metrics when generating multiple questions ( N = 3 , 5 ) .
First , we can see that the multimodal prior has the ability to improve the generation diversity compared with the standard one , which tallies with our conjecture .
Second , when experimenting with the setting N = K , almost all of the metrics are better .
We can explain this from the fact that samples of content selectors can be taken from different prior modes , which are more diverse .
Also , inference accords with the training process in this situation .
Qualitative Analysis
Figure 4 shows an example of the generated questions from our model 3 - M. Prior and its mixture model counterparts .
As shown in this example , our generations often varies in question types and exhibit more diversity .
Moreover , we highlight the selected contents of each generation from our model in Figure 1 , which shows the effectiveness of our content selection module .
As we use the multimodal prior technique , the diversity of generated questions can be reflected from both intra and inter modes .
We can see from Figure 5 that different from other mixture models which can only generate a fixed number of questions , our continuous modeling option makes it possible to produce more generations by sampling from each mode repeatedly .
In this example , questions from different modes exhibit a larger divergence compared with those from the same one , which demonstrates once more that the use of a multimodal prior makes a difference to the generation diversity .
Conclusion
In this paper , we explicitly diversify the question generation from the perspectives of contextual focuses and means of expression .
We model focuses through continuous content selectors and introduce a multimodal prior to allow for more diverse selectors .
We consider various means of expression through the modeling of question types and a related diversity - promoting algorithm .
On public datasets , our approach achieves the best trade - off between generation quality and diversity .
Further analysis also demonstrates the effectiveness of our proposed model components .
Figure 2 : 2 Figure 2 : The framework of our model for diverse question generation , which can be decomposed into three stages .
