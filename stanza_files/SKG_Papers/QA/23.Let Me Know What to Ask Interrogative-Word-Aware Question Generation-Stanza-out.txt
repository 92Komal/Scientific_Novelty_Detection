title
Let Me Know What to Ask : Interrogative -Word -Aware Question Generation
abstract
Question Generation ( QG ) is a Natural Language Processing ( NLP ) task that aids advances in Question Answering ( QA ) and conversational assistants .
Existing models focus on generating a question based on a text and possibly the answer to the generated question .
They need to determine the type of interrogative word to be generated while having to pay attention to the grammar and vocabulary of the question .
In this work , we propose Interrogative - Word - Aware Question Generation ( IWAQG ) , a pipelined system composed of two modules : an interrogative word classifier and a QG model .
The first module predicts the interrogative word that is provided to the second module to create the question .
Owing to an increased recall of deciding the interrogative words to be used for the generated questions , the proposed model achieves new state - of - the - art results on the task of QG
Introduction Question Generation ( QG ) is the task of creating questions about a text in natural language .
This is an important task for Question Answering ( QA ) since it can help create QA datasets .
It is also useful for conversational systems like Amazon Alexa .
Due to the surge of interests in these systems , QG is also drawing the attention of the research community .
One of the reasons for the fast advances in QA capabilities is the creation of large datasets like SQuAD ( Rajpurkar et al. , 2016 ) and TriviaQA ( Joshi et al. , 2017 ) .
Since the creation of such datasets is either costly if done manually or prone to error if done automatically , reliable and mean - * Equal contribution .
ingful QG can play a key role in the advances of QA ( Lewis et al. , 2019 ) .
QG is a difficult task due to the need for understanding of the text to ask about and generating a question that is grammatically correct and semantically adequate according to the given text .
This task is considered to have two parts : what to ask and how to ask .
The first one refers to the identification of relevant portions of the text to ask about .
This requires machine reading comprehension since the system has to understand the text .
The latter refers to the creation of a natural language question that is grammatically correct and semantically precise .
Most of the current approaches utilize sequence - to-sequence models , composed of an encoder model that first transforms a passage into a vector and a decoder model that given this vector , generates a question about the passage ( Liu et al. , 2019 ; Sun et al. , 2018 ; Zhao et al. , 2018 ; Pan et al. , 2019 ) .
There are different settings for QG .
Some authors like ( Subramanian et al. , 2018 ) assumes that only a passage is given , attempts to find candidate key phrases that represent the core of the questions to be created .
Others follow an answer - aware setting , where the input is a passage and the answer to the question to create ( Zhao et al. , 2018 ) .
We assume this setting and consider that the answer is a span of the passage , as in SQuAD .
Follow-ing this approach , the decoder of the sequence - tosequence model has to learn to generate both the interrogative word ( i.e. , wh- word ) and the rest of the question simultaneously .
The main claim of our work is that separating the two tasks ( i.e. , interrogative - word classification and question generation ) can lead to a better performance .
We posit that the interrogative word must be predicted by a well - trained classifier .
We consider that selecting the right interrogative word is the key to generate high-quality questions .
For example , a question with a wrong interrogative word for the answer " the owner " is : " what produces a list of requirements for a project ? " .
However , with the right interrogative word , who , the question would be : " who produces a list of requirements for a project ? " , which is clear that is more adequate regarding the answer than the first one .
According to our claim , the independent classification model can improve the recall of interrogative words of a QG model because 1 ) the interrogative word classification task is easier to solve than generating the interrogative word along with the full question in the QG model and 2 ) the QG model would be able to generate the interrogative word easily by using the copy mechanism , which can copy parts of the input of the encoder .
With these hypotheses , we propose Interrogative -Word - Aware Question Generation ( IWAQG ) , a pipelined system composed of two modules : an interrogative - word classifier that predicts the interrogative word and a QG model that generates a question conditioned on the predicted interrogative word .
Figure 1 shows a highlevel overview of our approach .
The proposed model achieves new state - of- theart results on the task of QG in SQuAD , improving from 46.58 to 47.69 in 21.24 to 22.33
Related Work Question Generation ( QG ) problem has been approached in two ways .
One is based on heuristics , templates and syntactic rules ( Heilman and Smith , 2010 ; Mazidi and Nielsen , 2014 ; Labutov et al. , 2015 ) .
This type of approach requires a heavy human effort , so they do not scale well .
The other approach is based on neural networks and it is becoming popular due to the recent progress of deep learning in NLP ( Pan et al. , 2019 ) .
Du et al. ( 2017 ) is the first one to propose an sequence - to- sequence model to tackle the QG problem and outperformed the previous state - of - the - art model using human and automatic evaluations .
Sun et al. ( 2018 ) proposed a similar approach to us , an answer - aware sequence - to- sequence model with a special decoding mode in charge of only the interrogative word .
However , we propose to predict the interrogative word before the encoding stage , so that the decoder can focus more on the rest of the question rather than on the interrogative word .
Besides , they cannot train the interrogativeword classifier using golden labels because it is learned implicitly inside the decoder .
Duan et al. ( 2017 ) proposed , in a similar way to us , a pipeline approach .
First , the authors create a long list of question templates like " who is author of " , and " who is wife of " .
Then , when generating the question , they select first the question template and next , they fill it in .
To select the question template , they proposed two approaches .
One is a retrievalbased question pattern prediction , and the second one is a generation - based question pattern prediction .
The first one has the problem that is computationally expensive when the question pattern size is large , and the second one , although it yields to better results , it is a generative approach and we argue that just modeling the interrogative word prediction as a classification task is easier and can lead to better results .
As far as we know , we are the first one to propose an explicit interrogativeword classifier that provides the interrogative word to the question generator .
3 Interrogative -Word -Aware Question Generation
Problem Statement Given a passage P , and an answer A , we want to find a question Q , whose answer is A .
More formally : Q = arg max Q P rob ( Q|P , A ) We assume that P is a paragraph composed of a list of words : P = {x t } M t=1 , and the answer is a subspan of P .
We model this problem with a pipelined approach .
First , given P and A , we predict the interrogative word I w , and then , we input into QG module P , A , and I w .
The overall architecture of our model is shown in 2 .
Interrogative -Word Classifier
As discussed in section 5.2 , any model can be used to predict interrogative words if its accuracy is high enough .
Our interrogative - word classifier is based on BERT , a state - of - the - art model in many NLP tasks that can successfully utilize the context to grasp the semantics of the words inside a sentence ( Devlin et al. , 2018 ) .
We input a passage that contains the answer of the question we want to build and add the special token [ ANS ] to let BERT knows that the answer span has a special meaning and must be used differently to the rest of the passage .
As required by BERT , the first token of the input is the special token [ CLS ] , and the last is [ SEP ] .
This [ CLS ] token embedding originally was designed for classification tasks .
In our case , to classify interrogative words , it learns how to represent the context and the answer information .
On top of BERT , we build a feed-forward network that receives as input the [ CLS ] token embedding concatenated with a learnable embedding of the entity type of the answer , as shown on the left side of Figure 2 .
We propose to utilize the entity type of the answer because there is a clear correlation between the answer type of the question and the entity type of the answer .
For example , if the interrogative word is who , the answer is very likely to have an entity type person .
Since we are using [ CLS ] token embedding as a representation of the context and the answer , we consider that using an explicit entity type embedding of the answer could help the system .
Question Generator
For the QG module , we employ one of the current state - of - the - art QG models ( Zhao et al. , 2018 ) .
This model is a sequence - to-sequence neural network that uses a gated self-attention in the encoder and an attention mechanism with maxout pointer in the decoder .
One way to connect the interrogative - word classifier to the QG model is to use the predicted interrogative word as the first output token of the decoder by default .
However , we cannot expect a perfect interrogative - word classifier and also , the first word of the questions is not necessarily an interrogative word .
Therefore , in this work , we add the predicted interrogative word to the input of the QG model to let the model decide whether to use it or not .
In this way , we can condition the generated question on the predicted interrogative word effectively .
Encoder
The encoder is composed of a Recurrent Neural Network ( RNN ) , a self-attention network , and a feature fusion gate ( Gong and Bowman , 2018 ) .
The goal of this fusion gate is to combine two intermediate learnable features into the final encoded passage - answer representation .
The input of this model is the passage P .
It includes the answer and the predicted interrogative word I w , which is located just before the answer span .
The RNN receives the word embedding of the tokens of this text concatenated with a learnable metaembedding that tags if the token is the interrogative word , the answer of the question to generate or the context of the answer .
Decoder
The decoder is composed of an RNN with an attention layer and a copy mechanism ( Gu et al. , 2016 ) .
The RNN of the decoder at time step t receives its hidden state at the previous time step t ?
1 and the previously generated output y t?1 .
At t = 0 , it receives the last hidden state of the encoder .
This model combines the probability of generating a word and the probability of copying that word from the input as shown on the right side of Figure 2 .
To compute the generative scores , it uses the outputs of the decoder , and the context of the encoder , which is based on the raw attention scores .
To compute the copy scores , it uses the outputs of the RNN and the raw attention scores of the encoder .
Zhao et al. ( 2018 ) observed that the repetition of words in the input sequence tends to create repetitions in the output sequence too .
Thus , they proposed a maxout pointer mechanism instead of the regular pointer mechanism ( Vinyals et al. , 2015 ) .
This new pointer mechanism limits the magnitude of the scores of the repeated words to their maximum value .
To do that , first , the attention scores are computed over the input sequence and then , the score of a word at time step t is calculated as the maximum of all scores pointing to the same word in the input sequence .
The final probability distribution is calculated by applying the softmax function on the concatenation of copy scores and generative scores and summing up the probabilities pointing to the same words .
Experiments
In our experiments , we study our proposed system on SQuAD dataset v1.1 .
( Rajpurkar et al. , 2016 ) , prove the validity of our hypothesis and compare it with the current state of the art .
Dataset
In order to train our interrogative - word classifier , we use the training set of SQuAD v1.1 ( Rajpurkar et al. , 2016 ) .
This dataset is composed of 87599 instances , however , the number of interrogative words is not balanced as seen in 1 .
For a fair comparison with previous models , we train the QG model on the training set of SQuAD and split by half the dev set into dev and test randomly as .
Implementation
The interrogative - word classifier is made using the PyTorch implementation of BERT - base-uncased made by HuggingFace 1 .
It was trained for three epochs using cross entropy loss as the objective function .
The entity types are obtained using spaCy 2 .
If spaCy cannot return an entity for a given answer , we label it as None .
The dimension of the entity type embedding is 5 .
The input dimension of the classifier is 773 ( 768 from BERT base hidden size and 5 from the entity type embedding size ) and the output dimension is 8 since we predict the interrogative words : what , which , where , when , who , why , how , and others .
The feed -forward network consists of a single layer .
For optimization , we used Adam optimizer with weight decay and learning rate of 5e - 5 .
The QG model is based on the model proposed by ( Zhao et al. , 2018 ) with small modifications using Py - Torch .
The encoder uses a BiLSTM and the decoder uses an LSTM .
During training , the QG model uses the golden interrogative words to enforce the decoder to always copy the interrogative word .
On the other hand , during inference , it uses the interrogative word predictions from the classifier .
Evaluation
We perform an automatic evaluation using the metrics : BLUE -1 , BLUE -2 , BLUE -3 , BLUE - 4 ( Papineni et al. , 2002 ) , METEOR ( Lavie and Denkowski , 2009 ) and ROUGE -L ( Lin , 2004 ) .
In addition , we perform a qualitative analysis where we compare the generated questions of the baseline ( Zhao et al. , 2018 ) , our proposed model , the upper bound performance of our model , and the golden question .
Results
Comparison with Previous Models
Our interrogative - word classifier achieves an accuracy of 73.8 % on the test set of SQuAD .
Using this model for the pipelined system , we compare the performance of the QG model with respect to the previous state - of - the - art models .
Table 2 shows the evaluation results of our model and the current state - of - the - art models , which are briefly described below .
?
Zhou et al . ( 2017 ) is one of the first authors who proposed a sequence - to-sequence model with attention and copy mechanism .
They also proposed the use of POS and NER tags as lexical features for the encoder .
?
Zhao et al. ( 2018 ) proposed the model in which we based our QG module .
?
Kim et al. ( 2019 ) proposed QG architecture that treats the passage and the target answer separately .
?
Liu et al. ( 2019 ) proposed a sequence - tosequence model with a clue word predictor using a Graph Convolutional Networks to identify if each word in the input passage is a potential clue that should be copied into the generated question .
Our model outperforms all other models in all the metrics .
This improvement is consistent , around 2 % .
This is due to the improvement in the recall of the interrogative words .
All these measures are based on the overlap between the golden question and the generated question , so using the right interrogative word , we can improve these scores .
In addition , generating the right interrogative word also helps to create better questions since the output of the RNN of the decoder at time step t also depends on the previously generated word .
Upper Bound Performance of IWAQG
We analyze the upper bound improvement that our QG model can have according to different levels of accuracy of the interrogative - word classifier .
In order to do that , instead of using our interrogativeword classifier , we use the golden labels of the test set and generated noise to simulate a classifier with different accuracy levels .
Table 3 and Figure 3 show a linear relationship between the accuracy of the classifier and the IWAQG .
This demonstrates the effectiveness of our pipelined approach regardless of the interrogative - word classifier model .
In addition , we analyze the recall of the interrogative words generated by our pipelined system .
As shown in the Table 4 , the total recall of using only the QG module is 68.29 % , while the recall of our proposed system , IWAQG , is 74.10 % , an improvement of almost 6 % .
Furthermore , if we assume a perfect interrogative - word classifier , the recall would be 99.72 % , a dramatic improvement which proves the validity of our hypothesis .
Effectiveness of the input of interrogative words into the QG model
In this section , we show the effectiveness of inserting explicitly the predicted interrogative word into the passage .
We argue that this simple way of connecting the two models exploits the characteristics of the copy mechanism successfully .
Table 3 : Performance of the QG model with respect to the accuracy of the interrogative - word classifier . " * " is our implementation of the QG module without our interrogative - word classifier ( Zhao et al. , 2018 ) . see in Figure 4 , the attention score of the generated interrogative word , who , is relatively high for the predicted interrogative word and lower for the other words .
This means that it is very likely that the interrogative word inserted into the passage is copied as intended .
Figure 4 : Attention matrix between the generated question ( Y-axis ) and the given passage ( X- axis ) .
Qualitative Analysis
In this section , we present a sample of the generated questions of our model , the upper bound model ( interrogative - word classifier accuracy is 100 % ) , the baseline ( Zhao et al. , 2018 ) , and the golden questions to show how our model improves the recall of the interrogative words with respect to the baseline .
In general , our model has a better recall of interrogative words than the baseline which leads us to a better quality of questions .
However , since we are still far from a perfect interrogativeword classifier , we also show that questions that our current model cannot generate correctly could be generated well if we had a better classifier .
As we can see in Table 5 , in the first three examples the interrogative words generated by the baseline are wrong , while our model is right .
In addition , due to the wrong selection of interrogative words , in the second example , the topic of the question generated by the baseline is also wrong .
On the other hand , since our model selects the right interrogative word , it can create the right question .
Each generated word depends on the previously generated word because of the generative LSTM model , so it is very important to select correctly the first word , i.e. the interrogative word .
However , the performance of our proposed interrogative - word classifier is not perfect , if it had a 100 % accuracy , then , we could improve the quality of the generated questions like in the last two examples .
Ablation Study
We tried to combine different features shown in Table 6 for the interrogative - word classifier .
In this section , we analyze their impact on the performance of the model .
The first model is only using the [ CLS ] BERT token embedding ( Devlin et al. , 2018 ) ( Zhao et al. , 2018 ) . is the passage where the answer appears but , the model does not know where the answer is .
The second model is the previous one with the entity type of the answer as an additional feature .
The performance of this model is a bit better than the first one but it is not enough to be utilized effectively for our pipeline .
In the third model , the input is the passage .
This model uses the average of the answer token embeddings generated by BERT along with the [ CLS ] token embedding .
As we can see , the performance noticeably increased , which indicates that answer information is the key to predict the interrogative word needed .
In the fourth model , we added the special token [ ANS ] at the beginning and at the end of the answer span to let BERT knows where the answer is in the passage .
So the input to the feedforward network is only the [ CLS ] token embedding .
This model clearly outperforms the previous one , which shows that BERT can exploit the answer information better if it is tagged with the [ ANS ] token .
The fifth model is the same as the previous one but with the addition of the entitytype embedding of the answer .
The combination of the three features ( answer , answer entity type , and passage ) yields to the best performance .
In addition , we provide the recall and precision per class for our final interrogative - word classifier ( CLS + AT in Table 7 ) .
As we can see , the overall recall is high , and it is also higher than just using the QG module ( Table 4 ) , which proves our hypothesis that modeling the interrogative - word prediction task as an independent classification problem yields to a higher recall than generating them with the full question .
However , the recall of which is very low .
This is due to the intrinsic difficulty of predicting this interrogative words .
Questions like " what country " and " which country " can be correct depending on the context , but the meaning is very similar .
Our model has also problem with why due to the lack of training instances for this class .
Lastly , the recall of ' when is also low because many questions of this type can be formulated with other interrogative words , e.g. : instead of " When did WWII start ? " , we can ask " In which year did WWII start ? " .
Class Recall Precision
Conclusion and Future Work
In this work , we proposed an Interrogative - Word - Aware Question Generation ( IWAQG ) , a pipelined model composed of an interrogative - word classifier and a question generator to tackle the question generation task .
First , we predict the interrogative word .
Then , the Question Generation ( QG ) model generates the question using the predicted interrogative word .
Thanks to this independent interrogative - word classifier and the copy mechanism of the question generation model , we are able to improve the recall of the interrogative words in the generated questions .
This improvement also leads to a better quality of the generated questions .
We prove our hypotheses through quantitative and qualitative experiments , showing that our pipelined system outperforms the previous state - of - the - art models .
Lastly , we also prove that which country is the most dependent on arab oil ?
Japan Table 5 : Qualitative Analysis .
Comparison between the baseline , our proposed model , the upper bound of our model , the golden question and the answer of the question . " * " is our implementation of the QG module without our interrogative - word classifier ( Zhao et al. , 2018 ) . our methodology is remarkably effective , showing a theoretical upper bound of the potential improvement using a more accurate interrogativeword classifier .
In the future , we would like to improve the interrogative - word classifier , since it would clearly improve the performance of the whole system as we showed .
We also expect that the use of the Transformer architecture ( Vaswani et al. , 2017 ) could improve the QG model .
In addition , we plan to test our approach on other datasets to prove its generalization capability .
Finally , an interesting application of this work could be to utilize QG to improve Question Answering systems .
Figure 1 : 1 Figure 1 : High- level overview of the proposed model .
