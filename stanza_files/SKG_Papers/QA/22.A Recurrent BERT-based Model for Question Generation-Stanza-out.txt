title
A Recurrent BERT - based Model for Question Generation
abstract
In this study , we investigate the employment of the pre-trained BERT language model to tackle question generation tasks .
We introduce three neural architectures built on top of BERT for question generation tasks .
The first one is a straightforward BERT employment , which reveals the defects of directly using BERT for text generation .
Accordingly , we propose another two models by restructuring our BERT employment into a sequential manner for taking information from previous decoded results .
Our models are trained and evaluated on the recent question - answering dataset SQuAD .
Experiment results show that our best model yields state - of - the - art performance which advances the BLEU 4 score of the existing best models from 16.85 to 22.17 .
Introduction Question generation ( QG ) problem , which takes a context text and an answer phase as input and generates a question corresponding to the given answer phase , has received tremendous interests in recent years from both industrial and academic natural language processing communities ( Zhao et al. , 2018 ; Du et al. , 2017 ) .
The state- of - the - art model mainly adopts neural QG approaches : training a neural network based on sequence - to-sequence framework .
So far , the best performing result is reported in ( Zhao et al. , 2018 ) , which advances the state - of - the - art results from 13.9 to 16.85 ( BLEU 4 ) .
The existing QG models mainly rely on recurrent neural networks ( RNN ) , e.g. long short - term memory LSTM network ( Hochreiter and Schmidhuber , 1997 ) or gated recurrent unit ( Chung et al. , 2014 ) , augmented by attention mechanisms ( Luong et al. , 2015 ) .
However , the inherent sequential nature of the RNN models suffers from the problem of handling long sequences .
Therefore , the existing QG models ( Du et al. , 2017 ; mainly use only sentence - level information as a context text for question generation .
When applied to a paragraph - level context , the existing models show significant performance degradation .
However , as indicated by ( Du et al. , 2017 ) , providing paragraph - level information can improve QG performance .
For handling long context , the work ( Zhao et al. , 2018 ) introduces a maxout pointer mechanism with a gated self-attention encoder for processing paragraph - level input .
The work reports state - of- the - art performance .
Recently , the NLP community has seen the excitement around neural learning models that make use of pre-trained language models ( Devlin et al. , 2018 ; Radford et al. , 2018 ) .
The latest development is BERT , which has shown significant performance improvement over various natural language understanding tasks , such as document summarization , document classification , etc .
Given the success of the BERT model , a natural question follows : can we leverage the BERT models to further advance the state - of - the - art for QG tasks ?
By our study , the answer is yes .
Intuitively , the BERT employment brings two advantages for tackling the QG problem .
First , as reported by studies ( Devlin et al. , 2018 ; Radford et al. , 2018 ) , employing pre-training language models has shown to be effective for improving NLP tasks .
Second , the BERT model is a stack of multi-layer Transformer block ( Vaswani et al. , 2017 ) , which eschews recurrence structure and relies entirely on self-attention mechanism to draw global dependencies between input sequences .
With the Transformer blocks , processing paragraph - level contexts for QG are therefore to be possible .
In this study , we investigate the employment of the pre-trained BERT language model to tackle question generation tasks .
We introduce three neural architectures built on top of BERT for question generation tasks .
The first one is a straightforward BERT employment , which reveals the defects of directly using BERT for text generation .
As will be shown in the experiment , the naive BERT employment ( called BERT - QG , BERT Question Generation ) offers poor performance , as by construction , BERT produces all tokens at a time without considering decoding results in previous steps .
We find that the question generated by the naive employment is not even a readable sentence .
As a result , we propose a sequential question generation model based on BERT as our second model called BERT - SQG ( BERT - Sequential Question Generation ) for taking information from previous decoded results .
As will shown in the performance evaluation , the BERT - SQG model outperforms the exiting best model ( Zhao et al. , 2018 ) by advancing the state - of- the - art results from 16.85 to 21.04 ( BLEU 4 ) . Furthermore , we propose an augmented model called BERT - HLSQG ( Highlight Sequential Question Generation ) for further enhancing the performance of the BERT - SQG .
Our BERT -HLSQG model works by marking the answer with [ HL ] tokens to avoid possible ambiguity in specifying answers for question generation .
Such design further improves the BLEU 4 score from 21.04 to 22.17 .
The contribution of this paper is summarized as follows . ?
In this paper , we investigate the employment of using the BERT model for QG tasks .
We show that the sequential structure is important for the decoding of text generation .
Aiming at this point , we propose two sequential question generation models based on BERT in this paper .
?
Furthermore , we propose a simple but effective input encoding scheme , which inserts special highlighting tokens [ HL ] before and after the given answer span , to address the ambiguity issue when an answer phase appears multiple times in the question .
?
Extensive experiments are conducted using benchmark datasets , and the experiment results show the effectiveness of our question generation model .
Our model outperforms the existing best models ( Zhao et al. , 2018 ) and pushes the state - of - the - art result from 16.85 to 22.17 ( BLEU 4 ) .
The rest of this paper is organized as follows .
In Section 2 , we discuss the related work for QG generation .
In Section 3 , we review the BERT model ( the basic building block for our model ) .
In Section 4 , we introduce our models for question generation , and Section 5 provides the experiment results .
In Section 6 , we conclude the paper and discuss future work .
Related Work
The question generation has been mainly tackled with two types of approaches .
One is built on top of heuristic rules that creates questions with manually constructed template and ranks the generated results ( Heilman and Smith , 2010 ; Mazidi and Nielsen , 2014 ; Labutov et al. , 2015 ) .
In ( Labutov et al. , 2015 ) , the authors propose to use a crowdsourcing policy to generate question templates from a large amount of text to generate question .
The research in ( Heilman and Smith , 2010 ) proposes to use manually written rules to perform a sequence of general - purpose syntactic transformations to turn declarative sentences into questions .
The generated questions are then ranked by a logistic regression model to select the qualified questions for later use .
And , the research in ( Yao et al. , 2012 ) proposes to convert the sentence into a Minimal Recursion Semantics ( MRS ) representation through linguistic parsing , and then construct semantic structures and grammar rules from the representation to generate questions through the manually designed rules .
Those approaches heavily depend on human effort , which makes them hard to scale up and being generalized in various domains .
The other one , which is becoming increasingly popular , is to train an end-to - end neural network from scratch by using sequence to sequence or encoder-decoder framework , e.g . ( Du et al. , 2017 ; Song et al. , 2017 ; Zhao et al. , 2018 ) .
( Du et al. , 2017 ) pioneered the work of automatic QG tasks using an end-to - end trainable seq2seq neural model .
Automatic and human evaluation results showed that the proposed model outperformed the previous rule- based systems ( Heilman and Smith , 2010 ; Rus et al. , 2010 ) .
However , in their study , there was no control about which part of the context text the generated question was asking about .
On the other hands , the work Figure 1 : BERT input architecture .
Input Transformer block embedding is the sum of the three embeddings , and then use the hidden vector to fine tune each task .
propose to encode answer location information using an annotation vector corresponding to the answer word positions .
utilized rich features of the passage including answer positions .
deployed a twostage neural model that detects important phrases and accordingly generates questions conditioned on the important phases .
combined supervised and reinforcement learning in the training of their model using policy gradient techniques to maximize several rewards that measure question quality .
Instead of using an annotation vector to tag the answer locations , the ( Song et al. , 2017 ) propose to employ a unified framework for QG and question answering by encoding both the answer and the passage with a multi-perspective matching mechanism .
Further , proposed joint models to address QG and question answering as a multi-task learning setting .
conducted QG for improving question answering .
Due to the mixed objectives including question answering , the performance reported by their work was lower than the state - of - the - art results .
In ( Zhao et al. , 2018 ) , authors propose a maxout pointer mechanism with a gated self-attention encoder to solve the problem of processing long context for question generation .
All above-mentioned models are RNN base models , which suffers from the issue of processing long context / sequences .
Compared with the RNN based model , our models based on BERT composed by transformer models ( Vaswani et al. , 2017 ) .
As shown in the later section , the question generated by our model is more semantically coherent and fluent .
BERT Overview
The BERT model is built by a stack of multi-layer bidirectional Transformer encoder ( Vaswani et al. , 2017 ) .
The BERT model has three architecture parameter settings : the number of layers ( i.e. , transformer blocks ) , the hidden size , and the number of self-attention heads in a transformer block .
For using BERT model , the input is required to be aligned as the BERT 's specific input sequence .
In general , a special token [ CLS ] is inserted as the first token for BERT 's input sequence .
The final hidden state of the [ CLS ] token is designed to be used as a final sequence representation for classification tasks .
The input token sequence can be a pack of multiple sentences .
To distinguish the information from different sentences , a special token [ SEP ] is added between the tokens of two consecutive sentences .
In addition , a learned embedding is added to every token to denote whether it belongs to which sentence .
For example , given a sentence pair ( s i , s j ) where s i contains |s i | tokens and s j contains |s j | tokens , the BERT input sequence is formulated as a sequence in the following form : X = ( [ CLS ] , t i ,1 , ... , t i , |s i | , [ SEP ] , t j,1 ... , t j , |s j | )
As shown in Figure 1 , the input representation of a given token is the sum of three embeddings : the token embeddings , the segmentation embeddings , and the position embeddings .
Then the input representation is fed forward into extra layers to perform a fine-tuning procedure .
The BERT model can be employed in three language modeling tasks : sequence - level classification , spanlevel prediction , and token - level prediction tasks .
The fine-tuning procedure is performed in a taskspecific manner .
The details of our fine-tuning procedure are introduced in the later subsections .
BERT for Question Generation
In the following subsections , we introduce our models for QG .
In Subsection 4.1 , we introduce the naive BERT employment ( BERT - QG ) , which serves as a first cut for using BERT for QG .
BERT - QG offer poor performance but draws some insights for using BERT in QG tasks .
Further , in Subsection 4.2 , we introduce BERT - SQG by considering sequential information when generating questions .
Last , in Subsection 4.3 , we introduce BERT - HLSQG which shows the SOTA results for QG based on BERT .
BERT -QG
As an initial attempt , we first adapt the BERT model for QG as follows .
First , for a given context paragraph C = [ c 1 , ... , c | C | ] and an answer phase A = [ a 1 , ... , a | A | ] , the input sequence X is aligned as X = ( [ CLS ] , C , [ SEP ] , A , [ SEP ] ) Let BERT ( ) be the BERT model .
We first obtain the hidden representation H ? R | X |?h by H = BERT ( X ) , where | X | is the length of the input sequence and h is the size of the hidden dimension .
Then , H is passed to a dense layer W ? R h?|V | followed by a softmax function as follows .
P r( w|x i ) = sof tmax ( H ? W + b ) , ?x i ?
X qi = argmax w P r( w| x i )
The softmax is applied along the dimension of the sequence .
All of the parameters of BERT and W are fine-tuned jointly to maximize the logprobability of the correct token q i .
The model architecture is shown in Figure 2 .
As such , a sequence of tokens [ w 1 , ... , w | x | ] is generated and we use the first generated [ SEP ] symbol as the end of the generated question sentence .
BERT -SQG
In text generation tasks , as proposed by ( Sutskever et al. , 2014 ) , considering the previous decoded results has significant impacts on the quality of the generated text .
However , in BERT - QG , the token generation is performed without previous decoded result information .
Due to this consideration , we propose a sequential question generation model based on BERT ( called BERT - SQG ) .
In BERT - SQG , we take into consideration the previous decoded results for decoding a token .
We adapt the BERT model for question generation as follows .
First , for a given context paragraph C = [ c 1 , ... , c | C | ] and an answer phase A = [ a 1 , ... , a | A | ] , and Q = [ q1 , ... , qi ] the input sequence X i is formulated as X i = ( [ CLS ] , C , [ SEP ] , A , [ SEP ] , q1 , ... , qi , [ MASK ] )
Then , the input sequence X i is represented by the BERT embedding layers and then travel forward into the BERT model .
After that , we take the final hidden state ( i.e. , the output of the Transformer blocks ) for the last token [ MASK ] in the input sequence .
We denote the final hidden vector of [ MASK ] as h [ MASK ] ?
R h .
We adapt BERT model by adding an affine layer W SQG ?
R h?|V | to the output of the [ MASK ] token .
We compute the label probabilities P r( w| X i ) ?
R |V | by a softmax function as follows .
P r( w| X i ) = sof tmax ( h [ MASK ] ?
W SQG + b SQG ) qi = argmax w P r( w| X i )
Subsequently , the newly generated token qi is appended into X and the question generation process is repeated ( as illustrated in Figure 3 ) with the new X until [ SEP ] is predicted .
We report the generated tokens as the predicted question .
In Table 1 , we give an example of the actual running of the model .
BERT -HLSQG
In BERT - SQG , we find there are two shortcomings for producing quality results .
First , when processing lengthy context , we find that the generated question is often with lower quality .
Second , when an answer phase appears multiple times in the context , there is ambiguity for select which one to generate questions .
As a result , poor results are reported when we use the BLEU score for performance evaluation .
To address these shortcomings , we propose to further restructure BERT - SQG as follows .
x i iter0 [ CLS ]
The Super Bowl 50 was played at Santa Clara , California . [ SEP ] Santa Clara , California .
[ SEP ] [ MASK ]
Where iter1 [ CLS ]
The Super Bowl 50 was played at Santa Clara , California . [ SEP ] Santa Clara , California . [ SEP ]
Where [ MASK ] did iter 2 [ CLS ]
The Super Bowl 50 was played at Santa Clara , California . [ SEP ] Santa Clara , California . [ SEP ]
Where did [ MASK ] Super iter3 [ CLS ]
The Super Bowl 50 was played at Santa Clara , California . [ SEP ] Santa Clara , California . [ SEP ]
Where did Super [ MASK ] Bowl iter4 [ CLS ]
The Super Bowl 50 was played at Santa Clara , California . [ SEP ] Santa Clara , California . [ SEP ]
Where did Super Bowl [ MASK ] 50 iter5 [ CLS ]
The Super Bowl 50 was played at Santa Clara , California . [ SEP ] Santa Clara , California . [ SEP ]
Where did Super Bowl 50 [ MASK ] take iter6 [ CLS ]
The Super Bowl 50 was played at Santa Clara , California . [ SEP ] Santa Clara , California . [ SEP ]
Where did Super Bowl 50 take [ MASK ] place ?
iter7 [ CLS ]
The Super Bowl 50 was played at Santa Clara , California . [ SEP ] Santa Clara , California . [ SEP ]
Where did Super Bowl 50 take place [ MASK ] [ SEP ] iter8 [ CLS ]
The Super Bowl 50 was played at Santa Clara , California .
The observation for doing so is that we observe that for a long context , the answer phase often appears multiple times in the context , which causes ambiguity for the model for knowing which one as a target to generate question sentence .
Thus , we design [ HL ] token to avoid possible ambiguity .
With C , the input sequence X can be formulated as X i = ( [ CLS ] , C , [ SEP ] , q1 , ... , qi , [ MASK ] )
Figure 4 shows the BERT - HLSQG model architecture .
At each iteration , for generating q i , we take the final hidden state vector h [ MASK ] ?
R h of the last token [ MASK ] in the input sequence .
and connect it to an affine layer W HLSQG ? R h?|V | .
We compute the label probabilities P r( w| X i ) ?
R |V | by a softmax function as follows .
P r( w| X i ) =sof tmax ( h [ MASK ] ?
W HLSQG + b HLSQG ) qi = argmax w P r( w| X i )
We show a running example of BERT - HLSQG in Table 2 .
Performance Evaluation
In this section , we present the performance evaluation results on the QG task on SQuAD ( Rajpurkar et al. , 2016 ) dataset .
Datasets
The SQuAD dataset contains 536 Wikipedia articles and 100K reading comprehension questions ( and the corresponding answers ) posed about the articles .
Answers of the questions are text spans in the articles .
We use the same data split settings as the previous work on the QG tasks ( Du et al. , 2017 ; Zhao et al. , 2018 ) to directly compare the state - of- theart results on QG tasks .
Table 3 summarizes statistics for the compared datasets .
? SQuAD 73 K
In this set , we follow the same setting as ( Du et al. , 2017 ) ; the accessible parts of the SQuAD training data are randomly divided into a training set ( 80 % ) , a development set ( 10 % ) , and a test set ( 10 % ) .
We report results on the 10 % test set .
Table 3 : Dataset statistics : SQuAD 73 K is the setting of ( Du et al. , 2017 ) , and SQuAD 81 K is the setting of ( Zhao et al. , 2018 ) . ? SQuAD 81K
In this set , we follow the same setting as ( Zhao et al. , 2018 ) ; the accessible SQuAD development data set is divided into a development set ( 50 % ) , and a test set ( 50 % ) .
We report results on the 50 % test set .
Performance Metrics
We use the evaluation package released by ( Sharma et al. , 2017 ) .
The package includes BLEU 1 , BLEU 2 , BLEU 3 , BLEU 4 ( Papineni et al. , 2002 ) , METEOR ( Denkowski and Lavie , 2014 ) and ROUGE ( Lin , 2004 ) evaluation scripts .
BLEU measures the average n-gram precision on a set of reference sentences , with a penalty for overly short sentences .
BLEU -n is a BLEU score variant that uses up to n-grams for counting cooccurrences .
METEOR is a recall-oriented metric , which computes the similarity between the generated sentences and ground truth sentences by considering synonyms , stemming and paraphrases .
ROUGE is commonly employed to evaluate ngrams recall of the summaries with gold standard sentences as references .
ROUGE -L ( measured based on the longest common subsequence ) results are reported .
Implementation Details
We use the PyTorch version of BERT 1 to train our BERT - QG , BERT -SQG and BERT - HLSQG models .
The pre-trained model uses the officially provided BERT base model ( 12 layers , 768 hidden dimensions , and 12 attention heads . ) with a vocab of 30522 words .
Dropout probability is set to 0.1 between transformer layers .
The Adamax optimizer is applied during the training process , with an initial learning rate of 5e - 5 .
The batch size for the update is set at 28 .
All our models use two TITAN RTX GPUs for 5 epochs training .
We use Dev. data for epoch model to make predictions and select the highest accuracy rate as our score evaluation model .
Also , in our BERT - SQG and BERT - HLSQG model , we use the Beam Search strategy for sequence decoding .
The beam size is set to 3 .
Model Comparison
In this paper , we compare our models with the best performing models ( Du et al. , 2017 ; Zhao et al. , 2018 ) in the literature .
The compared models in the experiment are : ? NQG -RC ( Du et al. , 2017 ) : A seq2seq question generation model based on bidirectional LSTMs .
? PLQG ( Zhao et al. , 2018 ) :
A seq2seq network which contains a gated self-attention encoder and a maxout pointer decoder to en-able the capability of handling long text input .
The PLQG model is the state - of - the - art models for QG tasks .
Quantitative Results
Table 5 shows the comparison results using sentence - level context texts and Table 6 shows the results on paragraph - level context .
We compare the models using standard metric BLEU , ROUGE -L , and METEOR .
We have the following findings to note about the results .
First , as can be observed , BERT - QG offers poor performance .
The performance of BERT - QG is far from the results by other models .
This result is expected as BERT - QG generates the sentences without considering the previous decoded results .
However , when taking into account the previous decoded results ( BERT - SQG ) , we effectively utilize the power of BERT and yield the state - of - theart result compared with the existing RNN variants for QG .
Also , we see that BERT - HLSQG successfully address the limitation of BERT - SQG .
As shown in Table 5 , BERT - HLSQG outperforms the existing best performing model by 4 - 5 % on both benchmark datasets .
Second , the results in Table 6 further show that BERT - SQG successfully processes the paragraphlevel contexts and further push the state - of - the - art from 16.85 to 21.04 in terms of BLEU 4 score .
Note that NQG - RC and PLQG both use the RNN architecture , and the RNN - based models all suffer from the issue of consuming long text input .
We see that the BERT model based on Transformer blocks effectively addresses the issue of processing long text .
In addition , the improvement of BERT - HLSQG is more obvious under paragraph - level , which advances the score from 21.04 to 22.17 in terms of BLEU 4 score .
Again , this result validates that our BERT - HLSQG model does improve the shortcomings of BERT - SQG and achieves the best score at the paragraph - level context .
Evaluation Result on Reading Comprehension Task
One issue we find in our performance evaluation is that we observe questions generated by our models are good but with a very low BLEU score .
The problem for this result comes from that BLEU score is token - basis ; the generated question is compared with a golden standard based on the token similarity .
A question might be expressed in different ways ( but semantically the same ) ; there are many different ways of describing the same thing / question .
We think the score computed based on tokens can not truly reflect the performance of our model .
In order to demonstrate the effectiveness of our model , we further evaluate our model through reading comprehension ( RC ) tasks .
Given a context and a question , a reading comprehension task returns the answer span to the question from the given context .
In this experiment , we compare and examine the impact of the question sentences generated by the BERT - SQG and BERT - HLSQG models on the RC task to further validate our model .
Implementation Details
In this set of experiments , our goal is to examine the difference between using human- generated questions and questions generated by our QG models to train a reading comprehension model .
Specifically , we use the training data set provided by the SQuAD and divided the training data set into QG set ( 50 % ) and RC set ( 50 % ) .
Then , we train BERT - SQG and BERT - HLSQG models using QG sets .
The model is then used to generate questions to generate the RC - SQG and RC - HLSQG sets .
Finally , we use RC , RC - SQG and RC - HLSQG sets for reading comprehension task training , and compare Exact Match and F1 score with the RC model ( the one trained by RC set ) .
Our RC model is also implemented based on the PyTorch version BERT model and fine-tuned on the officially BERT base pre-training model .
The dropout rate is set to 0.1 for all Transformer layers .
The optimizer is performed using AdamW , with an initial learning rate of 3e - 5 .
The batch size for the update is set at 8 .
All RC models use two TITAN RTX GPUs for 2 epochs training .
Results and Analysis Table 4 shows the human question and generated question experiment comparison results .
We observe that the RC - SQG and RC - HLSQG data sets generated using the model for question generation differed only 4 - 5 % from the results of the human question data set on the Exact Match and the F1 Score is only 3 - 4 % .
The average token on the question is also close to the human question .
Conclusion
In this paper , we propose models that generate a question from the input context ( sentence or paragraph ) and the target answer based on BERT models .
Our models are transformer models which can handle long-term dependencies well .
To make the generation process sequential , we propose to restructure our model to generate one word at a time , using the encoded task inputs and the previously generated words as inputs to the transformer .
The best model outperforms previous RNN - based state - of - the -arts in terms of standard NLG metrics ( BLEU , ROUGE , METEOR ) and of whether a standard QA model can correctly answer the generated questions .
While our model is simple , our model achieves state - of - the - art performance at both sentence - level and paragraph - level input and provides strong baselines for future research .
Figure 2 : 2 Figure 2 : The BERT - QG architecture
