title
End-to- End Non-Factoid Question Answering with an Interactive Visualization of Neural Attention Weights
abstract
Advanced attention mechanisms are an important part of successful neural network approaches for non-factoid answer selection because they allow the models to focus on few important segments within rather long answer texts .
Analyzing attention mechanisms is thus crucial for understanding strengths and weaknesses of particular models .
We present an extensible , highly modular service architecture that enables the transformation of neural network models for non-factoid answer selection into fully featured end-to - end question answering systems .
The primary objective of our system is to enable researchers a way to interactively explore and compare attentionbased neural networks for answer selection .
Our interactive user interface helps researchers to better understand the capabilities of the different approaches and can aid qualitative analyses .
The source-code of our system is publicly available .
1
Introduction Attention - based neural networks are increasingly popular because of their ability to focus on the most important segments of a given input .
These models have proven to be extremely effective in many different tasks , for example neural machine translation ( Luong et al. , 2015 ; Tu et al. , 2016 ) , neural image caption generation ( Xu et al. , 2015 ) , and multiple sub-tasks in question answering ( Hermann et al. , 2015 ; Yin et al. , 2016 ; Andreas et al. , 2016 ) .
Attention - based neural networks are especially successful in answer selection for non-factoid ques-tions , where approaches have to deal with complex multi-sentence texts .
The objective of this task is to re-rank a list of candidate answers according to a non-factoid question , where the best-ranked candidate is selected as an answer .
Models usually learn to generate dense vector representations for questions and candidates , where representations of a question and an associated correct answer should lie closely together within the vector space ( Feng et al. , 2015 ) .
Accordingly , the ranking score can be determined with a simple similarity metric .
Attention in this scenario works by calculating weights for each individual segment in the input ( attention vector ) , where segments with a higher weight should have a stronger impact on the resulting representation .
Several approaches have been recently proposed , achieving state - of- the - art results on different datasets Wang et al. , 2016 ) .
The success of these approaches clearly shows the importance of sophisticated attention mechanisms for effective answer selection models .
However , it has also been shown that attention mechanisms can introduce certain biases that negatively influence the results ( Wang et al. , 2016 ) .
As a consequence , the creation of better attention mechanisms can improve the overall answer selection performance .
To achieve this goal , researchers are required to perform in - depth analyses and comparisons of different approaches to understand what the individual models learn and how they can be improved .
Due to the lack of existing tool - support to aid this process , such analyses are complex and require substantial development effort .
This important issue led us to creating an integrated solution that helps researchers to better understand the capabilities of different attention - based models and can aid qualitative analyses .
In this work , we present an extensible service architecture that can transform models for non -
System Overview
To transform attention - based answer selection models into end-to - end question answering systems , we rely on a service orchestration that integrates multiple independent webservices with separate responsibilities .
Since all services communicate using a well - defined HTTP REST API , our system achieves strong extensibility properties .
This makes it simple to replace individual services with own implementations .
A high- level view on our system architecture is shown in Figure 1 .
For each question , we retrieve a list of candidate answers from a given dataset ( candidate retrieval ) .
We then rank these candidates with the answer selection component ( candidate ranking ) , which integrates the attention - based neural network model that should be explored .
The result contains the topranked answers and all associated attention weights , which enables us to interactively visualize the attention vectors in the user interface .
Our architecture is similar to the pipelined structures of earlier work in question answering that rely on a retrieval step followed by a more expensive supervised ranking approach ( Surdeanu et al. , 2011 ; Higashinaka and Isozaki , 2008 ) .
We primarily chose this architecture because it allows the user to directly relate the results of the system to the answer selection model .
The use of more advanced components ( e.g. query expansion or answer merging ) would negate this possibility due to the added complexity .
Because all components in our extensible service architecture are loosely coupled , it is possible to use multiple candidate ranking services with different attention mechanisms at the same time .
The user interface exploits this ability and allows researchers to interactively compare two models side- by- side within the same view .
A screenshot of our UI is shown in Figure 2 , and an example of a side-by-side comparison is available in Figure 4 .
In the following sections , we describe the individual services in more detail and discuss their technical properties .
Candidate Retrieval
The efficient retrieval of answer candidates is a key component in our question answering approach .
It allows us to narrow down the search space for more sophisticated , computationally expensive attentionbased answer selection approaches in the subsequent step , and enables us to retrieve answers within seconds .
We index all existing candidates of the target dataset with ElasticSearch , an opensource high- performance search engine .
Our service provides a unified interface for the retrieval of answer candidates , where we query the index with the question text using BM25 as a similarity measure .
The service implementation is based on Scala and the Play Framework .
Our implementation contains data readers that allow to index InsuranceQA ( Feng et al. , 2015 ) and all publicly available dumps of the StackExchange platform .
2 Researchers can easily add new datasets by implementing a single data reader class .
Analysis
Enabling researchers to directly relate the results of our question answering system to the answer selection component requires the absence of major negative influences from the answer retrieval component .
To analyze the potential influence , we evaluated the list of retrieved candidates ( size 500 ) for existing questions of InsuranceQA and of different StackExchange dumps .
Questions in these datasets have associated correct answers , 3 which we treat as the ground - truth that should be included in the retrieved list of candidates .
Otherwise it would be impossible for the answer selection model to find the correct answer , and the results would be negatively affected .
Table 1 shows the number of questions with candidate lists that include at least one ground -truth answer .
Since the ratio is sufficiently high for all analyzed datasets ( 83 % to 88 % ) , we conclude that the chosen retrieval approach is a valid choice for our end-to - end question answering system .
Candidate Ranking
The candidate ranking service provides an interface to the attention - based neural network , which the researcher chose to analyze .
It provides a method to rank a list of candidate answers according to a given question text .
An important property is the A high- level view on the framework structure is shown in Figure 3 .
A particularly important property is the dynamic instantiation and combination of module implementations .
A central configuration file is used to define all necessary options that enable to train and evaluate neural networks within our framework .
An excerpt of such configuration is shown in Listing 1 .
The first four lines describe the module import paths of the desired implementations .
Our framework dynamically loads and instantiates the configured modules and uses them to perform the training procedure .
The remaining lines define specific configuration options to reference resource paths or to set specific neural network settings .
This modular structure enables a high flexibility and provides a way to freely combine different models , training procedures , and data readers .
Additionally , our framework is capable of starting a seamlessly integrated webserver that uses a configured model to rank candidate answers .
Since model states can be saved , it is possible to load pretrained models to avoid a lengthy training process .
QA - Frontend and User Interface
The central part of our proposed system is the QA - Frontend .
This component coordinates the other services and combines them into a fully functional question answering system .
Since our primary goal was to provide a way to explore and compare attention - based models , we especially focused on the user interface .
Our UI fulfills the following requirements : 1 d a t a ? module : d a t a .
i n s u r a n c e q a . v2 2 model?
module : model .
a p l s t m 3 t r a i n i n g ? module : t r a i n i n g . dynamic 4 e v a l u a t i o n ? module : e v a l u a t i o n .
d e f a u l t 5 6 d a t a :
We implemented the user interface with modern web technologies , such as Angular , TypeScript , and SASS .
The QA - Frontend service was implemented in Python with Flask .
It is fully configurable and allows multiple candidate ranking services to be used at the same time .
A screenshot of our user interface is shown in Figure 2 .
In the top row , we include an input field that allows users to enter the question text .
This input field also contains a dropdown menu to select the target model that should be used for the candidate ranking .
This makes it possible to ask the same question for multiple models and compare the outputs to gain a better understanding of the key differences .
Below this input field we offer multiple ways to interactively change the attention visualization .
In particular , we allow to change the sensitivity s and the threshold t of the visualization component .
We calculate the opacity of an attention highlight o i that corresponds to the weight w i in position i as follows : a = min ( w std , w max ? w avg ) ( 1 ) o i = s ?
w i ? wavg a , if w i ? w avg + a ? t 0 , otherwise ( 2 ) Where w avg , w std and w max are the average , standard deviation and maximum of all weights in the text .
We use a instead of w std because in rare cases it can occur that w std > w max ?
w avg , which would lead to visualizations without fully opaque positions .
These two options make it possible to adapt the attention visualization to fit the need of the analysis .
For example , it is possible to only highlight the most important sections by increasing the threshold .
On the other hand , it is also possible to highlight all segments that are slightly relevant by increasing the sensitivity and at the same time reducing the threshold .
When the user hovers over an answer and the target model employs a two -way attention mechanism , the question input visualizes the associated attention weights .
To get a more in - depth view on the attention vectors , the user can hover over any specific word in a text to view the exact value of the associated weight .
This enables numerical comparisons and helps to get an advanced understanding of the employed answer selection model .
Finally , each answer offers the option to compare the attention weights to the output of another configured model .
This action enables a side-byside comparison of different attention mechanisms and gives researchers a powerful tool to explore the advantages and disadvantages of the different approaches .
A screenshot of a side-by-side visualization is shown in Figure 4 .
It displays two attention mechanisms that result in very different behavior .
Whereas the model to the left strongly focuses on few individual words ( especially in the question ) , the model to the right is less selective and focuses on more segments that are similar .
Our user interface makes it simple to analyze such attributes in detail .
Conclusion
In this work , we presented a highly extensible service architecture that can transform non-factoid answer selection models into fully featured end-toend question answering systems .
Our key contribution is the simplification of in- depth analyses of attention - based models to non-factoid answer selection .
We enable researchers to interactively explore and understand their models qualitatively .
This can help to create more advanced attention mechanisms that achieve better answer selection results .
Besides enabling the exploration of individual models , our user interface also allows researchers to compare different attention mechanisms side- by-side within the same view .
All components of our system are highly modular which allows it to be easily extended with additional functionality .
For example , our modular answer retrieval component makes it simple to integrate new datasets , and our answer ranking framework allows researchers to add new models without requiring to change any other part of the application .
The source-code of all presented components as well as the user interface is publicly available .
We provide a documentation for all discussed APIs .
Figure 2 : 2 Figure 2 : The user interface of our question answering system with the interactive visualization of neural attention weights .
The UI includes several options to adapt the attention visualization .
