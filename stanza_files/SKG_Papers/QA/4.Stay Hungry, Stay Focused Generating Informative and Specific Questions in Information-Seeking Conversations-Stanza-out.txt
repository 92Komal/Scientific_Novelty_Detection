title
abstract
We investigate the problem of generating informative questions in information - asymmetric conversations .
Unlike previous work on question generation which largely assumes knowledge of what the answer might be , we are interested in the scenario where the questioner is not given the context from which answers are drawn , but must reason pragmatically about how to acquire new information , given the shared conversation history .
We identify two core challenges : ( 1 ) formally defining the informativeness of potential questions , and ( 2 ) exploring the prohibitively large space of potential questions to find the good candidates .
To generate pragmatic questions , we use reinforcement learning to optimize an informativeness metric we propose , combined with a reward function designed to promote more specific questions .
We demonstrate that the resulting pragmatic questioner substantially improves the informativeness and specificity of questions generated over a baseline model , as evaluated by our metrics as well as humans .
Stay Hungry , Stay Focused : Generating Informative and Specific Questions in Information - Seeking Conversations
1 Introduction Conversations are a primary means to seek and communicate information between humans , where asking the right question is an important prerequisite for effective exchange of knowledge .
Learning to ask questions in conversations can help computer systems not only acquire new knowledge , but also engage human interlocutors by making them feel heard ( Huang et al. , 2017 ) .
Previous work on question generation often falls into three classes : generating questions according to a discrete schema or end goal ( Bordes et al. , 2017 ; Zhang et al. , 2018 b ) , transforming the answer statement into a question ( Mitkov et al. , 2003 ; Rus et al. , 2010 ; Heilman and Smith , 2010 ) , or generating questions with data-driven systems by con-Figure 1 : Asking questions in a conversation to acquire information .
In this communication setting , the question asker has access to the background and topic , but no access to the private textual knowledge that contains the answer .
In this example , the baseline non-pragmatic question generator ( BL ) generates an uninformative question ( one that has already been answered ) , while our pragmatic system ( Ours ) and humans ( Ref ) actively seek new information .
from ( Du et al. , 2017 ; . Despite their successful adaptation to conversations to predict the question that elicits the observed answer ( Gao et al. , 2019 ; Pan et al. , 2019 ; Nakanishi et al. , 2019 ) , they are not suitable for modeling communication of knowledge in open-domain conversations , because the crucial problem of what to communicate has already been assumed to be addressed by conditioning on the schema of information need or the context that contains the answer .
We instead study the problem of question generation in a more realistic setting , i.e. , in opendomain information -seeking conversations where the question asker cannot access the answering context .
This is an important step towards practical natural language processing ( NLP ) systems that can reason about the state of mind of agents they interact with purely through natural language interactions , so that they can generate more help - ful responses .
In this paper , we build a question generator that reasons pragmatically about what information the answerer can provide , and generates questions to gather new information in a conversation ( see Figure 1 for an example ) .
We identify several key challenges in this task : ( 1 ) generating informative questions without access to potential answers ; ( 2 ) evaluating generated questions beyond comparing them to the reference question , because multiple questions can reveal unseen information despite being very different to each other ; ( 3 ) navigating a large search space of potential questions to improve informativeness by reasoning about the other agent 's knowledge , which is more complex than limited reference games in previous work on computational pragmatics .
To address these issues , we first develop a baseline question generation model that generates questions in a conversation without conditioning on the unseen knowledge .
We then propose automatic metrics to quantify how much new information questions reveal , as well as how specific they are to the conversation .
Next , we use reinforcement learning to optimize our question generator on these metrics .
In our experiments on the QuAC dataset , we show that the proposed method substantially improves the specificity and informativeness of the generated questions as evaluated by our automatic metrics .
These results are corroborated by blinded human evaluation , where questions generated by our system are also of higher overall quality than those by the baseline system as judged by humans .
To recap , our main contributions are : 1 ?
To the best of our knowledge , our work represents the first attempt at studying question generation to seek information in open-domain communication , which involves challenging NLP problems , e.g. , evaluation of open-ended language generation and pragmatic reasoning ; ?
To address these problems , we propose automatic metrics to quantify the informativeness and specificity of questions , which are essential for efficient iterative system development ; ?
We show that optimizing the proposed metrics via reinforcement learning leads to a system that behaves pragmatically and has improved communication efficiency , as also verified by human evaluation .
This represents a practical method for pragmatic reasoning in an opendomain communication setting .
Related Work Question Generation .
Question generation has long been studied in the education and psychology communities as a means to assess and promote reading comprehension in humans ( Davey and McBride , 1986 ) .
In natural language processing , question generation has been explored to improve the systems in various natural language processing tasks , e.g. , the quality of question answering systems ( Duan et al. , 2017 ) as well as information retrieval in an open-domain question answering system ( Nogueira et al. , 2019 ) .
Some of the first question generation systems are rule- based ( Mitkov et al. , 2003 ; Rus et al. , 2010 ; Heilman and Smith , 2010 ) , while large-scale question answering datasets , e.g. , SQuAD ( Rajpurkar et al. , 2016 ( Rajpurkar et al. , , 2018 , have recently kindled research interest in data-driven approaches .
Du et al. ( 2017 ) and apply sequence - to-sequence ( seq2seq ) models to generate SQuAD questions from Wikipedia sentences containing the answers .
The release of large conversational question answering datasets such as QuAC ( Choi et al. , 2018 ) and CoQA ( Reddy et al. , 2019 ) enabled Gao et al. ( 2019 ) , Pan et al. ( 2019 ) and Nakanishi et al . ( 2019 ) to extend previous neural seq2seq question generators by conditioning them on the conversation history and the context that contains the answer , while Scialom and Staiano ( 2019 ) remove answers to the reference question to generate curiosity - driven questions from the rest of the context .
Despite their success , most existing approaches to question generation are limited to either reading comprehension settings where potential answers are known a priori , or goal-oriented settings where the schema of knowledge is limited ( Bordes et al. , 2017 ; Zhang et al. , 2018 b ) .
This prevents them from being applied to an open-domain communication setting , where the purpose of questions is to acquire information that is unknown ahead of time .
Evaluating System-generated Questions .
Automatic evaluation of system- generated text has long been an important topic in NLP .
Traditional ngram overlap- based approaches ( Papineni et al. , 2002 ; Lin , 2004 ) are computationally efficient , but have been shown to correlate poorly with human judgement of quality ( Novikova et al. , 2017 ) .
More recently , Zhang et al. ( 2020 ) leverage large pretrained language models ( BERT , Devlin et al. , 2019 ) to relax the limitation of exact n-gram overlap .
Hashimoto et al. ( 2019 ) combine human judgement with system-reported likelihood of generated text to make population - level estimates of quality and diversity .
However , most existing metrics either evaluate generated text against very few references , or provide only relative ranking for multiple systems at a population level rather than reliable feedback for each example .
This renders them inapplicable to generating informative questions in a conversation , where multiple questions can be equally informative and relevant in a given scenario , and per-example feedback is necessary .
Pragmatic Reasoning for Informativeness .
Pragmatic reasoning is tightly related to informativeness and efficiency in communication .
Starting from the cooperative maxims for conversational pragmatic reasoning ( Grice , 1975 ) , Frank and Goodman ( 2012 ) developed a computational framework that has been applied to reference games with images ( Andreas and Klein , 2016 ) and colors ( Monroe et al. , 2017 ) , as well as generating descriptions for images ( Cohn - Gordon et al. , 2019 ) . Decision-theoretic principles ( van Rooy , 2003 ) have also been applied to quantify the informativeness of community questions ( Rao and Daum ? III , 2018 ) .
These approaches usually assume that either the list of referents ( images , colors , or answers ) or the space of utterances ( descriptions or questions ) is enumerable or can be directly sampled from , or both .
More crucially , the speaker agent usually has complete access to this information to readily gauge the effect of different utterances .
We instead study a more realistic information - seeking setting , where the questioner cannot access the answers , let alone aggregate them for pragmatic reasoning , and where these simplifying assumptions will not hold .
Method
In this section , we outline the setup for the communication problem we set out to address , present a baseline system , and lay out our approach to extending it to reason pragmatically to acquire information more efficiently .
Problem Setup
We consider a communication game between two agents , a teacher and a student ( see Figure 1 for an example ) .
The two agents share a common topic of discussion T ( Background and Topic in the figure ) , as well as a common goal for the student to acquire some knowledge K on this topic that only the teacher has direct access to ( Private Knowledge in the figure ) .
We consider the scenario where the agents can only communicate to each other by engaging in a conversation , where the conversation history H is shared between the agents .
We further constrain the conversation to one where the student asks questions about the shared topic , and the teacher provides answers based on K .
Note that this setup is very similar to that of the " Game of Interrogation " by ( Groenendijk , 1999 ) , except we relax the definition , using natural language instead of focusing on predicate logic , as we will detail in the sections that follow .
In this paper , we are interested in building a model of the student ( question asker ) in this scenario .
Specifically , we investigate how to enable the student to reason pragmatically about which questions to ask to efficiently acquire knowledge , given only the topic T and the conversation history H .
This setting of information -seeking conversations involves many interesting and challenging problems in natural language processing : ?
Quantifying textual information .
We need to be able to quantify how much knowledge the student has acquired from K. ?
Evaluating language generation when a single reference is insufficient .
At any state in the conversation , there is usually more than one valid question , some more effective and more appropriate than others .
To address this problem , we need to come up with evaluation metrics and objective functions accordingly , rather than relying on the similarity between generated questions and the single reference that is available in existing datasets .
?
Pragmatic reasoning with partial information and a large search space .
In order to train computational agents capable of pragmatic reasoning , previous work typically takes the approach of either limiting the space of referents , or the space of possible utterances , or both .
However , the former is infeasible in a communication setting as the student does not have access to K beyond what is already revealed in the conversation , and the latter is also impractical for natural conversations that cover a diverse set of topics .
We address these challenges by proposing two automatic reward functions that evaluate the informativeness and specificity of questions , and optimizing them with reinforcement learning .
Generating Questions in Conversations
Before we delve into the proposed approaches for training a question generator model to be pragmatic , an introduction of the model itself is due .
For the purposes of this paper , we assume that the shared topic T , the shared conversation history H , and the teacher 's knowledge K ( which the student has no access to ) are all made available to agents in natural language .
Since we consider information -seeking conversations only , the conversation history is grouped into pairs of questions and answers : H = [ ( q 1 , a 1 ) , ( q 2 , a 2 ) , . . . , ( q | H | , a | H | ) ] .
To generate conversational questions , we build a sequence - to-sequence model that encodes the information available to the student and decodes it into the next question in the conversation ( see Figure 2 ( a ) ) .
Specifically , we first model the shared topic T with a bi-directional LSTM ( BiLSTM ) ( Hochreiter and Schmidhuber , 1997 ) , and use the resulting topic representation h T in the conversation encoder .
Then we obtain a representation of the conversation with hierarchical LSTM encoders : we first encode each pair of question and answer with h T using a BiLSTM , then feed these pair representations into a unidirectional LSTM in the direction that the conversation unfolds .
To generate the question , we apply an LSTM decoder with attention both on the topic and the conversation history ( Bahdanau et al. , 2015 ) .
This allows us to efficiently batch computation for each conversation by sharing these representations across different turns .
We include detailed description of the model in Appendix A .
As a baseline , we train this model to minimize the negative log likelihood ( NLL ) of questions observed in the training set : NLL = ?
1 N p N i=1 | H ( i ) | j=1 log P ? ( q ( i ) j | H ( i ) <j , T ) , ( 1 ) where ? stands for model parameters , N is the number total conversations in the training dataset , H ( i ) the conversation history of the i-th conversation in the dataset , and N p = N i=1 | H ( i ) | is the total number of question - answer pairs in the training dataset .
Intuitively , this trains the model to mimic the observed questions in the dataset , but does not provide guarantees or assessment of how well generated questions are actually able to acquire information from the teacher agent .
Evaluating Informativeness through Question Answering
In order to train the question generation model to generate pragmatically apt questions that reveal new information from K , we need to be able to quantify informativeness in communication first .
However , informativeness is difficult to quantify in an open-domain dialogue , and sometimes even subjective .
In this paper , we focus on providing an objective metric for how much new information is revealed by a question .
Since questions do not reveal information directly , but rather rely on the answers to them to introduce new facts into the conversation , we begin by defining the informativeness of an answer a once it is provided .
Specifically , we are interested in characterizing how much new information an answer a reveals about K beyond what is already provided in the conversation history H <j up until this point in the conversation .
Theoretical quantities like mutual information might seem appealing in this context given their strong grounding in information theory .
However , applying them would potentially require us to fully specify the state space the world can be in for an open-domain conversation , as well as estimating the probability distribution over potential configurations , neither of which is trivial , if feasible .
Therefore , we turn to more practical quantities in defining the informativeness of an answer a given the conversation history H <j by leveraging the observation that , the more new information an answer reveals about K , the more likely it involves words that have not already been mentioned in H <j .
Therefore , making use of the unigram precision function Prec( a , a ) between the predicted answer a and an answer a that is already provided in the conversation history H <j , we define the informativeness of the predicted answer as follows I ans ( a ; H <j ) := 1 ? max 1?k<j Prec( a , a k ) , ( 2 ) Intuitively , the more a overlaps with any of the previously revealed answers , the less new information it contains .
This metric of informativeness has the advantages of objectivity and ease of automatic evaluation .
Also note that the choice of unigram precision is here not one of necessity but simplicity and practicality .
It is in principle interchangeable
Question Answering Model Question Classifier
The band released their third album ... The follow - up album , Parade , was released in June 1984 , and ...
BiGRU BiGRU
What other songs were on the album ?
Bi-Attention BiGRUs + Self-Attention + Residual Conn . with more sophisticated models of fuzzy text overlap ( e.g. , BERTScore ( Zhang et al. , 2020 ) ) .
We use this definition of answer informativeness to define the utility of potential questions .
Specifically , we define the informativeness of a question as the amount of new information it can immediately reveal through its answer I( q ; C <j ) :=
I ans ( QA ( q , C <j ) , H <j ) , ( 3 ) where C <j = ( H <j , T , K ) is the complete context available to the teacher up until the question is raised , QA(q , C <j ) is a pretrained conversational question answering ( QA ) model that answers the question q from the knowledge source K given this context .
This is equivalent to using a point estimate for P ( a|q , C <j ) to evaluate q's expected utility , which is practical for pragmatic reasoning at scale by avoiding the need for aggregating over a large set of candidate answers for each question .
In contrast , previous work on pragmatics often require probabilistic normalization in the space of speaker utterances ( questions ) and listener actions ( answers ) , which is intractable in our setting .
This definition of informativeness is also explainable : it is easy for a human to inspect the answer provided by the QA model and compare it to previous ones to understand how much new information has been revealed .
Note that this definition itself also does not rely on any specific QA model , although more accurate QA models could result in more accurate estimates of informativeness .
For simplicity , we use a bidirectional attention flow model ( Seo et al. , 2017 ) with self-attention ( Clark and Gardner , 2018 ) as adapted for conversational QA by Choi et al . ( 2018 ) ( see Figure 2 ( b ) ) .
Evaluating Question Specificity
Now that we have a metric to evaluate informativess , can we maximize it and obtain a good model for generating pragmatic conversational questions ?
It turns out that there are two issues with na?vely optimizing this value : generated questions could be overly generic or disruptive of the conversation flow while still acquiring new information .
For instance , questions like
What else ?
almost always reveal new information .
On the other hand , in our example in Figure 1 , Did they go on tour for their 1983 album ?
seems more disruptive ( topicchanging ) as the next question in the conversation than the candidate questions in the figure .
To address this , we take a similar approach to previous work by selecting negative examples to target these issues and training a classifier to distinguish them from questions that were actually part of the conversation ( Lowe et al. , 2017 ; Rao and Daum ? III , 2018 ) .
Once this classifier is trained , we can make use of the score it assigns different candidate questions to evaluate how specific each is to the current conversation history .
Specifically , we select two kinds of negative questions : frequent questions from the training set ( frequency > 1 ) and random questions other than the observed one from the same conversation .
We train a model ( with shared parameters with the QA model , see Figure 2 ( b ) ) to assign a probability that a question is the true next question ( positive ) given the conversation history , and define this quantity as the specificity of the question S(q ; H <j , T ) := P ? ( q is positive |H <j , T ) , ( 4 ) where ? is the parameters of the classifier optimized with binary cross entropy loss .
Once this classifier is trained jointly with the QA model , we can use this specificity reward to bias the model towards generating questions that are not only informative , but also specific to the given conversation history .
Conceptually , our specificity idea is related to a few separate but connected concepts in NLP , namely discourse coherence , relevance , and reducing genericness in natural language generation .
The coherence and relevance of a piece of text in a discourse is highly correlated with the perceived quality of the generated text .
Previous work has approached generating coherent utterances in conversations through encouraging the model to learn similar distributed representations throughout the conversation ( Baheti et al. , 2018 ; Xu et al. , 2018 ; Zhang et al. , 2018a ) .
In contrast , we achieve the same goal with a discriminative classifier , which is trained to contrast the true follow - up question ( relevant and coherent ) against randomly sampled questions ( irrelevant ) from other conversations and out-of-order questions ( uncoherent ) .
The idea of discerning discourse consistency has also been applied to large pretrained language models ( Devlin et al. , 2019 ; Iter et al. , 2020 ) , which is demonstrated to sometimes yield performance gains when the they are finetuned on downstream tasks .
On the other hand , since we sample frequent questions in the training set as negative examples for the classifier , it also discourages the model from generating overly generic questions .
Previous work has attacked the problem of genericness in conversational natural language generation by proposing auxiliary training objectives , e.g. , ones that maximize the utility of the generated utterance estimated with adversarial networks ( Rao and Daum ?
III , 2019 ) , specificity estimates that are estimated from data ( Ko et al. , 2019 a , b ) , or the mutual information between the generated turn and previous ones ( Li et al. , 2016 ) .
Our proposed method can be viewed as a generalization of these approaches , where the objective to be optimized at the time of generation is implicitly specified via a parameterized model by choosing negative examples for contrast .
Generating Informative and Specific Questions Given the informativeness metric and specificity reward , we can improve upon these by maximizing the following reward function that blends the two in a weighted sum R( q ; C <j ) = ?
1 I( q ; C <j ) + ( 1 ? ? 1 ) S( q ; H <j , T ) . ( 5 ) Since this quantity can only be evaluated once a complete question has been generated , the nondifferentiability of the decoding process prevents us from directly optimizing it with respect to ? using gradient - based optimization .
However , we can still estimate the gradient of the expected reward of generated questions , E q?P ? [ R ( q ) ] using REIN -FORCE ( Williams , 1992 ) , a reinforcement learning technique .
For an example q , the gradient estimate is the gradient of the following loss function R ( q ) = ? R ( q ) ? b( q ) log P ? ( q ) ( 6 ) where q is a sample from P ? and we dropped the dependency on C <j for notational clarity . b( q ) is called the baseline function , which , if chosen carefully , reduces the variance of this gradient estimate and results in faster convergence .
We apply a technique called self-critical sequence training ( Rennie et al. , 2017 ) , which selects b( q ) = R( q G ) , the reward obtained by the greedily decoded sequence , q G , from the question generator .
To ensure that the generator maximizes the desired reward function without losing fluency in generated questions , we combine R with negative log likelihood during model finetuning ( Paulus et al. , 2018 ) .
We finetune a pretrained question generator ( with NLL ) using the following objective = ?
2 R + ( 1 ? ? 2 ) NLL . ( 7 ) Here , R = 1 Np N i=1 M i j=1 R ( q ( i ) j ) .
We choose ?
1 = 0.5 and ?
2 = 0.98 in our experiments , which were chosen by tuning the model on the dev set .
Experiments Data .
For our experiments , we use the QuAC dataset presented by Choi et al . ( 2018 ) .
Although other similar datasets share some common characteristics , some crucial differences render them inapplicable for our experiments .
For instance , CoQA ( Reddy et al. , 2019 ) gives both agents access to the context , while Wizard of Wikipedia ( Dinan et al. , 2019 ) into training and development partitions , ensuring that the Wikipedia entities discussed in conversations do not overlap between these partitions .
The goal of the split is to obtain a development set that is roughly as large as the repurposed test set .
The statistics of our data split can be found in Table 1 . Training .
We follow the recipe available in Al- lenNLP to train the QA model on QuAC , and make sure that it obtains performance on par with that reported by Choi et al . ( 2018 ) on the official dev set ( with multiple answer references ) .
2
We use the Adam optimizer ( Kingma and Ba , 2015 ) with default hyperparameters to train and finetune our question generator , and anneal the learning rate by 0.5 whenever dev performance does not improve for more than 3 consecutive epochs ( patience =3 ) .
When training finishes , the specificity classifier achieves approximately 75 % F 1 on the dev set when the true next question , sampled frequent questions and random questions from the same conversation have a balanced ratio of 1:1:1 .
For unanswerable questions in QuAC , we revise Equation ( 3 ) and set informativeness to zero if the predicted answer is CANNOTANSWER , as the answer does not reveal new information about the hidden knowledge K .
Results
Metric-based Evaluation
For the baseline model and our model finetuned for informativeness and specificity , we generate predictions with greedy decoding for simplicity .
We evaluate them on conventionally used metrics such as perplexity ( PPLX ) of the reference question and the F 1 score of the ROUGE -L metric ( ROUGE -L ) ( Lin , 2004 ) between the predicted questions and the reference .
The former helps verify the overall quality of our model , while the latter helps us compare single - reference metrics to our proposed ones .
We also report the informativeness metric ( INFO ) 2 However , in practice , we remove the ELMo component , which greatly speeds up computation at the cost of only losing 2 - 3 F1 in answer prediction .
and specificity reward ( SPEC ) for these models , and compare them to the reference questions on these measures on both the dev and test sets .
As shown in Table 2 , the baseline model and our pragmatically finetuned model achieve comparable performance when evaluated against the reference question using n-gram overlap metrics ( ROUGE - L ) , and the perplexity of the reference question is only slightly worse .
As expected , these metrics tell us nothing about how well the model is going to fare in actual communication , because perplexity does not evaluate the usefulness of generated questions , and ROUGE - L can barely tell these systems apart .
We can also see in Table 2 that the finetuned model improves upon the baseline model on both informativeness and specificity .
Further , we notice that despite their high specificity , the reference questions are only about as informative as our baseline questions on average , which is a bit surprising at first sight .
Further analysis reveals that about 12.6 % of dev questions and 15.7 % test ones are considered unanswerable by crowd workers , which is a byproduct of the information - asymmetric setting adopted when the data was collected .
As a result , many reference questions could be considered uninformative by our definition , since they might cause the QA model to abstain from answering .
Human Evaluation
Although the results in Table 2 show that our model sees substantial improvements on the proposed informativeness and specificity metrics , it remains unclear whether these improvements correlate well with human judgement of quality , which is critical in the application of the resulting system .
To study this , we conduct a comparative human evaluation .
We randomly selected 200 turns from the test set , and asked two NLP PhD students to evaluate the reference questions , as well as those generated by the baseline model and our model .
These questions are evaluated on their overall quality , informativeness , and specificity , where the annotators are asked to rank the candidate questions on each metric with ties allowed .
System identity is hidden from the annotators , and the order of the systems is shuffled for each comparison .
Prior to annotation , both annotators were educated to follow the same guidelines to encourage high agreement ( see Appendix D for details ) .
As shown in Table 3 : Human evaluation comparing questions our system generated to those from the baseline , as well as the original reference questions in QuAC .
We perform a bootstrap test with 10 6 samples for the difference between pairs of systems and report the p-values here .
( 93.75 % of our questions are considered equally or more informative ) , and to a lesser extent , overall quality ( 81.5 % ) and specificity ( 82 % ) .
We find that 26.1 % of questions generated by these systems are identical , which inflates the number of ties in human evaluation .
We expect a starker contrast if a sampling - based decoding strategy were applied for generation diversity , which we leave to future work .
We also attirbute this difference in humanperceived quality on these three aspects partly to the inherent nature of these annotation tasks : while our annotators agree on 80.3 % of the pair-wise judgments regarding informativeness , agreement decreases to 70.7 % for overall quality and 69.2 % for specificity since they are more subjective .
It is encouraging , however , that our system is also considered equally or more informative than the human reference 80.25 % of the time .
What negatively affects human's perception of the overall quality of questions our system generates is largely attributable to the over- genericness of these questions compared to the references , and a sometimes blatant lack of common sense ( e.g. , questions like " What did he do after his death ? " ) .
Analysis
We further analyze concrete examples of generated questions in conversations to understand the behavior of our informativeness and specificity metrics .
Case Study .
To sanity check whether our informativeness metric and specificity reward match human intuition , we manually inspect a few examples from the test set .
Figure 3 represents a case where all the questions our system generated are considered equal to or more informative than the reference and baseline generated questions by our metric .
As shown in the example , the baseline system is prone to generating topical but uninformative questions ( BL 2 and BL 3 ) .
Our system finetuned on our reward function is more pragmatic and asks about relevant questions that can likely be answered from the unseen paragraph K .
Our informativeness metric also correctly identifies that both Ours 3 and Ref 3 are good questions that reveal new information about K , although there is very little overlap between the two .
On the other hand , the specificity reward successfully identifies that BL 3 and Ref 4 are the least specific questions of their respective turn , where the former is disconnected from the most recent topic under discussion ( the song ) , the latter is phrased in an overly generic way .
We also demonstrate some clear failure cases .
In Figure 4 , we see that our informativeness and specificity measures make judgements a human will unlikely make , as the topic implies K is unlikely to contain information about Moyet 's first album / recording .
In fact , the QA model fails to recognize that these questions ( BL 1,2 , Ours 1,2,3 , Ref 1 ) are unanswerable , and instead assigns them high informativeness .
The specificity model , on the other hand , fails to recognize near paraphrases ( BL 1 vs Ours 1 ) and a question that was likely just answered ( BL 3 ) .
A positive finding in this example is that the informativeness metric is well - aligned with pragmatic behavior in the fourth turn - had Moyet won the Grammy , the previous answer ( A 3 ) would have mentioned it instead of just her nomination .
We include the answering contexts for these examples in Appendix B for the reader 's reference .
Explainable Informativeness .
As stated in Section 3.3 , our definition of informativeness is explainable to humans - we demonstrate this with concrete examples .
For instance , in the example in Figure 3 , although the question
What happened in 1983 ?
is phrased rather vaguely , the QA model is able to identify its correct answer from the paragraph
The band released their third album , True , in March 1983 , which offers new information ( note the answer in the figure only reflects the actual human-human conversation , not this hypothetical one ) .
Similarly , the QA model correctly identifies that the question our model generated on the second turn ( Ours 2 ) has the same answer as the human reference ( Ref 2 ) , which introduces a new entity into the conversation .
BL 2 and BL 3 are deemed uninformative in this case since the QA model offered the same answer about the album True again .
Although this answer is about an incorrect entity in this context ( the album True instead Parade , which is the focus of discussion ) , the large amount of overlap between this answer and Ans 1 is still sufficient to regard these questions as less informative .
We note that this informativeness metric does have an exploitable flaw - it does not prevent the questioner from asking vague , open-ended questions ( e.g. , What else do you know ? ) to acquire knowledge .
In fact , we find this strategy is also adopted by QuAC 's crowd workers .
However , our specificity reward penalizes genericness , and therefore alleviates this issue in the questions our system generates .
We show that our system repeats n-grams from previous questions less frequently , and refer the reader to Appendix C for details .
Conclusion
In this paper , we presented a question generation system in information -seeking conversations .
By optimizing our proposed automatic metrics for informativeness and specificity , the model is able to generate pragmatically relevant and specific questions to acquire new information about an unseen source of textual knowledge .
Our proposed method presents a practical if shallow implementation of pragmatics in an open-domain communication setting beyond simple reference games .
We hope that our work brings the community 's attention to this important problem of natural language communication under information asymmetry .
