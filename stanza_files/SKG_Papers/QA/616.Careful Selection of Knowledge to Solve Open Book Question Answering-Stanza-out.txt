title
Careful Selection of Knowledge to solve Open Book Question Answering
abstract
Open book question answering is a type of natural language based QA ( NLQA ) where questions are expected to be answered with respect to a given set of open book facts , and common knowledge about a topic .
Recently a challenge involving such QA , OpenBookQA , has been proposed .
Unlike most other NLQA tasks that focus on linguistic understanding , Open-BookQA requires deeper reasoning involving linguistic understanding as well as reasoning with common knowledge .
In this paper we address QA with respect to the OpenBookQA dataset and combine state of the art language models with abductive information retrieval ( IR ) , information gain based re-ranking , passage selection and weighted scoring to achieve 72.0 % accuracy , an 11.6 % improvement over the current state of the art .
Introduction Natural language based question answering ( NLQA ) not only involves linguistic understanding , but often involves reasoning with various kinds of knowledge .
In recent years , many NLQA datasets and challenges have been proposed , for example , SQuAD ( Rajpurkar et al. , 2016 ) , Trivi-aQA ( Joshi et al. , 2017 ) and MultiRC ( Khashabi et al. , 2018 ) , and each of them have their own focus , sometimes by design and other times by virtue of their development methodology .
Many of these datasets and challenges try to mimic human question answering settings .
One such setting is open book question answering where humans are asked to answer questions in a setup where they can refer to books and other materials related to their questions .
In such a setting , the focus is not on memorization but , as mentioned in Mihaylov et al . ( 2018 ) , on " deeper understanding of the materials and its application *
These authors contributed equally to this work .
to new situations ( Jenkins , 1995 ; Landsberger , 1996 ) . "
In Mihaylov et al. ( 2018 ) , they propose the OpenBookQA dataset mimicking this setting .
Question : A tool used to identify the percent chance of a trait being passed down has how many squares ?
( A ) Two squares ( B ) Four squares ( C ) Six squares ( D ) Eight squares Extracted from OpenBook : a punnett square is used to identify the percent chance of a trait being passed down from a parent to its offspring .
Retrieved Missing Knowledge :
Two squares is four .
The Punnett square is made up of 4 squares and 2 of them are blue and 2 of them are brown , this means you have a 50 % chance of having blue or brown eyes .
The OpenBookQA dataset has a collection of questions and four answer choices for each question .
The dataset comes with 1326 facts representing an open book .
It is expected that answering each question requires at least one of these facts .
In addition it requires common knowledge .
To obtain relevant common knowledge we use an IR system ( Clark et al. , 2016 ) front end to a set of knowledge rich sentences .
Compared to reading comprehension based QA ( RCQA ) setup where the answers to a question is usually found in the given small paragraph , in the OpenBookQA setup the open book part is much larger ( than a small paragraph ) and is not complete as additional common knowledge may be required .
This leads to multiple challenges .
First , finding the relevant facts in an open book ( which is much bigger than the small paragraphs in the RCQA setting ) is a challenge .
Then , finding the relevant common knowledge using the IR front end is an even bigger challenge , especially since standard IR approaches can be misled by distractions .
For example , Table 1 shows a sample question from the Open-BookQA dataset .
We can see the retrieved missing knowledge contains words which overlap with both answer options A and B. Introduction of such knowledge sentences increases confusion for the question answering model .
Finally , reasoning involving both facts from open book , and common knowledge leads to multi-hop reasoning with respect to natural language text , which is also a challenge .
We address the first two challenges and make the following contributions in this paper : ( a) We improve on knowledge extraction from the Open -Book present in the dataset .
We use semantic textual similarity models that are trained with different datasets for this task ; ( b) We propose natural language abduction to generate queries for retrieving missing knowledge ; ( c ) We show how to use Information Gain based Re-ranking to reduce distractions and remove redundant information ; ( d ) We provide an analysis of the dataset and the limitations of BERT Large model for such a question answering task .
The current best model on the leaderboard of OpenBookQA is the BERT Large model ( Devlin et al. , 2018 ) .
It has an accuracy of 60.4 % and does not use external knowledge .
Our knowledge selection and retrieval techniques achieves an accuracy of 72 % , with a margin of 11.6 % on the current state of the art .
We study how the accuracy of the BERT Large model varies with varying number of knowledge facts extracted from the OpenBook and through IR .
Related Work
In recent years , several datasets have been proposed for natural language question answering ( Rajpurkar et al. , 2016 ; Joshi et al. , 2017 ; Khashabi et al. , 2018 ; Richardson et al. , 2013 ; Lai et al. , 2017 ; Reddy et al. , 2018 ; Tafjord et al. , 2018 ; Mitra et al. , 2019 ) and many attempts have been made to solve these challenges ( Devlin et al. , 2018 ; Vaswani et al. , 2017 ; Seo et al. , 2016 ) .
Among these , the closest to our work is the work in ( Devlin et al. , 2018 ) which perform QA using fine tuned language model and the works of ( Sun et al. , 2018 ; Zhang et al. , 2018 ) which performs QA using external knowledge .
Related to our work for extracting missing knowledge are the works of ( Ni et al. , 2018 ; Musa et al. , 2018 ; Khashabi et al. , 2017 ) which respectively generate a query either by extracting key terms from a question and an answer option or by classifying key terms or by Seq2Seq models to generate key terms .
In comparison , we generate queries using the question , an answer option and an extracted fact using natural language abduction .
The task of natural language abduction for natural language understanding has been studied for a long time ( Norvig , 1983 ( Norvig , , 1987 Hobbs , 2004 ; Hobbs et al. , 1993 ; Wilensky , 1983 ; Wilensky et al. , 2000 ; Goldman , 1988 , 1989 ) .
However , such works transform the natural language text to a logical form and then use formal reasoning to perform the abduction .
On the contrary , our system performs abduction over natural language text without translating the texts to a logical form .
Approach Our approach involves six main modules : Hypothesis Generation , OpenBook Knowledge Extraction , Abductive Information Retrieval , Information Gain based Re-ranking , Passage Selection and Question Answering .
A key aspect of our approach is to accurately hunt the needed knowledge facts from the OpenBook knowledge corpus and hunt missing common knowledge using IR .
We explain our approach in the example given in Table 2 .
Question : A red-tailed hawk is searching for prey .
It is most likely to swoop down on what ?
( A ) a gecko Generated Hypothesis : H : A red-tailed hawk is searching for prey .
It is most likely to swoop down on a gecko .
Retrieved Fact from OpenBook : F : hawks eat lizards Abduced Query to find missing knowledge : K : gecko is lizard Retrieved Missing Knowledge using IR : K : Every gecko is a lizard .
In Hypothesis Generation , our system generates a hypothesis H ij for the ith question and jth answer option , where j ? { 1 , 2 , 3 , 4 }. In Open-Book Knowledge Extraction , our system retrieves appropriate knowledge F ij for a given hypothesis H ij using semantic textual similarity , from the OpenBook knowledge corpus F. In Abductive Information Retrieval , our system abduces missing knowledge from H ij and F ij .
The system formulates queries to perform IR to retrieve missing knowledge K ij .
With the retrieved K ij , F ij , Information Gain based Re-ranking and Passage Selection our system creates a knowledge passage P ij .
In Question Answering , our system uses P ij to answer the questions using a BERT Large based MCQ model , similar to its use in solving SWAG ( Zellers et al. , 2018 ) .
Hypothesis Generation
Our system creates a hypothesis for each of the questions and candidate answer options as part of the data preparation phase as shown in the example in Table 2 .
The questions in the OpenBookQA dataset are either with wh word or are incomplete statements .
To create hypothesis statements for questions with wh words , we use the rule- based model of Demszky et al . ( 2018 ) .
For the rest of the questions , we concatenate the questions with each of the answers to produce the four hypotheses .
This has been done for all the training , test and validation sets .
OpenBook Knowledge Extraction
To retrieve a small set of relevant knowledge facts from the knowledge corpus F , a textual similarity model is trained in a supervised fashion on two different datasets and the results are compared .
We use the large- cased BERT ( Devlin et al. , 2018 ) ( BERT Large ) as the textual similarity model .
BERT Model Trained on STS -B
We train it on the semantic textual similarity ( STS - B ) data from the GLUE dataset .
The trained model is then used to retrieve the top ten knowledge facts from corpus F based on the STS - B scores .
The STS -B scores range from 0 to 5.0 , with 0 being least similar .
BERT Model Trained on OpenBookQA
We generate the dataset using the gold Open-BookQA facts from F for the train and validation set provided .
To prepare the train set , we first find the similarity of the OpenBook F facts with respect to each other using the BERT model trained on STS - B dataset .
We assign a score 5.0 for the gold Fi fact for a hypothesis .
We then sample different facts from the OpenBook and assign the STS -B similarity scores between the sampled fact and the gold fact Fi as the target score for that fact F ij and H ij .
For example : Hypothesis : Frilled sharks and angler fish live far beneath the surface of the ocean , which is why they are known as Deep sea animals .
Gold Fact : deep sea animals live deep in the ocean : Score : 5.0 Sampled Facts : coral lives in the ocean : Score : 3.4 a fish lives in water : Score : 2.8
We do this to ensure a balanced target score is present for each hypothesis and fact .
We use this trained model to retrieve top ten relevant facts for each H ij from the knowledge corpus F .
Natural Language Abduction and IR
To search for the missing knowledge , we need to know what we are missing .
We use " abduction " to figure that out .
Abduction is a long studied task in AI , where normally , both the observation ( hypothesis ) and the domain knowledge ( known fact ) is represented in a formal language from which a logical solver abduces possible explanations ( missing knowledge ) .
However , in our case , both the observation and the domain knowledge are given as natural language sentences from which we want to find out a possible missing knowledge , which we will then hunt using IR .
For example , one of the hypothesis H ij is " A redtailed hawk is searching for prey .
It is most likely to swoop down on a gecko . " , and for which the known fact F ij is " hawks eats lizards " .
From this we expect the output of the natural language abduction system to be K ij or " gecko is a lizard " .
We will refer to this as " natural language abduction " .
For natural language abduction , we propose three models , compare them against a baseline model and evaluate each on a downstream question answering task .
All the models ignore stop words except the Seq2Seq model .
We describe the three models and a baseline model in the subsequent subsections .
Word Symmetric Difference Model
We design a simple heuristic based model defined as below : K ij = ( H ij ?F ij ) \( H ij ?F ij ) ?j ? { 1 , 2 , 3 , 4 } where i is the ith question , j is the jth option , H ij , F ij , K ij represents set of unique words of each instance of hypothesis , facts retrieved from knowledge corpus F and abduced missing knowledge of validation and test data respectively .
Supervised Bag of Words Model
In the Supervised Bag of Words model , we select words which satisfy the following condition : P ( w n ? K ij ) > ? where w n ?
{ H ij ?
F ij }.
To elaborate , we learn the probability of a given word w n from the set of words in H ij ?
F ij belonging to the abduced missing knowledge K ij .
We select those words which are above the threshold ?.
To learn this probability , we create a training and validation dataset where the words similar ( cosine similarity using spaCy ) ( Honnibal and Montani , 2017 ) to the words in the gold missing knowledge Ki ( provided in the dataset ) are labelled as positive class and all the other words not present in Ki but in H ij ?
F ij are labelled as negative class .
Both classes are ensured to be balanced .
Finally , we train a binary classifier using BERT Large with one additional feed forward network for classification .
We define value for the threshold ? using the accuracy of the classifier on validation set .
0.4 was selected as the threshold .
Copynet Seq2Seq Model
In the final approach , we used the copynet sequence to sequence model ( Gu et al. , 2016 ) to generate , instead of predict , the missing knowledge given , the hypothesis H and knowledge fact from the corpus F .
The intuition behind using copynet model is to make use of the copy mechanism to generate essential yet precise ( minimizing distractors ) information which can help in answering the question .
We generate the training and validation dataset using the gold Ki as the target sentence , but we replace out - of- vocabulary words from the target with words similar ( cosine similarity using spaCy ) ( Honnibal and Montani , 2017 ) to the words present in H ij ?
F ij .
Here , however , we did not remove the stopwords .
We choose one , out of multiple generated knowledge based on our model which provided maximum overlap score , given by overlap score = i count ( ( ? i ? F i ) ? K i ) i count ( Ki ) where i is the ith question , ? i being the set of unique words of correct hypothesis , F i being the set of unique words from retrieved facts from knowledge corpus F , K i being the set of unique words of predicted missing knowledge and Ki being the set of unique words of the gold missing knowledge .
Word Union Model
To see if abduction helps , we compare the above models with a Word Union Model .
To extract the candidate words for missing knowledge , we used the set of unique words from both the hypothesis and OpenBook knowledge as candidate keywords .
The model can be formally represented with the following : K ij = ( H ij ?
F ij ) ?j ? { 1 , 2 , 3 , 4 }
Information Gain based Re-ranking
In our experiments we observe that , BERT QA model gives a higher score if similar sentences are repeated , leading to wrong classification .
Thus , we introduce Information Gain based Re-ranking to remove redundant information .
We use the same BERT Knowledge Extraction model Trained on OpenBookQA data ( section 3.2.2 ) , which is used for extraction of knowledge facts from corpus F to do an initial ranking of the retrieved missing knowledge K .
The scores of this knowledge extraction model is used as relevancy score , rel .
To extract the top ten missing knowledge K , we define a redundancy score , red ij , as the maximum cosine similarity , sim , between the previously selected missing knowledge , in the previous iterations till i , and the candidate missing knowledge K j .
If the last selected missing knowledge is K i , then red ij ( K j ) = max( red i?1 , j ( K j ) , sim ( K i , K j ) ) rank score = ( 1 ? red i , j ( K j ) ) * rel( K j )
For missing knowledge selection , we first take the missing knowledge with the highest rel score .
From the subsequent iteration , we compute the redundancy score with the last selected missing knowledge for each of the candidates and then rank them using the updated rank score .
We select the top ten missing knowledge for each H ij .
Question Answering Once the OpenBook knowledge facts F and missing knowledge K have been extracted , we move onto the task of answering the questions .
Question -Answering Model
We use BERT
Large model for the question answering task .
For each question , we create a passage using the extracted facts and missing knowledge and fine- tune the BERT Large model for the QA task with one additional feed -forward layer for classification .
The passages for the train dataset were prepared using the knowledge corpus facts , F . We create a passage using the top N facts , similar to the actual gold fact Fi , for the train set .
The similarities were scored using the STS -B trained model ( section 3.2.1 ) .
The passages for the training dataset do not use the gold missing knowledge Ki provided in the dataset .
For each of our experiments , we use the same trained model , with passages from different IR models .
The BERT Large model limits passage length to be lesser than equal to 512 .
This restricts the size of the passage .
To be within the restrictions we create a passage for each of the answer options , and score for all answer options against each passage .
We refer to this scoring as sum score , defined as follows :
For each answer options , A j , we create a passage P j and score against each of the answer options A i .
To compute the final score for the answer , we sum up each individual scores .
If Q is the question , the score for the answer is defined as P r( Q , A i ) = 4 j=1 score ( P j , Q , A i ) where score is the classification score given by the BERT Large model .
The final answer is chosen based on , A = arg max A P r( Q , A i )
Passage Selection and Weighted Scoring
In the first round , we score each of the answer options using a passage created from the selected knowledge facts from corpus F.
For each question , we ignore the passages of the answer options which are in the bottom two .
We refer to this as Passage Selection .
In the second round , we score for only those passages which are selected after adding the missing knowledge K .
We assume that the correct answer has the highest score in each round .
Therefore we multiply the scores obtained after both rounds .
We refer to this as Weighted Scoring .
We define the combined passage selected scores and weighted scores as follows : P r( F , Q , A i ) = 4 j=1 score ( P j , Q , A i ) where P j is the passage created from extracted OpenBook knowledge , F .
The top two passages were selected based on the scores of P r( F , Q , A i ) .
P r( F ? K , Q , A i ) = 4 k=1 ? * score ( P k , Q , A i ) where ? = 1 for the top two scores and ? = 0 for the rest .
P k is the passage created using both the facts and missing knowledge .
The final weighted score is : wP r( Q , A i ) = P r( F , Q , A i ) * P r( F ? K , Q , A i )
The answer is chosen based on the top weighted scores as below : A = arg max A wP r( Q , A i )
Experiments
Dataset and Experimental Setup
The dataset of OpenBookQA contains 4957 questions in the train set and 500 multiple choice questions in validation and test respectively .
We train a BERT Large based QA model using the top ten knowledge facts from the corpus F , as a passage for both training and validation set .
We select the model which gives the best score for the validation set .
The same model is used to score the validation and test set with different passages derived from different methods of Abductive IR .
The best Abductive IR model , the number of facts from F and K are selected from the best validation scores for the QA task .
F
OpenBook Knowledge Extraction Question : .. they decide the best way to save money is ? ( A ) to quit eating lunch out ( B ) to make more phone calls ( C ) to buy less with monopoly money ( D ) to have lunch with friends Knowledge extraction trained with STS -B : using less resources usually causes money to be saved a disperser disperses each season occurs once per year Knowledge extraction trained with Open-BookQA : using less resources usually causes money to be saved decreasing something negative has a positive impact on a thing conserving resources has a positive impact on the environment Table 3 shows a comparative study of our three approaches for OpenBook knowledge extraction .
We show , the number of correct OpenBook knowledge extracted for all of the four answer options using the three approaches TF - IDF , BERT model trained on STS - B data and BERT model Trained on OpenBook data .
Apart from that , we also show the count of the number of facts present precisely across the correct answer options .
It can be seen that the Precision@N for the BERT model trained on OpenBook data is better than the other models as N increases .
The above example presents the facts retrieved from BERT model trained on OpenBook which are more relevant than the facts retrieved from BERT model trained on STS -B .
Both the models were able to find the most relevant fact , but the other facts for STS - B model introduce more distractors and have lesser relevance .
The impact of this is visible from the accuracy scores for the QA task in Table 3 .
The best performance of the BERT QA model can be seen to be 66.2 % using only OpenBook facts .
Abductive Information Retrieval
We evaluate the abductive IR techniques at different values for number of facts from F and number of missing knowledge K extracted using IR .
Figure 2 shows the accuracy against different combinations of F and K , for all four techniques of IR prior to Information gain based Re-ranking .
In general , we noticed that the trained models performed poorly compared to the baselines .
The Word Symmetric Difference model performs better , indicating abductive IR helps .
The poor performance of the trained models can be attributed to the challenge of learning abductive inference .
For the above example it can be seen , the pre-reranking facts are relevant to the question but contribute very less considering the knowledge facts retrieved from the corpus F and the correct answer .
Figure 3
Question Answering Table 4 shows the incremental improvement on the baselines after inclusion of carefully selected knowledge .
Passage Selection and Weighted Scoring are used to overcome the challenge of boosted prediction scores due to cascading effect of errors in each stage .
Supervised learned models
The supervised learned models for abduction under-perform .
The Bag of Words and the Seq2Seq models fail to extract keywords for many F ?
H pairs , sometimes missing the keywords from the answers .
The Seq2Seq model sometimes extracts the exact missing knowledge , for example it generates " some birds is robin " or " lizard is gecko " .
This shows there is promise in this approach and the poor performance can be attributed to insufficient train data size , which was 4957 only .
A fact verification model might improve the accuracy of the supervised learned models .
But , for many questions , it fails to extract proper keywords , copying just a part of the question or the knowledge fact .
Error Analysis
Other than errors due to distractions and failed IR , which were around 85 % of the total errors , the errors seen are of four broad categories .
Temporal Reasoning :
In the example 2 shown below , even though both the options can be considered as night , the fact that 2:00 AM is more suitable for the bats than 6:00 PM makes it difficult to reason .
Such issues accounted for 5 % of the errors .
Question : Owls are likely to hunt at ? ( A ) 3:00 PM ( B ) 2:00 AM ( C ) 6:00 PM ( D ) 7:00 AM
Negation :
In the example shown below , a model is needed which handles negations specifically to reject incorrect options .
Such issues accounted for 1 % of the errors .
Question :
Which of the following is not an input in photosynthesis ?
( A ) sunlight ( B ) oxygen ( C ) water ( D ) carbon dioxide Conjunctive Reasoning :
In the example as shown below , each answer options are partially correct as the word " bear " is present .
Thus a model has to learn whether all parts of the answer are true or not , i.e Conjunctive Reasoning .
Logically , all answers are correct , as we can see 2 Predictions are in italics , Correct answers are in Bold. an " or " , but option ( A ) makes more sense .
Such issues accounted for 1 % of the errors .
Question :
Some berries may be eaten by ( A ) a bear or person ( B ) a bear or shark ( C ) a bear or lion ( D ) a bear or wolf Qualitative Reasoning :
In the example shown below , each answer options would stop a car but option ( D ) is more suitable since it will stop the car quicker .
A deeper qualitative reasoning is needed to reject incorrect options .
Such issues accounted for 8 % of the errors .
Question :
Which of these would stop a car quicker ?
( A ) a wheel with wet brake pads ( B ) a wheel without brake pads ( C ) a wheel with worn brake pads ( D ) a wheel with dry brake pads
Conclusion
In this work , we have pushed the current state of the art for the OpenBookQA task using simple techniques and careful selection of knowledge .
We have provided two new ways of performing knowledge extraction over a knowledge base for QA and evaluated three ways to perform abductive inference over natural language .
All techniques are shown to improve on the performance of the final task of QA , but there is still a long way to reach human performance .
We analyzed the performance of various components of our QA system .
For the natural language abduction , the heuristic technique performs better than the supervised techniques .
Our analysis also shows the limitations of BERT based MCQ models , the challenge of learning natural language abductive inference and the multiple types of reasoning required for an OpenBookQA task .
Nevertheless , our overall system improves on the state of the art by 11.6 % .
Figure Figure 1 : Our approach
