title
Learning the Relative Usefulness of Questions in Community QA
abstract
We present a machine learning approach for the task of ranking previously answered questions in a question repository with respect to their relevance to a new , unanswered reference question .
The ranking model is trained on a collection of question groups manually annotated with a partial order relation reflecting the relative utility of questions inside each group .
Based on a set of meaning and structure aware features , the new ranking model is able to substantially outperform more straightforward , unsupervised similarity measures .
Introduction Open domain Question Answering ( QA ) is one of the most complex and challenging tasks in natural language processing .
In general , a question answering system may need to integrate knowledge coming from a wide variety of linguistic processing tasks such as syntactic parsing , semantic role labeling , named entity recognition , and anaphora resolution ( Prager , 2006 ) .
State of the art implementations of these linguistic analysis tasks are still limited in their performance , with errors that compound and propagate into the final performance of the QA system ( Moldovan et al. , 2002 ) .
Consequently , the performance of open domain QA systems has yet to arrive at a level at which it would become a feasible alternative to the current paradigms for information access based on keyword searches .
Recently , community - driven QA sites such as Yahoo !
Answers and WikiAnswers 1 have established 1 answers.yahoo.com , wiki.answers.com a new approach to question answering that shifts the inherent complexity of open domain QA from the computer system to volunteer contributors .
The computer is no longer required to perform a deep linguistic analysis of questions and generate corresponding answers , and instead acts as a mediator between users submitting questions and volunteers providing the answers .
An important objective in community QA is to minimize the time elapsed between the submission of questions by users and the subsequent posting of answers by volunteer contributors .
One useful strategy for minimizing the response latency is to search the QA repository for similar questions that have already been answered , and provide the corresponding ranked list of answers , if such a question is found .
The success of this approach depends on the definition and implementation of the question - to -question similarity function .
In the simplest solution , the system searches for previously answered questions based on exact string matching with the reference question .
Alternatively , sites such as WikiAnswers allow the users to mark questions they think are rephrasings ( " alternate wordings " , or paraphrases ) of existing questions .
These question clusters are then taken into account when performing exact string matching , therefore increasing the likelihood of finding previously answered questions that are semantically equivalent to the reference question .
In order to lessen the amount of work required from the contributors , an alternative approach is to build a system that automatically finds rephrasings of questions , especially since question rephrasing seems to be computationally less demanding than question answering .
According to previous work in this domain , a question is considered a rephrasing of a reference question Q 0 if it uses an alternate wording to express an identical information need .
For example , Q 0 and Q 1 below are rephrasings of each other , and consequently they are expected to have the same answer .
Q 0 What should I feed my turtle ?
Q 1 What do I feed my pet turtle ?
Paraphrasings of a new question cannot always be found in the community QA repository .
We believe that computing a ranked list of existing questions that at least partially address the original information need could also be useful to the user , at least until other users volunteer to give an exact answer to the original , unanswered reference question .
For example , in the absence of any additional information about the reference question Q 0 , the expected answers to questions Q 2 and Q 3 below may be seen as partially overlapping in information content with the expected answer for the reference question Q 0 .
An answer to question Q 4 , on the other hand , is less likely to benefit the user , even though it has a significant lexical overlap with the reference question .
Q 2 What kind of fish should I feed my turtle ?
Q 3 What do you feed a turtle that is the size of a quarter ?
Q 4 What kind of food should I feed a turtle dove ?
In this paper , we propose a supervised learning approach to the question ranking problem , a generalization of the question paraphrasing problem in which questions are ranked in a partial order based on the relative information overlap between their expected answers and the expected answer of the reference question .
Underlying the question ranking task is the expectation that the user who submits a reference question will find the answers of the highly ranked questions to be more useful than the answers associated with the lower ranked questions .
For the reference question Q 0 above , the learned ranking model is expected to produce a partial order in which Q 1 is ranked higher than Q 2 , Q 3 and Q 4 , whereas Q 2 and Q 3 are ranked higher than Q 4 .
Partially Ordered Datasets for Question Ranking
In order to enable the evaluation of question ranking approaches , we have previously created a dataset of 60 groups of questions ( Bunescu and Huang , 2010 b ) .
Each group consists of a reference question ( e.g. Q 0 above ) that is associated with a partially ordered set of questions ( e.g. Q 1 to Q 4 above ) .
For each reference questions , its corresponding partially ordered set is created from questions in Yahoo !
Answers and other online repositories that have a high cosine similarity with the reference question .
Out of the 26 top categories in Yahoo !
Answers , the 60 reference questions span a diverse set of categories .
Figure 1 lists the 20 categories covered , where each category is shown with the number of corresponding reference questions between parentheses .
Inside each group , the questions are manually annotated with a partial order relation , according to their utility with respect to the reference question .
We use the notation Q i ?
Q j | Q r to encode the fact that question Q i is more useful than question Q j with respect to the reference question Q r .
Similarly , Q i = Q j will be used to express the fact that questions Q i and Q j are reformulations of each other ( the reformulation relation is independent of the reference question ) .
The partial ordering among the questions Q 0 to Q 4 above can therefore be expressed concisely as follows : Q 0 = Q 1 , Q 1 ? Q 2 |Q 0 , Q 1 ? Q 3 |Q 0 , Q 2 ? Q 4 |Q 0 , Q 3 ? Q 4 |Q 0 . Note that we do not explicitly annotate the relation Q 1 ? Q 4 | Q 0 , since it can be inferred based on the transitivity of the more useful than relation :
Also note that no relation is specified between Q 2 and Q 3 , and similarly no relation can be inferred between these two questions .
This reflects our belief that , in the absence of any additional information regarding the user or the " turtle " referenced in Q 0 , we cannot compare questions Q 2 and Q 3 in terms of their usefulness with respect to Q 0 . Q 1 ? Q 2 |Q 0 ? Q 2 ? Q 4 |Q 0 ? Q 1 ? Q 4 |Q 0 . REFERENCE QUESTION ( Q r ) Q 5 What Table 1 shows another reference question Q 5 from our dataset , together with its annotated group of questions Q 6 to Q 20 .
In order to make the annotation process easier and reproducible , we have divided it into two levels of annotation .
During the first annotation stage , each question group is partitioned manually into 3 subgroups of questions : ?
P is the set of paraphrasing questions . ?
U is the set of useful questions . ?
N is the set of neutral questions .
A question is deemed useful if its expected answer may overlap in information content with the expected answer of the reference question .
The expected answer of a neutral question , on the other hand , should be irrelevant with respect to the reference question .
Let Q r be the reference question , Q p ?
P a paraphrasing question , Q u ?
U a useful question , and Q n ?
N a neutral question .
Then the following relations are assumed to hold among these questions :
1 . Q p ?
Q u | Q r : a paraphrasing question is more useful than a useful question .
2 . Q u ?
Q n | Q r : a useful question is more use - ful than a neutral question .
Note that as long as these relations hold between the 3 types of questions , the names of the subgroups and their definitions are irrelevant with respect to the implied set of more useful than relations , since only the implied ternary relations will be used for training and evaluating question ranking approaches .
We also assume that , by transitivity , the following ternary relations also hold : Q p ?
Q n | Q r , i.e. a paraphrasing question is more useful than a neutral question .
Furthermore , if Q p 1 , Q p 2 ?
P are two paraphrasing questions , this implies Q p 1 = Q p 2 | Q r .
For the vast majority of questions , the first annotation stage is straightforward and non-controversial .
In the second annotation stage , we perform a finer annotation of relations between questions in the middle group U. Table 1 shows two such relations ( using indentation ) : Q 8 ? Q 9 |Q 5 and Q 8 ? Q 10 | Q 5 . Question Q 8 would have been a rephrasing of the reference question , were it not for the noun " art " modifying the focus noun phrase " summer camp " .
Therefore , the information content of the answer to Q 8 is strictly subsumed in the information content associated with the answer to Q 5 .
Similarly , in Q 9 the focus noun phrase is further specialized through the prepositional phrase " for girls " .
Therefore , ( an answer to ) Q 9 is less useful to Q 5 than ( an answer to ) Q 8 , i.e. Q 8 ? Q 9 |Q 5 . Furthermore , the focus " art summer camp " in Q 8 conceptually subsumes the focus " summer camps for singing " in Q 10 , therefore Q 8 ? Q 10 | Q 5 .
We call this dataset simple since most of the reference questions are shorter than the other questions in their group .
We have also created a complex version of the same dataset , by selecting as the reference question in each group a longer question from the same group .
For example , if Q 0 were a reference question , it would be replaced with a more complex question , such as Q 2 , or Q 3 .
The annotation is redone to reflect the relative usefulness relations with respect to the new reference questions .
We believe that the new complex dataset is closer to the actual distribution of questions in community QA repositories : unanswered questions tend to be more specific ( longer ) , whereas general questions ( shorter ) are more likely to have been answered already .
Each dataset is annotated by two annotators , leading to a total of 4 datasets : Simple 1 , Simple 2 , Complex 1 , and Complex 2 . Table 2 presents the following statistics on the two types of datasets ( Simple , Complex ) for each annotator ( 1 , 2 ) : the total number of paraphrasings ( P ) , the total number of useful questions ( U ) , the total number of neutral questions ( N ) , the total number of more useful than ordered pairs encoded in the dataset , either explicitly or through transitivity , and the Inter-Annotator Agreement ( ITA ) .
We compute the ITA as the precision ( P ) and recall ( R ) with respect to the more useful than ordered pairs encoded in one annotation ( P airs
The statistics in Table 2 indicate that the second annotator was in general more conservative in tagging questions as paraphrases or useful questions .
Unsupervised Methods for Question Ranking
An ideal question ranking method would take an arbitrary triplet of questions Q r , Q i and Q j as input , and output an ordering between Q i and Q j with respect to the reference question Q r , i.e. one of Q i ?
Q j | Q r , Q i = Q j | Q r , or Q j ?
Q i | Q r .
One ap- proach is to design a usefulness function u( Q i , Q r ) that measures how useful question Q i is for the reference question Q r , and define the more useful than ( ? ) relation as follows : Q i ? Q j | Q r ? u( Q i , Q r ) > u( Q j , Q r ) If we define I ( Q ) to be the information need associated with question Q , then u( Q i , Q r ) could be defined as a measure of the relative overlap between I ( Q i ) and I ( Q r ) .
Unfortunately , the information need is a concept that , in general , is defined only intensionally and therefore it is difficult to measure .
For lack of an operational definition of the information need , we will approximate u( Q i , Q r ) directly as a measure of the similarity between Q i and Q r .
The similarity between two questions can be seen as a special case of text - to - text similarity , consequently one possibility is to use a general text - to - text similarity function such as cosine similarity in the vector space model ( Baeza - Yates and Ribeiro- Neto , 1999 ) : cos( Q i , Q r ) = Q T i Q r Q i Q r Here , Q i and Q r denote the corresponding tf ?idf vectors .
As a measure of question similarity , one major drawback of cosine similarity is that it is oblivious of the meanings of words in each question .
This particular problem is illustrated by the three questions below .
Q 22 and Q 23 have the same cosine similarity with Q 21 , they are therefore indistinguishable in terms of their usefulness to the reference question Q 21 , even though we expect Q 22 to be more useful than Q 23 ( a place that sells hydrangea often sells other types of plants too , possibly including cacti ) .
Q 21
Where can I buy a hydrangea ?
Q 22
Where can I buy a cactus ?
Q 23
Where can I buy an iPad ?
To alleviate the lexical chasm , we can redefine u( Q i , Q r ) to be the similarity measure proposed by ( Mihalcea et al. , 2006 ) as follows : mcs ( Q i , Q r ) = w? { Qi} maxSim(w , Q r ) * idf ( w ) w ? { Qi} idf ( w ) + w ? { Qr} maxSim(w , Q i ) * idf ( w ) w ? { Qr} idf ( w ) Since scaling factors are immaterial for ranking , we have ignored the normalization constant contained in the original measure .
For each word w ?
Q i , maxSim(w , Q r ) computes the maximum semantic similarity between w and any word w r ?
Q r .
The similarity scores are weighted by the corresponding idf's , and normalized .
A similar score is computed for each word w ?
Q r .
The score computed by maxSim depends on the actual function used to compute the word- to - word semantic similarity .
In this paper , we evaluated four of the knowledgebased measures explored in ( Mihalcea et al. , 2006 ) : wup ( Wu and Palmer , 1994 ) , res ( Resnik , 1995 ) , lin ( Lin , 1998 ) , and jcn ( Jiang and Conrath , 1997 ) .
Supervised Learning for Question Ranking Cosine similarity , henceforth referred as cos , treats questions as bags-of-words .
The meta-measure proposed in ( Mihalcea et al. , 2006 ) , henceforth called mcs , treats questions as bags- of-concepts .
Both cos and mcs ignore the syntactic relations between the words in a question , and therefore may miss important structural information .
In the next three sections we describe a set of structural features that we believe are relevant for judging question similarity .
These and other types of features will be integrated in an SVM model for ranking , as described later in Section 4.4 .
Matching the Focus Words
If we consider the question Q 24 below as reference , question Q 26 will be deemed more useful than Q 25 when using cos or mcs because of the higher relative lexical and conceptual overlap with Q 24 .
However , this is contrary to the actual ordering Q 25 ? Q 26 | Q 24 , which reflects the fact that Q 25 , which expects the same answer type as Q 24 , should be deemed more useful than Q 26 , which has a different answer type .
Q 24
What are some good thriller movies ?
Q 25
What are some thriller movies with happy ending ?
Q 26
What are some good songs from a thriller movie ?
The analysis above shows the importance of using the answer type when computing the similarity between two questions .
However , instead of relying exclusively on a predefined hierarchy of answer types , we identify the question focus of a question , defined as the set of maximal noun phrases in the question that corefer with the expected answer ( Bunescu and Huang , 2010a ) .
Focus nouns such as movies and songs provide more discriminative information than general answer types such as products .
We use answer types only for questions such as Q 27 or Q 28 below that lack an explicit question focus .
In such cases , an artificial question focus is created from the answer type ( e.g. location for Q 27 , or method for Q 28 ) .
Q 27
Where can I buy a good coffee maker ?
Q 28
How do I make a pizza ?
Let f i and f r be the focus words corresponding to questions Q i and Q r .
We introduce a focus feature ?
f , and set its value to be equal with the similarity between the focus words : ? f ( Q i , Q r ) = wsim ( f i , f r ) ( 1 )
We use wsim to denote a generic word meaning similarity measure ( e.g. wup , res , lin or jcn ) .
When computing the focus feature , the non-focus word " movie " in Q 26 will not be compared with the focus word " movies " in Q 24 , and therefore Q 26 will have a lower value for this feature than Q 25 , i.e. ? f ( Q 26 , Q 24 ) < ? f ( Q 25 , Q 24 ) .
Matching the Main Verbs
In addition to the question focus , the main verb of a question can also provide key information in estimating question - to -question similarity .
We define the main verb to be the content verb that is highest in the dependency tree of the question , e.g. buy for Q 27 , or make for Q 28 .
If the question does not contain a content verb , the main verb is defined to be the highest verb in the dependency tree , as for example are in Q 24 to Q 26 .
The utility of a question 's main verb in judging its similarity to other questions can be seen more clearly in the questions below , where Q 29 is the reference : Q 29
How can I transfer music from iTunes to my iPod ?
Q 30
How can I upload music to my iPod ?
Q 31
How can I play music in iTunes ?
The fact that upload , as the main verb of Q 30 , is more semantically related to transfer is essential in deciding that Q 30 ? Q 31 | Q 29 , i.e. Q 30 is more useful than Q 31 to Q 29 .
Let v i and v r be the main verbs corresponding to questions Q i and Q r .
We introduce a main verb feature ?
v as follows : ? v ( Q i , Q r ) = wsim( v i , v r ) ( 2 ) If Q 29 is considered as reference question , it is expected that the main verb feature for question Q 30 will have a higher value than the main verb feature for Q 31 , i.e. ? f ( Q 31 , Q 29 ) < ? f ( Q 30 , Q 29 ) .
Matching the Dependency Trees
The question focus and the main verb are only two of the nodes in the syntactic dependency tree of a question .
In general , all the words in a question are important when judging its semantic similarity with another question .
We therefore propose a more general feature that exploits the dependency structure of the question and , in doing so , it also considers all the words in the question , like cos and mcs .
For any given question we initially ignore the direction of the dependency arcs and change the question dependency tree to be rooted at the focus word , as illustrated in Figure 2 for questions Q 5 and Q 9 . Interrogative patterns such as " What is " or " Are there any " are automatically eliminated from the dependency trees .
We define the dependency tree similarity between two questions Q i and Q r to be a function of similarities wsim ( v i , v r ) computed between aligned nodes v i ?
Q i and v r ? Q r .
The nodes of two dependency trees are aligned through a function MaxMatch ( u i .C , u r .C ) that takes two sets of children nodes as arguments , one from Q i and one from Q r , and finds the maximum weighted bipartite matching between u i .C and u r .C.
Given two children nodes v i ?
u i .C and v r ? u r .C , the weight of a potential matching between v i and v r is defined simply as wsim ( v i , v r ) .
MaxMatch ( u i .C , u r .C ) is furthermore constrained to match only nodes that have compatible part- of-speech tags ( e.g. nouns are matched to nouns , verbs are matched to verbs ) , and children nodes that have the same head -modifier relationship with their parents ( i.e. they are both heads , or they are both dependents of their parents ) .
Table 3 shows the recursive algorithm used TreeMatch ( u i , u r ) [ In ] :
Two dependency tree nodes u i , u r . [ Out ] : A set of node pairs M. 1 . set M ? {( u i , u r ) } 2 . for each ( v i , v r ) ? MaxMatch ( u i .C , u r .C ) : 3 . set M ? M ? TreeMatch ( v i , v r ) 4 . return M for finding a matching between two question dependency trees rooted at the focus words .
The initial arguments of the algorithm are the two focus words u i = f i and u r = f r .
Thus , the pair ( f i , f r ) is the first pair of nodes to be added to the matching M in step 1 .
In the next step , we compute the maximum weighted matching between the children nodes u i .C and u r .C , and recursively call the matching algorithm on pairs of matched nodes ( v i , v r ) from M .
The algorithm stops when MaxMatch returns an empty matching , which may happen when reaching leaf nodes , or when no pair of children nodes has compatible POS tags , or child - parent dependencies .
Figure 2 shows the results of applying the tree matching algorithm on questions Q 5 and Q 9 .
Matched nodes share the same index and are shown in circles , whereas unmatched nodes are shown in italics .
We introduce a new feature ?
t ( Q i , Q r ) whose value is defined as the dependency tree similarity between questions Q i and Q r .
Once the optimum matching M( Q i , Q r ) between dependency trees has been found , ? t ( Q i , Q r ) is computed as the normalized sum of the similarities between pairs of matched nodes v i and v r , as shown in Equations 3 and 4 below .
When computing the similarity between two matched nodes , we factor in the similarities between corresponding pairs of words on the paths f i ; v i , f r ; v r between the focus words f i , f r and the nodes v i , v r , as shown in Equation 5 .
This has the effect of reducing the importance of words that are farther away from the focus word in the dependency tree .
? t ( Q i , Q r ) = sim ( Q i , Q r ) sim ( Q i , Q i ) sim ( Q r , Q r ) ( 3 ) sim ( Q i , Q r ) = ( vi , vr ) ?
M( Qi , Qr ) sim ( f i ; v i , f r ; v r ) ( 4 ) sim ( u
1 ; u n , v 1 ; v n ) = n i=1 wsim ( u i , v i ) ( 5 ) If the word similarity function is normalized and defined to return 1 for identical words , the normalizer in Equation 3 becomes equivalent with | Q i || Q r |.
Thus , words that are left unmatched implicitly decrease the dependency tree similarity .
An SVM Model for Ranking Questions
We consider learning a usefulness function u( Q i , Q r ) of the following general , linear form : u( Q i , Q r ) = w T ?( Q i , Q r ) ( 6 )
The vector ?( Q i , Q r ) is defined to contain the following generic features : 6 . ? l ( Q i , Q r ) = 1 if both questions contain loca - tions , 0 otherwise .
7 . ? d ( Q i , Q r ) = the normalized geographical dis- tance between the locations in Q i and Q r , 0 if ?
l ( Q i , Q r ) = 0 .
Given two location names , we first find their latitude and longitude using Google Maps , and then compute the spherical distance between them using the haversine formula .
The corresponding parameters w will be trained on pairs from one of the partially ordered datasets described in Section 2 .
We use the kernel version of the large-margin ranking approach from ( Joachims , 2002 ) which solves the optimization problem in Figure 3 below .
The aim of this formulation is to find a minimize : weight vector w such that 1 ) the number of ranking constraints u( Q i , Q r ) ?
u( Q j , Q r ) from the training data D that are violated is minimized , and 2 ) the ranking function u( Q i , Q r ) generalizes well beyond the training data .
The learned w is a linear combination of the feature vectors ?( Q i , Q r ) , which makes it possible to use kernels .
J( w , ? ) = 1 2 w 2 + C ? rij subject to : w T ?( Q i , Q r ) ? w T ?( Q j , Q r ) ? 1 ? ? rij ? rij ? 0 ?Q r , Q i , Q j ?
D , Q i ?
Q j | Q r
Experimental Evaluation
We use the four question ranking datasets described in Section 2 to evaluate the three similarity measures cos , mcs , and ?
t , as well as the SVM ranking model .
We report one set of results for each of the four word similarity measures wup , res , lin or jcn .
Each question similarity measure is evaluated in terms of its accuracy on the set of ordered pairs , and the performance is averaged between the two annotators for the Simple and Complex datasets .
If Q i ?
Q j | Q r is a relation specified in the anno- tation , we consider the tuple Q i , Q j , Q r correctly classified if and only if u( Q i , Q r ) > u( Q j , Q r ) , where u is the question similarity measure .
We used the SVM light 2 implementation of ranking SVMs , with a cubic kernel and the standard parameters .
The SVM ranking model was trained and tested using 10 - fold cross-validation , and the overall accuracy was computed by averaging over the 10 folds .
We used the NLTK 3 implementation of the four similarity measures wup , res , lin or jcn .
The idf values for each word were computed from frequency counts over the entire Wikipedia .
For each question , the focus is identified automatically by an SVM tagger trained on a separate corpus of 2,000 questions manually annotated with focus information ( Bunescu and Huang , 2010a ) .
The SVM tagger uses a combination of lexico-syntactic features and a quadratic kernel to achieve a 93.5 % accuracy in a 10 - fold cross validation evaluation on the 2,000 questions .
The head-modifier dependencies were derived automatically from the syntactic parse tree using the head finding rules from ( Collins , 1999 ) .
The syntactic tree is obtained using Spear 4 , a syntactic parser which comes pre-trained on an additional treebank of questions .
The main verb of a question is identified deterministically using a breadth first traversal of the dependency tree .
The overall accuracy results presented in Table 4 show that the SVM ranking model obtains by far the best performance on both datasets , a substantial 10 % higher than cos , which is the best performing unsupervised method .
The random baseline - assigning a random similarity value to each pair of questionsresults in 50 % accuracy .
Even though its use of word senses was expected to lead to superior results , mcs does not perform better than cos on this dataset .
Our implementation of mcs did however perform better than cos on the Microsoft paraphrase corpus ( Dolan et al. , 2004 ) .
One possible reason for this behavior is that mcs seems to be less resilient than cos to differences in question length .
Whereas the Microsoft paraphrase corpus was specifically designed such that " the length of the shorter of the two sentences , in words , is at least 66 % that of the longer " ( Dolan and Brockett , 2005 )
If we take Q 32 as reference question , the fact that the distance between Los Angeles and Anaheim is smaller than the distance between Vista and Anaheim leads the ranking system to rank Q 33 as more useful than Q 34 with respect to Q 32 , which is the expected result .
The preposition " around " from the city context in the first pattern is a good indicator that proximity relations are relevant in this case .
When the same three cities are used for instantiating the other two patterns , it can be seen that the proximity relations are no longer as relevant for judging the relative usefulness of questions .
Future Work
We plan to integrate context dependent word similarity measures into a more robust question utility function .
We also plan to make the dependency tree matching more flexible in order to account for paraphrase patterns that may differ in their syntactic structure .
The questions that are posted on community QA sites often contain spelling or grammatical errors .
Consequently , we will work on interfacing the question ranking system with a separate module aimed at fixing orthographic and grammatical errors .
Related Work
The question rephrasing subtask has spawned a diverse set of approaches .
( Hermjakob et al. , 2002 ) derive a set of phrasal patterns for question reformulation by generalizing surface patterns acquired automatically from a large corpus of web documents .
The focus of the work in ( Tomuro , 2003 ) is on deriving reformulation patterns for the interrogative part of a question .
In ( Jeon et al. , 2005 ) , word translation probabilities are trained on pairs of semantically similar questions that are automatically extracted from an FAQ archive , and then used in a language model that retrieves question reformulations .
( Jijkoun and de Rijke , 2005 ) describe an FAQ question retrieval system in which weighted combinations of similarity functions corresponding to questions , existing answers , FAQ titles and pages are computed using a vector space model .
( Zhao et al. , 2007 ) exploit the Encarta logs to automatically extract clusters containing question paraphrases and further train a perceptron to recognize question paraphrases inside each cluster based on a combination of lexical , syntactic and semantic similarity features .
More recently , ( Bernhard and Gurevych , 2008 ) evaluated various string similarity measures and vector space based similarity measures on the task of retrieving question paraphrases from the WikiAnswers repository .
The aim of the question search task presented in ( Duan et al. , 2008 ) is to return questions that are semantically equivalent or close to the queried question , and is therefore similar to our question ranking task .
Their approach is evaluated on a dataset in which questions are categorized either as relevant or irrelevant .
Our formulation of question ranking is more general , and in particular subsumes the annotation of binary question categories such as relevant vs. irrelevant , or paraphrases vs. non-paraphrases .
Moreover , we are able to exploit the annotated utility relations as supervision in a learning for ranking approach , whereas ( Duan et al. , 2008 ) use the annotated dataset to tune the 3 parameters of a mostly unsupervised approach .
The question ranking task was first formulated in ( Bunescu and Huang , 2010 b ) , where an initial version of the dataset was also described .
In this paper , we introduce 4 versions of the dataset , a more general meaning and structure aware similarity measure , and a supervised model for ranking that substantially outperforms the previously proposed utility measures .
Conclusion
We presented a supervised learning approach to the question ranking task in which previously known questions are ordered based on their relative utility with respect to a new , reference question .
We created four versions of a dataset of 60 groups of questions 5 , each annotated with a partial order relation reflecting the relative utility of questions inside each group .
An SVM ranking model was trained 5
The dataset will be made publicly available .
on the dataset and evaluated together with a set of simpler , unsupervised question - to -question similarity models .
Experimental results demonstrate the importance of using structure and meaning aware features when computing the relative usefulness of questions .
Figure 1 : 1 Figure 1 : The 20 categories represented in the dataset .
