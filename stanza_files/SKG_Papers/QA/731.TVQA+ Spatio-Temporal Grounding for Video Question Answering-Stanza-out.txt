title
TVQA + : Spatio-Temporal Grounding for Video Question Answering
abstract
We present the task of Spatio-Temporal Video Question Answering , which requires intelligent systems to simultaneously retrieve relevant moments and detect referenced visual concepts ( people and objects ) to answer natural language questions about videos .
We first augment the TVQA dataset with 310.8 K bounding boxes , linking depicted objects to visual concepts in questions and answers .
We name this augmented version as TVQA + .
We then propose Spatio-Temporal Answerer with Grounded Evidence ( STAGE ) , a unified framework that grounds evidence in both spatial and temporal domains to answer questions about videos .
Comprehensive experiments and analyses demonstrate the effectiveness of our framework and how the rich annotations in our TVQA + dataset can contribute to the question answering task .
Moreover , by performing this joint task , our model is able to produce insightful and interpretable spatio-temporal attention visualizations .
1
Introduction
We have witnessed great progress in recent years on image - based visual question answering ( QA ) tasks ( Antol et al. , 2015 ; Yu et al. , 2015 ; Zhu et al. , 2016 b ) .
One key to this success has been spatial attention ( Anderson et al. , 2018 ; Shih et al. , 2016 ; Lu et al. , 2016 ) , where neural models learn to attend to relevant regions for predicting the correct answer .
Compared to image- based QA , there has been less progress on the performance of video- based QA tasks .
One possible reason is that attention techniques are hard to generalize to the temporal nature of videos .
Moreover , due to the high cost of annotation , most existing video QA datasets only contain QA pairs , without providing labels for the key clips or regions needed to answer the question .
Inspired by previous work on grounded image and video captioning ( Lu et al. , 2018 ; Zhou et al. , 2019 ) , we propose methods that explicitly localize video clips as well as spatial regions for answering videobased questions .
Such methods are useful in many scenarios , such as natural language guided spatiotemporal localization , and adding explainability to video question answering , which is potentially useful for decision making and model debugging .
To enable this line of research , we also collect new joint spatio-temporal annotations for an existing video QA dataset .
In the past few years , several video QA datasets have been proposed , e.g. , MovieFIB ( Maharaj et al. , 2017 ) , MovieQA ( Tapaswi et al. , 2016 ) , TGIF - QA ( Jang et al. , 2017 ) , PororoQA , MarioQA ( Mun et al. , 2017 ) , and TVQA ( Lei et al. , 2018 ) .
TVQA is one of the largest video QA datasets , providing a large video QA dataset built on top of 6 famous TV series .
Be - cause TVQA was collected on television shows , it is built on natural video content with rich dynamics and complex social interactions , where questionanswer pairs are written by people observing both videos and their accompanying dialogues , encouraging the questions to require both vision and language understanding to answer .
Movie ( Tapaswi et al. , 2016 ; Maharaj et al. , 2017 ) and television show ( Lei et al. , 2018 ) videos come with the limitation of being scripted and edited , but they are still more realistic than cartoon / animation and game ( Mun et al. , 2017 ) videos , and they also come with richer , real-world - inspired inter-human interactions and span across diverse domains ( e.g. , medical , crime , sitcom , etc. ) , making them a useful testbed to study complex video understanding by machine learning models .
One key property of TVQA is that it provides temporal annotations denoting which parts of a video clip are necessary for answering a proposed question .
However , none of the existing video QA datasets ( including TVQA ) provide spatial annotation for the answers .
Actually , grounding spatial regions correctly could be as important as grounding temporal moments for answering a given question .
For example , in Fig. 1 , to answer the question of " What is Sheldon holding when he is talking to Howard about the sword ? " , we need to localize the moment when " he is talking to Howard about the sword ? " , as well as look at the region of " What is Sheldon holding " .
Hence , in this paper , we first augment a subset of the TVQA dataset with grounded bounding boxes , resulting in a spatio-temporally grounded video QA dataset , TVQA + .
It consists of 29.4 K multiplechoice questions grounded in both the temporal and the spatial domains .
To collect spatial groundings , we start by identifying a set of visual concept words , i.e. , objects and people , mentioned in the question or correct answer .
Next , we associate the referenced concepts with object regions in individual frames , if there are any , by annotating bounding boxes for each referred concept ( see examples in Fig. 1 ) .
Our TVQA + dataset has a total of 310.8 K bounding boxes linked with referred objects and people , spanning across 2.5 K categories ( more details in Sec. 3 ) .
With such richly annotated data , we then propose the task of spatio-temporal video question answering , which requires intelligent systems to localize relevant moments , detect referred objects and people , and answer questions .
We further design several metrics to evaluate the performance of the proposed task , including QA accuracy , object grounding precision , temporal localization accuracy , and a joint temporal localization and QA accuracy .
To address spatio-temporal video question answering , we propose a novel end-to - end trainable model , Spatio-Temporal Answerer with Grounded Evidence ( STAGE ) , which effectively combines moment localization , object grounding , and question answering in a unified framework .
We find that the QA performance benefits from both temporal moment and spatial region supervision .
Additionally , we provide visualization of temporal and spatial localization , which is helpful for understanding what our model has learned .
Comprehensive ablation studies demonstrate how each of our annotations and model components helps to improve the performance of the tasks .
To summarize , our contributions are : ?
We collect TVQA + , a large-scale spatiotemporal video question answering dataset , which augments the original TVQA dataset with frame- level bounding box annotations .
To our knowledge , this is the first dataset that combines moment localization , object grounding , and question answering .
?
We design a novel video question answering framework , Spatio-Temporal Answerer with Grounded Evidence ( STAGE ) , to jointly localize moments , ground objects , and answer questions .
By performing all three sub-tasks together , our model achieves significant performance gains over the baselines , as well as presents insightful , interpretable visualizations .
Related Work Question Answering
In recent years , multiple question answering datasets and tasks have been proposed to facilitate research towards this goal , in both vision and language communities , in the form of visual question answering ( Antol et al. , 2015 ;
Yu et al. , 2015 ; Jang et al. , 2017 ) and textual question answering ( Rajpurkar et al. , 2016 ; Weston et al. , 2016 ) , respectively .
Video question answering ( Lei et al. , 2018 ; Tapaswi et al. , 2016 ; with naturally occurring subtitles are particularly interesting , as it combines both visual and textual information for question answering .
Different from ( Tapaswi et al. , 2016 ) Movie QA 6.8K/6.5K - TGIF -QA ( Jang et al. , 2017 ) Tumblr QA 71.7K/165.2K - PororoQA Cartoon QA 16.1K/8.9K - DiDeMo ( Hendricks et al. , 2017 ) Flickr TL 10.5K/40.5 K - Charades-STA ( Gao et al. , 2017 ) Home TL -/19.5 K - TVQA ( Lei et al. , 2018 ) TV Show QA / TL 21.8K/152.5 K
- ANet-Entities ( Zhou et al. , 2019 )
Youtube existing video QA tasks , where a system is only required to predict an answer , we propose a novel task that additionally grounds the answer in both spatial and temporal domains .
Language - Guided Retrieval Grounding language in images / videos is an interesting problem that requires jointly understanding both text and visual modalities .
Earlier works ( Kazemzadeh et al. , 2014 ; Yu et al. , 2017
Yu et al. , , 2018 b Rohrbach et al. , 2016 ) focused on identifying the referred object in an image .
Recently , there has been a growing interest in moment retrieval tasks ( Hendricks et al. , 2017 ( Hendricks et al. , , 2018 Gao et al. , 2017 ) , where the goal is to localize a short clip from a long video via a natural language query .
Our work integrates the goals of both tasks , requiring a system to ground the referred moments and objects simultaneously .
Temporal and Spatial Attention Attention has shown great success on many vision and language tasks , such as image captioning ( Anderson et al. , 2018 ; Xu et al. , 2015 ) , visual question answering ( Anderson et al. , 2018 ; Trott et al. , 2018 ) , language grounding ( Yu et al. , 2018 b ) , etc .
However , sometimes the attention learned by the model itself may not agree with human expectations ( Liu et al. , 2016 ; Das et al. , 2016 ) .
Recent works on grounded image captioning and video captioning ( Lu et al. , 2018 ; Zhou et al. , 2019 ) show better performance can be achieved by explicitly supervising the attention .
Dataset
In this section , we describe the TVQA +
Dataset , the first video question answering dataset with both spatial and temporal annotations .
TVQA + is built on the TVQA dataset introduced by Lei et al ..
TVQA is a large-scale video QA dataset based on 6 popular TV shows , containing 152.5 K multiple choice questions from 21.8 K , 60 - 90 second long video clips .
The questions in the TVQA dataset are compositional , where each question is comprised of two parts , a question part ( " where was Sheldon sitting " ) , joined via a link word , ( " before " , " when " , " after " ) , to a localization part that temporally locates when the question occurs ( " he spilled the milk " ) .
Models should answer questions using both visual information from the video , as well as language information from the naturally associated dialog ( subtitles ) .
Since the video clips on which the questions were collected are usually much longer than the context needed for answering the questions , the TVQA dataset also provides a temporal timestamp annotation indicating the minimum span ( context ) needed to answer each question .
While the TVQA dataset provides a novel question format and temporal annotations , it lacks spatial grounding information , i.e. , bounding boxes of the concepts ( objects and people ) mentioned in the QA pair .
We hypothesize that object annotations could provide an additional useful training signal for models to learn a deeper understanding
Data Collection Identify Visual Concepts
To annotate the visual concepts in video frames , the first step is to identify them in the QA pairs .
We use the Stanford CoreNLP part- of-speech tagger to extract all nouns in the questions and correct answers .
This gives us a total of 152,722 words from a vocabulary of 9,690 words .
We manually label the non-visual nouns ( e.g. , " plan " , " time " , etc. ) in the top 600 nouns , removing 165 frequent non-visual nouns from the vocabulary .
Bounding Box Annotation
For the selected The Big Bang Theory videos from TVQA , we first ask Amazon Mechanical Turk workers to adjust the start and end timestamps to refine the temporal annotation , as we found the original temporal annotation were not ideally tight .
We then sample one frame every two seconds from each span for spatial annotation .
For each frame , we collect the bounding boxes for the visual concepts in each QA pair .
We also experimented with semi-automated annotation for people with face detection and recognition model ( Liu et al. , 2017 ) , but they do not work well mainly due to many partial occlusion of faces ( e.g. , side faces ) in the frames .
During annotation , we provide the original videos ( with subtitles ) to help the workers understand the context for the given QA pair .
More annotation details ( including quality check ) are presented in the appendix .
Dataset Analysis TVQA + contains 29,383 QA pairs from 4,198 videos , with 148,468 images annotated with 310,826 bounding boxes .
Statistics of TVQA + are shown in Table 2 .
Note that we follow the same data splits as the original TVQA dataset , supporting future research on both TVQA and TVQA + .
Table 1 compares TVQA + dataset with other videolanguage datasets .
TVQA + is unique as it supports three tasks : question answering , temporal localization , and spatial localization .
It is also of reasonable size compared to the grounded video captioning dataset ANet- Entities ( Zhou et al. , 2019 ) .
On average , we obtain 2.09 boxes per image and 10.58 boxes per question .
The annotated boxes cover 2,527 categories .
We show the number of boxes ( in log scale ) for each of the top 60 categories in Fig.
2 .
The distribution has a long tail , e.g. , the number of boxes for the most frequent category " sheldon " is around 2 orders of magnitude larger than the 60th category " glasses " .
We also show the distribution of bounding box area over image area ratio in Fig. 3 ( left ) .
The majority of boxes are fairly small compared to the image , which makes object grounding challenging .
Fig. 3 ( right ) shows the distribution of localized span length .
While most spans are T ? No ? d < l a t e x i t s h a 1 _ b a s e 6 4 = " 2 H 8 F 5 E d G 9 q Z N O 8 N 7 b 2 5 g a v R W L K I = " > A A A B / n i c b Z D L S s N A F I Y n X m u 9 R c W V m 8 E i u C p J F X R Z c O N K K v Q G b Q i T y a Q d O s m E m R O h h I K v c N j W M l O U t a g U U n U D o p n g C W s B B 8 G 6 q W I k D g T r B K O b a b 3 z w J T m M m n C O G V e T A Y J j z g l Y C z f P m 7 i P v C Y a X z n y x 8 M f b v i V J 2 Z 8 D K 4 B V R Q o Y Z v f / Z D S b O Y J U A F 0 b r n O i l 4 O V H A q W C T c j / T L C V 0 R A a s Z z A h Z o 2 X z 8 6 f 4 D P j h D i S y r w E 8 M z 9 P Z G T W O t x H J j O m M B Q L 9 a m 5 n + 1 X g b R t Z f z J M 2 A J X S + K M o E B o m n W e C Q K 0 Z B j A 0 Q q r i 5 F d M h U Y S C S a x s Q n A X v 7 w M 7 V r V v a i 6 9 5 e V e q 2 I o 4 R O 0 C k 6 R y 6 6 Q n V 0 i x q o h S j K 0 R N 6 Q a / W o / V s v V n v 8 9 Y V q 5 g 5 Q n 9 k f X w D 5 m e U w A = = < / l a t e x i t > T ? Ls ? d < l a t e x i t s h a 1 _ b a s e 6 4 = " o B J l / c W U f p H e s s m n S 3 Q r e S k a E t k = " > A A A B / n i c b Z D L S s N A F I Y n X m u 9 R c W V m 8 E i u C p J F X R Z c O P C R Y X e o A 1 h M p m 0 Q y e T M H M i l F D w V d y 4 U M S t z + H O t 3 H a R t D W H w Y + / n M O 5 8 w f p I J r c J w v a 2 V 1 b X 1 j s 7 R V 3 t 7 Z 3 d u 3 D w 7 b O s k U Z S 2 a i E R 1 A 6 K Z 4 J K 1 g I N g 3 V Q x E g e C d Y L R z b T e e W B K 8 0 Q 2 Y Z w y L y Y D y S N O C R j L t 4 + b u A 8 8 Z h r f + f o H Q 9 + u O F V n J r w M b g E V V K j h 2 5 / 9 M K F Z z C R Q Q b T u u U 4 K X k 4 U c C r Y p N z P N E s J H Z E B 6 x m U x K z x 8 t n 5 E 3 x m n B B H i T J P A p 6 5 v y d y E m s 9 j g P T G R M Y 6 s X a 1 P y v 1 s s g u v Z y L t M M m K T z R V E m M C R 4 m g U O u W I U x N g A o Y q b W z E d E k U o m M T K J g R 3 8 c v L 0 K 5 V 3 Y u q e 3 9 Z q d e K O E r o B J 2 i c + S i K 1 R H t 6 i B W o i i H D 2 h F / R q P V r P 1 p v 1 P m 9 d s Y q Z I / R H 1 s c 3 6 X 2 U w g = = < / l a t e x i t >
Lh ? d < l a t e x i t s h a 1 _ b a s e 6 4 = "
Y r 6 d m S x k 2 n d l g 8 w Y L d I 1 S 6 M y l V E = " > A A A B 8 3 i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k k V 9 F j w 4 s F D B f s B T S i b z a Z d u t m E 3 Y l Q S v + G F w + K e P X P e P P f u G 1 z 0 N Y H A 4 / 3 Z p i Z F 2 Z S G H T d b 2 d t f W N z a 7 u 0 U 9 7 d 2 z 8 4 r B w d t 0 2 a a 8 Z b L J W p 7 o b U c C k U b 6 F A y b u Z 5 j Q J J e + E o 9 u Z 3 3 n i 2 o h U P e I 4 4 0 F C B 0 r E g l G 0 k n / f H x I f R c I N i f q V q l t z 5 y C r x C t I F Q o 0 + 5 U v P 0 p Z n n C F T F J j e p 6 b Y T C h G g W T f F r 2 c 8 M z y k Z 0 w H u W K m r X B J P 5 z V N y b p W I x K m 2 p Z D M 1 d 8 T E 5 o Y M 0 5 C 2 5 l Q H J p l b y b +
5 / V y j G + C i V B Z j l y x x a I 4 l w R T M g u A R E J z h n J s C W V a 2 F s J G 1 J N G d q Y y j Y E b / n l V d K u 1 7 z L m v d w V W 3 U i z h K c A p n c A E e X E M D 7 q A J L W C Q w T O 8 w p u T O y / O u / O x a F 1 z i p k T + A P n 8 w c 6 X 5 E a < / l a t e x i t >
T ? Lh ? d < l a t e x i t s h a 1 _ b a s e 6 4 = "
6 g r h i p J s J / q a y l D s j 4 w 3 r D e R M S 8 = " > A A A B / n i c b Z D L S s N A F I Y n X m u 9 R c W V m 8 E i u C p J F X R Z c O P C R Y X e o A 1 h M p m 0 Q y e T M H M i l F D w V d y 4 U M S t z + H O t 3 H a R t D W H w Y + / n M O 5 8 w f p I J r c J w v a 2 V 1 b X 1 j s 7 R V 3 t 7 Z 3 d u 3 D w 7 b O s k U Z S 2 a i E R 1 A 6 K Z 4 J K 1 g I N g 3 V Q x E g e C d Y L R z b T e e W B K 8 0 Q 2 Y Z w y L y Y D y S N O C R j L t 4 + b u A 8 8 Z h r f + c M f D H 2 7 4 l S d m f A y u A V U U K G G b 3 / 2 w 4 R m M Z N A B d G 6 5 z o p e D l R w K l g k 3 I / 0 y w l d E Q G r G d Q E r P G y 2 f n T / C Z c U I c J c o 8 C X j m / p 7 I S a z 1 O A 5 M Z 0 x g q B d r U / O / W i + D 6 N r L u U w z Y J L O F 0 W Z w J D g a R Y 4 5 I p R E G M D h C p u b s V 0 S B S h Y B I r m x D c x S 8 v Q 7 t W d S + q 7 v 1 l p V 4 r 4 i i h E 3 S K z p G L r l A d 3 a I G a i G K c v S E X t C r 9 W g 9 W 2 / W + 7 x 1 x S p m j t A f W R / f 2 G 6 U t w = = < / l a t e x i t >
T ? Lh ? d < l a t e x i t s h a 1 _ b a s e 6 4 = "
6 g r h i p J s J / q a y l D s j 4 w 3 r D e R M S 8 = " > A A A B / n i c b Z D L S s N A F I Y n X m u 9 R c W V m 8 E i u C p J F X R Z c O P C R Y X e o A 1 h M p m 0 Q y e T M H M i l F D w V d y 4 U M S t z + H O t 3 H a R t D W H w Y + / n M O 5 8 w f p I J r c J w v a 2 V 1 b X 1 j s 7 R V 3 t 7 Z 3 d u 3 D w 7 b O s k U Z S 2 a i E R 1 A 6 K Z 4 J K 1 g I N g 3 V Q x E g e C d Y L R z b T e e W B K 8 0 Q 2 Y Z w y L y Y D y S N O C R j L t 4 + b u A 8 8 Z h r f + c M f D H 2 7 4 l S d m f A y u A V U U K G G b 3 / 2 w 4 R m M Z N A B d G 6 5 z o p e D l R w K l g k 3 I / 0 y w l d E Q G r G d Q E r P G y 2 f n T / C Z c U I c J c o 8 C X j m / p 7 I S a z 1 O A 5 M Z 0 x g q B d r U / O / W i + D 6 N r L u U w z Y J L O F 0 W Z w J D g a R Y 4 5 I p R E G M D h C p u b s V 0 S B S h Y B I r m x D c x S 8 v Q 7 t W d S + q 7 v 1 l p V 4 r 4 i i h E 3 S K z p G L r l A d 3 a I G a i G K c v S E X t C r 9 W g 9 W 2 / W + 7 x 1 x S p m j t A f W R / f 2 G 6 U t w = = < / l a t e x i t > T ? Lh ?
3d < l a t e x i t s h a 1 _ b a s e 6 4 = "
T r H Q j l T s y L A Z y j K P t K 4 T W w k o S r s = " > A A A B / 3 i c b Z D L S s N A F I Z P v N Z 6 i w p u 3 A w W w V V J W k G X B T c u X F T o D d o Q J p N p O 3 Q y C T M T o c Q u f B U 3 L h R x 6 2 u 4 8 2 2 c t h G 0 9 Y e B j / + c w z n z B w l n S j v O l 7 W y u r a + s V n Y K m 7 v 7 O 7 t 2 w e H L R W n k t A m i X k s O w F W l D N B m 5 p p T j u J p D g K O G 0 H o + t p v X 1 P p W K x a O h x Q r 0 I D w T r M 4 K 1 s X z 7 u I F 6 m k V U o V t / + I P V 0 L d L T t m Z C S 2 D m 0 M J c t V 9 + 7 M X x i S N q N C E Y 6 W 6 r p N o L 8 N S M 8 L p p N h L F U 0 w G e E B 7 R o U 2 O z x s t n 9 E 3 R m n B D 1 Y 2 m e 0 G j m / p 7 I c K T U O A p M Z 4 T 1 U C 3 W p u Z / t W 6 q + 1 d e x k S S a i r I f F E / 5 U j H a B o G C p m k R P O x A U w k M 7 c i M s Q S E 2 0 i K 5 o Q 3 M U v L 0 O r U n a r Z f f u o l S r 5 H E U 4 A R O 4 R x c u I Q a 3 E A d m k D g A Z 7 g B V 6 t R + v Z e r P e 5 6 0 r V j 5 z B H 9 k f X w D U g q U 9 A = = < / l a t e x i t >
T ? Lh ? d < l a t e x i t s h a 1 _ b a s e 6 4 = "
6 g r h i p J s J / q a y l D s j 4 w 3 r D e R M S 8 = " less than 10 seconds , the largest spans are up to 20 seconds .
The average span length is 7.2 seconds , which is short compared to the average length of the full video clips ( 61.49 seconds ) .
> A A A B / n i c b Z D L S s N A F I Y n X m u 9 R c W V m 8 E i u C p J F X R Z c O P C R Y X e o A 1 h M p m 0 Q y e T M H M i l F D w V d y 4 U M S t z + H O t 3 H a R t D W H w Y + / n M O 5 8 w f p I J r c J w v a 2 V 1 b X 1 j s 7 R V 3 t 7 Z 3 d u 3 D w 7 b O s k U Z S 2 a i E R 1 A 6 K Z 4 J K 1 g I N g 3 V Q x E g e C d Y L R z b T e e W B K 8 0 Q 2 Y Z w y L y Y D y S N O C R j L t 4 + b u A 8 8 Z h r f + c M f D H 2 7 4 l S d m f A y u A V U U K G G b 3 / 2 w 4 R m M Z N A B d G 6 5 z o p e D l R w K l g k 3 I / 0 y w l d E Q G r G d Q E r P G y 2 f n T / C Z c U I c J c o 8 C X j m / p 7 I S a z 1 O A 5 M Z 0 x g q B d r U / O / W i + D 6 N r L u U w z Y J L O F 0 W Z w J D g a R Y 4 5 I p R E G M D h C p u b s V 0 S B S h Y B I r m x D c x S 8 v Q 7 t W d S + q 7 v 1 l p V 4 r 4 i i h E 3 S K z p G L r l A d 3 a I G a i G K c v S E X t C r 9 W g 9 W 2 / W + 7 x 1 x S p m j t A f W R / f 2 G 6 U t w = = < / l a t e x i t >
T ? d < l a t e x i t s h a 1 _ b a s e 6 4 = "
9 o U I a m f a O E F + W 8 i G q u o B 9 y I C g 9 8 = " > A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o M e C F 4 8 V + o V t K J v N p l 2 6 2 Y T d i V B K / 4 U X D 4 p 4 9 d 9 4 8 9 + 4 b X P Q 1 g c D j / d m m J k X p F I Y d N 1 v p 7 C x u b W 9 U 9 w t 7 e 0 f H B 6 V j 0 / a J s k 0 4 y 2 W y E R 3 A 2 q 4 F I q 3 U K D k 3 V R z G g e S d 4 L x 3 d z v P H F t R K K a O E m 5 H 9 O h E p F g F K 3 0 2 C R 9 F D E 3 J B y U K 2 7 V X Y C s E y 8 n F c j R G J S / + m H C s p g r Z J I a 0 / P c F P 0 p 1 S i Y 5 L N S P z M 8 p W x M h 7 x n q a J 2 j T 9 d X D w j F 1 Y J S Z R o W w r J Q v 0 9 M a W x M Z M 4 s J 0 x x Z F Z 9 e b i f 1 4 v w + j W n w q V Z s g V W y 6 K M k k w I f P 3 S S g 0 Z y g n l l C m h b 2 V s B H V l K E N q W R D 8 F Z f X i f t W t W 7 q n o P 1 5 V 6 L Y + j C G d w D p f g w Q 3 U 4 R 4 a 0 A I G C p 7 h F d 4 c 4 7 w 4 7 8 7 H s r X g 5 D O n 8 A f O 5 w / H K Z B H < / l a t e x i t >
T ? d < l a t e x i t s h a 1 _ b a s e 6 4 = "
9 o U I a m f a O E F + W 8 i G q u o B 9 y I C g 9 8 = " > A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o M e C F 4 8 V + o V t K J v N p l 2 6 2 Y T d i V B K / 4 U X D 4 p 4 9 d 9 4 8 9 + 4 b X P Q 1 g c D j / d m m J k X p F I Y d N 1 v p 7 C x u b W 9 U 9 w t 7 e 0 f H B 6 V j 0 / a J s k 0 4 y 2 W y E R 3 A 2 q 4 F I q 3 U K D k 3 V R z G g e S d 4 L x 3 d z v P H F t R K K a O E m 5 H 9 O h E p F g F K 3 0 2 C R 9 F D E 3 J B y U K 2 7 V X Y C s E y 8 n F c j R G J S / + m H C s p g r Z J I a 0 / P c F P 0 p 1 S i Y 5 L N S P z M 8 p W x M h 7 x n q a J 2 j T 9 d X D w j F 1 Y J S Z R o W w r J Q v 0 9 M a W x M Z M 4 s J 0 x x Z F Z 9 e b i f 1 4 v w + j W n w q V Z s g V W y 6 K M k k w I f P 3 S S g 0 Z y g n l l C m h b 2 V s B H V l K E N q W R D 8 F Z f X i f t W t W 7 q n o P 1 5 V 6 L Y + j C G d w D p f g w Q 3 U 4 R U F r E 0 R 0 O E + i E r B e J Q H C G R u o W y q 6 E A E v U R X j A D P z R 2 Z Q 0 j k 4 d 6 i r R 6 + O J c U U I m v r 5 b q F o l + 1 J 0 U V w Z l A k s 6 p 1 C 1 + u H / M 0 h A i 5 Z F q 3 H T v B T s Y U C i 5 h l H d T D Q n j A 9 a D t s G I m T 2 d b H L X i B 4 b x a d B r M y L k E 7 U 3 x M Z C 7 U e h p 7 p D B n 2 9 b w 3 F v / z 2 i k G V 5 1 M R E m K E P H p o i C V F G M 6 D o n 6 Q g F H O T T A u B L m U Y g = " > A A A C B X i c b V D J S g N B E O 1 x j X E b 9 a i H w S B 4 C j M q m I s Q 8 O I x Q j b I x K G n U 0 m a 9 C x 0 1 w T D M B 6 8 + C t e P C j i 1 X / w 5 t / Y W Q 6 a + K D g 8 V 4 V V f X 8 W H C F t v 1 t L C 2 v r K 6 t 5 z b y m 1 v b O 7 v m 3 n 5 d R Y l k U G O R i G T T p w o E D 6 G G H A U 0 Y w k 0 8 A U 0 / M H 1 2 G 8 M Q S o e h V U c x d A O a C / k X c 4 o a s k z j 1 y E e 0 y H v A N R 9 u C m Q w / d z E v x y s n u q p 5 Z s I v 2 B N Y i c W a k Q G a o e O a X 2 4 l Y E k C I T F C l W o 4 d Y z u l E j k T k O X d R E F M 2 Y D 2 o K V p S A N Q 7 X T y R W a d a K V j d S O p K 0 R r o v 6 e S G m g 1 C j w d W d A s a / m v b H 4 n 9 d K s F t q p z y M E 4 S Q T R d 1 E 2 F h Z I 0 j s T p c A k M x 0 o Q y y f W t F u t T S R n q 4 P I 6 B G f + 5 U V S P y s 6 5 0 X n 9 q J Q L s 3 i y J F D c k x O i U M u S Z n c k A q p E U Y e y T N 5 J W / G k / F i v B s f 0 9 Y l Y z Z z Q P 7 A + P w B u r W Z T g = = < / l a t e x i t > subtitle { st } T t=1 < l a t e x i t s h a 1 _ b a s e 6 4 = " S R h G u S c V 9 o 2 S s j 1 x x R f L k l D b x b 8 = " > A A A C C H i c b V D L S s N A F J 3 U V 6 2 v q E s X B o v g q i Q q 2 I 1 Q c O O y Q l / Q x D C Z T t q h k w c z N 2 I J c e f G X 3 H j Q h G 3 f o I 7 / 8 Z p m o W 2 H h g 4 n H P P z N z j x Z x J M M 1 v r b S 0 v L K 6 V l 6 v b G x u b e / o u 3 s d G S W C 0 D a J e C R 6 H p a U s 5 C 2 g Q G n v V h Q H H i c d r 3 x 1 d T v 3 l E h W R S 2 Y B J T J 8 D D k P m M Y F C S q x / a Q O 8 h l Y m X h 7 M H O 5 U u 2 J m b w q W V 3 b Z c v W r W z B z G I r E K U k U F m q 7 + Z Q 8 i k g Q 0 B M K x l H 3 L j M F J s Q B G 1 P 0 V O 5 E 0 x m S M h 7 S v a I g D K p 0 0 X y Q z j p U y M P x I q B O C k a u / E y k O p J w E n p o M M I z k v D c V / / P 6 C f h 1 J 2 V h n A A N y e w h P + E G R M a 0 F W P A B C X A J 4 p g I p j 6 q 0 F G W G A C q r u K K s G a X 3 m R d E 5 r 1 l n N u j m v N u p F H W V 0 g I 7 Q C b L Q B W q g a 9 R E b U T Q I 3 p G r + h N e 9 J e t H f t Y z Z a 0 o r M P v o D 7 f M H U R y a v g = = < / l a t e x i t > hypothesis hk < l a t e x i t s h a 1 _ b a s e 6 4 = " U E b l 0 f 3 N i Y 3 G t 1 8 Y F f t f 3 1 X U J x I = " > A A A B / n i c b V B N S 8 N A E N 3 U r 1 q / o u L J S 7 A I n k q i g j 0 W v H i s Y D + g D W G z n T Z L N x / s T s Q Q K v 4 V L x 4 U 8 e r v 8 O a / c d v m o K 0 P B h 7 v z T A z z 0 8 E V 2 j b 3 0 Z p Z X V t f a O 8 W d n a 3 t n d M / c P 2 i p O J Y M W i 0 U s u z 5 V I H g E L e Q o o J t I o K E v o O O P r 6 d + 5 x 6 k 4 n F 0 h 1 k C b k h H E R 9 y R l F L n n n U R 3 j A P M i S G A N Q X E 0 e A 2 / s m V W 7 Z s 9 g L R O n I F V S o O m Z X / 1 B z N I Q I m S C K t V z 7 A T d n E r k T M C k 0 k 8 V J J S N 6 Q h 6 m k Y 0 B O X m s / M n 1 q l W B t Y w l r o i t G b q 7 4 m c h k p l o a 8 7 Q 4 q B W v S m 4 n 9 e L 8 V h 3 c 1 5 l K Q I E Z s v G q b C w t i a Z m E N u A S G I t O E M s n 1 r R Y L q K Q M d W I V H Y K z + P I y a Z / X n I u a c 3 t Z b d S L O M r k m J y Q M + K Q K 9 I g N 6 R J W o S R n D y T V / J m P B k v x r v x M W 8 t G c X M I f k D 4 / M H r + 2 W j w = = < / l a t e x i t >
Methods
Our proposed method , Spatio-Temporal Answerer with Grounded Evidence ( STAGE ) , is a unified framework for moment localization , object grounding and video QA .
First , STAGE encodes the video and text ( subtitle , QA ) via frame - wise regional visual representations and neural language representations , respectively .
The encoded video and text representations are then contextualized using a Convolutional Encoder .
Second , STAGE computes attention scores from each QA word to object regions and subtitle words .
Leveraging the attention scores , STAGE is able to generate QA - aware representations , as well as automatically detecting the referred objects / people .
The attended QA - aware video and subtitle representation are then fused together to obtain a joint frame -wise representation .
Third , taking the frame-wise representation as input , STAGE learns to predict QA relevant temporal spans , then combines the global and local ( span localized ) video information to answer the questions .
In the following , we describe STAGE in detail .
Formulation
In our tasks , the inputs are : ( 1 ) a question with 5 candidate answers ; ( 2 ) a 60 - second long video ; ( 3 ) a set of subtitle sentences .
Our goal is to predict the answer and ground it both spatially and temporally .
Given the question , q , and the answers , { a k } 5 k=1 , we first formulate them as 5 hypotheses ( QA - pair ) h k = [ q , a k ] and predict their correctness scores based on the video and subtitle context ( Onishi et al. , 2016 ) .
We denote the ground-truth ( GT ) answer index as y ans and thus the GT hypothesis as h y ans .
We then extract video frames { v t } T t=1 at 0.5 FPS ( T is the number of frames for each video ) .
Subtitle sentences are then temporally aligned with the video frames .
Specifically , for each frame v t , we pair it with two neighboring sentences based on the subtitle timestamps .
We choose two neighbors since this keeps most of the sentences at our current frame rate , and also avoids severe misalignment between the frames and the sentences .
The set of aligned subtitle sentences are denoted as {s t } T t=1 .
We denote the number of words in each hypothesis and subtitle as L h , L s , respectively .
We use N o to denote the number of object regions in a frame , and d = 128 as the hidden size .
STAGE Architecture Input Embedding Layer
For each frame v t , we use Faster R-CNN ( Ren et al. , 2015 ) pre-trained on Visual Genome ( Krishna et al. , 2017 ) to detect objects and extract their regional representation as our visual features ( Anderson et al. , 2018 ) .
We keep the top - 20 object proposals and use PCA to reduce the feature dimension from 2048 to 300 , to save GPU memory and computation .
We denote o t, r ?
R 300 as the r-th object embedding in the t-th frame .
To encode the text input , we use BERT ( Devlin et al. , 2019 ) , a transformer - based language model ( Vaswani et al. , 2017 ) that achieves state- ofthe - art performance on various NLP tasks .
Specifically , we first fine - tune the BERT - base model using the masked language model and next sentence pre-diction objectives on the subtitles and QA pairs from TVQA + train set .
Then , we fix its parameters and use it to extract 768D word - level embeddings from the second - to - last layer for the subtitles and each hypothesis .
Both embeddings are projected into a 128D space using a linear layer with ReLU .
Convolutional Encoder Inspired by the recent trend of replacing recurrent networks with CNNs ( Dauphin et al. , 2016 ; Yu et al. , 2018a ) and Transformers ( Vaswani et al. , 2017 ; Devlin et al. , 2019 ) for sequence modeling , we use positional encoding ( PE ) , CNNs , and layer normalization ( Ba et al. , 2016 ) to build our basic encoding block .
As shown in the bottom-right corner of Fig. 4 , it is comprised of a PE layer and multiple convolutional layers , each with a residual connection ( He et al. , 2016 ) and layer normalization .
We use Layernorm ( ReLU ( Conv ( x ) ) + x ) to denote a single Conv unit and stack N conv of such units as the convolutional encoder .
x is the input after PE , Conv is a depthwise separable convolution ( Chollet , 2017 ) .
We use two convolutional encoders at two different levels of STAGE , one with kernel size 7 to encode the raw inputs , and another with kernel size 5 to encode the fused video-text representation .
For both encoders , we set N conv = 2 .
QA - Guided Attention
For each hypothesis h k = [ q , a k ] , we compute its attention scores w.r.t. the object embeddings in each frame and the words in each subtitle sentence , respectively .
Given the encoded hypothesis H k ?
R L h ?d for the hypothesis h k with L h words , and encoded visual feature V t ?
R No?d for the frame v t with N o objects , we compute their matching scores M k, t ?
R L h ? No = H k V T t .
We then apply softmax at the second dimension of M k,t to get the normalized scores Mk , t .
Finally , we compute the QA - aware visual representation V att k , t ? R L h ?d = Mk , t V t .
Similarly , we compute QA - aware subtitle representation S att k , t . Video- Text Fusion
The above two QA - aware representations are then fused together as : F k, t = [ S att k , t ; V att k , t ; S att k , t V att k, t ] W F + b F , where denotes hadamard product , W F ? R 3d?d and b F ?
R d are trainable weights and bias , F k, t ?
R L h ?d is the fused video-text representation .
After collecting F att k, t from all time steps , we get F att k ?
R T ?L h ?d .
We then apply another convolutional encoder with a max-pooling layer to obtain the output A k ? R T ?d . Span Predictor
To predict temporal spans , we predict the probability of each position being the start or end of the span .
Given the fused input A k ?
R T ?d , we produce start probabilities p 1 k ?
R T and end probabilities p 2 k ?
R T using two linear layers with softmax , as shown in the top-right corner of Fig.
4 . Different from existing works
Yu et al. , 2018a ) that used the span predictor for text only , we use it for a joint localization of both video and text , which requires properly - aligned joint embeddings .
Span Proposal and Answer Prediction Given the max-pooled video- text representation A k , we use a linear layer to further encode it .
We run maxpool across all the time steps to get a global hypothesis representation G g k ?
R d .
With the start and end probabilities from the span predictor , we generate span proposals using dynamic programming .
At training time , we combine the set of proposals with IoU ? 0.5 with the GT spans , as well as the GT spans to form the final proposals { st p , ed p } ( Ren et al. , 2015 ) .
At inference time , we take the proposals with the highest confidence scores for each hypothesis .
For each proposal , we generate a local representation G l k ?
R d by maxpooling A k, stp:edp .
The local and global representations are concatenated to obtain G k ?
R 2d .
We then forward { G k } 5 k=1 through softmax to get the answer scores p ans ?
R 5 . Compared with existing works ( Jang et al. , 2017 ; Zhao et al. , 2017 ) that use soft temporal attention , we use more interpretable hard attention , extracting local features ( together with global features ) for question answering .
Training and Inference
In this section , we describe the objective functions used in the STAGE framework .
Since our spatial and temporal annotations are collected based on the question and GT answer , we only apply the attention loss and span loss on the targets associated with the GT hypothesis ( question + GT answer ) , i.e. , M k=y ans , t , p 1 k=y ans and p 2 k=y ans .
For brevity , we omit the subscript k=y ans in the following .
Spatial Supervision
While the attention described in Sec. 4.2 can be learned in a weakly supervised end-to - end manner , we can also train it with supervision from GT boxes .
We define a box as positive if it has an IoU ? 0.5 with the GT box .
Consider the attention scores M t , j ? R
No from a concept word w j in GT hypothesis h y ans to the set of proposal boxes ' representations { o t , r } No r=1 at frame v t .
We expect the attention on positive boxes to be higher than the negative ones , and therefore use LSE loss for the supervision : L t , j = rp?p , rn?n log 1 + exp ( M t, j , rn ?
M t, j, rp ) , where M t,j, rp is the r p - th element of the vector M t , j . ? p and ?
n denote the set of positive and negative box indices , respectively .
LSE loss is a smoothed alternative to the widely used hinge loss , it is easier to optimize than the original hinge loss .
During training , we randomly sample two negatives for each positive box .
We use L att i to denote the attention loss for the i-th example , which is obtained by summing over all the annotated frames {v t } and concepts { w j } for L att t , j .
We define the overall attention loss L att = 1 N N i=1 L att i .
At inference time , we choose the boxes with scores higher than 0.2 as the predictions .
Temporal Supervision Given softmax normalized start and end probabilities p 1 and p 2 , we apply cross-entropy loss : L span = ?
1 2N
N i=1 log p 1 y 1 i + log p 2 y 2 i , where y 1 i and y 2 i are the GT start and end indices .
Answer Prediction Similarly , given answer probabilities p ans , our answer prediction loss is : L ans = ?
1 N N i=1 log p ans y ans i , where y ans i is the index of the GT answer .
Finally , the overall loss is a weighted combination of the three objectives above : L = L ans + w att L att + w span L span , where w att and w span are set as 0.1 and 0.5 based on validation set tuning .
Experiments
As introduced , our task is spatio-temporal video question answering , requiring systems to temporally localize relevant moments , spatially detect referred objects and people , and answer questions .
In this section , we first define the evaluation metrics , then compare STAGE against several baselines , and finally provide a comprehensive analysis of our model .
Additionally , we also evaluate STAGE on the full TVQA dataset .
Model QA Grd. Temp. ASA Acc. mAP mIoU ST -VQA ( Jang et al. , 2017 ) 48.28 --two-stream ( Lei et al. , 2018 )
Metrics
To measure QA performance , we use classification accuracy ( QA Acc. ) .
We evaluate span prediction using temporal mean Intersection - over -Union ( Temp. m IoU ) following previous work ( Hendricks et al. , 2017 ) on language - guided video moment retrieval .
Since the span depends on the hypothesis ( QA pair ) , each QA pair provides a predicted span , but we only evaluate the span of the predicted answer .
Additionally , we propose Answer -Span joint Accuracy ( ASA ) , that jointly evaluates both answer prediction and span prediction .
For this metric , we define a prediction to be correct if the predicted span has an IoU ? 0.5 with the GT span , provided that the answer prediction is correct .
Finally , to evaluate object grounding performance , we follow the standard metric from the PASCAL VOC challenge ( Everingham et al. , 2015 ) and report the mean Average Precision ( Grd. mAP ) at IoU threshold 0.5 .
We only consider the annotated words and frames when calculating the mAP .
Comparison with Baseline Methods
We consider the two-stream model ( Lei et al. , 2018 ) as our main baseline .
In this model , two streams are used to predict answer scores from subtitles and videos respectively and final answer scores are produced by summing scores from both streams .
We retrain the model using the official code 2 on TVQA + data , with the same feature as STAGE .
We also consider ST - VQA ( Jang et al. , 2017 ) model , which is primarily designed for question answering on short videos ( GIFs ) .
We also provide STAGE variants that use only video or subtitle to study the effect of using only one of the modalities .
Table 3 shows the test results of STAGE and the baselines .
STAGE outperforms the baseline model ( two -stream ) by a large margin in QA Acc. , 3 with 9.83 % relative gains .
Additionally , STAGE also lo- calizes the relevant moments with temporal mIoU of 32.49 % and detects referred objects and people with mAP of 27.34 % .
However , a large gap is still observed between STAGE and human , showing space for further improvement .
Model Analysis Backbone Model Given the full STAGE model defined in Sec. 4 , we define the backbone model as the ablated version of it , where we remove the span predictor along with the span proposal module , as well as the explicit attention supervision .
We further replace the CNN encoders with RNN encoders , and remove the aligned fusion from the backbone model .
This baseline model uses RNN to encode input sequences and interacts QA pairs with subtitles and videos separately .
The final confidence score is the sum of the confidence scores from the two modalities .
In the backbone model , we align subtitles with video frames from the start , fusing their representation conditioned on the input QA pair , as in Fig.
4 .
We believe this aligned fusion is essential for improving QA performance , as the latter part of STAGE has a joint understanding of both video and subtitles .
With both changes , our backbone model obtains 68.31 % on QA Acc. , significantly higher than the baseline 's 65.79 % .
The results are shown in Table 4 . Model Temp .
Sup. val test-public two -stream ( Lei et al. , 2018 ) 65.85 66.46 PAMN ( Kim et al. , 2019 b ) 66.38 66.77 multi-task ( Kim et al. , 2019a ) 66 Temporal and Spatial Supervision In Table 4 , we also show the results when using temporal and spatial supervision .
After adding temporal supervision , the model is be able to ground on the temporal axis , which also improves the model 's performance on other tasks .
Adding spatial supervision gives additional improvements , particularly for Grd. mAP , with 121.92 % relative gain .
Span Proposal and Local Feature
In the second- to - last row of Table 4 , we show our full STAGE model , which is augmented with local features G l for question answering .
Local features are obtained by max-pooling the span proposal regions , which contain more relevant cues for answering the questions .
With G l , we achieve the best performance across all metrics , indicating the benefit of using local features .
Inference with GT Span
The last row of Table 4 shows our model uses GT spans instead of predicted spans at inference time .
We observe better QA Acc. with GT spans .
Accuracy by Question Type In Table 5 , we show a breakdown of QA Acc. by question type .
We observe a clear increasing trend on " what " , " who " , and " where " questions after using the backbone net and adding attention / span modules in each column .
Interestingly , for " why " and " how " questions , our full model fails to present overwhelming performance , indicating some reasoning ( textual ) module to be incorporated as future work .
Qualitative Examples
We show two correct predictions in Fig. 5 , where Fig. 5
TVQA Results
We also conduct experiments on the full TVQA dataset ( Table 6 ) , without relying on the bounding boxes and refined timestamps in TVQA + .
Without temporal supervision , STAGE backbone is able to achieve 3.91 % relative gain from the best published result ( multi-task ) on TVQA test-public set .
Adding temporal supervision , performance is improved to 70.23 % .
For a fair comparison , we also provided STAGE variants using GloVe ( Pennington et al. , 2014 ) instead of BERT ( Devlin et al. , 2019 ) as text feature .
Using GloVe , STAGE models still achieve better results .
Conclusion
We collected the TVQA + dataset and proposed the spatio-temporal video QA task .
This task requires systems to jointly localize relevant moments , detect referred objects / people , and answer questions .
We further introduced STAGE , an end-to - end trainable framework to jointly perform all three tasks .
Comprehensive experiments show that temporal and spatial predictions help improve QA performance , as well as providing explainable results .
Though our STAGE achieves state - of - the - art performance , there is still a large gap compared with human performance , leaving space for further improvement .
