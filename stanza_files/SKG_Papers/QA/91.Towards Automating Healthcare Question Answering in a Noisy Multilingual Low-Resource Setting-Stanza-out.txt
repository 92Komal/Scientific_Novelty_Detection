title
Towards Automating Healthcare Question Answering in a Noisy Multilingual Low-Resource Setting
abstract
We discuss ongoing work into automating a multilingual digital helpdesk service available via text messaging to pregnant and breastfeeding mothers in South Africa .
Our anonymized dataset consists of short informal questions , often in low-resource languages , with unreliable language labels , spelling errors and code-mixing , as well as template answers with some inconsistencies .
We explore crosslingual word embeddings , and train parametric and non-parametric models on 90 K samples for answer selection from a set of 126 templates .
Preliminary results indicate that LSTMs trained end-to - end perform best , with a test accuracy of 62.13 % and a recall@5 of 89.56 % , and demonstrate that we can accelerate response time by several orders of magnitude .
Introduction MomConnect is a project led by South Africa 's National Department of Health ( NDOH ) , and is freely available at public clinics to all mothers and pregnant women who wish to sign up .
The platform provides maternal support through text messaging in all 11 official languages of South Africa and has had 2.6 million registrations since 2014 .
MomConnect is unique in the public health sector , allowing users to pose questions to helpdesk staff who respond manually .
The recent introduction of WhatsApp as a channel additional to SMS has increased the volume of questions substantially .
This presents a significant challenge for the staffing complement , and median response time is currently 20 hours .
The templated - based nature of answers provides an immediate opportunity for using computational linguistics to automate the response pipeline .
In a similar study , Engelhard et evaluated the need for and feasibility of automated message triage to improve helpdesk responsiveness to high- priority messages .
During 2018 we worked with the NDOH to gain access to MomConnect data for research purposes .
After rigorous ethical clearance and user privacy protection protocols , we obtained a static copy of about 230,000 raw textual question - answer pairs .
The primary objective of this research is to investigate ways in which the burden on helpdesk staff can be reduced , which could enable MomConnect to scale towards a wider reach and a more effective service .
One aspect of this is to automate the question answering process .
A language label is recorded when a user signs up at the clinic , but users are free to ask questions in any language , which poses a significant challenge to the language processing problem .
However , the labels do provide a proxy of the language imbalance within the dataset , as indicated in Table 1 .
The challenge of automating MomConnect gives us rare access to a fairly large dataset of closed - domain multilingual questions paired with template English answers , with many questions in low-resource languages , unreliable language labels , a prevalence of code-mixing , misspellings and use of shorthand , and some inconsistencies in the answers .
While the data cannot be made public due to its highly sensitive nature , we can report our findings and provide guidelines for future problems of a similar nature .
In this paper we describe the process of acquiring , anonymizing and filtering the dataset , deduplicating the answer set , and our first attempts towards automating the answering of questions .
The work provides a unique opportunity to apply computational linguistic theories to a real-world problem for social impact .
Related Work Question Answering ( QA ) aims to interpret natural language questions and respond appropriately with natural language answers .
Current approaches achieve impressive results on factoid , list and definition questions , but struggle in real-world settings where the questions and answers are more complex ( Wang and Ittycheriah , 2015 ) .
FAQ approaches aim to economically reuse previously answered questions to guide future answers ( Burke et al. , 1997 ) , but a core challenge is to calculate similarity between questions with little word overlap .
Proposed solutions to this challenge include machine - readable dictionaries like the semantic lexicon WordNet ( Miller , 1995 ) , manual rules or templates ( Sneiders , 2002 ) , and statistical NLP and information retrieval techniques ( Berger et al. , 2000 ) .
Each has its drawbacks : machine - readable dictionaries exist only for a select few languages , manual template creation is time - consuming and does not scale well , and statistical methods usually require large datasets .
Distributional semantic models learn a mapping of words in their textual form to a dense , lowdimensional vector space ( Mikolov et al. , 2013a , b ; Pennington et al. , 2014 ) .
Such methods render contextual and co-occurrence information from large open-domain text repositories .
For lowresource languages there is often not sufficient data to develop useful word embedding models , and several approaches to deal with this issue have been proposed .
With a collaborative filtering technique called positive -unlabeled learning , unobserved word pairs can provide valuable information , especially in low-resource settings ( Jiang et al. , 2018 ) .
Another approach is cross-lingual word representations , where a shared word embedding is trained from multiple languages .
This approach facilitates knowledge transfer from high-resource to low-resource language models ( Ruder , 2017 ) , and can lead to more robust multilingual information retrieval ( Vuli ?
et al. , 2015 ) .
Embeddings trained on multilingual data with codemixing also seem to outperform those trained separately on monolingual data ( Pratapa et al. , 2018 ) .
Data Acquisition and Anonymization Data acquisition followed a rigorous ethical clearance process and , given the highly sensitive nature of the data ( disclosing HIV status , for example ) , access was conditional on anonymization and nondistribution of the data , and granted with the sole purpose of research .
A future goal of this work is to provide a process for data generated on similar platforms to be shared more widely for research purposes , without compromising any data protection rights of individuals .
Guided by the General Data Protection Regulations ( GDPR ) , and prior to any data processing or analysis , an anonymization protocol was established to meet the " motivated intruder " test .
It can be insufficient to simply remove identifiers such as name and telephone number , as identities might still be deducible from contextual information .
It would be problematic if , for example , a certain clinic location and sign - up language narrowed possibilities to a handful of individuals .
As such , we rank identifying information by importance to our research and algorithmically remove data in order of increasing priority , to ensure some lower bound on the size of any single distinguishable group .
This approach is conceptually similar to the k-anonymization algorithm ( Samarati and Sweeney , 1998 ; El Emam and Dankar , 2008 ) .
We also replace absolute identifiers ( e.g. expected delivery date ) with relative quantities ( e.g. days to delivery ) .
Age data is bucketed and district information hashed against one - time random numbers , to prevent direct identification .
In future we plan to improve the protocol with differential privacy techniques ( Dwork and Roth , 2014 ) where applicable .
Answer Selection
We proceed to describe our first attempts at automating answer selection for MomConnect .
We evaluate naive Bayes on bag-of-words , exact and approximate k-nearest neighbors on cross-lingual word embeddings , as well as long-short term memory networks trained end-to-end .
Question Template Answer
Data Preparation
In the raw dataset of 230K question - answer pairs we discovered 42,675 unique answers , approximating a power law distribution .
During initial investigations we found that many answers were near-duplicates of others , likely due to revisions and updates in the history of the manual answering process .
Near-duplicates were automatically identified using a word-level Jaccard similarity index , and substituted with the more frequently occurring answers .
A small sample of positive and negative word - level matches were manually verified .
This being a work - in- progress , we decided for now to focus on answers appearing at least 128 times in the dataset .
This number attempts to address the need to include as many training samples per answer as possible to reduce the variance , while ensuring that under-represented languages ( such as Ndebele , with only 97 registered users ) are not excluded in the reduced dataset .
This leaves us with 126 template answers , and account for close to 70 % of all the data .
Table 2 shows
The remaining 30 % of data makes up the long tail of the frequency distribution of answers , many of which occurring only once or twice , and modelling these answers is reserved for future work .
Cross-lingual Word Embeddings
Motivated by the literature on cross-lingual word embeddings ( Vuli ?
et al. , 2015 ; Ruder , 2017 ; Pratapa et al. , 2018 ) and our data having several low-resource languages , unreliable language labels and a prevalence of code-mixing , we opt to mix all the languages together into a shared cross-lingual continuous bag-of-words embedding space ( Mikolov et al. , 2013a )
Characters other than Latin symbols , spaces and numerals are removed .
We do not remove any stop words , in order to preserve the limited vocabulary of some of the low-resource languages , and end up with a dictionary size of 65,547 .
For the nearestneighbor classifiers , the word embeddings of all the words in a question are averaged into a single vector ( Wieting et al. , 2015 ) .
For a peek at what the continuous bag-of-words embedding model does , here is an example of the same word in English , Zulu and Xhosa , and their respective closest neighbors in embedding space , using cosine distance : child : baby , bbe , babe , bby , babay ingane : ingan , yami , ngane , umtwana umntwana : umtwana , wam , umntana , wami Different spellings and shorthand of the same word or concept tend to be clustered together , which is useful when working with SMS and WhatsApp messages .
Note also the slight overlap in the Zulu and Xhosa examples , due to the two languages being closely related .
Classification
We perform classification on the questions to select most appropriate answers from the 126 templates .
As a baseline we train a multinomial naive Bayes ( MNB ) classifier on bag-of-words represen-tations of the questions , using as dictionary only the 7,000 most frequent words across the training set .
We then consider k-nearest neighbor ( k - NN ) classification on the averaged word embeddings of questions , using uniformly weighted majority voting , and for increasing values of k .
We also consider locality -sensitive hashing ( LSH ) , an approximate nearest neighbor algorithm , which sacrifices accuracy in k-NN for efficiency .
With LSH , embedding vectors are randomly hashed into short binary encodings that preserve local information , thus enabling nearest neighbor searching in sublinear time ( Andoni and Indyk , 2008 ) .
Long short - term memory ( LSTM ) networks have been shown to model sequential text data well ( Tai et al. , 2015 ) .
We train various networks end-to-end , with increasing numbers of hidden units ( LSTM - k will denote a network with k hidden units ) .
Each model takes a variable - length sequence of word IDs as input and has a softmax output layer for classification .
The networks are optimized using Adam ( Kingma and Ba , 2014 ) , with a learning rate of 10 ?3 , batch size of 32 , and early stopping based on validation loss .
For regularization we apply 40 % dropout ( Srivastava et al. , 2014 ) to the final layer of the LSTM .
We experimented with using sequences of our cross-lingual word embeddings as input , but saw better performance with end-to - end training on sequences of word IDs .
We also tested bidirectional LSTMs but found no improvement in performance .
Results
The models are evaluated by classification accuracy on the test set of 30 K as yet unseen questionanswer pairs .
We also identify a " low - resource " part of the test set , and measure accuracy on that .
Given our inclusion of the entire dictionary of words and the absence of reliable language labels , we wish to understand how the model performs on uncommon words and sentences .
Thus , we rank each word in the dictionary by its frequency over the training set , as a proxy for belonging to highor low-resource languages .
Questions belonging to the test set are ranked based on how many of their words have high frequencies , and we extract the bottom 25 % as our " low- resource " ( LR ) test set .
Accuracies obtained by the various models are displayed in Table 3 .
The MNB baseline performs quite well , both on the full test set and the LR test set , but possibly due to bias for the high- resource languages in its bagof-words features .
The nearest neighbor models ( k- NN and k- LSH ) show almost no improvement over MNB , and do worse on the LR set .
The efficient LSH models perform almost the same as the NN models they approximate .
The LSTM models seem to perform best .
Increasing the number of hidden units improves accuracy on the full test set , but decreases accuracy on the LR set .
This could again be due to slight overfitting on the highresource languages during training .
While LSTM shows a significant improvement over the other models , it reaches an accuracy of only 62.13 % on the full test set .
This is understandable given the complexities of noisy data , multilinguality , and code-mixing , but succedding only 6 times out of 10 is insufficient for a realworld implementation .
In order to gauge the feasibility of a top - 5 recommender system assisting a human operator , we also measure recall@5 for the MNB baseline and LSTM models .
Results are shown in
The task of querying one of the trained models for an answer ( or top five answers ) to a question takes a second or two on an ordinary desktop computer .
This is a significant improvement in response time over the median of 20 hours currently required by the manual answering process , and can enable MomConnect to scale .
Conclusion and Future Work
We described the first steps towards automating a multilingual digital helpdesk for pregnant and breastfeeding mothers in South Africa .
Gaining access to data was subject to ethical clearance and data anonymization , due to the highly sensitive nature of the content and the vulnerability of individuals involved .
We considered various approaches to the answer selection problem in a noisy , multilingual , low-resource setting .
LSTM networks trained end-to - end outperformed all the other models tested , achieving accuracies of about 62 % and 56 % on the full and low-resource test sets , respectively .
The best LSTM further achieved a recall@5 of almost 90 % .
Such a model can serve in a semiautomated answer selection process , with a human in the loop to choose the final answer .
This could significantly reduce the burden of the current staffing compliment , if approximately 70 % of the queries can be dealt with in a semi-automated manner .
In the case where the human does not agree with any of the suggested answers , the option can remain for the human operator to manually select the correct standardized response , as is currently done .
This feedback can help improve the automated response service , and assist future research tasks .
A next step would be comprehensive error analysis for a better understanding of where the models succeed or fail in capturing semantic information , particularly for the low-resource languages .
We are also working to include into our models the long tail in the distribution of template answers .
We further intend to explore transfer learning techniques ( Zhang et al. , 2017 ) as well as deep architectures designed specifically for answer selection ( Lai et al. , 2018 ) .
There is also scope to develop language identification tools using the unreliable language labels as noisy priors .
This could assist with training separate models for the low-resource languages , or provide an answer in the same language as the question .
a few examples .
The reduced set of 150K questionanswer pairs were split into training , validation and test data ( 60:20:20 ) .
