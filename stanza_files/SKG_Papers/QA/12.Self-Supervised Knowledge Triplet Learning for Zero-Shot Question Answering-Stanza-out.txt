title
Self -Supervised Knowledge Triplet Learning for Zero- Shot Question Answering
abstract
The aim of all Question Answering ( QA ) systems is to generalize to unseen questions .
Current supervised methods are reliant on expensive data annotation .
Moreover , such annotations can introduce unintended annotator bias , making systems focus more on the bias than the actual task .
This work proposes Knowledge Triplet Learning ( KTL ) , a self-supervised task over knowledge graphs .
We propose heuristics to create synthetic graphs for commonsense and scientific knowledge .
We propose using KTL to perform zero-shot question answering , and our experiments show considerable improvements over large pre-trained transformer language models .
Introduction
The ability to understand natural language and answer questions is one of the core focuses in the field of natural language processing .
To measure and study the different aspects of question answering , several datasets are developed , such as SQuaD ( Rajpurkar et al. , 2018 ) , HotpotQA ( Yang et al. , 2018 ) , and Natural Questions ( Kwiatkowski et al. , 2019 ) which require systems to perform extractive question answering .
On the other hand , datasets such as SocialIQA ( Sap et al. , 2019 b ) , Common-senseQA ( Talmor et al. , 2018 ) , Swag ( Zellers et al. , 2018 ) and Winogrande require systems to choose the correct answer from a given set .
These multiple -choice question answering datasets are very challenging , but recent large pre-trained language models such as BERT ( Devlin et al. , 2018 ) , XLNET ( Yang et al. , 2019 b ) and RoBERTa ( Liu et al. , 2019 b ) have shown very strong performance on them .
Moreover , as shown in Winogrande , acquiring unbiased labels requires a " carefully designed crowdsourcing procedure " , which adds to the cost of data annotation .
This is also quantified in other natural language tasks such as Natural Language Inference ( Gururangan et al. , 2018 ) and Argument Reasoning Comprehension ( Niven and Kao , 2019 ) , where such annotation artifacts lead to " Clever Hans Effect " in the models ( Kaushik and Lipton , 2018 ; Poliak et al. , 2018 ) .
One way to resolve this is to design and create datasets in a clever way , such as in Winogrande , another way is to ignore the data annotations and to build systems to perform unsupervised question answering ( Teney and Hengel , 2016 ; .
In this paper , we focus on building unsupervised zero-shot multiple -choice QA systems .
Recent work ( Fabbri et al. , 2020 ; try to generate a synthetic dataset using a text corpus such as Wikipedia , to solve extractive QA .
Other works Shwartz et al. , 2020 ) use large pre-trained generative language models such as GPT - 2 ( Radford et al. , 2019 ) to generate knowledge , questions , and answers and compare against the given answer choices .
In this work , we utilize the information present in Knowledge Graphs such as ATOMIC ( Sap et al. , 2019a ) .
We define a new task of Knowledge Triplet Learning ( KTL ) over these knowledge graphs .
For tasks which do not have appropriate knowledge graphs , we propose heuristics to create synthetic knowledge graphs .
Knowledge Triplet Learning is like Knowledge Representation Learning and Knowledge Graph Completion but not limited to it .
Knowledge Representation Learning ( Lin et al. , 2018 ) learns the low-dimensional projected and distributed representations of entities and relations defined in a knowledge graph .
Knowledge Graph Completion ( Ji et al. , 2020 ) aims to identify new relations and entities to expand an incomplete input knowledge graph .
In KTL , as shown in Figure 1 , we define a triplet ( h , r , t ) , and given any two as input , we learn to generate the third .
This tri-directional reasoning forces the system to learn all the possible relations between the three inputs .
We map the question answering task to KTL , by mapping the context , question and answer to ( h , r , t ) respectively .
We define two different ways to perform self-supervised KTL .
This task can be designed as a representation generation task or a masked language modeling task .
We compare both the strategies in this work .
We show how to use models trained on this task to perform zero-shot question answering without any additional supervision .
We also show how models pre-trained on this task perform considerably well compared to strong pre-trained language models on few-shot learning .
We evaluate our approach on the three commonsense and three science multiplechoice QA datasets .
The contributions of this paper are summarized as follows : ?
We define the Knowledge Triplet Learning over Knowledge Graph and show how to use it for zero-shot question answering .
?
We compare two strategies for the above task . ?
We propose heuristics to create synthetic knowledge graphs . ?
We perform extensive experiments of our framework on three commonsense and three science question - answering datasets . ?
We achieve state - of - the - art results for zeroshot and propose a strong baseline for the fewshot question answering task .
Knowledge Triplet Learning
We define the task of Knowledge Triplet Learning ( KTL ) in this section .
We define G = ( V , E ) as a Knowledge Graph , where V is the set of vertices , E is the set of edges .
V consists of entities which can be phrases or named -entities depending on the given input Knowledge Graph .
Let S be a set of fact triples , S ? V ?E?V with the format ( h , r , t ) , where h and t belong to set of vertices V and r belongs to set of edges .
The h and t indicates the head and tail entities , whereas r indicates the relation between them .
For example , from the ATOMIC knowledge graph , ( PersonX puts PersonX 's trust in PersonY , How is PersonX seen as ? , faithful ) is one such triple .
Here the head is PersonX puts PersonX 's trust in PersonY , relation is How is PersonX seen as ?
and the tail is faithful .
Do note V does not contain homogenous entities , i.e , both faithful and PersonX puts PersonX 's trust in PersonY are in V .
We define the task of KTL as follows :
Given input a triple ( h , r , t ) , we learn the following three functions .
f t ( h , r ) ?
t , f h ( r , t ) ? h , f r ( h , t ) ? r ( 1 ) That is , each function learns to generate one component of the triple given the other two .
The intuition behind learning these three functions is as follows .
Let us take the above example : ( PersonX puts Per-sonX 's trust in PersonY , How is PersonX seen as ? , faithful ) .
The first function f t ( h , r ) learns to generate the answer t given the context and the question .
The second function f h ( r , t ) learns to generate one context where the question and the answer may be valid .
The final function f r ( h , t ) is a Jeopardystyle generating the question which connects the context and the answer .
In Multiple-choice QA , given the context , two choices may be true for two different questions .
Similarly , given the question , two answer choices may be true for two different contexts .
For example , given the context : PersonX puts PersonX 's trust in PersonY , the answers PersonX is considered trustworthy by others and PersonX is polite are true for two different questions
How does this affect others ?
and How is PersonX seen as ?.
Learning these three functions enables us to score these relations between the context , question , and answers .
Using KTL to perform QA After learning this function in a self-supervised way , we can use them to perform question answering .
Given a triple ( h , r , t ) , we define the following scoring function : Dt = D(t , ft ( h , r ) ) , D h = D( h , f h ( r , t ) ) , Dr = D ( r , fr(h , t ) ) score ( h , r , t ) = Dt * D h * Dr ( 2 ) where h is the context , r is the question and t is one of the answer options .
D is a distance function which measures the distance between the generated output and the ground-truth .
The distance function varies depending on the instantiation of the framework , which we will study in the following sections .
The final answer is selected as : ans = arg min t ( score ( h , r , t ) ) ( 3 )
As the scores are the distance from the ground - truth we select the choice that has the minimum score .
We define the different ways we can implement this framework in the following sections .
Knowledge Representation Learning
In this implementation , we use Knowledge representation learning to learn equation ( 1 ) .
In contrast to triplet classification and graph completion , where systems try to learn a score function f r ( h , t ) , i.e , is the fact triple ( h , r , t ) true or false ; in this method we learn to generate the inputs vector representations , i.e , f r ( h , t ) ? r. We can view equation 1 as generator functions , which given the two input vector encodings learns to generate a vector representation of the third .
The vector encodings can be pre-computed sentence vector representations or contextual vector representations .
As our triples ( h , r , t ) can have a many to many relations between each pair , we first project the two inputs from input vector encoding space to a different space similar to the work of TransD ( Ji et al. , 2015 ) .
We use a Transformer encoder Enc to encode our triples to the vector encoding space .
We learn two projection functions , M i1 and M i2 to project the two inputs , and a third projection function M o to project the entity to be generated .
We combine the two projected inputs using a function C .
These functions can be implemented using feedforward networks .
Ie1 = Enc ( I1 ) , Ie2 = Enc ( I2 ) , Oe = Enc ( O ) Ie1 = Mi1 ( Ie1 ) , Ie2 = Mi2 ( Ie2 ) , Op = Mo ( Oe ) ? = C ( Ie1 , Ie2 ) loss = LossF ( ? , Op ) where I i is the input , ? is the generated output vector and O p is the projected vector .
M and C functions are learned using fully connected networks .
In our implementation , we use RoBERTa as the Enc transformer , with the output representation of the [ cls ] token as the phrase representation .
We train this model using two types of loss functions , L2Loss where we try to minimize the L2 norm between the generated and the projected ground - truth , and Noise Contrastive Estimation ( Gutmann and Hyv?rinen , 2010 ) where along with the ground - truth we have k noise-samples .
These noise samples are selected from other ( h , r , t ) triples such that the target output is not another true fact triple , i.e , ( h , r , t noise ) is false .
The NCELoss is defined as : N CELoss ( ? , Op , [ N0 ... N k ] ) = ? log exp sim ( ? , Op ) exp sim ( ? , Op ) + k?N exp ( sim ( ? , N k ) where N k are the projected noise samples , sim is the similarity function which can be the L2 norm or Cosine similarity , ? is the generated output vector and O p is the projected vector .
The D distance function ( 2 ) for such a model is defined by the distance function used in the loss function .
For L2Loss , it is the L2 norm , and in the case of NCELoss , we use 1 ? sim function .
Span Masked Language Modeling In Span Masked Language Modeling ( SMLM ) , we model the equation 1 as a masked language modeling task .
We tokenize and concatenate the triple ( h , r , t ) with a separator token between them , i.e , [ cls ] [ h ] [ sep ] [ r ] [ sep ] [ t ] [ sep ] .
For the function f r ( h , t ) ?
r , we mask all the tokens present in r , i.e , [ cls ] [ h ] [ sep ] [ mask ] [ sep ] [ t ] [ sep ] .
We feed these tokens to a Transformer encoder Enc and use a feed forward network to unmask the sequence of tokens .
Similarly , we mask h to learn f h and t to learn f t We train the same Transformer encoder to perform all the three functions .
We use the crossentropy loss to train the model : CELoss ( h , r , mask ( t ) , t ) = ?
1 n n i=1 log2 PMLM ( ti|h , r , t1 ..t i ..tn ) where P M LM is the masked language modeling probability of the token t i , given the unmasked tokens h and r and other masked tokens in t.
Do note we do not do progressive unmasking , i.e , all the masked tokens are jointly predicted .
The D distance function ( 2 ) for this model is same as the loss function defined above .
Synthetic Graph Construction
This section describes our method to create a synthetic knowledge graph from a text corpus containing sentences .
Not all types of knowledge are present in a structured knowledge graph , such as ATOMIC , which might help answer questions .
For example , the questions in QASC dataset ( Khot et al. , 2019 ) require knowledge about scientific concepts , such as , " Clouds regulate the global engine of atmosphere and ocean . " .
The QASC dataset contains a textual knowledge corpus containing science facts .
Similarly , the Open Mind Commonsense ( OMCS ) knowledge corpus contains knowledge about different commonsense facts , such as , " You are likely to find a jellyfish in a book " .
Another kind of knowledge about social interactions and story progression is present in several story understanding datasets , such as RoCStories and the Story Cloze Test ( Mostafazadeh et al. , 2016 ) .
To perform question answering using this knowledge and KTL , we create the following two graphs : the Common Concept Graph and the Directed Story Graph .
Common Concept Graph
To create the Common Concept Graph , we extract noun-chunks and verb-chunks from each of the sentences using the Spacy Part-of - Speech tagger ( Honnibal and Montani , 2017 ) .
We assign all the extracted chunks as the graph 's vertices and the sentences as the graph 's edges .
To generate training samples for KTL , we assign triples ( h , R , t ) as ( e 1 , e 2 , v i ) where v i is the common concept present in both the sentences e 1 and e 2 .
For example , in the sentence Clouds regulate the global engine of atmosphere and ocean . , the extracted concepts are clouds , global engine , atmosphere , ocean and regulate .
The triplet assignment will be , [ Warm moist air from the Pacific Ocean brings fog and low stratus clouds to the maritime zone . , Clouds regulate the global engine of atmosphere and ocean . , clouds ] .
We create two such synthetic graphs using the QASC science corpus and the OMCS concept corpus .
Our hypothesis is this graph , and the KTL framework will allow the model to understand the concepts common in two facts , which allows question answering .
Directed Story Graph
This graph is created using short stories from the RoCStories and Story Cloze Test datasets .
This graph is different from the above graph as this graph has a directional property , and each story graph is disconnected .
To create this graph , we take each short story with k sentences , [ s 1 , s 2 , s 3 .. , s k ] and create a directed graph such that all sentences are vertices and each sentence is connected with a directed edge only to sentences that occur after it .
For example , s 1 is connected to s 2 with a directed edge but not vice versa .
We generate triples ( h , R , t ) by sampling vertices ( s i , s j , s k ) such that there is a directed path between the sentences s i and s k through s j .
This format captures a smaller story where the head is an event that occurs before the relation and the tail .
This graph is designed for story understanding and abductive reasoning using the KTL framework .
Random Sampling
There are around 17 M sentences in the QASC text corpus ; similarly , there are 640K sentences in the OMCS text corpus .
Our synthetic triple generation leads to a significantly large set of triples in order of 10 12 and more .
To restrict the train dataset size for our KTL framework , we randomly sample triples and limit the train dataset size to be at max 1 M samples ; we refer to this as Random Sampling .
Curriculum Filtering
Here , we extract the noun and verb chunks from the context , question , and answer options present in the question answering datasets .
We filter triples from the generated dataset and keep only those triples where at least one of the entities is present in the extracted noun and verb chunks set .
This filtering is analogous to a reallife human examination setting where a teacher provides the set of concepts upon which questions would be asked , and the students can learn the concepts .
We perform the sampling and filtering only on the huge Common Concept Graphs generated from QASC and OMCS corpus .
Datasets
We evaluate our framework on the following six datasets : SocialIQA ( Sap et al. , 2019 b ) , aNLI , CommonsenseQA ( Talmor et al. , 2018 ) , QASC ( Khot et al. , 2019 ) , Open-BookQA and ARC . SocialIQA , aNLI , and Common-senseQA require commonsense reasoning and external knowledge to answer the questions .
Similarly , QASC , OpenBookQA , and ARC require scientific knowledge .
Table 1 shows the dataset statistics and the corresponding knowledge graph used to train our KTL model .
Table 2 shows the statistics for the triples extracted from the graphs .
From the two tables we can observe our KTL triples have different number of words when compared to the target question answering tasks .
Especially where the context is significantly larger and human anno- tated as in SocialIQA , increasing the challenge for unsupervised learning .
Question to Hypothesis Conversion and Context Creation
We can observe the triples in our synthetic graphs , QASC - CCG and OMCS - CCG contain factual statements , and our target question answering datasets have questions that contain wh words or fill - in- theblanks .
We translate each question to a hypothesis using the question and each answer option .
To create hypothesis statements for questions containing wh words , we use a rule-based model ( Demszky et al. , 2018 ) .
For fill - in- the - blank and cloze style questions , we replace the blank or concat the question and the answer option .
For questions that do not have a context , such as in QASC or CommonsenseQA , we retrieve the top five sentences using the question and answer options as query and perform retrieval from respective source knowledge sentence corpus .
For each retrieved - context , we evaluate the answer option score using equation 2 and take the mean score .
Experiments
Baselines
We compare our models to the following baselines .
1 . GPT -2 Large with language modeling crossentropy loss as the scoring function .
We concatenate the context and question and find the cross-entropy loss for each answer choices and choose the answer with minimum loss .
2 . Pre-trained RoBerta-large used as is , without any fine-tuning or further pre-training , with scoring the same as our defined SMLM model .
We refer to it as Rob-MLM .
3 . RoBerta-large model further fine-tuned using the original Masked Language Modeling task over our concatenated fact triples ( h , r , t ) , with scoring same as SMLM .
We refer to it as Rob-FMLM .
4 . IR Solver described in ARC ( Clark et al. , 2016 ) , which sends the context , question , and answer option as a query to Elasticsearch .
The top retrieved sentence , which has a non-stopword overlap with both the question and the answer , is used as a representative , and its corresponding IR ranking score is used as confidence for the answer .
The option with the highest score is chosen as the answer .
KTL Training
We train the Knowledge Representation Learning ( KRL ) model using both L2Loss and NCELoss .
For NCELoss , we also train it with both L2 norm and Cosine similarity .
Both the KRL model ( 365 M ) and the SMLM model ( 358 M ) uses RoBERTa -large ( 355 M ) as the encoder .
We train the model for three epochs with the following hyper-parameters : batch sizes [ 512 , 1024 ] for SMLM and [ 32 , 64 ] for KRL ; learning rate in range : [ 1e - 5,5e - 5 ] ; warm - up steps in range [ 0,0.1 ] ; in 4 Nvidia V100s 16GB .
We use the transformers package ( Wolf et al. , 2019 ) .
All triplets from the training graphs are positive samples .
We learn using these triplets .
For NCE , we choose k equal to ten , i.e. , ten negative samples .
We perform three hyper-parameter trials using ten percent of the training data for each model , and train models with three different seeds .
We report the mean accuracy of the three random seed runs for each of our experiments and report the standard deviation if space permits .
Code is available here .
6 Results and Discussion
Unsupervised Question Answering Table 3 compares our different KTL methods with our four baselines for the six question - answering datasets on the zero-shot question answering task .
We use Hypothesis Conversion , Curriculum Filtering , and Context Creation for ARC , QASC , OBQA , and CommonsenseQA for both the baselines and our models .
We compare the models on the Train , Dev and Test split if labels are available , to capture the statistical significance better .
We can observe that our KTL trained models perform statistically significantly better than the baselines .
When comparing the different KRL models , the NCELoss with Cosine similarity performs the best .
This observation might be due to the additional supervision provided by the negative samples as the L2Loss model only tries to minimize the distance between the generated and the target projections .
When comparing different KTL instantiations , we can see that the SMLM model performs the best overall .
SMLM and KRL differ in their core approaches .
We hypothesize that multi-layered attention in a transformer encoder enables the SMLM model to distinguish between a true and false statement .
In KRL , we are learning from both positive and negative samples , but the model still under-performs .
On analysis , we observe the random negative samples may make the training task biased for KRL .
Our future work would be to utilize alternative negative sampling techniques , such as selecting samples closer in contextual vector space .
The improvements in ARC - Challenge task are considerably less .
It is observed that the fact corpus for QASC , although it contains a vast number of science facts , does not contain sufficient knowledge to answer ARC questions .
There is a substantial improvement in SocialIQA , aNLI , QASC , and Com-monsenseQA as the respective KTL knowledge corpus contains sufficient knowledge to answer the questions .
It is interesting to note that for QASC , we can reduce the problem from an eight - way to a four -way classification , as our top - 4 accuracy on QASC is above 92 % .
Our unsupervised model outperforms previous approaches , such as Self - Talk ( Shwartz et al. , 2020 ) .
It approaches prior supervised approaches like BIDAF ( Seo et al. , 2017 ) , and even surpasses it on two tasks .
Few - Shot Question Answering Table 4 compares our KTL pre-trained transformer encoder in the few-shot question answering task .
We fine- tune the encoder with a simple feedforward network for a n-way classification task , the standard question - answering approach using RoBerta with n being the number of answer options during training with only 8 % of the training data .
We train on three randomly sampled splits of training data and report the mean .
We can observe our KTL pretrained encoders perform significantly better than the baselines and approach the fully supervised model , with only 7.5 % percent behind the fully supervised model on SocialIQA .
We also observe that our pre-trained models have a lower deviation .
Ablation studies and Analysis Effect of Context , Question , Answer Distance In Table 5 , we compare the effect of the three different distance scores .
It is interesting to observe , in OpenBookQA , QASC , and CommonsenseQA , the three datasets which do not provide a context , the model is more perplexed to predict the question when given a wrong answer option , leading to higher accuracy for only Question distance score .
On the other hand , in aNLI all three distance scores have nearly equal performance .
In SocialIQA , the question has the least accuracy , whereas the model is more perplexed when predicting the context given a wrong answer option .
This observation confirms our hypothesis that given a task predicting context and question can contain more information than discriminating between options alone .
Effect of Hypothesis Conversion , Curriculum Filtering and Context Retrieval In Table 6 , we observe the effect of hypothesis conversion , curriculum filtering , and our context creation .
Converting the question to a hypothesis provides a slight Error Analysis
We sampled 50 error cases from each of our question - answering tasks .
Our KTL framework allows learning from knowledge graphs , that includes synthetic knowledge graphs .
Both our instantiation , SMLM , and KRL function as a knowledge base score generator , were given the inputs , and a target , the generator yields a score , how improbable is the target to be present in the knowledge base .
Most of our errors are when all context , question , and answer-option have a large distance score , and the model accuracy degenerates to that of a random model .
This more considerable distance indicates the model is highly perplexed to see the input text .
For aNLI and SocialIQA , we possess relevant context , and our performance is significantly better in these datasets , but for other tasks , we have another source of error , i.e. , context creation .
In several cases , the context is irrelevant and acts as a noise .
Other errors include when the questions require complex reasoning such as understanding negation , conjunctions , and disjunctions ; temporal reasoning such as " 6 am " being before " 10 am " , and multi-hop reasoning .
These complex reasoning tasks are required to answer a significant number of questions in the science and commonsense QA tasks .
We also tried to utilize a text generation model , such as GPT - 2 , to generate and compare with ground truth text using our KTL framework , but preliminary results show the model is overfitting to the synthetic dataset and leads to significantly low performance .
Other Instantiations
Our KTL framework can be implemented using other methods , such as using a Generator / Discriminator pre-training proposed in Electra , and sequence - tosequence methods .
The distance functions for sequence - to-sequence models can be similar to our SMLM model , the cross-entropy loss for the expected generated sequence .
Discriminator based methods can adapt to the negative class probabilities as the distance function .
Studying different instantiations and their implications are some of the fascinating future works .
7 Related Work 7.1 Unsupervised QA Recent work on unsupervised question answering approach the problem in two ways , a domain adaption or transfer learning problem ( Chung et al. , 2018 ) , or a data augmentation problem Dhingra et al. , 2018 ; . The work of Fabbri et al. , 2020 ; Puri et al. , 2020 ) use style transfer or template - based question , context and answer triple generation , and learn using these to perform unsupervised extractive question answering .
There is another approach to learning generative models , generating the answer given a question or clarifying explanations and questions , such as GPT - 2 ( Radford et al. , 2019 ) to perform unsupervised question answering ( Shwartz et al. , 2020 ; .
In the visual domain , zero-shot visual question answering is studied in ( Teney and Hengel , 2016 ) , and a self-supervised learning method for logical compositions of visual questions is proposed in ( Gokhale et al. , 2020 ) .
In contrast , our work focuses on learning from knowledge graphs and generate vector representations or sequences of tokens not restricted to the answer but including the context and the question using the masked language modeling objective .
Use of External Knowledge for QA
There are several approaches to add external knowledge into models to improve question answering .
Broadly they can be classified into two , learning from unstructured knowledge and structured knowledge .
In learning from unstructured knowledge , recent large pre-trained language models ( Peters et al. , 2018 ; Radford et al. , 2019 ; Devlin et al. , 2018 ; Liu et al. , 2019 b ; Clark et al. , 2020 ; Lan et al. , 2019 ; Joshi et al. , 2020 ; learn general - purpose text encoders from a huge text corpus .
On the other hand , learning from structured knowledge includes learning from structured knowledge bases ( Yang and Mitchell , 2017 ; Bauer et al. , 2018 ; Mihaylov and Frank , 2018 ; Wang and Jiang , 2019 ; by learning knowledge enriched word embeddings .
Using structured knowledge to refine pre-trained contextualized representations learned from unstructured knowledge is another approach ( Peters et al. , 2019 ; Yang et al. , 2019a ; Zhang et al. , 2019 ; Liu et al. , 2019a ) .
Another approach of using external knowledge includes retrieval of knowledge sentences from a text corpora ( Das et al. , 2019 ; Chen et al. , 2017 ; Banerjee , 2019 ) , or knowledge triples from knowledge bases ( Min et al. , 2019 ; Wang et al. , 2020 ) that are useful to answer a specific question .
Another recent approach uses language model as knowledge bases ( Petroni et al. , 2019 ) , where they query a language model to un-mask a token given an entity and a relation in a predefined template .
We use knowledge graphs to learn a self-supervised generative task to perform zero-shot multiple - choice QA in our work .
Knowledge Representation Learning
Over the years there are several methods discovered to perform the task of knowledge representation learning .
Few of them are : TransE ( Bordes et al. , 2013 ) that views relations as a translation vector between head and tail entities , TransH ( Wang et al. , 2014 ) that overcomes TransE 's inability to model complex relations , and TransD ( Ji et al. , 2015 ) that aims to reduce the parameters by proposing two different mapping matrices for head and tail .
KRL has been used in various ways to generate natural answers ( Yin et al. , 2016 ; He et al. , 2017 ) and generate factoid questions ( Serban et al. , 2016 ) .
The task of Knowledge Graph Completion ( Yao et al. , 2019 ) is to either predict unseen relations r between two existing entities : ( h , ? , t ) or predict the tail entity t given the head entity and the query relation : ( h , r , ? ) .
Whereas we are learning to predict including the head , ( ? , r , t ) .
In KTL , head and tail are not similar text phrases ( context and answer ) unlike Graph completion .
We further modify TransD and adapt it to our KTL framework to perform zero-shot QA .
Conclusion
This work proposes a new framework of Knowledge Triplet Learning over knowledge graph entities and relations .
We show learning all three possible functions , f r , f h , and f t help the model perform zero-shot multiple -choice question answering , where we do not use question - answering annotations .
We learn from both human-annotated and synthetic knowledge graphs and evaluate our framework on the six question - answering datasets .
Our framework achieves state - of- the - art in the zero-shot question answering task achieving performance like prior supervised work and sets a strong baseline in the few-shot question answering task .
Figure 1 : 1 Figure 1 : Knowledge Triplet Learning Framework , where given a triple ( h , r , t ) we learn to generate one of the inputs given the other two .
