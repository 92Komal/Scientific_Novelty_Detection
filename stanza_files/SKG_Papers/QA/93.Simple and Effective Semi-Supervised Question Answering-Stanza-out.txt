title
Simple and Effective Semi-Supervised Question Answering
abstract
Recent success of deep learning models for the task of extractive Question Answering ( QA ) is hinged on the availability of large annotated corpora .
However , large domain specific annotated corpora are limited and expensive to construct .
In this work , we envision a system where the end user specifies a set of base documents and only a few labelled examples .
Our system exploits the document structure to create cloze-style questions from these base documents ; pre-trains a powerful neural network on the cloze style questions ; and further finetunes the model on the labeled examples .
We evaluate our proposed system across three diverse datasets from different domains , and find it to be highly effective with very little labeled data .
We attain more than 50 % F1 score on SQuAD and TriviaQA with less than a thousand labelled examples .
We are also releasing a set of 3.2 M cloze-style questions for practitioners to use while building QA systems 1 .
Introduction
Deep learning systems have shown a lot of promise for extractive Question Answering ( QA ) , with performance comparable to humans when large scale data is available .
However , practitioners looking to build QA systems for specific applications may not have the resources to collect tens of thousands of questions on corpora of their choice .
At the same time , state - of - the - art machine reading systems do not lend well to low-resource QA settings where the number of labeled questionanswer pairs are limited ( c.f. Table 2 ) .
Semisupervised QA methods like aim to improve this performance by leveraging unlabeled data which is easier to collect .
In this work , we present a semi-supervised QA system which requires the end user to specify a set of base documents and only a small set of question - answer pairs over a subset of these documents .
Our proposed system consists of three stages .
First , we construct cloze-style questions ( predicting missing spans of text ) from the unlabeled corpus ; next , we use the generated clozes to pre-train a powerful neural network model for extractive QA ( Clark and Gardner , 2017 ; Dhingra et al. , 2017 ) ; and finally , we fine - tune the model on the small set of provided QA pairs .
Our cloze construction process builds on a typical writing phenomenon and document structure : an introduction precedes and summarizes the main body of the article .
Many large corpora follow such a structure , including Wikipedia , academic papers , and news articles .
We hypothesize that we can benefit from the un-annotated corpora to better answer various questions - at least ones that are lexically similar to the content in base documents and directly require factual information .
We apply the proposed system on three datasets from different domains - SQuAD ( Rajpurkar et al. , 2016 ) , TriviaQA - Web ( Joshi et al. , 2017 and the BioASQ challenge ( Tsatsaronis et al. , 2015 ) .
We observe significant improvements in a low-resource setting across all three datasets .
For SQuAD and TriviaQA , we attain an F1 score of more than 50 % by merely using 1 % of the training data .
Our system outperforms the approaches for semi-supervised QA presented in , and a baseline which uses the same unlabeled data but with a language modeling objective for pretraining .
In the BioASQ challenge , we outperform the best performing system from previous year 's challenge , improving over a baseline which does transfer learning from the SQuAD dataset .
Our analysis reveals that questions which ask for factual information and match to specific parts of the context documents benefit the most from pretraining on automatically constructed clozes .
Related Work Semi-supervised learning augments the labeled dataset L with a potentially larger unlabeled dataset U . presented a model , GDAN , which trained an auxiliary neural network to generate questions from passages by reinforcement learning , and augment the labeled dataset with the generated questions to train the QA model .
Here we use a much simpler heuristic to generate the auxiliary questions , which also turns out to be more effective as we show superior performance compared to GDAN .
Several approaches have been suggested for generating natural questions ( Tang et al. , 2017 ; Subramanian et al. , 2017 ; Song et al. , 2017 ) , however none of them show a significant improvement of using the generated questions in a semi-supervised setting .
Recent papers also use unlabeled data for QA by training large language models and extracting contextual word vectors from them to input to the QA model ( Salant and Berant , 2017 ; Peters et al. , 2018 ; McCann et al. , 2017 ) .
The applicability of this method in the low-resource setting is unclear as the extra inputs increase the number of parameters in the QA model , however , our pretraining can be easily applied to these models as well .
Domain adaptation ( and Transfer learning ) leverage existing large scale datasets from a source domain ( or task ) to improve performance on a target domain ( or task ) .
For deep learning and QA , a common approach is to pretrain on the source dataset and then fine-tune on the target dataset ( Chung et al. , 2017 ; Golub et al. , 2017 ) .
Wiese et al. ( 2017 ) used SQuAD as a source for the target BioASQ dataset , and used Book Test as source for the target SQuAD dataset .
Mihaylov et al. ( 2017 ) transfer learned model layers from the tasks of sequence labeling , text classification and relation classification to show small improvements on SQuAD .
All these works use manually curated source datatset , which in themselves are expensive to collect .
Instead , we show that it is possible to automatically construct the source dataset from the same domain as the target , which turns out to be more beneficial in terms of performance as well ( c.f. Section 4 ) .
Several cloze datasets have been proposed in the literature which use heuristics for construction ( Hermann et al. , 2015 ; Onishi et al. , 2016 ; Hill et al. , 2016 ) .
We further see the usability of such a dataset in a semi-supervised setting .
Methodology
Our system comprises of following three steps : Cloze generation :
Most of the documents typically follow a template , they begin with an introduction that provides an overview and a brief summary for what is to follow .
We assume such a structure while constructing our cloze style questions .
When there is no clear demarcation , we treat the first K % ( hyperparameter , in our case 20 % ) of the document as the introduction .
While noisy , this heuristic generates a large number of clozes given any corpus , which we found to be beneficial for semi-supervised learning despite the noise .
We use a standard NLP pipeline based on Stanford CoreNLP 2 ( for SQuAD , TrivaQA and PubMed ) and the BANNER Named Entity Recognizer 3 ( only for PubMed articles ) to identify entities and phrases .
Assume that a document comprises of introduction sentences {q 1 , q 2 , ...q n } , and the remaining passages {p 1 , p 2 , ..p m }.
Additionally , let 's say that each sentence q i in introduction is composed of words {w 1 , w 2 , ...w lq i } , where l q i is the length of q i .
We consider a match ( q i , p j ) , if there is an exact string match of a sequence of words { w k , w k+1 , ..w lq i } between the sentence q i and passage p j .
If this sequence is either a noun phrase , verb phrase , adjective phrase or a named entity in p j , as recognized by CoreNLP or BAN - NER , we select it as an answer span A. Additionally , we use p j as the passage P and form a cloze question Q from the answer bearing sentence q i by replacing A with a placeholder .
As a result , we obtain passage - question - answer ( P , Q , A ) triples ( Table 1 shows an example ) .
As a post-processing step , we prune out ( P , Q , A ) triples where the word overlap between the question ( Q ) and passage ( P ) is less than 2 words ( after excluding the stop words ) .
Passage ( P ) : Autism is a neurodevelopmental disorder characterized by impaired social interaction , verbal and non-verbal communication , and ... Question ( Q ) : People with autism tend to be a little aloof with little to no .
Answer ( A ) : social interaction Table 1 : An example constructed cloze .
The process relies on the fact that answer candidates from the introduction are likely to be discussed in detail in the remainder of the article .
In effect , the cloze question from the introduction and the matching paragraph in the body forms a question and context passage pair .
We create two cloze datasets , one each from Wikipedia corpus ( for SQuAD and TriviaQA ) and PUBMed academic papers ( for the BioASQ challenge ) , consisting of 2.2 M and 1 M clozes respectively .
From analyzing the cloze data manually , we were able to answer 76 % times for the Wikipedia set and 80 % times for the PUBMed set using the information in the passage .
In most cases the cloze paraphrased the information in the passage , which we hypothesized to be a useful signal for the downstream QA task .
We also investigate the utility of forming subsets of the large cloze corpus , where we select the top passage - question - answer triples , based on the different criteria , like i ) jaccard similarity of answer bearing sentence in introduction and the passage ii ) the tf- idf scores of answer candidates and iii ) the length of answer candidates .
However , we empirically find that we were better off using the entire set rather than these subsets .
Pre-training :
We make use of the generated cloze dataset to pre-train an expressive neural network designed for the task of reading comprehension .
We work with two publicly available neural network models - the GA Reader ( Dhingra et al. , 2017 ) ( to enable comparison with prior work ) and BiDAF + Self-Attention ( SA ) model from Clark and Gardner ( 2017 ) ( which is among the best performing models on SQuAD and TriviaQA ) .
After pretraining , the performance of BiDAF + SA on a dev set of the ( Wikipedia ) cloze questions is 0.58 F1 score and 0.55 Exact Match ( EM ) score .
This implies that the cloze corpus is neither too easy , nor too difficult to answer .
Fine Tuning :
We fine tune the pre-trained model , from the previous step , over a small set of labelled question - answer pairs .
As we shall later see , this step is crucial , and it only requires a handful of labelled questions to achieve a significant proportion of the performance typically attained by training on tens of thousands of questions .
Experiments & Results
Datasets
We apply our system to three datasets from different domains .
SQuAD ( Rajpurkar et al. , 2016 ) consists of questions whose answers are free form spans of text from passages in Wikipedia articles .
We follow the same setting as in , and split 10 % of training questions as the test set , and report performance when training on subsets of the remaining data ranging from 1 % to 90 % of the full set .
We also report the performance on the dev set when trained on the full training set ( 1 * in Table 2 ) .
We use the same hyperparameter settings as in prior work .
We compare and study four different settings : 1 ) the Supervised Learning ( SL ) setting , which is only trained on the supervised data , 2 ) the best performing GDAN model from , 3 ) pretraining on a Language Modeling ( LM ) objective and finetuning on the supervised data , and 4 ) pretraining on the Cloze dataset and fine-tuning on the supervised data .
The LM and Cloze methods use exactly the same data for pretraining , but differ in the loss functions used .
We report F1 and EM scores on our test set using the official evaluation scripts provided by the authors of the dataset .
TriviaQA ( Joshi et al. , 2017 ) comprises of over 95 K web question - answer - evidence triples .
Like SQuAD , the answers are spans of text .
Similar to the setting in SQuAD , we create multiple smaller subsets of the entire set .
For our semi-supervised QA system , we use the BiDAF +SA model ( Clark and Gardner , 2017 ) - the highest performing publicly available system for TrivaQA .
Here again , we compare the supervised learning ( SL ) settings against the pretraining on Cloze set and fine tuning on the supervised set .
We report F1 and EM scores on the dev set 4 .
We also test on the BioASQ 5 b dataset , which consists of question - answer pairs from PubMed abstracts .
We use the publicly available system 5 from Wiese et al . ( 2017 ) , and follow the exact same setup as theirs , focusing only on factoid and list questions .
For this setting , there are only 899 questions for training .
Since this is already a lowresource problem we only report results using 5 fold cross-validation on all the available data .
We report Mean Reciprocal Rank ( MRR ) on the factoid questions , and F1 score for the list questions .
Main Results Table 2 ( Wiese et al. , 2017 ) .
Cloze pretraining outperforms the GDAN baseline from using the same SQuAD dataset splits .
Additionally , we show improvements in the 90 % data case unlike GDAN .
Our approach is also applicable in the extremely low-resource setting of 1 % data , which we suspect GDAN might have trouble with since it uses the labeled data to do reinforcement learning .
Furthermore , we are able to use the same cloze dataset to improve performance on both SQuAD and Triv-iaQA datasets .
When we use the same unlabeled data to pre-train with a language modeling objective , the performance is worse 6 , showing the bias we introduce by constructing clozes is important .
similar phrasing to the answer sentences in context .
Hence , test questions with a similar property are answered correctly after pretraining , whereas those with a high overlap with the whole passage tend to have lower performance .
The pretraining also favors questions with short answers because the cloze construction process produces short answer spans .
Also passages and questions which consist of tokens infrequent in the SQuAD training corpus receive a large boost after pretraining , since the unlabeled data covers a larger domain .
A L A L P A L S P A R C A R S A S L D L FA L O Q P L O Q S L S Q P L S Q S P R C P R S Q L Q R C Q R S ?0 . Performance on question types :
Figure 2 shows the average gain in F1 score for different types of questions , when we pretrain on the clozes compared to the supervised case .
This analysis is done on the 10 % split of the SQuAD training set .
We consider two classifications of each question - one determined on the first word ( usually a wh-word ) of the question ( Figure 2 ( bottom ) ) and one based on the output of a separate question type classifier 7 adapted from ( Li and Roth , 7 https://github.com/brmson/question-classification 2002 ) .
We use the coarse grain labels namely Abbreviation ( ABBR ) , Entity ( ENTY ) , Description ( DESC ) , Human ( HUM ) , Location ( LOC ) , Numeric ( NUM ) trained on a Logistic Regression classification system .
While there is an improvement across the board , we find that abbreviation questions in particular receive a large boost .
Also , " why " questions show the least improvement , which is in line with our expectation , since these usually require reasoning or world knowledge which cloze questions rarely require .
Conclusion
In this paper , we show that pre-training QA models with automatically constructed cloze questions improves the performance of the models significantly , especially when there are few labeled examples .
The performance of the model trained only on the cloze questions is poor , validating the need for fine-tuning .
Through regression analysis , we find that pretraining helps with questions which ask for factual information located in a specific part of the context .
For future work , we plan to explore the active learning setup for this taskspecifically , which passages and / or types of questions can we select to annotate , such that there is a maximum performance gain from fine-tuning .
We also want to explore how to adapt cloze style pretraining to NLP tasks other than QA .
Figure 2 : 2 Figure 2 : Performance gain with pretraining for different subsets of question types .
