title
Incorporating External Knowledge into Machine Reading for Generative Question Answering
abstract
Commonsense and background knowledge is required for a QA model to answer many nontrivial questions .
Different from existing work on knowledge - aware QA , we focus on a more challenging task of leveraging external knowledge to generate answers in natural language for a given question with context .
In this paper , we propose a new neural model , Knowledge -Enriched Answer Generator ( KEAG ) , which is able to compose a natural answer by exploiting and aggregating evidence from all four information sources available : question , passage , vocabulary and knowledge .
During the process of answer generation , KEAG adaptively determines when to utilize symbolic knowledge and which fact from the knowledge is useful .
This allows the model to exploit external knowledge that is not explicitly stated in the given text , but that is relevant for generating an answer .
The empirical study on public benchmark of answer generation demonstrates that KEAG improves answer quality over models without knowledge and existing knowledge - aware models , confirming its effectiveness in leveraging knowledge .
Introduction Question Answering ( QA ) has come a long way from answer sentence selection , relational QA to machine reading comprehension .
The nextgeneration QA systems can be envisioned as the ones which can read passages and write long and abstractive answers to questions .
Different from extractive question answering , generative QA based on machine reading produces an answer in true natural language which does not have to be a sub-span in the given passage .
Most existing models , however , answer questions based on the content of given passages as the only information source .
As a result , they may not be able to understand certain passages or to answer certain questions , due to the lack of commonsense and background knowledge , such as the knowledge about what concepts are expressed by the words being read ( lexical knowledge ) , and what relations hold between these concepts ( relational knowledge ) .
As a simple illustration , given the passage : State officials in Hawaii on Monday said they have once again checked and confirmed that President Barack Obama was born in Hawaii .
to answer the question : Was Barack Obama born in the U.S. ? , one must know ( among other things ) that Hawaii is a state in the U.S. , which is external knowledge not present in the text corpus .
Therefore , a QA model needs to be enriched with external knowledge properly to be able to answer many nontrivial questions .
Such knowledge can be commonsense knowledge or factual background knowledge about entities and events that is not explicitly expressed but can be found in a knowledge base such as ConceptNet ( Speer et al. , 2016 ) , Freebase ( Pellissier Tanon et al. , 2016 ) and domain-specific KBs collected by information extraction ( Fader et al. , 2011 ; Mausam et al. , 2012 ) .
Thus , we aim to design a neural model that encodes pre-selected knowledge relevant to given questions , and that learns to include the available knowledge as an enrichment to given textual information .
In this paper , we propose a new neural architecture , Knowledge -Enriched Answer Generator ( KEAG ) , specifically designed to generate natural answers with integration of external knowledge .
KEAG is capable of leveraging symbolic knowledge from a knowledge base as it generates each word in an answer .
In particular , we assume that each word is generated from one of the four information sources : 1 . question , 2 . passage , 3 . vocabulary and 4 . knowledge .
Thus , we introduce the source selector , a sentinel component in KEAG that allows flexibility in deciding which source to look to generate every answer word .
This is crucial , since knowledge plays a role in certain parts of an answer , while in others text context should override the context- independent knowledge available in general KBs .
At each timestep , before generating an answer word , KEAG determines an information source .
If the knowledge source is selected , the model extracts a set of facts that are potentially related to the given question and context .
A stochastic fact selector with discrete latent variables then picks a fact based on its semantic relevance to the answer being generated .
This enables KEAG to bring external knowledge into answer generation , and to generate words not present in the predefined vocabulary .
By incorporating knowledge explicitly , KEAG can also provide evidence about the external knowledge used in the process of answer generation .
We introduce a new differentiable samplingbased method to learn the KEAG model in the presence of discrete latent variables .
For empirical evaluation , we conduct experiments on the benchmark dataset of answer generation MARCO ( Nguyen et al. , 2016 ) .
The experimental results demonstrate that KEAG effectively leverages external knowledge from knowledge bases in generating natural answers .
It achieves significant improvement over classic QA models that disregard knowledge , resulting in higher -quality answers .
Related Work
There have been several attempts at using machine reading to generate natural answers in the QA field .
Tan et al. ( 2018 ) took a generative approach where they added a decoder on top of their extractive model to leverage the extracted evidence for answer synthesis .
However , this model still relies heavily on the extraction to perform the generation and thus needs to have start and end labels ( a span ) for every QA pair .
Mitra ( 2017 ) proposed a seq2seq - based model that learns alignment between a question and passage words to produce rich question - aware passage representation by which it directly decodes an answer .
Gao et al. ( 2019 ) focused on product - aware answer generation based on large-scale unlabeled e-commerce reviews and product attributes .
Furthermore , natural answer generation can be refor-mulated as query -focused summarization which is addressed by Nema et al . ( 2017 ) .
The role of knowledge in certain types of QA tasks has been remarked on .
Mihaylov and Frank ( 2018 ) showed improvements on a cloze-style task by incorporating commonsense knowledge via a context-to- commonsense attention .
Zhong et al. ( 2018 ) proposed commonsense - based pre-training to improve answer selection .
Long et al. ( 2017 ) made use of knowledge in the form of entity descriptions to predict missing entities in a given document .
There have also been a few studies on incorporating knowledge into QA models without passage reading .
GenQA
( Yin et al. , 2016 ) combines knowledge retrieval and seq2seq learning to produce fluent answers , but it only deals with simple questions containing one single fact .
COREQA ( He et al. , 2017 ) extends it with a copy mechanism to learn to copy words from a given question .
Moreover , Fu and Feng ( 2018 ) introduced a new attention mechanism that attends across the generated history and memory to explicitly avoid repetition , and incorporated knowledge to enrich generated answers .
Some work on knowledge - enhanced natural language ( NLU ) understanding can be adapted to the question answering task .
CRWE ( Weissenborn , 2017 ) dynamically integrates background knowledge in a NLU model in the form of free-text statements , and yields refined word representations to a task-specific NLU architecture that reprocesses the task inputs with these representations .
In contrast , KBLSTM ( Yang and Mitchell , 2017 ) leverages continuous representations of knowledge bases to enhance the learning of recurrent neural networks for machine reading .
Furthermore , Bauer et al. ( 2018 ) proposed MHPGM , a QA architecture that fills in the gaps of inference with commonsense knowledge .
The model , however , does not allow an answer word to come directly from knowledge .
We adapt these knowledge- enhanced NLU architectures to answer generation , as baselines for our experiments .
Knowledge-aware Answer Generation Knowledge-aware answer generation is a question answering paradigm , where a QA model is expected to generate an abstractive answer to a given question by leveraging both the contextual passage and external knowledge .
More formally , given a knowledge base K and two sequences of input words : question q = {w q 1 , w q 2 , . . . , w q Nq } and passage p = {w p 1 , w p 2 , . . . , w p
Np } , the answer generation model should produce a series of answer words r = { w r 1 , w r 2 , . . . , w r Nr }.
The knowledge base K contains a set of facts , each of which is represented as a triple f = ( subject , relation , object ) where subject and object can be multi-word expressions and relation is a relation type , e.g. , ( bridge , U sedF or , cross water ) .
Knowledge-Enriched Answer Generator
To address the answer generation problem , we propose a novel KEAG model which is able to compose a natural answer by recurrently selecting words at the decoding stage .
Each of the words comes from one of the four sources : question q , passage p , global vocabulary V , and knowledge K .
In particular , at every generation step , KEAG first determines which of the four sources to inspect based on the current state , and then generates a new word from the chosen source to make up a final answer .
An overview of the neural architecture of KEAG is depicted in Figure 1 .
Sequence-to-sequence model KEAG is built upon an extension of the sequenceto-sequence attentional model ( Bahdanau et al. , 2015 ; Nallapati et al. , 2016 ; See et al. , 2017 ) .
The words of question q and passage p are fed one - byone into two different encoders , respectively .
Each of the two encoders , which are both bidirectional LSTMs , produces a sequence of encoder hidden states ( E q for question q , and E p for passage p ) .
In each timestep t , the decoder , which is a unidirectional LSTM , takes an answer word as input , and outputs a decoder hidden state s r t .
We calculate attention distributions a q t and a p t on the question and the passage , respectively , as in ( Bahdanau et al. , 2015 ) : a q t =softmax ( g q tanh ( W q E q + U q s r t + b q ) ) , ( 1 ) a p t = softmax ( g p tanh ( W p E p + U p s r t + V p c q + b p ) ) , ( 2 ) where g q , W q , U q , b q , g p , W p , U p and b p are learnable parameters .
The attention distributions can be viewed as probability distributions over source words , which tells the decoder where to look to generate the next word .
The coverage mechanism is added to the attentions to avoid generating repetitive text ( See et al. , 2017 ) .
In Equation 2 , we introduce c q , a context vector for the question , to make the passage attention aware of the question context .
c q for the question and c p for the passage are calculated as follows : c q t = i a q ti ?
e q i , c p t = i a p ti ?
e p i , ( 3 ) where e q i and e p i are an encoder hidden state for question q and passage p , respectively .
The context vectors ( c q t and c p t ) together with the attention distributions ( a q t and a p t ) and the decoder state 2524 ( s r t ) will be used downstream to determine the next word in composing a final answer .
Source Selector During the process of answer generation , in each timestep , KEAG starts with running a source selector to pick a word from one source of the question , the passage , the vocabulary and the knowledge .
The right plate in Figure 1 illustrates how the source selector works in one timestep during decoding .
If the question source is selected in timestep t , KEAG picks a word according to the attention distribution a q t ?
R Nq over question words ( Equation 1 ) , where N q denotes the number of distinct words in the question .
Similarly , when the passage source is selected , the model picks a word from the attention distribution a p t ?
R Np over passage words ( Equation 2 ) , where N p denotes the number of distinct words in the passage .
If the vocabulary is the source selected in timestep t , the new word comes from the conditional vocabulary distribution P v ( w| c q t , c p t , s r t ) over all words in the vocabulary , which is obtained by : P v ( w|c q t , c p t , s r t ) = softmax ( W v ? [ c
q t , c p t , s r t ] + b v ) , ( 4 ) where c q t and c p t are context vectors , and s r t is a decoder state .
W v and b v are learnable parameters .
To determine which of the four sources a new word w t+1 is selected from , we introduce a discrete latent variable y t ? { 1 , 2 , 3 , 4 } as an indicator .
When y t = 1 or 2 , the word w t+1 is generated from the distribution P ( w t+1 |y t ) given by : P ( w t+1 |y t ) = i:w i =w t+1 a q ti y t = 1 i:w i =w t+1 a p ti y t = 2 . ( 5 ) If y t = 3 , KEAG picks word w t+1 according to the vocabulary distribution P v ( w| c q t , c p t , s r t ) given in Equation 4 .
Otherwise , if y t = 4 , the word w t+1 comes from the fact selector , which will be described in the coming section .
Knowledge Integration
In order for KEAG to integrate external knowledge , we first extract related facts from the knowledge base in response to a given question , from which we then pick the most relevant fact that can be used for answer composition .
In this section , we present the two modules for knowledge integration : related fact extraction and fact selection .
Related Fact Extraction
Due to the size of a knowledge base and the large amount of unnecessary information , we need an effective way of extracting a set of candidate facts which provide novel information while being related to a given question and passage .
For each instance ( q , p ) , we first extract facts with the subject or object that occurs in question q or passage p.
Scores are added to each extracted fact according to the following rules : ? Score + 4 , if the subject occurs in q , and the object occurs in p. ? Score + 2 , if the subject and the object both occur in p. ? Score +1 , if the subject occurs in q or p .
The scoring rules are set heuristically such that they model relative fact importance in different interactions .
Next , we sort the fact triples in descending order of their scores , and take the top N f facts from the sorted list as the related facts for subsequent processing .
Fact Selection F = {f 1 , f 2 , . . . , f N f } is considered to be a short-term memory of the knowledge base while answering questions on given passages .
To enrich KEAG with the facts collected from the knowledge base , we propose to complete an answer with the most relevant fact ( s ) whenever it is determined to resort to knowledge during the process of answer generation .
The most relevant fact is selected from the related fact set F based on the dynamic generation state .
In this model , we introduce a discrete latent random variable z t ? P ( z t | ? ) = 1 Z ?exp( g f tanh ( W f f zt + U f s r t + b f ) ) , ( 7 ) where Z is the normalization term , Z = N f i=1 exp ( g f tanh ( W f f i + U f s r t + b f ) ) , and s r t is the hidden state from the decoder in timestep t. g f , W f , U f and b f are learnable parameters .
The presence of discrete latent variables z , however , presents a challenge to training the neural KEAG model , since the backpropagation algorithm , while enabling efficient computation of parameter gradients , does not apply to the nondifferentiable layer introduced by the discrete variables .
In particular , gradients cannot propagate through discrete samples from the categorical distribution P ( z t | F , s r t ) .
To address this problem , we create a differentiable estimator for discrete random variables with the Gumbel - Softmax trick ( Jang et al. , 2017 ) .
Specifically , we first compute the discrete distribution P ( z t | F , s r t ) with class probabilities ?
1 , ? 2 , . . . , ?
N f by Equation 7 . The Gumbel - Max trick ( Gumbel , 1954 ) allows to draw samples from the categorical distribution P ( z t | F , s r t ) by calculating one hot( arg max i [ g i + log ? i ] ) , where g 1 , g 2 , . . . , g N f are i.i.d. samples drawn from the Gumbel ( 0 , 1 ) distribution .
For the inference of a discrete variable z t , we approximate the Gumbel - Max trick by the continuous softmax function ( in place of arg max ) with temperature ? to generate a sample vector ?t : ? ti = exp ( ( log ( ? i ) + g i ) / ? ) N f j=1 exp ( ( log ( ? j ) + g j ) / ? ) . ( 8 ) When ?
approaches zero , the generated sample ?t becomes a one-hot vector .
? is gradually annealed over the course of training .
This new differentiable estimator allows us to backpropagate through z t ?
P ( z t | F , s r t ) for gradient estimation of every single sample .
The value of z t indicates a fact selected by the decoder in timestep t.
When the next word is determined to come from knowledge , the model appends the object of the selected fact to the end of the answer being generated .
Learning Model Parameters
To learn the parameters ? in KEAG with latent source indicators y , we maximize the loglikelihood of words in all answers .
For each answer , the log-likelihood of the words is given by : log P ( w r 1 , w where the word likelihood at each timestep is obtained by marginalizing out the latent source variable y t .
Unfortunately , direct optimization of Equation 9 is intractable , so we instead learn the objective function through optimizing its variational lower bound given in Equations 10 and 11 , obtained from Jensen 's inequality .
To estimate the expectation in Equation 11 , we use Monte Carlo sampling on the source selector variables y in the gradient computation .
In particular , the Gumbel - Softmax trick is applied to generate discrete samples ? from the probability P ( y t |c q t , c p t , s r t , x r t ) given by : P ( y t | ? ) = softmax ( W y ? [ c q t , c p t , s r t , x r t ] + b y ) , ( 12 ) where x r t is the embedding of the answer word in timestep t , W y and b y are learnable parameters .
The generated samples are fed to log P ( w t+1 |y t ) to estimate the expectation .
Experiments
We perform quantitative and qualitative analysis of KEAG through experiments .
In our experi-ments , we also study the impact of the integrated knowledge and the ablations of the KEAG model .
In addition , we illustrate how natural answers are generated by KEAG with the aid of external knowledge by analyzing a running example .
Dataset and Evaluation Metrics Given our objective of generating natural answers by document reading , the MARCO dataset ( Nguyen et al. , 2016 ) released by Microsoft is the best fit for benchmarking KEAG and other answer generation methods .
We use the latest MARCO V2.1 dataset and focus on the " Q&A + Natural Language Generation " task in the evaluation , the goal of which is to provide the best answer available in natural language that could be used by a smart device / digital assistant .
In the MARCO dataset , the questions are user queries issued to the Bing search engine and the contextual passages are from real web documents .
The data has been split into a training set ( 153,725 QA pairs ) , a dev set ( 12,467 QA pairs ) and a test set ( 101,092 questions with unpublished answers ) .
Since true answers are not available in the test set , we hold out the dev set for evaluation in our experiments , and test models for each question on its associated passages by concatenating them all together .
We tune the hyper-parameters by crossvalidation on the training set .
The answers are human-generated and not necessarily sub-spans of the passages , so the official evaluation tool of MARCO uses the metrics BLEU - 1 ( Papineni et al. , 2002 ) and ROUGE -L ( Lin , 2004 ) .
We use both metrics for our evaluation to measure the quality of generated answers against the ground truth .
For external knowledge , we use Concept - Net ( Speer et al. , 2016 ) , one of the most widely used commonsense knowledge bases .
Our KEAG is generic and thus can also be applied to other knowledge bases .
ConceptNet is a semantic network representing words and phrases as well as the commonsense relationships between them .
After filtering out non-English entities and relation types with few facts , we have 2,823,089 fact triples and 32 relation types for the model to consume .
Implementation Details
In KEAG , we use 300 - dimensional pre-trained Glove word embeddings ( Pennington et al. , LSTM .
The fact representation f has 500 dimensions .
The maximum number of related facts N f is set to be 1000 .
We use a vocabulary of 50 K words ( filtered by frequency ) .
Note that the source selector enables KEAG to handle out - of- vocabulary words by generating a word from given text or knowledge .
At both training and test stages , we truncate a passage to 800 words , and limit the length of an answer to 120 words .
We train on a single Tesla M40 GPU with the batch size of 16 .
At test time , answers are generated using beam search with the beam size of 4 .
Model Comparisons
Table 1 compares KEAG with the following stateof - the- art extractive / generative QA models , which do not make use of external knowledge : 1 . BiDAF
( Seo et al. , 2017 ) :
A multi-stage hierarchical process that represents the context at different levels of granularity , and using the bi-directional attention flow mechanism for answer extraction 2 .
BiDAF + Seq2Seq : A BiDAF model followed by an additional sequence - to-sequence model for answer generation 3 . S- Net ( Tan et al. , 2018 ) :
An extraction - thensynthesis framework to synthesize answers from extracted evidences 4 . S- Net+ Seq2Seq : An S- Net model followed by an additional sequence - to-sequence model for answer generation 5 . QFS ( Nema et al. , 2017 ) passages to verify each other based on their content representations 7 . gQA ( Mitra , 2017 ) :
A generative approach to question answering by incorporating the copying mechanism and the coverage vector Table 1 shows the comparison of QA models in Rouge -L and Bleu -1 .
From the table we observe that abstractive QA models ( e.g. , KEAG ) are consistently superior to extractive models ( e.g. , BiDAF ) in answer quality .
Therefore , abstractive QA models establish a strong base architecture to be enhanced with external knowledge , which motivates this work .
Among the abstractive models , gQA can be viewed as a simplification of KEAG , which generates answer words from passages and the vocabulary without the use of knowledge .
In addition , KEAG incorporates a stochastic source selector while gQA does not .
The result that KEAG significantly outperforms gQA demonstrates the effectiveness of KEAG 's architecture and the benefit of knowledge integration .
Table 2 shows the metrics of KEAG in comparison to those of the following state - of - the - art QA models that are adapted to leveraging knowledge : 1 . gQA w/ KBLSTM ( Yang and Mitchell , 2017 ) : KBLSTM is a neural model that leverages continuous representations of knowledge bases to enhance the learning of recurrent neural networks for machine reading .
We plug it into gQA to make use of external knowledge for natural answer generation .
2 . gQA w/ CRWE ( Weissenborn , 2017 ) : CRWE is a reading architecture with dynamic integration of background knowledge based on contextual refinement of word embeddings by leveraging supplementary knowledge .
We extend gQA with the refined word embedding for this model .
3 . MHPGM ( Bauer et al. , 2018 ) From Table 2 , it can be clearly observed that KEAG performs best with the highest Rouge -L and Bleu - 1 scores among the knowledge-enriched answer generation models .
The major difference between KEAG and the other models is the way of incorporating external knowledge into a model .
gQA w/ KBLSTM and gQA w/ CRWE extend gQA with the module that consumes knowledge , and MHPGM incorporates knowledge with selectively - gated attention while its decoder does not leverage words from knowledge in answer generation .
Different from these models , KEAG utilizes two stochastic selectors to determine when to leverage knowledge and which fact to use .
It brings additional gains in exploiting external knowledge to generate abstractive answers .
Since neither Rouge -L nor Bleu - 1 can measure the quality of generated answers in terms of their correctness and accuracy , we also conduct human evaluation on Amazon Mechanical Turk .
The evaluation assesses the answer quality on grammaticality and correctness .
We randomly select 100 questions from the dev set , and ask turkers for ratings in a Likert scale ( ? [ 1 , 5 ] ) on the generated answers .
Table 3 reports the human evaluation scores of KEAG and state - of - the - art answer generation models .
The KEAG model surpasses all the others in generating correct answers syntactically and substantively .
In terms of syntactic correctness , KEAG and MHPGM both perform well thanks to their architectures of composing answer text and integrating knowledge .
On the other hand , KEAG significantly outperforms all compared models in generating substantively correct answers , which demonstrates its power in exploiting external knowledge .
Ablation Studies
We conduct ablation studies to assess the individual contribution of every component in KEAG .
Table 4 reports the performance of the full KEAG model and its ablations .
We evaluate how much incorporating external knowledge as supplementary information contributes to natural answer generation by removing the supplementary knowledge and the corresponding fact selection module from KEAG 's architecture .
It can be seen that the knowledge component plays an important role in generating high-quality answers , with a drop to 49.98 on Rouge - L after the supplementary knowledge is removed .
To study the effect of our learning method , we further ablate the latent indicators y , which leads to degradation to gQA except that the new model can select answer words from the question source while gQA cannot .
Our learning method proves to be effective with a drop of about 5 % on Rouge -L and about 6 % on Bleu - 1 after ablation .
Finally , for ablating the source selector , we have a new model that generates answer words from the vocabulary alone .
It results in a significant drop to 38.33 on Rouge -L , confirming its effectiveness in generating natural answers .
Visualization and Interpretation
The source selector allows us to visualize how every word in an answer is generated from one of the sources of the question , passage , vocabulary and knowledge , which gives us insights about how KEAG works .
Table 5 visualizes a sample QA pair from KEAG and which source every word in the answer is selected from ( indicated by the sample value of the source selector variable y t ) .
As exemplified in the table , the source distribution P ( y t | ? ) varies over decoding timesteps .
To answer the question , at each timestep , KEAG first selects a source based on the sample from P ( y t | ? ) , followed by generating an answer word from the selected source .
It is observed that the in the generated answer the keyword personality comes from the knowledge source which relates psychopathy to personality .
The answer word psychopathy is selected from the question source , which leads to a well - formed answer with a complete sen-Question What 's psychopathy ?
Answer with source probabilities
Question src Psychopathy is a personality disorder .
Passage src Psychopathy is a personality disorder .
Vocabulary src Psychopathy is a personality disorder .
Knowledge src Psychopathy is a personality disorder .
Answer colored by source Psychopathy is a personality disorder .
Table 5 : Visualization of a sample QA pair and the source of individual words in the answer .
The Answer with source probabilities section displays a heatmap on answer words selected from the question , passage , vocabulary and knowledge , respectively .
A slot with a higher source probability is highlighted in darker cyan .
The Answer colored by source section shows the answer in which every word is colored based on the source it was actually selected from .
Words in blue come from the question , red from the passage , green from the vocabulary , and orange from the knowledge .
The visualization is best viewed in color .
tence .
Another keyword disorder , on the other hand , comes from the passage source .
This results from reading comprehension of the model on the passage .
To generate a final answer in good form , KEAG picks the filler words is and a as well as the period " . " from the vocabulary source .
It makes the generated answer semantically correct and comprehensive .
Conclusion and Future Work
This paper presents a new neural model KEAG that is designed to bring symbolic knowledge from a knowledge base into abstractive answer generation .
This architecture employs the source selector that allows for learning an appropriate tradeoff for blending external knowledge with information from textual context .
The related fact extraction and stochastic fact selection modules are introduced to complete an answer with relevant facts .
This work opens up for deeper investigation of answer generation models in a targeted way , allowing us to investigate what knowledge sources are required for different domains .
In future work , we will explore even tighter integration of symbolic knowledge and stronger reasoning methods .
Figure 2 2 Figure 2 displays how a fact is selected from the set of related facts for answer completion .
With the extracted knowledge , we first embed every related fact f by concatenating the embeddings of the subject e s , the relation e r and the object e o .
The embeddings of subjects and objects are initialized with pre-trained GloVe vectors ( and average pooling for multiple words ) , when the words are present in the vocabulary .
The fact embedding is followed by a linear transformation to relate subject e s to object e o with relation e r : f = W e ? [ e s , e r , e o ] + b e . ( 6 ) where f denotes fact representation , [ ? , ?] denotes vector concatenation , and W e and b e are learnable parameters .
The set of all related fact representations
F = {f 1 , f 2 , . . . , f N f } is consideredto be a short-term memory of the knowledge base while answering questions on given passages .
To enrich KEAG with the facts collected from the knowledge base , we propose to complete an answer with the most relevant fact ( s ) whenever it is determined to resort to knowledge during the process of answer generation .
The most relevant fact is selected from the related fact set F based on the dynamic generation state .
In this model , we introduce a discrete latent random variable z t ?
