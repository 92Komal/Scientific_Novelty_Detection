title
QUOREF : A Reading Comprehension Dataset with Questions Requiring Coreferential Reasoning
abstract
Machine comprehension of texts longer than a single sentence often requires coreference resolution .
However , most current reading comprehension benchmarks do not contain complex coreferential phenomena and hence fail to evaluate the ability of models to resolve coreference .
We present a new crowdsourced dataset containing more than 24 K span-selection questions that require resolving coreference among entities in over 4.7 K English paragraphs from Wikipedia .
Obtaining questions focused on such phenomena is challenging , because it is hard to avoid lexical cues that shortcut complex reasoning .
We deal with this issue by using a strong baseline model as an adversary in the crowdsourcing loop , which helps crowdworkers avoid writing questions with exploitable surface cues .
We show that state - of - the - art reading comprehension models perform significantly worse than humans on this benchmark - the best model performance is 70.5 F 1 , while the estimated human performance is 93.4 F 1 . 1 Links to dataset , code , models , and leaderboard available at https://allennlp.org/quoref.
Introduction Paragraphs and other longer texts typically make multiple references to the same entities .
Tracking these references and resolving coreference is essential for full machine comprehension of these texts .
Significant progress has recently been made in reading comprehension research , due to large crowdsourced datasets ( Rajpurkar et al. , 2016 ; Bajaj et al. , 2016 ; Joshi et al. , 2017 ; Kwiatkowski et al. , 2019 , inter alia ) .
However , these datasets focus largely on understanding local predicateargument structure , with very few questions requiring long-distance entity tracking .
Obtaining such questions is hard for two reasons : ( 1 ) teaching crowdworkers about coreference is challenging , with even experts disagreeing on its nuances ( Pradhan et al. , 2007 ; Versley , 2008 ; Re- Byzantines were avid players of tavli ( Byzantine Greek : ? ) , a game known in English as backgammon , which is still popular in former Byzantine realms , and still known by the name tavli in Greece .
Byzantine nobles were devoted to horsemanship , particularly tzykanion , now known as polo .
The game came from Sassanid Persia in the early period and a Tzykanisterion ( stadium for playing the game ) was built by Theodosius inside the Great Palace of Constantinople .
Emperor Basil I ( r. 867-886 ) excelled at it ; Emperor Alexander ( r. 912-913 ) died from exhaustion while playing , Emperor Alexios I Komnenos ( r. 1081 - 1118 ) was injured while playing with Tatikios , and John I of Trebizond ( r. 1235 Trebizond ( r. - 1238 died from a fatal injury during a game .
Aside from Constantinople and Trebizond , other Byzantine cities also featured tzykanisteria , most notably Sparta , Ephesus , and Athens , an indication of a thriving urban aristocracy . Q1 .
What is the Byzantine name of the game that Emperor Basil I excelled at ?
it ? tzykanion Q2 .
What are the names of the sport that is played in a Tzykanisterion ?
the game ? tzykanion ; polo Q3 .
What cities had tzykanisteria ?
cities ? Constantinople ; Trebizond ; Sparta ; Ephesus ; Athens Figure
1 : Example paragraph and questions from the dataset .
Highlighted text in paragraphs is where the questions with matching highlights are anchored .
Next to the questions are the relevant coreferent mentions from the paragraph .
They are bolded for the first question , italicized for the second , and underlined for the third in the paragraph .
casens et al. , 2011 ; Poesio et al. , 2018 ) , and ( 2 ) even if we can get crowdworkers to target coreference phenomena in their questions , these questions may contain giveaways that let models arrive at the correct answer without performing the desired reasoning ( see ?3 for examples ) .
We introduce a new dataset , QUOREF , 1 that contains questions requiring coreferential reasoning ( see examples in Figure 1 ) .
The questions are derived from paragraphs taken from a diverse set of English Wikipedia articles and are collected using an annotation process ( ?2 ) that deals with the aforementioned issues in the following ways : 5926
First , we devise a set of instructions that gets workers to find anaphoric expressions and their referents , asking questions that connect two mentions in a paragraph .
These questions mostly revolve around traditional notions of coreference ( Figure 1 Q1 ) , but they can also involve referential phenomena that are more nebulous ( Figure 1 Q3 ) .
Second , inspired by Dua et al . ( 2019 ) , we disallow questions that can be answered by an adversary model ( uncased base BERT , Devlin et al. , 2019 , trained on SQuAD 1.1 , Rajpurkar et al. , 2016 running in the background as the workers write questions .
This adversary is not particularly skilled at answering questions requiring coreference , but can follow obvious lexical cues - it thus helps workers avoid writing questions that shortcut coreferential reasoning .
QUOREF contains more than 24 K questions whose answers are spans or sets of spans in 4.7 K paragraphs from English Wikipedia that can be arrived at by resolving coreference in those paragraphs .
We manually analyze a sample of the dataset ( ?3 ) and find that 78 % of the questions cannot be answered without resolving coreference .
We also show ( ?4 ) that the best system performance is 70.5 % F 1 , while the estimated human performance is 93.4 % .
These findings indicate that this dataset is an appropriate benchmark for coreference - aware reading comprehension .
Dataset Construction Collecting paragraphs
We scraped paragraphs from Wikipedia pages about English movies , art and architecture , geography , history , and music .
For movies , we followed the list of English language films , 2 and extracted plot summaries that are at least 40 tokens , and for the remaining categories , we followed the lists of featured articles .
3 Since movie plot summaries usually mention many characters , it was easier to find hard QUOREF questions for them , and we sampled about 40 % of the paragraphs from this category .
Crowdsourcing setup
We crowdsourced questions about these paragraphs on Mechanical Turk .
We asked workers to find two or more co-referring spans in the paragraph , and to write questions such that answering them would require the knowledge that those spans are coreferential .
We did not ask them to explicitly mark the co-referring spans .
Workers were asked to write questions for a random sample of paragraphs from our pool , and we showed them examples of good and bad questions in the instructions ( see Appendix A ) .
For each question , the workers were also required to select one or more spans in the corresponding paragraph as the answer , and these spans are not required to be same as the coreferential spans that triggered the questions .
4
We used an uncased base BERT QA model ( Devlin et al. , 2019 ) trained on SQuAD 1.1 ( Rajpurkar et al. , 2016 ) as an adversary running in the background that attempted to answer the questions written by workers in real time , and the workers were able to submit their questions only if their answer did not match the adversary 's prediction .
5 Appendix
A further details the logistics of the crowdsourcing tasks .
Some basic statistics of the resulting dataset can be seen in Table 1 .
Semantic Phenomena in QUOREF
To better understand the phenomena present in QUOREF , we manually analyzed a random sample of 100 paragraph -question pairs .
The following are some empirical observations .
Requirement of coreference resolution
We found that 78 % of the manually analyzed questions cannot be answered without coreference resolution .
The remaining 22 % involve some form of coreference , but do not require it to be resolved for answering them .
Examples include a paragraph that mentions only one city , " Bristol " , and a sentence that says " the city was bombed " .
that can identify city names , making the content in the question after Which city unnecessary .
Types of coreferential reasoning Questions in QUOREF require resolving pronominal and nominal mentions of entities .
Table 2 shows percentages and examples of analyzed questions that fall into these two categories .
These are not disjoint sets , since we found that 32 % of the questions require both ( row 3 ) .
We also found that 10 % require some form of commonsense reasoning ( row 4 ) .
Baseline Model Performance on QUOREF
We evaluated two classes of baseline models on QUOREF : state - of - the art reading comprehension models that predict single spans ( ?4.1 ) and heuristic baselines to look for annotation artifacts ( ?4.2 ) .
We use two evaluation metrics to compare model performance : exact match ( EM ) , and a ( macro- averaged ) F 1 score that measures overlap between a bag-of-words representation of the gold and predicted answers .
We use the same implementation of EM as SQuAD , and we employ the F 1 metric used for DROP ( Dua et al. , 2019 )
Reading Comprehension Models
We test four single-span ( SQuAD -style ) reading comprehension models : ( 1 ) QANet ( Yu et al. , 2018 ) , currently the best-performing published model on SQuAD 1.1 without data augmentation or pretraining ; ( 2 ) QANet + BERT , which enhances the QANet model by concatenating frozen BERT representations to the original input embeddings ; ( 3 ) BERT QA ( Devlin et al. , 2019 ) , the adversarial baseline used in data construction , and ( 4 ) XLNet QA ( Yang et al. , 2019 ) , another large pretrained language model based on the Transformer architecture ( Vaswani et al. , 2017 ) that outperforms BERT QA on reading comprehension benchmarks SQuAD and RACE ( Lai et al. , 2017 ) .
We use the AllenNLP ) implementation of QANet modified to use the marginal objective proposed by Clark and Gardner ( 2018 ) and pytorch - transformers 6 implementation of base BERT QA 7 and base XLNet QA .
BERT is pretrained on English Wikipedia and BookCorpus ( Zhu et al. , 2015 ) ( 3.87 B wordpieces , 13 GB of plain text ) and XLNet additionally on Giga5 ( Napoles et al. , 2012 ) , ClueWeb 2012 - B ( extended from Smucker et al. , 2009 ) , and Common Crawl 8 ( 32.89B wordpieces , 78 GB of plain text ) .
Heuristic Baselines
In light of recent work exposing predictive artifacts in crowdsourced NLP datasets ( Gururangan et al. , 2018 ; Kaushik and Lipton , 2018 , inter alia ) , we estimate the effect of predictive artifacts by training BERT QA and XLNet QA to predict a single start and end index given only the passage as input ( passage-only ) .
Results
Table 3 presents the performance of all baseline models on QUOREF .
The best performing model is XLNet QA , which reaches an F 1 score of 70.5 in the test set .
However , it is still more than 20 F 1 points below human performance .
9 BERT QA trained on QUOREF under-performs XLNet QA , but still gets a decent F 1 score of 66.4 .
Note that BERT QA trained on SQuAD would have achieved an F 1 score of 0 , since our dataset was constructed with that model as the adversary .
The extent to which BERT QA does well on QUOREF might indicate its capacity for coreferential reasoning that was not exploited when it was trained on SQuAD ( for a detailed discussion of this phenomenon , see Liu et al. , 2019 ) .
Our analysis of model errors in ?4.4 shows that some of the improved performance may also be due to artifacts in QUOREF .
We notice smaller improvements from XLNet QA over BERT QA ( 4.12 in F 1 test score , 2.6 in EM test score ) on QUOREF compared to other reading comprehension benchmarks : SQuAD and RACE ( see Yang et al. , 2019 ) .
This might indicate the insufficiency of pretraining on more data ( XL Net was pretrained on 6 times more plain text , nearly 10 times more wordpieces than BERT ) , for coreferential reasoning .
The passage-only baseline under-performs all other systems ; examining its predictions reveals that it almost always predicts the most frequent entity in the passage .
Its relatively low performance , despite the tendency for Wikipedia articles and passages to be written about a single entity , indicates that a large majority of questions likely require coreferential reasoning .
Error Analysis
We analyzed the predictions from the baseline systems to estimate the extent to which they really understand coreference .
Since the contexts in QUOREF come from Wikipedia articles , they are often either about a specific entity , or are narratives with a single protagonist .
We found that the baseline models exploit this property to some extent .
We observe that 51 % of the QUOREF questions in the development set that were correctly answered by BERT QA were either the first or the most frequent entity in the paragraphs , while in the case of those that were incorrectly answered , this value is 12 % .
XLNet QA also exhibits a similar trend , with the numbers being 48 % and 11 % , respectively .
QA systems trained on QUOREF often need to find entities that occur far from the locations in the paragraph at which that the questions are anchored .
To assess whether the baseline systems exploited answers being close , we manually analyzed predictions of BERT QA and XLNet QA on 100 questions in the development set , and found that the answers to 17 % of the questions correctly answered by XLNet QA are the nearest entities , whereas the number is 4 % for those incorrectly answered .
For BERT QA , the numbers are 17 % and 6 % respectively .
Related Work Traditional coreference datasets
Unlike traditional coreference annotations in datasets like those of Pradhan et al . ( 2007 ) , Ghaddar and Langlais ( 2016 ) , Chen et al. ( 2018 ) and Poesio et al . ( 2018 ) , which aim to obtain complete coref-erence clusters , our questions require understanding coreference between only a few spans .
While this means that the notion of coreference captured by our dataset is less comprehensive , it is also less conservative and allows questions about coreference relations that are not marked in OntoNotes annotations .
Since the notion is not as strict , it does not require linguistic expertise from annotators , making it more amenable to crowdsourcing .
Guha et al. ( 2015 ) present the limitations of annotating coreference in newswire texts alone , and like us , built a non-newswire coreference resolution dataset focusing on Quiz Bowl questions .
There is some other recent work ( Poesio et al. , 2019 ; Aralikatte and S?gaard , 2019 ) in crowdsourcing coreference judgments that relies on a relaxed notion of coreference as well .
Reading comprehension datasets
There are many reading comprehension datasets ( Richardson et al. , 2013 ; Rajpurkar et al. , 2016 ; Kwiatkowski et al. , 2019 ; Dua et al. , 2019 , inter alia ) .
Most of these datasets principally require understanding local predicate - argument structure in a paragraph of text .
QUOREF also requires understanding local predicate - argument structure , but makes the reading task harder by explicitly querying anaphoric references , requiring a system to track entities throughout the discourse .
Conclusion
We present QUOREF , a focused reading comprehension benchmark that evaluates the ability of models to resolve coreference .
We crowdsourced questions over paragraphs from Wikipedia , and manual analysis confirmed that most cannot be answered without coreference resolution .
We show that current state - of - the - art reading comprehension models perform significantly worse than humans .
Both these findings provide evidence that QUOREF is an appropriate benchmark for coreference - aware reading comprehension .
