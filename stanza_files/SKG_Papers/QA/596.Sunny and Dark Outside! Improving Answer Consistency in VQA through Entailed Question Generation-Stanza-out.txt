title
Sunny and Dark Outside ?!
Improving Answer Consistency in VQA through Entailed Question Generation
abstract
While models for Visual Question Answering ( VQA ) have steadily improved over the years , interacting with one quickly reveals that these models lack consistency .
For instance , if a model answers " red " to " What color is the balloon ? " , it might answer " no " if asked , " Is the balloon red ? " .
These responses violate simple notions of entailment and raise questions about how effectively VQA models ground language .
In this work , we introduce a dataset , ConVQA , and metrics that enable quantitative evaluation of consistency in VQA .
For a given observable fact in an image ( e.g. the balloon 's color ) , we generate a set of logically consistent question - answer ( QA ) pairs ( e.g. Is the balloon red ? ) and also collect a humanannotated set of common-sense based consistent QA pairs ( e.g . Is the balloon the same color as tomato sauce ? ) .
Further , we propose a consistency - improving data augmentation module , a Consistency Teacher Module ( CTM ) .
CTM automatically generates entailed ( or similar-intent ) questions for a source QA pair and fine- tunes the VQA model if the VQA 's answer to the entailed question is consistent with the source QA pair .
We demonstrate that our CTM - based training improves the consistency of VQA models on the Con-VQA datasets and is a strong baseline for further research .
1 Introduction
" A skeptic , I would ask for consistency first of all . "
Sylvia Plath ( Plath , 2007 ) Visual Question Answering ( VQA ) ( Antol et al. , 2015 ) involves answering natural language questions about images .
Despite the recent progress on VQA , we observe that existing methods are prone to making blatant mistakes while answering questions regarding the same visual fact but from slightly different perspectives ( Figure 1 ) .
This reveals a critical limitation of the state - of - the - art models in maintaining consistency .
In particular , we motivate our definition of consistency based on classical deductive logic ( Tarski and Tarski , 1994 ) that defines a consistent theory as one that does not entail a contradiction .
Correspondingly , we define consistency , in the context of VQA , as being able to answer questions posed from different semantic perspectives about a certain fact without any contradiction .
In addition , consistent Question - Answer ( QA ) pairs can be derived based on simple notions of logic or by commonsense reasoning .
For instance , say an image contains a " large building " .
Logic- based QA pairs can be " is the building small ? no " and " what size is the building ?
large " .
On the other hand , if an image contains " vegetarian pizza " , commonsense - based QA pairs can be " is it a vegetarian pizza ? yes " and " is there pepperoni on the pizza ? no " , which requires commonsense knowledge that " pepperoni " is not vegetarian .
While attempts have been made to construct logic- based consistent VQA datasets ( Hudson and Manning , 2019 ) , they still fall short on commonsense - based consistency .
To this end , our ConVQA Dataset consists of two subsets : 1 ) a challenging human-annotated set comprised of commonsense - based consistent QA 's ( shown in Figure 3 ) , and 2 ) an automatically generated logicbased consistent QA dataset ( shown in Figure 2 ) .
To improve the consistency of VQA models , we propose a Consistency Teacher Module ( CTM ) , which consists of a Question Generator that synthesizes entailed ( or similar-intent ) questions given a seed QA pair and a Consistency Checker that examines whether answers to those similar-intent questions are consistent .
For training a consistent VQA model , our CTM acts as a consistency - based data augmentation scheme that trains a VQA model with consistent answers to entailed questions .
We demonstrate that our approach improves the performance of a baseline VQA model on our ConVQA testing sets in terms of both accuracy and consistency .
Our datasets and models will be available at https://bit.ly/32exlM7.
Related Work Checking for consistency can be considered as an interrogative Turing Test ( Radziwill and Benton , 2017 ) for linguistic robustness ( Stede , 1992 ) , .
Works such as Xu et al . ( 2018 ) explore the robustness of VQA with respect to image variations , whereas works such as Ray et al . ( 2016 ) and Mahendru et al . ( 2017 ) focus on the understanding of the premise of a question instead of relying on dataset biases ( Agrawal et al. , 2017 ) ( Goyal et al. , 2017 ) or linguistic biases ( Ramakrishnan et al. , 2018 ) .
Recently , the research community has shown great interest in evaluating VQA for consistency and plausibility .
GQA ( Hudson and Manning , 2019 ) is established as a scene-graph based QA dataset .
Their questions , similar to , require multiple hops of reasoning , and are not validated or annotated by humans .
Our Con-VQA differs from GQA in the following two aspects .
First , we provide a human-validated test set of the automatically generated logic- based consistent QA 's for a more accurate performance evaluation .
Second , we collect human-annotated QA pairs based on common-sense in addition to the logic- based QA's .
The most relevant work to ours is Shah et al . ( 2019 ) .
However , they focus strictly on question paraphrases that maintains the same answers as the source question .
We , however , focus on generating questions which can have different answers , but are about the same visual fact , which greatly increases the diversity of the resulting QA pairs .
To the best of our knowledge , the proposed ConVQA dataset is the first consistent QA dataset that contains human-annotated consistent QA 's based on common-sense .
Other works have also looked into question generation , ( Mostafazadeh et al. , 2016 ) for training better VQA models .
In Misra et al. ( 2017 ) , QA pairs are obtained from an oracle in a simulated environment .
In contrast , our CTMbased training operates on real images and uses a learned consistency measure to train the VQA module with consistent QA's .
ConVQA Datasets
The consistent QA pairs in our ConVQA are generated automatically based on simple notions of logical consistency or are human-annotated using commonsense reasoning .
Logic- based Consistent QA .
( L- ConVQA ) Consider the Visual Genome ( Krishna et al. , 2017 ) scene graph in Figure 2 consisting of objects , attributes , and their relationships .
We consider each triplet to encode a single ' visual fact ' , for instance , that the sofa is white .
We employ slot-filler NLP techniques to generate a set of QA pairs for each triplet ( object-relation - subject ) in the scene graph .
Currently , we focus on attribute ( e.g. , color , size ) , existential ( e.g. , is there ) and relational ( e.g. , sofa on floor ) consistency .
We leverage Wordnet ( Miller , 1995 ) and a manually generated list of antonyms ( e.g. , white vs. black ) and hypernyms ( e.g. , white ? color ) to generate these QA pairs .
For example , for the attribute " white " of an object " cup " , we generate QA pairs such as " is the cup white ?
yes " , " is the cup black ? no " and " what color is cup ?
white " .
We also filter objects and relationships by frequency and saliency ( e.g. , based on bounding boxes ) to avoid non-salient and infrequent objects or noisy relationships .
We have a total of 880,141 QA pairs in 255,910 sets on 70,292 images .
We split the data into a training set with 47,999 images , a validation set with 9,993 images , and a test set with 12,300 images .
Notably , we create a smaller clean test set ( 12,325 QA pairs on 725 images ) using Amazon Mechanical Turk ( AMT ) where three independent workers were asked to remove incorrect or unnatural QA's .
Approach
To improve VQA consistency , we propose training a VQA model using a Consistency Teacher Module ( CTM ) that generates entailed questions and performs a consistency - based data augmentation .
More specifically , CTM consists of two trainable components - an entailed question generator and a consistency checker .
Entailed Question Generator .
For a given a source question - answer pair , we define entailed questions as those for which the answer should be obvious given the source QA pair .
For example , given the source QA pair " Who is on court ?
Man " , an appropriate entailed question might be " Is the tennis court empty ? " .
We train a question generation model that given representations of the image and the source QA pair , generates a new question .
Specifically , our question generator concatenates the deep features of an image ( extracted using a ResNet152 network ) and a QA pair ( extracted using a 1 - layer LSTM ( Hochreiter and Schmidhuber , 1997 ) ) to represent a visual fact .
These features are fed into another LSTM model to generate a similar-intent question .
We train this module on the automatically generated Logical L-ConVQA train set .
We also include some closely related ( according to averaged Word2Vec ( Mikolov et al. , 2013 ) distance ) Visual Genome ( Krishna et al. , 2017 ) QA pairs in the training of the question generator to add some diversity to the generated questions .
Consistency Checker .
Once the VQA model produces an answer for the generated entailed question , it may or may not be consistent with the source question .
To evaluate this and provide feedback to the VQA model , we train a consistency checker that processes the image and both the source and entailed QA pairs .
Similar to the question generator architecture , the consistency checker takes in deep features of the image and two QA pairs and classifies the QA pairs as consistent , inconsistent or unrelated .
This model is trained using the automatically generated L-ConVQA train set alone .
Inconsistent examples in the L-ConVQA set are made using simple techniques such as flipping yes / no answers and replac- ing entities in the scene graph triplets .
Consistency Teacher Module ( CTM ) .
Putting these two components together , we can train a VQA model based on its consistency on generated entailed questions .
Figure 4 shows our pipeline .
During training , for each source VQA QA pair ( " Who is on court ?
Man " ) , we generate an entailed question ( " Is tennis court empty ? " ) and produce the VQA model 's answer ( " No " ) .
We then run the consistency checker to determine if the generated answer is consistent with the source QA ( in this case , " Who is on court ?
Man " ) .
If the answer is consistent ( and VQA confidence > 0.7 ) , we treat it as the ground truth for the entailed question and update the VQA model as if this example were part of the original dataset .
Likewise if it is deemed inconsistent , or if the question is deemed unrelated , it is unclear what the correct answer should be , so we do not update the model .
Experiments
To evaluate our approach , we apply the Consistency Teacher Module ( CTM ) module to a stateof - the- art VQA model trained on VQAv2 and evaluate performance on the ConVQA datasets .
We describe training procedures , metrics , and baselines in this section .
Consistency Teacher Module Training .
We train the components of CTM - Entailed Question Generation and Consistency Checker - using only the synthetic L-ConVQA train set ( referred as the standard CTM ) or a mix of Visual Genome and L-ConVQA train ( referred as CTMvg ) and keep them frozen when fine-tuning the VQA model .
When we train the VQA , the sets used to finetune the VQA or seed the CTM come from splits not Qualitative examples of our Entailed Question Generator trained only on L-ConVQA are shown in Figure 5 .
Despite only being trained on the automatically - generated L-ConVQA data , it generates reasonably well -entailed questions on human-annotated questions .
Our Consistency Checker has a high accuracy of classification on the L-ConVQA test set ( 90 % ) .
However , when tested with a mix of commonsense - based CS - ConVQA , the accuracy drops to 64 % ( chance is 33 % for 3 classes ) .
We find that precision is important when training the VQA using the pre-trained Consistency Checker .
Hence , we use the classifier at above 90 % confidence threshold , where the precision is 70.38 % .
Evaluation Metrics for ConVQA .
We report three metrics for ConVQA - capturing notions of consistency and performance .
- Perfect- Consistency ( Perf -Con ) .
A model is perfectly consistent for a question set if it answers all questions in the set correctly .
We report the percentage of such sets as Perf-Con .
- Average Consistency ( Avg - Con ) .
We also report the average accuracy within a consistent question set over the entire dataset as Avg- Con .
- Accuracy ( top - 1 ) .
Finally , we report the top - 1 accuracy over all questions in the dataset .
Baselines .
We compare to a number of baseline models to put our CTM results into context :
Results and Analysis Table 1 shows quantitative results on our L-ConVQA and CS - ConVQA datasets .
We make a number of observations below .
The state- of- the - art VQA has low consistency .
The baseline VQA system ( row a ) retains similarly high top - 1 accuracy on the ConVQA splits ( 63.58 % on VQAv2 vs 70.34 % / 60.03 % on L-ConVQA / CS - ConVQA ) ; however , it achieves only 26.13 % perfect consistency on the human generated CS - ConVQA questions .
Finetuning is an effective strategy for the synthetic L-ConVQA split .
Finetuning on L-ConVQA train results in 18.43 % gains in perfect consistency on L-ConVQA test ( row c vs a ) .
This is unsurprising given the templated questions and simple concepts in L-ConVQA ; however , perfect consistency is low in absolute terms at 54.68 % .
Finetuning does not lead to significant gains in consistency for human-generated questions .
Conclusion and Discussion
In this paper , we introduced a ConVQA dataset consisting of logic-based and commonsense - based consistent QA pairs about visual facts in an image .
We also proposed a Consistency Teacher Module that acts as a consistency - based data augmenter to teach VQA models to answer consistently .
As future work , we plan to look into improving our automatically generated consistent QA pairs using external knowledge - bases .
Figure 1 : 1 Figure 1 : Current VQA models often fail at consistently answering semantically rephrased questions .
To address this limitation , we construct a consistent VQA ( ConVQA ) dataset with diverse QA pairs that query the same visual fact .
We also propose a Consistency Teacher Module ( CTM ) that improves VQA consistency by rewarding consistent behavior .
