title
Automatic Identification of Rhetorical Questions
abstract
A question may be asked not only to elicit information , but also to make a statement .
Questions serving the latter purpose , called rhetorical questions , are often lexically and syntactically indistinguishable from other types of questions .
Still , it is desirable to be able to identify rhetorical questions , as it is relevant for many NLP tasks , including information extraction and text summarization .
In this paper , we explore the largely understudied problem of rhetorical question identification .
Specifically , we present a simple n-gram based language model to classify rhetorical questions in the Switchboard Dialogue Act Corpus .
We find that a special treatment of rhetorical questions which incorporates contextual information achieves the highest performance .
Introduction Rhetorical questions frequently appear in everyday conversations .
A rhetorical question is functionally different from other types of questions in that it is expressing a statement , rather than seeking information .
Thus , rhetorical questions must be identified to fully capture the meaning of an utterance .
This is not an easy task ; despite their drastic functional differences , rhetorical questions are formulated like regular questions .
Bhatt ( 1998 ) states that in principle , a given question can be interpreted as either an information seeking question or as a rhetorical question and that intonation can be used to identify the interpretation intended by the speaker .
For instance , consider the following example : ( 1 ) Did I tell you that writing a dissertation was easy ?
Just from reading the text , it is difficult to tell whether the speaker is asking an informational question or whether they are implying that they did not say that writing a dissertation was easy .
However , according to our observation , which forms the basis of this work , there are two cases in which rhetorical questions can be identified solely based on the text .
Firstly , certain linguistic cues make a question obviously rhetorical , which can be seen in examples ( 2 ) and ( 3 ) 1 . Secondly , the context , or neighboring utterances , often reveal the rhetorical nature of the question , as we can see in example ( 4 ) .
( 2 ) Who ever lifted a finger to help George ?
( 3 ) After all , who has any time during the exam period ?
( 4 ) Who likes winter ?
It is always cold and windy and gray and everyone feels miserable all the time .
There has been substantial work in the area of classifying dialog acts , within which rhetorical questions fall .
To our knowledge , prior work on dialog act tagging has largely ignored rhetorical questions , and there has not been any previous work specifically addressing rhetorical question identification .
Nevertheless , classification of rhetorical questions is crucial and has numerous potential applications , including questionanswering , document summarization , author identification , and opinion extraction .
We provide an overview of related work in Section 2 , discuss linguistic characteristics of rhetorical questions in Section 3 , describe the experimental setup in Section 4 , and present and analyze the experiment results in Section 5 .
We find that , while the majority of the classification relies on features extracted from the question itself , adding in n-gram features from the context improves the performance .
An F 1 - score of 53.71 % is achieved by adding features extracted from the preceding and subsequent utterances , which is about a 10 % improvement from a baseline classifier using only the features from the question itself .
2 Related work Jurafsky et al. ( 1997a ) and Reithinger and Klesen ( 1997 ) used n-gram language modeling on the Switchboard and Verbmobil corpora respectively to classify dialog acts .
Grau et al. ( 2004 ) uses a Bayesian approach with n-grams to categorize dialog acts .
We also employ a similar language model to achieve our results .
Samuel et al. ( 1999 ) used transformation - based learning on the Verbmobil corpus over a number of utterance features such as utterance length , speaker turn , and the dialog act tags of adjacent utterances .
Stolcke et al. ( 2000 ) utilized Hidden Markov Models on the Switchboard corpus and used word order within utterances and the order of dialog acts over utterances .
Zechner ( 2002 ) worked on automatic summarization of open-domain spoken dialogues i.e. , important pieces of information are found in the back and forth of a dialogue that is absent in a written piece .
Webb et al. ( 2005 ) used intra-utterance features in the Switchboard corpus and calculated n-grams for each utterance of all dialogue acts .
For each ngram , they computed the maximal predictivity i.e. , its highest predictivity value within any dialogue act category .
We utilized a similar metric for ngram selection .
Verbree et al. ( 2006 ) constructed their baseline for three different corpora using the performance of the LIT set , as proposed by Samuel ( 2000 ) .
In this approach , they also chose to use a compressed feature set for n-grams and POS n-grams .
We chose similar feature sets to classify rhetorical questions .
Our work extends these approaches to dialog act classification by exploring additional features which are specific to rhetorical question identification , such as context n-grams .
Features for Identifying Rhetorical Questions
In order to correctly classify rhetorical questions , we theorize that the choice of words in the question itself may be an important indicator of speaker intent .
To capture intent in the words themselves , it makes sense to consider a common unigram , while a bigram model will likely capture short phrasal cues .
For instance , we might expect the existence of n-grams such as well or you know to be highly predictive features of the rhetorical nature of the question .
Additionally , some linguistic cues are helpful in identifying rhetorical questions .
Strong negative polarity items ( NPIs ) , also referred to as emphatic or even - NPIs in the literature , are considered definitive markers .
Some examples are budge an inch , in years , give a damn , bat an eye , and lift a finger ( Giannakidou 1999 , van Rooy 2003 . Gresillon ( 1980 ) notes that a question containing a modal auxiliary , such as could or would , together with negation tends to be rhetorical .
Certain expressions such as yet and after all can only appear in rhetorical questions ( Sadock 1971 , Sadock 1974 .
Again , using common n-grams as features should partially capture the above cues because ngram segments of strong NPIs should occur more frequently .
We also wanted to incorporate common grammatical sequences found in rhetorical questions .
To that end , we can consider part of speech ( POS ) n-grams to capture common grammatical relations which are predictive .
Similarly , for rhetorical questions , we expect context to be highly predictive for correct classification .
For instance , the existence of a question mark in the subsequent utterance when spoken by the questioner , will likely be a weak positive cue , since the speaker may not have been expecting a response .
However , the existence of a question mark by a different speaker may not be indicative .
This suggests a need to decompose the contextbased feature space by speaker .
Similarly , phrases uttered prior to the question will likely give rise to a different set of predictive n-grams .
Using these observations , we decided to implement a simple n-gram model incorporating contextual cues to identify rhetorical questions .
Specifically , we used unigrams , bigrams , POS bigrams , and POS trigrams of a question and its immediately preceding and following context as feature sets .
Based on preliminary results , we did not use trigrams or POS unigrams .
POS tags did not capture sufficient contextual information and trigrams were not implemented since the utterances in our dataset were too small to fully utilize them .
Also , to capture the contextual information , we distinguish three distinct categories - questions , utterances immediately preceding questions , and utterances immediately following questions .
In order to capture the effect of a feature if it is used by the same speaker versus a different speaker , we divided the feature space contextual utterances into four disjoint groups : precedent - samespeaker , precedent - different - speaker , subsequentsame -speaker , and subsequent - different -speaker .
Features in each group are all considered independently .
4 Experimental Setup
Data
For the experiments , we used the Switchboard Dialog Act Corpus ( Godfrey et al.
1992 ; Jurafsky et al. 1997 b ) , which contains labeled utterances from phone conversations between different pairs of people .
We preprocessed the data to contain only the utterances marked as questions ( rhetorical or otherwise ) , as well as the utterances immediately preceding and following the questions .
Additionally , connectives like and and but were marked as t con , the end of conversation was as t empty , and laughter was marked as t laugh .
After filtering down to questions , we split the data into 5960 questions in the training set and 2555 questions in the test set .
We find the dataset to be highly skewed with only 128 2555 or 5 % of the test instances labeled as rhetorical .
Because of this , a classifier that naively labels all questions as non-rhetorical would achieve a 94.99 % accuracy .
Thus , we chose precision , recall and F 1 - measure as more appropriate metrics of our classifier performance .
We should note also that our results assume a high level of consistency of the hand annotations from the original taggging of the Switchboard Corpus .
However , based on our observation and the strict guidelines followed by annotators as mentioned in Jurafsky et al . ( 1997a ) , we are reasonably confident in the reliability of the rhetorical labels .
Learning Algorithm
We experimented with both Naive Bayes and a Support Vector Machine ( SVM ) classifiers .
Our Naive Bayes classifier was smoothed with an addalpha Laplacian kernel , where alpha was selected via cross-validation .
For our SVM , to account for the highly skewed nature of our dataset , we set the cost-factor based on the ratio of positive ( rhetorical ) to negative ( non- rhetorical ) questions in our training set as in Morik et al . ( 1999 ) .
We tuned the trade- off between margin and training error via cross validation over the training set .
In early experiments , Naive Bayes performed comparably to or outperformed SVM because the dimensionality of the feature space was relatively low .
However , we found that SVM performed more robustly over the large range and dimensionality of features we employed in the later experiments .
Thus , we conducted the main experiments using SVMLite ( Joachims 1999 ) .
As the number of parameters is linear in the number of feature sets , an exhaustive search through the space would be intractable .
So as to make this feasible , we employ a greedy approach to model selection .
We make a naive assumption that parameters of feature sets are independent or codependent on up to one other feature set in the same group .
Each pair of codependent feature sets is considered alone while holding other feature sets fixed .
Classifier parameters are also assumed to be independent for tuning purposes .
In order to optimize search time without sampling the parameter space too coarsely , we employed an adaptive refinement variant to a traditional grid search .
First , we discretely sampled the Cartesian product of dependent parameters sampled at regular geometric or arithmetic intervals between a user-specified minimum and maximum .
We then updated minimum and maximum values to center around the highest scoring sample and recursed on the search with the newly downsized span for a fixed recursion depth d .
In practice , we choose k = 4 and d = 3 .
Features Unigrams , bigrams , POS bigrams , and POS trigrams were extracted from the questions and neighboring utterances as features , based on the analysis in Section 3 .
Then , feature selection was performed as follows .
For all features sets , we considered both unigram and bigram features .
All unigrams and bigrams in the training data are considered as potential candidates for features .
For each feature set above , we estimated the maximal predictivity over both rhetorical and non-rhetorical classes , corresponding to using the MLE of P ( c|n ) , where n denotes the n-gram and c is the class .
We used these estimates as a score and select the j n-grams with the highest score for each n over each group , regardless of class , where j was selected via 4 - fold cross validation .
Each feature was then encoded as a simple occurrence count within its respective group for a given exchange .
The highest scoring unigrams and bigrams are as follows : " you " , " do " , " what " , " to " , " t con " , " do you " , " you know " , " going to " , " you have " , and " well , " .
POS features were computed by running a POS tagger on all exchanges and and then picking the j-best n-grams as described above .
For our experiments , we used the maximum entropy treebank POS tagger from the NLTK package ( Bird et al. 2009 ) to compute POS bigrams and trigrams .
Lastly , in order to assess the relative value of question - based and context - based features , we designed the following seven feature sets : ? Question ( baseline ) ? Precedent ? Subsequent ? Question + Precedent ? Question + Subsequent ? Precedent Subsequent ? Question + Precedent + Subsequent
The question-only feature set serves as our baseline without considering context , whereas the other feature sets serve to test the power of the preceding and following context alone and when paired with features from the question itself .
Results and Analysis Table 1 shows the performance of the feature sets cross-valided and trained on 5960 questions ( with context ) in the Switchboard corpus and tested on the 2555 remaining questions .
Our results largely reflect our intuition on the expected utility of our various feature sets .
Features in the question group prove by far the most useful single source , while features within the subsequent prove to be more useful than features in the precedent .
Somewhat surprisingly however , an F 1 - score of 30.14 % is achieved by training on contextual features alone while ignoring any cues from the question itself , suggesting the power of context in identifying a question as rhetorical .
Additionally , one of the highest scoring bigrams is you know , matching our earlier intuitions .
Some examples of the success and failings of our system can be found in Table 2 and 3 .
For instance , in our question -only feature space , the phrase what are you telling that student ?
was incorrectly classified as non-rhetorical .
When the contextual features were added in , the classifier correctly identified it as rhetorical as we might expect .
Failure cases of our simple language model based system can be seen for instance in the false positive question is she spayed which is inter-preted as rhetorical , likely due to the unigram yeah in the response .
Overall , we achieve our best results when including both precedent and subsequent context along with the question in our feature space .
Thus , our results suggest that incorporating contextual cues from both directly before and after the question itself outperforms classifiers trained on a naive question -only feature space .
Feature Dimensionality
After model selection via cross validation , our total feature space dimensionality varies between 2914 for the precedent only feature set and 16615 for the question + subsequent feature set .
Distinct n-gram and POS n-gram features are considered for each of same speaker and different speaker for precedents and subsequents so as to capture the distinction between the two .
Examining the relative number of features selected for these subfeature sets also gives a rough idea of the strength of the various cues .
For instance , same speaker feature dimensionality tended to be much lower than different speaker feature dimensionality , suggesting that considering context uttered by the respondent is a better cue as to whether the question is rhetorical .
Additionally , unigrams and bigrams tend to be more useful features than POS n-grams for the task of rhetorical question identification , or at least considering the less common POS n-grams is not as predictive .
Evenly Split Distribution
As the highly skewed nature of our data does not allow us to get a good estimate of error rate , we also tested our feature sets on a subsection of the dataset with a 50 - 50 split between rhetorical and non-rhetorical questions to get a better sense of the accuracy of our classifier .
The results can be seen in Table 4 .
Our classifier achieves an accuracy of 81 % when trained on the questions alone and 84 % when integrating precedent and subsequent context .
Due to the reduced size of the evenly split dataset , performing a McNemar 's test with Edwards ' correction ( Edwards 1948 ) does not allow us to reject the null hypothesis that the two experiments do not derive from the same distribution with 95 % confidence ( ?
2 = 1.49 giving a 2 - tailed p value of 0.22 ) .
However , over the whole skewed dataset , we find ?
2 = 30.74 giving a 2 - tailed p < 0.00001 so we have reason to believe that with a larger evenly - split dataset inte-grating context - based features provides a quantifiable advantage .
Conclusions
In this paper , we tackle the largely understudied problem of rhetorical question identification .
While the majority of the classification relies on features extracted from the question itself , adding in n-gram features from the context improves the performance .
We achieve a 53.71 % F 1 - score by adding features extracted from the preceding and the subsequent utterances , which is about a 10 % improvement from a baseline classifier using only the features from the question itself .
For future work , we would like to employ more complicated features like the sentiment of the context , and dictionary features based on an NPI lexicon .
Also , if available , prosodic information like focus , pauses , and intonation may be useful .
Table 3 : 3 Classification with Context Features ( AC : Actual Class , PC : Predicted Class .
X and Y denote the speakers ) Feature set Acc Pre Rec F1 Error 95 % Question 92.41 35.00 60.16 44.25 7.59 ?1.02 Precedent 85.64 12.30 30.47 17.53 14.36 ?1.36 Subsequent 78.98 13.68 60.16 22.29 21.02 ?1.58 Question + Precedent 93.82 41.94 60.94 49.68 6.18 ?0.93 Question + Subsequent 93.27 39.52 64.84 49.11 6.73 ?0.97 Precedent + Subsequent 84.93 19.62 64.84 30.14 15.07 ?1.38 Question + Precedent + 94.87 49.03 59.38 53.71 5.13 ?
0.86 Subsequent Table 1 : Experimental results ( % )
Table 4 : 4 Feature set Acc Pre Rec F1 Error 95 % Question 81.25 82.71 78.01 80.29 0.19 ?0.05
Experimental results ( % ) on evenly distributed data ( training set size : 670 & test set size : 288 ) Question + Precedent + 84.38 88.71 78.01 83.02 0.16 ?0.04
Subsequent
See Section 3 for more details .
