title
Multi-grained Attention with Object-level Grounding for Visual Question Answering
abstract
Attention mechanisms are widely used in Visual Question Answering ( VQA ) to search for visual clues related to the question .
Most approaches train attention models from a coarsegrained association between sentences and images , which tends to fail on small objects or uncommon concepts .
To address this problem , this paper proposes a multi-grained attention method .
It learns explicit wordobject correspondence by two types of wordlevel attention complementary to the sentenceimage association .
Evaluated on the VQA benchmark , the multi-grained attention model achieves competitive performance with stateof - the - art models .
And the visualized attention maps demonstrate that addition of objectlevel groundings leads to a better understanding of the images and locates the attended objects more precisely .
Introduction Visual Question Answering ( Antol et al. , 2015 ; Goyal et al. , 2017a ) is a multi-modal task requiring to provide an answer to the question with reference to a given image .
Most current VQA systems resort to deep neural networks and solve the problem by end-to - end learning .
First the question and the image are encoded into semantic representations independently .
Then the multi-modal features are fused into one unified representation for which the answer is predicted ( Malinowski et al. , 2015 ; Fukui et al. , 2016 ; Anderson et al. , 2018 ) .
A key point to a successful VQA system is to discover the most relevant image regions to the question .
This is commonly resolved by attention mechanisms , where a spatial attention distribution highlighting the visual focus is computed according to the similarity between the whole question and image regions ( Xu et al. , 2015 ; Lu et al. , 2016 ) .
Although such coarse sentence -image alignment reports promising results in general , it sometimes fails to locate small objects or understand a complicated scenario .
For the example in Figure 1 , the question is " What is the man wearing around his face " .
Human has no difficulty in finding the visual clue on the people 's faces , and accordingly provide the correct answer " glasses " .
However , by visualizing the attention map of a state - of - the - art VQA model , we find that the attention is mistakenly focused on the men's body rather than their faces .
In order to identify related objects more precisely , this paper proposes a multi-grained attention mechanism that involves object - level grounding complementary to the sentence -image association .
Specifically , a matching model is trained on an object-detection dataset to learn explicit correspondence between the content words in the question and their visual counterparts .
And the labels of the detected objects are considered and their similarity with the questions are computed .
Besides , a more sophisticated language model is adopted for better representation of the question .
Finally the three types of word-object , word-label and sentence -image attention are accumulated to enhance the performance .
The contributions of this paper are twofold .
First , this paper proposes a multi-grained attention mechanism integrating two types of object features that were not previously used in VQA atten - tion approaches .
Second , the deep contextualized word representation ELMo ( Peters et al. , 2018 ) is firstly adopted in the VQA task to facilitate a better question encoding .
Proposed Model
The flowchart of the proposed model is illustrated in Figure 2 .
We start from the bottom - up topdown ( up-down ) model ( Teney et al. , 2017 ; Anderson et al. , 2018 ) , which is the winning entry to the 2017 VQA challenge .
Then this model is enhanced with two types of object - level groundings to explore fine- grained information , and a more sophisticated language model for better question representation .
Image Features
We adopt the object- detection - based approach to represent the input image .
Specifically , following Anderson et al. ( 2018 ) , a state - of - the - art object detection model Faster R-CNN ( Ren et al. , 2015 ) with ResNet - 101 as its backbone is trained on the Visual Genome ( VG ) ( Krishna et al. , 2016 ) dataset .
Then the trained model 1 is applied to identify instances of objects with bounding boxes belonging to certain categories .
The target categories of this detection model contain 1600 objects and 400 attributes .
For each input image , the top -K objects with the highest confidence scores are selected to represent the image .
For each object , the output of ResNet 's pool-flat - 5 layer is used as its visual feature , which is a 2048 - dimensional vector v k .
Besides , the label of each object 's category c k is also kept as a visually grounded evidence .
c k is a Ndimensional one- hot vector , where N is the vocabulary size .
Then the input image is represented 1
The model is available at https://github.com/peteanderson80/bottom-up-attention by both its object features V = [ v 1 , v 2 , ... , v K ] ? R 2048 ?
K and object labels C = [ c 1 , c 2 , ... , c K ] ? R N ?K .
Text Features
In our model , text features include token features and sentence features for the question , which are respectively used for fine- grained and coarsegrained attention computation .
Word Features Let Q = [ q 1 , ... , q T ] ?
R N ?T denote the one-hot representation for the input question tokens , where T is the question length , and N is the vocabulary size .
Then each token q t is turned into two word embeddings : GloVe ( Pennington et al. , 2014 ) x G t = q t E G ? R D 1 , and ELMo x E t = ELM o( q t ) ? R D 2 . D 1 and D 2 are the dimensions of GloVe embedding and ELMo embedding respectively .
E G is the GloVe matrix pre-trained on the Wikipedia & Gigaword 2 .
The ELMo embedding is dynamically computed by a L-layer bi-LSTM language model ( Hochreiter and Schmidhuber , 1997 ) .
We use the publicly available pre-trained ELMo model 3 to get the contextualized embeddings .
Sentence Features
The above two sets of token embeddings are then concatenated x t = [ x G t ; x E t ] ?
R D 1 + D 2 , and fed into a GRU ( Cho et al. , 2014 ) to encode the question sentence .
The final hidden state of the GRU i.e. , h T ?
R D 3 is taken as sentence feature , where D 3 is the hidden state size for GRU .
Multi-grained Attentions Word -Label Matching Attention ( WL )
Object category labels are high- level semantic representation compared to visual pixels , and have proven to be useful for both visual tasks like scene classification ( Li et al. , 2010 ) and multi-modal tasks like image caption and VQA ( Wu et al. , 2018 ) .
For VQA task , we observed that the semantic similarity between the object category labels and the words in the question helps to locate the referred objects .
For the input image in Figure 1 , Faster - RCNN detected objects with labels of " man " , " head " .
Some labels are exactly the same as or are semantically close to the words in the question " What is the man wearing around his face ? " .
Therefore , we compute the WL attention vector , that indicates how much weight we should give to each of the K objects in the image , in terms of the semantic similarity between the category labels of the objects and the words in the question .
For the k-th object with label c k we encode it into GloVe embedding 4 l G k = c k E G , and compute its attention score by measuring its similarity to the question GloVe embedding as follows : s W L ( X G , l G k ) = arg max t cos( x
G t , l G k ) a W L ( X G , L G ) = sof tmax s W L ( X G , l G k ) ( 1 ) where X G = x G 1 , ... , x G T ? R D 1 ?T is the GloVe embeddings for the question tokens .
L G = l G 1 , ... , l G k ?
R D 1 ?K is the GloVe embeddings for the objects labels .
a W L ? R K is the WL attention vector .
In contrast to Anderson et al . ( 2018 ) that only use objects ' visual features without the labels , and unlike
Wu et al . ( 2018 ) that discard the visual features once the labels are generated , we utilize both category labels and the visual features to enhance the fine-grained attention with objectlevel grounding .
Word -Object Matching Attention ( WO ) s W O ( x G c , v b ) = ?
W s f ( W cx G c ) ? f ( W v v b ) loss = max 0 , ? ? s W O ( x G c , v b ) + s W O ( x G c , vb ) ( 2 ) where f is ReLU and ? is sigmoid activation function , ? means element wise multiplication .
W c , W v , W s are weight parameters 5 . And the margin is set ? = 0.5 .
After s W O is pre-trained , we forwardly select at most B noun tokens in the question and compute the WO attention a W O ( X , V ) over the K objects as follows : a W O ( X G , V ) = sof tmax B b=1 s W O ( x G b , v k ) ( 3 ) where the parameters of s W O are fine-tuned in down-streaming VQA task .
Sentence -Object Attention ( SO ) Following previous methods of sentence - level question guided visual attention , we also use the global semantic of the whole sentence to guide the focus on relevant objects .
Taking sentence feature h T and objects features V as input , SO attention vector a SO is computed as follows : s SO ( hT , v k ) = ?
( W j [ f ( W v v k ) ? f ( W thT ) ] ) a SO ( hT , V ) = sof tmax s SO ( hT , v k ) ( 4 ) where f is ReLU , ? is sigmoid activation function , and W j , W v , W t are weight parameters .
Multi-modal Fusion and Answer Prediction
The above three attentions are summed together for the final attention vector .
Then we get the weighted visual feature vector v a ?
R 2048 for the image : a = a W L + a W O + a SO v a = K k=1 a k v k ( 5 )
Then the question feature h T and the attended visual feature v a are transformed into the same dimension and fused together with element - wise multiplication , to get the joint representation vector r ? R D 4 . r = f ( W rthT ) ? f ( W rv v a ) ( 6 ) where f is ReLU , W rt , W rv are weight parameters .
Following Teney et al. ( 2017 ) , we treat VQA task as a classification problem , and use the binary cross-entropy loss to take multiple marked answers into consideration : ? = ? ( f ( W ar ) ) loss = A a=1 salog ( ?a ) ? ( 1 ? sa ) log ( ? a ) ( 7 ) where ? ?
R
A is the predicted score over all A answer candidates , s a is the target accuracy score 6 . 3 Experiments and Analysis
Settings Experiments are conducted on VQA v2 dataset ( Goyal et al. , 2017 b ) .
Questions are trimmed to a maximum of T = 14 words .
We set 6 accuracy = min( # humans that provided that answer 3 , 1 ) , i.e. an answer is accurate if at least 3 markers provided the answer .
Model
Model Analysis
To understand the effects of different components , the performance by adding one certain proposed component to the baseline is reported in Table 2 .
Adding our proposed two branches of fine- grained WL and WO attentions significantly improves the baseline performance .
The result also verifies that ELMo embeddings combined with GloVe embeddings provide more sophisticated text representations , thus improves the overall performance .
Study on Attention Maps
To validate the effectiveness of the enhanced attention mechanism , we visualize the attentions and compare them versus those of the up-down model .
As Figure 4 shows , the addition of object - level groundings leads to a better understanding of the images and locates the attended objects more precisely .
For example , in Figure 4 ( a ) , for question " Can you see its paws ? " , the attention generated by our method is focused on the " paws " , while the baseline does not focus on the key regions as accurate as we do .
In Figure 4
We also notice cases where though the final answer is wrong , our model generates appropriate attention maps .
As shown in Figure 4 ( e ) , for Yes / no question " Does his bow tie match his pants ? " , our model correctly finds " tie " and " pants " object regions , but we suspect that the model does not understand the meaning of " match " .
A mean opinion score ( MOS ) test to quantitatively compare our attention mechanism with the baseline model is also performed .
Specifically , we randomly select 100 cases and generate their attention maps .
Then , we asked subjects to rate a score from 0 ( bad quality ) , 0.5 ( medium quality ) and 1 ( excellent quality ) to these attention
Conclusion
This paper proposes a multi-grained attention mechanism .
It involves both word-object grounding and sentence-image association to capture different degrees of granularity and interpretability of the images .
Visualizations of object- level attention show a clear improvement in the ability of the model to attend to small details in complicated scenes .
Q : Figure 1 : An example of VQA and the attention maps produced by a state - of - the - art model and our model .
