title
Knowledge -Based Question Answering as Machine Translation
abstract
A typical knowledge- based question answering ( KB - QA ) system faces two challenges : one is to transform natural language questions into their meaning representations ( MRs ) ; the other is to retrieve answers from knowledge bases ( KBs ) using generated MRs .
Unlike previous methods which treat them in a cascaded manner , we present a translation - based approach to solve these two tasks in one unified framework .
We translate questions to answers based on CYK parsing .
Answers as translations of the span covered by each CYK cell are obtained by a question translation method , which first generates formal triple queries as MRs for the span based on question patterns and relation expressions , and then retrieves answers from a given KB based on triple queries generated .
A linear model is defined over derivations , and minimum error rate training is used to tune feature weights based on a set of question - answer pairs .
Compared to a KB - QA system using a state - of - the - art semantic parser , our method achieves better results .
Introduction Knowledge- based question answering ( KB - QA ) computes answers to natural language ( NL ) questions based on existing knowledge bases ( KBs ) .
Most previous systems tackle this task in a cascaded manner :
First , the input question is transformed into its meaning representation ( MR ) by an independent semantic parser ( Zettlemoyer and Collins , 2005 ; Mooney , 2007 ; Artzi and Zettlemoyer , 2011 ; Liang et al. , 2011 ; Cai and Yates , 2013 ; Poon , 2013 ; Kwiatkowski et al. , 2013 ; Berant et al. , 2013 ) ;
Then , the answers are retrieved from existing KBs using generated MRs as queries .
Unlike existing KB - QA systems which treat semantic parsing and answer retrieval as two cascaded tasks , this paper presents a unified framework that can integrate semantic parsing into the question answering procedure directly .
Borrowing ideas from machine translation ( MT ) , we treat the QA task as a translation procedure .
Like MT , CYK parsing is used to parse each input question , and answers of the span covered by each CYK cell are considered the translations of that cell ; unlike MT , which uses offline - generated translation tables to translate source phrases into target translations , a semantic parsing - based question translation method is used to translate each span into its answers on - the-fly , based on question patterns and relation expressions .
The final answers can be obtained from the root cell .
Derivations generated during such a translation procedure are modeled by a linear model , and minimum error rate training ( MERT ) ( Och , 2003 ) is used to tune feature weights based on a set of question - answer pairs .
Figure 1 shows an example : the question director of movie starred by Tom Hanks is translated to one of its answers Robert Zemeckis by three main steps : ( i ) translate director of to director of ; ( ii ) translate movie starred by Tom Hanks to one of its answers Forrest Gump ; ( iii ) translate director of Forrest Gump to a final answer Robert Zemeckis .
Note that the updated question covered by Cell [ 0 , 6 ] is obtained by combining the answers to question spans covered by Cell [ 0 , 1 ] and Cell [ 2 , 6 ] .
The contributions of this work are two -fold : ( 1 ) We propose a translation - based KB - QA method that integrates semantic parsing and QA in one unified framework .
The benefit of our method is that we do n't need to explicitly generate complete semantic structures for input questions .
6 ] Cell [ 2 , 6 ] Cell [ 0 , 1 ] ( 2 )
We propose a robust method to transform single-relation questions into formal triple queries as their MRs , which trades off between transformation accuracy and recall using question patterns and relation expressions respectively .
2 Translation - Based KB-QA
Overview Formally , given a knowledge base KB and an N-L question Q , our KB - QA method generates a set of formal triples - answer pairs { D , A } as derivations , which are scored and ranked by the distribution P ( D , A | KB , Q ) defined as follows : exp { M i=1 ? i ? h i ( D , A , KB , Q ) } D , A ?H ( Q ) exp { M i=1 ? i ? h i ( D , A , KB , Q ) } ?
KB denotes a knowledge base 1 that stores a set of assertions .
Each assertion t ?
KB is in the form of {e ID sbj , p , e ID obj } , where p denotes a predicate , e ID sbj and e ID obj denote the subject and object entities of t , with unique IDs 2 . ? H ( Q ) denotes the search space { D , A }. D is composed of a set of ordered formal triples {t 1 , ... , t n }.
Each triple t = {e sbj , p , e obj } j i ?
D denotes an assertion in KB , where i and j denotes the beginning and end indexes of the question span from which t is transformed .
The order of triples in D denotes the order of translation steps from Q to A. E.g. , director of , Null , director of 1 0 , Tom 1
We use a large scale knowledge base in this paper , which contains 2.3B entities , 5.5 K predicates , and 18B assertions .
A 16 - machine cluster is used to host and serve the whole data .
2 Each KB entity has a unique ID .
For the sake of convenience , we omit the ID information in the rest of the paper .
Hanks , Film.Actor .
Film , Forrest Gump 6 2 and Forrest Gump , Film .
Film .
Director , Robert Zemeckis 6 0 are three ordered formal triples corresponding to the three translation steps in Figure 1 .
We define the task of transforming question spans into formal triples as question translation .
A denotes one final answer of Q. ? h i ( ? ) denotes the i th feature function .
? ? i denotes the feature weight of h i ( ? ) .
According to the above description , our KB - QA method can be decomposed into four tasks as : ( 1 ) search space generation for H ( Q ) ; ( 2 ) question translation for transforming question spans into their corresponding formal triples ; ( 3 ) feature design for h i ( ? ) ; and ( 4 ) feature weight tuning for {?
i }.
We present details of these four tasks in the following subsections one-by-one .
Search Space Generation
We first present our translation - based KB - QA method in Algorithm 1 , which is used to generate H ( Q ) for each input NL question Q. Algorithm 1 : Translation - based KB -QA
The first half ( from Line 1 to Line 13 ) generates a formal triple set T for each unary span Q j i ?
Q , using the question translation method QT rans ( Q j i , KB ) ( Line 4 ) , which takes Q j i as the input .
Each triple t ?
T returned is in the form of {e sbj , p , e obj } , where e sbj 's mention occurs in Q j i , p is a predicate that denotes the meaning expressed by the context of e sbj in Q j i , e obj is an answer of Q j i based on e sbj , p and KB .
We describe the implementation detail of QT rans ( ? ) in Section 2.3 .
1 for l = 1 to | Q | do 2 for all i , j s.t. j ?
i = l do 3 H( Q j i ) = ? ; 4 T = QT rans ( Q j i ,
The second half ( from Line 14 to Line 31 ) first updates the content of each bigger span Q j i by concatenating the answers to its any two consecutive smaller spans covered by Q j i ( Line 18 ) .
Then , QT rans ( Q j i , KB ) is called to generate triples for the updated span ( Line 19 ) .
The above operations are equivalent to answering a simplified question , which is obtained by replacing the answerable spans in the original question with their corresponding answers .
The search space H ( Q ) for the entire question Q is returned at last ( Line 31 ) .
Question Translation
The purpose of question translation is to translate a span Q to a set of formal triples T . Each triple t ?
T is in the form of {e sbj , p , e obj } , where e sbj 's mention 3 occurs in Q , p is a predicate that denotes the meaning expressed by the context of e sbj in Q , e obj is an answer to Q retrieved from KB using a triple query q = {e sbj , p , ?}.
Note that if no predicate p or answer e obj can be generated , { Q , N ull , Q} will be returned as a special triple , which sets e obj to be Q itself , and p to be N ull .
This makes sure the un-answerable spans can be passed on to the higher - level operations .
Question translation assumes each span Q is a single-relation question ( Fader et al. , 2013 ) .
Such assumption simplifies the efforts of semantic parsing to the minimum question units , while leaving the capability of handling multiple -relation questions ( Figure 1 gives one such example ) to the outer CYK - parsing based translation procedure .
Two question translation methods are presented in the rest of this subsection , which are based on question patterns and relation expressions respectively .
Question Pattern- based Translation
A question pattern QP includes a pattern string QP pattern , which is composed of words and a slot 3 For simplicity , a cleaned entity dictionary dumped from the entire KB is used to detect entity mentions in Q. symbol [ Slot ] , and a KB predicate QP predicate , which denotes the meaning expressed by the context words in QP pattern .
Algorithm 2 shows how to generate formal triples for a span Q based on question patterns ( QP - based question translation ) .
For each entity mention e Q ?
Q , we replace it with [ Slot ] and obtain a pattern string Q pattern ( Line 3 ) .
If Q pattern can match one QP pattern , then we construct a triple query q ( Line 9 ) using QP predicate as its predicate and one of the KB entities returned by Disambiguate ( e Q , QP predicate ) as its subject entity ( Line 6 ) .
Here , the objective of Disambiguate ( e Q , QP predicate ) is to output a set of disambiguated KB entities E in KB .
The name of each entity returned equals the input entity mention e Q and occurs in some assertions where QP predicate are the predicates .
The underlying idea is to use the context ( predicate ) information to help entity disambiguation .
The answers of q are returned by AnswerRetrieve(q , KB ) based on q and KB ( Line 10 ) , each of which is used to construct a formal triple and added to T for Q ( from Line 11 to Line 16 ) .
Figure 2 gives an example .
Question patterns are collected as follows :
First , 5W queries , which begin with What , Where , Who , When , or Which , are selected from a large scale query log of a commercial search engine ;
Then , a cleaned entity dictionary is used to annotate each query by replacing all entity mentions it contains with the symbol From experiments ( Table 3 in Section 4.3 ) we can see that , question pattern based question translation can achieve high end-to - end accuracy .
But as human efforts are needed in the mining procedure , this method cannot be extended to large scale very easily .
Besides , different users often type the questions with the same meaning in different NL expressions .
For example , although the question Forrest Gump was directed by which moviemaker means the same as the question Q in Figure 2 , no question pattern can cover it .
We need to find an alternative way to alleviate such coverage issue .
Relation Expression - based Translation
Aiming to alleviate the coverage issue occurring in QP - based method , an alternative relation expression ( RE ) - based method is proposed , and will be used when the QP - based method fails .
We define RE p as a relation expression set for a given KB predicate p ? KB .
Each relation expression RE ? RE p includes an expression string RE expression , which must contain at least one content word , and a weight RE weight , which denotes the confidence that RE expression can represent p's meaning in NL .
For example , is the director of is one relation expression string for the predicate Film .
Film .
Director , which means it is usually used to express this relation ( predicate ) in NL .
Algorithm 3 shows how to generate triples for a question Q based on relation expressions .
For each possible entity mention e Q ?
Q and a K-B predicate p ?
KB that is related to a KB entity e whose name equals e Q , Sim(e Q , Q , RE p ) is computed ( Line 5 ) based on the similarity between question context and RE p , which measures how likely Q can be transformed into a triple query q = {e , p , ?}.
If this score is larger than 0 , which means there are overlaps between Q's context and RE p , then q will be used as the triple query of Q , and a set of formal triples will be generated based on q and KB ( from Line 7 to Line 15 ) .
The computation of Sim(e Q , Q , RE p ) is defined as follows : n 1 | Q| ? n + 1 ? { ?n?Q , ?n e Q =? P (? n | RE p ) } where n is the n-gram order which ranges from 1 to 5 , ? n is an n-gram occurring in Q without overlapping with e Q and containing at least one content word , P (?
n | RE p ) is the posterior probability which is computed by : P (? n | RE p ) = Count ( ?
n , RE p ) ? n ?REp Count ( ? n , RE p ) Count ( ? , RE p ) denotes the weighted sum of times that ? occurs in RE p : Count ( ? , RE p ) = RE?REp {# ? ( RE ) ? RE weight } where # ? ( RE ) denotes the number of times that ?
occurs in RE expression , and RE weight is decided by the relation expression extraction component .
Figure 3 gives an example , where n-grams with rectangles are the ones that occur in both Q's context and the relation expression set of a given predicate p = F ilm .
F ilm.Director .
Unlike the QPbased method which needs a perfect match , the Relation expressions are mined as follows :
Given a set of KB assertions with an identical predicate p , we first extract all sentences from English Wiki pages 4 , each of which contains at least one pair of entities occurring in one assertion .
Then , we extract the shortest path between paired entities in the dependency tree of each sentence as an RE candidate for the given predicate .
The intuition is that any sentence containing such entity pairs occur in an assertion is likely to express the predicate of that assertion in some way .
Last , all relation expressions extracted are filtered by heuristic rules , i.e. , the frequency must be larger than 4 , the length must be shorter than 10 , and then weighted by the pattern scoring methods proposed in ( Gerber and Ngomo , 2011 ; Gerber and Ngomo , 2012 ) .
For each predicate , we only keep the relation expressions whose pattern scores are larger than a pre-defined threshold .
Figure 4
We propose a dependency tree - based method to handle such multiple -constraint questions by ( i ) decomposing the original question into a set of sub-questions using syntax - based patterns ; and ( ii ) intersecting the answers of all sub-questions as the final answers of the original question .
Note , question decomposition only operates on the original question and question spans covered by complete dependency subtrees .
Four syntax - based patterns ( Figure 5 ) are used for question decomposition .
If a question matches any one of these patterns , then sub-questions are generated by collecting the paths between n 0 and each n i ( i > 0 ) in the pattern , where each n denotes a complete subtree with a noun , number , or question word as its root node , the symbol * above prep * denotes this preposition can be skipped in matching .
For the question mentioned at the beginning , its two sub-questions generated are movie starred by Tom Hanks and movie starred in 1994 , as its dependency form matches pattern ( a ) .
Similar ideas are used in IBM Watson ( Kalyanpur et al. , 2012 ) as well .
As dependency parsing is not perfect , we generate single triples for such questions without considering constraints as well , and add them to the search space for competition .
h syntax constraint ( ? ) is used to boost triples that are converted from subquestions generated by question decomposition .
The more constraints an answer satisfies , the better .
Obviously , current patterns used ca n't cover all cases but most-common ones .
We leave a more general pattern mining method for future work .
Feature Design
The objective of our KB - QA system is to seek the derivation D , ? that maximizes the probability P ( D , A | KB , Q ) described in Section 2.1 as : D , ? = argmax D, A ?H ( Q ) P ( D , A | KB , Q ) = argmax D, A ?H ( Q ) M i=1 ? i ? h i ( D , A , KB , Q ) We now introduce the feature sets {h i ( ? ) } that are used in the above linear model : ? h question word ( ? ) , which counts the number of original question words occurring in A .
It penalizes those partially answered questions .
? h span ( ? ) , which counts the number of spans in Q that are converted to formal triples .
It controls the granularity of the spans used in question translation .
? h syntax subtree ( ? ) , which counts the number of spans in Q that are ( 1 ) converted to formal triples , whose predicates are not N ull , and ( 2 ) covered by complete dependency subtrees at the same time .
The underlying intuition is that , dependency subtrees of Q should be treated as units for question translation .
? h syntax constraint ( ? ) , which counts the number of triples in D that are converted from sub-questions generated by the question decomposition component .
? h triple ( ? ) , which counts the number of triples in D , whose predicates are not N ull .
? h triple weight ( ? ) , which sums the scores of all triples {t i } in D as t i ?
D t i .score .
? h QP count ( ? ) , which counts the number of triples in D that are generated by QP - based question translation method .
? h REcount ( ? ) , which counts the number of triples in D that are generated by RE - based question translation method .
? h staticrank sbj ( ? ) , which sums the static rank scores of all subject entities in D's triple set as t i ?
D t i .e sbj .static rank .
? h staticrank obj ( ? ) , which sums the static rank scores of all object entities in D's triple set as t i ?
D t i .e obj .static rank .
? h conf idence obj ( ? ) , which sums the confidence scores of all object entities in D's triple set as t?D t.e obj .conf idence .
For each assertion {e sbj , p , e obj } stored in KB , e sbj .static rank and e obj .static rank denote the static rank scores 5 for e sbj and e obj respectively ; e obj .conf idence rank represents the probability p(e obj |e sbj , p ) .
These three scores are used as features to rank answers generated in QA procedure .
Feature Weight Tuning Given a set of question - answer pairs { Q i , A ref i } as the development ( dev ) set , we use the minimum error rate training ( MERT ) ( Och , 2003 ) algorithm to tune the feature weights ?
M i in our proposed model .
The training criterion is to seek the feature weights that can minimize the accumulated errors of the top - 1 answer of questions in the dev set : ?M
1 = argmin ?
M 1 N i=1 Err( A ref i , ?i ; ? M 1 ) N is the number of questions in the dev set , A ref i is the correct answers as references of the i th question in the dev set , ?i is the top - 1 answer candidate of the i th question in the dev set based on feature weights ? M 1 , Err ( ? ) is the error function which is defined as : Err( A ref i , ?i ; ? M 1 ) = 1 ? ?( A ref i , ?i ) where ?( A ref i , ?i ) is an indicator function which equals 1 when ? i is included in the reference set A ref i , and 0 otherwise .
Comparison with Previous Work
Our work intersects with two research directions : semantic parsing and question answering .
Some previous works on semantic parsing ( Zelle and Mooney , 1996 ; Zettlemoyer and Collins , 2005 ; Wong and Mooney , 2006 ; Zettlemoyer and Collins , 2007 ; Wong and Mooney , 2007 ; Kwiatkowski et al. , 2010 ; Kwiatkowski et al. , 2011 ) require manually annotated logical forms as supervision , and are hard to extend resulting parsers from limited domains , such as GEO , JOBS and ATIS , to open domains .
Recent works ( Clarke and Lapata , 2010 ; have alleviated such issues using question - answer pairs as weak supervision , but still with the shortcoming of using limited lexical triggers to link NL phrases to predicates .
Poon ( 2013 ) has proposed an unsupervised method by adopting groundedlearning to leverage the database for indirect supervision .
But transformation from NL questions to MRs heavily depends on dependency parsing results .
Besides , the KB used ( ATIS ) is limited as well .
Kwiatkowski et al. ( 2013 ) use Wiktionary and a limited manual lexicon to map POS tags to a set of predefined CCG lexical categories , which aims to reduce the need for learning lexicon from training data .
But it still needs human efforts to define lexical categories , which usually can not cover all the semantic phenomena .
Berant et al. ( 2013 ) have not only enlarged the KB used for Freebase ( Google , 2013 ) , but also used a bigger lexicon trigger set extracted by the open IE method ( Lin et al. , 2012 ) for NL phrases to predicates linking .
In comparison , our method has further advantages : ( 1 ) Question answering and semantic parsing are performed in an joint way under a unified framework ; ( 2 ) A robust method is proposed to map NL questions to their formal triple queries , which trades off the mapping quality by using question patterns and relation expressions in a cascaded way ; and ( 3 )
We use domain independent feature set which allowing us to use a relatively small number of question - answer pairs to tune model parameters .
Fader et al. ( 2013 ) map questions to formal ( triple ) queries over a large scale , open-domain database of facts extracted from a raw corpus by ReVerb ( Fader et al. , 2011 ) .
Compared to their work , our method gains an improvement in two aspects : ( 1 ) Instead of using facts extracted using the open IE method , we leverage a large scale , high-quality knowledge base ; ( 2 ) We can handle multiple-relation questions , instead of singlerelation queries only , based on our translation based KB - QA framework .
Espana -Bonet and Comas ( 2012 ) have proposed an MT - based method for factoid QA .
But MT in there work means to translate questions into n-best translations , which are used for finding similar sentences in the document collection that probably contain answers .
Echihabi and Marcu ( 2003 ) have developed a noisy - channel model for QA , which explains how a sentence containing an answer to a given question can be rewritten into that question through a sequence of stochastic operations .
Compared to the above two MT-motivated QA work , our method uses MT methodology to translate questions to answers directly .
Experiment
Data Sets Following Berant et al. ( 2013 ) , we use the same subset of WEBQUESTIONS ( 3,778 questions ) as the development set ( Dev ) for weight tuning in MERT , and use the other part of WEBQUES - TIONS ( 2,032 questions ) as the test set ( Test ) .
Table 1 shows the statistics of this data set .
Data Set # Questions # Words WEBQUESTIONS 5,810 6.7 Table 1 : Statistics of evaluation set .
# Questions is the number of questions in a data set , # Words is the averaged word count of a question .
Table 2 shows the statistics of question patterns and relation expressions used in our KB - QA system .
As all question patterns are collected with human involvement as we discussed in Section 2.3.1 , the quality is very high ( 98 % ) .
We also sample 1,000 instances from the whole relation expression set and manually label their quality .
The accuracy is around 89 % .
These two resources can cover 566 head predicates in our KB .
KB - QA Systems Since Berant et al. ( 2013 ) is one of the latest work which has reported QA results based on a large scale , general domain knowledge base ( Freebase ) , we consider their evaluation result on WE - BQUESTIONS as our baseline .
Our KB - QA system generates the k-best derivations for each question span , where k is set to 20 .
The answers with the highest model scores are considered the best answers for evaluation .
For evaluation , we follow Berant et al . ( 2013 ) to allow partial credit and score an answer using the F1 measure , comparing the predicted set of entities to the annotated set of entities .
One difference between these two systems is the KB used .
Since Freebase is completely contained by our KB , we disallow all entities which are not included by Freebase .
By doing so , our KB provides the same knowledge as Freebase does , which means we do not gain any extra advantage by using a larger KB .
But we still allow ourselves to use the static rank scores and confidence scores of entities as features , as we described in Section 2.4 .
Evaluation Results
We first show the overall evaluation results of our KB - QA system and compare them with baseline 's results on Dev and Test .
Note that we do not reimplement the baseline system , but just list their evaluation numbers reported in the paper .
Comparison results are listed in Table 3 .
Table 3 shows our KB - QA method outperforms baseline on both Dev and Test .
We think the potential reasons of this improvement include : ?
Different methods are used to map NL phrases to KB predicates .
Berant et al. ( 2013 ) have used a lexicon extracted from a subset of ReVerb triples ( Lin et al. , 2012 )
P recision is defined as the number of correctly answered questions divided by the number of questions with non-empty answers generated by our KB - QA system .
From Table 4 we can see that the accuracy of RE only on Test ( 32.5 % ) is slightly better than baseline 's result ( 31.4 % ) .
We think this improvement comes from two aspects : ( 1 )
The quality of the relation expressions is better than the quality of the lexicon entries used in the baseline ; and ( 2 ) We use the extraction - related statistics of relation expressions as features , which brings more information to measure the confidence of mapping between NL phrases and KB predicates , and makes the model to be more flexible .
Meanwhile , QP only perform worse ( 11.8 % ) than RE only , due to coverage issue .
But by comparing the precisions of these two settings , we find QP only ( 97.5 % ) outperforms RE only ( 73.2 % ) significantly , due to its high quality .
This means how to extract highquality question patterns is worth to be studied for the question answering task .
As the performance of our KB - QA system relies heavily on the k-best beam approximation , we evaluate the impact of the beam size and list the comparison results in Figure 6 .
We can see that as we increase k incrementally , the accuracy increase at the same time .
However , a larger k ( e.g. 200 ) cannot bring significant improvements comparing to a smaller one ( e.g. , 20 ) , but using a large k has a tremendous impact on system efficiency .
So we choose k = 20 as the optimal value in above experiments , which trades off between accuracy and efficiency .
Actually , the size of our system 's search space is much smaller than the one of the semantic parser used in the baseline .
This is due to the fact that , if triple queries generated by the question translation component cannot derive any answer from KB , we will discard such triple queries directly during the QA procedure .
We can see that using a small k can achieve better results than baseline , where the beam size is set to be 200 .
Error Analysis
Entity Detection Since named entity recognizers trained on Penn TreeBank usually perform poorly on web queries , We instead use a simple string -match method to detect entity mentions in the question using a cleaned entity dictionary dumped from our KB .
One problem of doing so is the entity detection issue .
For example , in the question who was Esther 's husband ? , we cannot detect Esther as an entity , as it is just part of an entity name .
We need an ad-hoc entity detection component to handle such issues , especially for a web scenario , where users often type entity names in their partial or abbreviation forms .
Predicate Mapping Some questions lack sufficient evidences to detect predicates .
where is Byron Nelson 2012 ? is an example .
Since each relation expression must contain at least one content word , this question cannot match any relation expression .
Except for Byron Nelson and 2012 , all the others are non-content words .
Besides , ambiguous entries contained in relation expression sets of different predicates can bring mapping errors as well .
For the following question who did Steve Spurrier play pro football for ?
as an example , since the unigram play exists in both Film .
Film .Actor and American Football .
Player .
Current
Team 's relation expression sets , we made a wrong prediction , which led to wrong answers .
Specific Questions
Sometimes , we cannot give exact answers to superlative questions like what is the first book Sherlock Holmes appeared in ?.
For this example , we can give all book names where Sherlock Holmes appeared in , but we cannot rank them based on their publication date , as we cannot learn the alignment between the constraint word first occurred in the question and the predicate Book .
Written Work .
Date Of First Publication from training data automatically .
Although we have followed some work ( Poon , 2013 ; to handle such special linguistic phenomena by defining some specific operators , it is still hard to cover all unseen cases .
We leave this to future work as an independent topic .
Conclusion and Future Work
This paper presents a translation - based KB - QA method that integrates semantic parsing and QA in one unified framework .
Comparing to the baseline system using an independent semantic parser with state - of - the - art performance , we achieve better results on a general domain evaluation set .
Several directions can be further explored in the future : ( i ) We plan to design a method that can extract question patterns automatically , using existing labeled question patterns and KB as weak supervision .
As we discussed in the experiment part , how to mine high-quality question patterns is worth further study for the QA task ; ( ii )
We plan to integrate an ad-hoc NER into our KB - QA system to alleviate the entity detection issue ; ( iii )
In fact , our proposed QA framework can be generalized to other intelligence besides knowledge bases as well .
Any method that can generate answers to questions , such as the Web- based QA approach , can be integrated into this framework , by using them in the question translation component .
Figure 1 : Translation - based KB - QA example
