title
QU -BIGIR at SemEval 2017
Task 3 : Using Similarity Features for Arabic Community Question Answering Forums
abstract
In this paper , we describe our QU - BIGIR system for the Arabic subtask D of the Se-mEval 2017 Task 3 .
Our approach builds on our participation in the past version of the same subtask .
This year , our system uses different similarity features that encodes lexical and semantic pairwise similarity of text pairs .
In addition to wellknown similarity measures such as cosine similarity , we use other measures based on the summary statistics of word embedding representation for a given text .
To rank a list of candidate question - answer pairs for a given question , we train a linear SVM classifier over our similarity features .
Our best resulting run came second in subtask D with a very competitive performance to the first-ranking system .
Introduction
The ubiquitous presence of community question answering ( CQA ) websites has motivated research on building automatic question answering ( QA ) systems that can benefit from previously - answered questions to answer newly - posed ones ( Shtok et al. , 2012 ) .
A core functionality of such systems is their ability to effectively rank previouslysuggested answers with respect to their degree / probability of relevance to a posted question .
Ranking is vital to push away irrelevant and low quality answers , which are commonplace in CQA as they are generally open with no restrictions on who can post or answer questions .
To this effect , SemEval 2017 Task 3 " Community Question Answering " has emphasized the ranking component in the main task of the challenge .
We have participated in Task 3 - Subtask D ( Arabic Subtask ) which is confined to the main given a new question and a set of 30 question - answer pairs ( QApairs ) retrieved by a search engine , re-rank those QApairs by their degree / probability of relevance to the new question .
Figure 1 shows an example of a question and four of its 30 given candidate question - answer pairs .
Further details about SemEval 2017
Task 3 can be found in ( Nakov et al. , 2017 ) .
In this paper , we describe the system we developed to participate in that task .
The system leverages a supervised learning approach over similarity features .
We utilize two types of similarity features .
First , we employ similarity features based on term representation for a given pairs of text .
Second , we utilize word2vec to build text representation following the same approach as in our last year 's submission for the same subtask ( Malhas et al. , 2016 ) .
We used similarity features based on that text representation to encode the semantic similarity for pairs of texts .
The rest of the paper is organized as follows ; the approach and description of features are in-troduced in section 2 ; the experimental setup followed in our submitted runs and the results are presented in section 3 .
Finally we conclude our study with final remarks in section 4 .
Approach
We tackled the answer ranking task with a supervised learning approach that uses linear SVM models .
The features used in classification are designed to capture both lexical and semantic information of pairs of texts .
Data Setup
We are given a set of questions Q ; each is associated with P question - answer pairs .
To compute our features , we define a text pair < T 1 , T 2 > according to three setups : ? QQA : We consider T 1 to be the original question q and the concatenation of one pair p of its associated question - answer pairs as T 2 . ? QA : We consider T 1 to be the original question q and one answer of its associated question - answer pairs as T 2 . ? QQ : We consider T 1 to be the original question q and one question of its associated question - answer pairs as T 2 .
Term - based Similarity Features
A recent study has showed that simple features like MK features ( Metzler and Kanungo , 2008 ) can be very effective for re-ranking candidate question answer pairs ( Yang et al. , 2016 ) .
We specifically use the following features described by Yang et al .
( Yang et al. , 2016 ) .
For all features , we assume the input to be two pieces of text : T 1 and T 2 as defined by any of the setups illustrated in section 2.1 . ?
SynonymsOverlap :
Before computing this feature , we first apply light text normalization to T 1 and T 2 including special character ( e.g. , ' , ' , '. ' , etc. ) and diacritics removal .
The feature is then computed as the portion of T 1 terms that have a synonym or the original term in T 2 . Synonyms are extracted from the Arabic WordNet .
1 1 Described here : http://bit.ly/2mzfc7X
To compute the remaining features , we normalize T 1 and T 2 following the same approach when computing SynonymsOverlap feature .
We also apply preprocessing steps including stemming and stopwords removal .
? LMScore :
The language model score is computed as the Dirichlet -smoothed loglikelihood score of generating T 1 given T 2 .
The score is computed using the following equation : LM Score ( T 1 , T 2 ) = w?T 1 tf w , T 1 log tf w , T 2 + ?P ( w|C ) | T 2 | + ? ( 1 ) where tf w , T 1 and tf w , T 2 is the frequency of term w in T 1 and T 2 respectively .
P ( w|C ) is the background language model computed using the maximum likelihood estimate with term statistics extracted from a recent large-scale crawl of the Arabic Web called ArabicWeb16 ( Suwaileh et al. , 2016 ) .
We set ? to 2000 as this is the default value used in Lucene 's language modeling retrieval model .
2 ? CosineSimialirty .
This feature computes the cosine similarity between T 1 and T 2 as follows .
CS ( T 1 , T 2 ) = T 1 ? T 2 || T 1 || || T 2 || ( 2 ) where T 1 and T 2 is the vector representation of T 1 and T 2 respectively and || T 1 || and || T 2 || is the Euclidean lengths of vectors T 1 and T 2 .
We represent texts as vectors using TF - IDF representation where term statistics are extracted from ArabicWeb16 ( Suwaileh et al. , 2016 ) . ? JaccardSimialirty .
This feature computes the Jaccard similarity between T 1 and T 2 as follows .
JS ( T 1 , T 2 ) = | T 1 ? T 2 | | T 1 ? T 2 | ( 3 ) ? JaccardSimialirtyV1 .
This is a variant of Jaccard similarity computed as follows .
JS 1 ( T 1 , T 2 ) = | T 1 ? T 2 | | T 1 | ( 4 ) ? JaccardSimialirtyV2 .
This is a second variant of Jaccard similarity computed as follows .
JS 2 ( T 1 , T 2 ) = | T 1 ? T 2 | | T 2 | ( 5 )
Semantic word2vec Similarity Features
Every text snippet T has a set of words .
Each word has a fixed - length word embedding representation , w ?
R d , where d is the dimensionality of the word embedding .
Thus for a text snippet T we define T = {w 1 , ? ? ? , w k } , where k is the number of words in T .
The word embedding representation is computed offline following Mikolov et al.
approach ( Mikolov et al. , 2013 ) .
To compute similarity scores , we represent each text snippet by a feature vector ; different alternatives for feature representations are adopted as described next .
Average Word Embedding Similarity
For a text snippet T that has k words , we compute the average vector as follows : T ? = k i=1 ( w i ) k ( 6 ) Notice that T ? =? R d .
This leads to the following cosine similarity feature .
CS ? ( T 1 , T 2 ) = T ? 1 ? T ? 2 || T ? 1 || || T ? 2 || ( 7 )
Covariance Word Embedding Similarity Instead of computing the average vector , we can compute a covariance matrix C ? R d?d .
The covariance matrix C is computed by treating each dimension as a random variable and every entry in C u , v is the covariance between the pair of variables ( u , v ) .
The covariance between two random variables u and v is computed as in eq. 8 , where k is the number of observations ( words ) .
C u, v = k i=1 ( u i ? ? ) ( v i ? v) k ? 1 ( 8 ) The matrix C ? R d?d is a symmetric matrix .
We compute a vectorized representation of the matrix C as the stacking of the lower triangular part of matrix C as in eq .
9 . This process produces a vector T Cov ? R d?( d+1 ) / 2 T Cov =vect ( C ) = { Cu , v:u?{1 , ? , d} , v?{u , ? , d}} ( 9 ) This leads to the following cosine similarity feature .
CS Cov ( T 1 , T 2 ) = T Cov 1 ? T Cov 2 || T Cov 1 || || T Cov 2 || ( 10 )
Ranking Using SVM
Although Subtask
D is a re-ranking task , it has also a classification task where answers need to be ranked and labeled with either true or false ; the former designates a Direct or Relevant answer to the new question , and the latter designates an Irrelevant answer .
In our last year 's submission ( Malhas et al. , 2016 ) we used learning - to - rank module for re-ranking pairs , but we used a simple heuristic to give labels to the candidate question - answer pairs .
This year we use SVM to give a label for every candidate pair using the SVM model .
In addition to labeling pairs , we use the decision scores from the SVM model for re-ranking the candidate question - answer pairs .
Experimental Evaluation
In this section we present the experimental setup and results of our primary , contrastive - 1 and contrastive - 2 submissions .
Experimental Setup
We used the Arabic collection of questions and their potentially related question - answer pairs provided by Task 3 organizers to train our word embedding model .
The Gensim 3 tool was used to generate the word2vec model from training data 4 , setting d = 100 .
We used the learned model to compute our features as described in section 2 .
Features were generated for the three data setups described in section 2.1 .
Submissions and Results
The differences among our submitted runs is based on the selection of the features .
In all cases we use linear SVM for classifying and ranking questionanswer pairs .
Details on our official submissions Primary .
We use the full set of similarity features defined in section 2.2 and section 2.3 .
In addition , we performed a weighted score fusion with an SVM model based on fixed length representation using Covariance word embedding .
The feature vectors we used are computed using equation 9 .
We tuned the model weights using the development set .
Discussion ?
Our best official submission is Contrastive - 2 using both term-based similarity features and semantic word2vec similarity features .
This indicates that the two similarity features types are complementing each other . ?
Our results justify the usage of SVM model for labeling and re-ranking question - answer pairs .
This is clear in the P , R , F1 and Acc scores reported across all other baselines .
We report very competitive MAP scores to the best performing ranking systems which are not using any form of labeling such as IRbaseline .
?
Score fusion in our primary run did not achieve best results on the official test set while it was the best run in our experiments on the development set .
We believe that this happened due to the difference in the source of question - answer pairs in the development set compared to the the official test set where the test set contains only medical questions .
Conclusion
This paper describes the system we developed to participate in SemEval - 2017 Task 3 on Community Question Answering .
Our system has focused on the Arabic Subtask D which is confined to Answer Selection in Community Question Answering , i.e. , finding good answers for a given new question .
We have adopted a supervised learning approach where linear SVM models were trained over similarity features .
In our best submission , term- based similarity features and word2vec similarity features were both used ; our system ranked second among the other participating teams .
Figure 1 : 1 Figure 1 : A question and 4 of its given 30 candidate QApairs
