title
Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question Answering
abstract
Existing work that augment question answering ( QA ) models with external knowledge ( e.g. , knowledge graphs ) either struggle to model multi-hop relations efficiently , or lack transparency into the model 's prediction rationale .
In this paper , we propose a novel knowledge - aware approach that equips pretrained language models ( PTLMs ) with a multi-hop relational reasoning module , named multi-hop graph relation network ( MHGRN ) .
It performs multi-hop , multi-relational reasoning over subgraphs extracted from external knowledge graphs .
The proposed reasoning module unifies path- based reasoning methods and graph neural networks and results in better interpretability and scalability .
We also empirically show its effectiveness and scalability on CommonsenseQA and OpenbookQA datasets , and interpret its behaviors with case studies , with the code for experiments released 1 .
Introduction
Many recently proposed question answering tasks require not only machine comprehension of the question and context , but also relational reasoning over entities ( concepts ) and their relationships by referencing external knowledge ( Talmor et al. , 2019 ; Sap et al. , 2019 ; . For example , the question in Fig.
1 requires a model to perform relational reasoning over mentioned entities , i.e. , to infer latent relations among the concepts : { CHILD , SIT , DESK , SCHOOLROOM } .
Background knowledge such as " a child is likely to appear in a schoolroom " may not be readily contained in the questions themselves , but are commonsensical to humans .
Despite the success of large-scale pre-trained language models ( PTLMs ) ( Devlin et al. , 2019 ; ?
The first two authors contributed equally .
The major work was done when both authors interned at USC .
Liu et al. , 2019 b ) , these models fall short of providing interpretable predictions , as the knowledge in their pre-training corpus is not explicitly stated , but rather is implicitly learned .
It is thus difficult to recover the evidence used in the reasoning process .
This has led many to leverage knowledge graphs ( KGs ) ( Mihaylov and Frank , 2018 ; Lin et al. , 2019 ; Yang et al. , 2019 ) .
KGs represent relational knowledge between entities with multi-relational edges for models to acquire .
Incorporating KGs brings the potential of interpretable and trustworthy predictions , as the knowledge is now explicitly stated .
For example , in Fig. 1 , the relational path ( CHILD AtLocation CLASSROOM Synonym SCHOOLROOM ) naturally provides evidence for the answer SCHOOLROOM .
A straightforward approach to leveraging a knowledge graph is to directly model these relational paths .
KagNet ( Lin et al. , 2019 ) and MH -PGM ( Bauer et al. , 2018 ) extract multi-hop relational paths from KG and encode them with sequence models .
Application of attention mechanisms upon these relational paths can further offer good interpretability .
However , these models are hardly scalable because the number of possible paths in a graph is ( 1 ) polynomial w.r.t. the number of nodes ( 2 ) exponential w.r.t. the path length ( see Fig. 2 ) .
Therefore , some ( Weissenborn et al. , 2017 ; Mihaylov and Frank , 2018 ) resort to only using one - hop paths , namely , triples , to balance scalability and reasoning capacities .
Graph neural networks ( GNNs ) , in contrast , enjoy better scalability via their message passing formulation , but usually lack transparency .
The most commonly used GNN variant , Graph Convolutional Networks ( GCNs ) ( Kipf and Welling , 2017 ) , perform message passing by aggregating neighborhood information for each node , but ignore the relation types .
RGCNs ( Schlichtkrull et al. , 2018 ) generalize GCNs by performing relationspecific aggregation , making it applicable to multirelational graphs .
However , these models do not distinguish the importance of different neighbors or relation types and thus cannot provide explicit relational paths for model behavior interpretation .
In this paper , we propose a novel graph encoding architecture , Multi-hop Graph Relation Network ( MHGRN ) , which combines the strengths of path- based models and GNNs .
Our model inherits scalability from GNNs by preserving the message passing formulation .
It also enjoys interpretability of path- based models by incorporating structured relational attention mechanism .
Towards multi-hop relational reasoning , our key motivation is to allow each node to directly attend to its multi-hop neighbors by performing multi-hop message passing within a single layer .
We outline the desired merits of knowledge - aware QA models in Table 1 and compare MHGRN with them .
We summarize the main contributions of this work as follows : 1 ) We propose MHGRN , a novel model architecture tailored to multi-hop relational reasoning , which explicitly models multi-hop relational paths at scale .
2 ) We propose a structured relational attention mechanism for efficient and interpretable modeling of multi-hop reasoning paths , along with its training and inference algorithms .
3 ) We conduct extensive experiments on two question answering datasets and show that our models bring significant improvements compared to knowledgeagnostic PTLMs , and outperform other graph encoding methods by a large margin .
Problem Formulation and Overview
In this paper , we limit the scope to the task of multiple -choice question answering , although the formulation can be easily generalized to other knowledge - guided tasks ( e.g. , natural language inference ) .
The overall paradigm of knowledgeaware QA is illustrated in Fig. 3 . Formally , given a question q and an external knowledge graph ( KG ) as the knowledge source , our goal is to identify the correct answer from a set C of given options .
We turn this problem into measuring the plausibility score between q and each option a " C , after which we select the option with the highest plausibility score .
To measure the score for q and a , we first concatenate q and a to form a statement s = [ q ; a ] and encode the statement s into the statement representation s .
Then we extract from the external KG a subgraph G ( i.e. , schema graph in KagNet ( Lin et al. , 2019 ) ) , with the guidance of s ( detailed in ?5.1 ) .
This contextualized subgraph is defined as a multi-relational graph G = ( V , E , ) .
Here V is a subset of entities in the external KG , containing only those relevant to s. E N V ? R ?
V is the set of edges that connect nodes in V , where R = { 1 , ? , m} are the ids of all pre-defined relation types .
The mapping function ( i ) ?
V T = { E q , E a , E o } takes node i "
V as input , and outputs E q if i is an entity mentioned in q , E a if it is mentioned in a , and E o otherwise 2 . Finally , we encode G into g , and concatenate s and g to calculate the plausibility score .
3 Background : Multi-Relational Graph Encoding Methods
We leave encoding of s to pre-trained language models and focus on the challenge of encoding graph G to capture latent relations between entities .
Current methods for encoding multi-relational graphs mainly fall into two categories : GNNs and path- based models .
GNNs encode structured information by passing messages between nodes , directly operating on the graph structure , while pathbased methods first decompose the graph into paths and then pool their representations to form a graph representation .
Graph Encoding with GNNs .
For a graph with n nodes , a graph neural network ( GNN ) takes a set of node features {h 1 , h 2 , . . . , h n } as input , and computes their corresponding node embeddings {h ?1 , h ?2 , . . . , h ?n} via message passing ( Gilmer et al. , 2017 ) .
A compact graph representation for G can thus be obtained by pooling the node embeddings {h ?i} : GNN ( G ) = Pool ( { h ?1 , h ?2 , . . . , h ?n} ) .
( 1 ) As a notable variant of GNNs , graph convolutional networks ( GCNs ) ( Kipf and Welling , 2017 ) additionally update node embeddings by aggregating messages from its direct neighbors .
RGCNs ( Schlichtkrull et al. , 2018 ) extend GCNs to encode multi-relational graphs by defining relationspecific weight matrix W r for each edge type : h ?i = ? ? = r" R ?N r i ?
1 = r" R = j" N r i W r h j ? ? , ( 2 ) where N r i denotes neighbors of node i under relation r. 3 While GNNs have proved to have good scalability , their reasoning is done at the node level , and are therefore incompatible with path modeling .
This property also hinders path- level interpretation of the model 's decisions .
Graph Encoding with Path - Based Models .
In addition to directly modeling the graph with GNNs , a graph can also be viewed as a set of relational paths connecting pairs of entities .
Relation Networks ( RNs ) ( Santoro et al. , 2017 ) can be adapted to multi-relational graph encoding under QA settings .
RNs use MLPs to encode all triples ( one - hop paths ) in G whose head entity is in Q = {j ? ( j ) = E q } and tail entity is in A = { i ?
( i ) = E a } .
It then pools the triple embeddings to generate a vector for G as follows , RN ( G ) = Pool ? { MLP ( h j h e r h h i ) ? j "
Q , i " A , ( j , r , i ) " E} . ( 3 )
Here h j and h i are features for nodes j and i , e r is the embedding of relation r " R , h denotes vector concatenation .
To further equip RN with the ability to model nondegenerate paths , KagNet ( Lin et al. , 2019 ) adopts LSTMs to encode all paths connecting question entities and answer entities with lengths no more than K .
It then aggregates all path embeddings via the attention mechanism : KAGNET ( G ) = Pool ? { LSTM ( j , r 1 , j 1 , . . . , r k , i ) ? ( j , r 1 , j 1 ) , ? , ( j k 1 , r k , i ) " E , 1 & k & K} . ( 4 )
Proposed Method : Multi-Hop Graph Relation Network ( MHGRN )
This section presents Multi-hop Graph Relation Network ( MHGRN ) , a novel GNN architecture that unifies both GNNs and path - based models .
MHGRN inherits path- level reasoning and interpretabilty from path - based models , while preserving good scalability of GNNs .
MHGRN : Model Architecture
We follow the GNN framework introduced in ?3 , where node features can be initialized with pretrained weights ( details in Appendix B ) .
Here we focus on the computation of node embeddings .
Type-Specific Transformation .
To make our model aware of the node type , we first perform node type specific linear transformation on the input node features : x i = U ( i ) h i + b ( i ) , ( 5 ) where the learnable parameters U and b are specific to the type of node i. Multi-Hop Message Passing .
As mentioned before , our motivation is to endow GNNs with the capability of directly modeling paths .
To this end , we propose to pass messages directly over all the relational paths of lengths up to K .
The set of valid k-hop relational paths is defined as : k = {( j , r 1 , . . . , r k , i ) ? ( j , r 1 , j 1 ) , ? , ( j k 1 , r k , i ) " E} ( 1 & k & K ) .
( 6 ) We perform k-hop ( 1 & k & K ) message passing over these paths , which is a generalization of the single - hop message passing in RGCNs ( see Eq. 2 ) : z k i = = ( j, r 1 , ... , r k , i ) " k ?( j , r 1 , . . . , r k , i ) / d k i W K 0 ?W k+1 0 W k r k ?W 1 r 1 x j ( 1 & k & K ) , ( 7 ) where the W t r ( 1 & t & K , 0 & r & m ) matrices are learnable 4 , ?( j , r 1 , . . . , r k , i ) is an attention score elaborated in ?4.2 and d k i = < ( j?i ) " k ?( j?i ) is the normalization factor .
The { W k r k ?W 1 r 1 ? 1 & r 1 , . . . , r k & m} matrices can be interpreted as the low rank approximation of a { m?m } k ?d?d tensor that assigns a separate transformation for each k-hop relation , where d is the dimension of x i .
Incoming messages from paths of different lengths are aggregated via attention mechanism ( Vaswani et al. , 2017 ) : z i = K = k=1 softmax bilinear s , z k i ?
z k i .
( 8 ) Non-linear Activation .
Finally , we apply shortcut connection and nonlinear activation to obtain the output node embeddings .
h ?i = ?V h i + V ?zi , ( 9 ) where V and V ?are learnable model parameters , and is a non-linear activation function .
Structured Relational Attention Naive parameterization of the attention score ?( j , r 1 , . . . , r k , i ) in Eq. 7 would require O( m k ) parameters for k-hop paths .
Towards efficiency , we first regard it as the probability of a relation sequence ( ( j ) , r 1 , . . . , r k , ( i ) ) conditioned on s : which can naturally be modeled by a probabilistic graphical model , such as conditional random field ( Lafferty et al. , 2001 ) : ?( j , r 1 , . . . , r k , i ) = p ( ( j ) , r 1 , . . . , r k , ( i ) ? s ) , ( 10 ) p ( ? ? s ) ? exp ?f ( ( j ) , s ) + k = t=1 ( r t , s ) + k 1 = t=1 ? ( r t , r t +1 ) + g ( ( i ) , s ) ?
= ( r 1 , . . . , r k , s ) ? " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " - " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " oe Relation Type Attention ( ( j ) , ( i ) , s ) ? " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " - " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " oe Node Type Attention , ( 11 ) where f ( ) , ( ) and g ( ) are parameterized by two -layer MLPs and ? ( ) by a transition matrix of shape m ?
m. Intuitively , ( ) models the importance of a k-hop relation while ( ) models the importance of messages from node type ( j ) to ( i ) ( e.g. , the model can learn to pass messages only from question entities to answer entities ) .
Our model scores a k-hop relation by decomposing it into both context - aware single - hop relations ( modeled by ) and two -hop relations ( modeled by ? ) .
We argue that ? is indispensable , without which the model may assign high importance to illogical multi-hop relations ( e.g. , [ AtLocation , CapableOf ] ) or noisy relations ( e.g. , [ RelatedTo , RelatedTo ] ) .
Computation Complexity Analysis
Although the message passing process in Eq. 7 and the attention module in Eq.11 handle potentially exponential number of paths , computation can be done in linear time with the help of dynamic programming ( see Appendix C ) .
As summarized in Table 2 , both the time complexity and space complexity of MHGRN on a sparse graph are linear w.r.t. the maximum path length K or the number of nodes n.
Expressive Power of MHGRN
In addition to efficiency and scalability , we now discuss the modeling capacity of MHGRN .
With the message passing formulation and relation -specific transformations , MHGRN is by nature the generalization of RGCN .
It is also capable of directly modeling paths , making it interpretable as are pathbased models such as RN and KagNet .
To show this , we first generalize RN ( Eq. 3 ) to the multi-hop setting and introduce K-hop RN ( formal definition in Appendix D ) , which models multi-hop relation as the composition of single - hop relations .
We
Model Time Space G is a dense graph K-hop KagNet O ?m K n K+1 K O ?m K n K+1 K K-layer RGCN O ?mn 2 K O ( mnK ) MHGRN O ?m 2 n 2 K O ( mnK ) G is a sparse graph with maximum node degree 8 n can show that MHGRN is capable of representing K-hop RN ( proof in Appendix E ) .
K-hop KagNet O ?m K nK K O ?m K nK K K-layer RGCN O ( mnK ) O ( mnK ) MHGRN O ?m 2 nK O ( mnK )
Learning , Inference and Path Decoding
We now discuss the learning and inference process of MHGRN instantiated for QA tasks .
Following the problem formulation in ?2 , we aim to determine the plausibility of an answer option a " C given the question q with the information from both text s and graph G .
We first obtain the graph representation g by performing attentive pooling over the output node embeddings of answer entities { h ?i ? i " A} .
Next we concatenate it with the text representation s and compute the plausibility score by ?( q , a ) = MLP ( s h g ) .
During training , we maximize the plausibility score of the correct answer ? by minimizing the cross-entropy loss : L = E q , ? , C log exp (?( q , a ) ) < a" C exp ( ?( q , a ) ) ? . ( 12 ) The whole model is trained end-to - end jointly with the text encoder ( e.g. , RoBERTa ) .
During inference , we predict the most plausible answer by argmax a " C ?( q , a ) .
Additionally , we can decode a reasoning path as evidence for model predictions , endowing our model with the interpretability enjoyed by path - based models .
Specifically , we first determine the answer entity i ? with the highest score in the pooling layer and the path length k ? with the highest score in Eq. 8 .
Then the reasoning path is decoded by argmax ?( j , r 1 , . . . , r k ? , i ? ) , which can be computed in linear time using dynamic programming .
Experimental Setup
We introduce how we construct G ( ?5.1 ) , the datasets ( ?5.2 ) , as well as the baseline methods ( ?5.3 ) .
Appendix
B shows more implementation and experimental details for reproducibility .
G from KG , we recognize entity mentions in s and link them to entities in ConceptNet , with which we initialize our node set V .
We then add to V all the entities that appear in any two -hop paths between pairs of mentioned entities .
Unlike KagNet , we do not perform any pruning but instead reserve all the edges between nodes in V , forming our G .
Extracting
Datasets
We evaluate models on two multiple -choice question answering datasets , CommonsenseQA and OpenbookQA .
Both require world knowledge beyond textual understanding to perform well .
CommonsenseQA ( Talmor et al. , 2019 ) necessitates various commonsense reasoning skills .
The questions are created with entities from Concept - Net .
It is noteworthy that although Common-senseQA is built upon ConceptNet , its questions are designed to probe multi-hop / compositional relations between entities that cannot be directly read from , or are even absent from ConceptNet .
This de -5 Models based on ConceptNet are no longer shown on the leaderboard , and we got our results from the organizers . ) .
We also make use of the official split of CommonsenseQA , denoted as CommonsenseQA ( OF ) .
The numbers of instances in different dataset splits are listed in Table 5 .
Methods
Compared Methods
We implement both knowledge - agnostic finetuning of pre-trained LMs and models that incorporate external KG as our baselines .
Additionally , we directly compare our model with the results from corresponding leaderboard .
These methods typically leverage textual knowledge or extra training data , as opposed to external KG .
In all our implemented models , we use pre-trained LMs as text encoders for s for fair comparison .
We do compare our models with those ( Ma et al. , 2019 ; Lv et al. , 2019 ; Khashabi et al. , 2020 ) augmented by other text - form external knowledge ( e.g. , Wikipedia ) , although we stick to our focus of encoding structured KG .
Specifically , we fine- tune BERT - BASE , BERT - LARGE ( Devlin et al. , 2019 ) , and ROBERTA ( Liu et al. , 2019 b ) for multiple -choice questions .
We take RGCN ( Eq. 2 in ?3 ) , RN 8 ( Eq. 3 in ?3 ) , KagNet ( Eq. 4 in ?3 ) and GconAttn as baselines .
GconAttn generalizes match -LSTM ( Wang and Jiang , 2016 ) and achieves success in language inference tasks .
Results and Discussions
In this section , we present the results of our models in comparison with baselines as well as methods on the leaderboards for both CommonsenseQA and OpenbookQA .
We also provide analysis of models ' components and characteristics .
Main Results For CommonsenseQA ( Table 3 ) , we first use the in-house data split ( IH ) ( see ?5.2 ) to compare our models with implemented baselines .
This is different from the official split used in the leaderboard methods .
Almost all KG - augmented models achieve performance gain over vanilla pre-trained LMs , demonstrating the value of external knowledge on this dataset .
Additionally , we evaluate For OpenbookQA ( Table 6 ) , we use official split and build models with ROBERTA - LARGE as text encoder .
MHGRN surpasses all implemented baselines , with an absolute increase of ?2 % on Test .
Also , as our approach is naturally compatible with the methods that utilize textual knowledge or extra data , because in our paradigm the encoding of textual statement and graph are structurallydecoupled ( Fig. 3 ) .
To empirically show MHGRN can bring gain over textual - knowledge empowered systems , we replace our text encoder with AristoRoBERTaV7 10 , and fine-tune our MHGRN upon OpenbookQA .
Empirically , MHGRN continues to bring benefits to strong - performing textualknowledge empowered systems .
One takeaway is that textual knowledge and structured knowledge are potentially complementary .
Performance Analysis Ablation Study on Model Components .
As shown in Table 7 , disabling type-specific transformation results in ? 1.3 % drop in performance , demonstrating the need for distinguishing node types for QA tasks .
Our structured relational attention mechanism is also critical , with its two Impact of Number of Hops ( K ) .
We investigate the impact of hyperparameter K for MHGRN with experiments on CommonsenseQA ( Fig. 6 ) .
The increase of K continues to bring benefits until K = 4 .
However , performance begins to drop when K > 3 .
This might be attributed to exponential noise in longer relational paths in the knowledge graph .
is m times that of RGCN , the ratio of their empirical cost only approaches 2 , demonstrating that our model can be better parallelized .
It is noteworthy that the text encoder part actually dominates the overall cost .
Therefore , the gap between our model and the RGCN are further narrowed if we consider the cost of the entire model .
Model Scalability
Model Interpretability
We can analyze our model 's reasoning process by decoding the reasoning path using the method described in ?4.5 .
Fig. 8 shows two examples from CommonsenseQA , where our model correctly answers the questions and provides reasonable path evidences .
In the example on the left , the model links question entities and answer entity in a chain to support reasoning , while the example on the right provides a case where our model leverage unmentioned entities to bridge the reasoning gap between question entity and answer entities , in a way that is coherent with the latent relation between CHAPEL and the desired answer in the question .
Where is known for a multitude of wedding chapels ?
A ( Yang and Mitchell , 2017 ; , triples ( Weissenborn et al. , 2017 ; Mihaylov and Frank , 2018 ) , paths ( Bauer et al. , 2018 ; Kundu et al. , 2019 ; Lin et al. , 2019 ) , or subgraphs ( Li and Clark , 2015 ) , and encode them to augment textual understanding .
Recent success of pre-trained LMs motivates many ( Pan et al. , 2019 ; Ye et al. , 2019 ; Zhang et al. , 2018 ; Banerjee et al. , 2019 ) to probe LMs ' potential as latent knowledge bases .
This line of work turn to textual knowledge ( e.g. Wikipedia ) to directly impart knowledge to pre-trained LMs .
They generally fall into two paradigms :
1 ) Finetuning LMs on large-scale general- domain datasets ( e.g. RACE ( Lai et al. , 2017 ) ) or on knowledge - rich text .
2 ) Providing LMs with evidence via information retrieval techniques .
However , these models cannot provide explicit reasoning and evidence , thus hardly trustworthy .
They are also subject to the availability of in-domain datasets and maximum input token of pre-trained LMs .
Neural Graph Encoding Graph Attention Networks ( GAT ) ( Velickovic et al. , 2018 ) incorporates attention mechanism in feature aggregation , RGCN ( Schlichtkrull et al. , 2018 ) proposes relational message passing which makes it applicable to multi-relational graphs .
However they only perform single - hop message passing and cannot be interpreted at path level .
Other work ( Abu -El - Haija et al. , 2019 ; Nikolentzos et al. , 2019 ) aggregate for a node its K-hop neighbors based on node-wise distances , but they are designed for non-relational graphs .
MHGRN addresses these issues by reasoning on multi-relational graphs and being interpretable via maintaining paths as reasoning chains .
Conclusion
We present a principled , scalable method , MHGRN , that can leverage general knowledge via multi-hop reasoning over interpretable structures ( e.g. Con-ceptNet ) .
The proposed MHGRN generalizes and combines the advantages of GNNs and path - based reasoning models .
It explicitly performs multi-hop relational reasoning and is empirically shown to outperform existing methods with superior scalablility and interpretability .
We merge relations that are close in semantics as well as in the general usage of triple instances in ConceptNet .
Our models are implemented in PyTorch .
We use cross-entropy loss and RAdam ( Liu et al. , 2019a ) optimizer .
We find it beneficial to use separate learning rates for the text encoder and the graph encoder .
We tune learning rates for text encoders and 9 .
We report the performance of these fine-tuned text encoders and also adopt their dataset -specific optimal learning rates in joint training with graph encoders .
For models that involve KG , the learning rate of their graph encoders are chosen from { 1 ? 10 4 , 3 ? 10 4 , 1 ? 10 3 , 3 ? 10 3 } , based on their best development set performance with ROBERTA - LARGE as the text encoder .
We report the optimal learning rates for graph encoders in Table 10 .
In training , we set the maximum input sequence length to text encoders to 64 , batch size to 32 , and perform early stopping .
AristoRoBERTaV7 +MHGRN is the only exception .
