title
Efficient One -Pass End-to- End Entity Linking for Questions
abstract
We present ELQ , a fast end-to - end entity linking model for questions , which uses a biencoder to jointly perform mention detection and linking in one pass .
Evaluated on WebQSP and GraphQuestions with extended annotations that cover multiple entities per question , ELQ outperforms the previous state of the art by a large margin of + 12.7 % and + 19.6 % F1 , respectively .
With a very fast inference time ( 1.57 examples / s on a single CPU ) , ELQ can be useful for downstream question answering systems .
In a proof-of-concept experiment , we demonstrate that using ELQ significantly improves the downstream QA performance of GraphRetriever ( Min et al. , 2019 ) . 1
Introduction Entity linking ( EL ) , the task of identifying entities and mapping them to the correct entries in a database , is crucial for analyzing factoid questions and for building robust question answering ( QA ) systems .
For instance , the question " when did shaq come to the nba ? " can be answered by examining Shaquille O'Neal 's Wikipedia article ( Min et al. , 2019 ) , or its properties in a knowledge graph ( Yih et al. , 2015 ; Yu et al. , 2017 ) .
However , real-world user questions are invariably noisy and ill-formed , lacking cues provided by casing and punctuation , which prove challenging to current end-to - end entity linking systems ( Yang and Chang , 2015 ; Sorokin and Gurevych , 2018 ) .
While recent pre-trained models have proven highly effective for entity linking ( Logeswaran et al. , 2019 ; , they are only designed for entity disambiguation and require mention boundaries to be given in the input .
Additionally , such systems Figure 1 : Overview of our end-to - end entity linking system .
We separately encode the question and entity .
We use the question representations to jointly detect mentions and score candidate entities through innerproduct with the entity vector .
have only been evaluated on long , well - formed documents like news articles ( Ji et al. , 2010 ) , but not on short , noisy text .
Also , most prior works have focused mainly on improving model prediction accuracy , largely overlooking efficiency .
In this work , we propose ELQ , a fast and accurate entity linking system that specifically targets questions .
Following the Wikification setup ( Ratinov et al. , 2011 ) , ELQ aims to identify the mention boundaries of entities in a given question and their corresponding Wikipedia entity .
We employ a biencoder based on BERT as shown in Figure 1 .
The entity encoder computes entity embeddings for all entities in Wikipedia , using their short descriptions .
Then , the question encoder derives token - level embeddings for the input question .
We detect mention boundaries using these embeddings , and disambiguate each entity mention based on an inner product between the mention embeddings ( averaged embedding over mention tokens ) and the entity embeddings .
Our model ex-tends the work of but with one major difference : our system does not require prespecified mention boundaries in the input , and is able to jointly perform mention detection and entity disambiguation in just one pass of BERT .
Thus , at inference time , we are able to identify multiple entities in the input question efficiently .
We extend entity disambiguation annotations from Sorokin and Gurevych ( 2018 ) to create an endto-end question entity linking benchmark .
Evaluated on this benchmark , we are able to outperform previous methods in both accuracy and run-time .
ELQ has much faster end-to - end inference time than any other neural baseline ( by 2 ? ) , while being more accurate than all previous models we evaluate against , suggesting that it is practically useful for downstream QA systems .
We verify the applicability of ELQ to practical QA models in a proofof-concept experiment , by augmenting GraphRetriever ( Min et al. , 2019 ) to use our model , improving its downstream QA performance on three open-domain QA datasets ( by up to 6 % ) .
Related Work Much prior work on entity linking has focused on long , grammatically coherent documents that contain many entities .
This setting does not accurately reflect the difficulties of entity linking on questions .
While there has been some previous work on entity linking for questions ( Sorokin and Gurevych , 2018 ; Blanco et al. , 2015 ; Chen et al. , 2018 ; Tan et al. , 2017 ) , such works ( mostly from the pre-BERT era ) utilize complex models with many interworking modules .
For example , Sorokin and Gurevych ( 2018 ) proposes a variable - context granularity ( VCG ) model to address the noise and lack of context in questions , which incorporates signals from various levels of granularity by using character - level , token - level , and knowledge - baselevel modules .
They also rely on external systems as a part of the modeling pipeline .
In this work , we take a much simpler approach that uses a biencoder .
Biencoder models have been used in a wide range of tasks ( Seo et al. , 2019 ; Karpukhin et al. , 2020 ; . They enable fast inference time through maximum inner product search .
Moreover , as we find , biencoders can be decomposed into reusable question and entity encoders , and we can greatly expedite training by training one component independently of the other .
Problem Definition & ELQ Model
We formally define our entity linking task as follows .
Given a question q and a set of entities E = {e i } from Wikipedia , each with titles t( e i ) and text descriptions d( e i ) , our goal is to output a list of tuples , ( e , [ m s , m e ] ) , whereby e ?
E is the entity corresponding to the mention span from the m s -th to m e -th token in q .
In practice , we take the title and first 128 tokens of the entity 's Wikipedia article as its title t( e i ) and description d( e i ) .
We propose an end-to- end entity linking system that performs both mention detection and entity disambiguation on questions in one pass of BERT .
Given an input question q = q 1 ? q n of length n , we first obtain question token representations based on BERT : [ q 1 ? q n ] = BERT ( [ CLS ] q 1 ? q n [ SEP ] ) ?
R n?h , where each q i is a h-dimensional vector .
We then obtain entity representations x e for every e i ? E. x e = BERT [ CLS ] ( [ CLS ] t ( e i ) [ ENT ] d( e i ) [ SEP ] ) ?
R h , where [ CLS ] indicates that we select the representation of the [ CLS ] token .
We consider candidate mentions as all spans [ i , j ] ( i- th to j-th tokens of q ) in the text up to length L.
Mention Detection
To compute the likelihood score of a candidate span [ i , j] being an entity mention , we first obtain scores for each token being the start or the end of a mention : s start ( i ) = w start q i , s end ( j ) = w end q j , where w start , w end ?
R h are learnable vectors .
We additionally compute scores for each token t being part of a mention : s mention ( t ) = w mention q t , where w mention ?
R h is a learnable vector .
We finally compute mention probabilities as : p( [ i , j ] ) = ?( s start ( i ) + s end ( j ) + j t=i s mention ( t ) ) .
Entity Disambiguation
We obtain a mention representation for each mention candidate [ i , j] by averaging q i ?
q j , and compute a similarity score s between the mention candidate and an entity candidate e ?
E : y i , j = 1 ( j ? i + 1 ) j t=i q t ? R h , s( e , [ i , j ] ) = x e y i , j .
We then compute a likelihood distribution over all entities , conditioned on the mention [ i , j ] : p( e | [ i , j ] ) = exp ( s( e , [ i , j ] ) ) e ?E exp ( s( e , [ i , j ] ) ) .
Training
We jointly train the mention detection and entity disambiguation components by optimizing the sum of their losses .
We use a binary crossentropy loss across all mention candidates : L MD = ? 1 N 1 ?i?j? min( i+L?1 , n ) y [ i , j ] log p ( [ i , j ] ) + ( 1 ? y [ i , j ] ) log ( 1 ? p( [ i , j ] ) ) , whereby y [ i , j ] = 1 if [ i , j ] is a gold mention span , and 0 otherwise .
N is the total number of candidates we consider .
2
The entity disambiguation loss is given by L ED = ? log p(e g | [ i , j ] ) , where e g is the gold entity corresponding to mention [ i , j ] .
To expedite training , we use a simple transfer learning technique : we take the entity encoder trained on Wikipedia by and freeze its weights , training only the question encoder on QA data .
In addition , we mine hard negatives .
As entity encodings are fixed , a fast search of hard negatives in real time is possible .
Inference Figure 1 shows our inference process .
Given an input question q , we use our mention detection model to obtain our mention set M = { [ i , j ] : 1 ? i ? j ? min( i+L?1 , n ) , p ( [ i , j ] ) > ?} , where ? is our threshold ( a hyperparameter ) .
We then compute p(e , [ i , j ] ) = p( e | [ i , j ] ) p ( [ i , j ] ) for each mention [ i , j ] ?
M , and threshold according to ?.
In contrast to a two -stage pipeline which first extracts mentions , then disambiguates entities ( F?vry et al. , 2020 ) us the flexibility to consider multiple possible candidate mentions for entity linking .
This can be crucial in questions as it can be difficult to extract mentions from short , noisy text in a single step .
More implementation details can be found in Appendix D.
Experiments
Data
We evaluate our approach on two QA datasets , We-bQSP ( Yih et al. , 2016 ) and GraphQuestions ( Su et al. , 2016 ) , with additional entity annotations provided by Sorokin and Gurevych ( 2018 ) .
The original datasets do not have all mention boundary labels annotated .
Therefore , in order to evaluate both mention detection and entity disambiguation , we extend previous labels and create new end-to - end question entity - linking datasets , WebQSP EL and GraphQ EL . 3
In line with our task definition , all entities presented in each question are labeled with ( e , [ m s , m e ] ) , whereby e ?
E is the entity corresponding to the mention span from the m s -th to m e -th token in q .
We ask four in-house annotators to identify corresponding mention boundaries , given gold entities in the questions .
We exclude examples that link to null or no entities , that are not in Wikipedia , or are incorrect or overly generic ( e.g. linking a concept like marry ) .
To check interannotation agreement amongst the 4 annotators , we set aside a shared set of documents ( comprised of documents from both datasets ) that all 4 annotators annotated .
We found exact- match inter-annotator agreement to be 95 % ( 39/41 ) on this shared set .
Table 1 reports the statistics of the resulting datasets , WebQSP EL and GraphQ EL . Following Sorokin and Gurevych ( 2018 ) if the groundtruth entity is identified and the predicted mention boundaries overlap with the groundtruth boundaries .
( This is sometimes known as " weak matching " . )
Specifically , let T be a set of gold entity -mention tuples and T be a set of predicted entity -mention tuples , we define precision ( p ) , recall ( r ) and F1 -score ( F 1 ) as follows : C = e ? E| [ m s , m e ] ? [ m s , m e ] = ? , ( e , [ m s , m e ] ) ?
T , ( e , [ m s , m e ] ) ?
T , p = | C| | T | , r = | C| | T | , F 1 = 2 pr p + r . Baselines
We use the following baselines : ( 1 ) TAGME ( Ferragina and Scaiella , 2012 ) , a lightweight , on - the-fly entity linking system that is popular for many downstream QA tasks , being much faster than most neural models ( Joshi et al. , 2017 ; Sun et al. , 2018 ; Min et al. , 2019 ) , ( 2 ) VCG ( Sorokin and Gurevych , 2018 ) , the current state - of - the - art entity linking system on WebQSP , and ( 3 ) biencoder from BLINK .
As BLINK requires pre-specified mention boundaries as input , we train a separate , BERT - based span extraction model on WebQSP in order to predict mention boundaries ( details in Appendix B ) .
Results
Table 2 show our main results .
We find that BERTbased biencoder models far outperform the stateof - the- art ( VCG ) on both datasets , in performance and in runtime .
Moreover , ELQ outperforms all other models trained in a comparable setting , and is much more efficient than every other neural baseline ( VCG and BLINK ) .
ELQ is also up to 2.3 ?
better than TAGME - in the case of WebQSP EL .
Performance ELQ outperforms BLINK , suggesting that it is possible to train representations ( Min et al. , 2019 ) . from a single model to resolve both entity references as well as mention boundaries of all entities in text , without restricting the model to focusing on a single marked entity as in BLINK .
Runtime
We record the inference speed on CPUs in number of questions processed per second for all models ( Table 2 ) .
For BLINK , we report the combined speed of our span extraction model and the BLINK entity linker , in order to compare the end-to - end speeds .
ELQ , which performs both detection and disambiguation in one pass of BERT , is approximately 2 ? faster than BLINK , which performs multiple passes , while also outperforming BLINK in F1 score .
Moreover , against TAGME ( Ferragina and Scaiella , 2012 ) , ELQ is only 1.5 ? slower on WebQSP EL and 2.0 ? slower on GraphQ EL , despite TAGME being a completely non-neural model ( with much lower accuracy ) .
QA Experiments
To demonstrate the impact of improved entity linking on the end QA accuracy , we experiment with the task of textual open-domain question answering , using GraphRetriever ( GRetriever ) ( Min et al. , 2019 ) .
GRetriever uses entity linking to construct a graph of passages in the retrieval step and deploys a reader model to answer the question .
The original model uses TAGME for entity linking ; we replace TAGME with ELQ and keep the other components the same , in order to isolate the impact of entity linking .
4
As an additional baseline , we also add the result of TF - IDF , implemented by Chen et al . ( 2017 ) , a widely used retrieval system .
Results are shown in Table 3 .
Following literature in open-domain QA , we evaluate our approach on three datasets , WebQuestions ( Berant et al. , 2013 ) , Natural Questions ( Kwiatkowski et al. , 2019 and TriviaQA ( Joshi et al. , 2017 ) .
In particular , WebQuestions ( WQ ) and Natural Questions ( NQ ) consist of short , noisy questions from Web queries , in line with the motivation of our work .
We observe that simply replacing TAGME with ELQ significantly improves performance , including 5.9 % and 3.9 % absolute improvements on WQ and NQ , respectively .
While ELQ trained on Wikipedia achieves good results overall , further fine-tuning on WebQSP EL gives extra gains on WQ .
This indicates that , if entity linking annotations in the same domain are available , using them to finetune ELQ can bring further gains .
Analysis Mention Detector vs. Entity Linker
We set up experiments to disentangle the capability of ELQ 's entity linker and mention detector .
First , to test just the mention detector ( MD only ) , we measure just the mention boundary overlap between predicted and groundtruth mentions , ignoring the entity label .
Next , to test just the entity linker ( EL only ) , we give the entity linking component gold mention boundaries , and compute the resulting F1 score .
We do this for both ELQ and BLINK .
For comparability , we use the version of ELQ trained on Wikipedia .
Results are reported in Table 4 . Surprisingly , we find that both components of ELQ Qualitative
We manually examine all our model 's errors on the WebQSP EL and GraphQ EL dev sets .
We identify four broad error categories : ( 1 ) technically correct - where our model was technically correct but limitations in evaluation falsely penalized our model ( i.e. , we found a more or less precise version of the same entity ) , ( 2 ) not enough entities - where the model did not fully identify all entities in the question , ( 3 ) wrong entities - where our model linked to the wrong entity , ( 4 ) insufficient context - where the model made reasonable mistakes due to the lack of context ( that even reasonable humans would make ) .
Error type breakdowns can be found in Table 5 .
Conclusion
We proposed an end-to- end model for entity linking on questions that jointly performs mention detection and disambiguation with one pass through BERT .
We showed that it is highly efficient , and that it outperforms previous state - of - the - art models on two benchmarks .
Furthermore , when applied to a QA model , ELQ improves that model 's end QA accuracy .
Despite being originally designed with questions in mind , we believe ELQ could also generalize to longer , well - formed documents .
