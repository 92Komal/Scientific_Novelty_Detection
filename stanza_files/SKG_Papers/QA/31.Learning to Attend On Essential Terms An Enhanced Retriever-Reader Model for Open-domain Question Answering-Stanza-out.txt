title
Learning to Attend On Essential Terms : An Enhanced Retriever -Reader Model for Open-domain Question Answering
abstract
Open-domain question answering remains a challenging task as it requires models that are capable of understanding questions and answers , collecting useful information , and reasoning over evidence .
Previous work typically formulates this task as a reading comprehension or entailment problem given evidence retrieved from search engines .
However , existing techniques struggle to retrieve indirectly related evidence when no directly related evidence is provided , especially for complex questions where it is hard to parse precisely what the question asks .
In this paper we propose a retriever - reader model that learns to attend on essential terms during the question answering process .
We build ( 1 ) an essential term selector which first identifies the most important words in a question , then reformulates the query and searches for related evidence ; and ( 2 ) an enhanced reader that distinguishes between essential terms and distracting words to predict the answer .
We evaluate our model on multiple open-domain multiplechoice QA datasets , notably performing at the level of the state - of- the - art on the AI2 Reasoning Challenge ( ARC ) dataset .
Introduction Open-domain question answering ( QA ) has been extensively studied in recent years .
Many existing works have followed the ' search - and - answer ' strategy and achieved strong performance ( Chen et al. , 2017 ; Kwon et al. , 2018 ; Wang et al. , 2018 b ) spanning multiple QA datasets such as TriviaQA ( Joshi et al. , 2017 ) , SQuAD ( Rajpurkar et al. , 2016 ) , MS - Macro ( Nguyen et al. , 2016 ) , ARC among others .
However , open-domain QA tasks become inherently more difficult when ( 1 ) dealing with questions with little available evidence ; ( 2 ) solving questions where the answer type is free-form text ( e.g. multiple-choice ) rather than a span among existing passages ( i.e. , ' answer span ' ) ; or when ( 3 ) the need arises to understand long and complex questions and reason over multiple passages , rather than simple text matching .
As a result , it is essential to incorporate commonsense knowledge or to improve retrieval capability to better capture partially related evidence ( Chen et al. , 2017 ) .
As shown in Table 1 , the TriviaQA , SQuAD , and MS - Macro datasets all provide passages within which the correct answer is guaranteed to exist .
However , this assumption ignores the difficulty of retrieving question - related evidence from a large volume of open-domain resources , especially when considering complex questions which require reasoning or commonsense knowledge .
On the other hand , ARC does not provide passages known to contain the correct answer .
Instead , the task of identifying relevant passages is left to the solver .
However , questions in ARC have multiple answer choices that provide indirect information that can help solve the question .
As such an effective model needs to account for relations among passages , questions , and answer choices .
Real-world datasets such as Amazon - QA ( a corpus of user queries from Amazon ) ( McAuley and Yang , 2016 ) also exhibit the same challenge , i.e. , the need to surface related evidence from which to extract or summarize an answer .
Figure 1 shows an example of a question in the ARC dataset and demonstrates the difficulties in retrieval and reading comprehension .
As shown for Choice 1 ( C1 ) , a simple concatenation of the 1 For SQuAD and TriviaQA , since the questions are paired with span-type answers , it is convenient to obtain ranking supervision where retrieved passages are relevant via distant supervision ; however free-form questions in ARC and Amazon - QA result in a lack of supervision which makes the problem more difficult .
For MS - Macro , the dataset is designed to annotate relevant passages though it has free-form answers .
Dataset # of questions Opendomain
Multiple choice
Passage retrieval
No ranking supervision 1 ARC ? 7 K Amazon-QA ( McAuley and Yang , 2016 ) ? 1.48 M SQuAD ( Rajpurkar et al. , 2016 ) ? 100K TriviaQA ( Joshi et al. , 2017 ) ? 650K MS - Macro ( Nguyen et al. , 2016 ) ? 1M
The planet lacks an atmosphere to hold heat . question and the answer choice is not a reliable query and is of little help when trying to find supporting evidence to answer the question ( e.g. we might retrieve sentences similar to the question or the answer choice , but would struggle to find evidence explaining why the answer choice is correct ) .
On the other hand , a reformulated query consisting of essential terms in the question and Choice 4 can help retrieve evidence explaining why Choice 4 is a correct answer .
To achieve this , the model needs to ( 1 ) ensure that the retrieved evidence supports the fact mentioned in both the question and the answer choices and ( 2 ) capture this information and predict the correct answer .
Retrieving evidence Query4 = Essential-term ( Q ) + C4
To address these difficulties , we propose an essential - term- aware Retriever -Reader ( ET - RR ) model that learns to attend on essential terms during retrieval and reading .
Specifically , we develop a two -stage method with an essential term selector followed by an attention - enhanced reader .
Essential term selector .
ET - Net is a recurrent neural network that seeks to understand the question and select essential terms , i.e. , key words , from the question .
We frame this problem as a classification task for each word in the question .
These essential terms are then concatenated with each answer choice and fed into a retrieval engine to obtain related evidence .
Attention - Enhanced Reader .
Our neural reader takes the triples ( question , answer choice , retrieved passage ) as input .
The reader consists of a sequence of language understanding layers : an input layer , attention layer , sequence modeling layer , fusion layer , and an output layer .
The attention and fusion layers help the model to obtain a refined representation of one text sequence based on the understanding of another , e.g. a passage representation based on an understanding of the question .
We further add a choice-interaction module to handle the semantic relations and differences between answer choices .
Experiments show that this can further improve the model 's accuracy .
We evaluate our model on the ARC Challenge dataset , where our model achieves an accuracy of 36.61 % on the test set , and outperformed all leaderboard solutions at the time of writing ( Sep. 2018 ) .
To compare with other benchmark datasets , we adapt RACE ( Lai et al. , 2017 ) and MCScript ( Ostermann et al. , 2018 ) to the open domain setting by removing their super-vision in the form of relevant passages .
We also consider a large-scale real-world open-domain dataset , Amazon - QA , to evaluate our model 's scalability and to compare against standard benchmarks designed for the open-domain setting .
Experiments on these three datasets show that ET - RR outperforms baseline models by a large margin .
We conduct multiple ablation studies to show the effectiveness of each component of our model .
Finally , we perform in - depth error analysis to explore the model 's limitations .
Related Work
There has recently been growing interest in building better retrievers for open-domain QA .
Wang et al. ( 2018 b ) proposed a Reinforced Ranker - Reader model that ranks retrieved evidence and assigns different weights to evidence prior to processing by the reader .
Min et al. ( 2018 ) demonstrated that for several popular MRC datasets ( e.g. SQuAD , TriviaQA ) most questions can be answered using only a few sentences rather than the entire document .
Motivated by this observation , they built a sentence selector to gather this potential evidence for use by the reader model .
Nishida et al. ( 2018 ) developed a multi-task learning ( MTL ) method for a retriever and reader in order to obtain a strong retriever that considers certain passages including the answer text as positive samples during training .
The proposed MTL framework is still limited to scenarios where it is feasible to discover whether the passages contain the answer span .
Although these works have achieved progress on open-domain QA by improving the ranking or selection of given evidence , few have focused on the scenario where the model needs to start by searching for the evidence itself .
Scientific Question Answering ( SQA ) is a representative open-domain task that requires capability in both retrieval and reading comprehension .
In this paper , we study question answering on the AI2 Reasoning Challenge ( ARC ) scientific QA dataset .
This dataset contains multiple -choice scientific questions from 3rd to 9th grade standardized tests and a large corpus of relevant information gathered from search engines .
The dataset is partitioned into " Challenge " and " Easy " sets .
The challenge set consists of questions that cannot be answered correctly by either of the solvers based on Pointwise Mutual Information ( PMI ) or Information Retrieval ( IR ) .
Existing models tend to achieve only slightly better and sometimes even worse performance than random guessing , which shows that existing models are not well suited to this kind of QA task .
Jansen et al. ( 2017 ) first developed a rule-based focus word extractor to identify essential terms in the question and answer candidates .
The extracted terms are used to aggregate a list of potential answer justifications for each answer candidate .
Experiments shown that focus words are beneficial for SQA on a subset of the ARC dataset .
Khashabi et al. ( 2017 ) also worked on the problem of finding essential terms in a question for solving SQA problems .
They published a dataset containing over 2,200 science questions annotated with essential terms and train multiple classifiers on it .
Similarly , we leverage this dataset to build an essential term selector using a neural network - based algorithm .
More recently , Boratko et al. ( 2018 ) developed a labeling interface to obtain high quality labels for the ARC dataset .
One finding is that human annotators tend to retrieve better evidence after they reformulate the search queries which are originally constructed by a simple concatenation of question and answer choice .
By feeding the evidence obtained by human-reformulated queries into a pre-trained MRC model ( i.e. , DrQA ( Chen et al. , 2017 ) ) they achieved an accuracy increase of 42 % on a subset of 47 questions .
This shows the potential for a " human- like " retriever to boost performance on this task .
Query reformulation has been shown to be effective in information retrieval ( Lavrenko and Croft , 2001 ) . Nogueira and Cho ( 2017 ) modeled the query reformulation task as a binary term selection problem ( i.e. , whether to choose the term in the original query and the documents retrieved using the original query ) .
The selected terms are then concatenated to form the new query .
Instead of choosing relevant words , Buck et al . ( 2018 ) proposed a sequence - to-sequence model to generate new queries .
Das et al. ( 2019 ) proposed Multistep Retriever - Reader which explores an iterative retrieve- and - read strategy for open-domain question answering .
It formulates the query reformulation problem in the embedding space where the vector representation of the question is changed to improve the performance .
Since there is no supervision for training the query reformulator , all these methods using reinforcement learning to maximize the task -specific metrics ( e.g. Recall for para- graph ranking , F1 and Exact Matching for spanbased MRC ) .
Different from these works , we train the query reformulator using an annotated dataset as supervision and then apply the output to a separate reader model .
We leave the exploration of training our model end-to - end using reinforcement learning as future work .
Approach
In this section , we introduce the essential - termaware retriever - reader model ( ET - RR ) .
As shown in Figure 2 , we build a term selector to discover which terms are essential in a question .
The selected terms are then used to formulate a more efficient query enabling the retriever to obtain related evidence .
The retrieved evidence is then fed to the reader to predict the final answer .
For a question with q words Q = {w Q t } q t=1 along with its N answer choices C = { C n } N n=1 where C n = {w C t } c t=1 , the essential - term selector chooses a subset of essential terms E ?
Q , which are then concatenated with each C n to formulate a query .
The query for each answer choice , E + C n , is sent to the retriever ( e.g. Elastic Search 2 ) , and the top K retrieved sentences based on the scores returned by the retriever are then concatenated into the evidence passage P n = {w P t } K t=1 .
Next , given these text sequences Q , C , and P = { P n } N n=1 , the reader will determine a matching score for each triple { Q , C n , P n } .
The answer choice C n * with the highest score is selected .
We first introduce the reader model in Section 3.1 and then the essential term selector in Section 3.2 .
Reader Model
Input Layer
To simplify notation , we ignore the subscript n denoting the answer choice until the final output layer .
In the input layer , all text inputs - the question , answer choices , and passages , i.e. , retrieved evidence - are converted into embedded representations .
Similar to Wang ( 2018 ) , we consider the following components for each word : Word Embedding .
Pre-trained GloVe word embedding with dimensionality d w = 300 .
Part-of- Speech Embedding and Named-Entity Embedding .
The part- of-speech tags and named entities for each word are mapped to embeddings with dimension 16 .
Relation Embedding .
A relation between each word in P and any word in Q or C is mapped to an embedding with dimension 10 .
In the case that multiple relations exist , we select one uniformly at random .
The relation is obtained by querying ConceptNet ( Speer et al. , 2017 ) . Feature Embeddings .
Three handcrafted features are used to enhance the word representations : ( 1 ) Word Match ; if a word or its lemma of P exists in Q or C , then this feature is 1 ( 0 otherwise ) .
( 2 ) Word Frequency ; a logarithmic term frequency is calculated for each word .
( 3 ) Essential Term ; for the i-th word in Q , this feature , denoted as w e i , is 1 if the word is an essential term ( 0 otherwise ) .
Let w e = [ w e 1 , w e 2 , . . . , w eq ] denote the essential term vector .
For Q , C , P , all of these components are concatenated to obtain the final word representations W Q ? R q?d Q , W C ? R c?d C , W P ? R p?d P , where d Q , d C , d P are the final word dimensions of Q , C , and P .
Attention Layer
As shown in Figure 2 , after obtaining word- level embeddings , attention is added to enhance word representations .
Given two word embedding sequences W U , W V , word-level attention is calculated as : M U V = W U U ? ( W V V ) M U V = softmax ( M U V ) W V U = M U V ? ( W V V ) , ( 1 ) where U ? R d U ?dw and V ? R d V ?dw are two matrices that convert word embedding sequences to dimension d w , and M U V contains dot products between each word in W U and W V , and softmax is applied on M U V row-wise .
Three types of attention are calculated using Equation ( 1 ) : ( 1 ) question - aware passage representation W Q P ? R p?dw ; ( 2 ) question - aware choice representation W Q C ? R c?dw ; and ( 3 ) passage - aware choice rep- resentation W P C ? R c?dw .
Sequence Modeling Layer
To model the contextual dependency of each text sequence , we use BiLSTMs to process the word representations obtained from the input layer and attention layer : where H q ?
R q?l , H c ?
R c?l , and H p ?
R p?l are the hidden states of the BiLSTMs , ' ; ' is feature - wise concatenation , and l is the size of the hidden states .
H q = BiLSTM[ W Q ] H c = BiLSTM[ W C ; W P C ; W Q C ] H p = BiLSTM[ W P ; W Q P ] , ( 2 )
Fusion Layer
We further convert each question and answer choice into a single vector : q ?
R l and c ?
R l : ? q = softmax ( [ H q ; w e ] ? w sq ) , q = H q ? q ? c = softmax ( H c ? w sc ) , c = H c ? c , ( 3 ) where the essential - term feature w e from Section 3.1.1 is concatenated with H q , and w sq and w sc are learned parameters .
Finally , a bilinear sequence matching is calculated between H p and q to obtain a question - aware passage representation , which is used as the final passage representation : ? p = softmax ( H p ? q ) ; p = H p ? p . ( 4 )
Choice Interaction
When a QA task provides multiple choices for selection , the relationship between the choices can provide useful information to answer the question .
Therefore , we integrate a choice interaction layer to handle the semantic correlation between multiple answer choices .
Given the hidden state H cn of choice c n and H c i of other choices c i , ?i = n , we calculate the differences between the hidden states and apply max-pooling over the differences : c inter = Maxpool ( H cn ?
1 N ? 1 i =n H c i ) , ( 5 ) where N is the total number of answer choices .
Here , c inter characterizes the differences between an answer choice c n and other answer choices .
The final representation of an answer choice is updated by concatenating the self-attentive answer choice vector and inter-choice representation as c final = [ c ; c inter ] .
Output Layer
For each tuple {q , p n , c n } N n=1 , two scores are calculated by matching ( 1 ) the passage and answer choice and ( 2 ) question and answer choice .
We use a bilinear form for both matchings .
Finally , a softmax function is applied over N answer choices to determine the best answer choice : s pc n = p n W pc c final n ; s qc n = qW qc c final n s = softmax ( s pc ) + softmax ( s qc ) , ( 6 ) where s pc n , s qc n are the scores for answer choice 1 ? n ?
N ; s pc , s qc are score vectors for all N choices ; and s contains the final scores for each answer choice .
During training , we use a crossentropy loss .
Essential Term Selector Essential terms are key words in a question that are crucial in helping a retriever obtain related evidence .
Given a question Q and N answer choices C 1 , . . . , C N , the goal is to predict a binary variable y i for each word Q i in the question Q , where y i = 1 if Q i is an essential term and 0 otherwise .
To address this problem , we build a neural model , ET - Net , which has the same design as the reader model for the input layer , attention layer , and sequence modeling layer to obtain the hidden state H q for question Q .
In detail , we take the question Q and the concatenation C of all N answer choices as input to
Question
If an object is attracted to a magnet , the object is most likely made of ( A ) wood ( B ) plastic ( C ) cardboard ( D ) metal # annotators 5 Annotation If ,0 ; an , 0 ; object , 3 ; is ,0 ; attracted , 5 ; to ,0 ; a ,0 ; magnet , , 5 ; the ,0 ; object , 1 ; is ,0 ; most , 0 ; likely , 0 ; made , 2 ; of , 0 ET - Net .
Q and C first go through an input layer to convert to the embedded word representation , and then word - level attention is calculated to obtain a choice - aware question representation W C Q as in Equation ( 1 ) .
We concatenate the word representation and word-level attention representation of the question and feed it into the sequence modeling layer : H q = BiLSTM[ W Q ; W C Q ] . ( 7 ) As shown in Figure 2 , the hidden states obtained from the attention layer are then concatenated with the embedded representations of Q and fed into a projection layer to obtain the prediction vector y ?
R q for all words in the question : y = [ H q ; W f Q ] ? w s , ( 8 ) where w s contains the learned parameters , and W f Q is the concatenation of the POS embedding , NER embedding , relation embedding , and feature embedding from Section 3.1.1 .
After obtaining the prediction for each word , we use a binary cross-entropy loss to train the model .
During evaluation , we take words with y i greater than 0.5 as essential terms .
Experiments
In this section , we first discuss the performance of the essential term selector , ET - Net , on a public dataset .
We then discuss the performance of the whole retriever - reader pipeline , ET - RR , on multiple open-domain datasets .
For both the ET - Net and ET - RR models , we use 96 - dimensional hidden states and 1 - layer BiLSTMs in the sequence modeling layer .
A dropout rate of 0.4 is applied for the embedding layer and the BiLSTMs ' output layer .
We use adamax ( Kingma and Ba , 2014 ) with a learning rate of 0.02 and batch size of 32 .
The model is trained for 100 epochs .
Our code is released at https://github.com/ nijianmo/ arc-etrr-code .
Performance on Essential Term Selection
We use the public dataset from Khashabi et al . ( 2017 ) which contains 2,223 annotated questions , each accompanied by four answer choices .
Table 2 gives an example of an annotated question .
As shown , the dataset is annotated for binary classification .
For each word in the question , the data measures whether the word is an " essential " term according to 5 annotators .
We then split the dataset into training , development , and test sets using an 8:1:1 ratio and select the model that performs best on the development set .
Table 3 shows the performance of our essential term selector and baseline models from Khashabi et al . ( 2017 ) .
The second best model ( ET Classifier ) is an SVM - based model from Khashabi et al . ( 2017 ) requiring over 100 handcrafted features .
As shown , our ET - Net achieves a comparable result with ET Classifier in terms of the F1 Score .
Table 4 shows example predictions made by ET - Net .
As shown , ET - Net is capable of selecting most ground - truth essential terms .
It rejects certain words such as " organisms " which have a high TF - IDF in the corpus but are not relevant to answering a particular question .
This shows its ability to discover essential terms according to the context of the question .
Performance on Open-domain Multiple-choice QA
We train and evaluate our proposed pipeline method ET - RR on four open-domain multiplechoice QA datasets .
All datasets are associated with a sentence - level corpus .
Detailed statistics are shown in Table 5 . ? ARC : We consider the ' Challenge ' set in the ARC dataset and use the provided corpus during retrieval .
? RACE - Open : We adapted the RACE dataset ( Lai et al. , 2017 ) to the open-domain setting .
Originally , each question in RACE comes Example questions
Which unit of measurement can be used to describe the length of a desk ?
One way animal usually respond to a sudden drop in temperature is by Organisms require energy to survive .
Which of the following processes provides energy to the body ? with a specific passage .
To enable passage retrieval , we concatenate all passages into a corpus with sentence deduplication .
3 ? MCScript-Open : The MCScript ( Ostermann et al. , 2018 ) dataset is also adapted to the open-domain setting .
Again we concatenate all passages to build the corpus .
4 ? Amazon-QA : The Amazon - QA dataset ( McAuley and Yang , 2016 ) is an opendomain QA dataset covering over one million questions across multiple product categories .
Each question is associated with a free-form answer .
We adapt it into a 2 - way multiple - choice setting by randomly sampling an answer from other questions as an answer distractor .
We split all product reviews at the sentence - level to build the corpus .
We consider three categories from the complete dataset in our experiments .
In the experiments , ET - RR uses ET - Net to choose essential terms in the question .
Table 6 shows example predictions on these target datasets .
Then it generates a query for each of the N answer choices by concatenating essential terms and the answer choice .
For each query , ET - RR obtains the top K sentences returned by the retriever and considers these sentences as a passage for the reader .
We set K = 10 for all experiments .
We compare ET - RR with existing retrieve-andread methods on both datasets .
As shown in Table 7 , on the ARC dataset , ET - RR outperforms all previous models without using pre-trained models and achieves a relative 8.1 % improvement over the second best BiLSTM Max-out method ( Mihaylov et al. , 2018 ) .
Recently , finetuning on pretrained models has shown great improvement over a wide range of NLP tasks .
Sun et al. ( 2019 ) proposed a ' Reading Strategies ' method to finetune the pre-trained model OpenAI GPT , a language model trained on the BookCorpus dataset ( Radford , 2018 ) .
They trained Reading Strategies on the RACE dataset to obtain more auxiliary knowledge and then finetune that model on the ARC corpus .
the Amazon - QA dataset .
As shown in Table 9 , ET - RR increases the accuracy by 10.33 % on average compared to the state - of- the - art model Moqa ( McAuley and Yang , 2016 ) .
We also compare ET - RR with ET - RR ( Concat ) , which is a variant of our proposed model that concatenates the question and choice as a query for each choice .
Among all datasets , ET - RR outperforms ET - RR ( concat ) consistently which shows the effectiveness of our essential - term-aware retriever .
Ablation study
We investigate how each component contributes to model performance .
Performance of reader .
Our reader alone can be applied on MRC tasks using the given passages .
Here , we evaluate our reader on the original RACE dataset to compare with other MRC models as shown in Table 10 .
As shown , the recently proposed Reading Strategies and OpenAI GPT models , that finetune generative pre-trained models achieve the highest scores .
Among nonpre-trained models , our reader outperforms other baselines : Bi-attn ( MRU ) ( Tay et al. , 2018 ) and Hierarchical Co-Matching ( Wang et al. , 2018a ) by a relative improvement of 3.8 % .
Attention components .
Table 11 demonstrates how the attention components contribute to the performance of ET - RR .
As shown , ET - RR with all attention components performs the best on the ARC test set .
The performance of ET - RR without passage-question attention drops the most significantly out of all the components .
It is worth noting that the choice interaction layer gives a further 0.24 % boost on test accuracy .
Essential term selection .
To understand the contribution of our essential - term selector , we compare ET - RR with two variants : ( 1 ) ET - RR ( Concat ) and ( 2 ) ET - RR ( TF - IDF ) .
For ET - RR ( TF - IDF ) , we calculate the TF - IDF scores and take words with the top 30 % of TF - IDF scores in the question to concatenate with each answer choice as a query .
5 Table 12 shows an ablation study comparing different query formulation methods and amounts of retrieved evidence K .
As shown , with the essential term selector ET - Net , the model consistently outperforms other baselines , given different numbers of retrievals K. Performance for all models is best when K = 10 .
Furthermore , only using TF - IDF to select essential terms in a question is not effective .
When K = 10 , the ET - RR ( TF - IDF )
This poses further challenges to design a retriever that can rewrite such queries .
Conclusion
We present a new retriever - reader model ( ET - RR ) for open-domain QA .
Our pipeline has the following contributions : ( 1 ) we built an essential term selector ( ET - Net ) which helps the model understand which words are essential in a question leading to more effective search queries when retrieving related evidence ; ( 2 ) we developed an attentionenhanced reader with attention and fusion among passages , questions , and candidate answers .
Experimental results show that ET - RR outperforms existing QA models on open-domain multiplechoice datasets as ARC Challenge , RACE - Open , MCScript-Open and Amazon -QA .
We also perform in - depth error analysis to show the limitations of current work .
For future work , we plan to explore the directions of ( 1 ) constructing multihop query and ( 2 ) developing end-to - end retrieverreader model via reinforcement learning .
Figure 1 : 1 Figure 1 : Example of the retrieve- and - read process to solve open-domain questions .
Words related with the question are in bold ; and words related with C1 and C4 are in italics .
