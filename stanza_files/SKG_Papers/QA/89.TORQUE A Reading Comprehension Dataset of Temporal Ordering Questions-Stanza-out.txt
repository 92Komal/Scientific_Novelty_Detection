title
TORQUE : A Reading Comprehension Dataset of Temporal Ordering Questions
abstract
A critical part of reading is being able to understand the temporal relationships between events described in a passage of text , even when those relationships are not explicitly stated .
However , current machine reading comprehension benchmarks have practically no questions that test temporal phenomena , so systems trained on these benchmarks have no capacity to answer questions such as " what happened before / after [ some event ] ? "
We introduce TORQUE , a new English reading comprehension benchmark built on 3.2 k news snippets with 21 k human- generated questions querying temporal relationships .
Results show that RoBERTa - large achieves an exact-match score of 51 % on the test set of TORQUE , about 30 % behind human performance .
1
Introduction
Time is important for understanding events and stories described in natural language text such as news articles , social media , financial reports , and electronic health records ( Verhagen et al. , 2007 ( Verhagen et al. , , 2010 UzZaman et al. , 2013 ; Minard et al. , 2015 ; Bethard et al. , 2016 Laparra et al. , 2018 ) .
For instance , " he won the championship yesterday " is different from " he will win the championship tomorrow " : he may be celebrating if he has already won it , while if he has not , he is probably still preparing for the game tomorrow .
The exact time of an event is often implicit in text .
For instance , if we read that a woman is " expecting the birth of her first child " , we know that the birth is in the future , while if she is " mourning the death of her mother " , the death is in the past .
These relationships between an event and a time point ( e.g. , " won the championship yesterday " ) or between two events ( e.g. , " expecting " is before 1 https://allennlp.org/torque.html
Heavy snow is causing disruption to transport across the UK , with heavy rainfall bringing flooding to the south -west of England .
Rescuers searching for a woman trapped in a landslide at her home in Looe , Cornwall , said they had found a body .
Q1 : What events have already finished ?
A : searching trapped landslide said found Q2 : What events have begun but has not finished ?
A : snow causing disruption rainfall bringing flooding Q3 : What will happen in the future ?
A : No answers .
Q4 : What happened before a woman was trapped ?
A : landslide Q5 : What had started before a woman was trapped ?
A : snow rainfall landslide Q6 : What happened while a woman was trapped ?
A : searching Q7 : What happened after a woman was trapped ?
A : searching said found " birth " and " mourning " is after " death " ) are called temporal relations ( Pustejovsky et al. , 2003 ) .
This work studies reading comprehension for temporal relations , i.e. , given a piece of text , a computer needs to answer temporal relation questions ( Fig. 1 ) .
Reading comprehension is a natural format for studying temporal phenomena , as the flexibility of natural language annotations allows for capturing relationships that were not possible in previous formalism - based works .
However , temporal phenomena are studied very little in reading comprehension ( Rajpurkar et al. , 2016 ( Rajpurkar et al. , , 2018 Dua et al. , 2019 ; Lin et al. , 2019 ) , and existing systems are hence brittle when handling questions in TORQUE ( Table 1 ) .
Reading comprehension for temporal relationships has the following challenges .
First , reading
TIME
We do n't know if " snow " started before " rainfall " or if " disruption " started before " flooding . "
Disruption / flooding may last longer than the snow / rainfall .
We know disruption / flooding started after snow / rainfall started , but we do n't know if they started earlier than the landslide .
comprehension works rarely require event understanding .
For the example in Fig. 1 , SQUAD ( Rajpurkar et al. , 2016 ) and most datasets largely only require an understanding of predicates and arguments , and would ask questions like " what was a woman trapped in ? "
But a temporal relation question would be " what started before a woman was trapped ? "
To answer it , the system needs to identify events ( e.g. , LANDSLIDE is an event and " body " is not ) , the time of these events ( e.g. , LANDSLIDE is a correct answer , while SAID is not because of the time when the two events happen ) , and look at the entire passage rather than the local predicate - argument structures within a sentence ( e.g. , SNOW and RAINFALL are correct answers to the question above ) .
Second , there are many events in a typical passage of text , so temporal relation questions typically query more than one relationship at the same time .
This means that a question can have multiple answers ( e.g. , " what happened after the landslide ? " ) , or no answers , because the question may be beyond the time scope ( e.g. , " what happened before the snow started ? " ) .
Third , temporal relations queried by natural language questions are often sensitive to a few key words such as before , after , and start .
Those questions can easily be changed to make contrasting questions with dramatically different answers .
Models that are not sensitive to these small changes in question words will perform poorly on this task , as shown in Table 1 .
In this paper , we present TORQUE , the first reading comprehension benchmark that targets these challenges .
We trained crowd workers to label events in text , and to write and answer questions that query temporal relationships between these events .
We also had workers write questions with contrasting changes to the temporal keywords , to give a comprehensive test of a machine 's temporal reasoning ability and minimize the effect of any data collection artifacts ( Gardner et al. , 2020 ) .
We annotated 3.2 k text snippets randomly selected from the TempEval3 dataset ( Uz - Zaman et al. , 2013 ) .
In total , TORQUE has 25 k events and 21 k user- generated and fully answered temporal relation questions .
20 % of TORQUE was further validated by additional crowd workers to be used as test data .
Results show that RoBERTalarge achieves 51 % in exactmatch on TORQUE after fine-tuning , about 30 % behind human performance , indicating that more investigation is needed to better solve this problem .
Definitions
Events
As temporal relations are relationships between events , we first define events .
Generally speak -
Events in different modes
The lion had a large meal and slept for 24 hours .
[ Negated ]
The lion did n't sleep after having a large meal .
[ Uncertain ]
The lion may have had a large meal before sleeping .
[ Hypothetical ]
If the lion has a large meal , it will sleep for 24 hours .
[ Repetitive ]
The lion used to sleep for 24 hours after having large meals .
[ Generic ]
After having a large meal , lions may sleep longer .
ing , an event involves a predicate and its arguments ( ACE , 2005 ; Mitamura et al. , 2015 ) .
When studying time , events were defined as actions / states triggered by verbs , adjectives , and nominals ( Pustejovsky et al. , 2003 ) .
Later works on event and time have largely followed this definition , e.g. , TempEval ( Verhagen et al. , 2007 ) , TimeBank - Dense , RED ( O'Gorman et al. , 2016 ) , and MATRES ( Ning et al. , 2018 b ) .
This work follows this line of event definition and uses event and event trigger interchangeably .
We define an event to be either a verb or a noun ( e.g. , TRAPPED and LANDSLIDE in Fig. 1 ) .
Specifically , in copular constructions , we choose to label the verb as the event , instead of an adjective or preposition .
This allows us to give a consistent treatment of " she was on the east coast yesterday " and " she was happy " , which we can easily teach to crowd workers .
Note that from the perspective of data collection , labeling the copula does not lose information as one can always do post-processing using dependency parsing or semantic role labeling to recover the connection between " was " and " happy . "
Note that events expressed in text are not always factual .
They can be negated , uncertain , hypothetical , or have other associated modalities ( see Fig. 3 ) .
Prior work dealing with events often tried to categorize and label these various aspects because they were crucial for determining temporal relations .
Sometimes certain categories were even dropped due to annotation difficulties ( Pustejovsky et al. , 2003 ; O'Gorman et al. , 2016 ; Ning et al. , 2018 b ) .
In this work , we simply have people label all events , irrespective of their modality , and use natural language to describe relations between them , as discussed in Sec. 3 .
Temporal Relations Temporal relations describe the relationship between two events with respect to time , or between one event and a fixed time point ( e.g. , yesterday ) .
2
We can use a triplet , ( A , r , B ) , to represent this relationship , where A and B are events or time points , and r is a temporal relation .
For example , the first sentence in Fig.
3 expresses a temporal relation ( HAD , happened before , SLEPT ) .
In previous works , every event is assumed to be associated with a time interval [ t start , t end ] .
When comparing two events , there are 13 possible relation labels ( see Fig. 4 ) ( Allen , 1984 ) .
However , there are still many relations that cannot be expressed because the assumption that every event has a time interval is inaccurate :
The time scope of an event may be fuzzy , an event can have a nonfactual modality , or events can be repetitive and invoke multiple intervals ( see Fig. 5 ) .
To better handle these phenomena , we move away from the fixed set of relations used in prior work and instead use natural language to annotate the relationships between events , as described in the next section .
Natural Language Annotation of Temporal Relations Motivated by recent works ( He et al. , 2015 ; Michael et al. , 2017 ; Levy et al. , 2017 ; Gardner et al. , 2019 b ) , we propose using natural language question answering as an annotation for -
Confusing relations between the following events Fuzzy time scope :
Heavy snow is causing disruption to transport across the UK , with heavy rainfall bringing flooding to the southwest of England .
" Follow " is negated : Colonel Collins did n't follow a normal progression anymore once she was picked as a NASA astronaut .
" Leaves " is a series of time intervals :
The bus leaves at 10 am every day , so we will go to the bus stop at 9 am today .
Figure 5 : It is confusing to label these relations using a fixed set of relations : they are not simply before or after , but they can be fuzzy , can have modalities as events , and / or need multiple time intervals to represent .
mat for temporal relations .
Recalling that we denote a temporal relation between two events as ( A , r , B ) , we use ( ? , r , B ) to denote a temporal relation question .
We instantiate these temporal relation questions using natural language .
For instance , ( ? , happened before , SLEPT ) means " what happened before a lion slept ? "
We then expect as an answer the set of all events A in the passage such that ( A , r , B ) holds , assuming for any deictic expression A or B the time point when the passage was written , and assuming that the passage is true .
Fuzzy relations
Heavy snow is causing disruption to transport across the UK , with heavy rainfall bringing flooding to the south -west of England .
Q : What happens at about the same time as the disruption ?
A : flooding Q : What started after the snow started ?
A : disruption Figure 6 : Fuzzy relations that used to be difficult to represent using a predefined label set can be captured naturally in a reading comprehension task .
Advantages
Studying temporal relations as a reading comprehension task gives us the flexibility to handle many of the aforementioned difficulties .
First , fuzzy relations can be described by natural language questions ( after all , the relations are expressed in natural language in the first place ) .
In Fig. 6 , DIS-RUPTION and FLOODING happened at about the same time , but we do not know for sure which one is earlier , so we have to choose vague .
Similarly for SNOW and DISRUPTION , we do not know which one ends earlier and have to choose vague again .
In contrast , the question - answer ( QA ) pairs Questions that query events in different modes [ Negated ]
What did n't the lion do after a large meal ?
[ Uncertain ]
What might the lion do before sleeping ?
[ Hypothetical ]
What will the lion do if it has a large meal ?
[ Repetitive ]
What did the lion use to do after large meals ?
[ Generic ]
What do lions do after a large meal ?
Figure 7 : Events in different modes can be distinguished using natural language questions .
" Often before " vs " before "
He used to take a walk after dinner .
Q : What did he often do after dinner ?
A : walk Q : What did he do after dinner today ?
A : No answers .
in Fig. 6 can naturally capture these fuzzy relations .
Second , natural language questions can conveniently incorporate different modes of events .
Figure 7 shows how to accurately query the relation between " having a meal " and " sleeping " in different modes ( original sentences can be found in Fig. 3 ) .
In contrast , if we could only choose one label , we must choose before for all these relations , although these relations are actually different .
For instance , a repetitive event may be a series of intervals rather than a single one , and often before is very different from before ( Fig. 8 ) .
Third , a major issue that prior works wanted to address was deciding when two events should have a relation Mostafazadeh et al. , 2016 ; O'Gorman et al. , 2016 ; Ning et al. , 2018 b ) .
To avoid asking for relations that do not exist , prior works needed to explicitly annotate certain properties of events as a preprocessing step , but it still remains difficult to have a the - When should two events have a relation ?
Service industries showed solid job gains , an area expected to be hardest hit when the crisis hit the America economy .
Some pairs have relations : ( showed gains ) , ( expected hit ) , ( gains crisis ) , etc .
Some do n't : ( showed hit ) , ( gains hit )
A passerby called the police to report the body , but the line was busy .
Some pairs have relations : ( called report ) , ( called was )
Some do n't : ( report was ) ory explaining , for instance , why hit can compare to expected and crisis , but not to gains .
Interestingly , when we annotate temporal relations in natural language , the annotator naturally avoids event pairs that do not have relations .
For instance , for the sentences in Fig. 9 , one will not ask questions like " what happened after the service industries are hardest hit ? " or " what happened after a passerby reported the body ? "
Instead , natural questions will be " what was expected to happen when the crisis hit America ? " and " what was supposed to happen after a passerby called the police ? "
The format of natural language questions bypasses the need for explicit annotation of properties of events or other theories .
While using QA as the format gives us many benefits in describing fuzzy relations and incorporating various temporal phenomena , we want to note that it may also lead to potential difficulties in transferring the knowledge to downstream tasks that are not in a QA format , and some special treatment in modeling may be needed ( e.g. , He et al . ( 2020 ) ) .
This paper focuses on constructing this QA dataset covering new phenomena , and the problem of successful transfer learning is beyond our scope here .
Penalize Shortcuts by Contrast Sets
An important problem in building datasets is to avoid trivial solutions ( Gardner et al. , 2019a ) . As Fig. 10 shows , there are two events ATE and WENT in the text .
Since ATE is already mentioned in the question , the answer of WENT seems a trivial option without the need to understand the underlying relationship .
To address this issue , we create contrast questions which slightly modify the original questions , but dramatically change the an- swers , so that shortcuts are penalized .
Specifically , for an existing question ( ? , r , B ) ( e.g. , " what happened after he ate his breakfast ? " ) , one should keep using B and change r ( e.g. , " what happened before / shortly after / ... he ate his breakfast ? " ) , or modify it to ask about the start / end time ( e.g. , " what happened after he started eating his breakfast ? " or " what would finish after he ate his breakfast ? " ) .
We also instructed workers to make sure that the answers to the new question are different from the original one to avoid trivial modifications ( e.g. , changing " what happened " to " what occurred " ) .
Data Collection
We used Amazon Mechanical Turk to build TORQUE .
Following prior work , we focus on passages that consist of two contiguous sentences , as this is sufficient to capture the vast majority of non-trivial temporal relations ( Ning et al. , 2017 ) .
We took all the articles used in the TempEval3 ( TE3 ) workshop ( 2.8 k articles ) ( UzZaman et al. , 2013 ) and created a pool of 26 k two -sentence passages .
Given a random passage from this pool , the annotation process for crowd workers was : 1 . Label all the events 2 . Repeatedly do the following 3 ( a) Ask a temporal relation question and point out all the answers from the list of events ( b ) Modify the temporal relation to create one or more new questions and answer them
The annotation guidelines 4 and interface 5 are public .
In the following sections , we further discuss issues of quality control and crowdsourcing cost .
Quality Control
We used three quality control strategies : qualification , pilot , and validation .
Qualification
We designed a separate qualification task where crowd workers were trained and tested on 3 capabilities : labeling events , asking temporal relation questions , and questionanswering .
They were tested on problems randomly selected from a pool we designed .
Crowd workers were considered level - 1 qualified if they could pass the test within 3 attempts .
In practice , about 1 out of 3 workers passed our qualification .
Pilot
We then asked level - 1 crowd workers to do a small amount of the real task .
We manually checked the annotations and gave feedback to them .
Those who passed this inspection were called level - 2 workers , and only they could work on the large-scale real task .
Roughly 1 out of 3 pilot submissions received a level - 2 qualification .
In the end , there were 63 level - 2 annotators , and 60 of them actually worked on our large-scale task .
Validation
We randomly selected 20 % of the articles from TORQUE for further validation .
We first validated the events by 4 different level - 2 annotators ( with the original annotator , there were in total 5 different humans ) .
We also intentionally added noise to the original event list so that the validators must carefully identify wrong events .
The final event list was determined by aggregating all 5 humans using majority vote .
Second , we validated the answers in the same portion of the data .
Two level - 2 workers were asked to verify the initial annotator 's answers ; we again added noise to the answer list as a quality control for the validators .
Instead of using majority vote as we did for events , the final answers from all workers are considered correct .
We did not do additional validation for the questions themselves , as a manual inspection found the quality to be very high already , with no bad questions in a random sample of 100 .
Cost
In each job of the main task , we presented 3 passages .
The crowd worker could decide to use some or all of them .
For each passage a worker decided to use , they needed to label the events , answer 3 hard - coded warm - up questions , and then ask and answer at least 12 questions ( including contrast questions ) .
The final reward is a base pay of $ 6 plus $ 0.5 for each extra question .
Crowd workers thus had the incentive to ( 1 ) use fewer passages so that they can do event labeling and warm - up questions fewer times , ( 2 ) modify questions instead of asking from scratch , and ( 3 ) ask extra questions in each job .
All these incentives were for more coverage of the temporal phenomena in each passage .
In practice , crowd workers on average used 2 passages in each job .
Validating the events in each passage and the answers to a specific question both cost $ 0.1 .
In total , TORQUE cost $ 15 k for an average of $ 0.70 / question .
TORQUE Statistics TORQUE has 3.2 k passage annotations ( ? 50 tokens / passage ) , 6 24.9 k events ( 7.9 events / passage ) , and 21.2 k user-provided questions ( half of them were labeled by crowd workers as modifications of existing ones ) .
Every passage comes with 3 hardcoded warm - up questions asking which events in the passage had already happened , were ongoing , or were still in the future .
Table 3 shows some basic statistics of TORQUE .
Note the 3 warm - up questions form a contrast set , and we treat the first as " original " and the others " modified . "
In a random sample of 200 questions in the test set of TORQUE , we found 94 questions querying about relations that cannot be directly represented by the previous single - interval - based labels .
Table 2 gives example questions capturing these phenomena .
More analysis of the event , answer , and workload distributions are in Appendix A-D .
Quality
To validate the event annotations , we took the events provided by the initial annotator , added noise , and asked different workers to validate .
We also trained an auxiliary event detection model using RoBERTa - large and added its predictions as event candidates .
This tells us about the quality of events in TORQUE in two ways .
First , the Worker Agreement with Aggregate ( WAWA ) F 1 here is 94.2 % ; that is , compare the majority - vote with all annotators , and perform micro-average on all instances .
Second , if an event candidate is labeled by both the initial annotator and the model , then almost all of them ( 99.4 % ) are kept by the validators ; if neither the initial annotator nor the model labeled a candidate , the candidate is almost surely removed ( 0.8 % ) .
As validators did not know which ones were noise or not before - hand , this indicates that the validators could identify noise terms reliably .
Similarly , the WAWA F 1 of the answer annotations is 84.7 % , slightly lower than that for events , which is expected because temporal relation QA is intuitively harder .
Results show that 12.3 % of the randomly added answer candidates were labeled as correct answers by the validators .
We manually inspected 100 questions and found 11.6 % of the added noise terms were correct answers ( very close to 12.3 % ) , indicating that the validators were actually doing a good job in answer validation .
More details of the metrics and the quality of annotations can be found in Appendix E.
Experiment
We split TORQUE into train ( 80 % of all the questions ) , dev ( 5 % ) , and test ( 15 % ) and these three parts do not have the same articles .
To solve TORQUE in an end-to- end fashion , the model here takes as input a passage and a question , then looks at every token in the passage and makes a binary classification of whether this token is an answer to the question or not .
Specifically , the model has a one-layered perceptron on top of BERT ( Devlin et al. , 2019 ) or RoBERTa , and the input to the perceptron layer is the transformers ' output corresponding to the token we 're looking at .
We fine- tuned BERT / RoBERTa ( both " base " and " large " ) on the training set of TORQUE .
We fixed batch size = 6 ( each instance is a tuple of one passage , one question , and all its answers ) with gradient accumulation step = 2 in all experiments .
We selected the learning rate ( from ( 1e ?5 , 2e ?5 ) ) , the training epoch ( within 10 ) , and the random seed ( from 3 arbitrary ones ) based on performance on the dev set of TORQUE .
7
To compute an estimate of human performance , one author answered 100 questions from the test set and compared with crowd workers ' annotations .
Both the human performance and system performances are shown in Table 4 .
We report the standard macro F 1 and exact-match ( EM ) metrics in question answering , and also EM consistency , the percentage of contrast question sets for which a model 's predictions match exactly to all questions in a group ( Gardner et al. , 2020 ) .
We see warm - up questions are easier than user-provided ones because warm - up questions focus on easier phenomena of past / ongoing / future events .
In addition , RoBERTa - large is expectedly the best system , but still far behind human performance , trailing by about 30 % in EM .
We further downsampled the training data to test the performance of RoBERTa .
We find that with 10 % of the original training data , RoBERTa fails to learn anything meaningful and simply pre -
Table 4 : Human / system performance on the test set of TORQUE .
System performance is averaged from 3 runs ; all std .
dev. were ?
4 % and those in [ 1 % , 4 % ] are underlined .
C ( consistency ) is the percentage of contrast groups for which a model 's predictions have F 1 ? 80 % for all questions in a group ( Gardner et al. , 2020 ) . dicts " not an answer " for all tokens .
With 50 % of the training data , RoBERTa is slightly lower than but already comparable to that of using the entire training set .
This means that the learning curve on TORQUE is already flat and the current size of TORQUE may not be the bottleneck for its low performance .
Our data and code are public to facilitate more investigations into TORQUE .
8
Related Work
The study of time is to understand when , how long , and how often things happen .
While how long and how often usually require temporal common sense knowledge ( Vempala et al. , 2018 ; Zhou et al. , 2019
Zhou et al. , , 2020 , the problem of when often boils down to extracting temporal relations .
Modeling .
Research on temporal relations often focuses on algorithmic improvement , such as structured inference ( Do et al. , 2012 ; Ning et al. , 2018a ) , structured learning ( Leeuwenberg and Moens , 2017 ; Ning et al. , 2017 ) , and neural networks Tourille et al. , 2017 ; Cheng and Miyao , 2017 ; Meng and Rumshisky , 2018 ; Leeuwenberg and Moens , 2018 ; . Formalisms .
The approach that prior works took to handle the aforementioned temporal phenemona was to define formalisms such as the different modes of events ( Fig. 3 ) , a predefined label set ( Fig. 4 ) , different time axes for events ( Ning et al. , 2018 b ) , and specific rules to follow when there is confusion .
For example , Bethard et al . ( 2007 ) ; Ning et al. ( 2018 b ) focused on a limited set of temporal phenomena and achieved high inter-annotator agreements ( IAA ) , while Styler IV et al . ( 2014 ) ; O'Gorman et al. ( 2016 ) aimed at covering more phenomena but suffered from low IAAs even between NLP researchers .
QA as annotation .
A natural choice is then to cast temporal relation understanding as a machine reading comprehension ( MRC ) problem .
TORQUE is motivated by the philosophy in QA - SRL ( He et al. , 2015 ) and QAMR ( Michael et al. , 2017 ) , where QA pairs were used as representations for predicate - argument structures .
In zeroshot relation extraction ( RE ) , they reduced relation slot filling to an MRC problem so as to build very large distant training data and improve zero-shot learning performance ( Levy et al. , 2017 ) .
However , our work differs from zero-shot RE since it centers around entities , while TORQUE is about events ; the way to ask and answer questions , and the way to design a corresponding crowdsourcing pipeline , are thus significantly different between us .
The QA - TempEval workshop ( Llorens et al. , 2015 ) , desipte its name , is actually not studying temporal relations in an RC setting .
The differences between TORQUE and QA - TempEval are as follows .
First , QA TempEval is an evaluation approach for systems that generate TimeML annotations and actually is not a QA task .
For instance , QA TempEval is to evaluate whether a system can answer questions like " IS < ENTITY 1 > < RELATION > < ENTITY 2 > ? " , where one clearly knows which event that < ENTITY > is referring to and where RELATION is selected from a predefined label set .
Second , QA - TempEval 's annotation relies on the existence of a TimeML corpus .
From the perspective of data collection for studying a particular phenomenon , TORQUE has done more on defining the task and developing a scalable crowdsourcing pipeline .
As a result , TORQUE is also much larger than QA - TempEval and the annotation pipeline of TORQUE can be easily adopted to collect even more data .
Conclusion
Understanding temporal ordering of events is critical in reading comprehension , but existing works have studied very little about it .
This paper presents TORQUE , a new English machine reading comprehension ( MRC ) dataset of temporal ordering questions .
TORQUE has 3.2 k news snippets , 9.5 k hard - coded questions asking which events had happened , were ongoing , or were still in the future , and 21.2 k human- generated questions querying more complex phenomena .
We argue that an MRC setting allows for more convenient representation of these temporal phenomena than conventional formalisms .
Results show that even a state - of - the - art language model , RoBERTa - large , falls behind human performance by a large margin , necessitating more investigation on improving MRC on temporal relationships in the future .
Figure 13 further shows the 50 most common events in TORQUE .
Unsurprisingly , the most comevents are reporting verbs ( e.g. , " say " , " tell " , " report " , and " announce " ) and copular verbs .
Other common events such as " meeting " , " killed " , " visit " , and " war " are also expected given that the passages of TORQUE were taken from news articles .
