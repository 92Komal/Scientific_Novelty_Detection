title
Answer Sequence Learning with Neural Networks for Answer Selection in Community Question Answering
abstract
In this paper , the answer selection problem in community question answering ( CQA ) is regarded as an answer sequence labeling task , and a novel approach is proposed based on the recurrent architecture for this problem .
Our approach applies convolution neural networks ( CNNs ) to learning the joint representation of questionanswer pair firstly , and then uses the joint representation as input of the long shortterm memory ( LSTM ) to learn the answer sequence of a question for labeling the matching quality of each answer .
Experiments conducted on the SemEval 2015 C-QA dataset shows the effectiveness of our approach .
Introduction Answer selection in community question answering ( CQA ) , which recognizes high-quality responses to obtain useful question - answer pairs , is greatly valuable for knowledge base construction and information retrieval systems .
To recognize matching answers for a question , typical approaches model semantic matching between question and answer by exploring various features ( Wang et al. , 2009a ; Shah and Pomerantz , 2010 ) .
Some studies exploit syntactic tree structures ( Wang et al. , 2009 b ; Moschitti et al. , 2007 ) to measure the semantic matching between question and answer .
However , these approaches require high-quality data and various external resources which may be quite difficult to obtain .
To take advantage of a large quantity of raw data , deep learning based approaches ( Wang et al. , 2010 ; Hu et al. , 2013 ) are proposed to learn the distributed representation of question - answer pair directly .
One disadvantage of these approaches lies in that *
* Corresponding author Hi. anyone can suggest a good tailor shop ( preferably Philippine nationality ) in Qatar ?
i heard there 's one over at Al Saad .
just not sure the details ... thanks !
There are a lot of tailor shops , it depends on what you want !
Sterling Tailors in Barwa Village , it is run by indians and sri lankans but service is good .
I 've seen some filipinos who are taking orders from them .
Just Check it out ... thanks .
will def check 'em out ...
Oh my ... they now sell Filipinos ?
Is there anything they do n't sell ?
Well , apart from Guitar Hero ... semantic correlations embedded in the answer sequence of a question are ignored , while they are very important for answer selection .
Figure 1 is a example to show the relationship of answers in the sequence for a given question .
Intuitively , other answers of the question are beneficial to judge the quality of the current answer .
Recently , recurrent neural network ( RNN ) , especially Long Short - Term Memory ( LST - M ) ( Hochreiter et al. , 2001 ) , has been proved superiority in various tasks ( Sutskever et al. , 2014 ; Srivastava et al. , 2015 ) and it models long term and short term information of the sequence .
And also , there are some works on using convolutional neural networks ( CNNs ) to learn the representations of sentence or short text , which achieve state - of - the - art performance on sentiment classification ( Kim , 2014 ) and short text matching ( Hu et al. , 2014 ) .
In this paper , we address the answer selection problem as a sequence labeling task , which identifies the matching quality of each answer in the answer sequence of a question .
Firstly , CNNs are used to learn the joint representation of question answer ( QA ) pair .
Then the learnt joint repre-sentations are used as inputs of LSTM to predict the quality ( e.g. , Good , Bad and Potential ) of each answer in the answer sequence .
Experiments conducted on the CQA dataset of the answer selection task in SemEval - 2015 1 show that the proposed approach outperforms other state - of - the - art approaches .
Related Work Prior studies on answer selection generally treated this challenge as a classification problem via employing machine learning methods , which rely on exploring various features to represent QA pair .
Huang et al. ( 2007 ) integrated textual features with structural features of forum threads to represent the candidate QA pairs , and used support vector machine ( SVM ) to classify the candidate pairs .
Beyond typical features , Shah and Pomerantz ( 2010 ) trained a logistic regression ( L- R ) classifier with user metadata to predict the quality of answers in CQA .
Ding et al. ( 2008 ) proposed an approach based on conditional random fields ( CRF ) , which can capture contextual features from the answer sequence for the semantic matching between question and answer .
Additionally , the translation - based language model was also used for QA matching by transferring the answer to the corresponding question ( Jeon et al. , 2005 ; Xue et al. , 2008 ; Zhou et al. , 2011 ) .
The translation - based methods suffer from the informal words or phrases in Q&A archives , and perform less applicability in new domains .
In contrast to symbolic representation , Wang et al . ( 2010 ) proposed a deep belief nets ( DBN ) based semantic relevance model to learn the distributed representation of QA pair .
Recently , the convolutional neural networks ( CNNs ) based sentence representation models have achieved successes in neural language processing ( NLP ) tasks .
Yu et al. ( 2014 ) proposed a convolutional sentence model to identify answer contents of a question from Q&A archives via means of distributed representations .
The work in Hu et al . ( 2014 ) demonstrated that 2 - dimensional convolutional sentence models can represent the hierarchical structures of sentences and capture rich matching patterns between two language objects .
Approach
We consider the answer selection problem in CQA as a sequence labeling task .
To label the matching quality of each answer for a given question , our approach models the semantic links between successive answers , as well as the semantic relevance between question and answer .
Figure 2 summarizes the recurrent architecture of our model ( R - CNN ) .
The motivation of R -CNN is to learn the useful context to improve the performance of answer selection .
The answer sequence is modeled to enrich semantic features .
At each step , our approach uses the pre-trained word embeddings to encode the sentences of QA pair , which then is used as the input vectors of the model .
Based on the joint representation of QA pair learned from CNNs , the LSTM is applied in our model for answer sequence learning , which makes a prediction to each answer of the question with softmax function .
Convolutional Neural Networks for QA Joint Learning Given a question - answer pair at the step t , we use convolutional neural networks ( CNNs ) to learn the joint representation p t for the pair .
Figure 3 illustrates the process of QA joint learning , which includes two stages : summarizing the meaning of the question and an answer , and generating the joint representation of QA pair .
To obtain high- level sentence representations of the question and answer , we set 3 hidden layers in two convolutional sentence models respectively .
The output of each hidden layer is made up of a set of 2 - dimensional arrays called feature map parameters ( w m , b m ) .
Each feature map is the outcome of one convolutional or pooling filter .
Each pooling layer is followed an activation function ?.
The output of the m th hidden layer is computed as Eq. 1 : H m = ?( pool ( w m H m?1 + b m ) ) ( 1 )
Here , H 0 is one real-value matrix after sentence semantic encoding by concatenating the word vectors with sliding windows .
It is the input of deep convolution and pooling , which is similar to that of traditional image input .
Finally , we combine the two sentence models by adding an additional layer H t on the top .
The learned joint representation p t for QA pair is formalized as Eq. 2 : p t = ?( w t H t + b t ) ( 2 ) where ? is an activation function , and the input vector is constructed by concatenating the sentence representations of question and answer .
LSTM for Answer Sequence Learning Based on the joint representation of QA pair , the LSTM unit of our model performs answer sequence learning to model semantic links between continuous answers .
Unlike the traditional recurrent unit , the LSTM unit modulates the memory at each time step , instead of overwriting the states .
The key component of LSTM unit is the memory cell c t which has a state over time , and the L-STM unit decides to modify and add the memory in the cell via the sigmoidal gates : input gate i t , forget gate f t and output gate o t .
The implementation of the LSTM unit in our study is close the one discussed by Graves ( 2013 ) .
Given the joint representation p t at time t , the memory cell c t is updated by the input gate 's activation i t and the forget gate 's activation f t .
The updating equation is given by Eq. 3 : The LSTM unit keeps to update the context by discarding the useless context in forget gate f t and adding new content from input gate i t .
The extents to modulate context for these two gates are computed as Eq. 4 and Eq. 5 : c t = f t c t?1 + i t tanh ( W xc p t + W hc h t?1 +b c ) ( 3 ) i t = ?( W xi p t + W hi h ( t?1 ) + W ci c t?1 + b i ) ( 4 ) f t = ?( W xf p t + W hf h t?1 + W cf c t?1 + b f ) ( 5 ) With the updated cell state c t , the final output from LSTM unit h t is computed as Eq 6 and Eq 7 : o t = ?( W xo p t + W ho h t?1 + W co c t + b o ) ( 6 ) h t = o t tanh ( c t ) ( 7 ) Note that ( W * , b * ) is the parameters of LSTM unit , in which W cf , W ci , and W co are diagonal matrices .
According to the output h t at each time step , our approach estimates the conditional probability of the answer sequence over answer classes , it is given by Eq. 8 : P (y 1 , ... , y T |c , p 1 , ... , p t?1 ) = T t=1 p(y t |c , y 1 , ... , y t?1 ) ( 8 ) Here , (y 1 , ... , y T ) is the corresponding label sequence for the input sequence ( p 1 , . 1 , where # question / answer denotes the number of questions / answers , and length stands for the average number of answers for a question .
Competitor Methods :
We compare our approach against the following competitor methods : SVM ( Huang et al. , 2007 ) : An SVM - based method with bag-of-words ( textual features ) , nontextual features , and features based on topic model ( i.e. , latent Dirichlet allocation , LDA ) .
CRF ( Ding et al. , 2008 ) : A CRF - based method using the same features as the SVM approach .
DBN ( Wang et al. , 2010 ) :
Taking bag-of-words representation , the method applies deep belief nets to learning the distributed representation of QA pair , and predicts the class of answers using a logistic regression classifier on the top layer .
mDBN
( Hu et al. , 2013 ) :
In contrast to DBN , multimodal DBN learns the joint representations of textual features and non-textual features rather than bag-of-words .
CNN : Using word embedding , the CNNs based model in Hu et al . ( 2014 ) is used to learn the representations of questions and answers , and a logistic regression classifier is used to predict the class of answers .
Evaluation Metrics :
The evaluation metrics include M acro ? precision ( P ) , M acro ? recall ( R ) , M acro ? F 1 ( F 1 ) , and F 1 scores of the individual classes .
According to the evaluation results on the development set , all the hyperparameters are optimized on the training set .
Model Architecture and Training Details :
The CNNs of our model for QA joint representation learning have 3 hidden layers for modeling question and answer sentence respectively , in which each layer has 100 feature maps for convolution and pooling operators .
The window sizes of convolution for each layer are [ 1 ?
1 , 2 ? 2 , 2 ? 2 ] , the window sizes of pooling are [ 2 ? 2 , 2 ? 2 , 1 ? 1 ] .
For the LSTM unit , the size of input gate is set to 200 , the sizes of forget gate , output gate , and memory cell are all set to 360 .
Stochastic gradient descent ( SGD ) algorithm via back - propagation through time is used to train the model .
To prevent serious overfitting , early stopping and dropout ( Hinton et al. , 2012 ) during the training procedure .
The learning rate ? is initialized to be 0.01 and is updated dynamically according to the gradient descent using the ADADELTA method ( Zeiler , 2012 ) .
The activation functions ( ? , ? ) in our model adopt the rectified linear unit ( ReLU ) ( Dahl et al. , 2013 ) .
In addition , the word embeddings for encoding sentences are pre-trained with the unsupervised neural language model ( Mikolov et al. , 2013 ) on the Qatar Living data 2 .
Results and Analysis Table 2 summarizes the Macro-averaged results .
The F1 scores of the individual classes are presented in Table 3 .
It is clear to see that the proposed R-CNN approach outperforms the competitor methods over the Macro-averaged metrics as expected from Table 2 .
The main reason lies in that R -CNN takes advantages of the semantic correlations between successive answers by LSTM , in addition to the semantic relationships between question and answer .
The joint representation of QA pair learnt by CNNs also captures richer matching patterns between question and answer than other methods .
It is notable that the methods based on deep learning perform more powerful than SVM and CRF , especially for complicate answers ( e.g. , Potential answers ) .
In contrast , SVM and CRF using a large amount of features perform better for the answers that have obvious tendency ( e.g. , Good and Bad answers ) .
The main reason is that the distributed representation learnt from deep learning architecture is able to capture the semantic relationships between question and answer .
On the other hand , the feature -engineers in both SVM and CRF suffer from noisy information of CQA and the feature sparse problem for short questions and answers .
Compared to DBN and mDBN , CNN and R - CNN show their superiority in modeling QA pair .
The convolutional sentence models , used in CN -N and R - CNN , can learn the hierarchical structure of language object by deep convolution and pooling operators .
In addition , both R -CNN and CNN encode the sentence into one tensor , which makes sure the representation contains more semantic features than the bag-of-words representation in DBN and mDBN .
Methods
The improvement achieved by R-CNN over C-NN demonstrates that answer sequence learning is able to improve the performance of the answer selection in CQA .
Because modeling the answer sequence can enjoy the advantage of the shared representation between successive answers , and complement the classification features with the learnt useful context from previous answers .
Furthermore , memory cell and gates in LSTM unit modify the valuable context to pass onwards by updating the state of RNN during the learning procedure .
The main improvement of R -CNN against with the competitor methods comes from the Potential answers , which are much less than other two type of answers .
It demonstrates that R-CNN is able to process the unbalance data .
In fact , the Potential answers are most difficult to identify among the three types of answers as Potential is an intermediate category ( M?rquez et al. , 2015 ) .
Nevertheless , R- CNN achieves the highest F1 score of 15. 22 % on Potential answers .
In CQA , Q&A archives usually form one multi-parties conversation when the asker gives feedbacks ( e.g. , " ok " and " please " ) to users responses , indicating that the answers of one question are sematic related .
Thus , it is easy to understand that R-CNN performs better performance than competitor methods , especially on the recall .
The reason is that R-CNN can model semantic correlations between successive answers to learn the context and the long range dependencies in the answer sequence .
Conclusions and Future Work
In this paper , we propose an answer sequence learning model R-CNN for the answer selection task by integrating LSTM unit and CNNs .
Based on the recurrent architecture of our model , our approach is able to model the semantic link between successive answers , in addition to the semantic relevance between question and answer .
Experimental results demonstrate that our approach can learn the useful context from the answer sequence to improve the performance of answer selection in C-QA .
In the future , we plan to explore the methods on training the unbalance data to improve the overall performances of our approach .
Based on this work , more research can be conducted on topic recognition and semantic roles labeling for human-human conversations in real-world .
Figure 1 : 1 Figure 1 : An Example of the Answer Sequence for a Question .
The dashed arrows depict the relationships of the answers in the sequence .
Figure 2 : 2 Figure 2 : The architecture of R-CNN
Figure 3 : 3 Figure 3 : CNNs for QA joint learning
Table 1 : 1 Statistics of experimental dataset Data # question # answer length training 2600 16541 6.36 development 300 1645 5.48 test 329 1976 6.00 all 3229 21062 6.00
Table 2 : 2 Macro-averaged results ( % ) are used
Table 3 : 3 F1 scores for the individual classes ( % ) Good Bad Potential SVM 79.78 76.65 0.00 CRF 79.32 75.50 5.38 DBN 76.99 71.33 13.89 mDBN 77.74 70.39 14.74 CNN 76.45 74.77 12.05 R-CNN 77.31 75.88 15.22
http://alt.qcri.org/semeval2015/task3/index.php?id=dataand-tools
