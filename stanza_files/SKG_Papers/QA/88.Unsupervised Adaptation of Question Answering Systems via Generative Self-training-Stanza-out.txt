title
Unsupervised Adaptation of Question Answering Systems via Generative Self-training
abstract
BERT - era question answering systems have recently achieved impressive performance on several question - answering ( QA ) tasks .
These systems are based on representations that have been pre-trained on self-supervised tasks such as word masking and sentence entailment , using massive amounts of data .
Nevertheless , additional pre-training closer to the end-task , such as training on synthetic QA pairs , has been shown to improve performance .
While recent work has considered augmenting labelled data and leveraging large unlabelled datasets to generate synthetic QA data , directly adapting to target data has received little attention .
In this paper we investigate the iterative generation of synthetic QA pairs as a way to realize unsupervised self adaptation .
Motivated by the success of the roundtrip consistency method for filtering generated QA pairs , we present iterative generalizations of the approach , which maximize an approximation of a lower bound on the probability of the adaptation data .
By adapting on synthetic QA pairs generated on the target data , our method is able to improve QA systems significantly , using an order of magnitude less synthetic data and training computation than existing augmentation approaches .
Introduction Supervised self - training methods have transformed applied machine learning recently .
Such tasks serve as " pre-training " for related downstream tasks , and have proven to be essential to attaining state - of - theart performance , particularly in NLP .
BERT - era Transformer - based question answering systems have recently achieved impressive performance on several question - answering ( QA ) tasks .
These systems are based on representations that have been pre-trained on self-supervised tasks such as word masking and sentence entailment , using massive amounts of data ( Devlin et al. , 2018 ; Dong et al. , 2019 ; Radford et al. , 2019 ) .
Nevertheless , additional pretraining closer to the end-task , such as training on synthetic QA pairs , has been shown to improve performance Dong et al. , 2019 ) .
While recent work has considered augmenting labelled data and leveraging large unlabelled datasets to generate synthetic QA data , directly adapting to target data has , to our knowledge , not been investigated in the context of BERT - era modeling and performance levels .
Recently , roundtrip consistency ( RTC ) was introduced as a criteria for filtering synthetic questionanswer pairs on unlabelled data , and has demonstrated solid gains when applied to large unlabelled datasets to generate millions of RTC - validated pairs as task -specific pre-training data .
Such unsupervised self-training can be an effective way to de-emphasize low confidence predictions , adapt to the target input distribution , distill decoding procedures , and instill input response invariances .
In this paper we present new theoretical justification for RTC , and explore novel iterative generalizations of RTC for adapting in a task -specific , target data specific manner .
We show that most of these approaches optimize an approximation of a lower bound of the probability of the data , and thereby can , beyond self-training , potentially also adapt to explain the target data more effectively .
Under the formulation , the question - answering system is used as a surrogate likelihood function for the question and answer generators .
In this manner , the difficult task of modeling the generation of entire contexts is avoided : instead abstractive parts of the context ( the answers ) , whose locations are latent and estimated , are utilized , similar to an autoencoder , but with questions as the latent code , and latent answer inputs .
By adapting on synthetic QA pairs generated on the target data , our method is able to improve QA systems significantly , using an order of magnitude less synthetic data and training computation than existing augmentation approaches .
The main contributions of this paper are as follows : ?
We consider the problem of adapting QA systems to target data using synthetically generated QA pairs , and show that this improves QA systems significantly , while using an order of magnitude less data and computation relative to existing methods that augment using large unlabelled data sets .
?
We present a solid theoretical foundation for understanding existing and developing new synthetic data filtering algorithms , including the effective but elusive Roundtrip Consistency ( RTC ) algorithm . ?
We compare several related methods for utilizing synthetic data to adapt to target data , by iteratively pre-training and fine-tuning the QA system and QA generators , and show that some variations optimize an approximation of a lower bound on the probability of the adaptation data , despite being composed on only inference networks .
Supervised Training of QA systems Typically question - answering ( QA ) systems are trained to produce an abstractive answer A , given a question Q , and the relevant local context C .
The QA model is generally trained to maximize the probability of QA pairs in context : l a|g = t log p( A * t | Q * t , C * t ) ( 1 )
Where { Q t , A t , C t } denotes a set of humanannotated question - answer -context triples ( Rajpurkar et al. , 2016 ) .
Supervised training of Question - Answering systems is effective , but ground -truth ( GT ) QA pairs are cumbersome and expensive to annotate .
Bootstrapping from pretrained representations such as BERT significantly reduces the number of GT QA pairs that are required to train a high quality QA system , but current systems still currently require hundreds of thousands of GT QA pairs on data matching the style and content of the target data to perform at state - of - the - art levels .
Generating Synthetic QA Data
To augment an existing QA training set , several authors have used the supervised data to train question generators ( Sachan and Xing , 2018 ; Dong et al. , 2019 ) .
As with the QA counterparts , these models are typically trained in a supervised manner : l q|a = t log p( Q * t | A * t , C * t ) ( 2 ) and then employed to augment the training set with additional , synthetic questions , for each groundtruth answer .
Most techniques that operate fully unsupervised also train an answer prior in the same manner , which generates answers for a given context , which the question generator then conditions on to generate questions : l a = t log p( A * t | C * t ) ( 3 ) 4 Iterative Pre-train then Fine-tune Based Adaptation Most existing methods for synthetic data augmentation utilize QA generators trained only on supervised data .
This synthetic data is used either as pre-training data for the QA system , which is then fine-tuned on ground - truth data , or just simply as additional GT data for training .
In this paper we investigate iteratively pretraining both the QA system and the QA generators on the synthetic data generated by the most recent fine - tuned QA generators .
The iterative procedure is as follows :
1 . Generate QA pairs using the current finetuned QA generators .
2 . Pre-train the QA generators and QA system on the ( filtered ) generated data from scratch .
3 . Fine-tune the QA generators and QA system on ground truth data .
4 . Repeat until converged or for a maximum number of iterations .
Fundamentals of Unsupervised Self-Training
Training a model on it 's own predictions , or pseudotruth , has a long history in machine learning and speech and language applications ( Scudder , 1965 ; Novotney and Callison - Burch , 2010 ) .
It generally leads to performance improvements , but why ?
The expected gradient of the log probability of a model p( y|x ) trained on its own predictions is zero : E y?p ? ( y|x ) [? ? log p ? ( y| x ) ] = 0 !
There are few ways that self-training can improve performance : ?
By re-prioritizing the model 's capacity based on a target input distribution x that is generated for , i.e. , by compensating for covariate shift ( Sugiyama and Kawanabe , 2012 ) . ?
By sharpening the model to eliminate prediction noise , via the distillation of any decoding processes ( e.g. beam search ) ( Grandvalet and Bengio , 2005 ) . ?
By introducing perturbations that the model 's predictions are trained to be invariant to .
Recently in ( He et al. , 2019 ) , it was shown that for machine translation , significant gains could be realized via self-training , and that most of this gain could be attributed to the last effect : in particular , invariance to input and dropout noise .
In summary , predictive models can benefit substantially from self-training : one does not need to improve a generative model of the data to improve prediction performance on that data .
Nevertheless , we will next relate the predictive models used in QA generators and QA systems to underlying implicit generative models , to derive insight and justify existing QA filtering methods , and derive new ones .
It turns out that several self-training algorithms for QA optimization actually optimize an approximate lower bound on the probability of the data , and so have the potential to evolve their representations to maximize the probability of the target data .
A Generative Framework for Adapting QA Systems
In this section we consider a simple generative framework for understanding predictive QA systems .
In unsupervised data settings , only the context C is observed , and the questions { Q} and answers { A} are latent , and must be inferred .
Here we consider the following generative model for each context : log p ( C ) = log Q , A p( C | Q , A ) p ( Q | A ) p ( A ) ( 4 )
Where the index of the context in the adaptation data has been omitted to avoid notational clutter .
Typically the factors of a generative models are explicit , and each factor is endowed with parameters , to maximize the probability of the data .
In this paper , these factors , as we shall see , are only implicit , and we explore the possibility of maximizing the probability of the data , using only inference networks , which condition on the context : namely the answer generator , p ( A|C ) , the question generator , p( Q|A , C ) , and the QA system , p ( A |Q , C ) .
Our interest in doing so is 3 fold : 1 ) To avoid having to specify and train a generative model of contexts , which may be difficult to learn .
2 ) To better understand the theoretical underpinnings of existing augmentation algorithms such as the roundtrip consistency algorithm ( RTC ) , and 3 ) Foremost , to achieve the end-goal of deriving insight that leads to more effective iterative algorithms for adapting QA systems .
We begin our quest by low-bounding the probability of the data , with a posterior distribution q( A , Q ) over the latent questions and answers for the context , as is done for VAES ( Kingma and Welling , 2013 ) . log p ( C ) ? Q , A q( Q , A|C ) log p ( C | Q , A ) p ( Q , A ) q( Q , A|C ) ( 5 ) Expanding the prior term p ( Q , A ) in terms of question and answer generators , p( Q|A , C ) and p ( A|C ) , we have : log p( Q , A ) = log C p( Q|A , C ) p ( A | C ) p( C ) ? log p( Q|A , C ) p ( A | C ) p ( C ) ( 6 ) Where p( C ) is the current estimate of probability of context C .
In general we expect this lower bound to be quite strong , as p( Q|A , C ) and p( A|C ) are identically or close to zero for most C = C .
The resulting lower bound is on p ( C ) is : log p ( C ) ? L = log p ( C ) + Q , A q( Q , A|C ) log p ( C | Q , A ) p ( Q|A , C ) p ( A |C ) q( Q , A |C ) ( 7 ) Which can be maximized to improve p ( C ) .
The resulting bound looks much like a typical variational bound ( e.g. 5 ) , the principle difference being that the distributions being optimized are themselves inference networks , and condition on C .
The effective joint distribution corresponding to this bound is : p( C , Q , A ) ? p( C|Q , A ) p ( Q|A , C ) p ( A | C ) ( 8 ) 7 Defining the inference distribution q
In contrast with how VAEs are typically optimized , here we utilize multiple samples to form a posterior over QA pairs ( Burda et al. , 2015 ; Rainforth et al. , 2018 ) , and employ a form of prioritized truncated variational inference ( TVI ) to combat mode-dropping , which was recently shown to be a proper variational bound that can be iterated ( L?cke , 2016 ) .
TVI in short , utilizes a subset ? of joint states to define the posterior , i.e. q( Q , A ) ? p( Q , A , C ) , { Q , A} ? ? , 0 o.w , but any subset of states , and any convex set of weights , can be used to form a lower bound .
log p ( C ) = log Q, A p( C , Q , A ) ? log Q , A? p( C , Q , A ) ? Q , A? q( Q , A|C ) log p( C , Q , A ) q( Q , A|C ) = L ? ( C ) ( 9 ) When q( Q , A|C ) ? p( Q , A , C ) , { Q , A} ? ? , 0 o.w , the second bound is tight .
In general q( Q , A|C ) takes the form : q( Q , A|C ) = q( Q , A , C ) q( C ) ( 10 ) where : q( C ) = Q , A? q( Q , A , C ) ( 11 )
The consequence of this is that all the bounds derived above hold , even if the sums over all QA pairs are taken over a prioritized set .
The process of defining q thus can be decomposed into two steps : 1 . define the subset of QA pairs to include in the set ?
2 . define the convex weights on the identified set , q ( Q , A|C ) .
Training Objectives
Typically when the end goal is data modeling , the probability of each data example is maximized indiscriminately .
The maximum likelihood objective over the adaptation dataset { C t } in this case is : O M L = t L ? ( C t ) ( 12 )
However , when training inference networks to make predictions for a given set of contexts in an unsupervised manner , it is natural to put more emphasis on confident predictions .
A natural selftraining objective in this context is to weigh each context according to it 's current estimated probability , so as to avoid re-enforcing poor predictions : O ST ( ? ) = t q( C t ) ? L ? ( C t ) ( 13 ) With ? ? 0 . When ? = 0 , The standard ML objective is recovered .
When ? = 1 , the normalization and the overall objective reduces to self-training on the current estimate of the joint distribution over { Q , A , C} triples : O ST ( 1 ) = t { Q , A}? t q( Q , A , C t ) log p( Q , A , C t ) ( 14 )
We found that O ST ( 1 ) consistently , leads to slightly better QA performance than pre-training with O M L , and so we optimize O ST ( 1 ) during the pre-training step for all results in this paper , except where otherwise noted .
9
The question answer posterior ( QAP ) based likelihood approximation
As discussed above , summing over all QA pairs is intractable , and so we select the set ? by prioritizing wrt p ( Q|A , C ) p ( A|C ) via beam search .
Given the set , we need to approximate p( Q , A , C ) .
Here we approximate the likelihood function p( C |Q , A ) by the QA posterior p( A |Q , C ) , which will attribute low scores to poor QA pairs .
Since the answers A are abstractive , this is a good approximation .
With p( C |Q , A ) ? p( A|Q , C ) , and ? prioritizing wrt p ( Q|A , C ) p ( A|C ) via beam search , we utilize the tightest possible TVI bound on ( 7 ) : q( Q , A , C ) ? p( A |Q , C ) p ( Q|A , C ) p ( A | C ) ( 15 )
Treating these prioritized samples as independent samples from p ( Q|A , C ) p ( A|C ) 1 , the final weight on each sample after importance sampling is proportional to p ( A | Q , C ) .
The resulting per example adaptation objective for our generators and QA system are given by : O = Q , A ? p( A|Q , C ) [ log p ? a |q ( A|Q , C ) + log p ? q|a ( Q|A , C ) + log p ?a ( A|C ) ] ( 16 ) Where p( A |Q , C ) = p( A | Q , C ) ( Q , A ? p( A| Q , C ) ) ? when the objective O ST ( ? ) is used ( p( A | Q , C ) when ? = 1 ) .
In essence , the generators are trained on the reward signal p( A |Q , C ) .
Note that the expected gradient of the QA term is not zero , because the set set has been prioritized .
In addition , dropout is used during training , to encourage invariance to dropout noise on generated inputs .
Adding noise to the input would similarly improve performance , but we leave this to future work .
Effectively this approach further prioritizes the set of questions produced by beam search based on the posterior distribution of the answer that the question was generated for , and encourages prediction consistency over noisy representations .
Interpreting existing approaches Current techniques can be understood in the context of the above bounds , which provides new theoretical justification for these methods , and furthermore justifies the iterative extensions of these methods presented herein .
Beam Search and Augment ( BSA )
The most straightforward approach to incorporating synthetic data is to generate QA pairs using beam search using the generators , p ( A|C ) , p( Q|A , C ) , and then treating the generated data as ground - truth data with weight 1 .
Selecting a subset ?
QA of QA pairs here corresponds to an instance of truncated variational inference to define the support of q( Q , A ) , and using weights of one corresponds to a uniform distribution over the selected set , for lack of any way to approximate the likelihood function , and use of O ST ( ? ) with ? = 1 .
Roundtrip Consistency Roundtrip consistency , introduced in , generates a set of candidate QA pairs using beam search , and accepts the QA pair only if arg max A p( A| Q i , C ) = A i .
The authors sketch two potential avenues for the formal justification of RTC in the appendix , but state that " a key question for future work is to develop a more formal understanding of why the roundtrip method improves accuracy " .
Under the presented framework , RTC can be easily interpreted .
The RTC procedure for validating QA pairs is an instance of the Iterated Conditional Modes ( ICM ) algorithm ( Besag , 1986 ) for identifying local modes of a posterior distribution .
The ICM algorithm consists of iteratively maximizing the conditional posterior of one variable ( set ) , given all the others , until convergence .
The resulting set of variables is a " conditional mode " : the posterior mode of each variable ( set ) , conditioned on all other variables are consistent .
RTC performs only one iteration , and discards the example if consistency is not satisfied .
In this manner RTC goes further than beam search , and accepts only prioritized generations that are conditional modes of the underlying posterior as members of the set ?
QA .
Like BSA , accepted QA pairs are given weight 1 , and so like BSA , corresponds to utilizing a uniform distribution over the ( smaller ) set ? that satisfies cycle consistency , and the self-training objective O ST ( ? ) ( 14 ) with ? = 1 . The QAP approach to approximating the likelihood represents a soft generalization of RTC , where QA pairs are re-weighted based on the probabilistic cycle consistency criterion q( Q , A ) ? p( A|C ) p ( Q|A , C ) p ( A ) , which , as we have shown , produces the tightest bounds on log p ( C ) under the model for any prioritized set ?.
To retain the speed and performance advantages of selecting only modes , but still assign soft scores , we set a threshold of 0.5 on the QAP filter , so that only modes will be selected , but they will have soft scores associated with them .
Related Work Several authors have recently investigated the use of synthetic data to improve question answering systems , with most of the work we are aware of either augmenting the training set , or generating synthetic QA pairs on auxiliary data ( Sachan and Xing , 2018 ; Du et al. , 2017 ; Song et al. , 2017 ; Dong et al. , 2019 ; Zhang and Bansal , 2019 ) .
In most existing work , the generators are trained only on ground - truth data .
One exception is ( Sachan and Xing , 2018 ) , which jointly self-trains question given answer ( RNN ) and answer given question ( Attentive Reader ) models .
Another is , which introduces a domain classifier to adapt to target domain data .
However , in contrast with our work , they train their QA system component only on ground - truth source data , because they found that synthetic data degrades QA system performance , whereas our PT QA systems trained only on synthetic data often outperform the baseline .
In this work , we focus on using synthetic data to adapt to target data , and iteratively improve both the generators and the QA system during the adaptation process using pre-trained Transformers , by alternating between self-supervised pre-training and ground - truth fine -tuning phases .
The closest existing work to ours that we are aware of is the Roundtrip Consistency paper itself , whose " fine- tuning only " experiments parallel ours , in that they fine- tune pre-trained BERT models to define the answer and question generators , and the QA system ( SQUAD2 gain 0.9 F1 with 3M QA pairs ) .
Note that , in contrast with our work , their generators were trained only on groundtruth data , and the augmentation process was not iterated .
An important element of this work is to show that iterative Roundtrip Consistency filtering and the Answer Posterior filtering method proposed here approximately optimize a lower bound on the probability of the target domain data .
In ( Lewis and Fan , 2018 ) , the authors propose a generative model for question answering , which purposefully avoids the use of a QA decoder , p( A |Q , C ) , instead inverting the generative model p ( A|C ) , p( Q|A , C ) explicitly over a prioritized set of answers using beam search , so that an answer must be able to generate the question to be an answer .
Additional discriminative training is required to make the approach work well , but nevertheless , this desiderata is captured by both Roundtrip Consistency filtering and Answer Posterior filtering : yes , generated questions are verified by virtue of matching generated and predicted answers , but also answers are validated by the generators ' ability to communicate it when the answer ?
question ? answer loop is enforced .
Yet another manifestation of this idea is regularizing the question generator p( Q|A , C ) and question answering system p( A |Q , C ) to encourage them to be consistent during ( supervised ) training .
In ( Dong et al. , 2019 ) the authors investigate the use of UniLM on SQUAD and fine- tune UniLM as a question generator to augment the training set with new questions for ground truth answers .
Our preliminary experiments indicate that this is a straightforward way to significantly boost baseline QA and Q generator performance before adapting to target data , given that the ground truth answers are available , using the methods described herein .
12 Experiments Method QAP selection QAP weight BSA T OP K [ p ( A |C ) p ( Q|A , C ) ] 1 RTC T OP K [ p ( A | C ) p ( Q|A , C ) ] & arg max A p( A | Q i , C ) = A i 1 QAP T OP K [ p ( A |C ) p ( Q|A , C ) ] & p( A i | Q i , C ) > 0.5 p( A i | Q i , C )
Test Scenarios
To evaluate the efficacy of unsupervised selftraining for QA system adaptation , we consider the following scenarios : ? Well Matched Target Conditions , Limited Training Data ( WM - LT ) ? Well Matched Target Conditions , Plentiful Training Data ( WM - PT ) ? Mismatched Target Conditions , Plentiful Training Data ( MM - PT )
In this paper well matched conditions ( WM - LT , WM - PT ) are assessed by adapting SQUAD - trained models on SQUAD development data ( Rajpurkar et al. , 2016 ) , and mismatched conditions by adapting SQUAD - trained models on the portion of the Natural - Questions ( NQ ) development data ( Kwiatkowski et al. , 2019 ) that contains abstractive short answers .
Models
In this paper all QA generation and answering models are trained by fine- tuning BERT ( base , uncased , unless noted otherwise ) .
The question generator is trained as a left-to- right sequence - to- sequence model , which conditions on the observed context .
Ground - truth or generated answers are marked in the context by introducing additional segment ids to mark the answer .
All models are iteratively pretrained on synthetic generated data and fine-tuned on SQUAD using essentially the standard BERT fine -tuning recipe for SQUAD ( ADAM optimizer , std .
parameters ; lr=3e-5 ; 2 epochs , " warmup linear " schedule for both PT and FT ) .
All experiments were conducted on 4 or 8 node V -100 machines , and all models were adapted for two iterations .
Model
Speed Both RTC and QAP generally are significantly faster than basic BSA while performing on - par or better than basic beam search augmentation ( 7 , 6 hrs vs 26 hrs during the first iteration using 8 V-100s , for 32 a. , 8 q/a per c , see Table 5 ) .
Qualitatively we found that BSA generally does n't benefit from additional iterations , while the performance of RTC and QAP generally saturates after 2 - 3 iterations , and delivers consistently better performance with lower total training time .
13 Discussion and Future Work
In this paper we have related self-training of question answering systems to the generative modeling of their associated context , and showed the former can be specified to optimize an approximate lower bound on the probability of the data .
We then investigated iterative " pre-train then fine- tune " approaches to target domain adaptation , proposed question answer posterior ( QAP ) as an alternative form of consistency filtering , and provided theoreti - Figure 1 : 1 Figure 1 : Overview of Approach :
We adapt to unlabelled target data by iterating between generating synthetic question answer ( QA ) pairs and approximately maximizing the probability of the observed contexts C , and finetuning on all available ground truth ( GT ) QA data .
Please refer to ( 6 - 9 ) for further details .
Table 1 : 1 Summary of iterative adaptation techniques investigated in this paper : Beam Search Adaptation ( BSA ) , Roundtrip Consistency ( RTC ) , and Question Answer Posterior ( QAP ) .
Composition of the approximate QAC likelihood , which defines the weight of each generated example during pre-training , consists of 1 ) Selecting a set ? of QA pairs for the context C , and 2 ) setting a weight for each selected pair .
Table 2 : 2 Limited training data scenario : Squad 1.1 QA adaptation EM / F1 results on dev ( GT data is 9 K of Squad1.1 train , bsize 24 , pt bsize 80 , 4/8 q/a per c , first / best iter. results shown ) .
See section 12.3 for details , and table 1 for a description of each technique .
EM F1 Baseline -FT -/69.6 -/79.7 BAS -PT 56.7/58.1 69.0/71.3 BAS -FT 71.1/71.1 81.2/81.2 RTC -PT 65.6/67.4 75.4/77.5 RTC -FT 71.6/71.6 81.2/81.5 QAP -PT 64.7/66.1 75.2/77.2 QAP -FT 71.5/71.7 81.4/81.8 12.3 Results : Well Matched , Limited Training Data Table 2 depicts SQUAD 1.1 QA adaptation results on dev ( GT data is 9 K of SQUAD 1.1 training set , ft batch size 24 , pre-training batch size 80 ) .
For this test the answer nbest and question nbest were set to 8 and 4 respectively , yielding 32 QA pairs per paragraph ( 2554 dev paragraphs ) before filtering .
The performance of the first and best iteration are depicted for each metric , for all models .
Looking at the results , we can see that RTC - FT and QAP - FT , which do additional filtering / re-weighting based on QA feedback , respectively , slightly outperform using the prioritized beam search results as gt adap - tation data ( BSA - FT ) .
Note however , that both RTC and QAP are generally significantly more efficient to train than BSA , as a significant percentage of the prioritized synthetic data is discarded ( the amount discarded depends on both the strength of the gener - ator and how mismatched the target conditions are , see table 5 for further analysis ) .
The performance gap between pre-trained models ( PT ) , which are trained only on generated data , in contrast , is more marked .
Filtering improves PT model performance substantially .
Table 3 : 3 Mismatched Target Data Scenario : NQ results on dev ( GT data is all of the Squad1.1 training set , bsize 24 , pt bsize 96 , 4/16 q/a per c , first / best iter .
results shown ) .
See section 12.4 and table 1 for details .
12.4 Results : Mismatched Target Conditions , Plentiful Training Data Table 3 depicts SQUAD 1.1 QA adaptation results on dev ( GT data is all , ? 90K , of the Squad1.1 training set , ft batch size 24 , pre-training batch size 96 ) .
For this test the answer nbest and question nbest were set to 16 and 4 respectively , yielding 64 QA pairs per paragraph ( 3703 dev paragraphs ) before filtering .
Here we also compare QAP - based adaptation to and in combination with BERT - based bidirectional LM - pretraining ( LMPT ) , and find that 1 ) QAP outperforms LMPT , and 2 ) LMPT and then QAP adaptation applied in succession leads to the best results .
Iterative QAP significantly outper - forms the baseline , and slightly outperforms both iterative RTC and BSA , but all results are signifi- cantly lower that on the SQUAD dev set .
An im - portant difference between SQUAD and NQ is that NQ answers are often significantly longer , which is a prominent bias in the definition of what an answer is , which may be difficult to overcome with just un - supervised adaptation .
Nevertheless , all 3 methods are able to improve the baseline system somewhat , and outperform and improve upon LMPT .
12.5 Results : Well Matched Target Conditions , Plentiful Training Data Table 4 depicts SQUAD 1.1 QA adaptation re- sults on dev ( GT data is 9 K of Squad1.1 train - ing set , ft batch size 24 , pre-training batch size 80 unless otherwise noted ) .
For this test the an- swer nbest and question nbest were set to 8 and 4 respectively , yielding 32 QA pairs per paragraph ( 2554 dev paragraphs ) before filtering .
Here all fil -
Table 4 : 4 Well Matched Target Data Scenario : Squad 1.1 results on dev ( GT data is all of the Squad1.1 training set , bsize 24 , pt bsize 32 , 4/8 q/a per c , first / best iter .
results shown ) .
See section 12.5 and table 1 for details .
* pt bsize 96 ; p( A|C ) not adapted ; 8,32 a , q / a ; 3 iterations .
Figure 2 : QAP performance as a function of iteration and # QA pairs ( WM - LT condition ) .
PT models are trained only on generated data .
Model EM F1 A , Q/A Synth .
Data Relative Speed Baseline -FT -/ 69.6 -/ 79.7 - - - 18 K Model ( 2X GTD ) -/ 72.3 -/ 81.7 - - - BSA-PT / FT 58.8 / 71.9 73.6 / 82.1 32 , 8 +1.1M , +1.1M 1X , 1X RTC -PT / FT 71.6 / 72.6 81.1 / 82.4 32 , 8 +220K , +360 K 5X , 3X QAP -PT / FT 72.5 / 72.7 81.7 / 82.2 32 , 8 +180K , +320 K 6X , 3X RTC -PT /FT * 71.7 / 72.5 81.4 / 82.3 32 , 8 + ( 220 , 350 , 422 ) K ( 5,3,4 ) X QAP -PT/FT * 72.9 / 73.4 82.2 / 83.0 32 , 8 + ( 180,310,385 ) K ( 6,3,3 ) X BSA -PT / FT 58.1 / 71.1 71.3 / 81.2 8 , 4 +130K , +130K 1X , 1X RTC -PT / FT 67.4 / 71.6 77.5 / 81.5 8 , 4 +60K , +90K 2X , 1.5X QAP -PT / FT 68.3 / 71.8 78.0 / 81.6 8 , 4 + 50K , +80K 2.5X , 1.5X
Table 5 : 5 Final PT / FT performance as a function of number of prioritized questions and answers per target data context paragraph .
Limited training data scenario : Squad 1.1 QA adaptation EM / F1 results on dev ( GT data is 9 K of Squad1.1 train ) .
See table 1 for a description of each technique .
Remarkably , the PT models , which are trained only on generated data , are able to outperform the baseline system , provided a sufficient number of synthetic example per target context paragraph are generated .
Each PT iteration with BSA 8,32 takes 13 hrs with an 8 V100s : RTC 8,32 is 5X , and 3X faster during iteration 1&2 , respectively , due to data pruning , and QAP 8 , 32 is faster than RTC .
All 8,32 adapted models outperform an 18 K GT only model , trained on 2X the data .
When we mean normalize the average training weight , do not adapt the answer prior p( A|C ) , and adapt for 3 iterations , RTC does not improve , but QAP is significantly more effective ( * ) .
cal justification for Roundtrip Consistency filtering .
In general , the techniques work quite well , particularly in better-matched conditions .
While effective , iteratively re-training the QA generators and QA system is inefficient , even with strong data filters .
How to most efficiently and effectively adapt Transformer - based QA systems remains an important topic for future research .
In practice prioritized samples often lead to better performance when treated as independent samples , we also found that this was the case .
