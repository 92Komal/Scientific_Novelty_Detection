title
Multi-passage BERT : A Globally Normalized BERT Model for Open-domain Question Answering
abstract
BERT model has been successfully applied to open-domain QA tasks .
However , previous work trains BERT by viewing passages corresponding to the same question as independent training instances , which may cause incomparable scores for answers from different passages .
To tackle this issue , we propose a multi-passage BERT model to globally normalize answer scores across all passages of the same question , and this change enables our QA model find better answers by utilizing more passages .
In addition , we find that splitting articles into passages with the length of 100 words by sliding window improves performance by 4 % .
By leveraging a passage ranker to select high-quality passages , multipassage BERT gains additional 2 % .
Experiments on four standard benchmarks showed that our multi-passage BERT outperforms all state - of - the - art models on all benchmarks .
In particular , on the OpenSQuAD dataset , our model gains 21.4 % EM and 21.5 % F 1 over all non-BERT models , and 5.8 % EM and 6.5 % F 1 over BERT - based models .
Introduction BERT model ( Devlin et al. , 2018 ) has achieved significant improvements on a variety of NLP tasks .
For question answering ( QA ) , it has dominated the leaderboards of several machine reading comprehension ( RC ) datasets .
However , the RC task is only a simplified version of the QA task , where a model only needs to find an answer from a given passage / paragraph .
Whereas , in reality , an open-domain QA system is required to pinpoint answers from a massive article collection , such as Wikipedia or the entire web .
Recent studies directly applied the BERT - RC model to open-domain QA ( Yang et al. , 2019 ; Nogueira et al. , 2018 ; Alberti et al. , 2019 ) .
They firstly leverage a passage retriever to retrieve multiple passages for each question .
During training , passages corresponding to the same question are taken as independent training instances .
During inference , the BERT - RC model is applied to each passage individually to predict an answer span , and then the highest scoring span is selected as the final answer .
Although this method achieves significant improvements on several datasets , there are still several unaddressed issues .
First , viewing passages of the same question as independent training instances may result in incomparable answer scores across passages .
Thus , globally normalizing scores over all passages of the same question ( Clark and Gardner , 2018 ) may be helpful .
Second , previous work defines passages as articles , paragraphs , or sentences .
However , the question of proper granularity of passages is still underexplored .
Third , passage ranker for selecting high-quality passages has been shown to be very useful in previous open-domain QA systems ( Wang et al. , 2018a ; Lin et al. , 2018 ; Pang et al. , 2019 ) .
However , we do not know whether it is still required for BERT .
Fourth , most effective QA and RC models highly rely on explicit inter-sentence matching between questions and passages ( Wang and Jiang , 2017 ; Wang et al. , 2016 ; Seo et al. , 2017 ; , whereas BERT only applies self-attention layers over the concatenation of a question - passage pair .
It is unclear whether the inter-sentence matching still matters for BERT .
To answer these questions , we conduct a series of empirical studies on the OpenSQuAD dataset ( Rajpurkar et al. , 2016 ; Wang et al. , 2018a ) .
Experimental results show that : ( 1 ) global normalization makes QA model more stable while pinpointing answers from large number of passages ; ( 2 ) splitting articles into passages with the length of 100 words by sliding window brings 4 % improvements ; ( 3 ) leveraging a BERT - based passage ranker gives us extra 2 % improvements ; and ( 4 ) explicit inter-sentence matching is not helpful for BERT .
We also compared our model with state- ofthe - art models on four standard benchmarks , and our model outperforms all state - of - the - art models on all benchmarks .
Model Open-domain QA systems aim to find an answer for a given question from a massive article collection .
Usually , a retriever is leveraged to retrieve m passages P = [ P 1 , ... , P i , ... , P m ] for a given question Q = ( q 1 , ... , q | Q | ) , where P i = ( p 1 i , ... , p |p i | i ) is the i-th passage , and q k ?
Q and p j i ?
P i are corresponding words .
A QA model will compute a score P r( a |Q , P ) for each possible answer span a .
We further decompose the answer span prediction into predicting the start and end positions of the answer span P r( a |Q , P ) = P s ( a s |Q , P ) P e ( a e |Q , P ) , where P s ( a s |Q , P ) and P e ( a e |Q , P ) are the probabilities of a s and a e to be the start and end positions .
BERT - RC model assumes passages in P are independent of each other .
The model concatenates the question Q and each passage P i into a new sequence " [ CLS ] p 1 i , ... , p |p i | i [ SEP ] q 1 , ... , q | Q | [ SEP ] " , and applies BERT to encode this sequence .
Then the vector representation of each word position from BERT encoder is fed into two separate dense layers to predict the probabilities P s and P e ( Devlin et al. , 2018 ) .
During training , the log-likelihood of the correct start and end positions for each passage is optimized independently .
For passages without any correct answers , we set the start and end positions to be 0 , which is the position for the first token [ CLS ] .
During inference , BERT - RC model is applied to each passage individually to predict an answer , and then the highest scoring span is selected as the final answer .
If answers from different passages have the same string , they are merged by summing up their scores .
Multi-passage BERT : BERT - RC model normalizes probability distributions P s and P e for each passage independently , which may cause incomparable answer scores across passages .
To tackle this issue , we leverage the global normalization method ( Clark and Gardner , 2018 ) to normalize answer scores among multiple passages , and dub this model as multi-passage BERT .
Concretely , all passages of the same question are pro-cessed independently as we do in BERT - RC until the normalization step .
Then , sof tmax is applied to normalize all word positions from all passages .
Passage ranker reranks all retrieved passages , and selects a list of high-quality passages for the multi-passage BERT model .
We implement the passage ranker as another BERT model , which is similar to multi-passage BERT except that at the output layer it only predicts a single score for each passage based on the vector representation of the first token [ CLS ] .
We also apply sof tmax over all passage scores corresponding to the same question , and train to maximize the log-likelihood of passages containing the correct answers .
Denote the passage score as P r( P i |Q , P ) , then the score of an answer span from passage P i will be P r( P i |Q , P ) P s ( a s |Q , P ) P e ( a e |Q , P ) .
Experiments Datasets :
We experiment on four open-domain QA datasets .
( 1 ) OpenSQuAD : question - answer pairs are from SQuAD 1.1 ( Rajpurkar et al. , 2016 ) , but a QA model will find answers from the entire Wikipedia rather than the given context .
Exact Match ( EM ) and F 1 scores ( Rajpurkar et al. , 2016 )
Model Analysis
To answer questions from section 1 , we conduct a series of experiments on OpenSQuAD dataset , and report the validation set results in Table 1 . Multipassage BERT model is used for experiments .
Effect of passage granularity : Previous work usually defines passages as articles ( Chen et al. , 2017 ) , paragraphs ( Yang et al. , 2019 ) , or sentences ( Wang et al. , 2018a ; Lin et al. , 2018 ) .
We explore the effect of passage granularity regarding to the passage length , i.e. , the number of words in each passage .
Each article is split into nonoverlapping passages based on a fixed length .
We vary passage length among { 50 , 100 , 200 } , and list the results as models ( 2 ) ( 3 ) ( 4 ) in Table 1 , respectively .
Comparing to single-sentence passages ( model ( 1 ) ) , leveraging fixed - length passages works better , and passages with 100 words works the best .
Hereafter , we set passage length as 100 words .
Effect of sliding window : Splitting articles into non-overlapping passages may force some nearboundary answer spans to lose useful contexts .
To deal with this issue , we split articles into overlapping passages by sliding window .
We set the window size as 100 words , and the stride as 50 words ( half the window size ) .
Result from the sliding window model is shown as model ( 6 ) in Table 1 .
We can see that this method brings us 4.7 % EM and 4.1 % F 1 improvements .
Hereafter , we use sliding window method .
Effect of passage ranker :
We plug the passage ranker into the QA pipeline .
First , the retriever returns top - 100 passages for each question .
Then , the passage ranker is employed to rerank these 100 passages .
Finally , multi-passage BERT takes top -30 reranked passages as input to pinpoint the final answer .
We design two models to check the effect of the passage ranker .
The first model utilizes the reranked passages but without using passage scores , whereas the second model makes use of both the reranked passages and their scores .
Results are given in Table 1 as models ( 8 ) and ( 9 ) respectively .
We can find that only using reranked passages gives us 0.9 % EM and 1.0 % F 1 improvements , and leveraging passage scores gives us 1.5 % EM and 1.7 % F 1 improvements .
Therefore , passage ranker is useful for multi-passage BERT model .
Effect of global normalization :
We train BERT - RC and multi-passage BERT models using the reranked passages , then evaluate them by taking as input various number of passages .
These models are evaluated on two setups : with and without using passage scores .
F 1 scores for BERT - RC based on different number of passages are shown as the dotted and solid green curves in Figure 1 . F 1 scores for our multi-passage BERT model with similar settings are shown as the dotted and solid blue curves .
We can see that all models start from the same F 1 , because multi-passage BERT is equivalent to BERT - RC when using only one passage .
While increasing the number of passages , BERT - RC without using passage scores decreases the performance significantly , which verifies that the answer scores from BERT - RC are incomparable across passages .
This issue is alleviated to some extent by leveraging passage scores .
On the other hand , performance from multi-passage BERT without using passage scores increases at the beginning , and then flattens out after passage number is over 10 .
By utilizing passage scores , multi-passage BERT gets better performance while using more passages .
This phenomenon shows the effectiveness of global normalization , which enables the model find better answers by utilizing more passages .
Does explicit inter-sentence matching matter ?
Almost all previous state - of- the - art QA and RC models find answers by matching pas - ( Lin et al. , 2018 ) 42.2 49.3 58.8 64.5 48.7 56.3 28.7 36.6 TraCRNet ( Dehghani et al. , 2019 ) 43.2 54.0 52.9 65.1 ----HAS -QA ( Pang et al. , 2019 ) 43.2 48.9 62.7 68.7 63.6 68.9 --BERT ( Large ) ( Nogueira et al. , sages with questions , aka inter-sentence matching ( Wang and Jiang , 2017 ; Wang et al. , 2016 ; Seo et al. , 2017 ; Song et al. , 2017 ) .
However , BERT model simply concatenates a passage with a question , and differentiates them by separating them with a delimiter token [ SEP ] , and assigning different segment ids for them .
Here , we aim to check whether explicit inter-sentence matching still matters for BERT .
We employ a shared BERT model to encode a passage and a question individually , and a weighted sum of all BERT layers is used as the final tokenlevel representation for the question or passage , where weights for all BERT layers are trainable parameters .
Then the passage and question representations are input into QANet to perform inter-sentence matching , and predict the final answer .
Model ( 10 ) in 6 ) .
Therefore , we conclude that the explicit inter-sentence matching is not helpful for multi-passage BERT .
One possible reason is that the multi-head self-attention layers in BERT has already embedded the inter-sentence matching .
Comparison with State- of- the- art Models
We evaluate BERT - RC and Multi-passage BERT on four standard benchmarks , where passage scores are leveraged for both models .
We build another multi-passage BERT for each dataset by initializing it with the pre-trained BERT - Large model .
Experimental results from our models as well as other state - of - the - art models are shown in Table 2 , where the first group are open-domain QA models without using the BERT model , the second group are BERT - based models , and the last group are our multi-passage BERT models .
From Table 2 , we can see that our multi-passage BERT model outperforms all state - of - the - art models across all benchmarks , and it works consistently better than our BERT - RC model which has the same settings except the global normalization .
In particular , on the OpenSQuAD dataset , our model improves by 21.4 % EM and 21.5 % F 1 over all non-BERT models , and 5.8 % EM and 6.5 % F 1 over BERT - based models .
Leveraging BERT - Large model makes multi-passage BERT even better on TriviaQA and OpenSQuAD datasets .
Conclusion
We propose a multi-passage BERT model for open-domain QA to globally normalize answer scores across mutiple passages corresponding to the same question .
We find two effective techniques to improve the performance of multipassage BERT : ( 1 ) splitting articles into passages with the length of 100 words by sliding window ; and ( 2 ) leveraging a passage ranker to select highquality passages .
With all these techniques , our multi-passage BERT model outperforms all stateof - the - art models on four standard benchmarks .
In future , we plan to consider inter-correlation among passages for open-domain question answering ( Wang et al. , 2018 b ; Song et al. , 2018 ) . Following Chen et al. ( 2017 ) , we use the 2016 - 12 - 21 English Wikipedia dump .
5,000 QA pairs are randomly selected from the original training set as our validation set , and the remaining QA pairs are taken as our new training set .
The original development set is used as our test set .
( 2 ) TriviaQA : TriviaQA unfiltered version ( Joshi et al. , 2017 ) are used .
Following Pang et al. ( 2019 ) , we randomly hold out 5,000 QA pairs from the original training set as our validation set , and take the remaining pairs as our new training set .
The original development set is used as our test set .
( 3 ) Quasar -T ( Dhingra et al. , 2017 ) and ( 4 ) SearchQA ( Dunn et al. , 2017 ) are leveraged with the official split .
Basic Settings :
If not specified , the pre-trained BERT - base model with default hyper-parameters is leveraged .
ElasticSearch with BM25 algorithm is employed as our retriever for OpenSQuAD .
Passages for other datasets are from the corresponding releases .
During training , we use top - 10 passages for each question plus all passages ( within the top - 100 list ) containing correct answers .
During inference , we use top - 30 passages for each question .
