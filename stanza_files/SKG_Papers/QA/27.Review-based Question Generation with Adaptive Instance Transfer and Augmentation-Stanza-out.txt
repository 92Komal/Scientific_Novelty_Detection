title
Review - based Question Generation with Adaptive Instance Transfer and Augmentation *
abstract
While online reviews of products and services become an important information source , it remains inefficient for potential consumers to exploit verbose reviews for fulfilling their information need .
We propose to explore question generation as a new way of review information exploitation , namely generating questions that can be answered by the corresponding review sentences .
One major challenge of this generation task is the lack of training data , i.e. explicit mapping relation between the user-posed questions and review sentences .
To obtain proper training instances for the generation model , we propose an iterative learning framework with adaptive instance transfer and augmentation .
To generate to the point questions about the major aspects in reviews , related features extracted in an unsupervised manner are incorporated without the burden of aspect annotation .
Experiments on data from various categories of a popular E-commerce site demonstrate the effectiveness of the framework , as well as the potentials of the proposed review - based question generation task .
Introduction
The user-written reviews for products or service have become an important information source and there are a few research areas analyzing such data , including aspect extraction ( Bing et al. , 2016 ; Chen et al. , 2013 ) , product recommendation ( Chelliah and Sarkar , 2017 ) , and sentiment analysis Zhao et al. , 2018a ) .
Reviews reflect certain concerns or experiences of users on products or services , and such information is valuable for other potential consumers .
However , there are few mechanisms assisting users for efficient review digestion .
It is time - consuming for users to locate critical review parts that they care about , particularly in long reviews .
We propose to utilize question generation ( QG ) as a new means to overcome this problem .
Specifically , given a review sentence , the generated question is expected to ask about the concerned aspect of this product , from the perspective of the review writer .
Such question can be regarded as a reading anchor of the review sentence , and it is easier to view and conceive due to its concise form .
As an example , the review for a battery case product in Table 1 is too long to find sentences that can answer a user question such as " How long will the battery last ? " .
Given the generated questions in the right column , it would be much easier to find out the helpful part of the review .
Recently , as a topic attracting significant research attention , question generation is regarded as a dual task of reading comprehension in most works , namely generating a question from a sentence with a fixed text segment in the sentence designated as the answer .
Two unique characteristics of our review - based question generation task differentiate it from the previous question generation works .
First , there is no review-question pairs available for training , thus a simple Seq2Seq - based question generation model for learning the mapping from the input ( i.e. review ) to the output ( i.e. question ) cannot be applied .
Even though we can easily obtain large volumes of user-posed review sets and question sets , they are just separate datasets and cannot provide any supervision of input-output mapping ( i.e. reviewquestion pair ) .
The second one is that different from the traditional question generation , the generated question from a review sentence will not simply take a fixed text segment in the review as its Review Question
It does n't heat up like most of the other ones , and I was completely fascinated by the ultra light and sleek design for the case .
Before I was using the Mophie case but I could n't wear it often because it was like having a hot brick in your pocket , hence I had to always leave it at home .
On the contrary , with PowerBear , I never take it off because I ca n't even tell the difference .
Also it is build in a super STRONG manner and even though I dropped my phone a few times , its shock resistant technology wo n't let a single thing happen to the case or the phone .
The PowerBear case became an extension to my phone that I never have to take off because when I charge it at night , it charges both my phone and the case .
I have battery life for more than two days for normal use , i.e. not power - consuming gaming .
Does this make the phone warm during charging ?
Have any of you that own this had a Mophie ?
Does this give protection to the phone ?
Can this charge the phone and the extra battery at the same time ?
How many days it can last ?
Table 1 : A product review and the example questions .
answer .
The reason is that some reviews describing user experiences are highly context-sensitive .
For the example in Table 1 , for the review " I have battery life for more than two days for normal use , i.e. not power - consuming gaming . " and its corresponding example question " How many days it can last ? " , obviously the text segment " more than two days " is a less precise answer , while the whole review sentence is much more informative .
In some other case , even such less precise answer span cannot be extracted from the review sentence , e.g. for the example question " Does this give protection to the phone ? " and the review sentence " Also it is ... even though I dropped my phone ... , its shock resistant technology wo n't let a single thing happen to the case or the phone . " .
Of course here , a simple " Yes " or " No " answer does not make much sense as well , while the whole review sentence is a vivid and informative answer .
The above two unique characteristics raise two challenges for our task .
The first challenge , namely lacking review-question pairs as training instances , appears to be intractable , particularly given that the current end-to - end models are very data-hungry .
One instant idea is to utilize user-posed ( question , answer ) pairs as substitute for training .
However , several instance-related defects hinder the learned generation model from being competent for the review - based question generation .
Some answers are very short , e.g. " more than two days " , therefore , without necessary context , they are not helpful to generate good questions .
The second challenge , namely the issue that some verbose answers contain irrelevant content especially for subjective questions .
To handle this challenge , we propose a learning framework with adaptive instance transfer and augmentation .
Firstly , a pre-trained generation model based on user-posed answer-question pairs is utilized as an initial question generator .
A ranker is designed to work together with the generator to improve the training instance set by distilling it via removing unsuitable answer-question pairs to avoid " negative transfer " ( Pan and Yang , 2009 ) , and augmenting ( Kobayashi , 2018 ) it by adding suitable reviewquestion pairs .
For selecting suitable reviews for question generation , the ranker considers two factors : the major aspects in a review and the review 's suitability for question generation .
The two factors are captured via a reconstruction objective and a reinforcement objective with reward given by the generator .
Thus , the ranker and the generator are iteratively enhanced , and the adaptively transferred answer-question pairs and the augmented reviewquestion pairs gradually relieve the data lacking problem .
In accordance with the second characteristic of our task , it is plausible to regard a review sentence or clause as the answer to the corresponding question originated from it .
Such treatment brings in the second challenge : how can we guarantee that the generated question concentrates on the critical aspect mentioned by the review sentence ?
For example , a question like " How was the experience for gaming ? " is not a favourable generation for " I have battery life for more than two days for normal use , i.e. not power - consuming gaming . " .
To solve this problem , we incorporate aspect- based feature discovering in the ranker , and then we integrate the aspect features and an aspect pointer network in the generator .
The incorporation of such aspect-related features and structures helps the generator to focus more on critical product aspects , other than the less important parts , which is complied with the real user-posed questions .
To sum up , our main contributions are threefold .
( 1 ) A new practical task , namely question generation from reviews without annotated instance , is proposed and it has good potential for multiple applications .
( 2 ) A novel adaptive instance transfer and augmentation framework is proposed for handling the data lacking challenge in the task .
( 3 ) Review - based question generation is conducted on E-commerce data of various product categories .
Related Work Question generation ( QG ) is an emerging research topic due to its wide application scenarios such as education , goal-oriented dialogue ( Lee et al. , 2018 ) , and question answering .
The preliminary neural QG models outperform the rule-based methods relying on hand -craft features , and thereafter various models have been proposed to further improve the performance via incorporating question type ( Dong et al. , 2018 ) , answer position , long passage modeling ( Zhao et al. , 2018 b ) , question difficulty , and to the point context ( Li et al. , 2019 ) .
Some works try to find the possible answer text spans for facilitating the learning .
Question generation models can be combined with its dual task , i.e. , reading comprehension or question answering with various motivations , such as improving auxiliary task performance Golub et al. , 2017 ) , collaborating QA and QG model ( Tang et al. , 2018 , and unified learning ( Xiao et al. , 2018 ) .
Although question generation has been applied on other datasets , e.g. , Wikipedia ( Du and Cardie , 2018 ) , most of the existing QG works treat it as a dual task of reading comprehension Cui et al. , 2017 ) , namely generating a question from a piece of text where a certain text span is marked as answer , in spite of several exceptions where only sentences without answer spans are used for generating questions Chali and Baghaee , 2018 ) .
Such generation setting is not suitable for reviews due to the lack of ( question , review ) pairs and improper assumption of text span answer as aforementioned .
There are works training the question generation model with the user-written QA pairs in E-commerce sites ( Hu et al. , 2018 ; Chali and Baghaee , 2018 ) , but the practicality is limited since the questions are only generated from answers instead of reviews .
Transfer learning ( Pan and Yang , 2009 ; Li et al. , 2020 ) refers to a broad scope of methods that exploit knowledge across domains for handling tasks in the target domain .
A few terms are used for describing specific methods in this learning paradigm , e.g. , self - taught learning ( Raina et al. , 2007 ) , domain adaptation ( Long et al. , 2017 ) , etc. Based on " what to transfer " , transfer learning is categorized into four groups ( Pan and Yang , 2009 ) , namely instance transfer , feature representation transfer , parameter transfer , and relational knowledge transfer .
Our learning framework can be regarded as a case of instance transfer with iterative instance adaptation and augmentation .
The Proposed AITA Framework For handling the aforementioned issues , we propose an Adaptive Instance Transfer and Augmentation ( AITA ) framework as shown in Figure 1 .
Since the review-related processing is always sentencebased , we use " review " for short to refer to review sentence in this paper .
Its two components , namely ranker and generator , are learned iteratively .
Initially , AITA simply transfers all available ( question , answer ) pairs and trains a generator .
Then it will iteratively enhance the generator with the help of the ranker .
The ranker takes a ( question , answer ) pair and a review as its input and calculates a ranking score s .
Thus , it can rank all reviews for a given QA pair .
The ranking objective incorporates the reward provided by the generator , which helps find out those suitable reviews to form ( review , question ) pairs for training ( i.e. augmenting the training data ) .
Meanwhile , the reward from the generator also helps remove unsuitable QA pairs for training , so that it makes the transfer more adaptive .
Note that the ranker also learns to model two hidden aspect related variables for the review , which are helpful for the generator to ask about the major aspects in review .
Such an iterative instance manipulation procedure gradually transfers and augments the training set for handling review - based question generation .
Review Ranker for Data Augmentation
There are two pieces of input text for ranker .
The first one is the concatenation of a ( question , answer ) pair qa and the second one is a review sentence r. qa and r are associated with the same product .
Since the ranker is responsible for instance augmentation that provides ( question , review ) pairs , it is trained to learn a score s( qa , r ) which can be used to return suitable r's for a given qa .
Ranking with Partially Shared Encoders .
The input qa and r are encoded with two Transformer encoders with the same structure and partially shared parameters , to leverage the advantage of , where e is a word embedding and n is the text length .
The number of heads in the multi-head selfattention is denoted as m , and the output of the j-th head is written as : Q j , K j , V j = EW j Q , EW j K , EW j V ( 1 ) head j ( E ) = softmax ( Q j K jT ? d ) V j ( 2 ) where d is the dimension of word embedding .
The outputs of different heads are concatenated and the encoding for the i-th word is written as h i = [ head 1 i ; ... ; head m i ] .
To obtain the sentence representation considering the complete semantics , we apply a global attention layer on the output of the Transformer encoder : h ? = n i=1 ? i h i ( 3 ) where the attention weight ?
i = exp( h i ?M?h ) / Z ? , Z ? is the normalization , and h = h i /n .
The parameter matrix M is shared by encoders for both qa and r for capturing the common attention features across them .
After encoding qa and r as h ? ( qa ) and h ? ( qa ) , a vector g( qa , r ) is assigned with the concatenation of h ? ( qa ) , h ? ( qa ) and their difference g( qa , r ) = [ h ? ( qa ) , h ? ( r ) , |h ? ( qa ) ? h ? ( r ) | ]
The review ranking score s( qa , r ) is calculated as : s( qa , r ) = ?( W s g( qa , r ) + b s ) ( 4 ) where ? is sigmoid function .
Reinforcement Objective for Ranker Learning .
To learn an appropriate s( qa , r ) , we encounter a major challenge , namely lacking ground truth labels for ( question , review ) .
Our solution takes the generator in our framework as an agent that can provide reward for guiding the learning of ranker .
The generator is initially trained with ( question , answer ) data , and is gradually updated with adapted and augmented training instances , so that the rewards from the generator can reflect the ability of review for generating the corresponding question .
Specifically , we propose a reinforcement objective that makes use of the reward from the generator , denoted as reward G ( r , q ) .
For each pair of question and review , we take the normalized log ppl ( q |r ) in the generator as reward : reward G ( r , q ) = log ppl ( q | r ) r * ? Rqa log ppl ( q |r * ) ( 5 ) where R qa is the reviews under the same product as qa , and log ppl ( q | r ) is the log perplexity of generating a question q from a review r : log ppl ( q | r ) = ? 1 |q| t? [ 1 , |q| ] p G ( q t |r , q 1 ...q t?1 )
The reinforcement objective for the ranker is to maximize the average reward for all the reviews given a question .
The sampling probabilities for reviews are obtained via normalized ranking score , namely p( r|qa ) = s( qa , r ) / Z qa , where Z qa = r * ? Rqa s( qa , r * ) .
The loss function is : L g ( qa , r ) = E r?p( r|qa ) reward G ( r , q ) ( 6 )
The gradient calculation for the above objective is an intractable problem .
As an approximated method which performs well in the iterative algorithm , the normalization term Z qa is fixed during the calculation of the policy gradient : ?L g ( qa , r ) = r ?s( qa , r) reward G ( r , q ) / Z qa Regularization with Unsupervised Aspect Extraction .
Product aspects usually play a major role in all of product questions , answers and reviews , since they are the discussion focus of such text content .
Thus , such aspects can act as connections in modeling input pairs of qa and r via the partially shared structure .
To help the semantic vector h ? in Eqn 3 capture salient aspects of reviews , an autoencoder module is connected to the encoding layer for reconstructing h ? .
Together with the matrix M , the autoencoder can be used to extract salient aspects from reviews .
Note that this combined structure is similar to the ABAE model , which has been shown effective for unsupervised aspect extraction .
Compared with supervised aspect detection methods , such a unsupervised module avoid the burden of aspect annotation for different product categories , and our experiments demonstrate that regularization based on this module is effective .
Specifically , h ? is mapped to an aspect distribution p ? and then reconstructed : p ? = softmax ( W p ? h ? + b p ) ( 7 ) h ? = p ? ? A ( 8 ) where each dimension in p ? stands for the probability that the review contains the corresponding aspect , and h ? is the reconstruction of review representation , and A is a learnable parameter matrix .
Note that we define " aspects " as implicit aspect categories , namely clusters of associated attributes of product , which is commonly used in unsupervised aspect extraction ( Wang et al. , 2015 ; .
The reconstruction objective is written as : L ? ( qa , r ) = [ h ? ( r ) ? h ? ( r ) ] 2 / 2 . ( 9 ) Only the reconstruction of review representations is considered since we focus on discovering aspects in reviews .
1
In this way , the aspect- based reconstruction will force h ? to focus on salient aspects that facilitate the reconstruction .
The final loss function of the ranker is regularized to : L( qa , r ) = L g ( qa , r ) ? ?L ? ( qa , r ) ( 10 ) where ? is a hyper-parameter .
1
We simplified the objective in AEAB model by eliminating the additional regularization term which is not necessary when combining L ? ( qa , r ) and L g ( qa , r ) .
Question Generator in Transfer Learning
We adapt the Seq2Seq model for the aspect-focused generation model , which is updated gradually via the transferred and augmented instances .
With the help of aspect-based variables learned in ranker , the generator can generate questions reflecting the major aspect in the review .
Aspect-enhanced Encoding .
To emphasize the words related to salient aspects , the attention weight ?
i obtained in the ranker is incorporated into the word embedding .
Given an input review sentence , we obtain the extended word embedding ? i at position i : ?i = [ e i , e P OS i , e N ER i , ? i ] ( 11 ) where e i is the pre-trained word embedding , e P OS i is the one- hot POS tag of i-th word , e N ER i is a BIO feature for indicating whether the i-th word is a named entity , and ?
i indicates the aspect-based weight for the i-th word .
Bi-LSTM is adopted as the basic encoder of generator , encoding the i-th word as the concatenation of hidden states with both directions : h g i = [ ? ? h i , ? ? h i ] .
Decoding with Aspect-aware Pointer Network .
Pointer network , i.e. , copy mechanism , can significantly improve the performance of text generation .
In our task , in addition to the word- level hidden state in the decoder , the overall aspect distribution of the review can also provide clues for how likely the generator should copy corresponding review aspect words into the generated question .
The question is generated with an LSTM decoder .
The word probability for the current time step is formulated as : p 0 ( q t ) = softmax ( W 2 ? + b 2 ) and related variables are calculated as : ? = ?( W 1 [ s t , c t ] + b 1 ) , s t = LSTM (y t , s t?1 ) , c t = j z tj h g j , z tj = softmax ( h g j W h s t ) where s t is the hidden state for the t-th word in question and c t is the context encoding based on attention weight z tj .
In the pointer network , for a particular position t in the generated text , the word may be copied from a distribution based on the attention weight z t ={z tj } , where the copy probability is assigned according to the current hidden state s t .
We also Data : QA set S qa = {( q , a ) } ; review set S r ={r} ; ?
Result : S ; generator trained with S Prepare pairs of ( qa , r ) under each product Initialize the training set S = S qa For each epoch Do 1 .
Train generator with S. 2 .
Prepare the reward G ( qa , r ) as generator reward for each pair of ( qa , r ) ( each answer a in qa pairs is regarded as a review for q ) .
3 . Adapt S via removing ? instances with low reward .
4 . Train ranker according to the objective in Eqn 10 .
5 . Augment S via adding ?
pairs of instances , which are ( q , r ) pairs with top s( qa , r ) in ranker .
6 . Collect ? and p ? for instances in S from ranker .
End Algorithm 1 : Learning algorithm of AITA .
consider the influence of the aspect distribution p ? in the copy probability ? for interpolation : ? = ?( p ? W c s t + b c ) ( 12 )
The incorporation of p ? helps the pointer network to consider the overall aspect distribution of context in addition to the semantics in the current position for copying words .
Finally , the t-th word is generated from the mixture of the two distributions : p( q t ) = ( 1 ? ? ) ? p 0 ( q t ) + ? ? z t . ( 13 ) The generator is trained via maximizing the likelihood of the question q given the review r : p( r|q ) = i p( r i |q , r 1 , ... , r i?1 ) ( 14 )
Iterative Learning Algorithm
The purpose of our iterative learning , as by Alg 1 , is to update the generator gradually via the instance augmentation .
The input data for the iterative learning consists of the transferred instance set of question - answer pairs S qa , an unlabeled review set S r , and an adaption parameter ?.
When the learning is finished , two outputs are produced : the final training instances S , and the learned generator .
The training set S for generator is initialized with S qa .
In each iteration of the algorithm , the generator is trained with current S , and then S is adapted accordingly .
The ranker is trained based on the rewards from the generation , which is used for instance augmentation in S .
Thus , the training set S is updated during the iterative learning , starting from a pure ( question , answer ) set .
Analysis on the influence of the composition of S , i.e. , instance numbers of two types , is presented in Section 4.5 .
There are two kinds of updates for the instance set S : ( 1 ) adaption via removing ( q , a ) pairs with low generator reward , in order to avoid " negative transfer " ; ( 2 ) augmentation via adding ( q , r ) pairs that are top ranked by ranker , in order to increase the proportion of suitable review -question instances in training set .
The instance number hyperparameter ? for removing and adding can be set according to the scale of S qa , and more details are given in our experimental setting .
To guarantee the effective instance manipulation , two interactions exist between generator and ranker .
First , aspect-related variables for reviews obtained by ranker are part of the generator input .
The second interaction is that a reward from generator is part of the learning objective for ranker , in order to teach ranker to capture the suitable reviews for generating the corresponding question .
Experiments
Datasets
We exploit the user-written QA dataset collected in ( Wan and McAuley , 2016 ) and the review set collected in ( McAuley et al. , 2015 ) as our experimental data .
The two datasets are collected from Amazon .com separately .
We filter and merge the two datasets to obtain products whose associated QA pairs and reviews can both be found .
The statistics for our datasets can be found in Table 2 , where the numbers of product for several very large product categories are restricted to 5000 .
According to the average lengths , we can find that the whole review tend to be very long .
It justified our assumption that it is not easy for users to exploit reviews , and questions with short length can be a good catalogue for viewing reviews .
To test our question generation framework , we manually labeled 100 ground truth review-question pairs for each product category .
6 volunteers are asked to select user-posed questions and the corresponding review sentences that can serve as answers .
Specifically , the volunteers are given pairs Table 2 : Data statistics .
# : number ; p , q , a , r : product , question , answer , whole review ; s : review sentence , L q , L a , L r , L s are their average lengths .
# p # q # a # r # ( of question and review , and only consider the relevance between question and review .
The answer to the question is also accessible but it is only used for helping annotators to understand the question .
All labeled pairs are validated by two experienced annotators with good understanding for the consumer information need in E-commerce . .
The labeled instances are removed from the training set .
Experimental Settings
For each product category , we train the AITA framework and use the learned generator for testing .
The fixed 300 dimension GloVe word embeddings ( Pennington et al. , 2014 ) are used as the basic word vectors .
For all text including question , answer and review , we utilize StanfordNLP for tokenizing , lower casing , and linguistic features extraction , e.g. , NER & POS for the encoder in generator .
In ranker , the dimension of aspect distribution is set to 20 and the ? in the final loss function in Eqn 10 is set to 0.8 .
In the multi-head self-attention , the head number is set to 3 and the dimension for Q , K , V is 300 .
The dimensions of matrices can be set accordingly .
The hidden dimension in generator is set to 200 .
In the iterative learning algorithm , we set the epoch number to 10 and the updating instance number ? to 0.05 ? | S qa |.
In testing , given a review r as input for generator , the additional input variables ?( r) and p ? ( r ) are obtained via the review encoder ( Eqn 3 ) and aspect extraction ( Eqn 8 ) , which are question - independent .
For testing the effectiveness of our learning framework and the incorporation of aspect , we compare our method with the following models : G a : A sentence - based Seq2Seq generation model trained with user-written answerquestion pairs .
G P N a : A pointer network is incorporated in the Seq2Seq decoding to decide whether to copy word from the context or select from vocabulary .
G P N ar : Review data is incorporated via a retrieval - based method .
Specifically , the most relevant review sentence for each question is retrieved via BM25 method , and such review-question pairs are added into the training set .
G P N a + aspect ( Hu et al. , 2018 ) : Aspect is exploited in this model .
We trained the aspect module in our framework , i.e. only using the reconstruction objective to obtain an aspect feature extractor from reviews .
Then the aspect features and distributions can be used in the same way as in our method .
AITA refers to our proposed framework .
AITA - aspect :
All the extracted aspect-related features are removed from AITA as an ablation for evaluating the effectiveness of the unsupervised module for aspect .
For every product category , we run each model for 3 times and report the average performance with four evaluation metrics , including BLEU1 ( B1 ) , BLEU4 ( B4 ) , METEOR ( MET ) and ROUGE -L ( R L ) .
Evaluation of Question Generation
The results are demonstrated in Table 3 . AITA achieves the best performance on all product categories regarding different evaluation metrics .
The significant improvements over other models demonstrate that our instance transfer and augmentation method can indeed reduce inappropriate answerquestion pairs and provide helpful review -question pairs for the generator .
The performance of G a is very poor due to the missing of attention mechanism .
Both G P N a and G P N a + aspect have worse performance than ours , even though some product categories have large volume of QA pairs ( > 100 k ) , e.g. , Electronics , Tools , etc .
This indicates that the answer-question instances are not capable of learning a review - based question generator because of the different characteristics between the answer set and review set .
G P N ar performs much worse than G P N a , which proves that a simple retrieval method is not effective for merging the instances related to reviews and answers .
AITA adapts and augments the QA set to select suitable review-question pairs considering both aspect and generation suitability , resulting in a better generator .
In addition , effectiveness of aspect feature and aspect pointer network can be illustrated via the slight but stable improvement of G P N a + aspect over G P N a and the performance drop of AITA - aspect on all the categories .
This proves that even without precise aspect annotation , our unsupervised aspect-based regularization is helpful for improving generation .
Human Evaluation and Case Study
We conduct human evaluation on two product categories to study the quality of the generated questions .
Two binary metrics Relevance and Aspect are used to indicate whether a question can be answered by the review and whether they share the same or related product aspect .
Fluency with the value set { 1 , 2 , 3 } , is adopted for judging the question fluency .
1 means not fluent and 3 means very fluent .
We selected 50 generated questions from each model and asked 4 volunteers for evaluation .
The average scores are reported in Table 4 , which shows that our framework achieves the best performance regarding all the metrics , especially for Relevance , showing that our AITA can help generate more accurate questions based on reviews and thus facilitates exploiting reviews .
Due to the incorporation of implicit aspect information , both AITA and G P N a + aspect significantly outperform G P N a regarding both Aspect and Relevance .
Again , G P N ar with a simple retrieval method for augmenting training instances cannot perform well .
The blue sentences in Table 5 are from a long review talking about some important information of a wat ch , and the questions generated by different models are also given .
These questions are more user-friendly and potential consumers can browse them to quickly locate the information they care about .
For example , if a user wants to know more about the battery replacement , the portion before the third sentence can be skipped .
According to the generated questions via three methods in the Table 5 , we can find that the questions from AITA are asking about major aspects of the review sentences .
G P N a failed to capture major aspects in the first three sentences , and the questions generated by G P N a + aspect are not as concrete as ours , owning to the insufficient training instances .
Analysis on Instances Composition
The training instance set for the generator , i.e. , S in Algorithm 1 , is initialized with QA set and gradually adapted and augmented .
Here , we investigate the effect of composition property of S on the generator performance at different epochs .
Conclusions
We propose a practical task of question generation from reviews , whose major challenge is the lack of training instances .
An adaptive instance transfer and augmentation framework is designed for handling the task via an iterative learning algorithm .
Unsupervised aspect extraction is integrated for aspect-aware question generation .
Experiments on real-world E-commerce data demonstrate the effectiveness of the training instance manipulation in our framework and the potentials of the review - based question generation task .
Figure 1 : 1 Figure 1 : AITA framework .
M is the shared parameter matrix for QA and review .
Figure 2 : 2 Figure 2 : Analysis for proposition of instances .
As shown in Fig 2 , two product categories and two metrics are illustrated , with the gradually changed training instance set S .
The proportion of review-question ( qr ) instances in S starts with 0 , and significant performance improvement can be observed while the qr proportion gradually increases .
The results stay stable until the qr proportion reach 80 % .
Table 3 : 3 Overall performance on question generation .
Table 4 : 4
The third metric , Performance of human evaluation .
Clothing & Jewelry Relevance Aspect Fluency G P N a G P N ar G P N a + aspect 0.58 0.47 0.66 0.62 0.58 0.72 2.58 2.29 2.76 AITA 0.80 0.80 2.86 Cell Phone Relevance Aspect Fluency G P N a G P N ar G P N a + aspect 0.42 0.35 0.58 0.55 0.41 0.63 2.79 2.44 2.83 AITA 0.72 0.72 2.90
Table 5 : 5 Case study of generated questions .
