title
Neural Compositional Denotational Semantics for Question Answering
abstract
Answering compositional questions requiring multi-step reasoning is challenging .
We introduce an end-to- end differentiable model for interpreting questions about a knowledge graph ( KG ) , which is inspired by formal approaches to semantics .
Each span of text is represented by a denotation in a KG and a vector that captures ungrounded aspects of meaning .
Learned composition modules recursively combine constituent spans , culminating in a grounding for the complete sentence which answers the question .
For example , to interpret " not green " , the model represents " green " as a set of KG entities and " not " as a trainable ungrounded vector- and then uses this vector to parameterize a composition function that performs a complement operation .
For each sentence , we build a parse chart subsuming all possible parses , allowing the model to jointly learn both the composition operators and output structure by gradient descent from endtask supervision .
The model learns a variety of challenging semantic operators , such as quantifiers , disjunctions and composed relations , and infers latent syntactic structure .
It also generalizes well to longer questions than seen in its training data , in contrast to RNN , its treebased variants , and semantic parsing baselines .
Introduction Compositionality is a mechanism by which the meanings of complex expressions are systematically determined from the meanings of their parts , and has been widely assumed in the study of both artificial and natural languages ( Montague , 1973 ) as a means for allowing speakers to generalize to understanding an infinite number of sentences .
Popular neural network approaches to question answering use a restricted form of compositionality , typically encoding a sentence word - by - word , and then *
Work done while interning with Facebook AI Research .
executing the complete sentence encoding against a knowledge source ( Perez et al. , 2017 ) .
Such models can fail to generalize from training data in surprising ways .
Inspired by linguistic theories of compositional semantics , we instead build a latent tree of interpretable expressions over a sentence , recursively combining constituents using a small set of neural modules .
Our model outperforms RNN encoders , particularly when test questions are longer than training questions .
Our approach resembles Montague semantics , in which a tree of interpretable expressions is built over the sentence , with nodes combined by a small set of composition functions .
However , both the structure of the sentence and the composition functions are learned by end-to - end gradient descent .
To achieve this , we define the parametric form of small set of composition modules , and then build a parse chart over each sentence subsuming all possible trees .
Each node in the chart represents a span of text with a distribution over groundings ( in terms of booleans and knowledge base nodes and edges ) , as well as a vector representing aspects of the meaning that have not yet been grounded .
The representation for a node is built by taking a weighted sum over different ways of building the node ( similar to Maillard et al . ( 2017 ) ) .
The trees induced by our model are linguistically plausible , in contrast to prior work on structure learning from semantic objectives ( Williams et al. , 2018 ) .
Typical neural approaches to grounded question answering first encode a question with a recurrent neural network ( RNN ) , and then evaluate the encoding against an encoding of the knowledge source ( for example , a knowledge graph or image ) ( Santoro et al. , 2017 ) .
In contrast to classical approaches to compositionality , constituents of complex expressions are not given explicit interpretations in isolation .
For example , in Which cubes are large or green ? , an RNN encoder will not explic -
A correct parse for a question given the knowledge graph on the right , using our model .
We show the type for each node , and its denotation in terms of the knowledge graph .
The words or and not are represented by vectors , which parameterize composition modules .
The denotation for the complete question represents the answer to the question .
Nodes here have types E for sets of entities , R for relations , V for ungrounded vectors , EV for a combination of entities and a vector , and ? for semantically vacuous nodes .
While we show only one parse tree here , our model builds a parse chart subsuming all trees .
itly build an interpretation for the phrase large or green .
We show that such approaches can generalize poorly when tested on more complex sentences than they were trained on .
Our approach instead imposes independence assumptions that give a linguistically motivated inductive bias .
In particular , it enforces that phrases are interpreted independently of surrounding words , allowing the model to generalize naturally to interpreting phrases in different contexts .
In our model , large or green will be represented as a particular set of entities in a knowledge graph , and be intersected with the set of entities represented by the cubes node .
Another perspective on our work is as a method for learning layouts of Neural Module Networks ( NMNs ) ( Andreas et al. , 2016 b ) .
Work on NMNs has focused on construction of the structure of the network , variously using rules , parsers and reinforcement learning ( Andreas et al. , 2016a ;
Hu et al. , 2017 ) .
Our end-to - end differentiable model jointly learns structures and modules by gradient descent .
Our model is a new combination of classical and neural methods , which maintains the interpretability and generalization behaviour of semantic parsing , while being end-to - end differentiable .
Model Overview
Our task is to answer a question q = w 1 ..| q| , with respect to a Knowledge Graph ( KG ) consisting of nodes E ( representing entities ) and labelled directed edges R ( representing relationship between entities ) .
In our task , answers are either booleans , or specific subsets of nodes from the KG .
Our model builds a parse for the sentence , in which phrases are grounded in the KG , and a small set of composition modules are used to combine phrases , resulting in a grounding for the complete question sentence that answers it .
For example , in Figure 1 , the phrases not and cylindrical are interpreted as a function word and an entity set , respectively , and then not cylindrical is interpreted by computing the complement of the entity set .
The node at the root of the parse tree is the answer to the question .
Our model answers questions by : ( a ) Grounding individual tokens in a KG , that can either be grounded as particular sets of entities and relations in the KG , as ungrounded vectors , or marked as being semantically vacuous .
For each word , we learn parameters that are used to compute a distribution over semantic types and corresponding denotations in a KG ( ? 3.1 ) .
( b) Combining representations for adjacent phrases into representations for larger phrases , using trainable neural composition modules ( ? 3.2 ) .
This produces a denotation for the phrase .
( c ) Assigning a binary - tree structure to the question sentence , which determines how words are grounded , and which phrases are combined using which modules .
We build a parse chart subsuming all possible structures , and train a parsing model to increase the likelihood of structures leading to the correct answer to questions .
Different parses leading to a denotation for a phrase of type t are merged into an expected denotation , allowing dynamic programming ( ? 4 ) .
( d ) Answering the question , with the most likely grounding of the phrase spanning the sentence .
2154 3 Compositional Semantics
Semantic Types
Our model classifies spans of text into different semantic types to represent their meaning as explicit denotations , or ungrounded vectors .
All phrases are assigned a distribution over semantic types .
The semantic type determines how a phrase is grounded , and which composition modules can be used to combine it with other phrases .
A phrase spanning w i..j has a denotation w i..j t KG for each semantic type t.
For example , in Figure 1 , red corresponds to a set of entities , left corresponds to a set of relations , and not is treated as an ungrounded vector .
The semantic types we define can be classified into three broad categories .
Grounded Semantic Types : Spans of text that can be fully grounded in the KG .
Ungrounded Semantic Types : Spans of text whose meaning cannot be grounded in the KG .
1 . Vector ( V ) :
This type is used for spans representing functions that cannot yet be grounded in the KG ( e.g. words such as and or every ) .
These spans are represented using 4 different real- valued vectors v 1 -v 4 ? R 2 -R 5 , that are used to parameterize the composition modules described in ?3.2 .
2 . Vacuous ( ? ? ? ) :
Spans that are considered semantically vacuous , but are necessary syntactically , e.g. of in left of a cube .
During composition , these nodes act as identity functions .
Partially - Grounded Semantic Types :
Spans of text that can only be partially grounded in the knowledge graph , such as and red or are four spheres .
Here , we represent the span by a combination of a grounding and vectors , representing grounded and ungrounded aspects of meaning respectively .
The grounded component of the representation will typically combine with another fully grounded representation , and the ungrounded vectors will parameterize the composition module .
We define 3 semantic types of this kind : EV , RV and TV , corresponding to the combination of entities , relations and boolean groundings respectively with an ungrounded vector .
Here , the word represented by the vectors can be viewed as a binary function , one of whose arguments has been supplied .
Composition Modules Next , we describe how we compose phrase representations ( from ? 3.1 ) to represent larger phrases .
We define a small set of composition modules , that take as input two constituents of text with their corresponding semantic representations ( grounded representations and ungrounded vectors ) , and outputs the semantic type and corresponding representation of the larger constituent .
The composition modules are parameterized by the trainable word vectors .
These can be divided into several categories : Composition modules resulting in fully grounded denotations : Described in Figure 2 . Composition with ? ? ?- typed nodes :
Phrases with type ? ? ? are treated as being semantically transparent identity functions .
Phrases of any other type can combined with these nodes , with no change to their type or representation .
Composition modules resulting in partially grounded denotations :
We define several modules that combine fully grounded phrases with ungrounded phrases , by deterministically taking the union of the representations , giving phrases with partially grounded representations ( ? 3.1 ) .
These modules are useful when words act as binary functions ; here they combine with their first argument .
For example , in Fig. 1 , or and not cylindrical combine to make a phrase containing both the vectors for or and the entity set for not cylindrical .
large red E E E p ei = 2 4 w 1 w 2 b 3 5 ? 2 4 p L ei p R ei 1 3 5 ! E + E ? E : This module performs a function on a pair of soft entity sets , parameterized by the model 's global parameter vector [ w1 , w2 , b] to produce a new soft entity set .
The composition function for a single entity 's resulting attention value is shown .
Such a composition module can be used to interpret compound nouns and entity appositions .
For example , the composition module shown above learns to output the intersection of two entity sets .
not cylindrical E V E p ei = ?
v 1 ? ? p R ei 1 ? V + E ? E : This module performs a function on a soft entity set , parameterized by a word vector , to produce a new soft entity set .
For example , the word not learns to take the complement of a set of entities .
The entity attention representation of the resulting span is computed by using the indicated function that takes the v1 ?
R 2 vector of the V constituent as a parameter argument and the entity attention vector of the E constituent as a function argument .
small or purple
E E EV p ei = v 2 ? 2 4 p L ei p R ei 1 3 5 ! EV + E ? E : This module combines two soft entity sets into a third set , parameterized by the v2 word vector .
This composition function is similar to a linear threshold unit and is capable of modeling various mathematical operations such as logical conjunctions , disjunctions , differences etc. for different values of v2 .
For example , the word or learns to model set union .
left of a red cube E E R p ei = max ej A ji ? p R ej R + E ? E : This module composes a set of relations ( represented as a single soft adjacency matrix ) and a soft entity set to produce an output soft entity set .
The composition function uses the adjacency matrix representation of the R-span and the soft entity set representation of the E-span .
E V T is anything cylindrical
True p true = v 1 3 " X ei ?
v 3 3 v 4 3 ? ? p R ei 1 !# + v 2 3 ! V + E ? T : This module maps a soft entity set onto a soft boolean , parameterized by word vector ( v3 ) .
The module counts whether a sufficient number of elements are in ( or out ) of the set .
For example , the word any should test if a set is non-empty .
p true = v 1 4 " X ei 2 4 v 3 4 v 4 4 v 5 4 3 5 ? 2 4 p L ei p R ei 1 3 5 !# + v 2 4 ! False E EV T is every cylinder blue EV + E ? T : This module combines two soft entity sets into a soft boolean , which is useful for modelling generalized quantifiers .
For example , in is every cylinder blue , the module can use the inner sigmoid to test if an element ei is in the set of cylinders ( p L e i ? 1 ) but not in the set of blue things ( p R e i ? 0 ) , and then use the outer sigmoid to return a value close to 1 if the sum of elements matching this property is close to 0 .
TV + T ? T : This module maps a pair of soft booleans into a soft boolean using the v2 word vector to parameterize the composition function .
Similar to EV + E ?
E , this module facilitates modeling a range of boolean set operations .
Using the same functional form for different composition functions allows our model to use the same ungrounded word vector ( v2 ) for compositions that are semantically analogous .
A ij = v 2 ? 2 4 A L ij A R ij 1 3 5 ! left of or above R R RV RV + R ? R : This module composes a pair of soft set of relations to a produce an output soft set of relations .
For example , the relations left and above are composed by the word or to produce a set of relations such that entities ei and ej are related if either of the two relations exists between them .
The functional form for this composition is similar to EV + E ? E and TV + T ? T modules .
Figure 2 : Composition
Modules that compose two constituent span representations into the representation for the combined larger span , using the indicated equations .
Parsing Model
Here , we describe how our model classifies question tokens into semantic type spans and computes their representations ( ? 4.1 ) , and recursively uses the composition modules defined above to parse the question into a soft latent tree that provides the answer ( ? 4.2 ) .
The model is trained end-to - end using only question - answer supervision ( ? 4.3 ) .
Lexical Representation Assignment
Each token in the question sentence is assigned a distribution over the semantic types , and a grounded representation for each type .
Tokens can only be assigned the E , R , V , and ? ? ? types .
For example , the token cylindrical in the question in Fig.
1 is assigned a distribution over the 4 semantic types ( one shown ) and for the E type , its representation is the set of cylindrical entities .
Semantic Type Distribution for Tokens :
To compute the semantic type distribution , our model represents each word w , and each semantic type t using an embedding vector ; v w , v t ?
R d .
The semantic type distribution is assigned with a softmax : p( t| w i ) ? exp( v t ? v w i ) Grounding for Tokens :
For each of the semantic type , we need to compute their representations : 1 . E-Type Representation : Each entity e ?
E , is represented using an embedding vector v e ?
R d based on the concatenation of vectors for its properties .
For each token w , we use its word vector to find the probability of each entity being part of the E-Type grounding : p w e i = ?( v e i ? v w ) ? e i ?
E For example , in Fig. 1 , the word red will be grounded as all the red entities .
2 . R-Type Representation : Each relation r ? R , is represented using an embedding vector v r ? R d .
For each token w i , we compute a distribution over relations , and then use this to compute the expected adjacency matrix that forms the R-type representation for this token .
p( r|w i ) ? exp( v r ? v w i ) A w i = r?R p( r|w i ) ?
A r e.g. the word left in Fig. 1 is grounded as the subset of edges with label ' left ' .
3 . V-Type Representation :
For each word w ?
V , we learn four vectors v 1 ? R 2 , v 2 ? R 3 , v 3 ? R 4 , v 4 ? R 5 , and use these as the representation for words with the V-Type .
4 . ? ? ?- Type Representation : Semantically vacuous words that do not require a representation .
Parsing Questions
To learn the correct structure for applying composition modules , we use a simple parsing model .
We build a parse-chart over the encompassing all possible trees by applying all composition modules , similar to a standard CRF - based PCFG parser using the CKY algorithm .
Each node in the parse-chart , for each span w i..j of the question , is represented as a distribution over different semantic types with their corresponding representations .
Phrase Semantic Type Potential ( ? t i , j ) : The model assigns a score , ? t i , j , to each w i..j span , for each semantic type t.
This score is computed from all possible ways of forming the span w i..j with type t.
For a particular composition of span w i..k of type t 1 and w k+1 ..j of type t 2 , using the t 1 + t 2 ? t module , the composition score is : ? t 1 +t 2 ?t i , k , j = ? t 1 i , k ? ? t 2 k+1 , j ? e ?f t 1 +t 2 ?t ( i , j , k |q ) where ? is a trainable vector and f t 1 +t 2 ?t ( i , j , k |q ) is a simple feature function .
Features consist of a conjunction of the composition module type and : the words before ( w i?1 ) and after ( w j +1 ) the span , the first ( w i ) and last word ( w k ) in the left constituent , and the first ( w k +1 ) and last ( w j ) word in the right constituent .
The final t-type potential of w i..j is computed by summing scores over all possible compositions : ? t i , j = j?1 k=i ( t 1 +t 2 ?t ) ? Modules ?
t 1 +t 2 ?t i , k , j Combining Phrase Representations ( w i..j t KG ) :
To compute w i..j 's t-type denotation , w i..j t KG , we compute an expected output representation from all possible compositions that result in type t. w i..j t KG = 1 ? t i , j j?1 k=i w i..k..j t KG w i..k..j t KG = ( t 1 +t 2 ?t ) ? Modules ? t 1 +t 2 ?t i , k , j ? w i..k..j t 1 +t 2 ?t KG where w i..j t KG , is the t-type representation of the span w i..j and w i..k..j t 1 +t 2 ?t KG is the representation resulting from the composition of w i..k with w k+1 ..j using the t 1 +t 2 ? t composition module .
Answer Grounding :
By recursively computing the phrase semantic-type potentials and representations , we can infer the semantic type distribution of the complete question and the resulting grounding for different semantic types t , w 1 ..| q| t KG . p( t|q ) ? ?( 1 , | q| , t ) ( 1 )
The answer-type ( boolean or subset of entities ) for the question is computed using : t * = argmax t?T, E p( t | q ) ( 2 ) The corresponding grounding is w 1 ..| q| t * KG , which answers the question .
Training Objective Given a dataset D of ( question , answer , knowledgegraph ) tuples , {q i , a , KG i } i= |D| i=1 , we train our model to maximize the log-likelihood of the correct answers .
We maximize the following objective : L = i log p( a i |q i , KG i ) ( 3 ) Further details regarding the training objective are given in Appendix A .
Dataset
We experiment with two datasets , 1 ) Questions generated based on the CLEVR ( Johnson et al. , 2017 ) dataset , and 2 ) Referring Expression Generation ( GenX ) dataset ( FitzGerald et al. , 2013 ) , both of which feature complex compositional queries .
CLEVRGEN : We generate a dataset of question and answers based on the CLEVR dataset ( Johnson et al. , 2017 ) , which contains knowledge graphs containing attribute information of objects and relations between them .
We generate a new set of questions as existing questions contain some biases that can be exploited by models .
1 We generate 75 K questions for training and 37.5 K for validation .
Our questions test various challenging semantic operators .
These include conjunctions ( e.g. Is anything red and large ? ) , negations ( e.g. What is not spherical ? ) , counts ( e.g. Are five spheres green ? ) , quantifiers ( e.g. Is every red thing cylindrical ? ) , and relations ( e.g . What is left of and above a cube ? ) .
We create two test sets : 1 . Short Questions : Drawn from the same distribution as the training data ( 37.5K ) .
2 . Complex Questions : Longer questions than the training data ( 22.5K ) .
This test set contains the same words and constructions , but chained into longer questions .
For example , it contains questions such as What is a cube that is right of a metallic thing that is beneath a blue sphere ?
and Are two red cylinders that are above a sphere metallic ?
Solving these questions require more multi-step reasoning .
REFERRING EXPRESSIONS ( GENX ) ( FitzGerald et al. , 2013 ) :
This dataset contains human-generated queries , which identify a subset of objects from a larger set ( e.g. all of the red items except for the rectangle ) .
It tests the ability of models to precisely understand human- generated language , which contains a far greater diversity of syntactic and semantic structures .
This dataset does not contain relations between entities , and instead only focuses on entity -set operations .
The dataset contains 3920 questions for training , 600 for development and 940 for testing .
Our modules and parsing model were designed independently of this dataset , and we re-use hyperparameters from CLEVRGEN .
Experiments
Our experiments investigate the ability of our model to understand complex synthetic and natural language queries , learn interpretable structure , and generalize compositionally .
We also isolate the effect of learning the syntactic structure and representing sub-phrases using explicit denotations .
Experimentation Setting
We describe training details , and the baselines .
Training Details :
Training the model is challenging since it needs to learn both good syntactic structures and the complex semantics of neural modules - so we use Curriculum Learning ( Bengio et al. , 2009 ) to pre-train the model on an easier subset of questions .
Appendix
B contains the details of curriculum learning and other training details .
CLEVRGEN ) : Performance of our model compared to baseline models on the Short Questions test set .
The LSTM ( NO KG ) has accuracy close to chance , showing that the questions lack trivial biases .
Our model almost perfectly solves all questions showing its ability to learn challenging semantic operators , and parse questions only using weak end-to - end supervision .
Baseline Models :
We compare to the following baselines .
( a) Models that assume linear structure of language , and encode the question using linear RNNs - LSTM ( NO KG ) , LSTM , BI - LSTM , and a RELATION -NETWORK ( Santoro et al. , 2017 ) augmented model .
2 ( b) Models that assume tree -like structure of language .
We compare two variants of Tree-structured LSTMs ( Zhu et al. , 2015 ; Tai et al. , 2015 ) - TREE - LSTM , that operates on preparsed questions , and TREE - LSTM ( UNSUP . ) , an unsupervised Tree-LSTM model ( Maillard et al. , 2017 ) that learns to jointly parse and represent the sentence .
For GENX , we also use an end-to - end semantic parsing model from Pasupat and Liang ( 2015 ) .
Finally , to isolate the contribution of the proposed denotational - semantics model , we train our model on pre-parsed questions .
Note that , all LSTM based models only have access to the entities of the KG but not the relationship information between them .
See Appendix C for details .
Experiments Short Questions Performance : Results for Complex Questions ( CLEVRGEN ) :
All baseline models fail to generalize well to questions requiring longer chains of reasoning than those seen during training .
Our model substantially outperforms the baselines , showing its ability to perform complex multi-hop reasoning , and generalize from its training data .
Analysis suggests that most errors from our model are due to assigning incorrect structures , rather than mistakes by the composition modules .
Complex Questions Performance :
Table 2 shows results on complex questions , which are constructed by combining components of shorter questions .
These require complex multi-hop reasoning , and the ability to generalize robustly to new types of questions .
We use the same models as in Table 1 , which were trained on short questions .
All baselines achieve close to random performance , despite high accuracy for shorter questions .
This shows the challenges in generalizing RNN encoders beyond their training data .
In contrast , the strong inductive bias from our model structure allows it to generalize well to complex questions .
Our model outperforms TREE - LSTM ( UNSUP . ) and the version of our model that uses pre-parsed questions , showing the effectiveness of explicit denotations and learning the syntax , respectively .
Performance on Human-generated Language : Table 3 shows the performance of our model on complex human-generated queries in GENX .
Our approach outperforms strong LSTM and semantic parsing baselines , despite the semantic parser 's use of hard -coded operators .
These results suggest that our method represents an attractive middle ground between minimally structured and highly structured approaches to interpretation .
Our model learns to interpret operators such as except that were not considered during development .
This shows that our model can learn to parse human language , which contains greater lexical and structural diversity than synthetic questions .
Trees induced by the model are linguistically plausible ( see Appendix D ) .
Error Analysis :
We find that most model errors are due to incorrect assignments of structure , rather than semantic errors from the modules .
For example , in the question Are four red spheres beneath a metallic thing small ? , our model 's parse composes metallic thing small into a constituent instead of composing red spheres beneath a metallic thing into a single node .
Future work should explore more sophisticated parsing models .
Discussion :
While our model shows promising results , there is significant potential for future work .
Performing exact inference over large KGs is likely to be intractable , so approximations such as KNN search , beam search , feature hashing or parallelization may be necessary .
To model the large number of entities in KGs such as Freebase , techniques proposed by recent work ( Verga et al. , 2017 ; Gupta et al. , 2017 ) that explore representing entities as composition of its properties , such as , types , description etc.
could be used .
The modules in this work were designed in a way to provide good inductive bias for the kind of composition we expected them to model .
For example , EV + E ?
E is modeled as a linear composition function making it easy to represent words such as and and or .
These modules can be exchanged with any other function with the same ' type signature ' , with different tradeoffs - for example , more general feed -forward networks with greater representation capacity would be needed to represent a linguistic expression equivalent to xor .
Similarly , more module types would be required to handle certain constructions - for example , a multiword relation such as much larger than needs a V + V ?
V module .
This is an exciting direction for future research .
Related Work Many approaches have been proposed to perform question - answering against structured knowledge sources .
Semantic parsing models have learned structures over pre-defined discrete operators , to produce logical forms that can be executed to answer the question .
Early work trained using goldstandard logical forms ( Zettlemoyer and Collins , 2005 ; Kwiatkowski et al. , 2010 ) , whereas later efforts have only used answers to questions ( Clarke et al. , 2010 ; Liang et al. , 2011 ; Krishnamurthy and Kollar , 2013 ) .
A key difference is that our model must learn semantic operators from data , which may be necessary to model the fuzzy meanings of function words like many or few .
Another similar line of work is neural program induction models , such as Neural Programmer and Neural Symbolic Machine ( Liang et al. , 2017 ) .
These models learn to produce programs composed of predefined operators using weak supervision to answer questions against semi-structured tables .
Neural module networks have been proposed for learning semantic operators ( Andreas et al. , 2016 b ) for question answering .
This model assumes that the structure of the semantic parse is given , and must only learn a set of operators .
Dynamic Neural Module Networks ( D- NMN ) extend this approach by selecting from a small set of candidate module structures ( Andreas et al. , 2016 a ) .
We instead learn a model over all possible structures .
Our work is most similar to N2NMN ( Hu et al. , 2017 ) model , which learns both semantic operators and the layout in which to compose them .
However , optimizing the layouts requires reinforcement learning , which is challenging due to the high variance of policy gradients , whereas our chart - based approach is end-to - end differentiable .
Conclusion
We have introduced a model for answering questions requiring compositional reasoning that combines ideas from compositional semantics with endto-end learning of composition operators and structure .
We demonstrated that the model is able to learn a number of complex composition operators from end task supervision , and showed that the linguistically motivated inductive bias imposed by the structure of the model allows it to generalize well beyond its training data .
Future work should explore scaling the model to other question answering tasks , using more general composition modules , and introducing additional module types .
Figure1 : A correct parse for a question given the knowledge graph on the right , using our model .
We show the type for each node , and its denotation in terms of the knowledge graph .
The words or and not are represented by vectors , which parameterize composition modules .
The denotation for the complete question represents the answer to the question .
Nodes here have types E for sets of entities , R for relations , V for ungrounded vectors , EV for a combination of entities and a vector , and ? for semantically vacuous nodes .
While we show only one parse tree here , our model builds a parse chart subsuming all trees .
