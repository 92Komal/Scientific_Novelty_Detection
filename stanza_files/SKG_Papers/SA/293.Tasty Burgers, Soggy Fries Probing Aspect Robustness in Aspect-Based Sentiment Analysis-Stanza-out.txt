title
Tasty Burgers , Soggy Fries : Probing Aspect Robustness in Aspect- Based Sentiment Analysis
abstract
Aspect - based sentiment analysis ( ABSA ) aims to predict the sentiment towards a specific aspect in the text .
However , existing ABSA test sets cannot be used to probe whether a model can distinguish the sentiment of the target aspect from the non-target aspects .
To solve this problem , we develop a simple but effective approach to enrich ABSA test sets .
Specifically , we generate new examples to disentangle the confounding sentiments of the non-target aspects from the target aspect 's sentiment .
Based on the SemEval 2014 dataset , we construct the Aspect Robustness Test Set ( ARTS ) as a comprehensive probe of the aspect robustness of ABSA models .
Over 92 % data of ARTS show high fluency and desired sentiment on all aspects by human evaluation .
Using ARTS , we analyze the robustness of nine ABSA models , and observe , surprisingly , that their accuracy drops by up to 69.73 % .
We explore several ways to improve aspect robustness , and find that adversarial training can improve models ' performance on ARTS by up to 32.85 % .
1
Introduction Aspect- based sentiment analysis ( ABSA ) is an advanced sentiment analysis task that aims to classify the sentiment towards a specific aspect ( e.g. , burgers or fries in the review " Tasty burgers , and crispy fries . " ) .
The key to a strong ABSA model is it is sensitive to only the sentiment words of the target aspect , and therefore not be interfered by the sentiment of any non-target aspect .
Although stateof - the- art models have shown high accuracy on existing test sets , we still question their robustness .
Specifically , given the prerequisite that a model outputs correct sentiment polarity for the test example , we have the following questions : ( Q1 ) If we reverse the sentiment polarity of the target aspect , can the model change its prediction accordingly ?
( Q2 )
If the sentiments of all non-target aspects become opposite to the target one , can the model still make the correct prediction ? ( Q3 )
If we add more non-target aspects with sentiments opposite to the target one , can the model still make the correct prediction ?
A robust ABSA model should both meet the prerequisite and have affirmative answers to all the questions above .
For example , if a model makes the correct sentiment classification ( i.e. , positive ) for burgers in the original sentence " Tasty burgers , and crispy fries " , it should flip its prediction ( to negative ) when seeing the new context " Terrible burgers , but crispy fries " .
Hence , these questions together form a probe to verify if an ABSA model has high aspect robustness .
Unfortunately , existing ABSA datasets have very limited capability to probe the aspect robustness .
For example , the Twitter dataset ( Dong et al. , 2014 ) has only one aspect per sentence , so the model does not need to discriminate against non-target aspects .
In the most widely used SemEval 2014 Laptop and Restaurant datasets ( Pontiki et al. , 2014 ) , for 83.9 % and 79.6 % instances in the test sets , the sentiments of the target aspect , and all non-target aspects are all the same .
Hence , we cannot decide whether models that make correct classifications attend only to the target aspect , because they may also wrongly look at the non-target aspects , which are confounding factors .
Only a small portion of the test set can be used to answer our target questions proposed in the beginning .
Moreover , when we test on the subset of the test set ( 59 instances in Laptop , and 122 instances in Restaurant ) where the target aspect sentiment differs from all nontarget aspect sentiments ( so that the confounding SubQ .
Generation Strategy Example Prereq .
SOURCE : The original sample from the test set Tasty burgers , and crispy fries .
( Tgt : burgers ) Q1 REVTGT : Reverse the sentiment of the target aspect Terrible burgers , but crispy fries .
Q2 REVNON : Reverse the sentiment of the non-target aspects with originally the same sentiment as target Tasty burgers , but soggy fries .
Q3 ADDDIFF : Add aspects with the opposite sentiment from the target aspect Tasty burgers , crispy fries , but poorest service ever ! factor is disentangled ) , the best model ( Xu et al. , 2019a ) drops from 78.53 % to 59.32 % on Laptop and from 86.70 % to 63.93 % on Restaurant .
This implies that the success of previous models may over-rely on the confounding non-target aspects , but not necessarily on the target aspect only .
However , no datasets can be used to analyze the aspect robustness more in depth .
We develop an automatic generation framework that takes as input the original test instances from SemEval 2014 , and applies three generation strategies showed in Table 1 .
New test instances generated by REVTGT , REVNON , and ADDDIFF can be used to answer the questions ( Q1 ) -( Q3 ) , respectively .
The generated new instances largely overlap with the content and aspect terms of the original instances , but manage to disentangle the confounding sentiment polarity of non-target aspects from the target , as showed in the examples in Table 1 .
In this way , we produce an " all - rounded " test set that can test whether a model robustly captures the target sentiment instead of other irrelevant clues .
We enriched the laptop dataset by 294 % from 638 to 1,877 instances and the restaurant dataset by 315 % from 1,120 to 3,530 instances .
By human evaluation , more than 92 % of the new aspect robustness test set ( ARTS ) shows high fluency , and desired sentiment on all aspects .
Our ARTS test set is in line with other recent works on NLP challenge sets ( McCoy et al. , 2019 ; Gardner et al. , 2020 ) .
Using our new test set , we analyze the aspect robustness of nine existing models .
Experiment results show that their performance degrades by up to 55.64 % on Laptop and 69.73 % on Restaurant .
We also use our generation strategy to conduct adversarial training and find it improves aspect robustness by at least 11.87 % and at most 35.37 % across various models .
The contributions of our paper are as follows : 1 . We develop simple but effective automatic generation methods that generate new test in-stances ( with over 92 % accuracy by human evaluation ) to challenge the aspect robustness .
2 . We construct ARTS , a new test set targeting at aspect robustness for ABSA models , and propose a new metric , Aspect Robustness Score .
3 . We probe the aspect robustness of nine models , and reveal up to 69.73 % performance drop on ARTS compared with the original test set .
4 . We provide several solutions to enhance aspect robustness for ABSA models , including adversarial training detailed in Section 5.4 .
Data Generation
As shown in Table 1 , we aim to build a systematic method to generate all possible aspect-related alternations , in order to remove the confounding factors in the existing ABSA data .
In the following , we will introduce three different ways to disentangle the sentiment of the target aspect from sentiments of non-target aspects .
REVTGT
The first strategy is to generate sentences that reverse the original sentiment of the target aspect .
The word spans of each aspect 's sentiment of Se-mEval 2014 data are provided by ( Fan et al. , 2019a ) .
We design two methods to reverse the sentiment , and one additional step of conjunction adjustment on top of the two methods to polish the resulting sentence .
Strategy Example Flip Opinion
It 's light and easy to transport . ?
It 's heavy and difficult to transport .
Add Negation
The menu changes seasonally . ?
The menu does not change seasonally .
Adjust
The food is good , and the decor is nice .
Conjunctions ?
The food is good , but the decor is nasty .
Flip Opinion Words
Suppose we have the sentence " Tasty burgers and crispy fries , " where the sentiment term for the target aspect is Tasty .
We aim to generate a new sentence that flips the sentiment Tasty .
A baseline approach is antonym replacement by looking up WordNet ( Miller , 1995 ) .
However , due to polysemy , the simple lookup is very likely to derive an inappropriate antonym and cause incompatibility with the context .
Among the retrieved set of antonyms , we only keep words with the same Part- of - Speech ( POS ) tag as original , using the stanza package 2 which takes the context into account by the state - of - the - art neural network - based model .
3 Lastly , in the case of multiple antonyms , we prioritize the words that are already in the existing vocabulary , and then randomly select an antonym from the candidate set .
Add Negation
As the above strategy of flipping by the antonym is constrained by whether appropriate antonyms are available .
For those cases without suitable antonyms , including long phrases , we add negation according to the linguistic features .
In most cases , the sentiment expression is an adjective or verb term , so we simply add negation ( i.e. , " not " ) in front of it .
If the sentiment term is not an adjective or verb , we add negation to its closest verb .
For example , in Table 2 , there are no available antonyms for " change " in the original example " The menu changes seasonally . " , so we simply negate it as " The menu does not change seasonally . "
Adjust Conjunctions
As pinpointed in Section 1 , 79.6?83.9 % of the original test data of SemEval 2014 ( Pontiki et al. , 2014 ) have the same sentiment for all aspects .
A possible result of reverting one aspect 's sentiment is that the other aspects ' sentiments will be opposite to the altered one .
So we need to adjust the conjunctions for language fluency .
If the two closest surrounding sentiments of a conjunction word have the same polarity , then cumulative conjunctions such as " and " should be applied ; otherwise , we should adopt adversative conjunctions such as " but . "
In the example in Table 2 , after flipping the sentiment , we derive the example " The food is good , and the decor is nasty " which is very unnatural , so we replace the conjunction " and " with " but , " and thus generate the example " The food is good , but the decor is nasty . "
REVNON
Changing the target sentiment by REVTGT can test if a model is sensitive enough towards the targetaspect sentiment , but we need to further complement this probe by perturbing the sentiments of the non-target aspects ( REVNON ) .
As showed in Table 3 , for all the non-target aspects with the same sentiment as the target aspect's , we reverse their sentiments using the same method as REVTGT .
And for all the remaining non-target aspects , whose sentiments are already opposite from the target sentiment , we exaggerate the extent by randomly adding an adverb ( e.g. , " very " , " really " and " extremely " ) from a dictionary of adverbs of degree that is collected based on the training set .
The resulting test example will be a solid proof of the ABSA quality , because only the target aspect has the desired sentiment , and all non-target aspects have been flipped to or exaggerated with the opposite sentiment .
ADDDIFF
The first two strategies , REVTGT and REVNON , have explored how the sentiment changes of existing aspects will challenge an ABSA model , and ADDDIFF further investigate if adding more nontarget aspects can confuse the model .
Moreover , the existing SemEval 2014 test sets have only on average 2 aspects per sentence , but the real-world applications can have more aspects .
With these motivations , we develop ADDDIFF as follows .
We first form a set of aspect expressions AspectSet 4 by extracting all aspect expressions from the entire dataset .
Specifically , for each example in the dataset , we first identify each sentiment term ( e.g. , " reasonable " in " Food at a reasonable price " ) and then extract its linguistic branch as the aspect expression ( e.g. , " at a reasonable price " ) by pretrained constituency parsing ( Joshi et al. , 2018 ) .
Table 4 shows several examples of AspectSet in the restaurant domain .
Using the AspectSet , we randomly sample 1 - 3 aspects that are not mentioned in the original test sample and whose sentiments are different from the target aspect's , and then append these to the end of the original example .
For example , " Great food and best of all GREAT beer ! "
ADDDIFF ? ? ?
" Great food and best of all GREAT beer , but management is less than accommodating , music is too heavy , and service is severely slow . "
In this way , ADDDIFF enables the advanced testing of whether the model will be confused when there are more irrelevant aspects with opposite sentiments .
ARTS Dataset
Overview
Our source data is the most 5 widely used ABSA dataset , SemEval 2014 Laptop and Restaurant Reviews ( Pontiki et al. , 2014 ) . 6
We follow ( Wang et al. , 2016 ; Ma et al. , 2017 ; Xu et al. , 2019a ) to remove instances with conflicting polarity and only keep positive , negative , and neutral labels .
We use the train- dev split as in ( Xu et al. , 2019a )
Quality Inspection
We conduct human evaluation to validate the generation quality of our ARTS dataset on two criteria :
1 . Fluency : Does the generated sentence maintain the fluency of the source sentence ?
2 . Sentiment Correctness :
Does the sentiment of each aspect have the desired polarity ?
? REVTGT : Is the target sentiment reversed ?
? REVNON : For non-target aspects with originally the same sentiment as the target , is it reversed ?
For the rest , are they exaggerated ?
? ADDDIFF : Is the target sentiment unchanged ?
Each task is completed by two native -speaker judges .
We first calculate the inter-agreement rate of the human annotators , and then resolve the divergent opinions on samples that they disagree with .
We accept the samples that both judges considered as correct or are resolved to be correct after our check .
Finally , we ask the annotators to fix the rejected samples by minimal edit which does not change the aspect term or the sentence meaning , but satisfies both criteria .
Fluency Check
The evaluation results on fluency are showed in Table 5 . Most samples ( 92.27 % of Laptop and 92.35 % of Restaurant test sets ) are accepted as fluent text .
The inter-agreement rate between the two human judges is also high , 91.10 % and 92.69 % on the two datasets .
Sentiment Check
We also evaluate the sentiment correctness of the generated text .
Note that for REVNON , we count the samples with all " yes " answers as accepted samples .
Overall , the acceptance rate of the generated samples is 93.93 % on Laptop and 95.24 % on Restaurant , along with interagreement rates of over 94.14 % on both datasets .
Dataset Analysis
After checking the quality of our enriched ARTS test set , we analyze the dataset characteristics and make comparisons with the original test sets .
For general statistics , we can see from Table 6 that the sentence length in the new test set is on average 4 words more than the original , and the vocabulary size is also larger by around two hundred .
For the label distribution , we can see that the new test set has an increasing number of all labels , and especially balances the ratio of positive - to- negative labels from the original 2.66 to 1.5 on Laptop , and from 3.71 to 1.77 on Restaurant .
For the aspect-related challenge in the test set , the new test set , first of all , has a larger number of aspects per sentence than the original .
Our test set also features the higher disentanglement of the target aspect from the non-target aspects that have the same sentiment as the target : the portion of samples with at least one non-target aspects of sentiments different from the target is 59 ? 67 % , and on average 45 % higher than the original test sets .
And the portion of the most challenging samples where all non-target aspects have sentiments different from the target one on the new test set is on average 30 % more than that of the original test set .
The average number of non-target aspects with opposite sentiments per sample in the new test set is on average 5 times that of the original set .
Aspect Robustness Score ( ARS )
As mentioned in Section 1 , a model is considered to have high aspect robustness if it satisfies both the prerequisite and all three questions ( Q1 ) -( Q3 ) .
So we propose a novel metric , Aspect Robustness Score ( ARS ) , that counts the correct classification of the source example and all its variations ( REVTGT , REVNON , and ADDDIFF ) as one unit of correctness .
Then we apply the standard calculation of accuracy .
Note that the three variations correspond to questions ( Q1 ) -( Q3 ) , respectively .
Evaluating ABSA Models
We use our enriched test set as a comprehensive test on the aspect robustness of ABSA models .
Models
For a comprehensive overview of the ABSA field , we conduct extensive experiments on models with a variety of neural network architectures .
TD - LSTM : ( Tang et al. , 2016a ) uses two Long Short - Term Memory Networks ( LSTM ) to encode the preceding and following contexts of the target aspect ( inclusive ) and concatenate the last hidden states of the two LSTMs to make the sentiment classification .
AttLSTM : Wang et al. ( 2016 ) apply an Attention - based LSTM on the concatenatation of the aspect and word embeddings of each token .
GatedCNN : Xue and Li ( 2018 ) use a Gated Convolutional Neural Networks ( CNN ) that applies a Tanh - ReLU gating mechanism to the CNNencoded text with aspect embeddings .
MemNet : Tang et al. ( 2016 b ) use memory networks to store the sentence as external memory and calculate the attention with the target aspect .
GCN : Aspect-specific Graph Convolutional Networks ( GCN ) ( Zhang et al. , 2019a ) first applies GCN over the syntax tree of the sentence and then imposes an aspect-specific masking layer on its top .
BERT : Xu et al. ( 2019a ) uses a BERT - based baseline ( Devlin et al. , 2019 ) and takes as input the concatenation of the aspect term and the sentence .
BERT - PT : Xu et al. ( 2019a ) post-train BERT on other review datasets such as Amazon laptop reviews ( He and McAuley , 2016 ) and Yelp Dataset Challenge reviews , and finetune on ABSA tasks .
CapsBERT : ( Jiang et al. , 2019 ) encode the sentence and the aspect term with BERT , and then feed it into Capsule Networks to predict the polarity .
BERT - Sent : For more in - depth analysis , we also implement a sentence classification baseline .
BERT - Sent takes as input sentences without aspect information , and predicts the " global " sentiment .
We use it because if other ABSA models fails to pay attention to aspects , they will degenerate to a sentence classifier .
If so , they will resemble BERT - Sent , which performs well on original tests and badly on ARTS .
So BERT - Sent is a reference to check degenerated aspect-level models .
Implementation Details
For all existing models , we use the authors ' official implementation .
For our self - proposed BERT - Sent , we use Adam optimizer with a learning rate of 5e - 5 , weight decay of 0.01 , and batch size of 32 .
We apply the l 2 regularization with ? = 10 ?4 , and train 50 epochs .
Note that the tokenization of the ASTS dataset is the same as the original SemEval 2014 , as we prepared the new test set by inverting the NLTK tokenization rules we used when applying the generation strategies .
7
Results on ARTS
We list the accuracy 8 of the nine models on the Laptop and Restaurant test sets in Table 7 . For Entire Test - New in Table 7 , accuracy is calculated using ARS .
Supplementary to ARS , Table 7 also decomposes ARS into single-strategy scores ( the right three columns ) by splitting the entire ARTS test set into three subsets according to the corresponding data generation techniques .
Each of the single-strategy scores explains from one perspective the reason for large performance drop in ARS , which will be elaborated later .
Overall Performance
On the entire test set , we can see that the accuracy of all models on the original test set is very high , achieving up to 78.53 % on Laptop and 86.70 % on Restaurant , but it drops drastically ( ? 69 % ? 25 % ) on our new test sets .
Performance of Different Models
From the overall performance on our new test set , we can see that BERT models on average are more robust to the aspect-targeted challenges that our new test set poses .
The most effective model BERT - PT scores the best on both original accuracy and robustness .
It has 53.29 % ARS on Laptop and 59.29 % on Restaurant .
However , the accuracy of non-BERT models on average drops drastically to under 30 % by over ?50 % .
Performance on Different Subsets
We list in detail the performance of each model on the three subsets of our new test set : REVTGT , REVNON , and ADDDIFF .
They correspond to the three questions ( Q1 ) -( Q3 ) .
REVTGT on average induces the most performance drop , as it requires the model to pay precise attention to the target sentiment words .
REVNON makes the performance of the sentence classifier BERT - Sent drops the most by up to ?45.93 % , and the model CapsBERT also drops by up to ?39.26 % .
The last subset ADDDIFF causes most non-BERT models to drop significantly , indicating that these models are not robust enough against an increased number of non-target aspects , which should have been irrelevant .
Laptop vs. Restaurant
The performance drop on Restaurant is higher than that on Laptop .
There are two possible reasons : ( 1 ) the original performance on restaurant is higher , and ( 2 ) the new test set is more challenging in the Restaurant domain .
We verify this by calculating the relative drop ( new ? old old ) in addition to the reported absolute values of the change .
The relative drop on Laptop is 64.76 % , which is higher than 60.36 % on Restaurant .
For the laptop dataset , both the lower original performance and the larger relative decrease of performance might be due to the nature of the dataset .
For example , Laptop restaurant has far fewer training data than Restaurant , which makes the models less accurate originally and weaker on ARTS .
Analysis
Variations of Generation Strategies Combining Multiple Strategies
Each sample in the ARTS test set is generated by one of the three Table 7 : Model accuracy on Laptop and Restaurant data .
We compare the accuracy on the Original and our New test sets ( Ori ? New ) , and calculate the change of accuracy .
Besides the Entire Test Set , we also list accuracy on subsets where the generation strategies REVTGT , REVNON and ADDDIFF can be applied .
The accuracy of Entire Test - New is calculated using ARS .
indicates whether the performance drop is statistically significant ( with p-value ? 0.05 by Welch 's t-test ) .
strategies .
However , it is also worth exploring whether combining several strategies can make a more challenging probe on the aspect robustness of ABSA models .
As a case study , we analyze the model robustness against test samples generated by the combination of REVNON + ADDDIFF .
By comparing the performance decrease caused ADDDIFF with More Aspects
Some strategies such as ADDDIFF can be parameterized by k , where k is the number of additional non-target aspects to be added .
We select three models ( the best , the worst , and an average- performing one ) , and plot their accuracy on test samples generated by ADDDIFF ( k ) on Laptop in Figure 1 .
As k gets larger , the test samples become more difficult .
The sentence classification baseline BERT - Sent drops drastically , BERT - PT remains high , and GCN lies in the middle .
How to Effectively Model the Aspect ?
An important usage of our ARTS is to understand what model components are key to aspect robustness .
We list the aspect-specific mechanisms of all models according to the ascending order of their ARS on Laptop dataset in Table 9 .
We can see that for BERT - based models , BERT - PT , which is further trained on large review corpora , gets the best accuracy and aspect robustness .
More complicated structures like CapsBERT underperforms the basic BERT by 25.08 % .
Table 9 : Models in the ascending order of their ARS on Laptop .
We list their aspect-specific mechanisms , including concatenating the aspect and word embeddings ( Asp +W Emb ) , position - aware mechanism for aspects ( Posi- Aware ) , and attention using the aspect ( Asp Att ) .
We highlight for Posi- Aware as it is the most related to aspect robustness for non -BERT models .
Among the non-BERT models , the aspect position - aware models TD - LSTM and GCN are the most robust , as they have a stronger sense of the location of the target aspect in a sentence .
On the contrary , the other models with poorer robustness ( 9.87% ?16.93 % in Table 9 ) only use mechanisms such as aspect-based attention , or concatenating the aspect embedding to the word embedding .
To summarize , the main takeaways are ?
For BERT models , additional pretraining is the most effective .
?
For non-BERT models , explicit positionaware designs lead to more aspect robustness .
Does a More Diverse Training Set Help ?
A recent dataset , Multi-Aspect Multi-Sentiment ( MAMS ) ( Jiang et al. , 2019 ) , is collected from the same data source as the SemEval 2014 Restaurant dataset ( Ganu et al. , 2009 ) .
However , its sentences are more complicated , each having at least two aspects with different sentiment polarities .
We use this dataset to inspect whether a stronger training set can help improve aspect robustness .
Training and Testing on MAMS Table 10a checks the aspect robustness of models trained on MAMS using the original MAMS test set ( O?O ) and the new test set that we produced by applying the same generation strategies to its test set ( O?N ) .
Models trained and tested on MAMS have a smaller decrease rate than those on the Restaurant dataset .
This shows that a more challenging training set can make models more robust .
Training on MAMS and Testing on Restaurant As MAMS and Restaurant are collected from the same source data , we test whether MAMS - trained models perform well on the new test set of Restaurant ( in the column " MAMS ?N " of Table 10 b ) .
We can see that all models trained on MAMS are more robust than those trained on the Restaurant dataset .
For example , the accuracy of BERT and BERT - PT on the new test set is lifted up to 62.77 % .
Does Adversarial Training Help ?
Although the MAMS described in Section 5.3 provides a training set with diversity , it remains difficult to improve aspect robustness for other domains , or future new datasets .
Therefore , we propose a flexible method , adversarial training , for aspect robustness , which is applicable to any given dataset .
We conducted adversarial training on the Laptop and Restaurant datasets , and analyze its effect in Table 10 b .
Specifically , for the column " Adv?N " , we generated an additional training set by applying the three proposed strategies on training data , then trained models on the augmented data obtained by combining the original training data and this newly generated data , and finally evaluated on the ARTS test data .
This practice follows Table 7 of ( Zhang et al. , 2019 b ) which is a similar stream of work as ours for the paraphrasing domain .
In both Restaurant and Laptop domains , adversarial training ( Adv?N ) leads to significant performance improvement than only training on the original datasets ( O?N ) .
On the Restaurant datasets , adversarial training is even more effective than training on MAMS , because our generated data instances comprehensively covered all possible perturbations of the non-target aspects , and naturally collected datasets might not be comparable .
Error Analysis for Data Generation
We analyze the error types in the subset of ARTS that was fixed by human judges .
as " the weight of the laptop is light ? dark " , and negation which causes grammatical errors ( ? 1.1 % ) .
In future work , we can fix the latter by applying a grammatical error correction system on top of our generation .
Also , REVTGT and REVNON cannot be applied to 1.4?6.6 % instances with complicated sentiment expressions which rely on commonsense .
For example , " a 2 - hour wait " is negative bust too difficult to alter in our current generation framework .
It needs more advanced models such as text style transfer ( Shen et al. , 2017 ; Jin et al. , 2019 b ) .
Related Work Robustness in NLP Robustness in NLP has attracted extensive attention in recent works ( Hsieh et al. , 2019 ; .
As a popular method to probe the robustness of models , adversarial text generation becomes an emerging research field in NLP .
Techniques include adding extraneous text to the input ( Jia and Liang , 2016 ) , character - level noise ( Belinkov and Bisk , 2018 ; Ebrahimi et al. , 2018 ) , and word replacement ( Alzantot et al. , 2018 ; Jin et al. , 2019a ) .
Using the adversarial generation techniques , new adversarial test sets are proposed for several tasks such as paraphrasing ( Zhang et al. , 2019 b ) and entailment ( Glockner et al. , 2018 ; Mc - Coy et al. , 2019 ) . Aspect - Based Sentiment Analysis ABSA has emerged as an active research area recently .
Early works hand -craft sentiment lexicons and syntactic features for rule- based classifiers ( Vo and Zhang , 2015 ; Kiritchenko et al. , 2014 ) .
Recent neural network - based models use architectures such as LSTM ( Tang et al. , 2016a ) , CNN ( Xue and Li , 2018 ) , Attention mechanisms ( Wang et al. , 2016 ) , Capsule Network ( Jiang et al. , 2019 ) , and the pretrained model BERT ( Xu et al. , 2019a ) .
Similar to the motivation in our paper , some work shows preliminary speculation that the current ABSA datasets might be downgraded to sentence - level sentiment classification ( Xu et al. , 2019 b ) .
Conclusion
In this paper , we proposed a simple but effective mechanism to generate test instances to probe the aspect robustness of the models .
We enhanced the original SemEval 2014 test sets by 294 % and 315 % in laptop and restaurant domains .
Using our new test set , we probed the aspect robustness of nine ABSA models , and discussed model designs and better training that can improve the robustness .
Figure 1 : 1 Figure 1 : Accuracy of BERT - PT , GCN , and BERT - Sent on the test samples in the laptop domain generated by ADDDIFF ( k ) where k varies from 1 to 5 .
