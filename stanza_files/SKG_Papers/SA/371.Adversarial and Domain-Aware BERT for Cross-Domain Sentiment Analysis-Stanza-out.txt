title
Adversarial and Domain- Aware BERT for Cross-Domain Sentiment Analysis
abstract
Cross-domain sentiment classification aims to address the lack of massive amounts of labeled data .
It demands to predict sentiment polarity on a target domain utilizing a classifier learned from a source domain .
In this paper , we investigate how to efficiently apply the pre-training language model BERT on the unsupervised domain adaptation .
Due to the pre-training task and corpus , BERT is taskagnostic , which lacks domain awareness and can not distinguish the characteristic of source and target domain when transferring knowledge .
To tackle these problems , we design a post-training procedure , which contains the target domain masked language model task and a novel domain-distinguish pre-training task .
The post-training procedure will encourage BERT to be domain-aware and distill the domain-specific features in a self-supervised way .
Based on this , we could then conduct the adversarial training to derive the enhanced domain-invariant features .
Extensive experiments on Amazon dataset show that our model outperforms state - of - the - art methods by a large margin .
The ablation study demonstrates that the remarkable improvement is not only from BERT but also from our method .
Introduction Sentiment analysis aims to automatically identify the sentiment polarity of the textual data .
It is an essential task in natural language processing with widespread applications , such as movie reviews and product recommendations .
Recently , deep networks have significantly improved the state - of - theart in sentiment analysis .
However , training deep networks is highly depended on a large amount of labeled training data which is time - consuming and requires expensive manual labeling .
Thus , there is a strong need to leverage or reuse rich labeled * Corresponding author .
data from a different but related source domain .
Cross-domain sentiment analysis , which transfers the knowledge learned from source domain to a new target domain , becomes a promising direction .
The main challenge of cross-domain sentiment analysis is domain discrepancy due to different expression of the user 's emotion across domains .
To address the problem , a wide- used approach is designed to extract domain invariant features , which means that the distributions of features from the source domain and target domain are similar ( Zellinger et al. , 2017 ; Persello and Bruzzone , 2016 ; Ganin et al. , 2016 ; Yu and Jiang , 2016 a ) .
One effective way to obtain domain-invariant features is adversarial training ( Ganin et al. , 2016 ; Li et al. , 2017 ; Zheng et al. , 2019 ) .
Specifically ,
A domain discriminator is learned by minimizing the classification error of distinguishing the source from the target domains , while a deep classification model learns transferable representations that are indistinguishable by the domain discriminator .
Very recently , pre-trained language models have shown to be effective for improving many language tasks ( Peters et al. , 2018 ) . Bidirectional Encoder Representations from Transformers ( BERT ) realized a breakthrough , which pre-trains its encoder using language modeling and by discriminating surrounding sentences in a document from random ones ( Devlin et al. , 2019 ) .
Pre-training in this manner can construct bidirectional contextual representation , and the large-scale pre-training enables BERT powerful ability in language understanding .
We only need to add one output layer and fine- tune BERT to get the state - of - the - art results in sentiment analysis .
Theoretically , BERT can enhance the performance in cross-domain sentiment analysis .
However , some important problems remain when directly fine-tuning BERT in the task of crossdomain sentiment analysis :
Firstly , there is no labeled data in the target do-main which brings many difficulties to the fine-tune procedure .
If we fine- tune BERT only by the source labeled data , the shift between training and test distributions will degrade the BERT 's performance .
Secondly , BERT is task - agnostic and has almost no understanding of opinion text .
BERT is pretrained by the universal language Wikipedia , leaving the domain challenges unresolved ( Xu et al. , 2019 ) .
For example , in the pre-training procedure , BERT may learn to guess the [ MASK ] in " The [ MASK ] is bright " as " sun " .
But in a laptop sentiment analysis , it is more likely to be " screen " .
Especially , in the cross-domain sentiment analysis scenario , the labeled data is limited , which is insufficient to fine - tune BERT to ensure full domainawareness .
Thirdly , cross-domain sentiment analysis also arises a new challenge for BERT to distinguish the characteristic of source and target domain to keep the transferable features and abandon domain-specific information .
To address above problems , we design a novel pre-training task to make BERT domain- aware and then improve the BERT 's fine -tuning procedure by adversarial training .
Specifically , a novel post-training procedure is implemented that adapts BERT with unlabeled data from different domains to enhance the domain-awareness .
Apart from the target domain masked language model task , we introduce the domain-distinguish pre-training task .
BERT will be pre-trained to distinguish whether the two sentences come from the same domain .
The domain-distinguish pre-training task will encourage BERT to distill syntactic and semantic domain-specific features , so as to be domain-aware .
The proposed post-training procedure gives us a new way to fully utilize language knowledge from the target domain and link the source and target in a self-supervised way .
Based on this , we could then conduct the adversarial training to derive the enhanced domain-invariant features .
Experiments on Amazon reviews benchmark dataset show that our model gets the average result 90.12 % in accuracy , 4.22 % absolute improvement compared with state - of - the - art methods .
The ablation study shows that the remarkable improvement is not only from BERT but also from our method .
The contributions of this paper can be summarized as follows : ?
We apply BERT to cross-domain sentiment analysis task and leverage the post-training method to inject the target domain knowledge to BERT .
?
A novel domain-distinguish pre-training task is proposed for the BERT post-training .
This design encourages BERT to be domain-aware and distill the domain-specific features in a self-supervised way .
2 Related Work
Cross-Domain Sentiment Analysis Cross-domain sentiment analysis aims to generalize a classifier that is trained on a source domain , for which typically plenty of labeled data is available , to a target domain , for which labeled data is scarce .
There are many pivot-based methods ( Blitzer et al. , 2007a ; Yu and Jiang , 2016 b ; Ziser and Reichart , 2018 ; Peng et al. , 2018 ) , which focus on inducing a low-dimensional feature representation shared across domains based on the cooccurrence between pivots and non-pivots .
However , selecting pivot words is very tedious , and the pivot words are manually selected , which may not be accurate .
Recently , some adversarial learning methods ( Ganin et al. , 2016 ; Li et al. , 2017 ; Zheng et al. , 2019 ) propose to train the feature generator to minimize the classification loss and simultaneously deceive the discriminator , which is end-to - end without manually selecting pivots .
Language Model Pre-training Pre-trained language representations with selfsupervised objectives have become standard in a wide range of NLP tasks .
Previous work can be divided into two main categories : feature - based approaches and fine-tuning approaches .
The recent proposed feature - based approaches mainly focus on learning contextualized word representations such as CoVe ( McCann et al. , 2017 ) and ELMo ( Peters et al. , 2018 ) .
As with traditional word embeddings , these learned representations are also typically used as features in a downstream model .
On the other hand , fine-tuning approaches mainly pre-train a language model on a large corpus with an unsupervised objective and then fine - tune the model with in - domain labeled data for downstream applications .
The advantage of these approaches is that few parameters need to be learned from scratch .
Specifically , Howard and Ruder ( 2018 ) propose ULMFiT , which uses a different learning rate for each layer with learning warmup and gradual unfreezing .
GPT ( Radford et al. , 2018 ) uses a transformer encoder ( Vaswani et al. , 2017 ) instead of an LSTM and jointly finetunes with the language modeling objective .
Moreover , BERT ( Devlin et al. , 2019 ) is a large-scale language model consisting of multiple layers of transformer , which further incorporates bidirectional representations .
BERT is the state- of - art pre-training language model .
However , in the crossdomain sentiment analysis scenario , BERT is taskagnostic and can not distinguish the characteristic of source and target domain .
Model
In this section , we introduce the proposed model for cross-domain sentiment analysis in detail .
We begin by giving the problem definition and notations .
Then BERT and post-training method are formally presented in the second subsection .
Finally , the adversarial training process is introduced .
We also give a theoretical analysis of our model .
Problem Definition and Notations
In the task of cross-domain sentiment analysis , we are given two domains D s and D t which denote a source domain and a target domain , respectively .
In the source domain , D l s = {x i s , y i s } N l s i=1 are N l s labeled source domain examples , where x i s means a sentence and y i s is the corresponding polarity label .
There are also N u s unlabeled data D u s = {x i s } N l s + N u s i=1 + N l s in the source domain .
In the target domain , there is a set of unlabeled data D t = {x i t } Nt i=1 , where N t is the number of unlabeled data .
Cross-domain sentiment analysis demands us to learn a robust classifier trained on labeled source domain data to predict the polarity of unlabeled sentences from the target domain .
Background of BERT BERT ( Devlin et al. , 2019 ) builds on the Transformer networks ( Vaswani et al. , 2017 ) , which relies purely on attention mechanism and allows modeling of dependencies without regard to their distance in the input sequences .
BERT is pre-trained by predicting randomly masked words in the input ( MLM task ) and classifying whether the sentences are continuous or not ( NSP task ) .
The MLM task allows the word representation to fuse the left and the right context , and the NSP task enables BERT to infer the sentences ' relationship .
The pre-trained BERT can be easily fine-tuned by one softmax output layer for classification task .
BERT Post-training Despite the success , BERT suffers from the domain challenge .
BERT is pre-trained by Wikipedia , leading to task - agnostic and little understanding of opinion text .
Especially in the cross-domain sentiment analysis scenario , the lack of abundant labeled data limits the fine- tune procedure , which degrades BERT due to the domain shift .
This task also demands BERT to distinguish the characteristic of source and target domain for better knowledge transfer .
Therefore , we propose BERT posttraining which takes BERT 's pre-trained weights as the initialization for basic language understanding and adapts BERT by novel self-supervised pretrained tasks : domain-distinguish task and target domain masked language model .
Domain-distinguish Task
The next sentence prediction ( NSP ) task encourages BERT to model the relationship between sentences beyond word-level , which benefits the task of Question Answering and Natural Language Inference .
However , cross-domain sentiment analysis operates on a single text sentence and does not require the inference ability .
Instead , the ability to distinguish domains plays an important role .
Therefore , during the post-training procedure , we replace the NSP task by domaindistinguish task ( DDT ) .
Specifically , we construct the sentence - pair input : [ CLS ] sentence A [ SEP ] sentence B [ SEP ] , where [ CLS ] and [ SEP ] are special embeddings for classification and separating sentences .
50 % of time sentence A and sentence B are all randomly sampled from target domain reviews , we label it TargetDomain .
And 50 % of time sentence A and sentence B come from target domain and another domain , whose label is MixDomain .
We do not fix the collocation , in another word , we only ensure that the two sentences come from different domains but the order is random .
For example : Input = [ CLS ]
The mouse is smooth and great [ SEP ]
The screen is plain [ SEP ] Label = TargetDomain Input = [ CLS ]
This book is boring [ SEP ]
The system of the laptop is stable [ SEP ]
Label = MixDomain
The domain-distinguish pre-training is a classifi-cation task .
We add one output layer on the pooled representation and maximize the likelihood of the right label .
The domain-distinguish pre-training enables BERT to distill the specific features for different domains , which enhances the downstream adversarial training and benefits the cross-domain sentiment analysis .
Target Domain MLM
To inject the target domain knowledge , we leverage the masked language model ( MLM ) ( Devlin et al. , 2019 ) .
It requires to predict the randomly masked words in the sentence , which encourages BERT to construct a deep bidirectional representation .
In the cross-domain sentiment analysis , there are no labeled data but plenty of unlabeled data in the target domain to post-train BERT by MLM .
Specifically , we replace 15 % of tokens by [ MASK ] at random .
The final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary .
We maximize the likelihood of the masked token id .
Post-training by unlabeled review data in target domain will effectively alleviate the shift of domain knowledge .
For example , if the masked word is an opinion word in " This movie is [ MASK ] " , this objective challenges BERT to learn the representation for fine - grained opinion words in movie review domain , such as " touchable " or " disturbing " .
One problem is that the DDT task mixes sentences from other domains in the sentence pair .
The sentence from other domains will be the noise which brings domain bias .
Therefore , we only mask the tokens in target domain sentences if the domain-distinguish task label is MixDomain .
The total loss of the post-training procedure is the sum of losses of target domain MLM and domain-distinguish task .
The adaptation takes about 5 hours to complete on one single NVIDIA P100 GPU .
Adversarial Training
The post-training procedure injects target domain knowledge and brings domain-awareness to BERT .
Based on the post-trained BERT , we now could utilize the adversarial training to abandon the distilled domain-specific features to derive the domaininvariant features .
Specifically , a sentiment classifier and a domain discriminator is designed operating on the hidden state h [ CLS ] of the special classification embedding [ CLS ] .
Sentiment Classifier
The sentiment classifier is simply a fully - connected layer and outputs the probabilities through a softmax layer : y s = softmax ( W s h [ CLS ] + b s ) .
( 1 )
The classifier is trained by the labeled data in the source domain and the loss function is crossentropy : L sen = ?
1 N l s N l s i=1 K j=1 ?i s ( j ) logy i s ( j ) , ( 2 ) where ?i s ? { 0 , 1 } is the ground truth label in the source domain , and K denotes the number of different polarities .
Domain Discriminator
The domain discriminator aims to predict domain labels of samples , i.e. , coming from the source or target domain .
The parameters of BERT are optimized to maximize the loss of the domain discriminator .
This target will encourage BERT to fool the domain discriminator to generate domain-invariant features .
Specifically , before feeding h [ CLS ] to the domain discriminator , the hidden state of classification embedding [ CLS ] h [ CLS ] goes through the gradient reversal layer ( GRL ) ( Ganin et al. , 2016 ) .
During the forward propagation , the GRL acts as an identity function but during the backpropagation , the GRL reverses the gradient by multiplying it by a negative scalar ?.
GRL can be formulated as a ' pseudo-function ' Q ? ( x ) by two equations below in order to describe its forward - and backwardbehaviors : Q ? ( x ) = x , ( 3 ) ?Q ? ( x ) ?x = ?I . ( 4 ) We denote the hidden state h [ CLS ] through the GRL as Q ? ( h [ CLS ] ) = ?[ CLS ] and then feed it to the domain discriminator as : d = softmax ( W d ?[ CLS ] + b d ) . ( ) 5
The target is to minimize the cross-entropy for all data from the source and target domains : L dom = ?
1 N s + N t Ns + Nt i K j di ( j ) logd i ( j ) , ( 6 ) where di ? { 0 , 1 } is the ground truth domain label .
Due to the GRL , the parameters for domain discriminator ? dd are optimized to increase the ability to predict domain labels , however , the parameters for BERT ?
BERT are optimized to fool the domain discriminator , leading to domain-invariant features .
Joint Learning
The sentiment classifier and the domain discriminator are jointly trained , and the total loss is : L total = L sen + L dom . ( 7 )
The post-training procedure and our proposed domain-distinguish pre-training task will enhance the adversarial training to obtain lower classification error in the target domain , we will analyze it in Sec 3.5 .
Theoretical Analysis
In this section , we provide a theoretical analysis of our approach .
First , we provide an insight into existing theory , then we introduce an expansion of the theory related to our method and explain how the post-training and adversarial training cooperate to obtain a remarkably better result than state - ofthe - art methods .
For each domain , there is a labeling function on inputs X , defined as f : X ? [ 0 , 1 ] .
The ideal label functions for source and target domain are denoted as : f s and f t , respectively .
We define a hypothesis label function h : X ? [ 0 , 1 ] and a disagreement function : ( h 1 , h 2 ) = E [ |h 1 ( x ) ? h 2 ( x ) | ] .
( 8 ) Then the expected error on the source samples of h is defined as : s ( h ) = s ( h , f s ) .
For the target domain , we have : t ( h ) = t ( h , f t ) .
The divergence between source and target domain could thus be measured by H?H-distance , which is defined as follows : d H?H
( D s , D t ) = 2 sup h , h ?H | s ( h , h ) ? t ( h , h ) | ( 9 ) This distance is firstly proposed in ( Ben - David et al. , 2010 ) and frequently used to measure the adaptability between different domains ( Shen et al. , 2018 ; .
Theorem 1 . Let H be the hypothesis class .
Given two different domains D s , D t , we have : ?h ? H , t ( h ) ? s ( h ) + 1 2 d H?H ( D s , D t ) + C ( 10 )
This theorem means that the expected error on the target domain is upper bounded by three terms : ( 1 ) the expected error on the source domain ; ( 2 ) the divergence between the distributions D s and D t ; ( 3 ) the error of the ideal joint hypothesis .
Normally , C is disregarded because it is considered to be negligibly small .
Therefore , the first and second terms are important quantitatively in computing the target error .
For the first term , the error rate of source domain s , it is easy to minimize with source labeled training data .
Moreover , we adopt BERT , which brings powerful contextual representation for lower error rate .
The second item in Eq. 10 demands us to generate similar features among different domains .
Our proposed domain-distinguish pre-training task and post-training for BERT enable the model to identify the specific features for different domains .
This ability will enhance the domain discriminator , which will help to find more complicated domain specific features and get abandoned by adversarial training .
Therefore , we further decrease the divergence between the domains , which is quantitatively measured by A-distance in Sec 4.6 .
Experiments
In this section , we empirically evaluate the performance of our proposed methods .
Datasets and Experimental Setting
We conduct the experiments on the widely - used Amazon reviews benchmark datasets collected by ( Blitzer et al. , 2007 b ) .
It contains reviews from four different domains : Books ( B ) , DVDs ( D ) , Electronics ( E ) and Kitchen appliances ( K ) .
For each domain , there are 2,000 labeled reviews and approximately 4000 unlabeled reviews .
Following the convention of previous works ( Ziser and Reichart , 2018 ; Ganin et al. , 2016 ; Qu et al. , 2019 ) , we construct 12 cross-domain sentiment analysis tasks .
For each task , we employ a 5 - fold cross-validation protocol , that is , in each fold , 1600 balanced samples are randomly selected from the labeled data for training and the rest 400 for validation .
Implementation Details
We adopt BERT base ( uncased ) as the basis for all experiments .
When generating the post-training data , each sentence in the target domain gets duplicated 10 times with different masks and sentences pair .
We limit the maximum sequence length is 256 .
During the post-training , we train with batch size of 16 for 10000 steps .
The optimizer is Adam with learning rate 2e - 5 , ? 1 = 0.9 , ? 2 = 0.999 , L2 weight decay of 0.01 .
During the adversarial training ,
The weights in sentiment classifier and domain discriminator are initialized from a truncated normal distribution with mean 0.0 and stddev 0.02 .
In the gradient reversal layer ( GRL ) , we define the training progress as p = t T , where t and T are current training step and the maximum training step , respectively , and the adaptation rate ? is increased as ? = 2 1 + exp ( ? 10p ) ? 1 .
Compared Methods
We compare our method with 5 state- of- the - art methods : DANN ( Ganin et al. , 2016 ) , PBLM ( Ziser and Reichart , 2018 ) , HATN ( Li et al. , 2018 ) , ACAN ( Qu et al. , 2019 ) , IATN .
We also design several variants of BERT as baselines : ? BERT : Fine-tuning vanilla BERT by the source domain labeled data .
? HATN - BERT : HATN ( Li et al. , 2018 ) model based on BERT .
? BERT -AT : This method conducts the adversarial training operating on vanilla BERT .
? BERT -DA : Fine-tuning domain- aware BERT by the source domain labeled data .
The domain- aware BERT is obtained by posttraining .
? BERT - DAAT : Our proposed method introduced in Sec 3 .
Experimental Results
Table 2 shows the classification accuracy of different methods .
We can observe that the proposed BERT - DAAT outperforms all other methods .
For the previous models , they mostly base on the word2vec ( Mikolov et al. , 2013 ) or glove ( Pennington et al. , 2014 ) .
Compared to BERT 's contextual word representation , they can not model complex characteristics of word use and how these uses vary across linguistic contexts , resulting in relatively worse overall performance .
We can see that the vanilla BERT , which is fine- tuned only by the source domain labeled data without utilizing target domain data , can still outperform all the previous methods .
For fair comparison , we reproduce the experiment of HATN model ( Li et al. , 2018 ) that incorporates BERT as the base model .
As shown in Table 2 , HATN - BERT achieves a comparable result with BERT - AT .
For the BERT variants , we did not see a remarkable improvement in the results of BERT - AT , which conducts adversarial training on BERT .
It demonstrates that , in the task of cross-domain sentiment analysis , the bottleneck of BERT is the lack of domain-awareness and can not be tackled purely
Visualization of Features
To intuitively assess the effects of the post-training and adversarial training on BERT , we further perform a visualization of the feature representations of the variants of BERT for the training data in the source domain and the testing data in the target domain for the B ? E task .
As shown in Figure 1 , the graphs are obtained by applying t-SNE on the set of all representation of source and target data points .
Every sample is mapped into a 768 - dimensional feature space through BERT and projected back into a two -dimensional plane by the t-SNE .
In the vanilla BERT representation ( first subgraph in Figure 1 ) , we could observe that data points of different polarities in source domain are well separated .
While for the target domain , some data points are mixed together .
It shows that only utilizing source domain labeled data is not enough for the target domain classification .
For the post-trained BERT ( subgraph for BERT - DA ) , data points belong to four clusters , indicating that domains and sentiment polarities are both well classified .
It verifies that our post-training strategy brings domain-awareness to BERT .
Moreover , compared to the first subgraph , the boundary for sentiment polarity classification is more clear , showing that injecting domain knowledge by post-training is beneficial to sentiment classification .
The latter two subgraphs in Figure 1 are the feature distributions obtained by adversarial training .
One common characteristic is that data samples from different domains are very close to each other through adversarial training .
However , the boundary for sentiment polarity classification is not very clear in BERT - AT 's feature representation , resulting in degraded performance .
For our proposed BERT - DAAT , the post-training enables the domainawareness and help to distill more complicated domain specific features .
The adversarial training is thus enhanced to get more domain-invariant features .
We can find that target points are homogeneously spread out among source points , which decreases the divergence between the domains .
According to Theorem 10 , it can lower the upper boundary of the target error .
A-distance
Theorem 10 shows that the divergence between domains d H?H
( D s , D t ) plays an important role .
To quantitatively measure it , we compare the Adistance , which is usually used to measure domain discrepancy ( Ben - David et al. , 2010 ) .
The definition of A-distance is : d A = 2 ( 1 ? 2 ) , where is the generalization error of a classifier trained with the binary classification task of discriminating the source domain and target domain .
More precisely , to obtain A-distance , we firstly split source and target domain data into two subsets of equal size and get the feature representation .
We then train a linear SVM on the first subset to predict which domain the sample comes from .
The error rate could be calculated on the second subset through the trained SVM , and A-distance is obtained by d A = 2 ( 1 ? 2 ) .
We compare the A-distance of BERT , BERT - AT , and BERT - DAAT .
Results are shown in Figure 2 .
For each cross-domain sentiment analysis task , the A-distance of BERT is highest .
It is easy to conclude that applying adversarial training can effectively decrease the A-distance .
Overall , the Adistance of BERT - DAAT is lower than BERT - AT , verifying that the post-training could enhance the adversarial training to decrease the domain discrepancy .
Ablation Studies
To analyze the effect of different components including post-training steps and post-training tasks , we conduct the ablation experiments .
Effects of Post-Training Steps
In this subsection , we study the effect of posttraining steps .
Figure 3 presents the accuracy on the task of E?K based on the checkpoint that has been post-trained for k steps .
The results for BERT - DA are obtained by fine-tuning source domain labeled data , BERT - DAAT is adversarial training by source labeled data and target unlabeled data .
We find that , with limited post-training steps ( fewer than 5000 steps ) , BERT - DA and BERT - DAAT perform similarly with BERT and BERT - AT , respectively .
However , given post-training steps more than 5000 , both the results of BERT - DA and BERT - DAAT see an increase .
Especially , after post-training more than 5000 steps , BERT - DAAT shows remarkable strengths compared to BERT -DA .
This shows that plenty of steps is necessary to inject domain knowledge and domainawareness .
separately and compare them with BERT - DAAT on the tasks of D?B , E?B , and K?B. Results in Table 2 indicate that : the target domain masked language model task ( MLM ) and domain-distinguish task ( DDT ) are both beneficial to cross-domain sentiment analysis .
Conclusion and Future Work
In this paper , we propose the BERT - DAAT model for cross-domain sentiment analysis .
Our purpose is to inject the target domain knowledge to BERT and encourage BERT to be domain-aware .
Specifically , we conduct post-training and adversarial training .
A novel domain-distinguish pre-training task is designed to distill the domain-specific features in a self-supervised .
Experimental results on Amazon dataset demonstrate the effectiveness of our model , which remarkably outperforms state- ofthe - art methods .
The proposed post-training procedure could also be applied to other domain adaptation scenarios such as named entity recognition , question answering , and reading comprehension .
In the future , we would like to investigate the application of our theory in these domain adaptation tasks .
Figure 1 : 1 Figure 1 : The effect of post-training and adversarial training on the distribution of the extracted features .
The figure shows t-SNE visualization of the BERT 's hidden state for the B ? E task .
The red , blue , green and black points denote the source negative , source positive , target negative and target positive examples , respectively .
