title
Octa : Omissions and Conflicts in Target-Aspect Sentiment Analysis
abstract
Sentiments in opinionated text are often determined by both aspects and target words ( or targets ) .
We observe that targets and aspects interrelate in subtle ways , often yielding conflicting sentiments .
Thus , a naive aggregation of sentiments from aspects and targets treated separately , as in existing sentiment analysis models , impairs performance .
We propose Octa , 1 an approach that jointly considers aspects and targets when inferring sentiments .
To capture and quantify relationships between targets and context words , Octa uses a selective self-attention mechanism that handles implicit or missing targets .
Specifically , Octa involves two layers of attention mechanisms for , respectively , selective attention between targets and context words and attention over words based on aspects .
On benchmark datasets , Octa outperforms leading models by a large margin , yielding ( absolute ) gains in accuracy of 1.6 % to 4.3 % .
Introduction
People share their opinions about almost anything : tourist attractions , restaurants , car dealerships , and products .
Such opinionated texts do not merely help people make decisions in their daily life , but also help businesses measure consumer satisfaction to improve their offerings .
Sentiment analysis involves many aspects of Natural Language Processing , e.g. , negation handling ( Zhu et al. , 2014 ) , entity recognition ( Mitchell et al. , 2013 ) , topic modeling Singh , 2018 , 2019 ) .
Importantly , opinionated texts often convey conflicting sentiments .
Distinct sentiments may refer to distinct aspects of the domain in questione.g. , food quality of a restaurant or battery life of a smartphone .
These predefined domain aspects may or may not appear in the texts .
Aspect - Based Sentiment Analysis ( ABSA ) approaches ( Wang et al. , 2016 ; Xue and Li , 2018 ; Liang et al. , 2019 ) predict sentiments from text about a given aspect .
And , Target - Based Sentiment Analysis ( TBSA ) approaches ( Chen et al. , 2017 ; Fan et al. , 2018 ; Du et al. , 2019 ; predict sentiments of targets that appear in an opinionated text .
Targets are usually entities in a review : e.g. , a dish for a restaurant and a salesperson for a car dealership .
We posit that aspects and targets provide subtle , sometimes contradictory , information about sentiment and should therefore be modeled , not in isolation , but jointly .
Considering them separately , as ABSA and TBSA approaches do , impairs performance .
Take this review sentence from SemEval - 15 as an example : Conflicting Sentiments on Aspect
We both had the filet , very good , did n't much like the frites that came with .
If we ask about aspect Food # Quality , by disregarding targets during training , ABSA models fail to address the contradiction in sentiment about filet and frites , as do TBSA models , which focus on targets and disregard aspects .
In the following review sentence from SemEval - 16 , the target fish is associated with opposite sentiments : positive for Food # Quality and negative for Food # Style options .
Conflicting Sentiments on Target
The fish was fresh , though it was cut very thin .
Opinionated text is often not structured .
Users may not always mention targets explicitly .
In some cases , the entities in a sentence are not the targets associated with the sentiment .
In other cases , users mention multiple targets with sentiments in a sen-tence , but we need the overall sentiment .
Consider the following two sentences from SemEval - 16 : Implicit or Missing Target ( 1 ) You are bound to have a very charming time .
( 2 ) Endless fun , awesome music , great staff !!!
Here , ( 1 ) contains entity
You and positive sentiment toward aspect Restaurant # General but omits mention of the target restaurant .
And , ( 2 ) contains positive sentiment toward aspects Ambience and Service .
It expresses a positive sentiment toward aspect Restaurant # General albeit with no target .
How can we extract sentiments given an aspect with or without a target ?
Contributions
We propose Octa , an approach that jointly considers aspects and targets .
Octa uses a selective attention mechanism to capture subtle Target -Context and Target - Target relationships that reduce noisy information from irrelevant relations .
Octa uses ( 1 ) aspect embeddings with attention to incorporate aspect dependencies and ( 2 ) a surrogate target with BERT sequence embeddings to handle implicit or missing targets .
Octa can classify different types of conflicting sentiments with aspects only , targets only , both , or none .
Octa yields strong results on six benchmark datasets including SentiHood and four SemEval datasets , i.e. , 2014 ( target and aspect ) , 2015 , and 2016 .
Octa outperforms 16 state - of- the - art baselines by absolute gains in accuracy from 1.6 % to 4.3 % .
Sample Results of Octa
We explain the benefit of Octa via a few examples from the SemEval - 16 test set in Table 1 .
In case ( a ) , the same target is paired with different aspects .
Octa detects positive sentiment toward aspect Food # Quality based on target fish and context fresh .
By attending to different context cut very thin but the same target , Octa detects negative sentiment toward aspect Food # Style options .
In case ( b ) where different aspects paired with the same or different targets , Octa correctly detects neutral sentiment toward target food for aspect Food # Quality .
For target restaurant , Octa successfully detects conflicting sentiments toward different aspects by locating different context words .
In case ( c ) , the same aspects are paired with different targets .
Octa correctly detects the conflicting sentiments toward the same aspect Ambience # General .
Case ( d ) has aspect with implicit target and case ( e ) has different as -pects with or without target .
Octa successfully detects the sentiment toward implicit or missing target .
Problem Definition
The input of our sentiment analysis task is a sequence of words , with an aspect , or a target , or both .
Our goal is to identify the sentiment polarity associated with the aspect and the target .
Formally , Octa has three inputs , ?
Sequence of words : W = {w 1 , . . . , w N } , ? Target T i = {t 1 , . . . , t M } where t i ?
W , and ?
Aspect a i ?
A = { a 1 , . . . , a | A| } where A is a set of aspects .
The remaining words that are not part of the target are context words C = {c 1 , . . . , c N ?M }.
Octa Model Overview Figure 1 shows the Octa architecture .
To infer the sentiment for an aspect and a target composed of words from the sequence , first , Octa uses BERT to generate word embeddings .
Second , Octa uses a selective attention mechanism to compute context word and target attention weights and applies them to word embeddings to generate targeted contextual embeddings .
Third , Octa constructs aspect embeddings and uses the embeddings to compute aspect attention over target and context words .
Octa uses a multihead architecture to learn attention in diverse embedding subspaces .
It fuses and normalizes embeddings from each head and uses a linear classification layer with a softmax activation for sentiment classification .
To introduce nonlinearity , Octa uses feed -forward networks ( shown in grey ) , each comprising two fully connected layers followed by a nonlinear activation .
BERT Embeddings Octa uses Bidirectional Encoder Representations from Transformers ( BERT ) ( Devlin et al. , 2019 ) to generate word embeddings .
BERT is a contextualized language representation model , pretrained on large corpora and fine-tuned on downstream tasks , including token - level classification ( named entity recognition and reading comprehension ) and sequence - level classification ( semantic similarity and sentiment analysis ) .
Despite its success on various benchmarks , BERT ignores the relationships among target words , context words , and aspects , which are crucial for sentiment analysis .
Selective Self-Attention Mechanism
Words connect with one another to form semantic relations and create meanings in different contexts .
Self-attention ( Vaswani et al. , 2017 ) seeks to quantify this process .
To capture relationships between words , it learns to represent each word using itself and the other words in the same sentence .
The flexible structure of self-attention provides benefits in capturing different relations without range restriction .
An ideal self-attention layer should attend to relations differently to create contexts for different goals .
In practice , such flexibility may introduce noisy relations that lead to less-focused attention and confuse the decision layer .
In opinionated texts , context words carry sentiment .
A context word can be associated with one or more targets .
Thus , capturing Target - Context relationships is pivotal .
We posit that capturing Target - Target relationships is important when targets contain multiple words .
Context words can carry different sentiment when the same target word paired with other target words .
For example , in the sentences
The wine list is long and The waiting list is long , context word , long , is positive for target wine list but negative for target waiting list .
Octa uses a selective self-attention encoder to capture the subtle Target -Context and Target - Target relationships .
Figure 2 shows the encoding process .
Formally , given a sentence containing one target t that consists of M target words and context c that consists of N context words , let B t = [ b t 1 , . . . , b t M ] ? R M ?d B , B c = [ b c 1 , . . . , b c N ] ? R N ?d
B denote the BERT embedding matrices of targets and context words , respectively , where d B is the dimension of BERT embeddings .
We use BERT 's [ CLS ] token as either a target ( when no target is provided ) or a context word .
Feed-Forward Networks .
Octa adopts a keyquery - value attention structure ( Vaswani et al. , 2017 ) where keys , queries , and values are projected vectors .
The structure first combines each query with all of keys through a compatibility function to generate attention weights .
Then , it uses the weights to combine corresponding values to generate the output .
Octa uses five feed -forward networks to construct keys , queries , and values for target and context words .
Each feed -forward network comprises two fully connected linear layers connected by a GELU ( Hendrycks and Gimpel , 2016 ) ? c3 ? c2 ? c1 ? c4 ? c5 ? c6 A t A c B P F t k , F t q , F c k , F c q , F v V heads ?
t1 ? t2 ? t3 Feed-Forward Aspect Attention ( flows to Figure 3 ) activation for element - wise nonlinear projection .
F t k = W t k1 ? ( GELU ( W t k2 ? B t ) ) , ( 1 ) F tq = W t q1 ? ( GELU ( W t q2 ? B t ) ) , ( 2 ) F c k = W c k1 ? ( GELU ( W c k2 ? B c ) ) , ( 3 ) F cq = W c q1 ? ( GELU ( W c q2 ? B c ) ) , ( 4 ) F v = W v 1 ? ( GELU ( W v 2 ? [ B t ? B c ] ) ) , ( 5 ) where F t k , F tq ? R M ?d
F are keys and queries of targets , F c k , F cq ? R N ?d
F are keys and queries of context words , F v ? R ( M +N ) ?d
F are values for both kinds of words , ? means matrix vertical concatenation , W ( ? ) are parameters to learn , and we omit the bias for simplicity .
Target Word Attention .
Octa constructs an affinity matrix A t = {?
t 1 , . . . , ? t M } ? R M ?( M +N ) by computing dot products of each target with each word in the sentence .
A t = softmax ( F tq ?
[ F t k ?
F c k ] T ) . ( 6 ) A t is normalized row-wise to generate a list of attention weights for each target .
These attention weights quantify relations between words and describe the amount of focus the encoder should place on other words when encoding a target .
For sentences with no target , Octa uses BERT 's [ CLS ] token as a surrogate target to leverage the aggregated sentence information .
Context Word Attention .
Octa creates a mask matrix K c = {k c 1 , . . . , k c N } ? R N ?( M +N ) .
Here , k c i equals 1.0 if the corresponding position is context word c i or a target and zero otherwise .
Octa constructs the affinity matrix + N ) by computing the dot products of each context word with itself and each target in the sentence masked by K c , where ? denotes Hadamard product .
A c = {?
c 1 , . . . , ? c N } ? R N ?( M
A c = softmax ( F cq ?
[ F t k ?
F c k ] T ? K c ) . ( 7 ) A c is normalized row-wise to generate a list of attention weights for each context word .
These attention weights quantify dependencies between each context word and each target .
Our mask removes noisy dependencies between the context words .
Targeted Contextual Embeddings .
Given target attention A t and context word attention A c , Octa computes targeted contextual embeddings P ? R ( M +N ) ?d
F as follows .
P = [ A t ? A c ] ?
F v . ( 8 )
Aspect Attention
How the aspects and words in a sentence relate is vital in inferring sentiments .
As the second review sentence in Section 1 shows , one target can associate with different sentiments for different aspects .
To incorporate aspect information , given L aspects , A = { a 1 , . . . , a L } , Octa learns a list of aspect embeddings F A = {f a 1 , . . . , f a L } ?
R L?d E as follows , F A = W A 1 ? ( GELU ( W A 2 ? E ) ) , ( 9 ) where E = {e a 1 , . . . , e a L } , e a i ?
R d E are a list of randomly initialized aspect keys , W A 1 and W A 2 are weights to learn , and bias is omitted for simplicity .
To capture the relationships , Octa builds the affinity matrix A a ?
R M +N between aspect embeddings f a i and targeted contextual embeddings P of the sentence .
An illustrative example of aspect attention is shown in Figure 3 . A a i = softmax ( f a i ? P T ) . ( 10 ) The aspect and targeted contextual embeddings Q a i for aspect a i , Q a i ?
R d E +d F , are computed as Q a i = [ A a i ?
P ] f a i , ( 11 ) where denotes horizontal matrix concatenation .
Multihead Fusion
To attend in parallel to relation information from different dimensional subspaces , Octa uses a multihead architecture with V heads .
The final aspect and targeted contextual embeddings H a i ?
R V * ( d E +d F ) for aspect a i is the fusion of all heads .
H a i = [ Q h 1 a i , . . . , Q h V a i ] . ( 12 )
Sentiment Classification
For sentiment classification , Octa first applies layer normalization ( Ba et al. , 2016 ) on the multihead fusion .
Then , it uses a fully connected linear layer followed by a softmax activation to to project H a i to y ?
R S , the posterior probability over S sentiment polarities , is y ( omitting the bias ) : y = softmax ( W y ? H a i ) , ( 13 ) where W y is parameter to learn .
We train Octa with cross-entropy loss .
4 Empirical Evaluation
Data
We train and evaluate Octa on six benchmark datasets , described in Table 2 , from three domains .
Parameter Settings
We set the dimension of aspect embeddings d E to 1,024 .
For all feed -forward networks , we use 1,024 as the dimension of both inner and outer states d F .
We train Octa with 16 attention heads and freeze aspect embeddings during training .
We follow the literature in that we do not further split SemEval training sets into training and validation sets due to their size .
Instead , we use SentiHood - dev for parameter tuning .
For regularization , we add dropouts with a rate of 0.1 between the two fully connected layers in each nonlinear feed -forward network .
For optimization , we use Adam ( Kingma and Ba , 2015 ) and set ?
1 = 0.9 , ? 2 = 0.99 , weight decay = 0.01 , and the learning rate = 1e - 5 , with a warmup over 0.1 % of training .
For all experiments , we train Octa for 10 epochs on mini-batches of 32 randomly sampled sequences of 128 tokens .
We repeat the training and testing cycle five times using different random seeds .
Our evaluation metrics include accuracy and macro F 1 score .
We perform the two -sampled t-test on the improvement of Octa over BERT .
As reported in ( Devlin et al. , 2019 ) , we observe unstable performance for both Octa and BERT .
We perform several restarts and select best performed models .
For model size , Octa introduces 2.5 % more parameters ( 343 M ) compared with BERT sequence classification ( 335 M , whole word masking ) .
Training on SemEval - 16 with single NVIDIA Tesla V100 takes 69 seconds / epoch for Octa and 65 seconds / epoch for BERT .
Baselines
We compare the performance of Octa against the following published models .
Feature based Baselines : NRC - Canada , DCU , Sentiue , and XRCE require feature engineering based on linguistic tools and external resources .
Of these , NRC - Canada and DCU achieve the best performance on SemEval 2014 sentiment classification for aspect category and aspect term , respectively .
Sentiue and XRCE are the best performing for SemEval 2015 and 2016 , respectively .
TBSA Baselines : RAM ( Chen et al. , 2017 ) builds position - weighted memory using two stacked BiLSTMs and the relative distance of each word to the left or right boundary of each target .
It uses a GRU with multiple attention computed using the memory .
TNet - AS dynamically associates targets with sentence words to generate target specific word representation and uses adaptive scaling to preserve context information .
MGAN ( Fan et al. , 2018 ) is an attention network based on BiLSTM that computes coarse-grained attention using averaged target embeddings and context words and leverages word Liang et al. , 2019 ) contains an aspect-guided encoder which consists of an aspect-guided GRU and a deep transition GRU to extract aspect-specific sentence representation .
Note that GCAE and AGDT can be extended for TBSA .
However , neither of them jointly considers both aspects and targets and therefore fails to handle conflicting sentiments .
Other Baselines : Sentic LSTM ( Ma et al. , 2018 ) uses an LSTM with a hierarchical attention mechanism to model both target and aspect attention .
It incorporates commonsense knowledge into sentence embeddings .
BERT does not consider aspects and targets .
We compare with BERT to evaluate the performance gain from selective attention .
We use the whole world masking pretrained BERT in our experiments .
Additional results using BERT base and large models are in Appendix A .
Results
Table 3 compares Octa with baselines on SemEval datasets .
For SemEval - 14 -A , AGDT outperforms GCAE , demonstrating the benefits of aspect-guided sentence representation .
Octa outperforms AGDT and NRC - Canada with accuracy gains of 4.3 % and 3.1 % , respectively .
Since SemEval -14 - A lacks target information , Octa uses the BERT [ CLS ] token as the target .
The result shows the benefit of selective attention to capture implicit target information .
Octa and BERT yield comparable performance .
We find that SemEval - 14 - A contains sentences with conflicting sentiments toward the same aspect .
In the testing split , of 146 sentences labeled NEU , 52 sentences show conflicting sentimentse.g. , " the falafal was rather over cooked and dried but the chicken was fine " is labeled NEU for aspect food but contains positive sentiment toward target chicken and negative sentiment toward target falafal .
We conjecture that such data defects undermine the benefit of selective attention .
SemEval - 14 - T lacks aspect labels so Octa treats it as one aspect .
Octa outperforms all baselines with an accuracy gain of 1.9 % compared with the best performing baseline , TD - GAT - BERT , of 4.0 % over the feature - based baseline DCU .
SemEval - 15 and SemEval - 16 associate sentiment with both aspect and targets .
Octa outperforms all baselines .
Specifically , Octa obtains a 2.6 % and 1.6 % accuracy improvement over BERT on SemEval - 15 and SemEval - 16 , respectively .
The F 1 improvements over BERT are 1.5 % and 1.8 % .
Also , Octa outperforms the top feature - based models , Sentiue and XRCE .
The results demonstrate the benefit of jointly considering aspects and targets .
Table 4 shows the results on SentiHood .
Octa outperforms the state- of- the- art Sentic LSTM with accuracy gains of 3.3 % and 2.0 % on dev and test , respectively .
Sentic LSTM jointly considers both aspects and targets through a hierarchical attention mechanism .
We attribute Octa 's performance to its nonrecurrent architecture , which alleviates the dependency range restriction in LSTM , and to its selective attention mechanism , which reduces noisy dependency information from irrelevant relations .
To further evaluate Octa 's capability of han-dling sentences with conflicting sentiments , we apply trained BERT and Octa only on the conflicting samples from SemEval - 15 , SemEval - 16 , and SentiHood -test .
There are 152 , 96 , 343 conflicting samples in SemEval - 15 , SemEval - 16 , and SentiHood -test , respectively .
Ablation Study
We evaluate variants of Octa on SemEval - 15 to understand the contribution of aspects , targets , and selective attention .
The same conclusion holds for the other datasets .
As Table 6 shows , using target selective attention ( Octa - Sel ) yields 1.1 % better accuracy but similar F 1 as using aspect attention ( Octa- Asp ) .
Combining aspect attention with target self-attention ( Octa - Asp - Full ) hurts performance and stability , as seen in the lower accuracy and F 1 , indicating that simply applying self-attention on targets and context words introduces noisy information .
Replacing self-attention with selective attention ( Octa ) yields gains in accuracy and F 1 of 4.3 % and 6.1 % respectively , indicating that selective attention is effective in combating noise .
Related Work Sentiment analysis has received substantial attention over the last few years .
We highlight here only the works most relevant to Octa .
Aspect - Based Sentiment Analysis ( ABSA )
For the ABSA task , Wang et al . ( 2016 ) concatenate aspect embeddings with LSTM hidden states and apply attention mechanism to focus on different parts of a sentence given different aspects .
Xue and Li ( 2018 ) extracts features from text using a convolutional layer and propagates the features to a max pooling layer based on either aspects or targets .
Liang et al. ( 2019 ) uses an aspect-guided encoder with an aspect-reconstruction step to generate either aspect- or target-specific sentence representation .
The above models do not jointly consider aspects and targets and suffer when a target has conflicting sentiments toward different aspects .
Target - Based Sentiment Analysis ( TBSA )
For TBSA task ,
Conclusion
The main innovation of Octa is to jointly consider aspects and targets .
It uses selective attention to model the relationships between target and context words , and aspects to attend to targeted contexts to predict sentiments .
Users can " query "
Octa about sentiment of a particular aspect or target , or both .
Our evaluation shows that Octa outperforms state - of- the - art models on SemEval , SentiHood , and conflicting sentiment datasets .
Our ablation study shows that jointly modeling aspects and targets with selective attention is superior to selective attention only , aspect attention only , and aspect with self-attention .
