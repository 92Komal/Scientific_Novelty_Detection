title
Structural Opinion Mining for Graph- based Sentiment Representation
abstract
Based on analysis of on- line review corpus we observe that most sentences have complicated opinion structures and they cannot be well represented by existing methods , such as frame- based and feature - based ones .
In this work , we propose a novel graph - based representation for sentence level sentiment .
An integer linear programming - based structural learning method is then introduced to produce the graph representations of input sentences .
Experimental evaluations on a manually labeled Chinese corpus demonstrate the effectiveness of the proposed approach .
Introduction Sentiment analysis has received much attention in recent years .
A number of automatic methods have been proposed to identify and extract opinions , emotions , and sentiments from text .
Previous researches on sentiment analysis tackled the problem on various levels of granularity including document , sentence , phrase and word ( Pang et al. , 2002 ; Riloff et al. , 2003 ; Dave et al. , 2003 ; Takamura et al. , 2005 ; Kim and Hovy , 2006 ; Somasundaran et al. , 2008 ; Dasgupta and Ng , 2009 ; Hassan and Radev , 2010 ) .
They mainly focused on two directions : sentiment classification which detects the overall polarity of a text ; sentiment related information extraction which tries to answer the questions like " who expresses what opinion on which target " .
Most of the current studies on the second direction assume that an opinion can be structured as a frame which is composed of a fixed number of slots .
Typical slots include opinion holder , opinion expression , and evaluation target .
Under this representa-tion , they defined the task as a slots filling problem for each of the opinions .
Named entity recognition and relation extraction techniques are usually applied in this task ( Hu and Liu , 2004 ; Kobayashi et al. , 2007 ; Wu et al. , 2009 ) .
However , through data analysis , we observe that 60.5 % of sentences in our corpus do not follow the assumption used by them .
A lot of important information about an opinion may be lost using those representation methods .
Consider the following examples , which are extracted from real online reviews :
Example 1 : The interior is a bit noisy on the freeway 1 . Example 2 : Takes good pictures during the daytime .
Very poor picture quality at night 2 . Based on the definition of opinion unit proposed by Hu and Liu ( 2004 ) , from the first example , the information we can get is the author 's negative opinion about " interior " using an opinion expression " noisy " .
However , the important restriction " on the freeway " , which narrows the scope of the opinion , is ignored .
In fact , the tuple ( " noisy " , " on the freeway " ) cannot correctly express the original opinion : it is negative but under certain condition .
The second example is similar .
If the conditions " during the daytime " and " at night " are dropped , the extracted elements cannot correctly represent user 's opinions .
Example 3 : The camera is actually quite good for outdoors because of the software .
Besides that , an opinion expression may induce other opinions which are not expressed directly .
In example 3 , the opinion expression is " good " whose target is " camera " .
But the " software " which triggers the opinion expression " good " is also endowed with a positive opinion .
In practice , this induced opinion on " software " is actually more informative than its direct counterpart .
Mining those opinions may help to form a complete sentiment analysis result .
Example 4 : The image quality is in the middle of its class , but it can still be a reasonable choice for students .
Furthermore , the relations among individual opinions also provide additional information which is lost when they are considered separately .
Example 4 is such a case that the whole positive comment of camera is expressed by a transition from a negative opinion to a positive one .
In order to address those issues , this paper describes a novel sentiment representation and analysis method .
Our main contributions are as follows : 1 . We investigate the use of graphs for representing sentence level sentiment .
The vertices are evaluation target , opinion expression , modifiers of opinion .
The Edges represent relations among them .
The semantic relations among individual opinions are also included .
Through the graph , various information on opinion expressions which is ignored by current representation methods can be well handled .
And the proposed representation is language - independent .
2 . We propose a supervised structural learning method which takes a sentence as input and the proposed sentiment representation for it as output .
The inference algorithm is based on integer linear programming which helps to concisely and uniformly handle various properties of our sentiment representation .
By setting appropriate prior substructure constraints of the graph , the whole algorithm achieves reasonable performances .
The remaining part of this paper is organized as follows :
In Section 2 we discuss the proposed representation method .
Section 3 describes the computational model used to construct it .
Experimental results in test collections and analysis are shown in Section 4 .
In Section 5 , we present the related work and Section 6 concludes the paper .
Graph - based Sentiment Representation
In this work , we propose using directed graph to represent sentiments .
In the graph , vertices are text spans in the sentences which are opinion expressions , evaluation targets , conditional clauses etc .
Two types of edges are included in the graph : ( 1 ) relations among opinion expressions and their modifiers ; ( 2 ) relations among opinion expressions .
The edges of the first type exist within individual opinions .
The second type of the edges captures the relations among individual opinions .
The following sections detail the definition .
Individual Opinion Representation
Let r be an opinion expression in a sentence , the representation unit for r is a set of relations { ( r , d k ) } .
For each relation ( r , d k ) , d k is a modifier which is a span of text specifying the change of r's meaning .
The relations between modifier and opinion expression can be the type of any kind .
In this work , we mainly consider two basic types : ? opinion restriction .
( r , d k ) is called an opinion restriction if d k narrows r's scope , adds a condition , or places limitations on r's original meaning .
? opinion expansion .
( r , d k ) is an opinion expansion if r's scope expands to d k , r induces another opinion on d k , or the opinion on d k is implicitly expressed by r.
Mining the opinion restrictions can help to get accurate meaning of an opinion , and the opinion expansions are useful to cover more indirect opinions .
As with previous sentiment representations , we actually consider the third type of modifier which d k is the evaluation target of r.
Figure 1 shows a concrete example .
In this example , there are three opinion expressions : " good " , " sharp " , " slightly soft " .
The modifiers of " good " are " indoors " and " Focus accuracy " , where relation ( " good " , " indoors " ) is an opinion restriction because " indoors " is the condition under which " Focus accuracy " is good .
On the other hand , the relation ( " sharp " , " little 3x optical zooms " ) is an opinion expansion because the " sharp " opinion on " shot " implies a positive opinion on " little 3x optical zooms " .
It is worth to remark that : 1 ) a modifier d k can relate to more than one opinion expression .
For example , multiple opinion expressions may share a same condition ; 2 ) d k itself can employ a set of relations , although the case appears occasionally .
The following is an example : Example 5 : The camera wisely get rid of many redundant buttons .
In the example , " redundant buttons " is the evaluation target of opinion expression " wisely get rid of " , but itself is a relation between " redundant " and " buttons " .
Such nested semantic structure is described by a path : " wisely get rid of " target ? ? ? [ " redundant " target ? ? ?" buttons " ] nested target .
Relations between Individual Opinion Representation Assume r i are opinion expressions ordered by their positions in sentence , and each of them has been represented by relations {( r i , d ik ) } individually ( the nested relations for d ik have also been determined ) .
Then we define two relations on adjacent pair r i , r i + 1 : coordination when the polarities of r i and r i+ 1 are consistent , and transition when they are opposite .
Those relations among r i form a set B called opinion thread .
In Figure 1 , the opinion thread is : { ( " good " , " sharp " ) , ( " sharp " , " slightly soft " ) } .
The whole sentiment representation for a sentence can be organized by a direct graph G = ( V , E ) .
Vertex set V includes all opinion expressions and modifiers .
Edge set E collects both relations of each individual opinion and relations in opinion thread .
The edges are labeled with relation types in label set L={" restriction " , " expansion " , " target " , " coordination " , " transition " }
3 . Compared with previous works , the advantages of using G as sentiment representation are : 1 ) for individual opinions , the modifiers will collect more information than using opinion expression alone .
3
We do n't define any " label " on vertices : if two span of text satisfy a relation in L , they are chosen to be vertices and an edge with proper label will appear in E .
In other words , vertices are identified by checking whether there exist relations among them .
Focus accuracy was good indoors , and although the little 3x optical zooms produced sharp shots , the edges were slightly soft on the Canon .
Thus G is a relatively complete and accurate representation ; 2 ) the opinion thread can help to catch global sentiment information , for example the general polarity of a sentence , which is dropped when the opinions are separately represented .
System Description
To produce the representation graph G for a sentence , we need to extract candidate vertices and build the relations among them to get a graph structure .
For the first task , the experimental results in Section 4 demonstrate that the standard sequential labeling method with simple features can achieve reasonable performance .
In this section , we focus on the second task , and assume the vertices in the graph have already been correctly collected in the following formulation of algorithm .
Preliminaries
In order to construct graph G , we use a structural learning method .
The framework is from the first order discriminative dependency parsing model ( Mcdonald and Pereira , 2005 ) .
A sentence is denoted by s ; x are text spans which will be vertices of graph ; x i is the ith vertex in x ordered by their positions in s .
For a set of vertices x , y is the graph of its sentiment representation , and e = ( x i , x j ) ? y is the direct edge from x i to x j in y .
In addition , x 0 is a virtual root node without inedge .
G = {( x n , y n ) }
N n is training set .
Following the edge based factorization , the score of a graph is the sum of its edges ' scores , score ( x , y ) = ? ( x i , x j ) ?y score ( x i , x j ) = ? ( x i , x j ) ?y ?
T f ( x i , x j ) , ( 1 ) f ( x i , x j ) is a high dimensional feature vector of the edge ( x i , x j ) .
The components of f are either 0 or 1 .
For example the k-th component could be f k ( x i , x j ) = ? ? ? 1 if x i .POS = JJ and x j .POS = NN and label of ( x i , x j ) is restriction 0 otherwise .
Then the score of an edge is the linear combination of f 's components , and the coefficients are in vector ?.
Algorithm 1 shows the parameter learning process .
It aims to get parameter ?
which will assign the correct graph y with the highest score among all possible graphs of x ( denoted by Y ) .
Algorithm 1 Online structural learning Training Set:G = {( x n , y n ) }
N n 1 : ? 0 = 0 , r = 0 , T =maximum iteration 2 : for t = 0 to T do 3 : for n = 0 to N do end for 10 : end for 11 : return ? = r/( N * T )
Inference
Like other structural learning tasks , the " arg max " operation in the algorithm ( also called inference ) ?
= arg max y?Y score ( x , y ) = arg max y?Y ? ( x i , x j ) ?y ?
T f ( x i , x j ) ( 2 ) is hard because all possible values of y form a huge search space .
In our case , Y is all possible directed acyclic graphs of the given vertex set , which number is exponential .
Directly solving the problem of finding maximum weighted acyclic graph is equivalent to finding maximum feedback arc set , which is a NP - hard problem ( Karp , 1972 ) .
We will use integer linear programming ( ILP ) as the framework for this inference problem .
Graph Properties
We first show some properties of graph G either from the definition of relations or corpus statistics .
Property 1 . The graph is connected and without directed cycle .
From individual opinion representation , each subgraph of G which takes an opinion expression as root is connected and acyclic .
Thus the connectedness is guaranteed for opinion expressions are connected in opinion thread ; the acyclic is guaranteed by the fact that if a modifier is shared by different opinion expressions , the inedges from them always keep ( directed ) acyclic .
Property 2 . Each vertex can have one outedge labeled with coordination or transition at most .
The opinion thread B is a directed path in graph .
Property 3 . The graph is sparse .
The average in - degree of a vertex is 1.03 in our corpus , thus the graph is almost a rooted tree .
In other words , the cases that a modifier connects to more than one opinion expression rarely occur comparing with those vertices which have a single parent .
An explaination for this sparseness is that opinions in online reviews always concentrate in local context and have local semantic connections .
ILP Formulation Based on the property 3 , we divide the inference algorithm into two steps : i ) constructing G's spanning tree ( arborescence ) with property 1 and 2 ; ii ) finding additional non-tree edges as a post processing task .
The first step is close to the works on ILP formulations of dependency parsing ( Riedel and Clarke , 2006 ; Martins et al. , 2009 ) .
In the second step , we use a heuristic method which greedily adds non-tree edges .
A similar approximation method is also used in ( Mcdonald and Pereira , 2006 ) for acyclic dependency graphs .
Step 1 . Find MST .
Following the multicommodity flow formulation of maximum spanning tree ( MST ) problem in ( Magnanti and Wolsey , 1994 ) , the ILP for MST is : max . ? i , j y ij ? score( x i , x j ) ( 3 ) s.t. ? i , j y ij = | V | ? 1 ( 4 ) ?
i f u ij ? ? k f u jk = ?
u j ,1 ? u , j ? | V | ( 5 ) ? k f u 0 k = 1 , 1 ? u ? | V | ( 6 ) f u ij ?
y ij , 1 ? u , j ? | V | , 0 ? i ? | V | ( 7 ) f u ij ?
0 , 1 ? u , j ? | V | , 0 ? i ? | V | ( 8 ) y ij ? { 0 , 1 } , 0 ? i , j ? | V |. ( 9 ) In this formulation , y ij is an edge indicator variable that ( x i , x j ) is a spanning tree edge when y ij = 1 , ( x i , x j ) is a non-tree edge when y ij = 0 .
Then output y is represented by the set {y ij , 0 ? i , j ? | V |} 4 . Eq ( 4 ) ensures that there will be exactly | V | ?
1 edges are chosen .
Thus if the edges corresponding to those non zero y ij is a connected subgraph , y is a well - formed spanning tree .
Objective function just says the optimal solution of y ij have the maximum weight .
The connectedness is guaranteed if for every vertex , there is exactly one path from root to it .
It is formulated by using | V | ?
1 flows { f u , 1 ? u ? | V |}. f u starts from virtual root x 0 towards vertex x u .
Each flow f u = {f u ij , 0 ? i , j ? | V |}. f u ij indi- cates whether flow f u is through edge ( x i , x j ) . so it should be 0 if edge ( x i , x j ) does not exist ( by ( 7 ) ) .
The Kronecker 's delta ?
u j in ( 5 ) guarantees f u is only assumed by vertex x u , so f u is a well - formed path from root to x u . ( 6 ) ensures there is only one flow ( path ) from root to x u .
Thus the subgraph is connected .
The following are our constraints : c1 : Constraint on edges in opinion thread ( 10 ) -( 11 ) .
From the definition of opinion thread , we impose a constraint on every vertex 's outedges in opinion thread , which are labeled with " coordination " or " transition " .
Let I ob be a characteristic function on edges : I ob ( ( j , k ) ) = 1 when edge ( x j , x k ) is labeled with " coordination " or " transition " , otherwise 0 .
We denote q variables for vertices : q j = ?
k y jk ?
I ob ( ( j , k ) ) , 0 ? j ? | V |. ( 10 )
Then following linear inequalities bound the number of outedges in opinion thread ( ? 1 ) on each vertex : q j ? 1 , 0 ? j ? | V |. ( 11 ) c2 : Constraint on target edge ( 12 ) .
We also bound the number of evaluation targets for a vertex in a similar way .
Let I t be characteristic function on edges identifing whether it is labeled with " target " , ? k y jk ?
I t ( ( j , k ) ) ?
C t , 0 ? j ? | V |. ( 12 ) The parameter C t can be adjusted according to the style of document .
In online reviews , authors tend to use simple and short comments on individual targets , so C t could be set small .
c3 : Constraint on opinion thread ( 13 ) -( 18 ) .
From graph property 2 , the opinion thread should be a directed path .
It implies the number of connected components whose edges are " coordination " or " transition " should be less than 1 .
Two set of additional variables are needed : {c j , 0 ? j ? | V |} and {h j , 0 ? j ?
| V |} , where c j = { 1 if an opinion thread starts at x j 0 otherwise , and h j = ?
i y ij ?
I ob ( ( i , j ) ) .
( 13 )
Then c j = ?h j ?
q j , which can be linearized by c j ? q j ? h j , ( 14 ) c j ?
1 ? h j , ( 15 ) c j ? q j , ( 16 ) c j ? 0 . ( 17 ) If the sum of c j is no more than 1 , the opinion thread of graph is a directed path .
Figure 2 illustrates the effects of c1 and c3 .
Equations ( 10 ) -( 18 ) , together with basic multicommodity flow model build up the inference algorithm .
The entire ILP formulation involves O ( |V | 3 ) variables and O ( |V | 2 ) constraints .
Generally , ILP falls into NPC , but as an important result , in the multicommodity flow formulation of maximum spanning tree problem , the integer constraints ( 9 ) on y ij can be dropped .
So the problem reduces to a linear programming which is polynomial solvable ( Magnanti and Wolsey , 1994 ) .
Unfortunately , with our additional constraints the LP relaxation is not valid .
? j c j ? 1 . ( 18 ) Step 2 .
Adding non-tree edges .
We examine the case that a modifier attaches to different opinion expressions .
That often occurs as the result of the sharing of modifiers among adjacent opinion expressions .
We add those edges in the following heuristic way :
If a vertex r i in opinion thread does not have any modifier , we search the modifiers of its adjacent vertices r i+ 1 , r i?1 in the opinion thread , and add edge ( r i , d * ) where d * = arg max d?S score ( r i , d ) , and S are the modifiers of r i?1 and r i + 1 .
Training
We use online passive aggressive algorithm ( PA ) with Hamming cost of two graphs in training ( Crammer et al. , 2006 )
Feature Construction
For each vertex x i in graph , we use 2 sets of features : inside features which are extracted inside the text span of x i ; outside features which are outside the text span of x i .
A vertex x i is described both in word sequence ( w 0 , w 1 , ? ? ? , w k ) and character sequence ( c 0 , c 1 , ? ? ? , c l ) , for the sentences are in Chinese .
? ? ? , w ?1 , w 0 , w 1 , w 2 , ? ? ? , w k?1 , w k x i , w k+1 ? ? ? ? ? ? , c ?1 , c 0 , c 1 , c 2 , ? ? ? , c l?1 , c l x i , c l + 1 ? ? ?
For an edge ( x i , x j ) , the high dimensional feature vector f(x i , x j ) is generated by using unigram features in Table 1 on x i and x j respectively .
The distance between parent and child in sentence is also attached in features .
In order to involve syntactic information , whether there is certain type of dependency relation between x i and x j is also used as a feature .
Experiments
Corpus
We constructed a Chinese online review corpus from Pcpop.com , Zol.com.cn , and It168.com , which have a large number of reviews about digital camera .
The corpus contains 138 documents and 1735 sentences .
Since some sentences do not contain any opinion , 1390 subjective sentences were finally chosen and manually labeled .
Two annotators labeled the corpus independently .
The annotators started from locating opinion expressions , and for each of them , they annotated other modifiers related to it .
In order to keep the reliability of annotations , another annotator was asked to check the corpus and determine the conflicts .
Finally , we extracted 6103 elements , which are connected by 6284 relations .
Table 2 shows the number of various relation types appearing in the labeled corpus .
We observe 60.5 % of sentences and 32.1 % of opinion expressions contain other modifiers besides " target " .
Thus only mining the relations between opinion expressions and evaluation target is actually at risk of inaccurate and incomplete results .
Relation
Experiments Configurations
In all the experiments below , we take 90 % of the corpus as training set , 10 % as test set and run 10 folder cross validation .
In feature construction , we use an external Chinese sentiment lexicon which contains 4566 positive opinion words and 4370 negative opinion words .
For Chinese word segment , we use ctbparser 5 . Stanford parser ( Klein and Manning , 2003 ) is used for dependency parsing .
In the settings of PA , the maximum iteration number is set to 2 , which is chosen by maximizing the testing performances , aggressiveness parameter C is set to 0.00001 .
For parameters in inference algorithm , C t = 2 , the solver of ILP is lpsolve 6 .
We evaluate the system from the following aspects : 1 ) whether the structural information helps to mining opinion relations .
2 ) How the proposed inference algorithm performs with different constraints .
3 ) How the various features affect the system .
Except for the last one , the feature set used for different experiments are the same ( " In + Out + Dep " in Table 5 ) .
The criteria for evaluation are similar to the unlabeled attachment score in parser evaluations , but due to the equation | E | = | V | ?
1 is not valid if G is not a tree , we evaluate precision P = # true edges in result graph # edges in result graph , recall R = # true edges in result graph # edges in true graph , and F-score F = 2P ?R P +R .
Results 1 . The effects of structural information .
An alternative method to extract relations is directly using a classifier to judge whether there is a relation between any two elements .
Those kinds of methods were used in previous opinion mining works ( Wu et al. , 2009 ; Kobayashi et al. , 2007 ) .
To show the entire structural information is important for mining relations , we use SVM for binary classification on candidate pairs .
The data point representing a pair ( x i , x j ) is the same as the high dimensional feature vectors f( x i , x j ) .
The setting of our algorithm " MST + c1 + c2 + c3 " is the basic MST with all the constraints .
The results are shown in the Table 3 .
From the results , the performance of SVM ( especially recall ) is relatively poor .
A possible reason is that the huge imbalance of positive and negative training samples ( only ?( n ) positive pairs among all n 2 pairs ) .
And the absence of global structural knowledge makes binary classifier unable to use the information provided by classification results of other pairs .
In order to examine whether the complicated sentiment representation would disturb the classifier in finding relations between opinion expressions and its target , we evaluate the system by discarding the modifiers of opinion restriction and expansion from the corpus .
The result is shown in the second row of Table 3 .
We observe that " MST + c1 + c2 + c3 " is still better which means at least on overall performance the additional modifiers do not harm .
2 . The effect of constraints on inference algorithm .
In the inference algorithm , we utilized the properties of graph G and adapted the basic multicommodity flow ILP to our specific task .
To evaluate how the constraints affect the system , we decompose the algorithm and combine them in different ways .
Table 4 : Results on inference methods .
" MST " is the basic multicommodity flow formulation of maximum spanning tree ; c1 , c2 , c3 are groups of constraint from Section 3.2.2 ; " g " is our heuristic method for additional non spanning tree edges .
P From Table 4 , we observe that with any additional constraints the inference algorithm outperforms the basic maximum spanning tree method .
It implies although we did not use high order model ( e.g. involving grandparent and sibling features ) , prior structural constraints can also help to get a better output graph .
By comparing with different constraint combinations , the constraints on opinion thread ( c1 , c3 ) are more effective than constraints on evaluation targets ( c2 ) .
It is because opinion expressions are more important in the entire sentiment representation .
The main structure of a graph is clear once the relations between opinion expressions are correctly determined .
3 . The effects of various features .
We evaluate the performances of different feature configurations in Table 5 .
From the results , the outside feature set is more effective than inside feature set , even if it does not use any external resource .
A possible reason is that the content of a vertex can be very complicated ( a vertex even can be a clause ) , but the features surrounding the vertex are relatively simple and easy to identify ( for example , a single preposition can identify a complex condition ) .
The dependency feature has limited effect , due to that lots of online review sentences are ungrammatical and parsing results are unreliable .
And the complexity of vertices also messes the dependency feature .
In " represents the result of inside feature set ; " In -s " is " In " without the external opinion lexicon feature ; " Out " uses the outside feature set ; " In + Out " uses both " In " and " Out " , " In + Out + Dep " adds the dependency feature .
The inference algorithm is " MST + c1 + c2 + c3 + g " in Table 4 .
We analyze the errors in test results .
A main source of errors is the confusion of classifier between " target " relations and " coordination " , " transition " relations .
The reason may be that for a modification on opinion expression ( r , d k ) , we allow d k recursively has its own modifiers ( Example 5 ) .
Thus an opinion expression can be a modifier which brings difficulties to classifier .
4 . Extraction of vertices .
Finally we conduct an experiment on vertex extraction using standard sequential labeling method .
The tag set is simply { B , I , O } which are signs of begin , inside , outside of a vertex .
The underlying model is conditional random field 7 . Feature templates involved are in Table 6 .
We only use basic features in the experiment .
10 folder cross validation results are in table 7 .
We suspect that the performances ( especially recall ) could be improved if some external resources ( i.e. ontology , domain related lexicon , etc. ) are involved .
Unigram Template c i .char character c i .isDigit digit c i .isAlpha english letter c i .isPunc punctuation c i .
inDict in a sentiment word c i .
BWord start of a word c i .EWord end of a word
Related Work Opinion mining has recently received considerable attentions .
Large amount of work has been done on sentimental classification in different levels and sentiment related information extraction .
Researches on different types of sentences such as comparative sentences ( Jindal and Liu , 2006 ) and conditional sentences ( Narayanan et al. , 2009 ) have also been proposed .
Kobayashi et al. ( 2007 ) presented their work on extracting opinion units including : opinion holder , subject , aspect and evaluation .
They used slots to represent evaluations , converted the task to two kinds of relation extraction tasks and proposed a machine learning - based method which used both contextual and statistical clues .
Jindal and Liu ( 2006 ) studied the problem of identifying comparative sentences .
They analyzed different types of comparative sentences and proposed learning approaches to identify them .
Sentiment analysis of conditional sentences were studied by Narayanan et al . ( 2009 ) .
They aimed to determine whether opinions expressed on different topics in a conditional sentence are positive , negative or neutral .
They analyzed the conditional sentences in both linguistic and computitional perspectives and used learning method to do it .
They followed the feature - based sentiment analysis model ( Hu and Liu , 2004 ) , which also use flat frames to represent evaluations .
Integer linear programming was used in many NLP tasks ( Denis and Baldridge , 2007 ) , for its power in both expressing and approximating various inference problems , especially in parsing ( Riedel and Clarke , 2006 ; Martins et al. , 2009 ) . Martins etc. ( 2009 ) also applied ILP with flow formulation for maximum spanning tree , besides , they also handled dependency parse trees involving high order features ( sibling , grandparent ) , and with projective constraint .
Conclusions
This paper introduces a representation method for opinions in online reviews .
Inspections on corpus show that the information ignored in previous sentiment representation can cause incorrect or incomplete mining results .
We consider opinion restriction , opinion expansions , relations between opinion expressions , and represent them with a directed graph .
Structural learning method is used to produce the graph for a sentence .
An inference algorithm is proposed based on the properties of the graph .
Experimental evaluations with a manually labeled corpus are given to show the importance of structural information and effectiveness of proposed inference algorithm .
gram( 2009BAH40B04 ) .
Figure 1 : 1 Figure 1 : Sentiment representation for an example sentence
max y?Y score ( x n , y ) Inference 5 : if ?
= y n then 6 : update ? t to ?
t+1 PA 7 : r = r + ? t+1
Figure 2 : 2 Figure 2 : The effects of c1 and c3 .
Assume solid lines are edges labeled with " coordination " and " transition " , dot lines are edges labeled with other types . ( a ) is an arbitrary tree . ( b ) is a tree with c1 constraints . ( c ) is a tree with c1 and c3 .
It shows c1 are not sufficient for graph property 2 : the edges in opinion thread may not be connected .
. Unigram Feature Template x i .text w 0 .text w 1 .text w 0 .POS w 1 .POS w k?1 .text w k .text Inside w k?1 .POS w k .POS Features x i .hasDigital x i .isSingleWord x i .hasSentimentWord x i .hasParallelPhrase w ?1 .text w ?2 .text w ?1 .POS w ?2 .POS w k+1 .text w k+2 .text Outside w k+1 .POS w k+2 .POS Features c ?1 .text c ?2 .text c ?1 .POS c ?2 .POS c l+1 .text c l+2 .text c l+1 .POS c l+2 .POS
Other Features distance between parent and child dependency parsing relations
Table 1 : 1 Feature set
Table 2 : 2 Statistics of relation types Number Target 2479 Coordinate 1173 Transition 154 Restriction 693 Expansion 386
Table 3 : 3 Binary classifier and structural learning
Table 5 : 5 Results with different features . " P R F In-s 66.3 66.3 66.3 In 66.7 66.4 66.6 Out 67.8 67.4 67.6 In + Out 72.0 70.5 71.0 In + Out+ Dep 72.5 72.3 72.4
Table 6 : 6 Features for vertex extraction .
The sequential labeling is conducted on character level ( c i ) .
The sentiment lexicon used in c i . inDict is the same as Table1 .
We also use bigram feature templates on c i .char , c i .isAlpha , c i .inDict with respect to c i?1 and c i+ 1 . P R F E+Unigram 56.8 45.1 50.3 E+Unigram + Bigram 57.3 47.9 52.1 O+Unigram 71.9 57.2 63.7 O+Unigram + Bigram 72.3 60.2 65.6
Table 7 : 7 Results on vertices extraction with 10 folder cross
We use two criterion : 1 ) the vertex is correct if it is exactly same as ground truth ( " E " ) , 2 ) the vertex is correct if it overlaps with ground trut h ( " O " ) .
http://reviews.carreview.com/blog/2010-ford-focusreview-the-compact-car-that-can/ 2 http://www.dooyoo.co.uk/digital-camera/sony-cyber-shotdsc-s500/1151680/
For simplicity , we overload symbol y from the graph of the sentiment represetation to the MST of it .
http://code.google.com/p/ctbparser/
http://sourceforge.net/projects/lpsolve/
We use CRF ++ toolkit , http://crfpp.sourceforge.net/
