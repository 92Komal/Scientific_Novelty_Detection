title
Entities ' Sentiment Relevance
abstract
Sentiment relevance detection problems occur when there is a sentiment expression in a text , and there is the question of whether or not the expression is related to a given entity or , more generally , to a given situation .
The paper discusses variants of the problem , and shows that it is distinct from other somewhat similar problems occurring in the field of sentiment analysis and opinion mining .
We experimentally demonstrate that using the information about relevancy significantly affects the final sentiment evaluation of the entities .
We then compare a set of different algorithms for solving the relevance detection problem .
The most accurate results are achieved by algorithms that use certain document - level information about the target entities .
We show that this information can be accurately extracted using supervised classification methods .
Introduction Sentiment extraction by modern sentiment analysis ( SA ) systems is usually based on searching the input text for sentiment - bearing words and expressions , either general ( language- wide ) or domain-specific .
In most common SA approaches , each such expression carries a polarity value ( " positive " or " negative " ) which is possibly weighted .
The sum of all polarity values from all expressions found in a text becomes the sentiment score for the whole text .
People are , however , usually interested in sentiments regarding some entity or situation , and not in sentiments of a particular document .
A natural way to make the SA more focused is to explicitly bind each sentiment expression to a specific entity , or to a small set of entities from among all entities mentioned in the document .
The choice of which entity to bind a sentiment expression to , can be made according to the proximity ( physical , syntactical , and / or semantic ) and / or salience of the entities .
In this paper , we argue that all of these methods can be useful in different contexts , and so the best single algorithm should use all available proximity information , of all kinds , together with additional context information - position in the document , section , or paragraph ; proximity of other entities ; lexical contents ; etc .
One of the most important context information is the type of relation between the target entity and the documentwhether the entity is the main topic of the document , or one of several main topics , or mentioned in passing , etc .
Another layer that we'd like to add concerns the interaction of different entity types during SA .
In a typical situation , there is only one entity type which is the target for SA .
In such cases , clearly distinguishing between the relevancy of target and non-target entities types is not essential .
For example , when the general topic is a COMPANY , and there is a sentiment expression referring to a PERSON or a PRODUCT , this sentiment expression is still relevant to the company and can be regarded as such .
In other situations , SA users may be specifically interested in an interaction between entities of different types .
For example , in a medical forum setting , it may be interesting to know the users ' sentiments regarding a given DRUG in the context of a given DISEASE .
We will show that such situations are modeled well enough using intersections of regions of relevance of the participating entity types , with the relevance region for each type calculated separately .
We purposefully exclude possible interactions between entities of the same type , because they behave in a different way .
The precise analysis of such interactions is a different topic from rele-vance detection , and so it is mostly ignored in this paper .
Related Work
The task of SA has drawn the attention of many researchers worldwide ( Connor et al. , 2010 ; Liu , 2012 ; Loughran and Mcdonald , 2010 ; Pang and Lee , 2004 ; Turney , 2002 ) .
While most SA research is focused on discovering and classifying the expressions , some are also concerned with the targets of the expressions and explicitly identify the syntactic targets of sentiment expressions ( Pang and Lee , 2004 ) .
Other related works belong to the Passage Retrieval field , since the relevance detection problem can be construed as a specific form of passage retrieval problem ( Liu and Croft , 2002 ; Tiedemann and Mur , 2008 ) .
Different approaches were suggested for passage retrieval ( Buscaldi et al. , 2010 ; Comas et al. , 2012 ; Hearst , 1997 ; Lafferty et al. , 2001 ; Lin et al. , 2012 ; Liu and Croft , 2002 ; Lloret et al. , 2012 ; O'Connor et al. , 2013 ; Otterbacher et al. , 2009 ; Salton et al. , 1993 ; Wachsmuth , 2013 ) , some are more sophisticated than others .
The closest approach to ours is the one of Scheible and Sch?tze ( 2013 ) , but in contrast to them , we strive to discover sentiments ' relevance for all entities ( of a given type ) mentioned in the document , not necessarily topical .
Entity Relevance
An instance of the sentiment relevance detection problem for a single entity consists of a text document , a sentiment expression within the document , and a target entity .
The task is a binary decision : ' relevant ' vs. ' irrelevant ' .
To solve this task , we can use any information that can be found by analyzing the document .
Thus , we can assume that we know the parse trees of all sentences and the locations of all references of all entities in the document , including co-references .
In addition , we make use of an extra piece of information for each target entityits " status within the document " , or " document type with respect to the entity " .
We distinguish between several types which are intuitively clearly different : ?
' Target ' the entity is the main topic of the document ; ?
' Accidental ' the entity is not the main topic of the document , and is mentioned in passing ; ?
' RelationTarget ' the main topic of the document is a relation between the entity and some other entities of the same type ; ?
' ListTarget ' the entity is one of a few equally important topics , dealt with sequentially .
In the datasets we use for experiments , each entity is manually annotated with its status within the document , which allows us to directly observe the influence of this data on the accuracy of relevance discernment .
We also show that this data can be automatically extracted using supervised classification .
Since this paper is primarily a study of sentiment relevance , the actual sentiment expressions are not always labeled in our datasets .
Instead , relevance ranges are annotated for each entity , in the style of passage retrieval problems , with the expectation that sentiment expressions relevant to an entity only appear in the parts of the document that are labeled as " relevant " , and conversely , that all expressions appearing in parts labeled " irrelevant " are irrelevant .
This way of annotating allows the comparing of different relevance detection strategies independently of the main sentiment extraction tool .
All of the algorithms discussed in this paper use the same document processing methods , thus allowing us to compare the algorithms themselves independent of the quality and specifics of the underlying NLP .
The multiple-entity relevance problem is distinguished from the single-entity relevance problem by the requirement for the sentiment expression to be relevant to several entities of different types .
The problem is close to Relation Extraction in this sense .
The examples we are interested in are in the medical domain and deal with three main entity types : PERSON , DRUG , and DISEASE , where PERSON is restricted to known physicians .
While each of the entity types can be the target of a sentiment expression , the more interesting questions in this domain involve multiple entities , specifically , DRUG + DISEASE ( " how effective is this drug for this disease ? " ) , and PERSON + DRUG + DISEASE ( " what does this physician say about using this drug to cure this disease ? " ) .
We solve the multiple-entity relevance problem by intersecting the relevance ranges of different - type entities , thus reducing the problem to the single-entity relevance detection .
As such , the experiments regarding the multiple-entity relevance need only check the accuracy of this reduction .
In the medical domain , at least , this accuracy appears to be adequate .
Relevance Algorithms
Each algorithm receives , as input , the text of the document , with labeled reference of the target entity and other entities of the same type .
The labeled references also include all coreferential references , extracted automatically by an NLP system .
The input text also includes labeled candidate sentiment expressions , either manually labeled or automatically extracted by a relevance - ignoring SA system 1 .
The task of the algorithms is to label each candidate expression as relevant or irrelevant to the target entity .
The algorithms are evaluated according to the accuracy ( recall , precision , and F1 ) of this labeling of individual sentiment expressions .
This method produces a reasonably wellunderstandable quality measure ( the percentage of expressions that the algorithms get right or wrong ) , and also allows us to compare algorithms focused on individual expressions and algorithms working on text ranges .
The algorithms we evaluate are as follows : ? Baseline - Every expression is declared relevant .
This is the standard mode of operation of document- level SA tools , although it is usually only applied to the ' Target ' entitiesthe main topic ( s ) of the document .
?
Physical - proximity - based - A text - range focused algorithm , which labels pieces of text as relevant or irrelevant according to their placement relative to the references of the target entity and other entities of the same type , as well as some other contextual clues , such as paragraph boundaries .
Generally , the mentioning of an entity starts its relevance range ( and stops the relevance range of the previously mentioned entity ) .
For the first entity reference in a paragraph , the range also extends backward to the beginning of the sentence .
There are three flavors of the algorithm , specifically adapted for different document- types - with - respect - tothe - target-entity : o ' Proximity - Accidental ' - stops relevance ranges at paragraph boundaries , o ' Proximity - Targeted ' - restarts relevance ranges at paragraph boundaries ( every para - 1
In our experiments , we also use a standalone automatic Financial SA system from Feldman et al . ( 2010 ) , working in the ' ignore relevance ' mode , which ( 1 ) finds and labels all entities of the target type ( s ) ; ( 2 ) resolves all coreferences for the target entity type ( s ) ; ( 3 ) finds and labels all sentiment expressions , regardless of their relevance ; and ( 4 ) provides dependency parses for all sentences in the corpus .
graph is assumed relevant at the start , unless another entity is mentioned ) .
o ' Proximity - List ' - interpolates relevance ranges over intermission paragraphs , unless they are explicitly irrelevant ( e.g. , containing references of other entities of the same type ) .
? Syntactic-proximity -based - An expressionfocused algorithm , which labels expressions as relevant or irrelevant according to their distance to various entity references in the dependency parse graph .
There are two flavors of the algorithm : direct and reverse .
The former considers an expression relevant only if it is closest to the target entity from among all entities of the same type , and the distance is sufficiently close .
The latter considers an expression irrelevant only if it has the abovedescribed relation to some non-target entity of the same type .
The rationale for the two flavors is the distinction between ' Targeted ' and ' Accidental ' document types regarding the target entity .
For the ' Accidental ' entities , a sentiment expression is assumed to be relevant only if it is explicitly connected to the entity .
For ' Targeted ' entities , an expression is irrelevant only if it is explicitly connected to some other entity of the same type .
?
Classification - based - This algorithm considers each candidate sentiment expression as an instance of a binary classification problem , to be solved using supervised classification .
For evaluating this algorithm , some part of the test corpus is used for training , and the other for testing , with N-fold cross-validation .
The features for classification may use any information present in the input .
In the current experiments , we use references of target and non-target entities , appearances of paragraph and document boundaries , length of syntactic connections to target and non-target entities , when available , and explicit entity status within documents , when available .
The ( binary ) classification features are built from sequences of up to 5 occurrences of the above-described pieces , with the pieces appearing before and after the sentiment expression tracked separately .
For classification , we use a linear classifier with Large Margin training ( regularized perceptron , as discussed in Scheible and Sch?tze , ( 2013 ) ) . ?
Sequence-classification-based
- The algorithm uses exactly the same features as the direct classification - based above , but instead of considering each expression separately , it con-siders them as a sequence , one per document .
So , instead of a Large Margin binary classifier , a probabilistic sequence classifier is used ( CRF , as discussed in Lafferty et al . ( 2001 ) ) .
Experiments
For the experiments , we use two manuallyannotated corpora 2 , a financial corpus 3 and a medical 4 corpus .
In the Financial corpus , COM - PANIEs are used as target entities and in the medical corpus , DISEASEs , DRUGs and PER - SONs are the entity types that are used as target entities .
For the purpose of the experiments , we are interested only in single-entity sentiments about DRUGs , and multiple-entity sentiments about DRUGs + DISEASEs , and DRUGs + DISEASEs + PERSONs .
The evaluation metrics in all of the experiments are precision , recall , and F1 .
For the classification - based algorithms , unless stated otherwise , we use 10 - fold cross-validation .
Experiment : Importance of relevance
In the first experiment , we demonstrate the importance of using relevance when calculating the consolidated sentiment score of an entity within a set of documents .
For each entity , we set the ' correct ' consolidated sentiment score to the average of polarities of all sentiments in a corpus which are labeled as relevant to the entity .
Then , we compare the correct value to the two scores calculated without considering relevance : ? ' Baseline ' - the average of polarities of all sentiments in all documents where the entity is mentioned , and ?
' TargetedOnly ' - the average of polarities of all sentiments in the documents where the entity is labeled as target ( main topic of the document ) .
This case models the typical state of a relevance - agnostic SA system .
For this evaluation , we only compare the sign of the final sentiment scores , without considering their magnitudes ( unless it is close to zero , in 2 Fully annotating texts for semantic relevance is an arduous task , thus the used annotated corpora are relatively small .
Sample can be found at http://goo.gl/6HONHP.
3
A corpus of 160 financial news documents on at least one entity of interest , of average size ~ 5 Kb , downloaded from various financial news websites .
The dataset mentions 424 different companies .
4 A corpus of 160 documents , of average size ~ 7 Kb , downloaded following Google queries on a set of a few common drugs and diseases .
The dataset mentions 722 different people , 46 diseases , and 175 drugs .
which it is considered ' neutral ' ) .
The errors at this level indicate definite SA errorsmiscalculating entity 's sentiment into its opposite .
The results of the evaluation are as follows :
The ' Baseline ' scores show a large difference from the correct scores , with 33 % and 38 % of entities having wrong final polarity in the financial ( COMPANY ) and medical ( DRUG ) domains , respectively .
The ' TargetedOnly ' scores are somewhat closer to correct , with 12 % and 28 % of entities with incorrect final polarities .
However , the ' TargetedOnly ' method naturally suffers from a very low recall , with only 19 % and 38 % of entities covered in the financial and medical domains , respectively .
Experiment : Influence of entity status
In this experiment , we compare the performance of various algorithms while either providing or withholding the information about the documenttype - with - respect - to - the - target-entity .
The performance of the physical proximity algorithms on the financial corpus is shown at the top left hand side of Table 1 .
The set of all instances of relevance detection problems in the corpus ( an instance consists of a sentiment expression within a text , together with a target entity ) is divided into three subsets , according to the status of the target entity within the document .
As expected , the three flavors of the physical proximity algorithm perform much better on the corpus subsets they are adapted to .
At the bottom left hand side of Table 1 , we similarly show the performance of the two flavors of the syntaxproximity - based algorithm on the medical domain ( DRUG entities ) .
Same as above , there is a large difference in the performance of the two flavors of the algorithm on different subsets of the problem set .
Finally , at the top of Table 2 , we compare the performance of the two classification - based algorithms on the two ( whole ) problem sets , while either keeping or withholding the entity status information from the classifier .
The difference in results is less pronounced here , but is still noticeable .
The reason for the smaller difference , we hypothesize , is the ability of the classifiers to partially infer the entity status from the various context clues that are used as classification features ( see the experiment 5.3 ) .
Experiment : Automatic identification of entity status using classification .
In this experiment , we confirm that it is possible to identify the entity status within documents using supervised classification .
The results of direct evaluation show that the accuracies of the Medical and Financial corpora ( using 10 - fold X-validation ) are 87.8 % and 82.2 % respectively , and the accuracy when using the Medical corpus for training the Financial corpus for testing and vice versa , are 78.2 % and 86.1 % , respectively .
The results of relevance detection using the automatically extracted entity status values are shown at the right hand side of Table 1 and in the middle of Table 2 , which utilize the same datasets and algorithms as at the left hand side of Table 1 and at the top of Table 2 .
As can be seen from the tables , the drop in performance is small , demonstrating the success of classification - based extraction of entity status information .
Experiment : Cross-domain applicability
In this experiment , we test how well the classifiers trained on data from one domain work on input from a different domain .
The classification results using different types of training data are shown in Table 3 .
The table confirms general independence of the classification performance on the domain .
Comparing the 2 - fold and 10 - fold crossvalidation results ( the difference is equivalent to doubling the amount of training data ) , shows that the amount of training data is sufficient .
Experiment : Overall performance of algorithms
In this experiment , we simply compare the overall accuracy of various algorithms for relevance discernment , operating at their best parameters .
The results are shown at the bottom of Table 2 . Overall , classification - based algorithms perform better than the deterministic ones , with sequenceclassification performing significantly better than direct classification .
Syntactic proximity - based is precise , but has relatively low recall , reducing its overall performance .
Physical proximity - based is simplest , and produce reasonably high overall results , although worse than the best-performing classification - based methods .
Conclusion
The results are mostly intuitively understood and confirm the expectations .
We confirmed that relevance detection is essential for producing correct consolidated SA results .
We found that the entity status within the document is one of the important clues for solving the relevance detection problem , and showed that this information can be effectively automatically extracted using supervised classification .
We also compared several algorithms for relevance detection , with the results that classification - based algorithms generally outperform simpler ones based on the same clues , although a very simple proximity - based algorithm performs reasonably well if allowed to use the entity status information .
Table 1 . 1 Performance of different algorithms on three subsets of the corpus with a different status of the target entity within the document .
Experiment Algorithm Financial Medical Experiment 5.2 Classification ( with entity 90/86/88 84/88/86 ( Prec. / Rec , / F1 ) .
status info ) Classification ( without 89/85/87 87/81/84 entity status info ) Sequence Classification 96/84/90 99/84/91 ( with entity status info ) Sequence Classification 96/83/89 95/85/90 ( without entity status info ) Experiment 5.3 Classification 86.7 ( - 0.9 ) 83.9 ( - 2.0 ) ( F1 , ( diff. in F1 Sequence Classification 89.7 ( + 0.1 ) 90.9 ( - 0.3 ) from exp. 5.2 ) ) Experiment 5.5 Baseline 37.2 28.6 ( F1 ) Physical Proximity 84.1 79.5 Syntactic -Proximity 43.8 54.6 Classification 87.6 85.9 Sequence -Classification 91.2 89.6
