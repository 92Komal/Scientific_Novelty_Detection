title
An Investigation of Transfer Learning - Based Sentiment Analysis in Japanese
abstract
Text classification approaches have usually required task -specific model architectures and huge labeled datasets .
Recently , thanks to the rise of text - based transfer learning techniques , it is possible to pre-train a language model in an unsupervised manner and leverage them to perform effectively on downstream tasks .
In this work we focus on Japanese and show the potential use of transfer learning techniques in text classification .
Specifically , we perform binary and multi-class sentiment classification on the Rakuten product review and Yahoo movie review datasets .
We show that transfer learning - based approaches perform better than task -specific models trained on 3 times as much data .
Furthermore , these approaches perform just as well for language modeling pre-trained on 1 30 of Wikipedia .
We release our pre-trained models and code as open source .
Introduction Sentiment analysis is a well - studied task in the field of natural language processing and information retrieval ( Sadegh et al. , 2012 ; Hussein , 2018 ) .
In the past few years , researchers have made significant progress from models that make use of deep learning techniques .
( Kim , 2014 ; Lai et al. , 2015 ; Chen et al. , 2017 ; Lin et al. , 2017 ) .
However , while there has been significant progress in sentiment analysis for English , not much effort has been invested in analyzing Japanese due to its sparse nature and the dependency on large datasets required by deep learning .
Japanese script contains no whitespace , and sentences may be ambiguous such that there are multiple ways to split characters into words , each with a completely different meaning .
To see if existing research can make progress in Japanese , we make use of recent transfer learning models such as ELMo ( Peters et al. , 2018 ) , ULMFiT ( Howard and Ruder , 2018 ) , and BERT ( Devlin et al. , 2018 ) to each pretrain a language model which can then be used to perform downstream tasks .
We test the models on binary and multi-class classification .
The training process involves three stages as illustrated in Figure 1 .
The basic idea is similar to how fine-tuning ImageNet ( Deng et al. , 2009 ) helps many computer vision tasks ( Huh et al. , 2016 ) .
However , this model does not require labeled data for pre-training .
Instead , we pre-train a language model in unsupervised manner and then fine - tune it on a domain-specific dataset to efficiently classify using much less data .
This is highly desired since there is a lack of large labeled datasets in practice .
Contributions
The following are the primary contributions of this paper : ?
We experiment ELMo , ULMFiT and BERT on Japanese datasets including binary and 5 class datasets . ?
We do several ablation studies that are helpful for understanding the effectiveness of transfer learning in Japanese sentiment analysis . ?
We release our pre-trained models and code 12 1 base.exawizards.com 2 allennlp.org/elmo
Related Work
Here we briefly review the popular neural embeddings and classification model architectures .
Word Embeddings
Word embedding is defined as the representation of a word as a dense vector .
There have been many neural network implementations , including word2vec ( Mikolov et al. , 2013 ) , fasttext ( Joulin et al. , 2016 ) and Glove ( Pennington et al. , 2014 ) that embed using a single layer and achieve state - of - the - art performance in various NLP tasks .
However , these embeddings are not context-specific : in the phrases " I washed my dish " and " I ate my dish " , the word " dish " refers to different things but are still represented by the same embedding .
Contextualized Word Embeddings Instead of fixed vector embeddings , Cove ( Mc - Cann et al. , 2017 ) uses a machine translation model to embed each word within the context of its sentence .
The model includes a bidirectional LSTM encoder and a unidirectional LSTM decoder with attention , and only the encoder is used for downstream task -specific models .
However , pre-training is limited by the availability of parallel corpora .
( e.g. English - French ) ELMo , short for Embeddings from Language Model ( Peters et al. , 2018 ) overcomes this issue by taking advantage of large monolingual data in an unsupervised way .
The core foundation of ELMo is the bidirectional language model which learns to predict the probability of a target word given a sentence by combining forward and backward language models .
ELMo also requires task-specific models for downstream tasks .
Howard and Ruder ( 2018 ) proposed a singlemodel architecture , ULMFiT , that can be used in both pre-training and task -specific fine-tuning .
They use novel techniques such as discriminative fine-tuning and slanted triangular learning rates for stable fine-tuning .
OpenAI extended the idea by introducing GPT , a multi-layer transformer decoder ( Radford et al. , 2018 ) .
While ELMo uses shallow concatenation of forward and backward language models , ULMFiT and OpenAI GPT are unidirectional .
Devlin et al. argues that this limits the power of pre-trained representations by not incorporating bidirectional context , crucial for word - level tasks such as question answering .
They proposed a multi-layer transformer encoder- based model , BERT , trained on masked language modeling ( MLM ) and next sentence prediction ( NSP ) tasks .
MLM allows bidirectional training by randomly masking 15 % of words in each sentence in order to predict them , and NSP helps tasks such as question answering by predicting the order of two sentences .
Text Classification
Many models have been invented for English text classification , including KimCNN ( Kim , 2014 ) , LSTM ( Chen et al. , 2017 ) , Attention ( Chen et al. , 2017 ) , RCNN ( Lai et al. , 2015 ) , etc .
However , not much has been done for Japanese .
To the best of our knowledge , the current state - of- theart for Japanese text classification uses shallow ( context-free ) word embeddings for text classification ( Peinan and Mamoru , 2015 ; Nio and Murakami , 2018 ) .
Sun et al. ( 2018 ) proposed the Super Characters method that converts sentence classification into image classification by projecting text into images .
Zhang and LeCun ( 2017 ) did an extensive study of different ways of encoding Chinese / Japanese / Korean ( CJK ) and English languages , covering 14 datasets and 473 combinations of different encodings including one-hot , character glyphs , and embeddings and linear , fasttext and CNN models .
This paper investigates transfer learning - based methods for sentiment analysis that is comparable to above mentioned models including Zhang and LeCun ( 2017 ) and Sun et al . ( 2018 ) for the Japanese language .
Dataset
Our work is based on the Japanese Rakuten product review binary and 5 class datasets , provided in Zhang and LeCun ( 2017 ) and an Yahoo movie review dataset .
3 Table 1 provides a summary .
The Rakuten dataset is used for comparison purposes , while the Yahoo dataset is used for ablation studies due to its smaller size .
For the Rakuten dataset , 80 % is used for training , 20 % for validation , and the test set is taken from Zhang and LeCun ( 2017 ) ; for the Yahoo dataset , 60 % is used for training , 20 % for validation , and 20 % for testing .
We used the Japanese Wikipedia 4 for pre-training the language model for all models so that comparison would be fair .
Training
Pre-Training Language Model Pre-training a language model is the most expensive part but we train it only once and fine-tune on a target task .
We used 1 NVIDIA Quadro GV100 for training ULMFiT and 4 NVIDIA Tesla V100s for ELMo .
Text extraction done by WikiExtractor 5 , then tokenized by Mecab 6 with IPADIC neologism dictionary 7 .
We did n't use the BERT multilingual model 8 due to its incompatible treatment of Japanese : it does not account for okurigana ( verb conjugations ) and diacritic signs which completely change the represented word ( e.g. aisu " to love " vs. aizu " signal " ) .
910 Instead , we use the pre-trained BERT BASE model by Kikuta ( 2019 ) which has been trained for 1 million steps with sequence length of 128 and 400 thousand additional steps with sequence length of 512 .
It used the Sen-tencePiece subword tokenizer ( Kudo and Richardson , 2018 ) for tokenization .
The models trained with the most frequent 32000 tokens or subwords .
Fine-Tuning We
Results
In this section , we compare the results of ELMo + BCN , ULMFiT , and BERT with models reported in Zhang and LeCun ( 2017 ) and other previous state - of - the - art models we mentioned in 3.3 .
Note that none of the source LM is fine-tuned on a target dataset .
Results with these models finetuned on target corpora are included in Section 7.1 .
Rakuten Datasets
We trained ELMo +BCN and ULMFiT on the Rakuten datasets for 10 epochs each and selected the one that performed best .
Since BERT finetunes all of its layers , we only train for 3 epochs as suggested by Devlin et al . ( 2018 ) .
Results are presented in Table 2 .
All transfer learning - based methods outperform previous methods on both datasets , showing that these methods still work well without being fine -tuned on target corpora .
Yahoo movie review dataset The Yahoo dataset is approximately 112 times smaller than the Rakuten binary dataset .
We believe that this dataset better represents real life / practical situations .
For establishing a baseline , we trained a simple one - layer RNN and an LSTM with one linear layer on top for classification , as well as convolutional , self-attention , and hybrid state - of - the - art models we mentioned in Section 3.3 for comparison .
Results shown on Table 3 .
Similar to rakuten datasets , transfer-learning based methods works better .
Kim ( 2014 ) 14.25 Self Attention Lin et al. ( 2017 ) 13.16 RCNN Lai et al. ( 2015 ) 12.67 BCN+ELMo 10.24 ULMFiT 12.20 BERTBASE 8.42 7 Ablation Study
Domain Adaptation Pre-trained language models are usually trained with general corpuses such as Wikipedia .
However , the target domain corpus distribution is usually different ( movie or product review in our case ) .
Therefore , we fine- tune each source language model on the target corpus ( without labels ) for a few iterations before training each classifier .
The results in Table 4 shows that fine-tuning ULMFiT improves the performance on all datasets while ELMo and BERT shows varied results .
We believe that the huge performance improvement of ULMFiT is due to the discriminative fine-tuning and slanted triangular learning rates ( Howard and Ruder , 2018 ) that are used during the domain adaptation process .
Low - Shot Learning Low -shot learning refers to the practice of feeding a model with a small amount of training data , contrary to the normal practice of using a large amount of data .
We chose the Yahoo dataset for this experiment due to its small size .
Experimental results in Table 5 show that , with only 1 3 of the total dataset , ULMFiT and BERT perform better than task -specific models , while BCN +ELMo shows a comparable result .
Clearly , this shows that the models have learned significantly during the transfer learning process .
Model Yahoo Binary RNN Baseline 35.29 LSTM Baseline 32.41 KimCNN Kim ( 2014 ) 14.25 Self-Attention Lin et al. ( 2017 ) 13.16 RCNN Lai et al. ( 2015 ) 12.67 BCN +ELMo [ Table 6 : Comparison of results using large and small corpora .
The small corpus is uniformly sampled from the Japanese Wikipedia ( 100 MB ) .
The large corpus is the entire Japanese Wikipedia ( 2.9GB ) .
Size of Pre-Training Corpus
We also investigate whether the size of the source language model affects the sentiment analysis performance on the Yahoo dataset .
This is especially important for low-resource languages that do not usually have large amounts of data available for training .
We used the ja.text8 16 small text corpus ( 100 MB ) from the Japanese Wikipedia to compare with the whole Wikipedia ( 2.9GB ) used in our previous experiments .
Table 6 shows slightly lower performance for BCN +ELMo and ULMFiT while BERT performed much worse .
Thus , for effective sentiment analysis , a large corpus is required for pre-training BERT .
Parameter Updating Methods
In its original implementation , when BERT is finetuned , all of its layers are trained .
This is quite different from fine-tuning ELMo , where its layers are frozen and only task -specific models ( BCN in our case ) are updated .
We experiment with the opposite case for both models and list the results on will not be changed .
The hidden state associated to the first character of the input is pooled and provided to a linear layer that sits on top .
This way , BERT is computationally much cheaper and faster .
Result shows that using BERT as a feature extractor shows competitive performance .
?
Unfreezing ELMo Pre-trained ELMo weights are used for initialization as well ; however , weights are changed along with BCN layers .
This experiment allows us to compare the performance of freezing / unfreezing ELMo layers .
Table 7 shows that fine-tuning ELMo improves performance , comparable to BERT .
Conclusion
Our work showed the possibility of using transfer learning techniques for addressing sentiment classification for the Japanese language .
We draw following conclusions for future researchers in Japanese doing transfer learning for sentiment analysis task based on experiments we did in Rakuten product review and Yahoo movie review datasets :
1 . Adapting domain for BERT likely will not yield good results when the task is binary classification .
For all other cases , domain adaptation performs just as well or better .
2 . ELMo and ULMFiT perform well even when trained on a small subset of the language model .
3 . Fune-tuning both ELMo and BCN layers on a target task improves the performance .
