title
Sentiment and Emotion help Sarcasm ?
A Multi-task Learning Framework for Multi-Modal Sarcasm , Sentiment and Emotion Analysis
abstract
In this paper , we hypothesize that sarcasm is closely related to sentiment and emotion , and thereby propose a multi-task deep learning framework to solve all these three problems simultaneously in a multi-modal conversational scenario .
We , at first , manually annotate the recently released multi-modal MUStARD sarcasm dataset with sentiment and emotion classes , both implicit and explicit .
For multitasking , we propose two attention mechanisms , viz .
Inter-segment Inter-modal Attention ( I e - Attention ) and Intra-segment Inter-modal Attention ( I a - Attention ) .
The main motivation of I e - Attention is to learn the relationship between the different segments of the sentence across the modalities .
In contrast , I a - Attention focuses within the same segment of the sentence across the modalities .
Finally , representations from both the attentions are concatenated and shared across the five classes ( i.e. , sarcasm , implicit sentiment , explicit sentiment , implicit emotion , explicit emotion ) for multi-tasking .
Experimental results on the extended version of the MUStARD dataset show the efficacy of our proposed approach for sarcasm detection over the existing state - of - theart systems .
The evaluation also shows that the proposed multi-task framework yields better performance for the primary task , i.e. , sarcasm detection , with the help of two secondary tasks , emotion and sentiment analysis .
Introduction Sarcasm is an essential aspect of daily conversation , and it adds more fun to the language .
Oscar Wilde , an Irish poet- playwright , quotes , " Sarcasm is the lowest form of wit , but the highest form of intelligence " .
Irrespective of its relation with intelligence , sarcasm is often challenging to understand .
Sarcasm is often used to convey thinly veiled disapproval humorously .
This can be easily depicted through the following example , " This is so good , that I am gonna enjoy it in the balcony .
I can enjoy my view , whilst I enjoy my dessert . "
This utterance , at an outer glance , conveys that the speaker is extremely pleased with his dessert and wants to elevate the experience by enjoying it in the balcony .
But , careful observation of the sentiment and emotion of the speaker helps us understand that the speaker is disgusted with the dessert and has a negative sentiment during the utterance ( c.f. Figure 1 ) .
This is where sentiment and emotion come into the picture .
Sentiment , emotion and sarcasm are highly intertwined , and one helps in the understanding of the others better .
Even though sentiment , emotion , and sarcasm are related , sarcasm was treated separately from its other counterparts in the past due to its complexity and its high dependency on the context .
Moreover , multi-modal input helps the model to understand the intent and the sentiment of the speaker with more certainty .
Thus in the context of a dialogue , multi-modal data such as video ( acoustic + visual ) along with text helps to understand the sentiment and emotion of the speaker , and in turn , helps to detect sarcasm in the conversation .
In this paper , we exploit these relationships , and make use of sentiment and emotion of the speaker for predicting sarcasm , specifically for the task , in a multi-modal conversational context .
The main contributions and / or attributes of our proposed research are as follows : ( a ) .
we propose a multi-task learning framework for multi-modal sarcasm , sentiment , and emotion analysis .
We leverage the utility of sentiment and emotion of the speaker to predict sarcasm .
In our multi-task framework , sarcasm is treated as the primary task , whereas emotion analysis and sentiment analysis are considered as the secondary tasks . ( b ) .
We also propose two attention mechanisms viz .
I e - Attention and I a - Attention to better combine the information across the modalities to effectively classify sarcasm , sentiment , and emotion . ( c ) .
We annotate the recently released Sarcasm dataset , MUStARD with sentiment and emotion classes ( both implicit and explicit ) , and ( d ) .
We present the state- of - the - art for sarcasm prediction in multi-modal scenario .
Related Work
A survey of the literature suggests that a multimodal approach towards sarcasm detection is a fairly new approach rather than a text - based classification .
Traditionally , rule- based classification ( Joshi et al. , 2017 ; Veale and Hao , 2010 ) approaches were used for sarcasm detection .
Poria et al. ( 2016 ) have exploited sentiment and emotion features extracted from the pre-trained models for sentiment , emotion , and personality on a text corpus , and use them to predict sarcasm through a Convolutional Neural Network .
In recent times , the use of multi-modal sources of information has gained significant attention to the researchers for affective computing .
Mai et al. ( 2019 ) proposed a new two -level strategy ( Divide , Conquer , and Combine ) for feature fusion through a Hierarchical Feature Fusion Network for multimodal affective computing .
exploits the interaction between a pair of modalities through an application of Inter-modal Interaction Module ( IIM ) that closely follows the concepts of an auto-encoder for the multi-modal sentiment and emotion analysis .
Ghosal et al. ( 2018 ) proposed a contextual inter-modal attention based framework for multi-modal sentiment classification .
In other work , an attention - based multitask learning framework has been introduced for sentiment and emotion recognition .
Although multi-modal sources of information ( e.g. , audio , visual , along with text ) offers more evidence in detecting sarcasm , this has not been attempted much , one of the main reasons being the non-availability of multi-modal datasets .
Recently , researchers ( Castro et al. , 2019 ) have started exploiting multi-modal sources of information for sarcasm detection .
It is true that the modalities like acoustic and visual often provide more evidences about the context of the utterance in comparison to text .
For sarcasm detection , the very first multimodal dataset named as MUStARD has been very recently released by Castro et al . ( 2019 ) , where the authors used a Support Vector Machine ( SVM ) classifier for sarcasm detection .
In our current work , we at first extend the MUS - tARD dataset ( Castro et al. , 2019 ) by manually labeling each utterance with sentiment and emotion labels .
Thereafter , we propose a deep learning based approach along with two attention mechanisms ( I e - Attention and I a - Attention ) to leverage the sentiment and emotion for predicting sarcasm in a multi-modal multi-task framework .
Further , to the best of our knowledge , this is the very first attempt at solving the multi-modal sarcasm detection problem in a deep multi-task framework .
We demonstrate through a detailed empirical evaluation that sarcasm detection can be improved significantly if we are successful in leveraging the knowledge of emotion and sentiment using an effective multi-task framework .
Dataset
The MUStARD ( Castro et al. , 2019 ) dataset consists of conversational audio-visual utterances ( total of 3.68 hours in length ) .
This dataset consists of 690 samples , and each sample consists of utterance accompanied by its context and sarcasm label .
The samples were collected from 4 popular TV Series viz. , Friends , The Big Bang Theory , The Golden Girls , and Sarcasmaholics Anonymous and manually annotated for the sarcasm label .
The dataset is balanced with an equal number of samples for both sarcastic and non-sarcastic labels .
The utterance in each sample consists of a single sentence , while the context associated with it consists of multiple sentences that precede the corresponding utterance in the dialogue .
We manually re-annotated this dataset to introduce sentiment and emotion labels in addition to sarcasm .
We define two kinds of emotion and sentiment values viz. , implicit and explicit , which are discussed in the following subsections .
Sentiment
For sentiment annotation of an utterance , we consider both implicit and explicit affect information .
The implicit sentiment of an utterance is determined with the help of context .
Whereas , explicit sentiment of an utterance is determined directly from itself , and no external knowledge from the context is required to infer it .
We consider three sentiment classes , namely positive , negative and neutral .
For the example in Figure 1 , the implicit sentiment would be Negative , whereas explicit sentiment is Positive .
Table 1 shows the overall ratio of implicit and explicit sentiment labels , respectively .
Whereas , Figure 2a and Figure 2b depict the show-wise ratio and distribution of each label .
Emotion
Like sentiment , we annotate each sentence on the context and utterance for the implicit and explicit emotion .
We annotate the dataset for 9 emotion values , viz . anger ( An ) , excited ( Ex ) , fear ( Fr ) , sad ( Sd ) , surprised ( Sp ) , frustrated ( Fs ) , happy ( Hp ) , neutral ( Neu ) and disgust ( Dg ) .
Each utterance and context sentence are annotated , and these can have multiple labels per sentence for both implicit and explicit emotion .
In the example of Figure 1 , the implicit emotion of the speaker would be disgust while the explicit emotion is happy .
Table 2 shows the overall ratio of implicit and explicit emotion labels , respectively .
Whereas Figure 3a and Figure 3 b depict the show-wise ratio and distribution of each label .
Annotation Guidelines
We annotate all the samples with four labels ( implicit sentiment / emotion and explicit sentiment / emotion ) .
We employ three graduate students highly proficient in the English language with prior experience in labeling sentiment , emotion , and sarcasm .
The guidelines for annotation , along with some examples , were explained to the annotators before starting the annotation process .
The annotators were asked to annotate every utterance with as many emotions present in the utterance as possible , along with the sentiment .
Initially , the dataset was annotated for explicit labels , with only the utterances provided to the annotators .
Later , for the implicit labels , we also made the corresponding context video available to provide the relevant information for each sample .
This method helps the annotators to resolve the ambiguity between the implicit and explicit labels .
A majority voting scheme was used for selecting the final emotion and sentiment .
We achieve an overall Fleiss ' ( Fleiss , 1971 ) kappa score of 0.81 , which is considered to be reliable .
Proposed Methodology
In this section , we describe our proposed methodology , where we aim to leverage the multi-modal sentiment and emotion information for solving the problem of multi-modal sarcasm detection in a multi-task framework .
We propose a segment - wise inter-modal attention based framework for our task .
We depict the overall architecture in Figure 4 .
The extended dataset with annotation guidelines and source code are available at http://www.iitp.ac. in / ?ai-nlp-ml/resources.html .
Each sample in the dataset consists of an utterance ( u ) accompanied by its context ( c ) and labels ( sarcasm , implicit sentiment , explicit sentiment , implicit emotion , and explicit emotion ) .
The context associated with the utterance consists of multiple sentences ( say , N ) that precede the corresponding utterance in the dialogue .
Each utterance and its ' context is associated with its ' speaker i.e. , speaker of utterance ( SP u ) and speaker of context ( SP c ) , respectively .
We represent SP u and SP c by using a one-hot vector embedding .
We divide our proposed methodology into three subsections i.e. , Input Layer , Attention Mechanism and Output Layer , which are described below :
Input Layer
The proposed model takes multi-modal inputs i.e. , text ( T ) , acoustic ( A ) , and visual ( V ) .
We describe the utterance and its ' context for all the modalities below :
Text
Utterance : Let us assume , in an utterance , there n t number of words w 1:nt = w 1 , ... , w nt , where w j ?
R dt , d t = 300 , and w j s are obtained using fastText word embeddings ( Joulin et al. , 2016 ) .
The utterance is then passed through a bidirectional Gated Recurrent Unit ( Cho et al. , 2014 ) ( BiGRU T 1 ) to learn the contextual relationship between the words .
We apply the attention over the output of BiGRU T to extract the important contributing words w.r.t. sarcasm .
Finally , we apply BiGRU F 2 to extract the sentence level features .
We then concatenate the speaker information of the utterance with the output of BiGRU F .
This is denoted by T u +SP u , where T u denotes the utterance for the text modality and SP u denotes the speaker for that particular utterance .
Context :
There are N c number of sentences in the context where each sentence has n tc words .
For each sentence , words are passed through BiGRU F to learn the contextual relationship between the words , and to obtain the sentence - wise representation .
Then , we apply self-attention over the output of BiGRU F to extract the important contributing sentences for the utterance .
Finally , we concatenate the speaker information with each sentence and pass through the BiGRU F to obtain the T c + SP c , where T c denotes the context of the text modality , and SP c denotes the speaker of that context .
Visual Utterance : Let us assume there are n v number of visual frames w.r.t. an utterance .
We take the average of all frames to extract the sentence level information for the visual modality ( Castro et al. , 2019 ) , and concatenate this with the speaker information .
This is denoted as V u + SP u , where V u ?
R dv and d v = 2048 .
Context : Given n vc number of visual frames w.r.t. all the sentences , we take the average of all the visual frames ( Castro et al. , 2019 ) to extract the context level information , and denote this as V c .
As sentence - wise visual frames are not provided in the dataset , speaker information is not considered .
Acoustic Utterance : Given n a number of frames for the acoustic w.r.t. an utterance , we take the average of all the frames to extract the sentence level in-formation ( Castro et al. , 2019 ) , and concatenate with the speaker of the utterance .
We denote this as A u + SP u , where A u ?
R da and d a = 283 corresponds to the utterance of the acoustic modality .
Context :
For text , we concatenate the utterance ( T u + SP u ) with its context ( T c + SP c ) .
For visual , we concatenate the utterance ( V u + SP u ) with its context ( V c ) while for acoustic , we consider only the utterance A u + SP u ( c.f. Figure 4 ) .
We do not consider any context information of the acoustics as it often contains information of many speakers , background noise , and noise due to laughter cues ( which is not a part of the conversation ) .
Hence , it might be difficult to disambiguate this with the laughter part of the conversation .
Whereas , in the case of visual modality , it majorly contains the image of the speaker along with sentiment and emotion information .
Thus , visual will not have a similar kind of problem as acoustic .
It is also to be noted that for a fair comparison with the state - of - the - art system ( Castro et al. , 2019 ) , we take the average of the acoustic and visual features across the sentences .
Attention Mechanism
In any multi-modal information analysis , it is crucial to identify the important feature segments from each modality , so that when these are combined together can improve the overall performance .
Here , we propose two attention mechanisms : ( i ) .
Inter-segment Inter-modal Attention ( I e - Attention ) , and ( ii ) .
Intra-segment Inter-modal Attention ( I a - Attention ) .
First , we pass the input representation from all the three modalities through a fully - connected layer ( Dense d ) to obtain the feature vector of length ( d ) .
These feature vectors are then forwarded to the aforementioned attention mechanisms .
Inter-segment Inter-modal Attention
For each modality , we first split the feature vector into k-segments to extract the fine level information .
We aim to learn the relationship between the feature vector of a segment of an utterance in one modality and feature vector of the another segment of the same utterance in another modality through this mechanism ( c.f. Figure 5 ) .
Then , an I e - Attention is applied among the segments for every possible pair of modalities viz. , TV , VT , TA , AT , AV , and VA .
The overall procedure of I e - Attention is depicted in Algorithm 1 . procedure I e - ATTENTION (X , Y ) for s ? 1 , ... , d/ k do s = segment S x [ g ] = X [ k * s , k * s + k]
X ? R sk S y [ g ] = Y [ k * s , k * s + k]
Y ?
R sk return ATTENTION ( S x , S y ) procedure I a - ATTENTION (X , Y , Z ) R = concatenate ( X , Y , Z ) for s ? 1 , ... , d/ k do s = segment S r [ s ] = R [ k * s , k * s + k]
X ?
R sk return ATTENTION ( S r , S r ) procedure ATTENTION ( B , C ) /*Cross-Segment Correlation * / M ? B.B T /*Cross- Segment Inter-modal Attention * / for i , j ? 1 , ... , L do L = length ( M ) P ( i , j ) ? e M ( i , j )
Intra-segment Inter-modal Attention
For each utterance , we first concatenate the feature vectors ( i.e. , ? R d ) obtained from the three modalities i.e. , ? R 3?d ( c.f. Figure 6 ) and then split the feature vector into k-segments ( i.e. , ? R 3 ? d k ) .
Now , we have a mixed representation of all the modalities , i.e. visual , audio and text .
The aim is , for a specific segment of any particular utterance , to establish the relationship between the feature vectors obtained from the different modalities .
Output Layer Motivated by the residual skip connection ( He et al. , 2016 ) , the outputs of I e - Attention and I a - Attention along with the representations of individual modalities are concatenated ( c.f Figure 4 ) .
Finally , the concatenated representation is shared across the five branches of our proposed network ( i.e. , sarcasm , I-sentiment , E-sentiment , I-emotion , & Eemotion ) corresponding to three tasks , classification for the prediction ( one for each task in the multi-task framework ) .
Sarcasm and sentiment branches contain a Softmax layer for the final classification , while the emotion branch contains a Sigmoid layer for the classification .
The shared representation will receive gradients of error from the five branches ( sarcasm , I-sentiment , E-sentiment , I-emotion , & E-emotion ) , and accordingly adjusts the weights of the models .
Thus , the shared representations will not be biased to any particular task , and it will assist the model in achieving better generalization for the multiple tasks .
Experiments and Analysis
We divide the whole process into four categories : i ) .
utterance without context without speaker ( i.e. , we do not use the information of context and its ' speaker with utterance ) ; ii ) .
utterance with context without speaker ( i.e. , we use the context information with utterance but not speaker information ) ; iii ) .
utterance without context with speaker ( i.e. , we use the speaker information with utterance but not context information ) ; and iv ) .
utterance with context with speaker ( i.e. , we use the context and its ' speaker information with utterance ) .
Experimental Setup
We perform all the experiments for the setup utterances without context and speaker information ( case i ) .
Hence , even though the sentiment and emotion labels were annotated for both the context and utterance , we use the labels associated with utterances only for our experiments .
Our experimental setup is mainly divided into two main parts ( Castro et al. , 2019 ) : ? Speaker Independent Setup :
In this experiment , samples from The Big Bang Theory , The Golden Girls , and Sarcasmaholics Anonymous were considered for the training , and samples from the Friends Series were considered as the test set .
Following this step , we were able to reduce the effect of the speaker in the model .
?
Speaker Dependent Setup :
This setup corresponds to the five-fold cross-validation experiments , where each fold contains samples taken randomly in a stratified manner from all the series .
We evaluate our proposed model on the multimodal sarcasm dataset 3 , which we extended by incorporating both emotion and sentiment values .
We perform grid search to find the optimal hyperparameters ( c.f. Table 3 ) .
Though we aim for a generic hyper-parameter configuration for all the experiments , in some cases , a different choice of the parameter has a significant effect .
Therefore , we choose different parameters for a different set of experiments .
We implement our proposed model on the Python- based Keras deep learning library .
As the evaluation metric , we employ precision ( P ) , recall ( R ) , and F1 -score ( F1 ) for sarcasm detection .
We use Adam as an optimizer , Softmax as a classifier for sarcasm and sentiment classification , and the categorical cross-entropy as a loss function .
For emotion recognition , we use Sigmoid as an activation function and optimize the binary cross-entropy as the loss .
Results and Analysis
We evaluate our proposed architecture with all the possible input combinations i.e. bi-modal ( T+V , T +A , A+V ) and tri-modal ( T+ V + A ) .
We do not consider uni-modal inputs ( T , A , V ) because our proposed attention mechanism requires at least two modalities .
We show the obtained results in Table 4 , that outlines the comparison between the multi-task ( MTL ) and single - task ( STL ) learning frameworks without taking context and speaker information into consideration .
We observe that Tri-modal ( T+ A +V ) shows better performance over the bi-modal setups .
T + V T + A A + V T + A + V Labels P R F1 P R F1 P R F1 P R F1 For STL , experiments with only sarcasm class are used , whereas for MTL , we use three sets of experiments , i.e. sarcasm with sentiment ( Sar + Sent ) , sarcasm with emotion ( Sar + Emo ) , and sarcasm with sentiment and emotion ( Sar + Sent + Emo ) .
For sarcasm classification , we observe that multitask learning with sentiment and emotion together shows better performance for both the setups ( i.e. speaker dependent and speaker independent ) over the single - task learning framework .
It is evident from the empirical evaluation , that both sentiment and emotion assist sarcasm through the sharing of knowledge , and hence MTL framework yields better prediction compared to the STL framework ( c.f. Table 4 ) .
We also show the results for the single- task ( T+ A +V ) experiments under speaker - dependent and speaker - independent setups for sentiment and emotion .
These results can be considered as baseline for the same .
The detailed description of sentiment and emotion are described in Section 3.1 and Section 3.2 , respectively .
For Sentiment Analysis , the results are shown in Table 5 .
Similarly , for emotion analysis , the results are shown in Table 6 .
Along with it , results from the single - Task experiments for each emotion under implicit emotion and explicit emotion for Speaker Dependent and Speaker Independent setups are shown in Table 7 and Table 8 , respectively .
As each utterance can have multiple emotion labels , we take all the emotions whose respective values are above a threshold .
We optimize and cross-validate the evaluation metrics and set the threshold as 0.5 0.45 for speaker - dependent and speaker - independent setups , respectively .
We further evaluate our proposed model by incorporating context and speaker information to form the three combinations of experiments viz .
With Context Without Speaker , Without Context With Speaker , With Context and Speaker ( c.f. Table 9 ) .
The experiments without context and without speaker information are same as the tri-modal setup in Table 4 .
The maximum improvement ( 1 - 5 % ? ) in performance is observed when the speaker information alone is incorporated in the tri-modal setup .
Whereas in Speaker Independent Setup , incorporating both context and speaker information significantly improves the performance ( 1 - 5 % ? ) .
To understand the contribution of I e - Attention and I a - Attention towards the performance of the model , an ablation study was performed without the attention - mechanisms ( c.f.
Comparative Analysis
We compare , under the similar experimental setups , the results obtained in our proposed model ( without context and speaker ) against the existing models called as baseline ( Castro et al. , 2019 ) , which also made use of the same dataset .
The comparative analysis is shown in Table 11 .
For tri-modal experiments , our proposed multi-modal multi-task framework achieves the best precision of 73.40 % ( 1.5 % ? ) , recall of 72.75 % ( 1.4 % ? ) and F1 -score of 72.57 % ( 1.1 % ? ) for the proposed multi-task model ( Sar + Sent + Emo ) as compared to precision of 71.9 % , recall of 71.4 % , F1 - score of 71.5 % of the state - of - the - art system .
We observe that both sentiment and emotion help in improving the efficiency of sarcasm detection .
Similarly , for the Speaker Independent setup , we obtain an improvement of 5.2 % in precision , 3.4 % in recall , and 3.1 % in F1 - score .
We perform statistical significance test ( paired T-test ) on the obtained results and observe that performance improvement in the proposed model over the state - of - the - art is significant with 95 % confidence ( i.e. p-value < 0.05 ) .
Error Analysis
We analyze the attention weights to understand the learning behavior of the proposed framework .
We take an utterance i.e. , " I love that you take pride in your looks , even when I have to pee in the morning , and you 're in there spending an hour on your hair . "
( c.f Table 12 ) from the dataset which is a sarcastic utterance .
The MTL ( Sar + Sent + Emo ) correctly classifies this utterance as sarcastic , while the STL ( Sar ) predicts it as non-sarcastic .
In this utterance , we feel that the speaker is pleased and happy ( explicit emotion ) where he is angry ( implicit emotion ) on the other person and is expressing that anger sarcastically .
We analyze the heatmaps of the attention weights ( I e - Attention and I a - Attention ) for the above utterance .
Each cell of heatmaps for I e - Attention ( c.f. Figure 7 ) represents the different segments of the sentence across the modalities .
Cell ( i , j ) of the heatmap for the modalities ( say , TV ) represents the influence of s j of visual on s i of textual modality , in predicting the output ( where s i represents i th segment of the feature vector from the respective modality ) .
In Figure 7a , for the first segment of the utterance ( i.e. , s 1 ) of the textual modality , the model puts more attention weights to the different segments of the utterance ( i.e. , s 6 , s 7 , s 9 , and s 10 ) of visual modality to classify the signifies the influence of s j on s i in predicting the output ( where s i represents i th segment of the concatenated feature vector from all modalities ) .
We observe that for a particular segment of the utterance ( say s 6 ) , the model puts more weights to itself rather than the others .
We also observe that in the bi-modal ( T+A ) experiment ( c.f. Table 4 ) our model does not perform at par when we attempt to solve all the three tasks , i.e. sarcasm , sentiment , and emotion together .
This may be attributed to the reason of not incorporating the visual information that contains rich affect cues in the forms of sentiment and emotion .
Hence , the introduction of sentiment in the T+A setting might be confusing the model .
Conclusion
In this paper , we have proposed an effective deep learning - based multi-task model to simultaneously solve all the three problems , viz .
sentiment analysis , emotion analysis and sarcasm detection .
As there was no suitable labeled data available for this problem , we have created the dataset by manually annotating an existing dataset of sarcasm with sentiment and emotion labels .
we have introduced two attention mechanisms ( i.e. , I e - Attention and I a - Attention ) , and incorporated the significance of context and speaker information w.r.t. sarcasm .
Empirical evaluation results on the extended version of the MUStARD dataset suggests the efficacy of the proposed model for sarcasm analysis over the existing state - of - the - art systems .
The evaluation also showed that the proposed multi-tasking framework achieves better performance for the primary task , i.e. sarcasm detection , with the help of emotion analysis and sentiment analysis , the two secondary tasks in our setting .
During our analysis , we found that the dataset is not big enough for a complex framework to learn from .
Along with investigating new techniques , we hope that assembling a bigger curated dataset with quality annotations will help in better performance . "
This is so good , that I am gonna enjoy it in the balcony .
I can enjoy my view , whilst I enjoy my desert . "
