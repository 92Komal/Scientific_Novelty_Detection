title
uOttawa : System description for SemEval 2013 Task 2 Sentiment Analysis in Twitter
abstract
We present two systems developed at the University of Ottawa for the SemEval 2013 Task 2 .
The first system ( for Task A ) classifies the polarity / sentiment orientation of one target word in a Twitter message .
The second system ( for Task B ) classifies the polarity of whole Twitter messages .
Our two systems are very simple , based on supervised classifiers with bag-ofwords feature representation , enriched with information from several sources .
We present a few additional results , besides results of the submitted runs .
Introduction
The Semeval 2013
Task 2 focused on classifying Twitter messages ( " tweets " ) as expressing a positive opinion , a negative opinion , a neutral opinion , or no opinion ( objective ) .
In fact , the neutral and objective were joined in one class for the requirements of the shared task .
Task
A contained target words whose sense had to be classified in the context , while Task B was to classify each text into one of the three classes : positive , negative , and neutral / objective .
The training data that was made available for each task consisted in annotated Twitter message .
There were two test sets for each task , one composed of Twitter messages and one of SMS message ( even if there was no specific training data for SMS messages ) .
See more details about the datasets in ( Wilson et al. , 2013 ) .
System Description
We used supervised learning classifiers from Weka ( Witten and Frank , 2005 ) .
Initially we extracted simple bag-of- word features ( BOW ) .
For the submitted systems , we also used features calculated based on SentiWordNet information ( Baccianella et al. , 2010 ) .
SentiWordNet contains positivity , negativity , and objectivity scores for each sense of a word .
We explain below how this information was used for each task .
As classifiers , we used Support Vector Machines ( SVM ) ( SMO and libSVM from Weka with default values for parameters ) , because SVM is known to perform well on many tasks , and Multinomial Naive Bayes ( MNB ) , because MNB is known to perform well on text data and it is faster than SVM .
Task A
Our system for Task A involved two parts : the expansion of our training data and the classification .
The expansion was done with information from SentiWordNet .
Stop words and words that appeared only once in the training data were filtered out .
Then the classification was completed with algorithms from Weka .
As mentioned , the first task was to expand all of the tweets that were provided as training data .
This was doing using Python and the Python NLTK library , as well as SentiWordNet .
SentiWordNet provides a score of the sentient state for each word ( for each sense , in case the word has more than one sense ) .
As an example , the word " want " can mean " a state of extreme poverty " with the Senti-WordNet score of ( Positive : 0 Objective : 0.75 Negative : 0.25 ) .
The same word could also mean " a specific feeling of desire " with a score of ( Positive : 0.5 Objective : 0.5 Negative : 0 ) .
We also used for expansion the definitions and synonyms of each word sense , from WordNet .
The tweets in the training data are labeled with their sentiment type ( Positive , Negative , Objective and Neutral ) .
Neutral and Objective are treated the same .
The provided training data has the target word marked , and also the sentiment orientation of the word in the context of the tweeter message .
These target words were the ones expanded by our method .
When the target was a multi-word expression , if the expression was found in WordNet , then the expansion was done directly ; if not , each word was expanded in a similar fashion and concatenated to the original tweet .
These target words were looked up in SentiWordNet and matched with the definition that had the highest score that also matched their sentiment label in the training data .
Original Tweet
The great Noel Gallagher is about to hit the stage in St. Paul .
Plenty of room here so we 're 4th row center .
Plenty of room .
Pretty fired up Key Words Great Sentiment Positive Definition very good ; " he did a bully job " ; " a neat sports car " ; " had a great time at the party " ; " you look simply smashing "
Synonyms Swell , smashing , slap- up , peachy , not_bad , nifty , neat , keen , groovy , dandy , cracking , corking , bully , bang-up
Expanded Tweet
The great Noel Gallagher is about to hit the stage in St. Paul .
Plenty of room here so were 4th row center .
Plenty of room .
Pretty fired up swell smashing slap - up peachy not_bad nifty neat keen groovy dandy cracking corking bully bang - up very good he did a bully job a neat sports car had a great time at the party you look simply smashing
Table 1 : Example of tweet expansion for Task A
The target word 's definition and synonyms were then concatenated to the original tweet .
No additional changes were made to either the original tweet or the features that were added from SentiWordNet .
An example follows in Table 1 .
The test data ( Twitter and SMS ) was not expanded , because there are no labels in the test data to be able to choose the sense with corresponding sentiment .
Task B
For this task , we used the following resources : SentiwordNet ( Baccianella et al , 2010 ) , the Polarity Lexicon ( Wilson et al. , 2005 ) , the General Inquirer ( Stone et al. , 1966 ) , and the Stanford NLP tools ( Toutanova et al. , 2003 ) for preprocessing and feature selection .
The preprocessing of Twitter messages is implemented in three steps namely , stop-word removal , stemming , and removal of words with occurrence frequency of one .
Several extra features will be used : the number of positive words and negative words identified by three lexical resources mentioned above , the number of emoticons , the number of elongated words , and the number of punctuation tokens ( single or repeated exclamation marks , etc. ) .
As for SentiWordNet , for each word a score is calculated that shows the positive or negative weight of that word .
No sense disambiguation is done ( the first sense is used ) , but the scores are used for the right part- of-speech ( in case a word has more than one possible part-ofspeech ) .
Part- of - Speech tagging was done with the Stanford NLP Tools .
As for General Inquirer and Polarity Lexicon , we simply used the list positive and negative words from these resources in order to count how many positive and how many negative terms appear in a message .
Results
Task A
For classification , we first trained on our expanded training data using 10 - fold cross-validation and using the SVM ( libSVM ) and Multinomial Na-iveBayes classifiers from Weka , using their default settings .
The training data was represented as a bag of words ( BOW ) .
These classifiers were chosen as they have given us good results in the past for text classification .
The classifiers were run with 10 - fold cross-validation .
See Table 2 for the results .
Without expanding the tweets , the accuracy of the SVM classifier was equal to the baseline of classifying everything into the most frequent class , which was " positive " in the training data .
For MNB , the results were lower than the baseline .
After expanding the tweets , the accuracy increased to 73 % for SVM and to 80.36 % for MNB .
We concluded that MNB works better for Task A .
This is why the submitted runs used the MNB model that was created from the expanded training data .
Then we used this to classify the Twitter and SMS test data .
The average F-score for the positive and the negative class for our submitted runs can be seen in Table 3 , compared to the other systems that participated in the task .
We report this measure because it was the official evaluation measure used in the task .
The precision , recall and F-score on the Twitter and SMS test data for our submitted runs can be seen in Tables 4 and 5 , respectively .
All our submitted runs were for the " constrained " task ; no additional training data was used .
System
Class
Task B First we present results on the training data ( 10 fold cross-validation ) , then we present the results for the submitted runs ( also without any additional training data ) .
Table 6 shows the overall accuracy for BOW features for two classifiers , evaluated based on 10 fold cross validation on the training data , for two classifiers : SVM ( SMO in Weka ) and Multidimensional Na?ve Bays ( MNB in Weka ) .
The BOW plus SentiWordNet features also include the number of positive and negative words identified from SentiWordNet .
The BOW plus extra features representation includes the number of positive and negative words identified from SentiWordNet , General Inquirer , and Polarity Lexicon ( six extra features ) .
The last row of the table shows the overall accuracy for BOW features plus all the extra features mentioned in Section 2.2 , including information extracted from SentiWordNet , Polarity Lexicon , and General Inquirer .
We can see that the SentiWordNet features help , and that when including all the extra features , the results improve even more .
We noticed that the features from the Polarity Lexicon contributed the most .
When we removed GI , the accuracy did not change much ; we believe this is because GI has too small coverage .
The baseline in Table 6 is the accuracy of a trivial classifier that puts everything in the most frequent class , which is neutral / objective for the training data ( ZeroR classifier in Weka ) .
System
The results of the submitted runs are in Table 7 for the two data sets .
The features representation was BOW plus SentiWordNet information .
The official evaluation measure is reported ( average Fscore for the positive and negative class ) .
The detailed results for each class are presented in Tables 8 and 9 . In Table 7 , we added an extra row for a new uOttawa system ( SVM with BOW plus extra features ) that uses the best classifier that we designed ( as chosen based on the experiments on the training data , see
Conclusions and Future Work In Task A , we expanded upon the Twitter messages from the training data using their keyword 's definition and synonyms from SentiWordNet .
We showed that the expansion helped improve the classification performance .
In future work , we would like to try an SVM using asymmetric softboundaries to try and penalize the classifier for missing items in the neutral class , the class with the least items in the Task A training data .
The overall accuracy of the classifiers for Task B increased a lot when we introduced the extra features discussed in section 2.2 .
The overall accuracy of SVM increased from 58.75 % to 82.42 % ( as measures by cross-validation on the training data ) .
When applying this classifier on the two test data sets , the results were very surprisingly good ( even higher that the best system submitted by the SemEval participants for Task B 1 ) .
Table 2 : 2 Accuracy results for task A by 10 - fold crossvalidation on the training data System Tweets SMS uOttawa system 0.6020 0.5589 Median system 0.7489 0.7283
Best system 0.8893 0.8837
Table 3 : 3 Results for Task A for the submitted runs ( Average F-score for positive / negative class )
Table 4 : 4 Results for Tweet test data for Task A , for each class .
Precision Recall F-Score Positive 0.6934 0.7659 0.7278 Negative 0.5371 0.4276 0.4762 Neutral 0.0585 0.0688 0.0632
Table 5 : 5 Results for SMS test data for Task A , for each class .
Table 6 : 6 Accuracy results for task B by 10 - fold crossvalidation on the training data
Table 7 : 7 Table6 ) .
This classifier uses SVM with BOW and all the extra features .
Results for Task B for the submitted runs ( Average F-score for positive / negative ) .
System Tweets SMS uOttawa submitted 0.4251 0.4051 system uOttawa new system 0.8684 0.9140 Median system 0.5150 0.4523
Best system 0.6902 0.6846 Class Precision Recall F-score Positive 0.6206 0.5089 0.5592 Negative 0.4845 0.2080 0.2910 Neutral 0.5357 0.7402 0.6216
Table 8 : 8 Results for each class for task B , for the submitted system ( SVM with BOW plus SentiWordNet features ) for the Twitter test data .
Class Precision Recall F-score Positive 0.4822 0.5508 0.5142 Negative 0.5643 0.2005 0.2959 Neutral 0.6932 0.7988 0.7423
Table 9 : 9 Results for each class for task B , for the submitted system ( SVM with BOW plus SentiWordNet features ) for the SMS test data .
