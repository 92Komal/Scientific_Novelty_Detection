title
TakeLab at SemEval-2017
Task 4 : Recent Deaths and the Power of Nostalgia in Sentiment Analysis in Twitter
abstract
This paper describes the system we submitted to SemEval - 2017 Task 4 ( Sentiment Analysis in Twitter ) , specifically subtasks A , B , and D .
Our main focus was topic-based message polarity classification on a two -point scale ( subtask B ) .
The system we submitted uses a Support Vector Machine classifier with rich set of features , ranging from standard to more creative , task - specific features , including a series of rating - based features as well as features that account for sentimental reminiscence of past topics and deceased famous people .
Our system ranked 14th out of 39 submissions in subtask A , 5th out of 24 submissions in subtask B , and 3rd out of 16 submissions in subtask D .
Introduction Sentiment analysis ( Pang et al. , 2002 ) , a task of determining polarity of text towards some topic , recently gained a lot of interest , mostly due to its applicability in various fields , such as public relations ( Pang et al. , 2008 ) and market analysis ( He et al. , 2013 ) .
Following the growing popularity of social networks and an increasing number of user comments that can be found there , sentiment analysis of texts on social networks , such as tweets from Twitter , has been the focus of much research .
However , determining the sentiment of a tweet is often not an easy task , since the length of the tweet is limited and language is mostly informal , including slang , abbreviations , and hashtags .
Various systems have been proposed for tackling this problem , ranging from simple unsupervised models that use precompiled sentiment lexicons for evaluating polarity of tweets ( O' Connor et al. , 2010 ) to more complex supervised models that use textual feature representations in combination with machine learning algorithms such as Support Vector Machines ( SVM ) ( Khan et al. , 2015 ; Barbosa and Feng , 2010 ) or deep neural networks ( Dos Santos and Gatti , 2014 ; Tang et al. , 2014 ) .
In this paper , we present our system for determining sentiment of tweets , which we submitted SemEval - 2017 Task 4 ( Rosenthal et al. , 2017 ) , more specifically to the English versions of subtasks A , B , and D .
In subtask A , the goal was to predict the sentiment of a tweet as either positive , neutral , or negative .
Subtask
B consisted of predicting the sentiment of a given tweet on a 2 - point scale ( positive or negative ) given a topic .
In subtask D , the task was to determine the distribution of positive and negative tweets for each topic in a given set of tweets annotated with topics .
The system we submitted uses an SVM classifier with a linear kernel and a number of features .
We experiment with basic features such as tf-idf and pretrained word embeddings , as well as more task -specific features including sentiment lexicons , ratings - based , " nostalgia features " , and " recent deaths " .
Ratings - based features use external data from different online resources to leverage the information such as rating of a movie or an actor mentioned in a tweet .
" Recent deaths " features make use of information about recent deaths of notable people , while " nostalgia feature " makes use of topic 's " age " - the rationale being that people usually reminisce about past events in a sentimental and positive way .
Our system ranked 3rd out of 16 teams in subtask D , 5th out of 24 teams in subtask B , and 14th out of 39 teams in subtask A .
Features
To build our model , we first preprocess tweets and extract various features .
We use standard features such as bag-of-words ( more precisely tf - idf ) , pre-trained word embeddings , and count- based stylistic features .
Additionally , we design task -specific features based on publicly available ratings for certain topics .
We next describe the preprocessing and the features in more detail .
Preprocessing of Tweets
As a first preprocessing step , we tokenize tweets using a basic Twitter - adapted tokenizer .
1 After tokenization , we lemmatize and stem tweets and remove stopwords from each tweet using the NLTK toolkit ( Bird et al. , 2009 ) .
Additionally , since some of our features require recognizing named entities in a tweet , we use the named entity tagger , also from the NLTK toolkit , for recognizing entities in tweet .
Unfortunately , using NLTK - provided named entity tagger yielded unsatisfactory results , i.e. , many of named entities in tweets were not recognized correctly .
We assume this is due to tweets generally having poor capitalization , which is something named entity taggers in general are rather sensitive to .
As a remedy , we replaced the tagger with a greedy search algorithm .
This approach simply looks for an occurrence of any string in tweet in some of our named -entity databases ( introduced in the following sections ) .
This proved to be a working solution for named entities longer than one word , but led to problems with unigrams , which were falsely recognized as named entities due to the existence of a movie or a game with that exact name .
For instance , the word " her " would falsely be recognized as the 2013 movie " Her " .
Finally , we settled for a combination of the two approaches .
We introduced parameters for the length range of word sequences .
For the named entity tagger , we set the length range to [ 0 , 1 ] , while for the greedy search algorithm we used the range [ 2 , 7 ] .
This way , we try to reduce the number of falsely recognized named entities in the greedy search algorithm ( by omitting single word entities from its scope ) , while ensuring that some single word entities still get recognized by the named entity chunker .
Standard Features
We use a number of standard features typically used in sentiment analysis and other text classification tasks .
Word embeddings .
For word embeddings we use GloVe ( Pennington et al. , 2014 ) .
We use 200 dimensional word embeddings , pretrained on 2B tweets .
Final vector representation of a tweet is calculated as an average of the sum of vectors of all the words in a tweet .
Tf -idf .
The standard tf - idf vectorizer from Python 's scikit-learn package .
2 Counting features .
For each tweet we count the number of occurrences of various stylistic features : exclamation marks , question marks , elongated words , capitalized words , emoticons , and hashtags .
User information .
We collected the information about authors of the tweets using the script provided by the organizers .
From this data we extracted the number of followers , friends , and tweets of each user .
Sentiment polarity lexicons .
We use sentiment lexicons developed by Bravo- Marquez et al. ( 2016 ) , which contain three weights per word , indicating word 's positive , neutral , and negative sentiment .
These lexicons are built from automatically annotated tweets and existing hand -made opinion lexicons .
We use the most positive word sentiment and the most negative word sentiment in each tweet as features , together with the number of extremely positive and extremely negative words in a tweet .
A word is considered extremely positive if its positive weight in lexicon is higher than 0.75 , and extremely negative if its negative weight is higher than 0.8 .
Nostalgia Feature
We presume that some topics , such as games , movies , and music from years ago , are usually mentioned in positive light due to nostalgia .
3
To leverage this , for many of our topic-based features we try to take the age of the certain topic into account .
We use the following metric for calculating nostalgia feature , where applicable : nost ( y ) = min(m , y curr ? y ) ( 1 ) where y is the year of the content 's release , y curr current year , and m empirically determined upper bound for age .
Ratings Features
We introduce a series of features that are calculated if a certain " rateable " topic is mentioned in a tweet .
In order to build those features , we collected information from publicly available ratings for various domains : movies and TV shows , actors , games , musicians , historically influential people , and companies .
It is worth noting that we are collecting these publicly available ratings independently of training the classifier , which makes our system applicable to tweets about new movies , TV shows , actors , etc. Movies and TV shows .
To gather movies ' and TV shows ' data , we used IMDb 's publicly available plaintext database .
4
As the plaintext database is quite comprehensive , we filtered the data , leaving only movies and TV shows released in 2005 or later , with more than 50,000 user votes , and a minimum average rating of 4.0 .
This reduction left us with an acceptable amount of ~ 4,300 entries .
Movie-ratings features are implemented as a vector of 14 values : a binary value indicating if a movie was found in a tweet , movie 's rating , number of user votes , movie 's nostalgia value ( as defined above ) , and 10 values representing user votes distribution per rating ( from 0 to 9 ) .
Actors .
In the same manner as for the movies , we obtained the IMDb 's plaintext database of actors using the same resource .
We filtered out all actors that do not appear in the previously filtered movies database , to reduce the number of entries in an otherwise huge database .
This left us with approximately 135,000 actor entries .
If an actor is mentioned in a tweet , his or her mention is represented with a single value in the feature vector - actor 's rating .
This rating is calculated by taking into account actor 's appearances in various movies , as well as actor 's position in movie 's credits ; the latter captures the intuition that each actor in a movie does not equally contribute to the movie 's overall rating .
We calculated each actor 's ( a ) rating for a single movie ( m ) as follows : r( a , m ) = r( m ) ? 1 + c 1?pos ( a , m ) k ?1 ( 2 ) where c is the percentage of the movie 's rating taken into account , k is the rate of how much the position affects the rating , r( m ) is the movie 's rating , and pos ( a , m ) is the actor 's position in the movie 's credits .
Actor 's final rating is then defined as mean of their ratings in all the movies they participated in .
During the evaluation , we set the hypeparameters c and k to 0.1 and 50 , respectively .
Games .
We obtained the data about video games by scraping GameRankings 5 website for all games with at least 10 reviews .
This way we gathered about 8,500 game entries .
A mention of a game in a tweet is represented with three values in the feature vector : a binary value representing game 's mention in a tweet , game 's rating , and game 's nostalgia metric , as defined in Section 2.3 .
Musicians .
For musicians ' data , we scraped Metacritic 's Music People pages .
6
This gave us names , numbers of albums , and ratings for about 10,500 artists and bands .
Three values were added to the feature vector for musicians : a binary value representing musician 's mention in a tweet , number of musician 's albums , and musician 's rating .
Important people .
We use MIT 's Pantheon 7 list of historically influential people , with around 10,000 entries , to obtain a number of useful features .
Based on this list , we compute 28 features , as follows : ? binary value indicating a person has been found in the obtained list ; ? person 's historic ranking on Pantheon ; ? person 's nostalgia value , derived from their birth year ; ? person's Wikipedia page views ; ? person's Wikipedia page views standard deviation ; ? person 's historic popularity ; ? person 's place of birth as a vector of 10 binary values representing the highest - ranked birth sites by Pantheon : U.S. , U.K. , France , Italy , Germany , Russia , Spain , Turkey , Poland , the Netherlands ; ? person 's occupation as a vector of 10 binary values representing the most historically influential occupations by Pantheon : Politician , Actor , Writer , Soccer Player , Religious Figure , Singer , Musician , Philosopher , Physicist , Composer .
Companies .
Using Good Company Ratings ' 2014 report , 8 we gathered the data about various companies , consisting of about 300 entries .
Company features contain four values : a binary value indicating company 's mention in a tweet , its Fortune rank , seller ratings , and steward ratings .
Recent Deaths
Due to the impact celebrity deaths have on social media and the fact that people usually reminisce in a positive way about deceased people , we posit that the information whether a person mentioned in a tweet died recently would prove useful for sentiment analysis .
To this end , we gathered from Wikipedia 9 a list of significant people who died in the last three years .
We represent deaths using a single value indicating the number of years that have passed from the person 's death .
While at first similar to nostalgia metric introduced above , the death metric accounts for recent events and , therefore , emphasizes values that are the opposite of the values obtained with nostalgia metric .
The death metric is given by 1 ? nost ( y ) , where y is the year of death , with m set to 3 ( last three years only ) .
Controversy
We encode controversial topics as 41 binary values , one for each of the currently controversial events listed in the University of To further improve the performance of correct identification of controversial topics , we additionally provide alternative phrases for some of the issues , for example " Affordable Care Act " is also triggered by its popular nickname " Obamacare " .
Curse Words
We use a list of 165 curse words often used in tweets , compiled by Kukova?ec et al . ( 2017 ) .
Presence of a curse word in a tweet is encoded with a single binary value , indicating whether a curse word was found in tweet or not .
Topics and Hashtags
We built the above-mentioned features for three cases : topic identified from tweet 's text , topic explicitly given as tweet 's topic in subtasks B and D , and for the topic that might appear in a hashtag as a part of tweet .
Since hashtags usually contain more than one word in a single string , we adapt the greedy splitting procedure proposed by Tutek et al . ( 2016 ) , which uses a dictionary of known words to split a hashtag that is a concatenation of multiple words .
Additionally , for each word obtained from hashtag splitting we generate sentiment lexicon features , acknowledging that sentiment is often expressed via a hashtag .
Evaluation
We started with a number of different classifiers and chose the one that gave the best result on a hold - out test set ( 2016 test set ) ( Nakov et al. , 2016 ) for each subtask .
After the official results were published , together with the gold labels for test sets , we additionally performed a simple feature analysis over predefined feature groups to analyze the impact each of these groups has on the final result .
Evaluation Metrics
We submitted our solution to three subtasks : A , B , and D. Subtask A uses macro- averaged recall ( ? ) over all three classes ( positive , neutral , and negative ) as an official metric .
For subtask B , macroaveraged recall over positive and negative classes ( ? PN ) is used , while subtask D uses Kullback - Leibler Divergence ( KLD ) as the official measure .
Classifier Selection
We experimented with a number of classification algorithms from the scikit-learn package ( Pedregosa et al. , 2011 ) : Support Vector Machine with a linear kernel ( SVM ) , Logistic Regression ( LR ) , Multinomial Naive Bayes ( MB ) , Random Forest ( RF ) , and a Stohastic Gradient Descent classifier ( SGD ) .
For training , we used all the available data provided by the organizers , except for the 2016 test set , which we used for testing the classifiers .
We performed 5 - fold cross-validation
We observed that macro- averaged recall in subtask B ( the official measure ) improved ( from 0.705 to 0.752 ) when we completely excluded the ratings and user information features , although the accuracy notably dropped ( from 0.899 to 0.855 ) .
For this reason , we decided to submit our solution to both subtasks B and D without those two feature groups , since it led to higher recall at the expense of lower overall accuracy .
Table 1 shows the results in terms of macroaveraged recall on the 2016 test set for subtasks A and B for all of the models we experimented with and the final set of features that we included in the final submissions .
We chose SVM with a linear kernel for all of our submissions , as it gave the best result on both subtask A and B .
For subtask D , we used the outputs of the classifier built for subtask B and calculated the distribution of tweets using a simple " classify and count " approach .
Our submissions ranked 14th on the leaderboard for subtask A , 5th for subtask B , and 3rd for subtask D. Table 2 shows scores of top 7 submissions for subtasks B and D .
Feature Analysis
After the testing phase finished , we carried out an analysis of the impact of the specific feature groups in classification , for each of the three subtasks we took part in .
We performed an ablation study over all feature groups .
The results of these analyses are shown in Table 3 .
The analysis confirmed that excluding some feature groups indeed helps in obtaining higher recall in both subtask A and B .
More specifically , excluding counting or lexicon features improved recall in subtask B , while excluding counting , lexicon , or ratings features from the set of features used for subtask A led to an increase in recall as well .
Moreover , KLD score improves as well , when the group of counting features is excluded from complete set of features .
Conclusion
In this paper we described our solutions to Se-mEval 2017 Task 4 - subtasks A , B , and D .
Our solution is based on a linear SVM classifier with some standard and a series of task -specific features , including rating - based features obtained from various websites as well as features that account for sentimental reminiscence of past topics and deceased famous people .
Our system performed relatively well in all three subtasks .
We ranked 14th out of 39 in subtask A , 5th out of 24th in subtask B , and 3rd out of 14 in subtask D. For future work , it would be interesting to expand our model 's feature set to non-covered domains ( e.g. , sports ) , and also to investigate how our model behaves on a more diverse set of topics .
Expanding the system with a topic classifier as a pre-sentiment processing step might also be worth investigating , since the way sentiment is expressed varies across different domains .
Table 3 : 3 Feature analysis results for all subtasks Team ? PN Team KLD BB twtr 0.882 BB twtr 0.036 DataStories 0.856 DataStories 0.048
Tweester 0.854 TakeLab 0.050 TopicThunder 0.846 CrystalNest 0.056 TakeLab 0.845
Tweester 0.057 funSentiment 0.834 funSentiment 0.060 YNU-HPCC 0.834 NileTMRG 0.077
Table 2 : Official results for subtasks B and D ( top 7 teams only ) Excluded group A ( ? ) B (? PN ) D ( KLD ) None 0.615 0.849 0.054 Counting 0.616 0.852 0.050 Lexicon 0.617 0.850 0.052 Ratings 0.617 0.846 0.053 Tf-idf 0.610 0.840 0.061 User info 0.615 0.849 0.053 Word2vec 0.602 0.798 0.116
