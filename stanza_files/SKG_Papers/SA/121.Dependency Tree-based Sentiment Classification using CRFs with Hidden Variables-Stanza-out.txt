title
Dependency Tree-based Sentiment Classification using CRFs with Hidden Variables
abstract
In this paper , we present a dependency treebased method for sentiment classification of Japanese and English subjective sentences using conditional random fields with hidden variables .
Subjective sentences often contain words which reverse the sentiment polarities of other words .
Therefore , interactions between words need to be considered in sentiment classification , which is difficult to be handled with simple bag-of-words approaches , and the syntactic dependency structures of subjective sentences are exploited in our method .
In the method , the sentiment polarity of each dependency subtree in a sentence , which is not observable in training data , is represented by a hidden variable .
The polarity of the whole sentence is calculated in consideration of interactions between the hidden variables .
Sum-product belief propagation is used for inference .
Experimental results of sentiment classification for Japanese and English subjective sentences showed that the method performs better than other methods based on bag-of-features .
Introduction Sentiment classification is a useful technique for analyzing subjective information in a large number of texts , and many studies have been conducted ( Pang and Lee , 2008 ) .
A typical approach for sentiment classification is to use supervised machine learning algorithms with bag-of-words as features ( Pang et al. , 2002 ) , which is widely used in topic-based text classification .
In the approach , a subjective sentence is represented as a set of words in the sentence , ignoring word order and head-modifier relation between words .
However , sentiment classification is different from traditional topic-based text classification .
Topic- based text classification is generally a linearly separable problem ( ( Chakrabarti , 2002 ) , p.168 ) .
For example , when a document contains some domain-specific words , the document will probably belong to the domain .
However , in sentiment classification , sentiment polarities can be reversed .
For example , let us consider the sentence " The medicine kills cancer cells . "
While the phrase cancer cells has negative polarity , the word kills reverses the polarity , and the whole sentence has positive polarity .
Thus , in sentiment classification , a sentence which contains positive ( or negative ) polarity words does not necessarily have the same polarity as a whole , and we need to consider interactions between words instead of handling words independently .
Recently , several methods have been proposed to cope with the problem ( Zaenen , 2004 ; Ikeda et al. , 2008 ) .
However , these methods are based on flat bag-of-features representation , and do not consider syntactic structures which seem essential to infer the polarity of a whole sentence .
Other methods have been proposed which utilize composition of sentences ( Moilanen and Pulman , 2007 ; Choi and Cardie , 2008 ; Jia et al. , 2009 ) , but these methods use rules to handle polarity reversal , and whether polarity reversal occurs or not cannot be learned from labeled data .
Statistical machine learning can learn useful information from training data and generally robust for noisy data , and using it instead of rigid rules seems useful .
Wilson et al. ( 2005 ) proposed a method for sentiment classification which utilizes head-modifier relation and machine learning .
However , the method is based on bag-of-features and polarity reversal occurred by content words is not handled .
One issue of the approach to use sentence composition and machine learning is that only the whole sentence is labeled with its polarity in general corpora for sentiment classification , and each component of the sentence is not labeled , though such information is necessary for supervised ma -
In this paper , we propose a dependency tree - based method for Japanese and English sentiment classification using conditional random fields ( CRFs ) with hidden variables .
In the method , the sentiment polarity of each dependency subtree , which is not observable in training data , is represented by a hidden variable .
The polarity of the whole sentence is calculated in consideration of interactions between the hidden variables .
The rest of this paper is organized as follows :
Section 2 describes a dependency tree- based method for sentiment classification using CRFs with hidden variables , and Section 3 shows experimental results on Japanese and English corpora .
Section 4 discusses related work , and Section 5 gives conclusions .
Dependency Tree-based Sentiment Classification using CRFs with Hidden Variables
In this study , we handle a task to classify the polarities ( positive or negative ) of given subjective sentences .
In the rest of this section , we describe a probabilistic model for sentiment classification based on dependency trees , methods for inference and parameter estimation , and features we use .
A Probabilistic Model based on Dependency Trees Let us consider the subjective sentence " It prevents cancer and heart disease . "
In the sentence , cancer and heart disease have themselves negative polari - s 0 s 1 s 2 s 3 s 4 g 1 g 2 g 3 g 4 g 5 g 6 g 7 g 8 Figure 3 : Factor Graph ties .
However , the polarities are reversed by modifying the word prevents , and the dependency subtree " prevents cancer and heart disease " has positive polarity .
As a result , the whole dependency tree " It prevents cancer and heart disease . " has positive polarity ( Figure 1 ) .
In such a way , we can consider the sentiment polarity for each dependency subtree of a subjective sentence .
Note that we use phrases as a basic unit instead of words in this study , because phrases are useful as a meaningful unit for sentiment classification 1 .
In this paper , a dependency subtree means the subtree of a dependency tree whose root node is one of the phrases in the sentence .
We use a probabilistic model as shown in Figure 2 .
We consider that each phrase in the subjective sentence has a random variable ( indicated by a circle in Figure 2 ) .
The random variable represents the polarity of the dependency subtree whose root node is the corresponding phrase .
Two random variables are dependent ( indicated by an edge in Figure 2 ) if their corresponding phrases have head-modifier relation in the dependency tree .
The node denoted as < root > in Figure 2 indicates a virtual phrase which represents the root node of the sentence , and we regard that the random variable of the root node is the polarity of the whole sentence .
In usual annotated corpora for sentiment classification , only each sentence is labeled with its polarity , and each phrase ( dependency subtree ) is not labeled , so all the random variables except the one for the root node are hidden variables that cannot be observed in labeled data ( indicated by gray circles in Figure 2 ) .
With such a probabilistic model , it is possible to utilize properties such that phrases which contain positive ( or negative ) words tend to have positive ( negative ) polarities , and two phrases with head-modifier relation tend to have opposite polarities if the head contains a word which reverses sentiment polarity .
Next , we define the probabilistic model as shown in Figure 2 in detail .
Let n denote the number of phrases in a subjective sentence , w i the i-th phrase , and h i the head index of the i-th phrase .
Let s i denote the random variable which represents the polarity of the dependency subtree whose root is the i-th phrase ( s i ? { + 1 , ?1 } ) , and let p denote the polarity of the whole sentence ( p ? { + 1 , ?1 } ) .
We regard the 0- th phrase as a virtual phrase which represents the root of the sentence .
w , h , s respectively denote the sequence of w i , h i , s i . w = w 1 ? ? ? w n , h = h 1 ? ? ? h n , s = s 0 ? ? ? s n , p = s 0 .
For the example sentence in Figure 1 , w 1 = It , w 2 =prevents , w 3 =cancer , w 4 =and heart disease . , h 1 = 2 , h 2 = 0 , h 3 = 2 , h 4 = 2 .
We define the joint probability distribution of the sentiment polarities of dependency subtrees s , given a subjective sentence w and its dependency tree h , using loglinear models : P ? ( s|w , h ) =
1 Z ? ( w , h ) exp { K ? k=1 ? k F k ( w , h , s ) } , ( 1 ) Z ? ( w , h ) = ? s exp { K ? k=1 ? k F k ( w , h , s ) } , ( 2 ) F k ( w , h , s ) =
n ?
i=1 f k ( i , w , h , s ) , ( 3 ) where ? = {?
1 , ? ? ? , ?
K } is the set of parameters of the model .
f k ( i , w , h , s ) is the feature function of the i-th phrase , and is classified to node feature which considers only the corresponding node , or edge feature which considers both the corresponding node and its head , as follows : f k ( i , w , h , s ) =
{ f n k ( w i , s i ) ( k ? K n ) , f e k ( w i , s i , w h i , s h i ) ( k ? K e ) , ( 4 ) where K n and K e respectively represent the sets of indices of node features and edge features .
Classification of Sentiment Polarity
Let us consider how to infer the sentiment polarity p ? { + 1 , ?1 } , given a subjective sentence w and its dependency tree h .
The polarity of the root node ( s 0 ) is regarded as the polarity of the whole sentence , and p can be calculated as follows : p=argmax p P ? ( p |w , h ) , ( 5 ) P ? ( p|w , h ) = ? s:s 0 =p P ? ( s|w , h ) .
( 6 ) That is , the polarity of the subjective sentence is obtained as the marginal probability of the root node polarity , by summing the probabilities for all the possible configurations of hidden variables .
However , enumerating all the possible configurations of hidden variables is computationally hard , and we use sum-product belief propagation ( MacKay , 2003 ) for the calculation .
Belief propagation enables us to efficiently calculate marginal probabilities .
In this study , the graphical model to be solved has a tree structure ( identical to the syntactic dependency tree ) which has no loops , and an exact solution can be obtained using belief propagation .
Dependencies among random variables in Figure 2 are represented by a factor graph in Figure 3 .
The factor graph consists of variable nodes s i indicated by circles , and factor ( feature ) nodes g i indicated by squares .
In the example in Figure 3 , g i ( 1 ? i ? 4 ) correspond to the node features in Equation ( 4 ) , and g i ( 5 ? i ? 8 ) correspond to the edge features .
In belief propagation , marginal distribution is calculated by passing messages ( beliefs ) among the variables and factors connected by edges in the factor graph ( Refer to ( MacKay , 2003 ) for detailed description of belief propagation ) .
Parameter Estimation
Let us consider how to estimate model parameters ? , given L training examples D = { w l , h l , p l } L l=1 .
In this study , we use the maximum a posteriori estimation with Gaussian priors for parameter estimation .
We define the following objective function L ? , and calculate the parameters ?
which maximize the value : L ? = L ? l=1 log P ? ( p l |w l , h l ) ? 1 2 ?
2 K ? k=1 ? 2 k , ( 7 ) ?=argmax ? L ? , ( 8 ) where ? is a parameter of Gaussian priors and is set to 1.0 in later experiments .
The partial derivatives of L ? are as follows : ?L ? ? k = L ? l=1 [ ? s P ? ( s|w l , h l , p l ) F k ( w l , h l , s ) ? ? s P ? ( s|w l , h l ) F k ( w l , h l , s ) ] ?
1 ? 2 ? k . ( 9 ) The model parameters can be calculated with the L-BFGS quasi-Newton method ( Liu and Nocedal , 1989 ) using the objective function and its partial derivatives .
While the partial derivatives contain summation over all the possible configurations of hidden variables , it can be calculated efficiently using belief propagation as explained in Section 2.2 .
This parameter estimation method is same to one used for Latent-Dynamic Conditional Random Field ( Morency et al. , 2007 ) .
Note that the objective function L ? is not convex , and there is no guarantee for global optimality .
The estimated model parameters depend on the initial values of the parameters , and the setting of the initial values of model parameters will be explained in Section 2.4 .
Features
Table 1 shows the features used in this study .
Features ( a ) -( h ) in Table 1 are used as the node features ( Equation ( 4 ) ) for the i-th phrase , and features ( A ) -( E ) are used as the edge features for the i-th and j-th phrases ( j=h i ) .
In Table 1 , s i denotes the hidden variable which represents the polarity of the dependency subtree whose root node is the ith phrase , q i denotes the prior polarity of the i-th phrase ( explained later ) , r i denotes the polarity reversal of the i-th phrase ( explained later ) , m i denotes the number of words in the i-th phrase , We used the morphological analysis system JU - MAN and the dependency parser KNP 2 for processing Japanese data , and the POS tagger MX - POST ( Ratnaparkhi , 1996 ) and the dependency parser MaltParser 3 for English data .
KNP outputs phrase - based dependency trees , but MaltParser outputs word - based dependency trees , and we converted the word - based ones to phrase - based ones using simple heuristic rules explained in Appendix A. u i , k , b i , k , c i , k , f i , k respectively denote the surface form , base form , coarse- grained part- of-speech ( POS ) tag , Node Features a s i b s i &q i c s i &q i &r i d s i &u i,1 , ? ? ? , s i &u i , m i e s i &c i,1 , ? ? ? , s i &c i , m i f s i &f i,1 , ? ? ? , s i &f i , m i g s i &u i,1 &u i, 2 , ? ? ? , s i &u i , m i ?1 &u i , m i h s i &b i ,1 &b i, 2 , ? ? ? , s i &b i , m i ?1 &b i , m i Edge Features
A s i &s j B s i &s j &r j C s i &s j &r j &q j D s i &s j &b i,1 , ? ? ? , s i &s j &b i , m i E s i &s j &b j,1 , ? ? ? , s i &s j &b j , m j
The prior polarity of a phrase q i ? { + 1 , 0 , ?1 } is the innate sentiment polarity of a word contained in the phrase , which can be obtained from sentiment polarity dictionaries .
We used sentiment polarity dictionaries made by Kobayashi et al . ( 2007 ) and Higashiyama et al . ( 2008 ) 4 for Japanese experiments
( The resulting dictionary contains 6,974 positive expressions and 8,428 negative expressions ) , and a dictionary made by Wilson et al . ( 2005 ) 5 for English experiments ( The dictionary contains 2,289 positive expressions and 4,143 negative expressions ) .
When a phrase contains the words registered in the dictionaries , its prior polarity is set to the registered polarity , otherwise the prior polarity is set to 0 .
When a phrase contains multiple words in the dictionaries , the registered polarity of the last ( nearest to the end of the sentence ) word is used .
The polarity reversal of a phrase r i ? { 0 , 1 } represents whether it reverses the polarities of other phrases ( 1 ) of not ( 0 ) .
We prepared polarity reversing word dictionaries , and the polarity reversal of a phrase is set to 1 if the phrase contains a word in the dictionaries , otherwise set to 0 .
We constructed polarity reversing word dictionaries which contain such words as decrease and vanish that reverse sentiment polarity .
A Japanese polarity reversing word dictionary was constructed from an automatically constructed corpus , and the construction procedure is described in Appendix B ( The dictionary contains 219 polarity reversing words ) .
An English polarity reversing word dictionary was constructed from the General Inquirer dictionary 6 in the same way as Choi and Cardie ( 2008 ) , by collecting words which belong to either NOTLW or DECREAS categories ( The dictionary contains 121 polarity reversing words ) .
Choi and Cardie ( 2008 ) categorized polarity reversing words into two categories : function - word negators such as not and content - word negators such as eliminate .
The polarity reversal of a phrase r i explained above handles only the content- word negators , and function - word negators are handled in another way , since the scope of a function - word negator is generally limited to the phrase containing it in Japanese , and the number of function - word negators is small .
The prior polarity q i and the polarity reversal r i of a phrase are changed to the following q i and r i , if the phrase contains a function - word negator ( in Japanese ) or if the phrase is modified by a functionword negator ( in English ) : q i =?q i , ( 10 ) r i =1 ? r i . ( 11 ) In this paper , unless otherwise noted , the word polarity reversal is used to indicate polarity reversing caused by content - word negators , and function - word negators are assumed to be applied to q i and r i in the above way beforehand .
As described in Section 2.3 , there is no guarantee of global optimality for estimated parameters , since the objective function is not convex .
In our preliminary experiments , L- BFGS often did not converge and classification accuracy was unstable when the initial values of parameters were randomly set .
Therefore , in later experiments , we set the initial values in the following way .
For the feature ( A ) in Table 1 in which s i and s j are equal , we set the initial parameter ?
i of the feature to a random number in [ 0.9 , 1.1 ] , otherwise we set to a random number in [?0.1 , 0.1 ]
7 .
By setting such initial values , the initial model parameters have a property that two phrases with head -modifier relation tend to have the same polarity , which is intuitively reasonable .
Experiments
We conducted experiments of sentiment classification on four Japanese corpora and four English corpora .
Data
We used four corpora for experiments of Japanese sentiment classification : the Automatically Constructed Polarity - tagged corpus ( ACP ) ( Kaji and Kitsuregawa , 2006 ) , the Kyoto University and NTT Blog corpus ( KNB ) 8 , the NTCIR Japanese opinion corpus ( NTC - J ) ( Seki et al. , 2007 ; Seki et al. , 2008 ) , the 50 Topics Evaluative Information corpus ( 50 Topics ) ( Nakagawa et al. , 2008 ) .
The ACP corpus is an automatically constructed corpus from HTML documents on the Web using lexico-syntactic patterns and layout structures .
The size of the corpus is large ( it consists of 650,951 instances ) , and we used 1/100 of the whole corpus .
The KNB corpus consists of Japanese blogs , and is manually annotated .
The NTC-J corpus consists of Japanese newspaper articles .
There are two NTCIR Japanese opinion corpora available , the NTCIR - 6 corpus and the NTCIR -7 corpus ; and we combined the two corpora .
The 50 Topics corpus is collected from various pages on the Web , and is manually annotated .
We used four corpora for experiments of English sentiment classification : the Customer Review data 7
The values of most learned parameters distributed between - 1.0 and 1.0 in our preliminary experiments .
Therefore , we decided to give values around the upper bound ( 1.0 ) and the mean ( 0.0 ) to the features in order to incorporate minimal prior knowledge into the model .
8 http://nlp.kuee.kyoto-u.ac.jp/kuntt/ ( CR ) 9 , the MPQA Opinion corpus ( MPQA ) 10 , the Movie Review Data ( MR ) 11 , and the NTCIR English opinion corpus ( NTC - E ) ( Seki et al. , 2007 ; Seki et al. , 2008 ) .
The CR corpus consists of review articles about products such as digital cameras and cellular phones .
There are two customer review datasets , the 5 products dataset and the 9 products dataset , and we combined the two datasets .
In the MPQA corpus , sentiment polarities are attached not to sentences but expressions ( sub-sentences ) , and we regarded the expressions as sentences and classified the polarities .
There are two NTCIR English corpora available , the NTCIR - 6 corpus and the NTCIR - 7 corpus , and we combined the two corpora .
The statistical information of the corpora we used is shown in Table 2 .
We randomly split each corpus into 10 portions , and conducted 10 - fold cross validation .
Accuracy of sentiment classification was calculated as the number of correctly predicted labels ( polarities ) divided by the number of test examples .
Compared Methods
We compared our method to 6 baseline methods , and this section describes them .
In the following , p 0 ? { + 1 , ?1 } denotes the major polarity in training data , H i denotes the set consisting of all the ancestor nodes of the i-th phrase in the dependency tree , and sgn ( x ) is defined as below : sgn ( x ) = ? ? ? ? ? + 1 ( x > 0 ) , 0 ( x = 0 ) , ?1 ( x < 0 ) .
Voting without Polarity Reversal
The polarity of a subjective sentence is decided by voting of each phrase 's prior polarity .
In the case of a tie , the major polarity in the training data is adopted .
p=sgn ( n ? i=1 q i + 0.5p 0 ) . ( 12 ) Voting with Polarity Reversal Same to Voting without Polarity Reversal , except that the polarities of phrases which have odd numbers of 9 http://www.cs.uic.edu/ liub/FBS/sentiment-analysis.html 10 http://www.cs.pitt.edu/mpqa/ 11 http://www.cs.cornell.edu/People/pabo/movie-reviewdata/ reversal phrases in their ancestors are reversed before voting .
p=sgn ( n ? i=1 q i ? j?H i ( ? 1 ) r j + 0.5p 0 ) . ( 13 ) Rule
The polarity of a subjective sentence is deterministically decided basing on rules , by considering the sentiment polarities of dependency subtrees .
The polarity of the dependency subtree whose root is the i-th phrase is decided by voting the prior polarity of the i-th phrase and the polarities of the dependency subtrees whose root nodes are the modifiers of the i-th phrase .
The polarities of the modifiers are reversed if their head phrase has a reversal word .
The decision rule is applied from leaf nodes in the dependency tree , and the polarity of the root node is decided at the last .
s i =sgn ( q i + ? j:h j =i s j ( ? 1 ) r i ) , ( 14 ) p=sgn( s 0 + 0.5p 0 ) . Bag-of-Features with No Dictionaries
The polarity of a subjective sentence is classified using Support Vector Machines .
Surface forms , base forms , coarse- grained POS tags and finegrained POS tags of word unigrams and bigrams in the subjective sentence are used as features 12 .
The second order polynomial kernel is used and the cost parameter C is set to 1.0 .
No prior polarity information ( dictionary ) is used .
odd numbers of reversal phrases in their ancestors are reversed before voting .
Bag-of-Features without Polarity Reversal Tree-CRF
The proposed method based on dependency trees using CRFs , described in Section 2 .
Experimental Results
The experimental results are shown in Table 3 .
The proposed method Tree - CRF obtained the best accuracies for all the four Japanese corpora and the four English corpora , and the differences against the second best methods were statistically significant ( p < 0.05 ) with the paired t-test for the six of the eight corpora .
Tree - CRF performed better for the Japanese corpora than for the English corpora .
For both the Voting methods and the Bag-of - Features methods , the methods with polarity reversal performed better than those without it 13 . Both BoF -w / Rev. and Tree -CRF use supervised machine learning and the same dictionaries ( the 13 The Japanese polarity reversing word dictionary was constructed from the ACP corpus as described in Appendix B , and it is not reasonable to compare the methods with and without polarity reversal on the ACP corpus .
However , the tendency can be seen on the other 7 corpora .
prior polarity dictionaries and the polarity reversing word dictionaries ) , but the latter performed better than the former .
Our error analysis showed that BoF -w / Rev. was not robust for erroneous words in the prior polarity dictionaries .
BoF -w / Rev. uses the voting result of the prior polarities as a feature , and the feature is sensitive to the errors in the dictionary , while Tree - CRF uses several information as well as the prior polarities to decide the polarities of dependency subtrees , and was robust to the dictionary errors .
We investigated the trained model parameters of Tree - CRF , and found that the features ( E ) in Table 1 , in which the head and the modifier have opposite polarities and the head word is such as protect and withdraw , have large positive weights .
Although these words were not included in the polarity reversing word dictionary , the property that these words reverse polarities of other words seems to be learned with the model .
Related Work Various studies on sentiment classification have been conducted , and there are several methods pro-posed for handling reversal of polarities .
In this paper , our method was not directly compared with the other methods , since it is difficult to completely implement them or conduct experiments with exactly the same settings .
Choi and Cardie ( 2008 ) proposed a method to classify the sentiment polarity of a sentence basing on compositional semantics .
In their method , the polarity of the whole sentence is determined from the prior polarities of the composing words by pre-defined rules , and the method differs from ours which uses the probabilistic model to handle interactions between hidden variables .
Syntactic structures were used in the studies of Moilanen and Pulman ( 2007 ) and , Jia et al . ( 2009 ) , but their methods are based on rules and supervised learning was not used to handle polarity reversal .
As discussed in Section 1 , Wilson et al . ( 2005 ) studied a bag-of-features based statistical sentiment classification method incorporating head -modifier relation .
Ikeda et al. ( 2008 ) proposed a machine learning approach to handle sentiment polarity reversal .
For each word with prior polarity , whether the polarity is reversed or not is learned with a statistical learning algorithm using its surrounding words as features .
The method can handle only words with prior polarities , and does not use syntactic dependency structures .
Conditional random fields with hidden variables have been studied so far for other tasks .
Latent-Dynamic Conditional Random Fields ( LDCRF ) ( Morency et al. , 2007 ; are probabilistic models with hidden variables for sequential labeling , and belief propagation is used for inference .
Out method is similar to the models , but there are several differences .
In our method , only one variable which represents the polarity of the whole sentence is observable , and dependency relation among random variables is not a linear chain but a tree structure which is identical to the syntactic dependency .
Conclusion
In this paper , we presented a dependency tree - based method for sentiment classification using conditional random fields with hidden variables .
In this method , the polarity of each dependency subtree of a subjective sentence is represented by a hidden variable .
The values of the hidden variables are calculated in consideration of interactions between variables whose nodes have head-modifier relation in the dependency tree .
The value of the hidden variable of the root node is identified with the polarity of the whole sentence .
Experimental results showed that the proposed method performs better for Japanese and English data than the baseline methods which represents subjective sentences as bag-of-features .
Figure 1 : 1 Figure 1 : Polarities of Dependency Subtrees chine learning to infer the sentence polarity from its components .
In this paper , we propose a dependency tree - based method for Japanese and English sentiment classification using conditional random fields ( CRFs ) with hidden variables .
In the method , the sentiment polarity of each dependency subtree , which is not observable in training data , is represented by a hidden variable .
The polarity of the whole sentence is calculated in consideration of interactions between the hidden variables .
The rest of this paper is organized as follows :
Section 2 describes a dependency tree- based method for sentiment classification using CRFs with hidden variables , and Section 3 shows experimental results on Japanese and English corpora .
Section 4 discusses related work , and Section 5 gives conclusions .
