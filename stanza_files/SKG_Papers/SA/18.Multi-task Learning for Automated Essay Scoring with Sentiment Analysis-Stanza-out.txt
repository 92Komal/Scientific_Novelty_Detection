title
Multi-task Learning for Automated Essay Scoring with Sentiment Analysis
abstract
Automated Essay Scoring ( AES ) is a process that aims to alleviate the workload of graders and improve the feedback cycle in educational systems .
Multi-task learning models , one of the deep learning techniques that have recently been applied to many NLP tasks , demonstrate the vast potential for AES .
In this work , we present an approach for combining two tasks , sentiment analysis , and AES by utilizing multitask learning .
The model is based on a hierarchical neural network that learns to predict a holistic score at the document - level along with sentiment classes at the word-level and sentence - level .
The sentiment features extracted from opinion expressions can enhance a vanilla holistic essay scoring , which mainly focuses on lexicon and text semantics .
Our approach demonstrates that sentiment features are beneficial for some essay prompts , and the performance is competitive to other deep learning models on the Automated Student Assessment Prize ( ASAP ) benchmark .
The Quadratic Weighted Kappa ( QWK ) is used to measure the agreement between the human grader 's score and the model 's prediction .
Our model produces a QWK of 0.763 .
Introduction Automatic essay scoring ( AES ) is the task of grading student essays , using natural language processing to assess quality .
The system is designed to reduce time and cost from the human graders ' workload .
Recently , neural network models based on deep learning techniques have been proposed for AES .
These approaches involve the use of both recurrent neural networks , e.g. , a basic recurrent unit ( RNN ) ( Elman , 1990 ) , gated recurrent unit ( GRU ) ( Cho et al. , 2014 ) , or long short - term memory unit ( LSTM ) ( Hochreiter and Schmidhuber , 1997 ) , and convolutional neural networks ( Lecun et al. , 1998 ; Kim , 2014 ) .
More specifically , Taghipour and Ng ( 2016 ) proposed a convolutional recurrent neural network over the word sequence to construct a document representation .
Dong et al. employed hierarchical CNN and LSTM structure ( Dong and Zhang , 2016 ; to construct sentences and document representation separately .
Similarly , several text properties are utilized for scoring an essay , such as grammatical roles ( i.e. , subject , object ) ( Burstein et al. , 2010 ) , discourse ( Song et al. , 2017 ) , or coherence ( Tay et al. , 2017 ; Mesgar and Strube , 2018 ) .
The divergent and polarizing writers ' opinions in their essays create overall essay structure and quality , especially in persuasive and controversial articles ( Pang and Lee , 2008 ) .
Sentiment analysis has typically been designed for use with specific domains , such as movie reviews ( Thongtan and Phienthrakul , 2019 ) , product reviews ( Shrestha and Nasoz , 2019 ) , social media ( Song et al. , 2019 ) , and news ( Godbole et al. , 2007 ) . Beigman Klebanov et al. ( 2012 ) are the first who attempted to incorporate sentiments with essay data .
It involves the use of subjective lexicons to recognize the polarity of a sentence .
Another notable work ( Klebanov et al. , 2013 ) found a way to measure the compositionality of multi-word expression 's sentiment profile ( relative degree of polarities ) in essays .
Farra et al. ( 2015 ) built essay scoring systems that incorporate persuasiveness based on the analysis of opinions expressed in the essay .
Janda et al. ( 2019 ) showed sentiment - based features are in the top-ranked features giving the best performance on essay evaluation .
However , these works are based on feature engineering , which must be carefully handcrafted and selected to fit the appropriate model .
Recently , Multi-Task Learning ( MTL ) approach has been shown promising results in many NLP tasks .
The primary purpose is to leverage useful information in multiple related tasks to improve all the tasks ' generalization performance .
The objectives are applied together , such as predicting the probability of the sequence and the probability that the sequence contains names ( Cheng et al. , 2015 ) , the frequency of the next word with part-of-speech ( POS ) ( Plank et al. , 2016 ) , surrounding words with other several tasks ( Rei , 2017 ) , error detection with additional linguistic information ( Rei and Yannakoudakis , 2017 ) .
More advanced , Augenstein and S?gaard ( 2017 ) explored MTL for classifying keyphrase boundaries incorporating semantic super-sense tagging and identifying multi-word expression .
Sanh et al. ( 2018 ) introduced a hierarchical model supervising a set of low-level tasks at the bottom layers and more complex tasks at the model 's top layers .
There are merely a few existing works that utilize multi-task learning in AES ( Cummins et al. , 2016 ; Cummins and Rei , 2018 ) .
In this paper , we propose a method to incorporate sentiment analysis and AES .
The proposed method utilizes the sentiment aspect for improving an essay scoring system .
The sentiment information by both word- based and sentence - based , shown in Figure 1 , is applied to enhance textual representations for sentiment perception of the model .
To the best of our knowledge , this is the first approach on multi-task learning incorporating sentiment analysis and AES .
Our proposed system is based on a hierarchical structure model to learn the features and relations between an essay score and its sentiments .
The model is trained to predict a holistic score at the top-level ( document- level ) along with sentence and word sentiments at the lower levels , i.e. , sentence - level and word-level , respectively .
Multi-Task Learning
We employ a hierarchical multi-task learning model similar to the model of Farag and Yannakoudakis ( 2019 ) , shown in Figure 2 .
The model considers an essay d composed of a sequence of sentences d = {s 1 , s 2 , ... , s m } , and each sentence s i consists of a sequence of words s i = {w 1 , w 2 , ... , w n }.
We describe each layer in our framework in detail .
Sentence Representation Firstly , we consider the left - hand side of the framework in Figure 2 .
This part aims to learn the context representation of a sentence by taking a word sequence as an input .
A word embedding lookup table maps the words in the vocabulary into low dimensional vectors , x i = Ew i , ( i = 1 , 2 , ... , n ) , ( 1 ) where E ? R |V |?D be the embedding matrix , | V | is the vocabulary size , and D is the word embedding dimension .
x i ?
R D is the embedding vector of w i , and w i is a one- hot representation of w i .
After the word embedding sequence is obtained from the embedding layer , a bidirectional LSTM is applied to the sequence to capture the context representations .
In addition to a bi-direction , we concatenate the output vectors from both directions : ? ? h w i = LST M ( x i , ? ? ? h w i?1 ) , ? ? h w i = LST M ( x i , ? ? ? h w i + 1 ) , h w i = [ ? ? h w i , ? ? h w i ] .
( 2 ) To construct a representation of a sentence s j , we follow to use an attention pooling layer to automatically calculate weights of the word context representations obtained from the intermediate hidden states of Bi-LSTM {h w 1 , h w 2 , ... , h w n } : u w i = tanh ( W w u h w i ) , a w i = exp ( W w a u w i ) i exp ( W w a u w i ) , s j = i a w i h w i , ( 3 ) where W w u and W w a refer to learnable parameters , u w i and a w i are the attention vector and the attention weight of the i-th word in the sentence s j , respectively .
The attention mechanism ( Xu et al. , 2015 ; Luong et al. , 2015 ) emphasizes the salient words to build better sentence representation s j .
Essay Representation
An essay representation is constructed similarly to the sentence representation .
Instead of taking a sequence of words {w 1 , w 2 , ... , w n } as an input , we employ another Bi-LSTM over a sequence of sentence representations {s 1 , s 2 , ... , s m } , as shown on the right - hand side of Figure 2 : ? ? h s j = LST M ( x j , ? h s j?1 ) , ? ? h s j = LST M ( x j , ? h s j+1 ) , h s j = [ ? ? h s j , ? ? h s j ] . ( 4 ) In the same way as constructing sentence representation , attention pooling is used to summarize all of the sentence contexts : u s j = tanh ( W s u h s j ) , a s j = exp ( W s a u s j ) j exp ( W s a u s j ) , d = j a s j h s j , ( 5 ) where W s u and W s a are learnable parameters , u s i and a s i are the attention vector and attention weight of the j-th sentence in the essay d , respectively .
Objective Essay scoring
The main task of our model is to predict the score of an essay .
It is predicted by applying a fully connected layer to an essay representation d .
Then we bound the score in the range [ 0 , 1 ] with a sigmoid function , ? = ?( W d d ) , ( 6 ) where W d is a learnable weight matrix of a fully connected layer , and ? is a predicted score .
Since it is a regression task , we use mean square error ( MSE ) as a loss function , L sc = 1 N N i=1 ( y i ? ?i ) 2 , ( 7 ) where N is the total number of training data , y i is the ground - truth score , and ? i is a predicted score obtained by the model .
Sentiment Prediction
Another objective of the model is to predict the sentiments of the words and sentences .
To obtain such a probability distribution over word sentiment classes , we use a fully connected layer normalized by a softmax function over the hidden states of word- level Bi-LSTM {h w 1 , h w 2 , ... , h w n } , P ( stm wc i | h w i ) = sof tmax ( W w stm h w i ) , ( 8 ) where W w stm is a learnable weight matrix of a fully connected layer , P ( stm wc i |h i w ) is a predicted probability distribution of the word i-th sentiment , and c denotes a class ( e.g. , positive , negative ) .
A similar method is employed for sentences , where W s stm is a learnable weight matrix of a fully connected layer , and P ( stm sc j |h s j ) is a predicted probability distribution of the sentence j-th sentiment .
The word and sentence sentiment prediction losses are calculated by using the negative log-probability of the correct sentiment labels , P ( stm sc j |h s j ) = sof tmax ( W s stm h s j ) , ( 9 ) L w = ?
i j c stm wc ij logP ( stm wc ij | h w ij ) , ( 10 ) L s = ? j c stm sc j logP ( stm sc j |h s j ) , ( 11 ) where i indicates a number of words in a sentence , j refers to the number of sentences in an essay .
c shows the number of classes of sentiment .
stm w ijc and stm s jc indicate the ground - truth labels of word and sentence sentiment , respectively .
To learn in a multi-task manner , the model optimizes the total loss of the main and auxiliary objectives with different weight indicators , as shown in Eq. ( 12 ) .
L total = ?L sc + ?L w + ?L s , ( 12 ) where ? , ? , and ? ? [ 0 , 1 ] are hyperparameters .
Experiments
Dataset
In our experiments , we used the Automated Student Assessment Prize ( ASAP ) 1 public dataset on Kaggle to evaluate our methods .
The dataset contains eight different prompts of the essay , as described in Table 1 .
The prompts elicit responses of different genres and of different lengths .
The essays were written by students ranging from grade 7 to grade 10 and graded by at least two human graders .
We followed Taghipour and Ng ( 2016 ) to use 5 - fold cross-validation for the evaluation with the same splits .
In 5 folds , three folds of the data are used as a training set , one fold as the development set , and one fold as the test set .
The final result is then calculated from the average of the five folds .
Sentiment annotation
We tokenize an essay into sentences and extract its sentiments using the Stanford CoreNLP 2 based on Recursive Neural Tensor Network ( Socher et al. , 2013 ) .
It first split an essay into sentences , then annotate each sentence and the words in it with sentiment labels , as shown in Figure 1 .
The extracted sentiments are represented within five sentiment classes , i.e. , very negative , negative , neutral , positive , and very positive .
The model was trained on the Standford Sentiment Treebank dataset extracted from movie reviews .
Evaluation Metric
The Quadratic Weighted Kappa ( QWK ) is used as the evaluation metric , which measures correlation or agreement between two raters ( Yannakoudakis and Cummins , 2015 ) , since it is the official evaluation metric of the ASAP competition .
The QWK score ranges from 0 to 1 .
It can also become negative if there is less agreement than expected by chance .
Therefore , the higher the value of QWK , the better the results .
It is calculated using K = 1 ? i , j w i , j O i , j i , j w i , j E i , j , ( 13 ) where matrices w , O , and E are the matrices of weights , observed scores , and expected scores , respectively .
A value of O i , j denotes the number of essays that receive a score i by the first-rater and a score j by the second-rater .
w i , j = ( i? j ) 2 ( N ?1 ) 2 , where N is the number of possible scores , matrix E is calculated as the outer product between two histogram vectors of the scores .
Then matrices O and E are normalized to have the same sum and then calculate the QWK score .
Baselines
We compare our model with several baselines : Single - task uses only one objective , which is essay scoring , without any utilization of sentiment analysis .
The model is an attention - based hierarchical Bi-LSTM .
The loss weight indicator ? is fixed to 1 , ? , and ? are fixed to 0 in Eq. ( 1 ) .
Multi-task has a combination objective of essay scoring and sentiment prediction .
Multi-task ( word sentiment ) focuses only on the prediction of word sentiment .
Hence , the loss weight indicator ? is fixed to 0 in Eq. ( 1 ) .
Similarly , the loss weight indicator ? is set to 0 for multi-task ( sentence sentiment ) for switching off the task .
We also compare our model with several neural deep learning approaches for AES : Hierarchical CNN ( Dong and Zhang , 2016 ) comprises two layers of CNN , in which one convolutional layer is used to extract sentence representations , and the other is stacked on sentence vectors to learn essay representations .
Concatenation of max-pooling and average pooling is used to produce the sentence and essay vectors .
RNN ( Taghipour and Ng , 2016 ) with long shortterm memory units ( LSTM ) .
LSTM units make use of three gates to forget or pass the information through time .
They showed that using a mean-overtime layer is much more effective than using the last state vector or attention mechanism .
Attention - based RCNN ) is similar to hierarchical CNN .
Instead , the convolutional layer is replaced by an LSTM layer at the sentence - level to learn global coherence .
Above the CNN layer and LSTM layer , an attention pooling layer is employed to acquire sentence representations and essay representations , respectively .
SKIPFLOW ( Tay et al. , 2017 ) is based on long short - term memory ( LSTM ) network .
SKIPFLOW mechanism possesses a neural tensor layer to model the relationship between two positional outputs of LSTM across time steps .
The tensor generates a coherence feature and also acts as an auxiliary memory .
The coherence feature vector is then concatenated with the essay representation obtained from a mean pooling over the entire LSTM layer 's hidden states .
Implementation Setup
In the embedding layer , we used the pre-trained word embedding GloVe 3 ( Pennington et al. , 2014 ) trained on 6 billion words from Wikipedia 2014 and Gigaword 5 .
During the training process , word embeddings are fine-tuned .
The vocabulary was set to the 4,000 most frequent words by following Taghipour and Ng ( 2016 ) and treating other words as unknown words .
We set the number of the essay sentences to the maximum for each essay prompts and the maximum sentence length to 128 and trained the models on batch size 16 for 50 epochs .
The following hyperparameters were tuned by using optuna 4 in 100 trials .
?
Embedding dimension : { 50 , 100 , 200 , 300 } ? LSTM dimension : { 50 , 100 , 200 , 300 }
The sentence sentiment prediction task 's difficulty and two auxiliary tasks might make the main objective of the model , essay scoring , unstable .
Table 3 reports the Multi-task objective weights after tuning hyperparameters .
We observe the model could find the proper objective weights for Multi-task ( word sentiment ) and Multi-task ( sentence sentiment ) .
The model performed best when the main objective weight ? is greater than auxiliary objective weights , ? , and ?.
In contrast , in Multi-task ( word&sentence sentiment ) , the model seems unable to find proper objective weights to balance the main and auxiliary tasks .
As the summation of auxiliary task weights , ? , and ? , is larger than that of ?.
Comparing with other related neural deep learning models on AES , we also found that Single-task and Multi-task ( word sentiment ) are better than Dong and Zhang ( 2016 ) and Taghipour and Ng ( 2016 ) and comparable to and Tay et al . ( 2017 ) .
Our models perform poorly on prompt 8 .
One reason is that prompt 8 has the longest average length , and we limit the sentence length too short .
We need to utilize the full text of an essay to the models for further improvement .
Conclusion
In this paper , we described a neural approach incorporating sentiment analysis and automatic essay scoring ( AES ) .
Our method is based on a hierarchical structure multi-task learning model .
We compared our approach to several neural deep learning approaches ( Dong and Zhang , 2016 ; Taghipour and Ng , 2016 ; Tay et al. , 2017 ) on the Automated Student Assessment Prize ( ASAP ) benchmark .
Overall , our approach is competitive with the best ones .
Using word sentiment with multi-task learning , we report better results on four prompts compared to the single- task .
We intend to make the model more sophisticated in future work and to apply other auxiliary tasks .
We also would like to investigate the use of contextualized embeddings ( e.g. , BERT Devlin et al . ( 2019 ) ) that shows amazing performance in many NLP tasks .
Figure 1 : 1 Figure 1 : Example of sentiments in a sentence ( This sentence is taken from an actual essay , and it is not grammatical ) .
