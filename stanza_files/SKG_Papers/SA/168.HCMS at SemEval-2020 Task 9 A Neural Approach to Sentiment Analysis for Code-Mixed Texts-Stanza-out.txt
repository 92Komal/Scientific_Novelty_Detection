title
HCMS at SemEval-2020 Task 9 : A Neural Approach to Sentiment Analysis for Code-Mixed Texts
abstract
Problems involving code-mixed language are often plagued by a lack of resources and an absence of materials to perform sophisticated transfer learning with .
In this paper we describe our submission to the Sentimix Hindi-English task involving sentiment classification of code-mixed texts , and with an F1 score of 67.1 % , we demonstrate that simple convolution and attention may well produce reasonable results .
Introduction Sentiment analysis has gained importance in the current world where social media is crucial in gauging public opinion on products , political campaigns , latest trends and more .
A large portion of the world 's population is multilingual , and so is the content on social networks .
Code-mixing is a natural phenomenon among bilinguals where phrases and words from one language are employed in another .
Typically , the underlying grammar of the primary language that is being spoken is kept intact and phrases from another language are embedded into it .
India in particular , has 23 officially recognized languages and a majorly bilingual population , requiring robust computational tools to effectively exploit the data it produces .
Deep neural networks have had great success in predicting sentiment encapsulated in text - a big part of this success has been their ability to utilize pretrained word embeddings .
In code-mixed tweets however , Hindi words are written in Roman script instead of its native Devanagari script , forcing one to use a transliterator in order to take advantage of pretrained aligned word embeddings , like fastText ( Bojanowski et al. , 2016 ) .
These words in Roman script are unnormalized and may have multiple spellings ; for example , bahut can be spelt in Roman script as bahut , bohot , bohut etc. , and such errors propagate from the transliterator to our downstream task , thereby reducing performance .
In this work , we propose a deep convolution network with self-attention which shows promising results , without any pretraining .
Convolution neural networks ( CNNs ) are known to capture local relations ( Kim , 2014 ) or local context , and work here as feature extractors acting as a substitute for handcrafted sentiment features .
A self-attention layer is then applied over these features ( presenting as vectors ) , which allow each individual feature to attend to all other features ( Bahdanau et al. , 2014 ) , providing global context .
We call this the Hierarchical Context Modeling System ( henceforth referred to as HCMS ) , due to the hierarchical nature of context extraction performed on both levels .
Background Early sentiment analysis systems made use of corpus based statistics ( Wiebe and others , 2000 ) , linguistic tools like Wordnet ( Miller , 1998 ) and lexicon based classifiers ( Turney and Littman , 2002 ) .
More recently , a large portion of work has involved Recurrent Neural Networks ( RNNs ) , Long Short Term Memory Networks ( LSTMs ) and CNNs at word and character levels ( Mulder et al. , 2015 ; Zhang et al. , 2015 ) .
Usage of sub-word level representations ( based on character n-grams ) ( Joshi et al. , 2016 ) , combining both CNNs and LSTMs have also been demonstrated , where the CNN was used to generate sub-word features which were then augmented to word level features .
Sharma et al. ( 2015 ) also set precedent for normalizing code mixed text by dealing with the multiple transliterated word forms being generated as a result of the Romanization of non-English scripts .
Attention mechanisms were first introduced for neural machine translation , and achieved state - of - the - art results at the time .
The idea , which produces significant gains in our own model , has been applied to a variety of tasks such as image captioning , text classification , question answering ( Cheng et al. , 2016 ; Xu et al. , 2015 ; Choi et al. , 2017 ) , and even aspect level sentiment classification for more fine grained feature extraction ( Wang et al. , 2016 ) .
Attention helps in obtaining better representations of text which in turn boosts downstream performance and thus , is a vital component of HCMS where self-attention is used to boost the performance of the underlying architecture .
Approaching the problem as one of sequence classification , we present the HCMS architecture as depicted in figure 1 . For every tweet ( represented as a sequence of word embeddings ) , our model first performs a convolution and maxpooling operation to get local context between words that are adjacent to one another .
In the next step , self-attention allows every context vector to attend to every other context vector in the sentence to get global context , summarizing the tweet .
Lastly , the model passes this summary through a dense layer to produce probabilities over the three classes , namely positive , negative and neutral .
System Overview
Algorithm Given a tweet X as [ x 1 , ... , x u ] , where every x i ?
R d is a d dimensional word embedding .
The model first performs a ReLU activated , 1D convolution operation along the tweet followed by a max-pooling operation , to produce localized context vectors C as [ c 1 , ...c v ] , where every c i ?
R d .
The dimensions of C will depend on the parameters of the CNN and max-pooling layers .
C = MaxPool ( ReLU ( CNN ( X ) ) ( 1 ) Next , the model performs self-attention between the local context vectors in C. Self-attention lets every c i ?
C attend to every other c i ?
C to produce attention vectors A as [ a 1 , ... a v ] .
These vectors are then concatenated to produce global context vector G as [ g 1 , ...g w ] , where every g i ?
R. For every t th local context vector , h t, t = tanh ( c T t W t + c T t W c + b t ) c t , c t ?
C and t = t e t, t = ?( W a h t, t + b a ) q t = Softmax ( e t ) a t = t q t , t c t a t ? A G = Concatenate ( l ) ( 2 ) Finally , the global context vector G is passed through a fully connected layer with Softmax activation to predict probabilities Y over the three classes .
Y = Softmax ( GW + b ) ( 3 )
Loss
The model computes categorical cross-entropy loss L between the predicted class probabilities and the correct class for the sample , as given below L( Y , Y ) = ?
Y log ( Y ) ( 4 ) 4 Experimental Setup
Dataset
The data , provided by the task organizers Patwa et al . ( 2020 ) , comprises of tweets that employ the Hinglish mix .
The data is in CONLL format , with each word being tagged as belonging to the English or Hindi language if applicable , else tagged as O if neither or EMT if an emoticon .
Every tweet has a corresponding ID and sentiment label attached to it .
Data Preprocessing
We experimented with multiple steps for cleaning the tweets , including but not limited to lowering text case , expanding English contractions , replacing emoticons by their meanings in English , removing character repetitions , and removing hashtags , usernames and links .
After cleaning , the data was tokenized and indexed by word and converted into embeddings .
Features such as language labels for each word were converted to one hot vectors and concatenated to the word vectors ( not present in final submission ) .
Additional Details
The 1D CNN layer has 200 filters , each of kernel size 8 with stride of 1 .
We used randomly initialized word embeddings of size 200 .
The model was optimized using Adam with a learning rate of 0.01 , beta1 as 0.9 , beta2 as 0.99 and epsilon as 1e ? 7 . All the experiments were carried out on a single Nvidia GeForce RTX 2080 Ti GPU .
The experiments were written in Python3 , using the Keras framework ( Chollet , 2015 ) .
The code itself is hosted on GitHub under the MIT license 1 .
Evaluation Metric
For evaluation we simply use F1 as our metric , emulating the task leaderboard .
Results and Analysis
We used the official validation and test sets provided .
While our contest results stand at 67.1 % F1 , we have fine tuned our model since , and the latest scores for all experiments are shown in table 1 The SVM ( Hearst , 1998 ) trained on n-grams of sizes 1 , 2 and 3 returned an F1 score of 65.97 % .
Given its ability to handle out -of- vocab words , fastText ( Joulin et al. , 2017 ) ( also an n-gram based model ) outperformed the SVM model , achieving 66.26 % F1 .
Ablating the self-attention mechanism from the HCMS model left a vanilla 1D CNN with max pooling .
Both , the LSTM and the CNN performed similarly to the n-gram based fastText model , but it is clear from the results that applying self-attention on top of features extracted by the respective models improved their performance significantly , providing credibility to our hypothesis of local and global features in the data .
HCMS can be seen outperforming the models lacking self-attention by a significant ~ 1 % , giving a score of 68.44 % .
It was observed that cleaning the data improved performance .
Table 2 shows how specific forms of data preprocessing affect HCMS .
Emoji replacement refers to the substitution of graphical emojis with their ASCII counterparts .
3 Contraction replacement refers to the substitution of English contractions like ca n't or would n't with their complete forms cannot and would not respectively .
We can see in the table that using both gives the best performance .
We also analysed the data and observed language distribution across the corpus .
The numbers have been presented in figure 2 .
As expected , the vocab is not equally split between the two languages .
In fact , there is greater presence of Hindi , and that too is Romanized and not in its native Devnagari script .
In our experiments we tried to use a pretrained transliterator to de-Romanize the Hindi words , and then tried to employ pretrained word-embeddings , models and sentiment lexicon - due to the noisy Devnagari-to - Roman transliteration by the authors of the tweets however , our reverse transliteration failed and most words did not map into their respective embedding spaces .
4
The results of those experiments are presented in table 3 ( note that the other steps of the data cleaning process were preserved ) .
In our experiments , another noteworthy occurrence was that even though some models showed high validation scores , they were outperformed on the test set by models that produced lower validation score in comparison .
This was a recurring theme in our A-B testing over validation and test data , and we feel that this can again be attributed to the noisy nature of code-mixed language .
Conclusion
In this paper we have discussed a way to perform sentiment analysis on code-mixed Hindi-English data , and we feel that the system is versatile enough to be applied to any mix of languages , especially when resources for the mixture are unavailable or hard to find .
We have also looked at ways that such data can be preprocessed for training , and how a multilevel neural architecture is able to extract context from text .
While it is encouraging to see that our model performs consistently , there are a couple of caveats that need to be addressed - even though the lack of pretraining , a low resource setting , and noise in data seem surmountable hurdles to the task , it is evident that the most promising avenues for improvement lie in exploring data cleaning / normalization and transfer learning .
Further analysis of the present methodology could also reveal the kind of transfer learning required - whether it be pretrained word embeddings , large scale language modeling or perhaps a better transliteration pipeline to name a few .
To do so would require larger quantities of data and the application of the system to other linguistic tasks of settings similar to that of Sentimix .
All of these steps qualify within the scope of future work , and committing to them will reward us with more confidence in the hypotheses we propose in this paper , providing further proof of the system 's robustness and effectiveness .
We hope to undertake efforts for the same soon .
Figure 1 : Model Architecture
