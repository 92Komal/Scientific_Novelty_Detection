title
Keep Calm and Switch On !
Preserving Sentiment and Fluency in Semantic Text Exchange
abstract
In this paper , we present a novel method for measurably adjusting the semantics of text while preserving its sentiment and fluency , a task we call semantic text exchange .
This is useful for text data augmentation and the semantic correction of text generated by chatbots and virtual assistants .
We introduce a pipeline called SMERTI that combines entity replacement , similarity masking , and text infilling .
We measure our pipeline 's success by its Semantic Text Exchange Score ( STES ) : the ability to preserve the original text 's sentiment and fluency while adjusting semantic content .
We propose to use masking ( replacement ) rate threshold as an adjustable parameter to control the amount of semantic change in the text .
Our experiments demonstrate that SMERTI can outperform baseline models on Yelp reviews , Amazon reviews , and news headlines .
Introduction
There has been significant research on style transfer , with the goal of changing the style of text while preserving its semantic content .
The alternative where semantics are adjusted while keeping style intact , which we call semantic text exchange ( STE ) , has not been investigated to the best of our knowledge .
Consider the following example , where the replacement entity defines the new semantic context : Original Text :
It is sunny outside !
Ugh , that means I must wear sunscreen .
I hate being sweaty and sticky all over .
Replacement Entity : weather = rainy Desired Text :
It is rainy outside !
Ugh , that means I must bring an umbrella .
I hate being wet and having to carry it around .
The weather within the original text is sunny , * Authors contributed equally whereas the actual weather may be rainy .
Not only is the word sunny replaced with rainy , but the rest of the text 's content is changed while preserving its negative sentiment and fluency .
With the rise of natural language processing ( NLP ) has come an increased demand for massive amounts of text data .
Manually collecting and scraping data requires a significant amount of time and effort , and data augmentation techniques for NLP are limited compared to fields such as computer vision .
STE can be used for text data augmentation by producing various modifications of a piece of text that differ in semantic content .
Another use of STE is in building emotionally aligned chatbots and virtual assistants .
This is useful for reasons such as marketing , overall enjoyment of interaction , and mental health therapy .
However , due to limited data with emotional content in specific semantic contexts , the generated text may contain incorrect semantic content .
STE can adjust text semantics ( e.g. to align with reality or a specific task ) while preserving emotions .
One specific example is the development of virtual assistants with adjustable socio-emotional personalities in the effort to construct assistive technologies for persons with cognitive disabilities .
Adjusting the emotional delivery of text in subtle ways can have a strong effect on the adoption of the technologies ( Robillard et al. , 2018 ) .
It is challenging to transfer style this subtly due to lack of datasets on specific topics with consistent emotions .
Instead , large datasets of emotionally consistent interactions not confined to specific topics exist .
Hence , it is effective to generate text with a particular emotion and then adjust its semantics .
We propose a pipeline called SMERTI ( pronounced ' smarty ' ) for STE .
1 Combining entity replacement ( ER ) , similarity masking ( SM ) , and text infilling ( TI ) , SMERTI can modify the semantic content of text .
We define a metric called the Semantic Text Exchange Score ( STES ) that evaluates the overall ability of a model to perform STE , and an adjustable parameter masking ( replacement ) rate threshold ( MRT / RRT ) that can be used to control the amount of semantic change .
We evaluate on three datasets : Yelp and Amazon reviews ( He and McAuley , 2016 ) , and Kaggle news headlines ( Misra , 2018 ) .
We implement three baseline models for comparison : Noun WordNet Semantic Text Exchange Model ( NWN - STEM ) , General WordNet Semantic Text Exchange Model ( GWN - STEM ) , and Word2Vec Semantic Text Exchange Model ( W2V - STEM ) .
We illustrate the STE performance of two SMERTI variations on the datasets , demonstrating outperformance of the baselines and pipeline stability .
We also run a human evaluation supporting our results .
We analyze the results in detail and investigate relationships between the semantic change , fluency , sentiment , and MRT / RRT .
Our major contributions can be summarized as : ?
We define a new task called semantic text exchange ( STE ) with increasing importance in NLP applications that modifies text semantics while preserving other aspects such as sentiment . ?
We propose a pipeline SMERTI capable of multi-word entity replacement and text infilling , and demonstrate its outperformance of baselines . ?
We define an evaluation metric for overall performance on semantic text exchange called the Semantic Text Exchange Score ( STES ) .
Related Work
Word and Sentence-level Embeddings Word2Vec ( Mikolov et al. , 2013 a , b) allows for analogy representation through vector arithmetic .
We implement a baseline ( W2V - STEM ) using this technique .
The Universal Sentence Encoder ( USE ) ( Cer et al. , 2018 ) encodes sentences and is trained on a variety of web sources and the Stanford Natural Language Inference corpus ( Bowman et al. , 2015 ) .
Flair embeddings ( Akbik et al. , 2018 ) are based on architectures such as BERT ( Devlin et al. , 2019 ) .
We use USE for SMERTI as it is designed for transfer learning and shows higher performance on textual similarity tasks compared to other models ( Perone et al. , 2018 ) .
Text Infilling
Text infilling is the task of filling in missing parts of sentences called masks .
MaskGAN
( Fedus et al. , 2018 ) is restricted to a single word per mask token , while SMERTI is capable of variable length infilling for more flexible output .
Zhu et al . ( 2019 ) uses a transformer - based architecture .
They fill in random masks , while SMERTI fills in masks guided by semantic similarity , resulting in more natural infilling and fulfillment of the STE task .
Style and Sentiment Transfer Notable works in style / sentiment transfer include ( Shen et al. , 2017 ; Fu et al. , 2018 ; Xu et al. , 2018 ) .
They attempt to learn latent representations of various text aspects such as its context and attributes , or separate style from content and encode them into hidden representations .
They then use an RNN decoder to generate a new sentence given a targeted sentiment attribute .
Review Generation Hovy ( 2016 ) generates fake reviews from scratch using language models .
( Lipton et al. , 2015 ; Dong et al. , 2017 ; Juuti et al. , 2018 ) generate reviews from scratch given auxiliary information ( e.g. the item category and star rating ) .
Yao et al. ( 2017 ) generates reviews using RNNs with two components : generation from scratch and review customization ( Algorithm 2 in Yao et al . ( 2017 ) ) .
They define review customization as modifying the generated review to fit a new topic or context , such as from a Japanese restaurant to an Italian one .
They condition on a keyword identifying the desired context , and replace similar nouns with others using WordNet ( Miller , 1995 ) .
They require a " reference dataset " ( required to be " on topic " ; easy enough for restaurant reviews , but less so for arbitrary conversational agents ) .
As noted by Juuti et al . ( 2018 ) , the method of Yao et al . ( 2017 ) may also replace words independently of context .
We implement their review customization algorithm ( NWN - STEM ) and a modified version ( GWN - STEM ) as baseline models .
SMERTI
Overview
The task is to transform a corpus C of lines of text S i and associated replacement entities RE i : C = { ( S 1 , RE 1 ) , ( S 2 , RE 2 ) , . . . , ( S n , RE n ) } to a modified corpus ? = { ?1 , ?2 , . . . , ?n } , where ?i are the original text lines S i replaced with RE i and overall semantics adjusted .
SMERTI consists of the following modules , shown in Figure 1 : 1 . Entity Replacement Module ( ERM ) : Identify which word ( s ) within the original text are best replaced with the RE , which we call the Original Entity ( OE ) .
We replace OE in S with RE .
We call this modified text S . 2 . Similarity Masking Module ( SMM ) : Identify words / phrases in S similar to OE and replace them with a [ mask ] .
Group adjacent [ mask ] s into a single one so we can fill a variable length of text into each .
We call this masked text S . 3 . Text Infilling Module ( TIM ) : Fill in [ mask ] tokens with text that better suits the RE .
This will modify semantics in the rest of the text .
This final output text is called ?.
Entity Replacement Module ( ERM )
For entity replacement , we use a combination of the Universal Sentence Encoder ( Cer et al. , 2018 ) and Stanford Parser ( Chen and Manning , 2014 ) .
Stanford Parser
The Stanford Parser is a constituency parser that determines the grammatical structure of sentences , including phrases and part-of-speech ( POS ) labelling .
By feeding our RE through the parser , we are able to determine its parse-tree .
Iterating through the parse-tree and its sub-trees , we can obtain a list of constituent tags for the RE .
We then feed our input text S through the parser , and through a similar process , we can obtain a list of leaves ( where leaves under a single label are concatenated ) that are equal or similar to any of the RE constituent tags .
This generates a list of entities having the same ( or similar ) grammatical structure as the RE , and are likely candidates for the OE .
We then feed these entities along with the RE into the Universal Sentence Encoder ( USE ) .
Universal Sentence Encoder ( USE )
The USE is a sentence - level embedding model that comes with a deep averaging network ( DAN ) and transformer model ( Cer et al. , 2018 ) .
We choose the transformer model as these embeddings take context into account , and the exact same word / phrase will have a different embedding depending on its context and surrounding words .
We compute the semantic similarity between two embeddings u and v : sim ( u , v ) , using the angular ( cosine ) distance , defined as : cos ( ? u, v ) = ( u ? v)/(||u||||v|| ) , such that sim ( u , v ) = 1 ? 1 ? arccos( cos ( ? u, v ) ) .
Results are in [ 0 , 1 ] , with higher values representing greater similarity .
Using USE and the above equation , we can identify words / phrases within the input text S which are most similar to RE .
To assist with this , we use the Stanford Parser as described above to obtain a list of candidate entities .
In the rare case that this list is empty , we feed in each word of S into USE , and identify which word is the most similar to RE .
We then replace the most similar entity or word ( OE ) with the RE and generate S .
An example of this entity replacement process is in Figure 2 As seen in Figure 2 ( d ) , we calculate semantic similarities between RE and entities within S which have noun constituency tags .
Looking at the row for our RE restaurant , the most similar entity ( excluding itself ) is hotel .
We can then generate : S = i love this restaurant ! the beds are comfortable and the service is great !
Similarity Masking Module ( SMM ) Next , we mask words similar to OE to generate S using USE .
We look at semantic similarities between every word in S and OE , along with semantic similarities between OE and the candidate entities determined in the previous ERM step to broaden the range of phrases our module can mask .
We ignore RE , OE , and any entities or phrases containing OE ( for example , ' this hotel ' ) .
After determining words similar to the OE ( discussed below ) , we replace each of them with a We set a base similarity threshold ( ST ) that selects a subset of words to mask .
We compare the actual fraction of masked words to the masking rate threshold ( MRT ) , as defined by the user , and increase ST in intervals of 0.05 until the actual masking rate falls below the MRT .
2 Some sample masked outputs ( S ) using various MRT - ST combinations for the previous example are shown in Table 1 ( more examples in Appendix A ) .
The MRT is similar to the temperature parameter used to control the " novelty " of generated text in works such as Yao et al . ( 2017 ) .
A high MRT means the user wants to generate text very semantically dissimilar to the original , and may be desired in cases such as creating a lively chatbot or correcting text that is heavily incorrect se- mantically .
A low MRT means the user wants to generate text semantically similar to the original , and may be desired in cases such as text recovery , grammar correction , or correcting a minor semantic error in text .
By varying the MRT , various pieces of text that differ semantically in subtle ways can be generated , assisting greatly with text data augmentation .
The MRT also affects sentiment and fluency , as we show in Section 6.5 .
Text Infilling Module ( TIM )
We use two seq2seq models for our TIM : an RNN ( recurrent neural network ) model ( Sutskever et al. , 2014 ) ( called SMERTI - RNN ) , and a transformer model ( called SMERTI - Transformer ) .
Bidirectional RNN with Attention
We use a bidirectional variant of the GRU ( Cho et al. , 2014 ) , and hence two RNNs for the encoder : one reads the input sequence in standard sequential order , and the other is fed this sequence in reverse .
The outputs are summed at each time step , giving us the ability to encode information from both past and future context .
The decoder generates the output in a sequential token - by- token manner .
To combat information loss , we implement the attention mechanism ( Bahdanau et al. , 2015 ) .
We use a Luong attention layer ( Luong et al. , 2015 ) which uses global attention , where all the encoder 's hidden states are considered , and use the decoder 's current time -step hidden state to calculate attention weights .
We use the dot score function for attention , where h t is the current target decoder state and hs is all encoder states : score ( h t , hs ) = h T t hs .
Transformer
Our second model makes use of the transformer architecture , and our implementation replicates Vaswani et al . ( 2017 ) .
We use an encoder-decoder structure with a multi-head self-attention token decoder to condition on information from both past and future context .
It maps a query and set of keyvalue pairs to an output .
The queries and keys are of dimension d k , and values of dimension d v .
To compute the attention , we pack a set of queries , keys , and values into matrices Q , K , and V , respectively .
The matrix of outputs is computed as : Attention ( Q , K , V ) = sof tmax QK T ? d k V ( 1 ) Multi-head attention allows the model to jointly attend to information from different positions .
The decoder can make use of both local and global semantic information while filling in each [ mask ] .
Experiment
Datasets
We train our two TIMs on the three datasets .
The Amazon dataset ( He and McAuley , 2016 ) contains over 83 million user reviews on products , with duplicate reviews removed .
The Yelp dataset includes over six million user reviews on businesses .
The news headlines dataset from Kaggle contains approximately 200 , 000 news headlines from 2012 to 2018 obtained from HuffPost ( Misra , 2018 ) .
We filter the text to obtain reviews and headlines which are English , do not contain hyperlinks and other obvious noise , and are less than 20 words long .
We found that many longer than twenty words ramble on and are too verbose for our purposes .
Rather than filtering by individual sentences we keep each text in its entirety so SMERTI can learn to generate multiple sentences at once .
We preprocess the text by lowercasing and removing rare / duplicate punctuation and space .
For Amazon and Yelp , we treat reviews greater than three stars as containing positive sentiment , equal to three stars as neutral , and less than three stars as negative .
For each training and testing set , we include an equal number of randomly selected positive and negative reviews , and half as many neutral reviews .
This is because neutral reviews only occupy one out of five stars compared to positive and negative which occupy two each .
Our dataset statistics can be found in Appendix B .
Experiment Details
To set up our training and testing data for text infilling , we mask the text .
We use a tiered masking approach : for each dataset , we randomly mask 15 % of the words in one - third of the lines , 30 % of the words in another one - third , and 45 % in the remaining one - third .
These masked texts serve as the inputs , while the original texts serve as the ground-truth .
This allows our TIM models to learn relationships between masked words and relationships between masked and unmasked words .
The bidirectional RNN decoder fills in blanks one by one , with the objective of minimizing the cross entropy loss between its output and the ground -truth .
We use a hidden size of 500 , two layers for the encoder and decoder , teacher - forcing ratio of 1.0 , learning rate of 0.0001 , dropout of 0.1 , batch size of 64 , and train for up to 40 epochs .
For the transformer , we use scaled dotproduct attention and the same hyperparameters as Vaswani et al . ( 2017 ) .
We use the Adam optimizer ( Kingma and Ba , 2014 ) with ?
1 = 0.9 , ? 2 = 0.98 , and = 10 ?9 . As in Vaswani et al. ( 2017 ) , we increase the learning rate linearly for the first warmup steps training steps , and then decrease the learning rate proportionally to the inverse square root of the step number .
We set f actor = 1 and use warmup steps = 2000 .
We use a batch size of 4096 , and we train for up to 40 epochs .
Baseline Models
We implement three models to benchmark against .
3 First is NWN -STEM ( Algorithm 2 from Yao et al . ( 2017 ) ) .
We use the training sets as the " reference review sets " to extract similar nouns to the RE ( using MIN sim = 0.1 ) .
We then replace nouns in the text similar to the RE with nouns extracted from the associated reference review set .
Secondly , we modify NWN - STEM to work for verbs and adjectives 4 , and call this GWN - STEM .
From the reference review sets , we extract similar nouns , verbs , and adjectives to the RE ( using MIN sim = 0.1 ) , where the RE is now not restricted to being a noun .
We replace nouns , verbs , and adjectives in the text similar to the RE with those extracted from the associated reference review set .
Lastly , we implement W2V-STEM using Gensim ( ?eh?ek and Sojka , 2010 ) .
We train uni-gram Word2 Vec models for single word REs , and fourgram models for phrases .
Models are trained on the training sets .
We use cosine similarity to determine the most similar word / phrase in the input text to RE , which is the replaced OE .
For all other words / phrases , we calculate w i = w i ?
w OE + w RE , where w i is the original word / phrase 's embedding vector , w OE is the OE's , w RE is the RE's , and w i is the resulting embedding vector .
The replacement word / phrase is w i 's nearest neighbour .
We use similarity thresholds to adjust replacement rates ( RR ) and produce text under various replacement rate thresholds ( RRT ) .
Evaluation
Evaluation Setup
We manually select 10 nouns , 10 verbs , 10 adjectives , and 5 phrases from the top 10 % most frequent words / phrases in each test set as our evaluation REs .
We filter the verbs and adjectives through a list of sentiment words ( Hu and Liu , 2004 ) to ensure we do not choose REs that would obviously significantly alter the text 's sentiment .
5
For each evaluation RE , we choose onehundred lines from the corresponding test set that does not already contain RE .
We choose lines with at least five words , as many with less carry little semantic meaning ( e.g. ' Great ! ' , ' It is okay ' ) .
For Amazon and Yelp , we choose 50 positive and 50 negative lines per RE .
6
We repeat this process three times , resulting in three sets of 1000 lines per dataset per POS ( excluding phrases ) , and three sets of 500 lines per dataset for phrases .
Our final results are averaged metrics over these three sets .
For SMERTI - Transformer , SMERTI - RNN , and W2V-STEM , we generate four outputs per text for MRT / RRT of 20 % , 40 % , 60 % , and 80 % , which represent upper-bounds on the percentage of the input that can be masked and / or replaced .
Note that NWN -STEM and GWN -STEM can only evaluate on limited POS and their maximum replace-ment rates are limited .
7
We select MIN sim values of 0.075 and 0 for nouns and 0.1 and 0 for verbs , as these result in replacement rates approximately equal to the actual MR / RR of the other models ' outputs for 20 % and 40 % MRT / RRT , respectively .
Key Evaluation Metrics Fluency ( SLOR )
We use syntactic log-odds ratio ( SLOR ) ( Kann et al. , 2018 ) for sentence level fluency and modify from their word-level formula to character - level ( SLOR c ) .
We use Flair perplexity values from a language model trained on the One Billion Words corpus ( Chelba et al. , 2013 ) : SLORc ( S ) = 1 | S | ( ln ( pM ( S ) ) ? ln( w?S pM ( w ) ) w?S | w| ( 2 ) = ?ln( P P Ls ) + w?S | w|ln ( P P LW ) w?S | w| ( 3 ) where | S| and | w | are the character lengths of the input text S and the word w , respectively , p M ( S ) and p M ( w ) are the probabilities of S and w under the language model M , respectively , and P P L S and P P L w are the character - level perplexities of S and w , respectively .
SLOR ( from hereon we refer to character - level SLOR as simply SLOR ) measures aspects of text fluency such as grammaticality .
Higher values represent higher fluency .
We rescale resulting SLOR values to the interval [ 0,1 ] by first fitting and normalizing a Gaussian distribution .
We then truncate normalized data points outside [ - 3,3 ] , which shifts approximately 0.69 % of total data .
Finally , we divide each data point by six and add 0.5 to each result .
Sentiment Preservation Accuracy ( SPA ) is defined as the percentage of outputs that carry the same sentiment as the input .
We use VADER ( Hutto and Gilbert , 2014 ) to evaluate sentiment as positive , negative , or neutral .
It handles typos , emojis , and other aspects of online text .
Content Similarity Score ( CSS ) ranges from 0 to 1 and indicates the semantic similarity between generated text and the RE .
A value closer to 1 indicates stronger semantic exchange , as the output is closer in semantic content to the RE .
We also use the USE for this due to its design and strong performance as previously mentioned .
Semantic Text Exchange Score ( STES )
We come up with a single score to evaluate overall performance of a model on STE that combines the key evaluation metrics .
It uses the harmonic mean , similar to the F 1 score ( or F-score ) ( Chinchor , 1992 ; Rijsbergen , 1979 ) , and we call it the Semantic Text Exchange Score ( STES ) : ST ES = 3 * A * B * C A * B + A * C + B * C ( 4 ) where A is SPA , B is SLOR , and C is CSS .
STES ranges between 0 and 1 , with scores closer to 1 representing higher overall performance .
Like the F 1 score , STES penalizes models which perform very poorly in one or more metrics , and favors balanced models achieving strong results in all three .
Automatic Evaluation Results
Table 2 shows overall average results by model .
8 Table 3 shows outputs for a Yelp example .
9
As observed from Table 3 ( see also Appendix F ) , SMERTI is able to generate high quality output text similar to the RE while flowing better than other models ' outputs .
It can replace entire phrases and sentences due to its variable length infilling .
Note that for nouns , the outputs from GWN - STEM and NWN - STEM are equivalent .
10
Human Evaluation Setup
We conduct a human evaluation with eight participants , 6 males and 2 females , that are affiliated project researchers aged 20 - 39 at the University of Waterloo .
11
We randomly choose one evaluation line for a randomly selected word or phrase for each POS per dataset .
The input text and each model 's output ( for 40 % MRT / RRT - chosen as a good middle ground ) for each line is presented to participants , resulting in a total of 54 pieces of text , and rated on the following criteria from 1 - 5 : 8 See Appendix E for tables and graphs of detailed results broken down by POS , dataset , and MRT / RRT 9 See Appendix F for many more example outputs from each model for various POS and datasets 10 See Appendix C for explanations 11 The authors are not part of the human evaluation ?
RE Match : " How related is the entire text to the concept of [ X ] " , where [ X ] is a word or phrase ( 1 - not at all related , 3 - somewhat related , 5 - very related ) .
Note here that [ X ] is a given RE .
?
Fluency : " Does the text make sense and flow well ? "
( 1 - not at all , 3 - somewhat , 5 - very ) ?
Sentiment : " How do you think the author of the text was feeling ? "
( 1 - very negative , 3 - neutral , 5 - very positive )
Each participant evaluates every piece of text .
They are presented with a single piece of text at a time , with the order of models , POS , and datasets completely randomized .
Human Evaluation Results Average human evaluation scores are displayed in Table 4 . Sentiment Preservation ( between 0 and 1 ) is calculated by comparing the average Sentiment rating for each model 's output text to the Sentiment rating of the input text , and if both are less than 2.5 ( negative ) , between 2.5 and 3.5 inclusive ( neutral ) , or greater than 3.5 ( positive ) , this is counted as a valid case of Sentiment Preservation .
We repeat this for every evaluation line to calculate the final values per model .
Harmonic means of all three metrics ( using rescaled 0 - 1 values of RE Match and Fluency ) are also displayed .
Analysis
Performance by Model
As seen in Table 2 , both SMERTI variations achieve higher STES and outperform the other models overall , with the WordNet models performing the worst .
SMERTI excels especially on fluency and content similarity .
The transformer variation achieves slightly higher SLOR , while the RNN variation achieves slightly higher CSS .
The WordNet models perform strongest in sentiment preservation ( SPA ) , likely because they modify little of the text and only verbs and nouns .
They achieve by far the lowest CSS , likely in part due to this limited text replacement .
They also do not account for context , and many words ( e.g. proper nouns ) do not exist in WordNet .
Overall , the WordNet models are not very effective at STE .
W2V-STEM achieves the lowest SLOR , especially for higher RRT , as supported by the example in Table 3 ( see also Appendix F ) .
W2V-STEM and WordNet models output grammatically incorrect text that flows poorly .
In many cases , words are Input text : great food , large portions ! my family and i really enjoyed our saturday morning breakfast .
Replacement entity : pizza MRT / RRT Generated Output SMERTI - Transformer 20 % great pizza , large slices !
my family and i really enjoyed our saturday morning lunch . 40 % ,60 % great pizza , large slices ! service was terrific and i really enjoyed our saturday morning lunch . 80 % great pizza , chewy crust ! nice ambiance and i really enjoyed it .
SMERTI - RNN 20 % great pizza , large delivery !
my family and i really enjoyed our saturday morning place . 40 % ,60 % great pizza , large delivery ! good beer and i really enjoyed our saturday morning place . 80 % great pizza , amazing pizza ! reasonable and i really enjoyed everyone .
W2V-STEM 20 % great pizza , large portions !
my family and i really enjoyed our saturday morning breakfast . 40 % great pizza , large slices !
my family dough i crust enjoyed our saturday morning breakfast . 60 % awesome pizza , large slices !
my mom dough i crust enjoyed our saturday morning bagel . 80 % awesome pizza , slices slices ! my mom dough we crust liked our sunday morning bagel .
GWN / NWN -STEM 20 % great food , large stuff !
my family and i really enjoyed our saturday i breakfast . 40 % great food , large stuff !
my i and i really enjoyed our saturday i breakfast . repeated multiple times .
We analyze the average Type Token Ratio ( TTR ) values of each model 's outputs , which is the ratio of unique divided by total words .
As shown in Table 5 , the SMERTI variations achieve the highest TTR , while W2V-STEM and NWN - STEM the lowest .
Note that while W2V-STEM achieves lower CSS than SMERTI , it performs comparably in this aspect .
This is likely due to its vector arithmetic operations algorithm , which replaces each word with one more similar to the RE .
This is also supported by the lower TTR , as W2V-STEM frequently outputs the same words multiple times .
These results correspond well with our automatic evaluation results in Table 2 .
We look at the Pearson correlation values between RE Match , Fluency , and Sentiment Preservation with CSS , SLOR , and SPA , respectively .
These are 0.9952 , 0.9327 , and 0.8768 , respectively , demonstrating that our automatic metrics are highly effective and correspond well with human ratings .
Performance By Model - Human Results
As seen in
SMERTI 's Performance By POS
As seen from Table 6 12 , SMERTI 's SPA values are highest for nouns , likely because they typically carry little sentiment , and lowest for adjectives , likely because they typically carry the most .
SLOR is lowest for adjectives and highest for phrases and nouns .
Adjectives typically carry less semantic meaning and SMERTI likely has more trouble figuring out how best to infill the text .
In contrast , nouns typically carry more , and phrases the most ( since they consist of multiple words ) .
SMERTI 's CSS is highest for phrases then nouns , likely due to phrases and nouns carrying Overall , SMERTI appears to be more effective on nouns and phrases than verbs and adjectives .
SMERTI 's Performance By Dataset
As seen in Table 7 , SMERTI 's SPA is lowest for news headlines .
Amazon and Yelp reviews naturally carry stronger sentiment , likely making it easier to generate text with similar sentiment .
Both SMERTI 's and the input text 's SLOR appear to be lower for Yelp reviews .
This may be due to many reasons , such as more typos and emojis within the original reviews , and so forth .
SMERTI 's CSS values are slightly higher for news headlines .
This may be due to them typically being shorter and carrying more semantic meaning as they are designed to be attention grabbers .
Overall , it seems that using datasets which inherently carry more sentiment will lead to better sentiment preservation .
Further , the quality of the dataset 's original text , unsurprisingly , influences the ability of SMERTI to generate fluent text .
SMERTI 's Performance By MRT / RRT From Table 8 , it can be seen that as MRT / RRT increases , SMERTI 's SPA and SLOR decrease while CSS increases .
These relationships are very strong as supported by the Pearson correlation values of - 0.9972 , -0.9183 , and 0.9078 , respectively .
When SMERTI can alter more text , it has the opportunity to replace more related to sentiment while producing more of semantic similarity to the RE .
Further , SMERTI generates more of the text itself , becoming less similar to the human-written input , resulting in lower fluency .
To further demonstrate this , we look at average SMERTI BLEU ( Papineni et al. , 2002 ) scores against MRT / RRT , shown in Table 8 . BLEU generally indicates how close two pieces of text are in content and structure , with higher values indicating greater similarity .
We report our final BLEU scores as the average scores of 1 to 4 - grams .
As expected , BLEU decreases as MRT / RRT increases , and this relationship is very strong as supported by the Pearson correlation value of - 0.9960 .
It is clear that MRT / RRT represents a trade- off between CSS against SPA and SLOR .
It is thus an adjustable parameter that can be used to control the generated text , and balance semantic exchange against fluency and sentiment preservation .
Conclusion and Future Work
We introduced the task of semantic text exchange ( STE ) , demonstrated that our pipeline SMERTI performs well on STE , and proposed an STES metric for evaluating overall STE performance .
SMERTI outperformed other models and was the most balanced overall .
We also showed a trade- off between semantic exchange against fluency and sentiment preservation , which can be controlled by the masking ( replacement ) rate threshold .
Potential directions for future work include adding specific methods to control sentiment , and fine-tuning SMERTI for preservation of persona or personality .
Experimenting with other text infilling models ( e.g. fine - tuning BERT ( Devlin et al. , 2019 ) ) is also an area of exploration .
Lastly , our human evaluation is limited in size and a larger and more diverse participant pool is needed .
We conclude by addressing potential ethical misuses of STE , including assisting in the generation of spam and fake - reviews / news .
These risks come with any intelligent chatbot work , but we feel that the benefits , including usage in the detection of misuse such as fake-news , greatly outweigh the risks and help progress NLP and AI research .
Figure 1 : 1 Figure 1 : Overall architecture and example , showing the three modules : Entity Replacement ( ERM ) , Similarity Masking ( SMM ) , and Text Infilling ( TIM )
