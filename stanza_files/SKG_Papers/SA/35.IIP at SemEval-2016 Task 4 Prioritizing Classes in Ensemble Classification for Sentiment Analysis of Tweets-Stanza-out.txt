title
IIP at SemEval - 2016 Task 4 : Prioritizing Classes in Ensemble Classification for Sentiment Analysis of Tweets
abstract
This paper describes the submission of team IIP in SemEval - 2016 Task 4 Subtask A .
The presented system is a novel weighted sum ensemble approach for sentiment analysis of short informal texts .
The ensemble combines member classifiers that output classification confidence metrics .
For the ensemble classification decision the members are combined by weights .
In the presented approach the weights are derived to prioritize specific classes in multi-class classification .
The presented results confirm that this improves results for the prioritized classes .
The official task submission achieved a macro-averaged negative positive F1 of 57.4 % .
Post submission changes resulted in a F1 score of 60.2 % .
The evaluation also shows that the system outperforms other ensemble methods .
Introduction
The SemEval workshops offer the opportunity to compete across a variety of natural language processing tasks .
The SemEval - 2016 Task 4 Subtask
A targets message polarity classification of tweets ( Nakov et al. , 2016 ) .
The polarity can be negative , neutral or positive while the submissions are ranked omitting performance on the neutral class .
In practical use cases some classes of a multiclassification problem might be deemed more important than others .
For example some work looks explicitly at negative sentiment ( Tetlock , 2007 ) .
Combining diverse methods has shown success in sentiment analysis .
The combination of machine learners and opinion lexicons has resulted in some of the best submissions in previous SemEval competitions ( Kiritchenko et al. , 2014 ; Miura et al. , 2014 ) .
Along the line of combining different methods , ensemble approaches have also shown top results in previous runs of this task .
Both ensembles of a small number of sophisticated systems ( Hagen et al. , 2015 ) as well as large numbers of simpler approaches have been evaluated ( Wicentowski , 2015 ) .
Ensemble classification with regard to combining different machine learners and feature spaces has previously been evaluated extensively for document level sentiment classification ( Xia et al. , 2011 ) .
In that context , weighted sum ensemble methods have shown the best performance .
This paper describes a weighted sum ensemble that prioritizes some classes in multi-class classification .
Results compare the system against two baselines .
One baseline is the equivalent approach without prioritizing classes , while the other is an unweighted combination of ensemble members .
Naive Bayes and logistic regression classifiers are explored as members across a variety of feature spaces .
These classifiers are know to perform differently ( Ng and Jordan , 2002 ) .
The presented results show : 1 . The presented approach successfully prioritizes classes in a multi-class classification problem .
2 . The ensemble method outperforms individual members and the baseline ensembles .
The system description will start by a brief outline of the evaluation data .
Then the ensemble members are described before the ensemble method is detailed .
Finally , the results on all SemEval test sets allow an assessment of the approach and future work .
Data Training data for this approach is constrained to data provided through the SemEval competitions .
Table 1 shows the evaluation data used in the approach .
This is a subset of the original data , as some tweets were unavailable when querying the Twitter API .
The test data corresponds to the data used in the official task ranking ( Nakov et al. , 2016 ) .
Ensemble Members
The ensemble members are the basic exchangeable building blocks of this approach .
In this work Naive Bayes and logistic regression are chosen as differently performing members .
Naive Bayes
The Naive Bayes classifier is based on the Bayes theorem .
The assumption that features are statistically independent might seem too naive .
However , this approach often performs surprisingly well .
The implementation uses the multinomial Naive Bayes classifier of the datumbox library 1 .
Logistic Regression and Opinion Lexicons Logistic regression is the second classification approach for ensemble members .
Input for this method are text features , as well as scores from five opinion lexicons .
Three lexicons have been created automatically from large corpora , namely SentiWordNet ( Baccianella et al. , 2010 ) , NRC Hashtag Sentiment Lexicon and Sentiment140 Lexicon ( Kiritchenko et al. , 2014 ) . Bing Liu's Opinion Lexicon ( Hu and 1 https://github.com/datumbox/datumbox-framework Liu , 2004 ) was created manually and a fifth lexicon was created automatically and then curated manually .
For each lexicon , the two sums over all negative as well as positive opinion scores corresponding to unigrams or lemmas in a message are added as features .
The implementation uses the logistic regression classifier available in LIBLINEAR ( Fan et al. , 2008 ) .
Feature Extraction User names , URLs and retweet handles are removed before feature extraction .
Part- of-speech tags of the CMU ARK Tagger ( Owoputi et al. , 2013 ) are used for truecasing if words in a tweet are mostly lowercase or mostly capitalized .
ClearNLP ( now NLP4J 2 ) is used for tokenization , part- of-speech tagging and dependency parsing ( Choi and Mccallum , 2013 ; Choi , 2016 ) .
Feature inputs for the classifiers are bag-of-words of unigrams ( uni ) , part- of-speech tags ( pos ) , bigrams ( bi ) , dependency pairs ( dp ) ( Xia et al. , 2011 ) and brown clusters ( cl ) ( Owoputi et al. , 2013 ) .
In this work a classifier only ever uses one of the different text feature sets .
Ensemble Ensemble methods combine a set of multiple member classifiers .
These members can be various classifiers of different methods and different feature sets .
Individual classifiers output a classification decision or a classification score for each class .
In the context of this approach classification scores are required for all ensemble members .
These scores o ki ?
0 for every class i ?
C and every member classifier k ?
M are assumed to be normalized , C i=1 o ki = 1 . ( 1 )
The score o ki can be interpreted as a probability or confidence measure of classifier k for class i .
A basic score based classification method would be to derive the classification decision from the sum over all scores .
The highest accumulated class sum would determine the decision , as in arg max i?C M k=1 o ki .
( 2 ) Instead the approach uses a weighted sum , with a weight w ki for every classifier k and every class i.
Differentiating weights by class represents a finer grained weighting scheme than only differentiating by classifier .
The ensemble output is determined by arg max i?C M k=1 w ki o ki , ( 3 ) as the class with the highest weighted sum .
The essential aspect of a weighed ensemble approach then is how the weights are calculated .
The following sections describe optimization conditions that can be used to calculate weights .
Standard Weight Optimization
A weighted sum ensemble attempts to improve classification by weighing the class scores of individual members .
Weights can be calculated based on a gold dataset where the class scores o ki and the correct gold label g are known .
The decision function ( 3 ) is based on the maximal weighted sum of scores .
It is straight forward that a lower difference between weighted sums of different classes is more prone to an erroneous decision through inaccuracies .
Thus an intuitive condition for optimal weights could be to maximize the difference between the weighted sum for the correct class and all sums of incorrect classes .
For every known gold label g and the corresponding scores o kg the conditions would be M k=1 w kg o kg ?
w kj o kj = | M | , ( 4 ) for all labels besides the gold label , j ? C \{g} .
The unweighted sum of classification scores was defined equal one for a single classifier ( 1 ) .
This would also be the maximal difference in case of one classifier .
Consequently , the conditions for maximal difference between weighted sum scores over the classifier set M are set equal to the cardinality of the set .
Prioritizing Weight Optimization
In contrast to the previously introduced weight optimization conditions the following conditions aim to prioritize valid classification of some classes over others .
Low priority classes are defined by the set L ? C .
This also defines the priority classes as P = C \ L .
The approach does not aim to improve the ensemble classification across low priority classes L. For a low priority label l ?
L the weights are fixed to w kl = 1 .
( 5 ) Based on this , the standard weight conditions ( 4 ) for any low priority gold label g ?
L and all priority labels p ?
P are rephrased as M k=1 w kp o kp = ?
| M | + M k=1 o kg ?
0 . ( 6 ) This is problematic because the unweighted sum over scores is positive by definition , since scores ca n't be negative .
However , the derivation shows that the priority weights w kp would be conditioned to change this sum to negative in favor of low-priority classification decisions .
This is a contradiction to the concept of priority classes .
The conditions ( 6 ) for any low priority gold label g ?
L and all priority labels p ?
P are relaxed to M k=1 w kp o kp = 0 . ( 7 )
This can be understood as a lower bound for priority weights .
Priority weights are also still conditioned to improve priority classification decisions as per the standard conditions ( 4 ) for any g ?
P . Based on the gold dataset the conditions for low priority gold labels ( 7 ) and for priority gold labels ( 4 ) form an overdetermined system of equations .
The solution to this are the priority weights optimized to improve the decision of the ensemble approach .
The weights are calculated by solving the conditions as a least squares problem .
This requires gold labels from a development dataset different from classifier training data .
Results
The following section compares results of the introduced approach against the ensemble members and two baseline ensemble methods .
The results for Naive Bayes and logistic regression ensemble members on different feature spaces as well as the results for the ensembles are presented in the following .
1 ) .
Table 3 shows the results for Naive Bayes ( NB ) and logistic regression ( LR ) ensemble members .
For both classification approaches brown clusters ( cl ) show the best performance .
Table 2 shows results for three ensemble approaches and three sets of members .
The Sum columns show results for an unweighted sum over contributing classifier scores , as in ( 2 ) .
IIP std is a weighted sum approach with standard weight optimization as in ( 4 ) .
IIP prio adds the priority condition ( 7 ) with positive and negative as priority classes .
The bottom result set for weight optimization on 2013 and 2016 development data shows substantially better results than the top one , where weights were optimized on 2016 development data .
While the weighted sum approach is of course unaffected by this , this holds true for all ensemble member sets in the weighted sum ensembles .
Across both result sets the IIP prio ensemble always outperforms the other two baseline ensemble methods .
The standard ensemble which does not prioritize classes IIP std outperforms the sum baseline in the bottom result set but often does not in the top one .
For all ensemble approaches the member sets of classifiers for unigram , bigram and cluster feature spaces usually show the best results .
The system for the official submission * used all members with priority weight optimization , obtain- ing a macro-averaged F1 of 57.4 % .
Though this outperforms the equivalent baseline ensembles it performs on a similar level as the best logistic regression member in Table 3 .
In contrast the best IIP prio result used unigram , bigram and cluster members in an ensemble optimized for 2013 and 2016 development data , achieving a macro-averaged F1 of 60.2 % .
Conclusion
This paper presented two methods for weighted sum ensemble classification .
The introduced class prioritizing method outperformed the standard method in all evaluations .
Furthermore , the results show that the class prioritizing weight ensemble method usually outperformed the basic sum ensemble approach substantially .
This shows that combining different classifiers across various feature spaces while prioritizing some classes in multi-class classification works well with the presented system .
The results varied significantly depending on the data used for optimizing weights .
Optimization on a more diverse data set showed better performance .
Questions of domain dependence and over-fitting need to be explored further .
With the modular nature of an ensemble a variety of classifiers and features are left to be evaluated in the context of this approach .
Table 2 : 2 Macro-averaged positive negative F1 [ % ] for all test data sets across three ensemble methods and three member sets .
Set all corresponds to all classifier , feature combinations evaluated for 2016 - test data in Table3 .
Ensemble members were trained on the full 2016 training set ( A ) while ensemble weights were optimized on 2016 ( B ) and 2013/ 2016 development sets ( C , Table1 ) .
Test , weight data 2013 - test , B 2014 -test , B 2015 -test , B 2016 - test , B Sum uni-bi uni-bi-cl 55.5 56.8 61.6 62.9 57.4 59.4 56.1 58.0 all 55.7 61.3 58.3 56.6 IIP std uni-bi uni-bi-cl 55.1 55.4 59.5 60.5 58.5 57.6 55.0 56.0 all 53.7 59.4 56.8 55.4 IIP prio uni-bi uni-bi-cl 60.5 60.5 65.3 65.9 63.3 62.9 58.3 58.7 all 59.8 64.5 61.0 57.4 * 2013 -test , C 2014-test , C 2015-test , C 2016 -test , C 55.5 61.6 57.4 56.1 56.8 62.9 59.4 58.0 55.7 61.3 58.3 56.6 59.7 64.7 60.7 58.2 59.4 66.1 62.3 59.0 59.7 65.3 61.3 58.8 63.6 68.0 64.1 59.4 64.3 69.9 66.2 60.2 64.6 68.5 65.5 59.5 2016 - test uni NB 54.5 30.3 40.7 37.6 54.4 pos bi dp cl LR 55.5 53.5 52.0 52.7 57.6
