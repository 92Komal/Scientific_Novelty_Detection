title
Relation - Aware Collaborative Learning for Unified Aspect - Based Sentiment Analysis
abstract
Aspect - based sentiment analysis ( ABSA ) involves three subtasks , i.e. , aspect term extraction , opinion term extraction , and aspect-level sentiment classification .
Most existing studies focused on one of these subtasks only .
Several recent researches made successful attempts to solve the complete ABSA problem with a unified framework .
However , the interactive relations among three subtasks are still underexploited .
We argue that such relations encode collaborative signals between different subtasks .
For example , when the opinion term is " delicious " , the aspect term must be " food " rather than " place " .
In order to fully exploit these relations , we propose a Relation - Aware Collaborative Learning ( RACL ) framework which allows the subtasks to work coordinately via the multi-task learning and relation propagation mechanisms in a stacked multi-layer network .
Extensive experiments on three real-world datasets demonstrate that RACL significantly outperforms the state- ofthe - art methods for the complete ABSA task .
Introduction Aspect- based sentiment analysis ( ABSA ) is a finegrained task which aims to summarize the opinions of users towards specific aspects in a sentence .
ABSA normally involves three subtasks , namely aspect term extraction ( AE ) , opinion term extraction ( OE ) , and aspect-level sentiment classification ( SC ) .
For example , given a review " The place is small and cramped but the food is delicious . " , AE aims to extract a set of aspect terms { " place " , " food " } .
OE aims to extract a set of opinion terms { " small " , " cramped " , " delicious " } .
Meanwhile , it is expected for SC to assign a sentiment polarity " negative " and " positive " to the aspect " place " and " food " , respectively .
* Corresponding author .
Most existing works treat ABSA as a two -step task containing AE and SC .
They develop one separate method for each subtask ( Tang et al. , 2016 ; Xu et al. , 2018 ; Li et al. , 2018a ; Hu et al. , 2019 ) , or take OE as an auxiliary task of AE Li et al. , 2018 b ) .
In order to perform ABSA for practical use , the separate methods need to be pipelined together .
Recently , several studies attempt to solve ABSA in a unified framework ( Wang et al. , 2018a ;
He et al. , 2019 ; Luo et al. , 2019 ) .
Despite their effectiveness , we argue that these methods are not sufficient to yield satisfactory performance for the complete ABSA task .
The key reason is that the interactive relations among different subtasks have been largely neglected in existing studies .
These relations convey collaborative signals which can enhance the subtasks in a mutual way .
For example , the opinion term " delicious " can serve as the evidence of the aspect term " food " , and vice versa .
In the following , we first analyze the interactive relations among different subtasks , and then present our RACL framework which is developed to exploit these relations .
The detailed relations are summarized in Figure 1 ( left ) , where each arrow ? denotes one specific relation R i . ?
R 1 indicates the dyadic relation between AE and OE .
In practice , the aspect terms must be the targets of opinion , indicating that most aspect terms like " place " can only be modified by corresponding opinion terms like " small " and " cramped " rather than a term like " delicious " .
Hence AE and OE might hold informative clues to each other . ?
R 2 indicates the triadic relation between SC and R 1 .
One critical problem in SC is to determine the dependency between the aspect and its context .
For example , the context " small and cramped " plays an important role in predicting the polarity of " place " .
Such a dependency is highly in accordance with R 1 which emphasizes the interaction between the aspect and opinion terms .
Hence SC and R 1 can help refine the selection process for each other . ?
R 3 indicates the dyadic relation between SC and OE .
The specific opinion terms generally convey specific polarities .
For example , " fantastic " is often positive .
The opinion terms extracted in OE should be paid more attention when predicting the sentiment polarity in SC .
?
R 4 indicates the dyadic relation between SC and AE .
In the complete ABSA task , the aspect terms are unknown and SC will assign a polarity to every word .
The aspect terms , e.g. , " place " and " food " , will have their corresponding polarities , while other words are considered as the background ones without sentiment .
That is to say , the results from AE should be helpful in supervising the training of SC .
When reviewing the literature on the ABSA task , we find that existing separate methods either do not utilize any relations , or only utilize R 1 by treating OE as an auxiliary task of AE .
Meanwhile , the unified methods at most explicitly utilize R 3 and R 4 .
In view of this , we propose a novel Relation - Aware Collaborative Learning ( RACL ) framework to fully exploit the interactive relations in the complete ABSA task .
We compare our model with existing methods by their capability in utilizing interactive relations in Table 1 . RACL is a multi-layer multi-task learning framework with a relation propagation mechanism to mutually enhance the performance of subtasks .
For multi-task learning , RACL adopts the sharedprivate scheme ( Collobert and Weston , 2008 ; Liu et al. , 2017 ) . Subtasks AE , OE , and SC first jointly train the low-level shared features , and then they train their high - level private features independently .
In this way , the shared and private features can embed the task - invariant and task - oriented knowledge respectively .
For relation propagation , RACL improves the model capacity by exchanging informative clues among three subtasks .
Moreover , RACL can be stacked to multiple layers to perform collaborative learning at different semantic levels .
We conduct extensive experiments on three datasets .
Results demonstrate that RACL significantly outperforms the state - of - the - art methods for both the single subtasks and the complete ABSA task .
Related Work Aspect- based sentiment analysis ( ABSA ) is first proposed by Hu and Liu ( 2004 ) and has been widely studied in recent years ( Zhang et al. , 2018 ) .
We organize existing studies by how the subtasks are performed and combined to perform ABSA .
Separate Methods
Most existing studies treat ABSA as a two -step task containing aspect term extraction ( AE ) and aspect- based sentiment classification ( SC ) , and develop separate methods for AE ( Popescu and Etzioni , 2005 ; Wu et al. , 2009 ; Li et al. , 2010 ; Qiu et al. , 2011 ; Liu et al. , 2012 ; Chen et al. , 2014 ; Chernyshevich , 2014 ; Toh and Wang , 2014 ; Vicente et al. , 2015 ; Liu et al. , 2015
Yin et al. , 2016 ; Wang et al. , 2016 ; Li and Lam , 2017 ; Clercq et al. , 2017 ; He et al. , 2017 ; Xu et al. , 2018 ; Yu et al. , 2019 ) , and SC ( Jiang et al. , 2011 ; Mohammad et al. , 2013 ; Kiritchenko et al. , 2014 ; Dong et al. , 2014 ; Ma et al. , 2017 ; Wang et al. , 2018 b ; Zhu and Qian , 2018 ; Zhu et al. , 2019 ) , respectively .
Some of them resort to the auxiliary task opinion term extraction ( OE ) and exploit their relation for boosting the performance of AE .
For the complete ABSA task , results from two steps must be merged together in a pipeline manner .
In this way , the relation between AE / OE and SC is totally neglected , and the errors from the upstream AE / OE will be propagated to the downstream SC .
The overall performance of ABSA task is not promising for pipeline methods .
Unified Methods Recently , several studies attempt to solve ABSA task in a unified framework .
The unified methods fall into two types : collapsed tagging ( Mitchell et al. , 2013 ; Wang et al. , 2018a ; and joint training ( He et al. , 2019 ; Luo et al. , 2019 ) .
The former combines the labels of AE and SC to construct collapsed labels like { B-senti , I-senti , O} .
The subtasks need to share all trainable features without distinction , which is likely to confuse the learning process .
Moreover , the relations among subtasks cannot be explicitly modeled for this type of methods .
Meanwhile , the latter constructs a multi-task learning framework where each subtask has independent labels and can have shared and private features .
This allows the interactive relations among different subtasks to be modeled explicitly for the joint training methods .
However , none of existing studies along this line has fully exploited the power of such relations .
We differentiate our work from aforementioned methods in that we propose a unified framework which exploits all dyadic and triadic relations among subtasks to enhance the learning capability .
?
SC aims to predict a tag sequence Y S = {y S 1 , ... , y S i , ... , y S n } for sentiment classification , where y S i ?
{ pos , neu , neg} denotes the positive , neutral , and negative sentiment polarities towards each word .
Methodology
Model Architecture
Our proposed RACL is a unified multi-task learning framework which enables propagating the interactive relations ( denoted as the same R 1 ..R 4 as those in Figure 1 ) for improving the ABSA performance , and it can be stacked to multiple layers to interact subtasks at different semantic levels .
We present the overall architecture of RACL in In particular , a single RACL layer contains three modules : AE , OE , and SC , where each module is designed for the corresponding subtask .
These modules receive a shared representation of the input sentence , then encode their task - oriented features .
After that , they propagate relations R 1 ..R 4 for collaborative learning by exchanging informative clues to further enhance the task - oriented features .
Finally , three modules will make predictions for the corresponding tag sequences Y A , Y O , and Y S based on the enhanced features .
In the following , we first illustrate the relationaware collaborative learning in one layer , then show the stacking and the training of the entire RACL .
Relation - Aware Collaborative Learning Input Word Vectors Given a sentence S e , we can map the word sequence in S e with either pretrained word embeddings ( e.g. , GloVe ) or pretrained language encoders ( e.g. , BERT ) to generate a sequence of word vectors E={e 1 , ... , e i , ... , e n } ?
R dw ?n , where d w is the dimension of word vectors .
We will examine the effects of these two types of word vectors in the experiments .
Multi-task Learning with Shared -Private Scheme
To perform multi-task learning , different subtasks should focus on the different characteristics of a shared training sample .
Inspired by the shared - private scheme ( Collobert and Weston , 2008 ; Liu et al. , 2017 ) , we extract both the shared and private features to embed task - invariant and task - oriented knowledge for the AE , OE , and SC modules .
To encode the shared task - invariant features , we simply feed each e i in E into a fully - connected layer and generate a transformed vector h i ?
R d h .
We then obtain a sequence of shared vectors H={h 1 , ... , h i , ... , h n } ?
R d h ?n for each sentence which will be jointly trained by all subtasks .
Upon the shared task - invariant features H , the AE , OE , and SC modules will encode the taskoriented private features for the corresponding subtasks .
We choose a simple CNN as the encoder function F due to its high computation efficiency .
For subtasks AE and OE , the key features for determining the existence of aspect and opinion terms are the representations of the original and adjacent words .
Therefore , we construct two encoders to extract local AE -oriented features X A and OE-oriented features X O : F A : H ? X A , X A ? R dc? n , F O : H ? X O , X O ? R dc ? n ( 1 ) For subtask SC , the process of feature generation is different from that in AE / OE .
In order to determine the sentiment polarity towards an aspect term , we need to extract related semantic information from its context .
The critical problem in SC is to determine the dependency between an aspect average pooling AE OE Y A ( 1 ) Y S( 1 ) Y O( 1 ) Y A ( 2 ) Y S( 2 ) Y O( 2 ) Y A( L ) Y S( L ) Y O( L ) layer ( 2 ) layer ( 1 ) layer ( L ) ... term and its context .
Moreover , in the complete ABSA task , the aspect terms are unknown in SC and it needs to assign a polarity to every word in S e .
Based on these observations , we first encode the contextual features X ctx from H : Y A Y S Y O SC AE OE SC AE OE SC ( a) Architecture with L stacked RACL layers .
X O X A X A2O X O2A M O2A Next Y A Y S SC X S R 4 R 4 R 3 R 3 R 2 R 2 R 1 R 1 Y O Y O M O2A M O2A M ctx FC FC Y O FC + OE Y O AE M A2O Input X ctx + Next Next Y A ?A ?nput Input ( b) Details of a single RACL layer .
F ctx : H ? X ctx , X ctx ? R d h ?n ( 2 ) Then we treat the shared vector h i as the query aspect and compute the semantic relation between the query and contextual features using the attention mechanism : ds ( i =j ) i , j = ( ( h i ) T ? X ctx j ) ? [ log2 ( 2 + | i ? j| ) ] ?1 , M ctx i , j = exp( dsi , j ) n k=1 exp ( ds i , k ) , ( 3 ) where ds ( i =j ) i , j denotes the dependency strength between the i-th query word and the j-th context word , and M ctx i , j is the normalized attention weight of ds ( i =j ) i , j .
We add a coefficient [ log 2 ( 2 +| i? j | ) ] ?1 based on the absolute distance between two words .
The rationale is that the adjacent context words should contribute more to the sentiment polarity .
Finally , for the aspect query w i , we can obtain the global SC - oriented features X S i by a weighted sum of all contextual features ( except the one for w i ) : X S i = n j=1 ( M ctx i , j ? X ctx j ) ( 4 ) Propagating Relations for Collaborative Learning After encoding task - oriented features , we propagate the interactive relations ( R 1 ..R 4 ) among subtasks to mutually enhance the AE , OE , and SC modules . ( 1 ) R 1 is the dyadic relation between AE and OE , which indicates that AE and OE might hold informative clues to each other .
In order to model R 1 , we want the AE - oriented features X A and the OE - oriented features X O to exchange useful information based on their semantic relations .
Take the subtask AE as an example , the semantic relation between the word in AE and that in OE is defined as follows : sr ( i =j ) i , j = ( X A i ) T ? X O j , M O2A i , j = exp( sri , j ) n k=1 exp( sr i , k ) ( 5 ) For the word w i in AE , we can obtain the useful clues X O2A i from OE by applying a weighted sum of semantic relations to all words in OE ( except the word w i itself ) , i.e. , X O2A i = n j=1 ( M O2A i , j ? X O j ) ( 6 ) We then concatenate the original AE - oriented features X A and the useful clues X O2A from OE as the final features for AE , and feed them into a fullyconnected layer to predict the tags of aspect terms : Y A = sof tmax ( W A ( X A ? X O2A ) ) , ( 7 ) where W A ? R 3?2dc is a transformation matrix , Y A ? R 3 ?n is the predicted tag sequence of AE .
For subtask OE , we use the transposed matrix of sr ( i =j ) i , j in Eq. 5 to compute the corresponding M A2O .
In this way , the semantic relation between AE and OE will be consistent without regard to the direction .
Then we can obtain the useful clues X A2O from AE and generate the predicted tag sequence Y O ? R 3 ?n in a similar way , i.e. , Y O = sof tmax ( W O ( X O ? X A2O ) ) ( 8 )
Additionally , each w i cannot be an aspect term and an opinion term at the same time , so we add a regularization hinge loss to constrain Y A and Y O : L R = n i=1 max ( 0 , P y A i ?{ B , I } + P y O i ?{ B , I } ?1.0 ) , ( 9 ) where P denotes the probability under the given conditions . ( 2 ) R 2 is the triadic relation between SC and R 1 .
Remember that the dependency between the aspect term and its context is critical for subtask SC , and we have already calculated this dependency using the normalized attention weight M ctx .
Hence we can model R 2 by propagating R 1 to M ctx .
We use M O2A as the representative of R 1 , and add it on M ctx to denote the influence from R 1 to SC .
More formally , we define R 2 as the following operation : M ctx i , j ? M ctx i , j + M O2A i , j ( 10 ) Actually , M O2A characterizes the dependency between aspect terms and contexts in the view of term extraction while M ctx characterizes it in the view of sentiment classification .
The dual-view relation R 2 can help refine the selection processes for both extraction and classification subtasks . ( 3 ) R 3 is the dyadic relation between SC and OE , which indicates that the extracted opinion terms should be paid more attention when predicting the sentiment polarity .
In order to model R 3 , similarly to the method for R 2 , we update M ctx in SC using the generated tag sequence Y O from OE : M ctx i , j ? M ctx i , j + P y O j ?{ B , I } ? [ log2 ( 2 + | i ? j| ) ] ?1 ( 11 )
By doing this , the opinion terms can get larger weights in the attention mechanism .
Consequently , they will contribute more to the prediction of the sentiment polarity .
After getting the interacted values for M ctx , we can recompute the SC - oriented features X S in Eq.4 accordingly .
Then we concatenate H and X S as the final features for SC and feed them into a fullyconnected layer to predict sentiment polarities for the candidate aspect terms : Y S = sof tmax ( W S ( H ? X S ) ) , ( 12 ) where W S ? R 3 ? 2d h is a transformation matrix , Y S ? R 3 ?n is the predicted tag sequence of SC . ( 4 ) R 4 is the dyadic relation between SC and AE , which indicates that the results from AE are helpful in supervising the training of SC .
Clearly , only aspect terms have sentiment polarities .
Although SC needs to assign a polarity to every word , we know the ground truth aspect terms in AE during the training process .
Therefore , we directly use the ground truth tag sequence ?
A of AE to refine the labeling process in SC .
Specifically , only the predicted tags towards true aspect terms would be counted in the training procedure : y S i ? I ( ?
A i ) ? y S i , ( 13 ) where I ( ?
A i ) equals to 1 if w i is an aspect term and to 0 if not .
Notice that this approach is only used in the training procedure .
Stacking RACL to Multiple Layers
When using one single RACL layer , AE , OE , and SC modules only extract corresponding features in a relatively low linguistic level , which may be insufficient to serve as the evidence to label each word .
Hence we stack RACL to multiple layers to obtain high- level semantic features for subtasks , which helps to conduct deep collaborative learning .
Specifically , we first encode features X ctx ( 1 ) , X A ( 1 ) ?X O2A ( 1 ) , and X O ( 1 ) ?X A2O ( 1 ) in layer ( 1 ) .
Then in layer ( 2 ) , we input these features for SC , AE , and OE to generate X ctx ( 2 ) , X A ( 2 ) , and X O ( 2 ) .
In this way , we can stack RACL to L layers .
We then conduct average pooling on results from all layers to obtain the final prediction : Y T = avg ( [ Y T ( 1 ) , Y T ( 2 ) , ... , Y T ( L ) ] ) , ( 14 ) where T ? { A , O , S} denotes the specific subtask , and L is the number of layers .
This shortcut - like architecture can enforce the features in the low layers to be meaningful and informative , which in turn helps the high layers to make better predictions .
Training Procedure
After generating the tag sequences Y A , Y O , and Y S for the sentence S e , we compute the crossentropy loss of each subtask : L T = ? n i=1 J j=1 ?T ij ? log(y T ij ) , ( 15 ) where T ? { A , O , S} denotes the subtask , n is the length of S e , J is the category of labels , y T i and ?T i are the predicted tags and ground truth labels .
The final loss L of RACL is the combination of the loss for subtasks and the loss for regularization , i.e. , L = L T + ? ?
L R , where ? is a coefficient .
We then train all parameters with back propagation .
Experiments
Datasets and Settings Datasets
We evaluate RACL on three real- world ABSA datasets from SemEval 2014 ( Pontiki et al. , 2014 ) and 2015 ( Pontiki et al. , 2015 ) , which include reviews from two domains : restaurant and laptop .
Original datasets only have ground truth labels for aspect terms and corresponding sentiment polarities , while labels for opinion terms are annotated by two previous works ( Wang et al. , 2016 .
All datasets have a fixed training / test split .
We further randomly sample 20 % training data as the development set to tune hyper-parameters , and only use the remaining 80 % for training .
The statistics for datasets are summarized in Table 2 . Settings
We examine RACL with two types of word vectors : the pre-trained word embedding and pre-trained language encoder .
In the word embedding implementation , we follow the previous studies ( Xu et al. , 2018 ; He et al. , 2019 ; Luo et al. , 2019 ) and use two types of embeddings , i.e. , general - purpose and domain-specific embeddings .
The former is from GloVe vectors with 840B tokens ( Pennington et al. , 2014 ) , and the latter is trained on a large domain-specific corpus using fastText and published by Xu et al . ( 2018 ) .
Two types of embeddings are concatenated as the word vectors .
In the language encoder implementation , we follow Hu et al . ( 2019 ) by using the BERT Large ( Devlin et al. , 2019 ) as the backbone and fine-tuning it during the training process .
We denote these two implementations as RACL - GloVe and RACL - BERT 1 . For RACL - GloVe , we set the dimension d w = 400 , d h = 400 , d c = 256 and the coefficient ?=1e -5 .
Other hyper-parameters are tuned on the development set .
The kernel size K of CNN and the layer number L is set to { 3,3,5 } and { 4,3,4 } for three datasets , respectively .
We train the model for fixed epochs using Adam optimizer ( Kingma and Ba , 2015 ) with learning rate 1e - 4 and batch size 8 .
For RACL - BERT , we set d w to 1024 and learning rate to 1e - 5 for fine- tuning BERT , and other hyper-parameters are directly inherited from RACL - GloVe .
We use four metrics for evaluation , i.e. , AE -F 1 , OE -F 1 , SC -F 1 , and ABSA - F 1 .
The first three denote the F 1 - score of each subtask , while the last one measures the overall performance for complete ABSA 2 .
To compute ABSA -F 1 , the result for an aspect term would be considered as correct only when both AE and SC results are correct .
The model achieving the minimum loss on the development set is used for evaluation on the test set .
Baselines
To demonstrate the effectiveness of RACL for the complete ABSA task , we compare it with the following pipeline and unified baselines .
The hyper-parameters for baselines are set to the optimal values as reported in their papers .
? { CMLA , DECNN }+ { TNet , TCap} : CMLA and DECNN ( Xu et al. , 2018 ) are the state - of - the - art methods for AE , while TNet ( Li et al. , 2018a ) and T( rans ) Cap are the top-performing methods for SC .
We then construct four pipeline baselines through combination .
? MNN ( Wang et al. , 2018a ) : is a unified method utilizing the collapsed tagging scheme for AE and SC .
? E2E- ABSA : is a unified method using the collapsed tagging scheme for AE and SC , and it introduces the auxiliary OE task without explicit interaction .
? DOER ( Luo et al. , 2019 ) : is a multi-task unified method which jointly trains AE and SC , and it explicitly models the relation R 4 . ? IMN -D ( He et al. , 2019 ) : is a unified method involving joint training for AE and SC with separate labels .
The OE task is fused into AE to construct five -class labels .
It explicitly models relations R 3 and R 4 3 . ? SPAN -BERT
( Hu et al. , 2019 ) : is a pipeline method using BERT Large as the backbone .
A multi-target extractor is used for AE , then a polarity classifier is used for SC .
? IMN - BERT : is an extension of the best unified baseline IMN -D with BERT Large .
By doing this , we wish to conduct convincing comparisons for the BERT - style methods .
The input dimension and learning rate of IMN - BERT are the same as our RACL - BERT , and other hyper-parameters are inherited from IMN -D .
Comparison Results
The comparison results for all methods are shown in Table 3 .
The methods are divided into three groups : M1 ?
M4 are GloVe-based pipeline methods , M5 ?
M9 are GloVe-based unified methods , and M10 ?
M12 are BERT - based methods .
Firstly , among all GloVe-based methods ( M1 ? M9 ) , we can observe that RACL - GloVe consistently outperforms all baselines in terms of He et al . ( 2019 ) , while other results are the average scores of 5 runs with random initialization . " - " denotes that the method does not contain the subtask OE .
the overall metric ABSA -F 1 , and achieves 2.12 % , 2.92 % , and 2.40 % absolute gains over the strongest baselines on three datasets .
The results prove that jointly training all subtasks and comprehensively modeling the interactive relations are critical for improving the performance of the complete ABSA task .
Moreover , RACL - GloVe also achieves the best or second best results on all subtasks .
This further demonstrates that the learning process of each subtask can be enhanced by the collaborative learning .
Another observation from M1 ?
M9 is that the unified methods ( M5 ? M9 ) perform better than the pipeline ones ( M1 ? M4 ) .
Model Restaurant14 ( Res14 ) Laptop14 ( Lap14 ) Restaurant15 ( Res15 ) AE -F1 OE-F1 SC-F1 ABSA -F1 AE-F1 OE-F1 SC-F1 ABSA -F1 AE-F1 OE-F1 SC-F1 ABSA -F1 M1 CMLA + TNet Secondly , among the GloVe-based unified methods , RACL - GloVe , IMN -D , and DOER perform better than MNN and E2E-TBSA in general .
This can be due to the fact that the former three methods explicitly model interactive relations among subtasks while the latter two do not .
We notice that DOER gets a poor SC - F 1 score .
The reason might be that it utilizes an auxiliary sentiment lexicon to enhance the words with " positive " and " negative " sentiment .
It is hard for DOER to handle words with " neutral " sentiment and this results in a low SC - F 1 score .
Thirdly , the BERT - based methods ( M10 ?
M12 ) achieve a better performance than GloVe-based methods by utilizing the large-scale external knowledge encoded in the pre-trained BERT Large backbone .
Specifically , SPAN - BERT is a strong baseline in subtask AE by reducing the search space with a multi-target extractor .
However , its performance on SC drops a lot because it cannot capture the dependency between the extracted aspect terms in AE and the opinion terms in SC without interactions among subtasks .
IMN - BERT achieves relatively high scores on OE and SC , but its performance on AE is the worst among three without the guidance from the relations R 1 and R 2 .
In contrast , RACL - BERT gets significantly better overall scores than SPAN - BERT and IMN - BERT on all three datasets .
This again shows the superiority of our RACL framework for the complete ABSA task by using all interactive relations .
Analysis
Ablation Study
To investigate the effects of different relations on RACL - GloVe / - BERT , we conduct the following ablation study .
We sequentially remove each interactive relation and obtain four simplified variants .
As expected , all simplified variants in Table 4 have a performance decrease of ABSA - F 1 .
The results clearly demonstrate the effectiveness of the proposed relations .
Moreover , we find that the relations play more important roles on small datasets than on large ones .
The reason might be that it is hard to train a complicated model on small datasets , and the relations can absorb external knowledge from other subtasks .
Effects of Hyper-Parameters
There are two key hyper-parameters in our model : the kernel size K of the CNN encoder and the layer number L .
To investigate their impacts , we first vary K in the range of [ 1 , 9 ] stepped by 2 while fixing L to the values in section 4.1 , and then vary L in the range of [ 1 , 7 ] stepped by 1 while fixing K. Table 5 : Case study .
The columns " AE +SC " and " OE " denote the results generated by corresponding subtasks , where " None " denotes that no aspect / opinion terms are extracted .
Words in blue and italic are annotated opinion terms , and those in red are annotated aspect terms with the subscripts denoting their sentiment polarities .
[ Sushi ] pos so fresh that it crunches in your mouth .
[ Sushi ] neg ( ) fresh [ Sushi ] pos fresh [ Sushi ] pos fresh
We only present the ABSA -F 1 results for RACL - GloVe in Figure 3 since the hyper-parameters of RACL - BERT are inherited from RACL - GloVe .
In Figure 3 ( a ) , K=1 yields extremely poor performance because the raw features are generated only by the current word .
Increasing K to 3 or 5 can widen the receptive field and remarkably boosts the performance .
However , when further increasing K to 7 or 9 , many irrelevant words are added as noises and thus deteriorate the performance .
In Figure 3 ( b ) , increasing L can , to some extent , expand the learning capability and achieve high performance .
However , too many layers introduce excessive parameters and make the learning process over complicated .
Case Study
This section details the analysis on results of several examples by different methods for a case study .
We choose CMLA + TCap ( denoted as PIPELINE ) , IMN -D , and RACL - GloVe as three competitors .
We do not include the BERT - based methods as we wish to investigate the power of the models without the external resources .
S1 and S2 verify the effectiveness of relation R 1 .
In S1 , due to the existence of the conjunction " and " , two baselines incorrectly extract " offers " as an opinion term as " easy " .
In contrast , RACL - GloVe can successfully filter out " offers " in OE by using R 1 .
The reason is that " offers " has never co-occured as an opinion term with the aspect term " OS " in the training set , and R 1 which connects the AE and OE subtasks will treat them as irrelevant terms .
This information will be passed to OE subtask during the testing phase .
Similarly , in S2 , both baselines fail to recognize " looking " as an aspect term , because it might be the present participle of " look " without opinion information .
Instead , RACL - GloVe correctly labels it as R 1 provides useful clues from opinion terms " faster " and " sleeker " .
S3 shows the superiority of relation R 2 which is critical to connect the three subtasks but has never been employed in previous studies .
Both baselines successfully extract " Dessert " and " die for " for AE and OE , but assign the incorrect " neutral " sentiment polarity even if IMN - D has emphasized the opinion terms .
The reason is that these two terms have not co-occurred in the training samples , and it is hard for SC to recognize their dependency .
In contrast , since " Dessert " and " die for " are typical words in AE and OE , RACL - GloVe is able to encode their dependency in R 1 .
By propagating R 1 to SC using R 2 , RACL - GloVe can assign a correct polarity for " Dessert " .
To take a close look , we visualize the averaged predicted results ( left ) and the attention weights ( right ) of all layers in Figure 4 .
Clearly , the original attention M ctx ? bef ore of " Dessert " does not concentrate on " die for " .
After getting enhanced by M O2A and OE , M ctx ? af ter successfully highlights the opinion words and SC makes a correct prediction .
S4 shows the benefits from relation R 3 . IMN -D and RACL - GloVe assign a correct polarity towards " Sushi " in SC since they both get the guidance from " fresh " in OE , while PIPELINE gets lost in contexts and makes a false prediction without the help of the opinion term .
Notice that S1 ?
S4 simultaneously demonstrate the necessity for R 4 , since RACL - GloVe is not biased by background words and can make correct sentiment predictions in all examples .
Analysis on Computational Cost
To demonstrate that our RACL model does not incur the high computational cost , we compare it with two strong baselines DOER and IMN - D in terms of the parameter number and running time .
We run three models on the Restaurant 2014 dataset with the same batch size 8 in a single 1080 Ti GPU , and present the results in Table 6 .
Obviously , our proposed RACL has similar computational complexity with IMN -D , and they are both much simpler than DOER .
Conclusion
In this paper , we highlight the importance of interactive relations in the complete ABSA task .
In order to exploit these relations , we propose a Relation - Aware Collaborative Learning ( RACL ) framework with multi-task learning and relation propagation techniques .
Experiments on three realworld datasets demonstrate that our RACL framework with its two implementations outperforms the state - of - the - art pipeline and unified baselines for the complete ABSA task .
Figure 1 : Interactive relations among subtasks in ABSA ( left ) , and a list of abbreviations ( right ) .
