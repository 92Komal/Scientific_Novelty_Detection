title
UNIMELB at SemEval - 2016
Tasks 4A and 4B : An Ensemble of Neural Networks and a Word2Vec Based Model for Sentiment Classification
abstract
This paper describes our sentiment classification system for microblog-sized documents , and documents where a topic is present .
The system consists of a softvoting ensemble of a word2vec language model adapted to classification , a convolutional neural network ( CNN ) , and a longshort term memory network ( LSTM ) .
Our main contribution consists of a way to introduce topic information into this model , by concatenating a topic embedding , consisting of the averaged word embedding for that topic , to each word embedding vector in our neural networks .
When we apply our models to SemEval 2016
Task 4 subtasks A and B , we demonstrate that the ensemble performed better than any single classifier , and our method of including topic information achieves a substantial performance gain .
According to results on the official test sets , our model ranked 3rd for ?
PN in the message-only subtask A ( among 34 teams ) and 1st for accuracy on the topic-dependent subtask B ( among 19 teams ) .
Introduction
The rapid growth of user- generated content , much of which is sentiment - laden , has fueled an interest in sentiment analysis ( Pang and Lee , 2008 ; Liu , 2010 ) .
One popular form of sentiment analysis involves classifying a document into discrete classes , depending on whether it expresses positive or negative sentiment ( or neither ) .
The classification can also be dependent upon a particular topic .
In this work , we describe the method we used for the sentiment classification of tweets , with or without a topic .
Our approach to the document classification task consists of an ensemble of 3 classifiers via soft-voting , 2 of which are neural network models .
One is the convolutional neural network ( CNN ) architecture of Kim ( 2014 ) , and another is a Long Short Term Memory ( LSTM ) - based network ( Hochreiter and Schmidhuber , 1997 ) .
Both were first tuned on a distant - labelled data set .
The third classifier adapted word2vec to output classification probabilities using Bayes ' formula , a slightly modified version of Taddy ( 2015 ) .
Despite the word2vec classifier being intended as a baseline , and having a small weight in the ensemble , it proved crucial for the ensemble to work well .
To adapt our models to the case where a topic is present , in the neural network models , we concatenated the embedding vectors for each word with a topic embedding , which consisted of the element - wise average of all word vectors in a particular topic .
We applied our approach to SemEval 2016 Task 4 , including the message-only subtask ( Task A ) and the topic-dependent subtask ( Task B ) ( Nakov et al. , to appear ) .
Our model ranked third for ?
PN in the message-only subtask A ( among 34 teams ) and first for accuracy 1 on the topic-dependent subtask B ( among 19 teams ) .
The source code for our approach is available at https://github.com/stevenxxiu/senti.
Models
We now describe the classifiers we used in detail , our ensemble method , and our motivations behind choosing these classifiers .
Convolutional neural network
We used the dynamic architecture of Kim ( 2014 ) for our convolutional neural network .
This consists of a single 1 - d convolution layer with a nonlinearity , a max-pooling layer , a dropout layer , and a softmax classification layer .
This model was chosen since it was a good performer empirically .
However , due to maxpooling , this model is essentially a bag-ofphrases model , which ignores important ordering information if the tweet contains a long argument , or 2 sentences .
We now give a review of the layers used .
Word embedding layer
The input to the model is a document , treated as a sequence of words .
Each word can possibly be represented by a vector of occurrences , a vector of counts , or a vector of features .
A vector of learnt , instead of hand -crafted features , is also called a word embedding .
This tends to have dimensionality ? ? |? | , the vocabulary size .
Hence the vectors are dense , which allows us to learn more functions of features with limited data .
Given an embedding of dimension ? , ? emb ? R ?|? | , we mapped map each document ? to a matrix ? emb , ? ? R ?|?| , with each word corresponding to a row vector in the order they appear in .
? emb can be trained .
1 -d convolution layer A 1 -d convolution layer aims to extract patterns useful for classification , by sliding a fixedlength filter along the input .
The convolution operation for an input matrix ? ? R ?|?| and a single filter ? ? R ? of width ? creates a feature ? conv ? R |?|+?1 by : ? conv , ? = ? ? , ? ( ? [:,?:?+?1 ] ? ? ) ? , ? + ? conv where ? is element - wise multiplication , and ? conv is a bias .
There are typically ? > 1 filters , which by stacking the feature vectors , results in ?1 ) .
Each filter has its own separate bias .
? conv ? R ?(|?|+?
We used a common modification to the filter sliding , by padding the document embedding matrix with ? ?
1 zeroes on its top and bottom .
This is done so that every word in the document is covered by ? filters .
Max pooling layer
There may be very few phrases targeted by a feature map in the document .
For this reason , we only need to know if the desired feature is present in the document , which can be obtained by taking the maximum .
Formally , we obtain a vector ? ? R ? , such that : ? pool , ? = max ? ? conv , ? , ?
Softmax layer
To convert our features into classification probabilities , we first use a dense layer , defined by : ? dense = ? dense ? ? pool + ? dense with a softmax activation function : softmax ( ? ) ? = ? ? ? ? ? ? ? ? such that the output dimension is the same as the number of classes .
Note that output values are non-negative and sum to 1 , which form a discrete probability distribution .
Regularization
To regularize our CNN model , dropout ( Srivastava et al. , 2014 ) is used after the max pooling layer .
Intuitively , dropout assumes that we can still obtain a reasonable classification even when some of the features are dropped .
To do this , each dimension is randomly set to 0 using a Bernoulli distribution ?(? ) .
In order to have the training and testing to be of the same order , the test outputs can be scaled by ?.
The softmax layer for the CNN model also uses a form of empirical Bayes regularization , where each row of ? soft is restricted using an ?
2 norm , by re-normalizing the vector if the norm threshold is exceeded .
Long short term memory network
We used an LSTM for our recurrent architecture , which consisted of an embedding layer , LSTM layer , and a softmax classification layer .
A recurrent neural network is a neural network designed for sequential problems .
Even simple RNNs are Turing complete , and they can theoretically obtain information from the entire sequence instead of only an unordered bag of phrases .
But finding good architectures which can capture this and training them can be difficult .
Indeed , there were many instances where our LSTM failed to capture important ordering information .
We now give a brief review of the LSTM .
Given an input sequence ? = [?
1 , . . . , ? ? ] , a recurrent network defines an internal state function ? ? and an output function ? ? to iterate over ? , so that at time step ? : ? ? = ? ? ( ? ? , ? ?1 , ? ?1 ) ? ? = ? ? (? ? , ? ?1 , ? ? ) where ?
0 and ?
0 are initial bias states .
The simplest RNN , where : ? ? (? ? , ? ?1 , ? ? ) = tanh ( ? ? [? ? ? ? ?1 ] ? ) suffers from the gradient vanishing and exploding problem ( Hochreiter and Schmidhuber , 1997 ) .
In particular , products of saturated tanh activations can vanish the gradient , and products of ? can vanish or explode the gradient .
The LSTM is a way to remedy this ( Hochreiter and Schmidhuber , 1997 ) .
It sets : ? ? = ? ? (? ? ? ? + ? ? ? ?1 + ? ? ? ? ?1 + ? ? ) ? ? = ? ? (? ? ? ? + ? ? ? ?1 + ? ? ? ? ?1 + ? ? ) ? ? = ? ? ? ? ?1 + ? ? ? ? ? (? ? ? ? + ? ? ? ?1 + ? ? ) ? ? = ? ? (? ? ? ? + ? ? ? ?1 + ? ? ? ? ? + ? ? ) ? ? ? (? ? )
Here , ? ? is made up of weights ; ? ? , ? ? , ? ? , ? ? are the input gates , forget gates , cell states and output states .
Cell states have an identity activation function .
The gradient will not vanish if input ? ? needs to be carried to ? ? , since this can only happen when the forget gates are near 1 .
However , gradient explosion can still be present .
We use the common approach of cutting off gradients above a threshold for the gradients inside ? ? , ? ? , ? ? , ? ? .
To use the LSTM , we first used a document embedding matrix in the same manner as our convolutional neural network architecture .
This was fed into to an LSTM layer .
The output for the final timestep of the LSTM layer was then fed into a final softmax layer with the appropriate output size for classification .
We also experimented with a similar and commonly used network , the Gated Recurrent Network ( GRU ) , but it was not used due to lower results compared to the LSTM .
word2vec Bayes
Our word2vec
Bayes model is our baseline model , and described in Taddy ( 2015 ) , with the inclusion of a class prior .
Taddy ( 2015 ) uses Bayes formula to compute the probabilities of a document belonging to a sentiment class .
Given a document ? , its words {?} ? , label ? , Bayes formula is : ?(?|? ) = ?(?|? ) ?(? ) ?(? )
For classification problems , we can ignore ?(? ) since ? is fixed . ?(?|? ) is estimated by first training word2vec on a subset of the corpus with label ? , then using the skipgram objective composite likelihood as an approximation : log ?(?|? ) ? ? ? |?| ? ?=1 |?| ? ?=1 1 1 ?|?|? log ?(? ? |? ? , ?)
We estimated ?(? ) via class frequencies , i.e. the MLE for the categorical distribution , compared to Taddy ( 2015 ) , who used the discrete uniform prior .
This model was chosen since it provides a reasonable baseline , and also appears to be independent enough from our neural networks to provide a performance gain in the ensemble .
The word2vec based model benefits from being a semi-supervised method , but it also loses ordering information outside a word 's context window , and the prediction of neighboring words also ignores distance to that word .
The limited amount of data for each class is also an issue .
Ensemble
If the errors made by each classifier are independent enough , then combining them in an ensemble can reduce the overall error rate .
We used soft voting as a method to combine the outputs of the above classifiers .
We define soft voting as : ? vote = ? ? ? ? ? ? , s.t. ? ? ? ? = 1 , ? : ? ? ? 0 where ? ? is the output of classifier ?.
Topic dependent models
To adapt our neural networks to topicdependent sentiment classification , in our neural network models , we augmented each embedding vector by concatenating it with a topic embedding .
The motivation behind this approach is to allow the model to interpret each word to be within the context of some topic .
Our topic embeddings were obtained by the element - wise average of word embeddings for each word in that topic .
We found that empirically , this is a simple yet effective way of achieving a document embedding .
When used directly as a feature vector in logistic regression for sentiment analysis , we have found this to outperform methods described in Le and Mikolov ( 2014 ) .
Word embeddings of dimension ? = 300 pretrained over Google News were used directly , without any further tuning .
Words in the tweet which were not present in this pretrained embedding were ignored .
Experiments and evaluation
We evaluated our models on SemEval 2016
Task 4 subtask A , the message-only subtask , and subtask B , the topic-dependent subtask .
Data Task
A consisted of 3 sentiment classes - positive , neutral and negative - whilst Task B consisted of 2 sentiment classes - positive and negative .
We only managed to download 90 % of the entire set of tweets for the 2016 SemEval data , due to tweets becoming " not available " .
In addition to the Twitter data for 2016 , for Task A we also used training data from SemEval 2016 Task 10 .
The data is summarized in Table 1 .
To pretrain our network using distant learning ( described below ) , we took a random sample of 10M English tweets from a 5.3TB Twitter dataset crawled from 18 June to 4 Dec , 2014 using the Twitter Trending API .
We then processed tweets with a regular expression : tweets which contained emoticons like :) were considered positive , while those which contained emoticons like :( were considered negative ; tweets which contained both positive and negative emoticons , or others emoticons such as :- , were ignored .
We extracted 1 M tweets each for the positive and negative classes .
Evaluation Evaluation consisted of accuracy , macroaveraged ?
1 across the positive and negative classes , which we denote ?
PN , and macroaveraged recall across the positive and negative classes , which we denote ? PN .
Preprocessing
All methods use the same preprocessing .
We normalized the tweets by first replacing URLs with url and author methods such as @Ladiibubblezz with author .
Casing was preserved , as the pretrained word2vec vectors included casing .
The tweets were tokenized using twokenize , with it 's being split into it and 's .
Training and hyperparameters
Neural networks
For both our models , we initialized ?
emb to word embeddings pretrained using word2vec 's skip-gram model ( Mikolov et al. , 2013 ) on the Google News corpus , where ? = 300 .
Unknown words were drawn from ? [?0.25 , 0.25 ] to match the variance of the pretrained vectors .
For the CNN model , we also stripped words so all documents had a length ? 56 .
For our CNN , we used used 3 1 - d convolutions , with filter sizes of 3 , 4 , 5 , each with 100 filters .
Our dropout rate was 0.5 , and our ?
2 norm restriction was 3 . For our LSTM , we used a cell dimension size of 300 , and our activations were chosen empirically to be ? ? = ? ? = ? ? = sigmoid , ? ? = tanh , our gradient cutoff was 100 .
To train our neural networks , we used cross entropy loss and minibatch gradient descent with a batch size of 64 .
For our CNN , we used the adadelta ( Zeiler , 2012 ) gradient descent method with the default settings .
For our LSTM , we used the rmsprop gradient descent method with a learning rate of 0.01 .
Due to limited training data , we can use distant learning ( Severyn and Moschitti , 2015 ) , by initializing the weights of our neural networks by first training them on a silver standard data set ( generated using Twitter emoticons which we describe below ) , then tuning them further on the gold standard data set ( Severyn and Moschitti , 2015 ) .
However , we did not use this for our topic-dependent models , as there was no performance gain .
We split the distant data into 10 4 tweets per epoch , and took the best epoch on the validation set as the initial weights , using ?
PN as our scoring metric .
We repeatedly iterated over the SemEval data with 10 3 tweets per epoch for 10 2 epochs , and again took the best epoch on the validation set as the final weights .
word2vec Bayes Gensim ( ?eh?ek and Sojka , 2010 ) was used to train the word embeddings and obtain ?(?|? ) .
We used the skipgram objective , with a embedding dimension of 100 , window size of 10 , hierarchical softmax with 5 samples , 20 training iterations , no frequent word cutoff , and a sam- ple coefficient of 0 .
Soft voting To find ? ? , we first relaxed the sum condition of ? ? by setting ?
1 = 1 and noting that max ? ? ? , ? , ? is invariant under scaling .
We then used the L-BFGS -B algorithm , with initial weights of 1 , combined with basin-hopping for 1000 iterations .
We optimized for accuracy , since this most-consistently improved results .
Results
The official evaluation results are shown in Table 2 and Table 3 .
The results for Task A suggest that our models are overfitting .
Our best position was achieved on the Twitter 2016 dataset , and indeed , this is what our parameters were chosen on .
Our own evaluation of our different classifiers , using Twitter 2016 - test , is shown in Soft voting outperforms all classifiers , showing that there is some independence amongst the errors made .
In Task A , there appears to be more correlation between the CNN and LSTM classifiers , as excluding the word2vec Bayes model reduces the performance .
In Task B , the word2vec Bayes model appears to perform too poorly to provide a marked benefit .
From Table 5 we can see that the inclusion of topic information provides a substantial boost to both of our neural networks .
This shows that our method of incorporating topic information is a useful way of modifying neural networks , and provides a strong baseline for alternative ways of doing this .
Conclusions
We described our ensemble approach to sentiment analysis both the task of topic-dependent document classification and document classifi-cation by itself .
We gave a detailed description of how to modify our classifiers to be topicdependent .
The results show that ensembles can work for neural nets , and that our way of including topics achieves performance gains , and forms a good basis for future research in this area .
Table 1 : 1 Semeval -2016 data .
The negative : neutral : positive split was 16 : 42 : 42 for all of 2016 Task A used .
The negative : positive split was 19 : 81 for all of 2016 Task B used .
Datset
A total A used B total B used Twitter 2016 - train 6000 5465 4346 3941 ( + 11340 ) Twitter 2016 - dev 2000 1829 1325 1210 Twitter 2016 - test 2000 1807 1417 1270
Table 2 : 2 Official test scores and ranks for Task A. Dataset ? PN Twitter 2013 0.687 7 Twitter 2014 0.706 7 Twitter 2015 0.650 4 Twitter 2016 0.617 3 Twitter Sarcasm 2014 0.449 11 SMS 2013 0.593 10 LiveJournal 2014 0.683 9 Dataset Val metric ? PN ? PN Acc Twitter 2016 Twitter 2016 ? PN ? PN 0.758 7 0.788 2 0.870 1 0.807 1 0.806 1 0.867 1
Table 3 : 3 Test scores and ranks for Task B .
The official run incorrectly used ?
PN as the validation metric .
Table 4 4 and
Table 4 : 4 Results for Task A , sorted by ?
PN . Model Accuracy ?
PN soft voting all 0.8008 0.7849 soft voting cnn + lstm 0.7976 0.7846 cnn topic 0.8047 0.7762 lstm topic 0.6756 0.7494 cnn 0.8354 0.7253 word2vec Bayes 0.7654 0.7138 lstm 0.8118 0.6916
Table 5 : 5 Results for Task B , sorted by ? PN .
The dashed line separates topic models from message -only models .
The lstm topic model has poorer accuracy due to being optimized on ? PN .
Table 5 . 5 Taking into account all evaluation metrics , we can see that in both tasks , our CNN outperforms our LSTM , in Task A slightly and in Task B substantially .
The word2vec Bayes model is worse than both , moreso in Task B .
There were some issues surrounding the evaluation metrics .
We only got 7th for ? PN and 2nd for ?
PN officially , but when we retrained our model using ?
PN as the subtask intended , we place first across all metrics .
