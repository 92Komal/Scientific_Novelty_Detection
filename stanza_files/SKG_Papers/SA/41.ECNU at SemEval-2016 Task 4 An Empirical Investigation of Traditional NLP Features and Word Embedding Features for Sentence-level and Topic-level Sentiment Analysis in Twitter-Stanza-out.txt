title
ECNU at SemEval - 2016 Task 4 : An Empirical Investigation of Traditional NLP Features and Word Embedding Features for Sentence-level and Topic-level Sentiment Analysis in Twitter
abstract
This paper reports our submissions to Task 4 , i.e. , Sentiment Analysis in Twitter ( SAT ) , in SemEval 2016 , which consists of five subtasks grouped into two levels : ( 1 ) sentence level , i.e. , message polarity classification ( subtask A ) , and ( 2 ) topic level , i.e. , tweet classification and quantification according to two -point scale ( subtask B and D ) or five-point scale ( subtask C and E ) .
We participated in all these five subtasks .
To address these subtasks , we investigated several traditional Natural Language Processing ( NLP ) features including sentiment lexicon , linguistic and domain specific features , and word embedding features together with supervised machine learning methods .
Officially released results showed that our systems rank above average .
Introduction
In recent years , with the emergence of social media , more and more users have shared and obtained information through microblogging websites , such as Twitter .
As a result , a huge amount of available data attracts a lot of researchers .
SemEval 2016 provides such a universal platform for researchers to explore in the task of Sentiment Analysis in Twitter ( Nakov et al. , 2016 ) ( Task 4 ) , which includes five subtasks grouped into two levels , i.e. , sentence level and topic level .
Subtask
A is a sentence level task aiming at sentiment polarity classification of the whole tweet .
The other four subtasks are at topic level , i.e. , given one topic , the sentiment polarity of tweets are classified or assigned by a two -point scale ( i.e. , subtask B and D ) and by a five-point scale ( i.e. , subtask C and E ) .
Specifically , subtask B is to identify the sentiment polarity label ( i.e , Positive and Negative ) of tweets with respect to the given topic while subtask D aims at estimating the sentiment distribution of tweets with respect to the given topic .
Both subtask B and D are on a two -point scale .
The purposes of subtask C and E are similar with that of subtask B and D , except for using a five-point scale , that is , the class labels are of 5 values , i.e , 2 , 1 , 0 , -1 and - 2 representing Very Positive , Positive , Neutral , Negative and Very Negative .
Given the character limitations on tweets , sentiment orientation classification on tweets can be regarded as a sentence - level sentiment analysis .
Many researchers focus on feature engineering to improve the performance of SAT .
For example , ( Turian et al. , 2010 ; Liu , 2012 ; Zhang et al. , 2006 ) showed that one- hot representation on n-gram features is a relatively strong baseline .
Furthermore , ( Mohammad et al. , 2013 ) proposed a state - of - the - art model which implemented several sentiment lexicons and a variety of manual features .
Apart from the traditional methods , more and more researchers have paid their attention to use deep learning methods .
Word embedding is one of such methods , where each word is represented as a continuous , low-dimension vector and has been applied into NLP tasks as a critical and fundamental step .
Commonly , there are several types of word embedding models , e.g. , Bengio proposed a Neural Probabilistic Language Model ( NNLM ) in ( Bengio et al. , 2003 ) to learn distributed representation for each word and Mikolov simplified the structure of NNLM and presented t-wo efficient log-linear models in ( Mikolov et al. , 2013 ) . Moreover , Tang et al. , 2014 ) further proposed learning sentiment - based word embeddings to settle SAT .
Meanwhile , topicbased opinion always adheres on certain words or phrases rather than whole tweet .
To address topicbased SAT , ( Wang et al. , 2011 ) used the hashtag information , ( Lin and He , 2009 ) utilized the topic model to extract topic information from tweets and picked out related words rather than all words in whole tweet as pending words for consequential feature extraction .
Previous work showed that feature engineering has a significant impact on this task .
Thus , in this work , we presented multiple types of traditional NLP features to perform SAT , e.g. , sentiment lexicon features ( e.g. , MPQA , IMDB , Bing Liu opinion lexicon , etc ) , linguistic features ( e.g. , negations , n-gram at the word level and character level , etc ) and tweet specific features ( e.g. , emoticons , capital words , elongated words , hashtags , etc , ) .
Besides , the word embedding features were adopted .
We also performed a series of experiments to select effective feature subsets and supervised machine learning algorithms with optimal parameters .
The rest of this paper is organized as follows .
Section 2 describes our system framework including preprocessing , feature engineering , evaluation metrics , etc .
The experiments are reported in Section 3 .
Finally , this work is concluded in Section 4 .
System Description
Data Preprocessing
With the aid of approximate 5 , 000 abbreviations and slangs collected from Internet , we converted the informal writing into regular forms , e.g. , " asap " replaced by " as soon as possible " , " 3 q " replaced by " thank you " , etc .
And we recovered the elongated words to their original forms , e.g. , " soooooo " to " so " .
Finally , the processed data was performed for tokenization , POS tagging and parsing by using C-MU Parsing tools ( Owoputi et al. , 2013 ) .
Feature Engineering
We used four types of features , i.e , linguistic features ( e.g. , negations , n-gram , etc ) , tweet specific features ( e.g. , emoticons , all - caps , hashtags , etc ) , sentiment lexicon features ( the score calculated from eight sentiment lexicons ) and word embedding features .
Linguistic Features : ?
Character n-grams :
The character - level ngrams are used , where n = { 3 , 4 , 5 }. ?
Word n-grams :
The word- level unigrams , bigrams , trigrams and 4 - grams are adopted .
? POS : The absolute frequency of each part-ofspeech tag is recorded .
?
Negation : Negation in a message always reverses its sentiment orientation .
We collected 29 negations from Internet and recorded the frequency of negations in the whole tweet .
?
Cluster :
The CMU TweetParser tool provides 1 , 000 token clusters produced with the Brown clustering algorithm on 56 million English language tweets .
We recorded the existence of tokens in tweets with respect to these 1 , 000 clusters .
?
Dependency triple :
The dependency tree is generated by Stanford Parser tool and each tweet contains several dependency triples ( e.g. , relation ( government , dependent ) ) .
We used a binary feature to record if a dependency triple is present or absent in a tweet .
Tweet Specific Features : ? Punctuation : Punctuation marks ( e.g , exclamation mark ( ! ) and question mark ( ? ) ) usually indicate the strength of sentiment .
Therefore , we recorded the numbers of these marks in isolation and in combination .
Besides , the position of punctuation in tweet is also an important clue for sentiment , thus we used a binary feature to indicate whether it is the last token of tweet .
?
All- caps :
The number of words in uppercase is recorded .
?
Hashtag :
We recorded the number of hashtags in the tweet .
?
Emoticon :
We collected 67 emoticons from Internet and this feature type records the number of positive and negative emoticons respectively .
Moreover , two binary values are to record whether the last token is a positive or negative emoticon respectively .
?
Elongated :
It indicates the number of elongated words in the raw text of tweet .
Sentiment Lexicon Features ( SentiLexi ) :
We employed the following eight sentiment lexicons to extract sentiment lexicon features : Bing Liu lexicon 1 , General Inquirer lexicon 2 , AFINN 3 , IMD-B 4 , MPQA 5 , NRC Emotion Sentiment Lexicon 6 , NR -C Hashtag Sentiment Lexicon 7 , and NRC Sentimen - t140 Lexicon 8 . Generally , we transformed the scores of all words in all sentiment lexicons to the range of - 1 to 1 , where the positive number indicates positive sentiment and the minus sign denotes negative sentiment .
The following six scores are calculated on the whole data for each sentiment lexicon : ( 1 ) the ratio of positive words to all words , ( 2 ) the ratio of negative words to all words , ( 3 ) the maximum sentiment score , ( 4 ) the minimum sentiment score , ( 5 ) the sum of sentiment scores , and ( 6 ) the sentiment score of the last word in tweet .
If a word does not exist in one sentiment lexicon , its corresponding score is set to 0 .
Word Embedding Features :
In this work , we employed three different types of word vectors .
The general word vectors are trained by Google on huge amount of News , which is a different domain from Twitter .
The other two sentiment word vectors are both trained on tweets but using different methods .
The purpose of this feature type is to examine the effects of word embedding and sentiment word embedding on performance .
? General Word Vector ( GeneralW2V ) :
We adopted the word2vec tool 9 to obtain word vectors with the dimensionality of 300 ( i.e. , Gen-eralW2V ) , trained on 100 billion words from Google News .
? Sentiment Word Vector ( SWV ) : proposed a Combined -Sentiment Word Embedding Model to learn sentiment word vectors ( SWV ) for sentiment analysis task .
In this work , we learn SWV on NRC140 tweet corpus ( Go et al. , 2009 ) , where the corpus is made up of 1.6 million tweets ( 0.8 million positive and 0.8 million negative ) .
The vector dimension is set as 100 .
? Sentiment-specific Word Embedding ( SSWE ) : Similar with SWV , the sentiment -specific word embedding model proposed by ( Tang et al. , 2014 ) used a multi-hidden - layers neural network to train SSWE with dimensionality of 50 .
To convert the above word vectors into a sentence vector , we simply adopted the min , max and average operations .
Obviously , this combination strategy neglects the word sequence in tweet but it is simple and straightforward .
As a result , the final sentence vector V ( s ) was concatenated by V min ( s ) , V max ( s ) and V average ( s ) .
Evaluation Metrics
For subtask A , we used the macro- averaged F score of positive and negative classes ( i.e. , F macro = Fpos+ Fneg 2 ) to evaluate the performance .
Subtask B and D just contain positive and negative labels .
The official metric for subtask B is macro- averaged recall among positive and negative ( i.e. , R macro = R P os + R N eg 2 ) .
As for subtask D , it is Kullback - Leibler Divergence ( KLD ) among distributions of two classes ( i.e. , KLD ( pos , neg ) = c j ? pos , neg P ( c j ) ? log P ( c j ) P ( c j ) , where P denotes the probability of predicted label and P is the probability of gold label ) .
There are 5 classes existing in subtask C and E , and the organizers adopted Macroaveraged Mean Absolute Error ( i.e. , M AE M ) and Earth Mover 's Distance ( EM D ) among 5 predefined classes for two subtasks respectively , where the detail information of two metrics for evaluation is described in the official document available on the website 10 .
Experiments
Datasets
Since only tweet IDs are provided by organizers , different participants may collect different numbers of tweets due to missing tweets or system errors .
Subtask B and D are of the same data set .
And subtask C and E share one common data set .
The statistics of all datasets for these subtasks are shown in Tables 1 , 2 , and 3 , respectively .
For subtask A , the training data set consists of four parts which are shown in Table 1 , i.e. , 2013train , 2013 dev , 2016train and 2016 dev .
The data set 2013train means SemEval - 2013
Task 2 training data set ( Nakov et al. , 2013 ) , and the following data sets are named in the same way .
Actually , in consideration of the difference of polarity distribution between data set 2016 devtest and 2013&2014test , we just adopted 2016 devtest as development data .
For subtask B , C , D and E , the data is divided into many topic sets .
Experiments on Training Data
In order to improve the performance of sentiment analysis , we performed feature selection experi- ments on all subtasks and the optimum feature sets are shown in Table 4 . From Table 4 , it is interesting to find that : ( 1 ) Negation features and tweet specific features such as emoticon and all - caps make contributions to almost all subtasks .
( 2 ) The feature set with the best performance of subtask B is not quite beneficial for subtask D even though they have the same data set , perhaps because of the essential difference between binary classification and binary quantification : in the latter , errors of different polarity compensate each other .
The similar observation is found in subtask C and E. ( 3 )
The sentiment lexicon features make contributions to performance improvement of subtask A , B and C , but are not quite useful for subtask D and E .
A possible reason is that the latter two subtasks focus on quantification analysis while the sentiment lexicon only contains sentiment orientation rather than sentiment strength .
( 4 ) The word embedding features are not as effective as expected .
It maybe because we obtained sentence vectors by the simplest combination method described above , which does not take into account contextual information and semantic relations among words .
Besides , since subtask B , C , D and E focus on topic-level sentiment analysis , we tried to extract features from related words rather than whole tweet .
But the preliminary experimental results showed that extracting features from related words underperformed the latter strategy for extracting features .
The possible reason is that in many cases a tweet only has one single sentiment polarity .
Thus the sentiment polarity of sentence can always represent that of topic and extracting features from the related words may drop important information .
Learning Algorithm
For these subtasks , we examined several supervised machine learning classification algorithms with different parameters ( e.g. , Logistic Regression with c= {0 .
Forest with n= { 20 , 50 , 100 , 400 , 1000 , 2000 } , SGD with loss={hinge , log} , etc ) .
Finally , Logistic Regression with c = 1 implemented in Liblinear 11 was adopted for all five subtasks for its good performance in preliminary experiments .
Results on Test Data Based on the optimum feature sets shown in Table 5 shows the results of our systems and the top-ranked systems on all five subtasks .
Our systems ranked 10th out of 34 submissions for sub -11 https://www.csie.ntu.edu.tw/ cjlin/liblinear / task A , 4th/19 for subtask B , 2nd/11 for subtask C , 10th / 14 for subtask D and 5th/10 for subtask E. Compared with the top ranked systems , there is much room for improvement in our work .
Although word embedding features were adopted in this work , we used the simplest combination method to convert word vectors to sentence vectors .
The effective convolution method is expected to be able to improve the performance of sentiment analysis .
Conclusion
In this paper , we extracted several traditional NLP features ( e.g. , linguistic features , tweet specific features , etc ) and word embedding features from whole tweet and constructed classifiers using supervised machine learning algorithms to accomplish sentiment analysis towards sentence level ( i.e. , subtask A ) and topic level ( i.e. , subtask B , C , D and E ) .
Word embedding features are not as effective as expected since the way of using these features are quite simple and naive , thus it is too hasty to make a conclusion that the word embedding features make marginal contribution .
In future work , we consider to focus on developing advanced convolution neural network to model sentence with the aid of sentiment word vector .
Table 1 : 1 Statistics of data sets in training ( train ) , development ( dev ) , test ( test ) data for subtask A. train dataset 2013train 2013 dev 2016train 2016 dev Positive 3,250 ( 37 % ) 575( 35 % ) 2,839( 51 % ) 772( 42 % ) Negative 1,276( 15 % ) 340( 20 % ) 787( 14 % ) 359 ( 20 % ) Neutral 4,151 ( 48 % ) 739 ( 46 % ) 1,892(34 % ) 702( 38 % ) Total 8,677 1,654 5,518 1,833 dev 2016 devtest 2013&2014test 886( 49 % ) 5,078( 40 % ) 287( 16 % ) 2,142( 16 % ) 626(35 % ) 5,580 ( 44 % ) 1,799 12,800 test 2016test 7,059 ( 34 % ) 3,231( 16 % ) 10,342( 50 % ) 20,632 dataset train dev test Positive 4,191 ( 81 % ) 1,027( 81 % ) 8,212( 78 % ) 2,339 ( 22 % ) 10,551 Negative Total 997( 19 % ) 5,188 238( 19 % ) 1,265
