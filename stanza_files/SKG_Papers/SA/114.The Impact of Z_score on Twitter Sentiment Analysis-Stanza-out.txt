title
The Impact of Z_score on Twitter Sentiment Analysis
abstract
Twitter has become more and more an important resource of user-generated data .
Sentiment Analysis in Twitter is interesting for many applications and objectives .
In this paper , we propose to exploit some features which can be useful for this task ; the main contribution is the use of Z-scores as features for sentiment classification in addition to pre-polarity and POS tags features .
Our experiments have been evaluated using the test data provided by SemEval 2013 and 2014 .
The evaluation demonstrates that Z_scores features can significantly improve the prediction performance .
Introduction
The interactive Web has changed the relation between the users and the web .
Users have become an important source of content .
They express their opinion towards different issues .
These opinions are important for others who are interested in understanding users ' interests such as buyers , sellers and producers .
Twitter is one of the most important platforms in which the users express their opinions .
Many works have exploited this media for predicting valuable issues depending on Sentiment Analysis ( SA ) .
The authors in ( Asur and Huberman 2010 ) predicted the box-office revenues of movies in advance of their releases using the tweets talking about them .
In ( Bae and Lee 2012 ) Sentiment
This work is licensed under a Creative Commons Attribution 4.0 International Licence .
Page numbers and proceedings footer are added by the organisers .
Licence details : http://creativecommons.org/licenses/by/4.0/
Analysis has been used to study the impact of 13 twitter accounts of famous persons on their followers and also for forecasting the interesting tweets which are more probably to be reposted by the followers ( Naveed , Gottron et al. 2011 ) . Sentiment Analysis can be done in different levels ; Document level ; Sentence level ; Clause level or Aspect - Based level .
SA in Twitter can be seen as a sentence level task , but some limitations should be considered in such sentences .
The size of tweets is limited to 140 characters , informal language , emotion icons and non-standard expressions are commonly used , and many spelling errors can be found due to the absence of correctness verification .
Three different approaches can be identified in the literature of Sentiment Analysis in Twitter , the first approach is lexicon based , using specific types of lexicons to derive the polarity of a text , this approach suffers from the limited size of lexicon and requires human expertise to build manual lexicon ( Joshi , Balamurali et al. 2011 ) , in the other hand the automatic lexicons are not so efficient .
The second one is machine learning approach which uses annotated texts with a given labels to learn a classification model , an early work was done on a movie review dataset ( Pang , Lee et al. 2002 ) .
Both lexicon and machine learning approaches can be combined to achieve a better performance ( Khuc , Shivade et al. 2012 ) .
These two approaches are used for SA task but the third one is specific for Twitter or social content , the social approach exploits social network properties and data for enhancing the accuracy of the classification ( Speriosu , Sudan et al. 2011 ) .
In this paper , we exploit machine learning algorithm with the aid of some features : ?
The original Terms : the terms representing the tweet after the tokenization and stemming ; ?
Pre-polarity features : the number of negative , positive and neutral words extracted from two sentiment lexicons ; ?
POS tags : the number of adjectives , connectors , verbs , nouns , adverbs in the tweet ; ?
Z-score :
The numbers of terms having Zscore value more than three for each class positive , negative and neutral .
We extended the original terms with these last features .
We also constructed a dictionary for the abbreviations and the slang words used in Twitter in order to overcome the ambiguity of the tweets .
We tested the performance of every possible combination of these features .
The rest of this paper is organized as follows .
Section 2 outlines previous work that focused on sentiment analysis in Twitter .
Section 3 presents the Z_score features and the others which we used for training a classifier .
Our experiments are described in section 4 , conclusion and future work is presented in section 5 .
Related Works
We can identify three main approaches for sentiment analysis in Twitter .
The lexicon based approaches which depend on sentiment lexicons containing positive , negative and neutral words or expressions ; they calculate the polarity according to the number of common opinionated words between the lexicons and the text .
Many dictionaries have been created manually such as ANEW ( Affective Norms for English Words ) or automatically such as SentiWordNet ( Baccianella , Esuli et al. 2010 ) .
Four lexicon dictionaries were used to overcome the lack of words in each one ( Joshi , Balamurali et al.
2011 ; Mukherjee , Malu et al. 2012 ) .
Automatically construction of a Twitter lexicon was implemented by ( Khuc , Shivade et al. 2012 ) .
Machine learning approaches were employed from annotated tweets by using Naive Bayes , Maximum Entropy MaxEnt and Support Vector Machines ( SVM ) .
The authors ( Go , Bhayani et al. 2009 ) reported that SVM outperforms other classifiers .
They tried a unigram and a bigram model in conjunction with parts - of-speech ( POS ) features ; they noted that the unigram model outperforms all other models when using SVM and that POS features decrease the quality of results .
The authors in ( Kouloumpis , Wilson et al. 2011 ) found that N-gram with lexicon features and micro-blogging features are useful but POS features are not .
In contrast , in ( Pak and Paroubek 2010 ) they reported that POS and bigrams both help .
In ( Barbosa and Feng 2010 ) the authors proposed the use of syntax features of tweets like retweet , hashtags , link , punctuation and exclamation marks in conjunction with features like prior polarity of words and POS tags , in ( Agarwal , Xie et al. 2011 ) this approach was extended by using real valued prior polarity and by combining prior polarity with POS .
Authors in ( Saif , He et al. 2012 ) proposed to use the semantic features , therefore they extracted the named entities in the tweets .
Authors in ( Hamdan , B?chet et al. 2013 ) used the concepts extracted from DBpedia and the adjectives from WordNet , they reported that the DBpedia concepts are useful with Na?ve - Bayes classifier but less useful with SVM .
The third main approach takes into account the influence of users on their followers and the relation between the users and the tweets they wrote .
It assumes that using the Twitter follower graph might improve the polarity classification .
In ( Speriosu , Sudan et al. 2011 ) they demonstrated that using label propagation with Twitter follower graph improves the polarity classification .
In ( Tan , Lee et al. 2011 ) they employed social relation for user-level sentiment analysis .
In ( Hu , Tang et al. 2013 ) a Sociological Approach to handling the Noisy and short Text ( SANT ) for supervised sentiment classification is used ; they reported that social theories such as Sentiment Consistency and Emotional Contagion could be helpful for sentiment analysis .
Feature Selection
We used different types of features in order to improve the accuracy of sentiment classification .
- Bag of words ( Terms )
The most commonly used features in text analysis are the bag of words which represent a text as unordered set of words or terms .
It assumes that words are independent from each other and also disregards their order of appearance .
We stemmed the words using Porter Stemmer and used them as a baseline features .
- Z_score Features ( Z ) We suggest using a new type of features for Sentiment Analysis , Z_score can distinguish the importance of each term in each class .
We compute the number of terms having Z_score more than three for each class over each tweet .
We assume that the term frequencies follow the multinomial distribution .
Thus , Z_score can be seen as a standardization of the term .
We compute the Z_score for each term t i in a class Cj ( t ij ) by calculating its term relative frequency tfr ij in a particular class C j , as well as the mean ( mean i ) which is the term probability over the whole corpus multiplied by n j the number of terms in the class C j , and standard deviation ( sd i ) of term t i according to the underlying corpus ( see Eq. ( 1,2 ) ) .
Z = Eq. ( 1 ) Z = * ( ) * ( ) * ( ( ) ) Eq. ( 2 ) The term which has salient frequency in a class in compassion to others will have a salient Z_score .
Z_score was exploited for SA by ( Zubaryeva and Savoy 2010 ) , they choose a threshold ( > 2 ) for selecting the number of terms having Z_score more than the threshold , then they used a logistic regression for combining these scores .
We use Z_scores as added features for classification because the tweet is too short , therefore many tweets does not have any words with salient Z_score .
The three following figures 1,2,3 show the distribution of Z_score over each class , we remark that the majority of terms has Z_score between - 1.5 and 2.5 in each class and the rest are either vey frequent ( > 2.5 ) or very rare ( <- 1.5
- Sentiment Lexicon Features ( POL )
We used two sentiment lexicons , MPQA Subjectivity Lexicon ( Wilson , Wiebe et al. 2005 ) and Bing Liu's Opinion Lexicon which is created by ( Hu and Liu 2004 ) and augmented in many latter works .
We extract the number of positive , negative and neutral words in tweets according to these lexicons .
Bing Liu 's lexicon only contains negative and positive annotation but Subjectivity contains negative , positive and neutral .
- Part Of Speech ( POS )
We annotate each word in the tweet by its POS tag , and then we compute the number of adjectives , verbs , nouns , adverbs and connectors in each tweet .
Evaluation
Data collection
We used the data set provided in SemEval 2013 and 2014 for subtask B of sentiment analysis in Twitter ( Rosenthal , Ritter et al. 2014 ) ( Wilson , Kozareva et al. 2013 ) .
The participants were provided with training tweets annotated as positive , negative or neutral .
We downloaded these tweets using a given script .
Among 9646 tweets , we could only download 8498 of them because of protected profiles and deleted tweets .
Then , we used the development set containing 1654 tweets for evaluating our methods .
We combined the development set with training set and built a new model which predicted the labels of the test set 2013 and 2014 .
Experiments
Official Results
The results of our system submitted for SemEval evaluation gave 46.38 % , 52.02 % for test set 2013 and 2014 respectively .
It should mention that these results are not correct because of a software bug discovered after the submission deadline , therefore the correct results is demonstrated as non-official results .
In fact the previous results are the output of our classifier which is trained by all the features in section 3 , but because of index shifting error the test set was represented by all the features except the terms .
Non-official Results
We have done various experiments using the features presented in Section 3 with Multinomial Na?ve-Bayes model .
We firstly constructed feature vector of tweet terms which gave 49 % , 46 % for test set 2013 , 2014 respectively .
Then , we augmented this original vector by the Z_score features which improve the performance by 6.5 % and 10.9 % , then by pre-polarity features which also improve the f-measure by 4 % , 6 % , but the extending with POS tags decreases the fmeasure .
We also test all combinations with these previous features , Table2 demonstrates the results of each combination , we remark that POS tags are not useful over all the experiments , the best result is obtained by combining Z_score and pre-polarity features .
We find that Z_score features improve significantly the f-measure and they are better than pre-polarity features .
We repeated all previous experiments after using a twitter dictionary where we extend the tweet by the expressions related to each emotion icons or abbreviations in tweets .
The results in Table3 demonstrate that using that dictionary improves the f-measure over all the experiments , the best results obtained also by combining Z_scores and pre-polarity features .
Features
Conclusion
In this paper we tested the impact of using Twitter Dictionary , Sentiment Lexicons , Z_score features and POS tags for the sentiment classification of tweets .
We extended the feature vector of tweets by all these features ; we have proposed new type of features Z_score and demonstrated that they can improve the performance .
We think that Z_score can be used in different ways for improving the Sentiment Analysis , we are going to test it in another type of corpus and using other methods in order to combine these features .
Figure 1 1 Figure 1 Z_score distribution in positive class
