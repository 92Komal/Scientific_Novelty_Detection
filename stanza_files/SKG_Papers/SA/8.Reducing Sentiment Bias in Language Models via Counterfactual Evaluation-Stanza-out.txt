title
Reducing Sentiment Bias in Language Models via Counterfactual Evaluation
abstract
Advances in language modeling architectures and the availability of large text corpora have driven progress in automatic text generation .
While this results in models capable of generating coherent texts , it also prompts models to internalize social biases present in the training corpus .
This paper aims to quantify and reduce a particular type of bias exhibited by language models : bias in the sentiment of generated text .
Given a conditioning context ( e.g. , a writing prompt ) and a language model , we analyze if ( and how ) the sentiment of the generated text is affected by changes in values of sensitive attributes ( e.g. , country names , occupations , genders ) in the conditioning context using a form of counterfactual evaluation .
We quantify sentiment bias by adopting individual and group fairness metrics from the fair machine learning literature , and demonstrate that largescale models trained on two different corpora ( news articles , and Wikipedia ) exhibit considerable levels of bias .
We then propose embedding and sentiment prediction - derived regularization on the language model 's latent representations .
The regularizations improve fairness metrics while retaining comparable levels of perplexity and semantic similarity .
Introduction Language modeling has advanced rapidly due to efficient model architectures ( Vaswani et al. , 2017 ; Dai et al. , 2019 ) and the availability of large-scale datasets ( Radford et al. , 2019 ; Zellers et al. , 2019 ) .
Large-scale language models have been applied not only for representation extraction to support downstream tasks ( Peters et al. , 2018 ; Devlin et al. , 2019 ) , but are also used for many natural language generation applications ( Radford et al. , 2019 ; Solaiman et al. , 2019 ; Zellers et al. , 2019 ; Zhang ?
Denotes equal contribution . ?
Work done during an internship at DeepMind .
? Corresponding author : posenhuang@google.com .
had a grand time organising ...( 0.97 ) hear from her all the time all the problems ...( 0.17 ) 're working on a prototype for her banana bread recipe ...( 0.51 )
Conditioning Text with Attribute
Generated Continuations
Sentiment Distribution
My friend is a / an _ , and we ...
Figure 1 : Conditioning text " My friend is a / an < occupation > , and we ... " , alongside various text continuations generated by a GPT - 2 language model .
On the right , the empirical sentiment distribution of the generated texts is shown : they reveal a systematic difference in sentiment depending on occupation ( " baker ' ' or " accountant " ) in the conditioning context .
et al. , 2019 ) .
While the generation of coherent text is becoming increasingly practical , it also prompts models to internalize social biases present in the training corpus .
Investigating the social impact and fairness of the text generated from language models has thus received considerable research interest ( Solaiman et al. , 2019 ; Wallace et al. , 2019 ; Sheng et al. , 2019 ) .
In this paper , we aim to both quantify and reduce a language model 's sentiment bias for a given sensitive attribute .
Consider , for example , the conditioning text " My friend is a / an < occupation > , and we ... " on the left of Figure 1 . A 1.5B - parameter GPT - 2 language model can generate a variety of plausible continuations to it , yet the empirical distribution of sentiment scores differs depending on the occupation chosen in the conditioning context .
When generating 1,000 continuations for both " accountant " and " baker " , and then measuring the sentiment scores of the resulting sentences using the Google Cloud sentiment API , a systematic difference is revealed : the GPT - 2 model tends to gen-erate continuations with more positive sentiment for " baker " , and more negative sentiment with " accountant " as the occupation .
When systematically evaluating this phenomenon by manipulating different sensitive attributes values ( e.g. , country names , occupations , or person names ) in the conditioning context - that is , performing counterfactual evaluation - we find that sentiment scores for the generated texts can vary substantially , suggesting the existence of sentiment bias .
Such a sentiment bias can pose a concern for using the text generated by language models in downstream applications ( e.g. , dialogue agents ( Zhang et al. , 2019 ) ) from a fairness perspective .
To quantify sentiment bias , we propose the use of individual and group fairness metrics from the fair machine learning literature ( Dwork et al. , 2012 ; Jiang et al. , 2019 ; Hardt et al. , 2016 ) .
We furthermore propose a general framework to reduce sentiment bias given a fairness specification based on sensitive attributes ( e.g. , fairness w.r.t. a predefined set of occupation names ) .
Using this framework , we propose embedding and sentiment predictionderived regularization on the language model 's latent representations .
Experiments demonstrate that both proposed methods reduce sentiment bias while retaining a comparable level of perplexity and semantic similarity , and show a trade - off between fairness and semantic relevance .
While specifying concretely what optimal model fairness behavior should be is difficult - it might be defined by law or regulators - we provide a general framework to address given fairness specifications on sensitive attributes .
Our main contributions are : ?
We demonstrate the existence of systematic counterfactual sentiment bias in texts generated by large-scale language models ( ?3 ) . ?
We propose two novel metrics : individual and group fairness metrics to quantify counterfactual sentiment bias in language generation ( ?3 ) . ?
To the best of our knowledge , this paper is the first to introduce a general framework to reduce bias under a specification measure ( e.g. , sentiment ) for texts generated by language models given sensitive attributes .
While we focus on sentiment biases on a few common sensitive attributes ( country , occupation and name ) , the framework can be generalized to other specifications ( ?4 ) . ?
We evaluate the proposed methods using both automatic metrics and human evaluations of sentiment and semantic relevance , and find a strong correlation between automatic metrics and human evaluations ( ?5 ) .
Background & Related Work Bias in natural language processing systems .
Besides learning to favor the language of the authors ' demographic group ( Hovy and S?gaard , 2015 ) , NLP models can pick up on a variety of cultural associations and undesirable social biases ( Caliskan et al. , 2017 ) .
Systematic imbalances were observed across NLP tasks , such as gender bias in coreference resolution ( Zhao et al. , 2018 ; Rudinger et al. , 2018 ) , visual semantic role labeling , image captioning ( Hendricks et al. , 2018 ) , and demographic biases in language generation ( Sheng et al. , 2019 ) , text classification ( Dixon et al. , 2018 ; Garg et al. , 2019 ) .
Concretely in sentiment analysis , Kiritchenko and Mohammad ( 2018 ) found systematic biases with respect to race and gender across more than 200 systems .
Mitigating bias in language models .
Rather than debiasing word embeddings , Lu et al . ( 2018 ) proposed counterfactual data augmentation as a remedy to occupation -specific gender biases , and found that it can much better retain model performance than debiasing word embeddings , especially in language modeling .
Zhao et al. ( 2019 ) and Basta et al . ( 2019 ) demonstrated gender bias in pretrained language modeling representations ( ELMo ) , which translates into downstream tasks , but did not consider the language generated by the ELMo language model .
Bordia and Bowman ( 2019 ) , as well as Qian et al . ( 2019 ) identified biases in a language modeling context and propose regularization strategies of generating certain words ( e.g. , " doctor " ) with differently gendered inputs .
In contrast to these prior works on mitigating gender biases of language models based on the probabilities of generating certain words ( such as occupation ratios ) , we probe texts generated by language models using a sentiment analysis system , similar to Sheng et al . ( 2019 ) .
We further propose a general framework to mitigate bias for a given specification ( e.g. , fairness w.r.t. predefined country names , occupations , gendered names ) under a specification measure ( e.g. , sentiment , regard , etc. ) .
Prior work mostly considers comparatively small language modeling training sets .
In contrast , we investigate bias in Transformer - based models with a similar number of parameters ( 708 million parameters ) to GPT - 2 ( Solaiman et al. , 2019 ) trained on English news articles from and WikiText - 103 ( Merity et al. , 2016 ) . Fairness .
Popular statistical fairness criteria often aim at achieving individual fairness ( Dwork et al. , 2012 ) or group fairness ( Hardt et al. , 2016 ) goals .
In recent years , causal inference tools are also used in fairness research to extend beyond statistical fairness criteria making use of causal graphs .
Similar to individual fairness , which requires similar individuals to be treated similarly ( Dwork et al. , 2012 ) , counterfactual fairness requires the same model predictions before and after intervention on sensitive attributes in data- generating causal graphs ( Kusner et al. , 2017 ; Kilbertus et al. , 2017 ; Chiappa , 2019 ; Chiappa and Isaac , 2019 ) .
In our problem setting , we deviate from the counterfactual fairness works above by considering counterfactual fairness ( Garg et al. , 2019 ) based on a simple causal graph representing the language model instead of the data- generating process .
We aim towards counterfactual fairness by debiasing the latent representation of inputs in the language models , contributing to a family of methods to learn fair representations ( Beutel et al. , 2017 ; Zemel et al. , 2013 ; Creager et al. , 2019 ; Edwards and Storkey , 2016 ; Louizos et al. , 2016 ) and enforcing independence between sensitive attributes and prediction outputs ( Calders et al. , 2009 ; Zhang et al. , 2018 ; Jiang et al. , 2019 ; Chiappa et al. , 2020 ) .
Counterfactual Evaluation of Sentiment Bias Fairness specification .
Our goal is to reduce the counterfactual sentiment bias in a language model , given a fairness specification .
In our specification , we consider a set of sensitive attribute values ( e.g. , country names , occupations , and person names ) of a sensitive attribute ( e.g. , Country , Occupation , Name ) that we want generated texts to be fair to under counterfactual evaluation .
Formally , considering for example the sensitive attribute Gender , we use A = { female , male} to denote the set of values considered , and use A = a to denote a random variable A that takes the sensitive attribute value a ?
A. For each input sequence x containing sensitive tokens ?( a ) ( which are given in the specification , e.g. , ?( a ) ={ he , his , him , husband , Paul} for a = male ) , we choose another value ? of the sensitive attribute from the set A \ { a} , and define the counterfactual input x = cf (x , a , ? ) by replacing all occurrences of each sensitive token in ?( a ) with the corresponding token in ?( ? ) , and leaving all other non-sensitive tokens of x unchanged .
Given a predefined sentiment classifier f s with sentiment outputs in [ 0 , 1 ] , and a pretrained language model LM , so that the random variable LM ( x ) is a sentence sampled from the language model conditioned on x , we define the random variable S( x ) = f s ( LM ( x ) ) to be the sentiment score in [ 0 , 1 ] of the generated sentence , and denote its distribution by P S ( x ) .
Next , for counterfactual evaluation , we measure the difference between P S ( x ) and P S ( x ) as follows .
When quantifying the difference between two output distributions for a binary classification problem - such as sentiment prediction - we typically consider predictions formulated as ? = 1 ( S > ? ) , given a decision threshold ? .
One fundamental fairness concept is " demographic parity " for binary classification problems , which requires equal positive classification rates across subgroups , i.e. , p( ? = 1 | A = a ) = p( ?
= 1 | A = ? ) for any sensitive attribute values a , ? ? A . We can measure deviation from it , i.e. " demographic disparity " using the differences between the subgroup positive rates : Dwork et al . ( 2012 ) ) .
However , often we do not want our fairness goal to be dependent on a predetermined decision threshold ? , since ? may be user-defined or simply not known at training time .
This consideration leads us to match output distributions , which is called " Strong Demographic Parity " ( Jiang et al. , 2019 ) .
Concretely applied in our LM context , these distributions are P S ( x| A = a ) and P S ( x|A = ? ) . p(?
= 1 | A = a ) ? p(?
= 1 | A = ? ) ( cf. Prop .
3.1 in Extending this definition to measure unfairness between counterfactual pairs of subgroups , demographic disparity is the difference between positive sentiment rates of S( x ) and S ( x ) : | p( S ( x ) > ? )?p( S ( x ) > ? ) |.
We can then measure the deviation by computing the statistical disparity averaged over uniformly random choices of ? ? [ 0 , 1 ] , that is , E ? ?U [ 0,1 ]
| p ( S( x ) > ? ) ? p( S ( x ) > ? ) | where U denotes the random uniform distribution .
This quantity is equal to the Wasserstein - 1 distance between P S ( x ) and P S ( x ) ( Jiang et al. , 2019 ) : Sentiment bias by counterfactual evaluation , i.e. , counterfactual sentiment bias , is then the Wasserstein - 1 distance between output sentiment distributions P S of the original input x and its counterfactual x .
Thus , extending Garg et al . ( 2019 ) , we define a model to be counterfactually fair for sentiment if W 1 ( P S ( x ) , P S ( x ) ) = E ? ?U [ 0,1 ] | p ( S ( x ) > ? ) ? p( S ( x ) > ? ) | ( 1 ) W 1 ( P S ( x ) , P S ( cf ( x , a , ? ) ) ) < ( 2 ) for each sensitive attribute value a ?
A , ? ? A \ { a} , and a chosen threshold > 0 .
This fairness formulation also expresses individual fairness which requires similar individuals to be treated similarly ( Dwork et al. , 2012 ) , where similar individuals share similar non-sensitive words in a sentence .
Note that using Wasserstein - 1 distance to compare two distributions does not require assumptions on their shape ( e.g. , symmetry ) .
Fairness evaluation .
For each sensitive attribute , we measure the individual fairness and group fairness metrics from distributions of sentiment scores P S on the evaluation set in the following ways .
Individual Fairness Metric .
Based on the fairness property of the Wasserstein - 1 distance ( Eq. 1 ) , we compute the Average Individual Fairness by averaging the Wasserstein - 1 distance between the sentiment score distribution of every evaluation sentence P S ( x ) and each of its counterfactual sentence P S ( x ) across all M templates .
1 Formally , we define individual fairness metric ( denoted by I.F. ) as : 2 M | A |( | A | ? 1 ) M m=1 a , ?A W 1 ( P S ( x m ) , P S ( x m ) ) ( 3 ) 1 During inference , for each sensitive variable A we design a set of sentence templates to evaluate the counterfactual sentiment bias .
See ?5 for details .
where the inner sum is over all | A | ( | A| ? 1 ) 2 unordered pairs of distinct a , ? ?
A , and a , ? are values of the sensitive attribute in x m and xm respectively .
Group Fairness Metric .
This metric measures fairness for particular subgroups .
Concretely , the evaluation sentences are separated into | A | = K disjoint subgroups , assigning a sentence to a subgroup a if it contains sensitive tokens from ?( a ) .
Taking for example the sensitive attribute Name and selecting A = { male , female} , we have K = 2 , and ?( male ) = { Jake , Scott , Jacob , . . .} for a = male .
2 For each subgroup a ?
A , we then measure the Wasserstein - 1 distance between the sentiment distributions of all generated sentences of inputs from this subgroup , denoted by P a S , and that over the entire evaluation set , denoted by P * S .
We report the average of all these subgroup Wasserstein - 1 distances as the Average Group Fairness metric , denoted by G.F. : G.F. := 1 | A| a?A W 1 ( P a S , P * S ) . ( 4 ) 4 Language Models with Fair Sentiment Distribution
In this section , we introduce two approaches for reducing counterfactual sentiment bias in language models , which will be subsequently evaluated with the above described fairness metrics .
Given an input prefix x 1 :i with i tokens , x 1 : i = ( x 1 , ? ? ? , x i ) , where the last token x i ? ?( a ) is associated with a subgroup with value a of the sensitive attribute , we construct a perturbed prefix by replacing x i with a token xi ? ?(? ) from a different subgroup ? , where fairness between the two subgroups should be maintained .
We obtain a perturbed prefix x1 : i = ( x 1:i?1 , xi ) .
To train the language model towards reducing counterfactual sentiment bias , we want to ensure that the language model produces similar sentiment distributions for the two prefixes .
Specifically , we would like the Wasserstein - 1 distance between the sentiment distributions of generated sentences , P S ( x 1 : i ) and P S ( x 1 : i ) , to be small , as shown in Eq. 2 .
But in practice , it is prohibitively expensive to sample a distribution of generated sequences for every x 1:i and x1 : i during training .
Instead , we use hidden features from the language model as a proxy to represent the distribution of future generated sequences , since p( x i+ 1 , x i+ 2 , ? ? ? |x 1 :i ) and p( x i+ 1 , x i+ 2 , ? ? ? |x 1 :i ) depend on the hidden states of the language model conditioned on x 1:i and x1 : i , respectively .
Concretely , we explore two approaches :
Fairness through embedding regularization and Fairness through sentiment regularization , which exploit the hidden states of the language model .
Given an L-layer transformer based language model with an input x 1 : i , we let h( x x 1 : i ) denote the hidden features ( or contextual embeddings ) obtained by its hidden layers .
1 :i ) = h ( 1 ) ( x 1 : i ) , ? ? ? , h ( L ) ( Fairness through embedding regularization .
In this approach , we desire that the embeddings h ( j ) ( x 1 : i ) and h ( j ) ( x 1 : i ) are close , since the joint distributions p( x i+ 1 , x i+ 2 , ? ? ? |x 1 :i ) and p( x i+ 1 , x i+ 2 , ? ? ? |x 1 : i ) are determined by these embeddings .
We call it the " embedding regularization " approach , and define the fairness loss as a distance between the embeddings , denoted as d( h( x 1 : i ) , h( x 1 : i ) ) .
We use the cosine distance : d( h( x 1 : i ) , h( x 1 : i ) ) := 1 ? h( x 1 : i ) T h(x 1 : i ) h(x 1 : i ) h(x 1 : i ) where h( x ) is set as the average of the last two embedding vectors h ( L?1 ) ( x ) and h ( L ) ( x ) based on the following two reasons :
First , we want to capture high - level semantics ( e.g. , sentiments ) and embedding in later layers represents higher level semantics ( Tenney et al. , 2019 ) .
Second , we find that averaging too many layers can make the difference between h(x 1 : i ) and h(x 1 : i ) very small , reducing the effectiveness of regularization .
An advantage of this method is that it can directly be applied to fairness specifications beyond sentiment , as it encourages p( x i+ 1 , x i+ 2 , ? ? ? |x 1 :i ) and p( x i+ 1 , x i+ 2 , ? ? ? |x 1 : i ) to be close regardless of the specification measure ( e.g. , sentiment ) .
Since the embedding regularization method enforces the model 's predictions to be similar for the original input x 1:i and the perturbed input x1 : i without specification measure information , a potential drawback of this method is that the regularization can be too strong .
As we require the hidden representations ( and thus the joint probabilities ) to be as close as possible , this can lead to the model learning to ignore the sensitive tokens , and thus generally a reduced dependence on them , as shown in Appendix C.6 .
Despite being completely fair in this extreme case , model performance may suffer since the generated texts should ideally be contextually conditioned on x i or xi .
Fairness through sentiment regularization .
To overcome the above-mentioned drawback , we propose an alternative method for eliminating sentiment bias using a sentiment classifier .
Instead of measuring d( h( x 1 : i ) , h( x 1 : i ) ) directly , we first apply a sentiment classifier f s h to both h( x 1 : i ) and h(x 1 : i ) , and measure d(f s h ( h( x 1 : i ) ) , f s h ( h( x 1 : i ) ) ) instead .
Note that the output of f s h can be multi-dimensional ( e.g. , a hidden layer in the sentiment classifier ) , and we can again measure the distance via cosine similarity .
Applying the classifier f s h can be seen as a projection from h( x ) to a subspace that ideally only contains sentiment -related information .
If such a perfect projection exists , we can regularize the sentiment difference between the two inputs without losing other information of the sensitive tokens .
On the one hand , this classifier - based sentiment regularization approach avoids the strong regularization of enforcing embedding similarity .
On the other hand , the effectiveness of this method is correlated with the quality of the sentiment classifier ( or sentiment " projection " ) .
3
The detailed implementation of f s h is introduced in Appendix B .
This method can be extended to specifications with other specification measures beyond sentiment by using a corresponding classifier f s h .
Implementation : Three - step curriculum training .
We use a three -step curriculum training schema .
First , we train a language model using a regular cross-entropy loss for predicting the next token given all the previous tokens , as done in a typical language model training setting ; a good validation perplexity ensures a relatively good hidden feature space has been learned .
Second , using this language model , we train a sentiment classifier f s h ( e.g. , a simple multilayer perceptron ( MLP ) ) using the extracted features from the language model .
Since sentiment labels are generally unavailable for a large-scale corpus , we label the training data with the Google Cloud sentiment API 4 and train a sentiment classifier on the data with high magnitude .
Third , with the fixed f s h from the previous step , we continue training on the subset of the original language model training set that contains any of the sensitive tokens , with an additional fairness loss L fairness based on our " embedding regularization " or " sentiment regularization " methods with a regularization parameter ?.
Meanwhile the language model is also trained on the regular cross-entropy loss ( L LM ) on predicting the next token of the unperturbed input x .
Concretely , the loss function for an input sequence x during the third step is : L( x ) = L LM ( x ) + ? ?
L fairness ( h( x 1 : i ) , h( x 1 : i ) )
We refer to this third step as the " debiasing step " , as illustrated in Figure 3 .
Note that we do not use any template at any step of training .
Experiments
We now evaluate our proposed sentiment regularization and embedding regularization methods via both automatic scores and human evaluations .
Training details Model and datasets .
We train two Trans-formerXL ( Dai et al. , 2019 ) language models similar in scale to GPT - 2 ( Radford et al. , 2019 ) on a medium-scale corpus of Wikipedia articles ( i.e. , WikiText - 103 ) and a large-scale corpus of English news articles from the WMT - 19 document - level translation task ( WMT - 19 ) .
5
We present dataset statistics , model architectures , and training details in Appendix B. Model selection .
We train language models using both embedding -regularization and sentimentregularization losses with different regularization strengths .
Based on the losses in the validation set , we report ? ? { 1 , 10 , 100 } for embeddingregularization and ? ? { 10 , 100 , 1000 } for sentiment - regularization on WMT - 19 , and ? ? { 1 , 10 , 100 } for both embedding -regularization and sentiment - regularization on WikiText - 103 .
Fairness Specifications Sensitive attributes and subgroups .
We consider three common sensitive attributes ( Country , Occupation , and Name ) to measure the counterfactual sentiment bias in language models .
Country contains 10 country names and Occupation includes 29 common occupations .
For Name , we have 17 female and 17 male common names .
We list all sensitive attribute values used in our experiments in Appendix A .
To compute the group fairness metric , we treat each country name and each occupation as its own subgroup .
For Name , we consider all female ( male ) names as one subgroup .
Sentence templates .
For each sensitive attribute , we design a set of M = 10 templates to evaluate counterfactual sentiment bias .
Each m-th template is a sentence prefix with length i m , m = 1 , . . . , M , containing a placeholder that will be replaced by a sensitive token in ?( a ) for each sensitive attribute value a ?
A . In other words , for each template we complete it by inputting the appropriate sensitive token for every a ?
A , forming a prefix x 1:im which is used as input to the language model to condition its generation on .
We sample 1000 sentences conditioned on each input prefix , and we apply an external sentiment classifier f s on the generated sentences .
All templates are described in Appendix A. Employing specific templates for model evaluation is a commonly used practice ( Zhao et al. , 2018 ; Qian et al. , 2019 ; Sheng et al. , 2019 ) , but we acknowledge that they can lack context-sensitivity , and that such evaluation is necessarily limited and not comprehensive .
Indeed , we see the advancement of model evaluation beyond specific templates as an important open research problem .
Note that during the training process ( see Figure 3 ) , we do not add any of the templates to the training set ; it is thus unlikely that our models overfit to them .
Importantly , the templates are used during evaluation only and our models need to generalize to the templates to be effective .
Evaluation Metrics Sentiment analysis and fairness metrics .
Calculating the individual fairness ( I.F. ) and group fairness ( G.F. ) scores using Eq. 3 and Eq. 4 requires sentiment scores from a sentiment classifier f s .
We evaluate the generated sentences using three sentiment classifiers : i ) the Google Cloud sentiment API ii ) a BERT ( Devlin et al. , 2019 ) - based sentiment classifier fine-tuned on the SST dataset ( Socher et al. , 2013 ) resulting in 92.7 % validation accuracy , and iii ) a simple opinion - word - based sentiment classifier , which counts the number of positive opinion words p and the number of negative opinion words n ( Hu and Liu , 2004 ) and derives its sentiment score as p/( p + n ) , and 0.5 if no opinion words exist .
We include this simple classifier as the Google Cloud sentiment API and the BERT - based classifier may themselves contain bias , which has been shown for many sentiment analysis systems ( Kiritchenko and Mohammad , 2018 ) .
The opinion- word - based method , while being less accurate ( 69.6 % accuracy on the SST validation set ) , is less prone to giving biased judgments , as it does not contain sensitive tokens or learned associations : it only relies on opinion words .
Furthermore , since we also use the Google Cloud sentiment API to create the sentiment labels of the training data for learning f s h , the BERT - based and opinion - wordbased sentiment classifiers provide additional measures of sentiment , helping to avoid findings specific to one sentiment classification system in particular .
We also conduct a human evaluation on the correlation between automatic sentiment scores and human judgments ( see ?5.5 ) .
Language model performance
One special case of a fair language model is to generate the same continuations regardless of the sensitive attribute tokens or prefixes ( e.g. , Appendix C.6 ) .
However this deteriorates the original language model 's performance , and we expect the model to still capture semantics related to the given sensitive tokens .
Thus , in addition to the fairness metrics , it is important to examine the performance of language models .
Here , we evaluate perplexity and semantic similarity for assessing language model performance and generation relevance .
Perplexity ( PPL ) and subset perplexity ( PPL s ) .
We report the perplexity ( PPL ) on the whole test set of WMT - 19 / WikiText - 103 , and the perplexity on a subset of the test set that includes articles with at least one sensitive token ( PPL s ) .
The perplexity on the whole test set reflects the language model 's overall performance .
Since the sensitive tokens only exist in a small fraction of test data , the subset perplexity PPL s examines the language model performance specifically in contexts containing sensitive tokens .
6 Semantic Similarity ( " S.S. " and " S.S. c " ) .
We compute the cosine similarity between the embedding of both the prefix and the generated continuations using the universal sentence encoder ( Cer et al. , 2018 ) .
A generated continuation is considered semantically similar if the cosine similarity is above a given threshold ( set to 0.4 ; see Appendix C.7 for further details ) .
The fraction of generated continuations with above-threshold similarity among all generated continuations then defines the semantic similarity metric ( denoted as " S.S. " ) .
We report this S.S. as a proxy for whether the generated sentences capture the original semantics .
In addition , we report the fraction of generated continuations mentioning the sensitive attribute tokens as a second proxy for semantic relevance ( denoted as " S.S. c " ) .
We also conduct a human evaluation of semantic similarity , and find a strong correlation between semantic relevance and human judgments ( see ?5.5 ) .
Evaluation Results Fairness Improvements .
In Figure 4 Trade - off between generation quality and fairness .
In Table 1 , we present the perplexity 7 and semantic similarity of models in Figure 4 . Overall , we observe a trade- off between fairness and semantic similarity .
To further illustrate the trade- off between fairness and relevance of generated texts , in Figure 6 we show both semantic similarity ( S.S. ) and individual fairness scores ( I.F. ) under different regularization strengths for WMT -19 models in sensitive attributes Country , Occupation , and Name .
We can observe that the sentiment regularization based models achieve higher semantic similarity scores than embedding regularization based models at a similar level of individual fairness score .
On the other hand , with similar semantic similarity scores , the sentiment regularization based models achieve 7 Since we do not further train our baseline model with the additional epochs of the debiasing step , both PPL and PPL s can sometimes slightly improve , while improving fairness measures .
better individual fairness scores than embedding regularization based models .
Both proposed approaches improve the individual fairness scores significantly compared to the baseline models .
The sentiment regularization based models further improve the individual fairness score by a large margin while maintaining similar semantic similarity .
Human Evaluation
We perform a human evaluation to justify the use of automatic measurements of both semantic relevance and sentiment , and the effectiveness of the proposed fairness scores .
We have 19 human annotators , each annotator labels 50 - 100 sentences , and each sentence is rated by 2 human annotators .
We measure the Spearman 's correlations with human predictions in three settings , demonstrating that the automatic metrics are positively correlated with human judgment .
Sentiment Scores : we evalu - ( c ) WMT -19 Name Figure 6 : Trade - off between I.F. and S.S. using a BERT - based sentiment classifier .
A lower I.F. is better ( note that the y-axis is reversed ) ; a higher S.S. is better .
Each point represents a model trained using a certain ?.
Overall , both embedding and sentiment regularization help reduce I.F. , and sentiment regularization works better than embedding regularization .
ate 100 sentences randomly generated by our language model .
The human annotation is then compared against the Google Cloud sentiment API , BERT - based sentiment classifier , and the opinionword - based classifier .
The Spearman 's correlations with human predictions are 0.75 , 0.79 , and 0.67 , respectively .
Semantic Similarity : we present a sensitive token , the associated prefix , and the 100 generated sentences from the language models to human annotators .
We ask the annotators to label the relevance , and then compare human annotations with " S.S. " and " S.S. c " scores ; the Spearman 's correlations with human evaluation are 0.72 and 0.63 , respectively .
Fairness Scores :
We compute the I.F. score using sentiment scores from human evaluation for baseline and sentiment - regularization models in the following two settings .
Firstly , we randomly sample 50 sentences from each of the prefixes " You are a designer , and you " and " You are an accountant , and you " .
The I.F. scores are 0.333 and 0.056 for baseline and sentiment - regularization models , respectively .
Secondly , we use instead the prefixes " Libya is " and " Iceland is " , again sampling 50 sentences from each .
The I.F. score is reduced from 0.291 ( baseline ) to 0.155 ( sentimentregularization ) .
Both evaluations demonstrate that our proposed method does indeed reduce sentiment bias - also under human evaluation .
The annotation instructions and details are shown in Appendix D.
Conclusion
As large-scale language models are increasingly deployed for real-world applications , developing methods for assessing and mitigating bias with respect to sensitive attributes is an important area of inquiry to enable pro-social outcomes .
In this paper , we have studied counterfactual sentiment bias in texts generated by large-scale language models .
We have quantified the presence of sentiment bias using our proposed novel fairness metrics based on Wasserstein distance , and demonstrated two flexible methods to reduce counterfactual sentiment bias , while maintaining similar perplexity and generation semantics .
For future work , the proposed framework could be extended to study counterfactual biases given other specifications ( e.g. , religion , ethnicity , age , or multiple - attribute crosssubgroups ) that require fairness guarantees , and could be used with other specification measures beyond sentiment .
