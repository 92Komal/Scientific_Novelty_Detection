title
PhraseRNN : Phrase Recursive Neural Network for Aspect- based Sentiment Analysis
abstract
This paper presents a new method to identify sentiment of an aspect of an entity .
It is an extension of RNN ( Recursive Neural Network ) that takes both dependency and constituent trees of a sentence into account .
Results of an experiment show that our method significantly outperforms previous methods .
Introduction Aspect- based sentiment analysis ( ABSA ) has been found to play a significant role in many applications such as opinion mining on product or restaurant reviews .
It is a task to determine an attitude , opinion and emotions of people toward aspects in a sentence .
For example , given a sentence " Except the design , the phone is bad for me " , the system should classify positive and negative as the sentiments for the aspects ' design ' and ' phone ' , respectively .
The simple approach is to calculate a sentiment score of a given aspect as the weighted sum of opinion scores , which are defined by a sentiment lexicon , of all words in the sentence ( Liu and Zhang , 2012 ; Pang and Lee , 2008 ) .
This method is further improved by identifying the aspect-opinion relations using tree kernel method ( Nguyen and Shirai , 2015 a ) .
Other researches have attempted to use unsupervised topic modeling methods .
To identify the sentiment category of the aspect , topic models which can simultaneously exploit aspect and sentiment have been proposed , such as TSLDA ( Nguyen and Shirai , 2015 b ) , ASUM ( Jo and Oh , 2011 ) , JST ( Lin and He , 2009 ) and FACTS model ( Lakkaraju et al. , 2011 ) . Recursive Neural Network ( RNN ) is a kind of deep neural network .
Using distributed representations of words ( aka word embedding ) ( Bengio et al. , 2003 ; Hinton , 1986 ) , RNN merges word representations to represent phrases or sentences .
It is one of the best methods to predict sentiment labels for the phrases ( Socher et al. , 2011 ; Socher et al. , 2012 ; Socher et al. , 2013 ) . AdaRNN ( Adaptive Recursive Neural Network ) is an extension of RNN for Twitter sentiment classification ( Dong et al. , 2014a ; Dong et al. , 2014 b ) .
This paper proposes a new method PhraseRNN for ABSA .
It is an extended model of RNN and AdaRNN , which are briefly introduced in Section 2 .
The basic idea is to make the representation of the target aspect richer by using syntactic information from both the dependency and constituent trees of the sentence .
Recursive Neural Networks for ABSA
In RNN and AdaRNN , given a sentence containing a target aspect , " binary dependency tree " is built from a dependency tree of the sentence .
Intuitively , it represents syntactic relations associated with the aspect .
Each word ( leaf ) or phrase ( internal node ) in the binary dependency tree is represented as a d-dimensional vector .
From bottom to up , the representations of a parent node v is calculated by combination of left and right child vector representations ( v l and v r ) using a global function g in RNN : g( v l , v r ) = W v l v r + b ( 1 ) where W ? d?2d is the composition matrix and b ? d is the bias vector .
Then v = f ( g( v l , v r ) ) where f is a nonlinear function such as tanh .
Instead of using only a global function g , AdaRNN employed n compositional functions G = {g 1 , ? ? ? , g n } and selected them depending on the linguistic tags and combined vectors as follows : v = f n i=1 P ( g i |v l , v r , e ) g i ( v l , v r ) ( 2 ) ? ? P ( g 1 | v l , v r , e ) ? ? ? P ( g n |v l , v r , e ) ? ? = sof tmax ? ? ?R ? ?
v l v r e ? ? ? ? ( 3 ) where ? ? is a hyper-parameter , and R ? n?( 2d +|e| ) is the parameter matrix .
The vector of the root node of the binary dependency tree is regarded as a representation of the target aspect .
It is fed to a logistic regression to predict the sentiment category of the aspect .
PhraseRNN : Phrase Recursive Neural Network
In this model , a representation of an aspect will be obtained from a " target dependent binary phrase dependency tree " constructed by combining the constituent and dependency trees .
In addition , instead of using a list of global functions G as in AdaRNN , two kinds of composition functions G in inner-phrase and H in outer - phrase are used .
Building Hierarchical Structure First , the basic phrases ( noun phrases , verb phrases , preposition phrases and so on ) are extracted from the constituent tree of the sentence .
For example , a list of phrases P = { PP [ Except the design ] , NP [ the phone ] , VP [ is bad for me ] } is extracted from the constituent tree in Figure 1 . Given a dependency tree and a list of phrases , a phrase dependency tree is created by Algorithm 1 .
The input is a dependency tree T = ( V , E ) consisting of a set of vertices V = {v 1 , ? ? ? , v | V | } and a set of relation edges E = {( r ji , v i , v j ) } between two vertices , and a list of phrases P = {p 1 , ? ? ? , p K } extracted from the constituent tree .
The output is a phrase dependency tree pT = ( pV , pE ) where pV = { T 1 , ? ? ? , T K } ( T i = ( V i , E i ) is a subtree ) and pE = {( r ji , T i , T j ) } ( a set of relations between two subtrees ) .
With the dependency tree and the phrase list in Figure 2 ( a ) , the algorithm will output a phrase dependency tree in Figure 2 ( b ) . = { T 1 , ? ? ? , T K } , T i = ( V i , E i ) and pE = {( r ji , T i , T j ) } 1 for each phrase p i ?
P do 2 V i ? { v j |v j ? p i } 3 end 4 for each edge ( r nm , v m , v n ) ?
E do 5 v m ?
p k , v n ?
p l 6 if k = l then 7 E k ?
E k ? {( r nm , v m , v n ) }
8 else 9 pE ? pE ? {( r nm , T k , T l ) } 10 end 11 end
The phrase dependency tree is transformed into a target dependent binary phrase dependency tree bpT by Algorithm 2 .
The input of the algorithm is a phrase dependency tree pT = ( pV , pE ) and a target word v t ( the aspect word we want to predict the sentiment category ) .
The output is the binary tree bpT .
Note that the leaves of the binary tree bpT are binary subtrees bT 1 , ? ? ? , bT K which are the binary versions of subtrees T 1 , ? ? ? , T K .
On the other hand , the leaves of binary subtree bT i are the words in phrase p i .
bpT and bT i are obtained by convert function defined as Algorithm 3 .
It can convert an arbitrary tree to a binary tree 1 . Figure 2 ( c ) and Figure 3 show the outputs for the aspect ' design ' and ' phone ' , respectively .
Constructing the Aspect Representation Each node in the binary tree is represented as a ddimensional vector .
In this research , we use the pre-trained Google News dataset 2 by word2vec algorithms ( Mikolov et al. , 2013 ) .
Each word is Input : phrase dependency tree : pT = ( pV , pE ) , target v t Output : target dependent binary phrase dependency tree : bpT 1 for T i = ( V i , E i ) ?
pV do 2 if v t ?
V i then 3 h ?
v t 4 else 5 h ? vertex having no head in E i 6 end 7 bT i ? convert ( E i , h ) 8 end 9 T vt ?
T i that contains v t 10 bpT ? convert ( pE , T vt ) 11 Replace all T i in bpT with bT i Algorithm 3 : Convert to a Binary Tree
The vector of the parent node v in in the binary subtree bT i , where v l and v r are the vectors of the left and right children , is computed as : 1 Function convert ( E , v t ) : 2 v ? v t 3 for v i ?
v t , v t ?
v i in E do 4 if v t ?
v i then 5 E ? E \ { v t ? v i } 6 w ? [ convert ( E , v i ) , v] 7 else 8 E ? E \ { v i ? v t } 9 w ? [ v , convert ( E , v i ) ]
v in = f n i=1 P ( g i |v l , v r , e in ) g i ( v l , v r ) ( 4 ) where e in is the external feature vector .
P ( g i |v l , v r , e in ) is the probability of function g i given the child vectors v l , v r and e in .
It is defined as Equation ( 5 ) .
? ? P ( g 1 | v l , v r , e in ) ? ? ?
P ( g n |v l , v r , e in ) ? ? = sof tmax ? ? ?R ? ?
v l v r e in ? ? ? ? ( 5 ) where ? ? is a hyper-parameter , and R ? n?( 2d +|e in | ) is the parameter matrix .
In the target dependent binary phrase dependency tree bpT , the vector of the parent node v out , where the vectors of the left and right children are v l and v r , is computed as : v out = f m i=1 P ( h i | v l , v r , e out ) h i ( v l , v r ) ( 6 ) P ( h i |v l , v r , e out ) is the probability of function h i given the child vectors v l , v r and external feature vector e out as shown in Equation ( 7 ) . 2d+| eout | ) is the parameter matrix .
? ? P ( h 1 | v l , v r , e out ) ? ? ? P ( h m |v l , v r , e out ) ? ? = sof tmax ? ? ?S ? ?
v l v r e out ? ? ? ? ( 7 ) where S ? m? (
The external features e i ( e in and e out ) of the node v i consists of three types of features : Label l , Label r and DepT ype i .
Label l and Label r are the labels of the left and right child nodes , respectively .
If node v l is a leaf word , Label l is the POS of the word v l .
Otherwise , it is the non-terminal symbol of the lowest common parent of descendants of v l in the constituent tree .
For example , the Label of the node combined from ' the ' and ' design ' in Figure 2 ( c ) is ' NP ' which is the lowest common parent of these two words in the constituent tree in Figure 1 . DepT ype i is the dependency relation for node v i .
If the left and right children of v i are leaf nodes , it is the direct relation in the dependency tree between them .
Otherwise , DepT ype i is the relation between head words of the left and right nodes .
For instance , in Figure 2 ( c ) , let a be the parent of ' is ' and ' bad ' , b is the parent of ' for ' and ' me ' , c is the parent of a and b.
DepT ype of a and b are ' COP ' and ' POBJ ' that are direct relations between two child nodes in the dependency tree in Figure 2 ( a ) .
While , DepT ype of c is ' PREP ' that is the dependency relation between two head words ' bad ' and ' for ' .
e i is a binary vector where the weight of the vector represents the presence of each feature .
We suppose a batch training data consisting of B instances { ( x ( 1 ) , t ( 1 ) ) , ? ? ? , ( x ( B ) , t ( B ) ) } , where x ( b ) and t ( b ) are the aspect and its sentiment category of b-th instance .
Let y ( b ) be the predicted sentiment category for aspect x ( b ) by PhraseRNN .
The goal is to minimize the loss function which is the sum of the mean of negative log likelihood and L2 regularization penalty in a batch training set as in Equation ( 8 ) .
L = ? 1 B B b=1 log ( P ( y ( b ) = t ( b ) | x ( b ) , ? ) ) + ? ? i ? ? i 2 ( 8 ) where ? is a constant controlling the degree of penalty , ? is all the parameters in the model .
Stochastic gradient descent is used to optimize the loss function .
Backpropagation is employed to propagate the errors from the top node to the leaf nodes .
The derivatives of parameters are used to update the parameters .
Evaluation
We use the restaurant reviews dataset in Se-mEval2014
Task 4 consisting of over 3000 English sentences .
For each aspect , " positive " , " negative " or " neutral " is annotated as its polarity .
Dataset is divided into three parts : 70 % training , 10 % development and 20 % test .
We compare the following methods : ASA w/o RE : It defines a sentiment score of a given aspect as the weighted sum of opinion scores of all words in the sentence , where the weight is defined by the distance from the aspect ( Liu and Zhang , 2012 ; Pang and Lee , 2008 ) . ASA with RE : It improves " ASA w/o RE " by firstly identifying the aspect-opinion relations using tree kernel , then integrating them to the sentiment calculation ( Nguyen and Shirai , 2015a ) . RNN : It uses only one global function g 1 over the binary dependency tree .
AdaRNN :
It uses multi-composition functions G = {g 1 , ? ? ? , g n } over a binary dependency tree ( Dong et al. , 2014a ) . PhraseRNN -1 : our PhraseRNN with only one global function : G = H = g 1 PhraseRNN - 2 : our PhraseRNN with two global functions .
One for inner-phrase , the other for outer- phrase : G = g 1 and H = h 1 PhraseRNN -3 : our PhraseRNN with multiple global functions : G = H = {g 1 , ? ? ? , g n } PhraseRNN - 4 : our PhraseRNN with two lists of global functions .
One for inner-phrase , the other for outer - phrase : ( Manning et al. , 2014 ) is used to parse the sentence and obtain constituent and dependency trees .
For RNN , AdaRNN and PhraseRNN , the optimal parameters , which minimize the error in the development set , are used for the sentiment classification of the test set .
We set ? = 1 for AdaRNN and PhraseRNN since it is reported that ? = 1 is the best parameter ( Dong et al. , 2014a ) .
The optimized number of composition functions n and m = n 2 are selected by grid search with n = { 2 , 4 , 6 , 8 , 10 } on the development set .
? = 0.0001 is employed .
Accuracy ( A ) , Precision ( P ) , Recall ( R ) and F-measure ( F ) are used as evaluation metrics 3 . G = {g 1 , ? ? ? , g n } and H = {h 1 , ? ? ? , h m } Stanford CoreNLP
Table 1 shows the results of the methods .
Differences of PhraseRNN and RNN are verified by statistical significance tests .
We use the paired randomization test because it does not require additional assumption about distribution of outputs ( Smucker et al. , 2007 ) .
The results indicate that four variations of our PhraseRNN outperform " ASA w/ o RE " , " ASA with RE " , RNN and AdaRNN methods from 5.35 % to 19.44 % accuracy and 8 % to 16.48 % F-measure .
Among four variations , PhraseRNN - 2 and PhraseRNN - 3 achieved the best performance .
By using different global functions in the inner and outer phrases , PhraseRNN - 2 improves PhraseRNN - 1 by 2.54 % F-measure while keeping the comparable accuracy .
Using multi-composition functions is also effective since PhraseRNN - 3 was better than PhraseRNN - 1 by 1.55 % accuracy .
PhraseRNN - 4 improved PhraseRNN - 3 by 6.38 % precision while keeping comparable in other metrics .
Since our PhraseRNN - 1 and PhraseRNN - 3 outperform RNN and AdaRNN ( the models relying on the binary dependency tree ) respectively , we can conclude that our target dependent binary phrase dependency tree is much effective than binary dependency tree for ABSA .
In the data used in ( Dong et al. , 2014a ) , one sentence contains only one aspect .
On the other hand , two or more aspects can be appeared in one sentence in SemEval 2014 data .
It is common in the real text .
To examine in which cases our method is better than the others , we conduct an additional experiment by dividing the test set into three disjoint subsets .
The first subset ( S1 ) contains sentences having only one aspect .
The second subset ( S2 ) 3 Precision , Recall and F-measure are the average for three polarity categories weighted by the number of true instances .
and third subset ( S3 ) have two or more aspects in each sentence .
All aspects in a sentence in S2 have the same sentiment category , while different sentiment categories in S3 .
The number of aspects in S1 , S2 and S3 are 200 , 323 and 187 , respectively .
Table 2 shows the number of aspects where their sentiments are correctly identified by the methods in the subsets S1 , S2 and S3 .
The accuracies are also shown in parentheses .
Among three subsets , S3 is the most difficult and ambiguous case .
In all methods , the performance in S3 is worse than S1 and S2 .
Comparing with other methods in each subset , PhraseRNN improves the accuracy in S2 more than in S1 and S3 .
Conclusion
We proposed PhraseRNN to identify the sentiment of the aspect in the sentence .
Propagating the semantics through the binary dependency tree in RNN and AdaRNN could not be enough to represent the sentiment of the aspect .
A new hierarchical structure was constructed by integrating the dependency relations and phrases .
The results indicated that our PhraseRNN outperformed " ASA w/ o RE " , " ASA with RE " , RNN and AdaRNN .
Figure 1 : 1 Figure 1 : Example of a Constituent Tree
