title
CityU-DAC : Disambiguating Sentiment -Ambiguous Adjectives within Context
abstract
This paper describes our system participating in task 18 of SemEval - 2010 , i.e. disambiguating Sentiment -Ambiguous Adjectives ( SAAs ) .
To disambiguating SAAs , we compare the machine learning - based and lexiconbased methods in our submissions :
1 ) Maximum entropy is used to train classifiers based on the annotated Chinese data from the NTCIR opinion analysis tasks , and the clause -level and sentence - level classifiers are compared ;
2 ) For the lexicon- based method , we first classify the adjectives into two classes : intensifiers ( i.e. adjectives intensifying the intensity of context ) and suppressors ( i.e. adjectives decreasing the intensity of context ) , and then use the polarity of context to get the SAAs ' contextual polarity based on a sentiment lexicon .
The results show that the performance of maximum entropy is not quite high due to little training data ; on the other hand , the lexicon - based method could improve the precision by considering the polarity of context .
Introduction
In recent years , sentiment analysis , which mines opinions from information sources such as news , blogs , and product reviews , has drawn much attention in the NLP field ( Hatzivassiloglou and McKeown , 1997 ; Pang et al. , 2002 ; Turney , 2002 ; Hu and Liu , 2004 ; Pang and Lee , 2008 ) .
It has many applications such as social media monitoring , market research , and public relations .
Some adjectives are neutral in sentiment polarity out of context , but they could show positive , neutral or negative meaning within specific context .
Such words can be called dynamic sentiment -ambiguous adjectives ( SAAs ) .
However , SAAs have not been intentionally tackled in the researches of sentiment analysis , and usually have been discarded or ignored by most previous work .
Wu et al. , ( 2008 ) presents an approach of combining collocation information and SVM to disambiguate SAAs , in which the collocationbased method was first used to disambiguate adjectives within the context of collocation ( i.e. a sub-sentence marked by comma ) , and then the SVM algorithm was explored for those instances not covered by the collocation - based method .
According to their experiments , their supervised algorithm achieves encouraging performance .
The task 18 at SemEval - 2010 is intended to create a benchmark dataset for disambiguating SAAs .
Given only 100 trial sentences , but not provided with any official training data , participants are required to tackle this problem data by unsupervised approaches or use their own training data .
The task consists of 14 SAAs , which are all high -frequency words in Mandarin Chinese .
They are ?| big , ?|small , ?| many , ? | few , ?|high , ?| low , ?| thick , ?| thin , ?| deep , ?| shallow , ?| heavy , ?| light , ?|huge , ? | grave .
This task deals with Chinese SAAs , but the disambiguating techniques should be language - independent .
Please refer to ( Wu and Jin , 2010 ) for more descriptions of the task .
In our participating system , the annotated Chinese data from the NTCIR opinion analysis tasks is used as training data with the help of a combined sentiment lexicon .
A machine learning - based method ( namely maximum entropy ) and the lexicon - based method are compared in our submissions .
The results show that the performance of maximum entropy is not quite high due to little training data ; on the other hand , the lexicon - based method could improve the precision by considering the context of SAAs .
In Section 2 , we briefly describe data preparation of sentiment lexicon and training data .
Our approaches for disambiguating SAAs are given in Section 3 .
The experiment and results are presented in Section 4 , followed by a conclusion in Section 5 .
Data Preparation
Sentiment Lexicon Several traditional Chinese resources of polar words / phrases are collected , including NTU Sentiment Dictionary 1 , The Lexicon of Chinese Positive Words ( Shi and Zhu , 2006 ) , The Lexicon of Chinese Negative Words ( Yang and Zhu , 2006 ) 0 , and City U 's sentiment - bearing word / phrase list , which were manually marked in the political news data by trained annotators ( Benjamin and Lu , 2008 ) .
Sentimentbearing items marked with the SENTIMENT_KW tag ( SKPI ) , including only positive and negative items but not neutral ones , were also automatically extracted from the Chinese sample data of NTCIR -6 OAPT ( Seki et al. , 2007 ) .
All these polar item lexicons were combined , and the combined polar item lexicon consists of 13,437 positive items and 18,365 negative items , a total of 31,802 items .
Training Data
The training data is extracted from the Chinese sample and test data from the NTCIR opinion analysis task , including NTCIR - 6 ( Seki et al. , 2007 ) , NTCIR -7 ( Seki et al. , 2008 ) and NTCIR -8 ( Seki et al. , 2010 ) .
The NTCIR opinion analysis tasks provide an opportunity to evaluate the techniques used by different participants based on a common evaluation framework in Chinese ( simplified and traditional ) , Japanese and English .
For data from NTCIR -6 and NTCIR -7 , three annotators manually marked the polarity of each opinionated sentence , and the lenient polarity is used here as the gold standard ( please refer to Seki et al. , 2008 for explanation of lenient and strict standard ) .
For each opinionated sentence from NTCIR -8 , only two annotators marked and the strict polarity is used as the gold standard .
The traditional Chinese sentences are transferred into simplified Chinese .
In total , there are about 12 K opinionated sentences annotated with polarity , out of which about 9 K are marked as 1 http://nlg18.csie.ntu.edu.tw:8080/opinion/index.html positive or negative , and others neutral .
All the 9K sentences plus the 100 sentences from the trial data are used as the sentence - level training data .
Meanwhile , we also try to get the clause - level training data since the context of collocation within sub-sentences are quite crucial for disambiguating SAAs according to Wu et al . ( 2008 ) .
From the 9K positive / negative sentences above , we automatically extract the clause for each occurrence of SAAs .
Note the polarity for a whole sentence is not necessarily the same with that of the clause containing SAAs .
Consider the sentence ? ? ? ? ? ? ? ? ? ? ? ?
( In the current large circumstance of the world , China and Russia support each other ) .
The polarity of the whole sentence is positive , while the clause ?( In the current large circumstance of the world ) containing a SAA ? ( large ) is neutral , and the polarity lies in the second part of the whole sentence , i.e. ? ? ( support each other ) .
Thus , we manually checked the polarity of clauses containing SAAs .
Due to time limitation , we only checked 465 clauses .
Plus the clauses extracted from 100 trial sentences , the final clause - level training data consist of 565 positive / negative clauses containing SAAs .
Our Approach for Disambiguating SAAs
To disambiguating SAAs , we use the maximum entropy algorithm and the sentiment lexiconbased method , and also combine them together .
The Maximum Entropy - based Method Maximum entropy classification ( MaxEnt ) is a technique which has proven effective in a number of natural language processing applications ( Berger et al. , 1996 ) .
Le Zhang 's maximum entropy tool 2 is used for classification .
The Chinese sentences are segmented into words using a production segmentation system .
Unigrams of words are used as basic features for classification .
Bigrams are also tried , but does not show improvement , and thus are not described in details here .
The Lexicon- based Method For the lexicon- based method , we first classify the 14 adjectives into two classes : intensifiers
The hypothesis here is that intensifiers will receive the polarity of their collocations while suppressors will get the opposite polarity of their collocations .
For example , ? ?
| achievement could be collocated with one of the following intensifiers : ?| big , ?| many or ?|high , and the adjectives just receive the polarity of ? ?
| achievement , which is positive .
Meanwhile , ? ?| pollution could be collocated with one of the following suppressors : ?| small , ?| few , ?|low , and the adjectives just receive the opposite polarity of ?| pollution , which is also positive .
Based on this hypothesis , we could get the polarity of SAAs through theirs collocation nouns within the clauses containing SAAs .
The context of SAAs is a sub-sentence marked by comma .
The sentiment lexicon mentioned in Section 2.1 is used to find polarity of collocation nouns .
Combining Maximum Entropy and Lexicon
To combine the two methods above , the lexiconbased method is first used to disambiguate the sentiment of SAAs , and the context of collocation is a sub-sentence marked by comma .
Then for those instances that are not covered by the lexicon - based method , the maximum entropy algorithm is explored .
Experiment and Results
The dataset contains two parts : some sentences were extracted from Chinese Gigaword ( LDC corpus : LDC2005T14 ) , and other sentences were gathered through the search engine like Google .
Firstly , these sentences were automatically segmented and POS - tagged , and then the ambiguous adjectives were manually annotated with the correct sentiment polarity within the sentence context .
Two annotators annotated the sentences double blindly , and the third annotator checks the annotation .
All the data of 2,917 sentences is provided as the test set , and evaluation is performed in terms of micro accuracy and macro accuracy .
We submitted 4 runs : run 1 is based on the sentence - level MaxEnt classifier ; run 2 on the clause- level MaxEnt classifier ; run 3 is got by combining the lexicon - based method and the sentence - level MaxEnt classifier ; and run 4 by combining the lexicon - based method and the clause - level MaxEnt classifier .
The official scores for the 4 runs are shown in Table 2 . 2 ) By integrating the lexicon- based method and maximum entropy ( run 3 and 4 ) , we improve the accuracy by considering the context of SAAs ; 3 ) The sentence - level maximum entropy classifier shows better macro accuracy , and clause - level one better micro accuracy .
In addition to the official scores , we also evaluate the performance of the lexicon - based method alone .
The micro and macro accuracy are respectively 0.847 and 0.835665 , showing that the lexicon - based method is more accurate than the maximum entropy algorithm ( run 1 and 2 ) .
But it only covers 1,436 ( 49 % ) of 2,917 test instances .
Because the data from the NTCIR opinion analysis task is not specifically annotated for this task , and the manually checked clauses are less than 600 , the performance of our system is not quite high compared to the highest performance achieved by other teams .
Conclusion
To disambiguating SAAs , we compare machine learning - based and lexicon- based methods in our submissions :
1 ) Maximum entropy is used to train classifiers based on the annotated Chinese data from the NTCIR opinion analysis tasks , and the clause -level and sentence - level classifiers are compared ;
2 ) For the lexicon- based method , we first classify the adjectives into two classes : intensifiers ( i.e. adjectives intensifying the intensity of context ) and suppressors ( i.e. adjectives decreasing the intensity of context ) , and then use the polarity of context to get the SAAs ' contextual polarity .
The results show that the performance of maximum entropy is not quite high due to little training data ; on the other hand , the lexicon - based method could improve the precision by considering the context of SAAs .
and suppressors .
Intensifiers refer to adjectives intensifying the intensity of context , including ?
| big , ? | many , ? | high , ?
| thick , ? | deep , ?
| heavy , ?|huge , ?| grave , while suppressors refer to adjectives decreasing the intensity of context , including ?| small , ?| few , ?|low , ? | thin , ?| shallow , ?| light .
Meanwhile , the collocation nouns are also classified into two classes : positive and negative .
Positive nouns include ? ? | quality , ? ? | standard , ? ? | level , ? ? | benefit , ? ? | achievement , etc. Negative nouns include ?
| pressure , ? ? | disparity , ? ? | problem , ? ? | risk , ?| pollution etc .
