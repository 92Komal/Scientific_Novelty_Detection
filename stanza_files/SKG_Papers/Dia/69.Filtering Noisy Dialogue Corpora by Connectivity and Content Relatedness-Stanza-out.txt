title
Filtering Noisy Dialogue Corpora by Connectivity and Content Relatedness
abstract
Large -scale dialogue datasets have recently become available for training neural dialogue agents .
However , these datasets have been reported to contain a non-negligible number of unacceptable utterance pairs .
In this paper , we propose a method for scoring the quality of utterance pairs in terms of their connectivity and relatedness .
The proposed scoring method is designed based on findings widely shared in the dialogue and linguistics research communities .
We demonstrate that it has a relatively good correlation with the human judgment of dialogue quality .
Furthermore , the method is applied to filter out potentially unacceptable utterance pairs from a large-scale noisy dialogue corpus to ensure its quality .
We experimentally confirm that training data filtered by the proposed method improves the quality of neural dialogue agents in response generation .
1
Introduction
Some million -scale datasets such as movie scripts and social media posts have become available in recent years for building neural dialogue agents ( Lison and Tiedemann , 2016 ; Henderson et al. , 2019 ) .
Such large-scale datasets can be expected to improve the performance of dialogue response generation models based on deep neural networks ( DNNs ) since the combination of DNNs and large-scale training datasets has led to considerable performance improvement in many sentence generation tasks ( Koehn and Knowles , 2017 ; Sennrich and Zhang , 2019 ; Adiwardana et al. , 2020 ) .
In contrast to the quantity of the data , the quality of the data has often been problematic .
For example , OpenSubtitles ( Lison and Tiedemann , 2016 ; Lison et al. , 2018 ) , the most widely used largescale English dialogue corpus , was constructed by collecting two consecutive lines of movie subtitles under the simplified assumption that one line of a movie subtitle is one utterance and the next line is the next utterance follow it .
Inevitably , this corpus includes unacceptable utterance pairs from the viewpoint of a conversational sequence , e.g. , caused by scene switching or flashback .
Several previous studies have identified such flaws and reported that the corpus is noisy ( Vinyals and Le , 2015 ; Li et al. , 2016 ; Baheti et al. , 2018 ) , where noisy refers to unacceptable utterance pairs in this context .
Figure 1 shows the result of our experimental investigation regarding the acceptability rate of the utterance pairs in the OpenSubtitles corpus .
2
It can be noticed from the figure that only half of the utterance pairs can be considered acceptable ( i.e. , were rated with score 5 : Strongly agree or 4 : Agree ) , and over 25 % of utterance pairs are clearly unacceptable ( i.e. , were rated with score 1 : Strongly disagree or 2 : Disagree ) from the human perspective .
3
With this situation , a straightforward research question arises , namely , Can we further improve the performance of neural response generation models by ablating unacceptable utterance pairs from training data ?
To the best of our knowledge , no previous study has explicitly focused on this question .
Thus , the goal of this paper is to provide an answer to this question .
Furthermore , it is not clear whether and how one can effectively discover unacceptable utterance pairs within largescale training datasets .
This study explores a way of constructing a scoring method for filtering noisy data filtering to improve the performance of response generation models .
To achieve the set goals , we started with a review of previous arguments about the criteria for identifying appropriate utterances in dialogues and designed our scoring function that is consistent with reflects as much of the community 's consensus as possible .
In particular , the proposed scoring method estimates the quality of utterance pairs based on the following two aspects : ( i ) the connectivity between source and target utterances and ( ii ) their content relatedness ( Section 4 ) .
The contributions of this study are the following : ?
We propose a scoring method for estimating the quality of utterance pairs in an unsupervised manner ( Section 5 ) ; ?
We reveal that our scoring method effectively detects unacceptable utterance pairs , and thus , be appropriate for noisy data filtering ( Section 6 ) ; ?
We empirically prove that our proposed data filtering method improves the performance of neural response generation models ( Section 7 ) ; and ?
We confirm that our noisy data filtering approach is effective across different languages and dataset sizes ( Section 8 ) .
2 Task Definition : Noisy Data Filtering
Let x be an utterance and y be a response to x .
Then , an utterance pair can be denoted as we refer to ( x , y ) .
Let D be a dataset that comprising a set of utterance pairs , D = { ( x , y ) } .
Then , the task can be formulated as ablating unacceptable utterance pairs from D to obtain a less noisy subset D ? D , hereinafter referred to as filtered dataset .
D can then be used to train response generation models .
This paper refers to this process as noisy data filtering , where noisy means unacceptable utterance pairs in this context .
Furthermore , we establish a function S : D ?
R is used to score the degree of acceptability of each utterance pair ( x , y ) ?
D. 3 Background 2019 ) 's method is most relevant to our study in that it cleanses dialogue corpora .
However , the main goal of their method is to eliminate generic , or boring , responses , whereas the goal of the method proposed here is to eliminate unacceptable utterance pairs .
This difference in goals leads to the essential difference in filtering strategies .
Effectiveness of filtering noisy data in neural machine translation .
Researchers in the field of neural machine translation ( NMT ) have recognized that collecting high-quality training data to be equally or even more important than exploring sophisticated model architectures ( Koehn et al. , 2018 ; Junczys -Dowmunt , 2018 ; Morishita et al. , 2018 ) .
Techniques used in neural response generation and NMT are nearly identical ; e.g. , sequenceto-sequence models ( Sutskever et al. , 2014 ) and Transformers ( Vaswani et al. , 2017 ) are often used as base model architectures .
We hypothesize that high-quality filtered dialogue data can also improve the performance of dialogue response generators .
However , the straightforward application of methods proposed for filtering noisy data in NMT may not work well due to the different nature of NMT and neural response generation tasks .
In particular , MT data have one- to- one ( ignoring paraphrases ) correspondence in source and target sentences , whereas dialogues have many - to - many mappings ( Zhao and Eskenazi , 2017 ) .
The experiments presented in this paper provide an answer to whether NMT filtering methods can perform well in dialogue response generation .
Requirements to Utterance Pairs
In this section , we investigate the requirements that should be satisfied by an acceptable utterance pair .
Criteria for Manual Evaluation
The instructions for manual evaluation provided by the dialogue community explain the key factors for distinguishing acceptable and unacceptable utterance pairs .
In many previous studies , human raters were asked to evaluate the connectivity of utterance pairs .
For instance , Shang et al. ( 2015 ) asked whether a response could be considered as an appropriate and natural response to the post .
Xing et al. ( 2017 ) asked whether the response can be used as a reply .
Pei and Li ( 2018 ) asked whether the answer is natural for the question .
Other studies have also evaluated the same or similar aspects by using keywords related to the connectivity , such as semantically appropriate for ( Akama et al. , 2017 ) or coherent with ( Shen et al. , 2017 ) , and coherence ( Lowe et al. , 2017 b ) .
Another frequently used metric is content relatedness .
For instance , Galley et al . ( 2015 ) asked human evaluators to evaluate each response in terms of their relevance to a given utterance .
Li et al. ( 2016 ) asked for the preference of responses that were more specific to certain utterances .
Ritter et al. ( 2011 ) suggested that an appropriate response should be on the same topic as the utterances .
Several other studies have also focused on evaluating the relevance between an utterance and its response ( Xu et al. , 2018 b ; Pei and Li , 2018 ; Lowe et al. , 2017 b ) .
In summary , the most widely used criteria can be categorized into connectivity and content relatedness of utterance pairs .
In fact , these two aspects are considered in the field of sociolinguistics as crucial features of conversation ( Sacks , 1989 ; Sidnell , 2010 ) .
Observation Furthermore , we investigated how the two aforementioned aspects can be observed in actual utterance pairs .
For this investigation , we use the utterance pairs scored by human raters that were used in our preliminary experiments shown in Figure 1 . Some examples are shown in Table 1 .
We observe that typical phrase pair patterns can often be found in utterance pairs with high scores .
For example , the pair ( where is , at ) in Table 1 is one of the typical phrase pair patterns that asks a place and provides an answer to it .
Other typical examples include ( why , because ) and ( what do you want , I want ) .
In discourse linguistics , such phrase pair patterns are known as the concept of cohesive devices .
Hereafter , we refer to such a typical phrase pair pattern as key phrase pair .
Moreover , in high scored utterance pairs , both an utterance and response are on the same topic .
For example , in the third example listed in Table 1 , both the utterance and response mention [ money ] .
Proposed Method
As per the discussion in the previous section , each acceptable utterance pair should satisfy the following criteria : ? connectivity - existence of key phrase pairs ? content relatedness - topic commonality
This section presents the proposed scoring functions to assess the degree of satisfying the above two criteria in an unsupervised manner .
4
Connectivity
Let f and e represent phrases obtained from x and y , respectively .
Let ?( x , y ) be a function that returns a set of all possible phrase ( n- gram ) pairs obtained from the utterance pair ( x , y ) .
We can define a finite set of all possible phrase pairs obtained from the entire dialogue data D as P D = ( x,y ) ? D ?( x , y ) .
Then , let P represent a set of key phrase pairs ( defined in Section 4.2 ) .
We assume that P is a subset of P D , i.e. , P ? P D .
To obtain P , we take advantage of a phrase table extraction technique developed in statistical machine translation , e.g. , Moses ( Koehn et al. , 2007 ) .
In this task , we require only some phrase pairs that can contribute to the connectivity of an utterance pair ( as mentioned in Section 4.2 ) , unlike the translation task where the whole sentence must correspond in mutual .
Accordingly , in our experiments , we set the null alignment ratio ( i.e. , probability of no alignment ) to 0.5 and extend the phrase extraction algorithm to include only the explicitly corresponding range as phrases in our table .
Then , we define the scoring function S C to esti- mate connectivity as : S C ( x , y ) := ( f,e ) ? ( x, y ) ?
P max nPMI ( f , e ) , 0 ? |f | | x | ? |e| |y | , ( 1 ) where |? | denotes the number of words in the phrase or utterance .
To calculate the co-occurrence , we use the normalized pointwise mutual information ( nPMI ) ( Bouma , 2009 ) , which normalizes the value so that low-frequency phrases do not take an extremely large value .
Note that we ignore the negative nPMI scores by the max ( ? , 0 ) operation because we aim only to consider the positive effect of connectivity .
The intuition behind Equation 1 is as follows : ?
If a phrase pair ( f , e ) has a high co-occurrence , the association strength of ( x , y ) including ( f , e ) might also be high . ?
If a phrase f or e occupies almost the entire sentence x or y , ( f , e ) is a strong indicator of the association of ( x , y ) .
Content Relatedness Let v( x ) and v( y ) be sentence vector of x and y , respectively .
We compute topic commonality of x and y , that is , content relatedness as follows : S R ( x , y ) := max cos ( v( x ) , v( y ) ) , 0 . ( 2 ) Cosine similarity between certain kinds of sentence vectors is known to be a good proxy of the topical relatedness of two sentences ( Conneau et al. , 2017 ; Subramanian et al. , 2018 ; Xu et al. , 2018a ) .
For the same reasons as Equation 1 , we ignore the negative cos scores by the max ( ? , 0 ) operation .
Summary Eventually , combining the above two scoring measures , we propose the following function : S C+R ( x , y ) := ?S C ( x , y ) + ?S R ( x , y ) , ( 3 ) where ? , ? ? R ?0 are hyperparameters that weigh the two viewpoints .
For our experiments , we fix ? and ? as follows : ? = 1 1 | D| ( x,y ) ? D S C ( x , y ) , ? = 1 1 | D| ( x,y ) ? D S R ( x , y ) . ( 4 ) 6 Experiments : Data Scoring
In this section , we describe our experiments that validate the effectiveness of the proposed scoring method .
Experimental Setup Dataset .
We conducted our experiments on a noisy English dialogue corpus from OpenSubtitles ( Lison et al. , 2018 ) containing roughly 441 M lines .
As explained in Section 1 , this corpus includes many unacceptable utterance pairs ( Section 1 ) .
We first applied several rule- based filtering as rudimentary preprocesses , which are typically used in the related literature .
Then , we obtained 79,445,453 utterance pairs as our training data , which excludes our test and validation data .
5 Proposed method : detailed setup .
To compute the connectivity S C , we obtained a phrase table on our training data by using Moses ( Koehn et al. , 2007 ) with fastAlign ( Dyer et al. , 2013 ) .
We then removed phrase pairs with a low co-occurrence frequency ( here , less than 200 times ) or composed of the same phrases from the table .
As a result , the phrase table included 68,891 phrase pairs , which were used as the key phrase set P as described in Section 5.1 .
To compute the content relatedness S R , we created a sentence vector from pre-trained fast - Text word embeddings ( Bojanowski et al. , 2017 ; following Arora et al. ( 2017 ) 's method , i.e. , using SIF weighting and common component removal .
Their method is reported to be useful for computing the relatedness of two given sentences and used in many studies ( Marelli et al. , 2014 b , a ; Conneau et al. , 2017 ; Subramanian et al. , 2018 ; Baheti et al. , 2018 ) .
We learned common components using 30 K sentences randomly selected from the training costs appropriately .
We then removed the first common component for all sentence vectors .
Baselines .
For comparison , we prepared the following : ? Human evaluation .
To validate the ability of the proposed method to estimate the quality of utterance pairs , we measured the correlation between its scores and those assigned by humans through crowdsourcing .
We used Amazon Mechanical Turk .
7
We randomly extracted 200 8 scored utterance pairs and asked native English-speaking crowdworkers to answer the following question for each pair :
Is the sequence of the two utterances acceptable as a dialogue ?
Workers were instructed to provide an answer on a five-point Likert scale ( from 5 : Strongly agree to 1 : Strongly disagree ) ( Likert , 1932 ) .
Unqualified workers were filtered out using attention checks .
Eventually , we used the average of the scores provided by five workers as the human score for each pair .
Results and Analysis Table 2 shows the correlation between human scores and those automatically computed by each method .
Among the methods , S C+R achieved the highest correlation with human scores .
Additionally , we also evaluated S C and S R as an ablation study of S C+R .
We found that both scores were less correlated than S C+R .
This result supports the hypothesis that both aspects , namely , connectivity and content relatedness , should be considered when evaluating the quality of utterance pairs .
Figure 2 shows the distribution of automatically computed scores corresponding to human scores .
9
As shown in ( c ) , S C+R rarely overestimates utterance pairs with low human scores but underestimates those with high human scores .
The baseline methods presented in ( a ) and ( b ) do not show such behavior .
This behavior unique to S C+R is safe for Table 3 shows several examples of utterance pairs well -scored by S C , S R , and S C+R .
Note that the score ranges differ ; e.g. , human scores are in [ 1 , 5 ] , while S R is in the range [ 0 , 1 ] .
10
Thus , we discuss relative score values ; the comparison of absolute score values across the different methods would be meaningless .
These examples demonstrate that the complementary contributions of both S C and S R allow S C+R to provide quality estimations close to human judgments .
Discussion on Low Recall Property
What types of pairs cause low recall ?
Since the proposed method prefers precision over recall , it tends to discard a certain number of acceptable utterance pairs during filtering .
To investigate the characteristics of such discarded ( yet acceptable ) pairs , we analyzed 27 pairs .
11
These pairs were selected from those that obtained a human score of 4.0 or above ( 77 pairs ) and were among the worst 50 % as scored by S C+R ( 100 pairs ) .
Consequently , we found two potential issues .
One is that human annotators may sometimes easily find the connectivity or the content relatedness for the utterance pairs with the low S C+R scores .
This observation indicates that S C and S R are still not perfect for scoring functions , and there remains room for improvement .
The possible drawbacks we have already noticed in S C and S R are that S C sometimes fails to capture the connectivity because of the limited coverage by a discrete phrase table - based approach , and S R is not robust for outof-vocabulary of word vector .
The other case is that the human annotators gave high scores , but we found no connectivity and content relatedness in the utterance pairs .
We found that some utterance pairs without any connectivity and content relatedness can be judged as acceptable by the human annotators since they can imagine the underlying context and situation of the utterance pairs using human world knowledge , such as commonsense .
We think this is a challenging issue that exceeds our focus in this paper , and thus , remains as future work .
Does our filtering undermine diversity ?
One might think that our method succeeds in filtering by assigning high scores to generic responses such as dull responses .
This concern makes sense since it is known that dialogue systems learned from the training data , including many generic utterances , tend to generate bland responses ( Cs? ky et al. , 2019 ) .
To answer this interesting question , we confirmed the diversity of utterance pairs with a high score ( i.e. , remained as training data ) and a low score ( i.e. , removed from training data ) in our S C+R ( Table 4 ) .
12
As a result , there was no significant difference between them .
Therefore , we conclude that the proposed method does not prefer only generic responses and maintains the diversity of data .
It is an essential future attempt to improve the quality of dialogue data further ( e.g. , more diversity ) after using the proposed method to remove unacceptable pairs .
Experiments : Response Generation
This section reports on the effectiveness of the proposed method for filtering noisy data in neural response generation .
Experimental Setup Training .
We obtained the filtered training data D by removing utterance pairs with low scores from the original dataset D ( approximately 10 % or 50 % of total utterance pairs were removed ) .
As a response generation model , we used a Transformer ( Vaswani et al. , 2017 ) based encoderdecoder model implemented in the fairseq toolkit ( Ott et al. , 2019 ) . 13
Transformer has demonstrated high performance in response generation ( Dinan et al. , 2019 ) and other NLP tasks .
Automatic evaluation .
Here , we report the following metrics : the average response length in tokens ( len ) , type-token ratio for { 1 , 2 } - grams ( distinct - { 1 , 2 } ) , and and BLEU - 1 ( Papineni et al. , 2002 ) .
The latter was used as a reference - based metric ; while it is widely used in previous studies ( Zhao and Eskenazi , 2017 ; Baheti et al. , 2018 ; Cs?ky et al. , 2019 ) , some studies ( e.g. , ( Liu et al. , 2016 ) ) have reported that BLEU - 1 may not be highly correlated with the human evaluation of response generation .
14 Human evaluation .
We evaluated the quality of the generated responses manually .
We asked human evaluators recruited via Amazon Mechanical Turk to evaluate responses that are generated for 100 15 input utterances randomly sampled from the test data .
We used the same task setting and protocol as described in Section 6.1 to obtain the human scores for each pair .
Higher human scores indicate that the better results .
Results and Analysis Table 5 shows the results of automatic and human evaluations of the generated responses .
The model trained on the data filtered using the proposed method S C+R produced more than three times as many distinct { 1 , 2 } - grams as the model trained on non-filtered data .
Furthermore , it outperformed the model trained on non-filtered data in the human evaluation , achieving the highest percentage of acceptable responses of 85 % .
Additionally , these results of our S C+R were better than other baselines .
To conclude , these experimental results indicate that the proposed scoring method can help generate diverse responses that are judged as acceptable by humans .
This experiment provides empirical evidence for supporting our hypothesis that the performance of neural response generation models can be improved by just removing unacceptable utterance pairs from training data , which answers the research question formulated at the start of this paper .
Multilingual Availability
While the proposed method S C+R was tested on an English corpus , it can potentially work for other languages as well .
To demonstrate this , we selected Japanese dialogue data as another case study .
16
The linguistic phenomena in Japanese are quite different from those in English , thus making this experiment to be a good test of the applicability of the proposed method to non-English languages .
Japanese dataset .
We prepare the Japanese dialogue data from Japanese OpenSubtitles ( Lison et al. , 2018 ) containing roughly 3 M lines .
We obtain 1,893,477 utterance pairs as our training data , which excludes our test and validation data .
17
Data Scoring Settings .
To compute S C , we defined a low cooccurrence frequency as less than 20 , considering the size of the Japanese corpus , and consequently obtained the key phrase pairs | P| = 19,992 .
To compute S R , we used pre-trained fastText and learned common components from all sentences in the training data .
For human evaluation , we used Yahoo ! crowdsourcing 18 to hire native Japanese -speaking workers .
The task setting and protocol are the same as those for English ( Section 6.1 ) , regardless of the crowdsourcing platform .
Results and analysis .
Table 7 shows the correlation between human scores and those automatically computed by each method .
Our method S C+R has the highest correlation with human scores , although the overall result is lower than that obtained for the English dataset .
Figure 3 shows the distribution of our S C+R corresponding to human scores .
Similar to the result obtained for English as presented in Figure 2 ( c ) , S C+R rarely overestimates utterance pairs with low human scores but underestimates those with high human scores in Japanese .
Response Generation Settings .
We used the same experimental settings described in Section 7.1 for the preparation of filtered data D and model training .
Results and analysis .
Table 8 shows the results of evaluations of the generated responses .
The filtered data generated by S C+R provided the best results in terms of almost all the metrics , including human evaluation .
It supports our hypothesis that the proposed method is also suitable for non-English languages .
Relationship with Evaluation Metric
The proposed method S C+R maps an utterance pair to a score ( scalar value ) in terms of the quality of dialogue .
That is , formally , our method is similar to the reference - free automatic evaluation metrics for dialogue agents ; both of them evaluate the response given an input utterance and also map into a score .
Recently , the novel reference -free metrics for evaluating generated responses such as USR ( Mehri and Eskenazi , 2020 ) or MAUDE ( Sinha et al. , 2020 ) ware developed .
While it is possible to use them as a scoring method for filtering noisy data , in theory , there are some concerns with applying them in practice .
One is the difference of the data of interest ; since evaluation metrics are intended for responses generated as dialogue , i.e. , somewhat valid dialogue data , it is unclear whether they also work for apparently noisy data .
Another one is the difference of desired properties ; evaluation metrics need to be sensitive to " how good is it ? " while the filtering requires to detect " is it a dialogue ? " with high accuracy .
It would be interesting to investigate the effectiveness of reference -free metrics for noisy dialogue data filtering tasks , and vice versa .
We leave these investigations for future work .
In contrast , reference - based metrics require a reference response ( i.e. , ground truth ) when they calculate scores ; such metrics include the traditional overlap- based BLEU , ROUGE , METEOR , embedding - based metrics ( Liu et al. , 2016 ) , and neural network - based RUBER ( Tao et al. , 2018 ) and ADEM ( Lowe et al. , 2017 a )
Thus , these methods cannot straightforwardly be considered as alternatives to the proposed method , which aims at filtering .
Conclusion
In light of the success of noisy corpus filtering in neural machine translation , we attempted to filter out unacceptable utterance pairs from large dialogue corpora in an unsupervised manner .
The proposed scoring method estimates the quality of utterance pairs by focusing on the two crucial aspects of dialogue , namely , the connectivity and content relatedness of utterance pairs .
We demonstrated that our scoring method has a higher correlation with human judgment than recently proposed methods .
Furthermore , we provided empirical evidence that our method improves the performance of a response generation model by removing unacceptable utterance pairs from its training data .
We hope that this study will facilitate discussions in the dialogue response generation community regarding the issue of filtering noisy corpora .
