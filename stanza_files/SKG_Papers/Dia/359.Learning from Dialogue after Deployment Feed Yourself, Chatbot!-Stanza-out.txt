title
Learning from Dialogue after Deployment : Feed Yourself , Chatbot !
abstract
The majority of conversations a dialogue agent sees over its lifetime occur after it has already been trained and deployed , leaving a vast store of potential training signal untapped .
In this work , we propose the self-feeding chatbot , a dialogue agent with the ability to extract new training examples from the conversations it participates in .
As our agent engages in conversation , it also estimates user satisfaction in its responses .
When the conversation appears to be going well , the user 's responses become new training examples to imitate .
When the agent believes it has made a mistake , it asks for feedback ; learning to predict the feedback that will be given improves the chatbot 's dialogue abilities further .
On the PERSONACHAT chitchat dataset with over 131 k training examples , we find that learning from dialogue with a selffeeding chatbot significantly improves performance , regardless of the amount of traditional supervision .
Introduction
Training a dialogue agent to converse like a human requires extensive supervision .
The most common approach is to train models to imitate humans in large corpora of crowdsourced or scraped conversations ( Serban et al. , 2015 ) .
These fullysupervised conversations tend to be expensive to collect in sufficient quantity and / or occur in settings with significant differences from the deployment environment ( Ross et al. , 2009 ) .
Instead , dialogue agents would ideally learn directly from dialogue , the conversations they participate in after deployment , which are usually abundant , taskspecific , dynamic , and cheap .
This corresponds to the way humans learn to converse - not merely observing others engaging in " expert- level " conver - sations , but instead actively adjusting and correcting our speech based on feedback woven throughout our own conversations ( Bassiri , 2011 ; Werts et al. , 1995 ) .
Giving a dialogue agent this ability would enable it to continuously improve and adapt over its lifetime , rather than requiring additional annotation costs for each and every improvement .
However , naively training a dialogue agent on its own conversations yields poor results .
For example , training a model on its own output can simply reinforce its existing failure modes , and mistakes by the agent can lead to absurd conversations that no longer resemble the target domain ( Hashimoto and Sassano , 2018 ) .
To combat this , one approach is to allow the agent to request feed - back during conversations ( Zhang et al. , 2018a ; Li et al. , 2017 b ) , e.g. , when it believes it is about to make a mistake .
This approach , however , falls victim to the Dunning - Kruger effect ( Kruger and Dunning , 1999 ) , which in this case suggests that a bad model will also be bad at knowing when it is doing a bad job .
Regardless of when feedback is requested , existing methods typically require accompanying scalar rewards or adherence to particular templates or structure to ensure that the feedback is usable by the model ( Rieser and Lemon , 2011 ; .
These requirements may be acceptable for paid annotators , but they impose unnatural workflows on unpaid conversation partners in a standard dialogue environment .
Humans are able to request and provide feedback using only natural language ; ideally , dialogue agents would be able to do the same .
In this work we propose the self-feeding chatbot , a dialogue agent with the ability to extract new examples from the conversations it participates in after deployment ( Figure 1 ) .
Concretely , in addition to being trained on the primary DIALOGUE task , the agent is trained to predict its speaking partner 's satisfaction with its responses .
When the conversation seems to be going well , the user 's responses ( but not the bot 's own utterances ) become the targets in new training examples for the DIA - LOGUE task .
When the agent believes it has made a mistake , it instead requests feedback on what it could have said instead .
Predicting the feedback that will be provided in a given context becomes an auxiliary task ( FEEDBACK ) on which the model is also trained .
Importantly , these new examples improve the agent 's dialogue abilities while using only natural responses from the user that do not require special structure , accompanying numerical feedback , or additional human intervention in order to be used .
With this approach , the conversations the chatbot participates in are sliced into two complementary datasets - one largely protected from the chatbot 's mistakes ( DIALOGUE examples ) , and one which directly addresses them ( FEEDBACK examples ) .
We validate our approach on the PER-SONACHAT ( Zhang et al. , 2018 b ) dialogue dataset , finding empirically that regardless of the number of available supervised examples , the dialogue ability of the chatbot is always improved by adding the automatically extracted examples of either type , and improves the most by adding both .
The main contributions of this work thus include the following : ?
We propose the self-feeding chatbot , a dialogue agent with the ability to extract new training examples for itself from the conversations it participates in during deployment . ?
We show that dialogue ability improves by imitating human responses when the human is satisfied , or by asking for feedback when they are not , predicting it as an auxiliary task .
?
We demonstrate that classifying user satisfaction is a learnable task important for the selffeeding process , significantly outperforming an approach based on model uncertainty .
The datasets and models described in this paper are available via the ParlAI platform ( Miller et al. , 2017 ) , along with training code .
Hyperparameter values are included in Appendix G.
Related Work
The general concepts of lifelong learning ( Silver et al. , 2013 ) and never-ending ( language ) learning ( Carlson et al. , 2010 ) are related to the topics discussed in this work , as is active learning ( Tong and Koller , 2001 ) and predictive modeling ( Schmidhuber and Huber , 1991 ) .
The specific case of learning actively from dialogue during deployment was explored for the question answering ( QA ) setting in ( Weston , 2016 ) and ( Li et al. , 2017a ) , where the authors examined multiple learning strategies on a suite of dialogue tasks with varying types of feedback , such as verbal cues ( e.g. , " Yes , that 's right ! " ) and scalar rewards .
Most relevant to our work was their use of forward prediction , where the learner improved in quality by trying to predict the teacher 's responses without an explicit reward signal .
Our work extends this idea , adding the ability for the model to recognize its mistakes and request feedback explicitly , and moving beyond QA to the more general chit- chat setting where there may be many valid responses in a given context .
Learning to ask questions is another area that has been studied ( Strub et al. , 2017 ; , 2018 ; Rao and Daum ? , 2018 ) .
While those works focused on identifying which question to ask in a given context , in this work we are more interested in first learning when to ask a question .
Li et al. ( 2017 b ) considered this question as well , but again in the context of a QA setting rather than dialogue .
Hashimoto and Sassano ( 2018 ) used user responses to detect mistakes made by a deployed virtual assistant , showing that model mistakes can be identified in chit-chat , weather , or web search domains .
However , they did not explore how to use these identified mistakes to improve the model further ; their agent was not equipped to feed itself .
Eskenazi et al. ( 2018 ) also found that the correctly assessing the appropriateness of chatbot responses is highly dependent on user responses and not preceding context alone .
There are other , somewhat less related , ways to use feedback during dialogue for learning , notably for collecting knowledge to answer questions ( Mazumder et al. , 2018 ; Hixon et al. , 2015 ; Pappu and Rudnicky , 2013 ) , and more commonly in reinforcement learning settings , where the feedback is a scalar rather than the dialogue messages themselves ( Levin et al. , 2000 ; Schatzmann et al. , 2006 ; Rieser and Lemon , 2011 ; Hong et al. , 2019 ) .
In particular ( Serban et al. , 2017 ) employ user sentiment detection for reward shaping in their Alexa prize entry .
Finally , our work improves dialogue quality by utilizing larger datasets with noisier labels than traditional supervision .
Other applications of weak supervision to dialogue ( Mallinar et al. , 2019 ) and relation extraction have observed similar results ( Bunescu and Mooney , 2007 ; Hancock et al. , 2018 ; Ratner et al. , 2017 ) .
The Self-Feeding Chatbot
The lifecycle of a self-feeding chatbot is outlined in Figure 2 .
In the initial training phase , the dialogue agent is trained on two tasks - DIALOGUE ( next utterance prediction , or what should I say next ? ) and SATISFACTION ( how satisfied is my speaking partner with my responses ? ) - using whatever supervised training data is available .
We refer to these initial DIALOGUE examples as Human-Human ( HH ) examples , since they were generated in conversations between two humans .
In the deployment phase , the agent engages in multi-turn conversations with users , extracting new deployment examples of two types .
Each turn , the agent observes the context x ( i.e. , the conversation history ) and uses it to predict its next utterance ? and its partner 's satisfaction ?.
If the satisfaction score is above a specified threshold t , the agent extracts a new Human - Bot ( HB ) DIALOGUE example using the previous context x and the human 's response y and continues the conversation .
If , however , the user seems unsatisfied with its previous response ( ? < t ) , the agent requests feedback with a question q , and the resulting feedback response f is used to create a new example for the FEEDBACK task ( what feedback am I about to receive ? ) .
The agent acknowledges receipt of the feedback and the conversation continues .
The rate at which new DIALOGUE or FEEDBACK examples are collected can be adjusted by raising or lowering the satisfaction threshold t ( we use t = 0.5 ) .
1 Periodically , the agent is retrained using all available data , thereby improving performance on the primary DIALOGUE task .
It is important to note that the user 's responses are always in the form of natural dialogue .
In particular , at no point are the new FEEDBACK examples inspected , post-processed , or cleaned .
Instead , we rely on the fact that the feedback is not random : regardless of whether it is a verbatim response , a description of a response , or a list of possible responses ( see Table 2 for examples ) , there is a learnable relationship between conversation contexts and their corresponding feedback which requires many of the same language understanding skills to master as does carrying on a normal conversation .
The experiments in this paper are limited to the setting where the number of supervised and deployment examples are on the same order of magnitude ; however , we envision scenarios in which the number of deployment examples can easily grow to 100 ?
or more the number of supervised examples over the chatbot 's deployment lifetime , effectively providing a massive task -specific corpus at minimal cost .
Table 1 reports the sizes of each dataset , all of which are available via ParlAI .
Task 1 : DIALOGUE
The chatbot 's primary task ( DIALOGUE ) is to carry on a coherent and engaging conversation with a speaking partner .
Training examples take the form of ( x , y ) pairs , where x is the context of the conversation ( the concatenation of all responses so far up to some history length , delimited with tokens marking the speaker ) , and y is the appropriate response given by the human .
The Human-Human ( HH ) portion of the DIA - LOGUE dataset comes from the PERSONACHAT dataset ( Zhang et al. , 2018 b ) short dialogues ( 6 - 8 turns ) between two crowdworkers ( humans ) who have been assigned short text profiles and are instructed to " chat with the other person naturally and try to get to know each other . "
We chose this dataset because of its size ( over 145 k total examples ) , the breadth of topics it covers , and its focus on promoting engaging conversations , which we anticipate being a necessary property of a chatbot that people will be willing to chat with voluntarily and repeatedly .
We use the standard splits of the dataset made available in ParlAI as a part of the ConvAI2 challenge ( Burtsev et al. , 2018 ) .
Since the question of how to incorporate external knowledge ( such as profiles ) in dialogue is an open research question of its own ( Li et al. , 2016 ; Luan et al. , 2017 ; Luo et al. , 2018 ) and we are primarily interested in the question of learning from dialogue , we discard the profiles and simply train and test on the conversations themselves , making the dataset more challenging in terms of raw performance scores .
The Human-Bot ( HB ) portion of the DIA - LOGUE dataset is extracted during deployment as described earlier , where the user is again a crowdworker instructed to chat naturally .
The context may contain responses from both the human and the bot , but the target response is always from the human , as we will see experimentally that targeting bot responses degrades performance .
Because the chit-chat domain is symmetric , both the HH and HB DIALOGUE examples are used for the same task .
In an asymmetric setting where the bot has a different role than the human , it is unclear whether HB examples may still be used as an auxiliary task , but FEEDBACK examples will remain usable .
Task 2 : SATISFACTION
The objective of the SATISFACTION auxiliary task is to predict whether or not a speaking partner is satisfied with the quality of the current conversation .
Examples take the form of ( x , s ) pairs , where x is the same context as in the DIALOGUE task , and s ? [ 0 , 1 ] , ranging from dissatisfied to satisfied .
Crucially , it is hard to estimate from the bot 's utterance itself whether the user will be satisfied , but much easier using the human 's response to the utterance , as they may explicitly say something to that effect , e.g .
" What are you talking about ? " .
The dataset for this task was collected via crowdsourcing .
Workers chatted with our baseline dialogue agent and assigned a rating 1 - 5 for the quality of each of the agent 's responses .
2 Contexts with rating 1 were mapped to the negative class ( dissatisfied ) and ratings [ 3 , 4 , 5 ] mapped to the positive class ( satisfied ) .
Contexts with rating 2 were discarded to increase the separation between classes for a cleaner training set .
Note that these numeric ratings were requested only when collecting the initial training data , not during deployment , where only natural dialogue is used .
Task 3 : FEEDBACK
The objective of the FEEDBACK auxiliary task is to predict the feedback that will be given by the speaking partner when the agent believes it has made a mistake and asks for help .
Examples take the form of ( x , f ) pairs , where x is the same context as the other two tasks and f is the feedback utterance .
2 A snapshot of the data collection interface and sample conversations are included in the Appendix .
Training data for this task is collected during deployment .
Whenever the user 's estimated satisfaction is below a specified threshold , the chatbot responds " Oops !
Sorry .
What should I have said instead ? " .
3
A new example for the FEEDBACK task is then extracted using the context up to but not including the turn where the agent made the poor response as x and the user 's response as f ( as shown in Figure 1 ) .
At that point to continue the conversation during deployment , the bot 's history is reset , and the bot instructs the user to continue , asking for a new topic .
Examples of FEEDBACK responses are shown in Table 2 .
Model and Settings
Model Architecture
The self-feeding chatbot has two primary components : an interface component and a model component .
The interface component is shared by all tasks , and includes input / output processing ( tokenization , vectorization , etc. ) , conversation history storage , candidate preparation , and control flow ( e.g. , when to ask a question vs. when to give a normal dialogue response ) .
The model component contains a neural network for each task , with embeddings , a network body , and a task head , some of which can be shared .
In our case , we obtained maximum performance by sharing all parameters between the FEEDBACK and DIALOGUE tasks ( prepending FEEDBACK responses with a special token ) , and using separate model parameters for the SATISFACTION task .
Identifying optimal task structure in multi-task learning ( MTL ) architectures is an open research problem ( Ruder , 2017 ) .
Regardless of what parameters are shared , each training batch contains examples from only one task at a time , candidate sets remain separate , and each task 's cross-entropy loss is multiplied by a task -specific scaling factor tuned on the validation set to help account for discrepancies in dataset size , loss magnitude , dataset relevance , etc .
Our dialogue agent 's models are built on the Transformer architecture ( Vaswani et al. , 2017 ) , which has been shown to perform well on a variety of NLP tasks ( Devlin et al. , 2018 ; Radford et al. , 2018 ) , including multiple persona- based chat applications ( Shuster et al. , 2018a , b ; Rashkin et al. , 2018 ) .
For the SATISFACTION task , the context x is encoded with a Transformer and converted to the scalar satisfaction prediction ? by a final linear layer in the task head .
The DIALOGUE and FEEDBACK tasks are set up as ranking problems , as in ( Zhang et al. , 2018 b ; Mazar ?
et al. , 2018 ) , where the model ranks a collection of candidate responses and returns the top-ranked one as its response .
The context x is encoded with one Transformer and ? and f candidates are encoded with another .
The score for each candidate is calculated as the dot product of the encoded context and encoded candidate .
During training , negative candidates are pulled from the correct responses for the other examples in the mini-batch .
During evaluation , however , to remain independent of batch size and data shuffling , each example is assigned a static set of 19 other candidates sampled at random from its split of the data .
During deployment , all 127,712 unique HH DIALOGUE candidates from the train split are encoded once with the trained model and each turn the model selects the top-ranked one for the given context .
Model Settings Contexts and candidates are tokenized using the default whitespace and punctuation tokenizer in ParlAI .
We use a maximum dialogue history length of 2 ( i.e. , when making a prediction , the dialogue agent has access to its previous utterance and its partner 's response ) .
Tokens are embedded with fastText ( Bojanowski et al. , 2017 ) 300 - dimensional embeddings .
We do not limit the vocabulary size , which varies from 11.5 k to 23.5 k words in our experiments , depending on the training set .
The Transformer is implemented in PyTorch ( Paszke et al. , 2017 ) within the ParlAI framework .
We use the AdaMax ( Kingma and Ba , 2014 ) optimizer with a learning rate schedule that decays based on the inverse square root of the step number after 500 steps of warmup from 1e - 5 .
We use proportional sampling ( Sanh et al. , 2018 ) to select batches from each task for training , with batch size 128 .
Each Transformer layer has two attention heads and FFN size 32 .
The initial learning rate ( 0.001-0.005 ) , number of Transformer layers ( 1 - 2 ) , and task - specific loss factors ( 0.5 - 2.0 ) are selected on a per-experiment basis based on a grid search over the validation set averaged over three runs ( we use the DIALOGUE validation set whenever multiple tasks are involved ) .
We use early stopping based on the validation set to decide when to stop training .
The hyperparameter values for the experiments in Section 5 are included in Appendix G. Note that throughout development , a portion of the DIALOGUE validation split was used as an informal test set .
The official hidden test set for the DIALOGUE task was used only to produce the final numbers included in this paper .
Experimental Results
Throughout this section , we use the ranking metric hits @X / Y , or the fraction of the time that the correct candidate response was ranked in the top X out of Y available candidates ; accuracy is another name for hits @1 / Y.
Statistical significance for improvement over baselines is assessed with a two -sample one- tailed T-test .
Benefiting from Deployment Examples
Our main result , reported in Table 3 , is that utilizing the deployment examples improves accuracy on the DIALOGUE task regardless of the number of available supervised ( HH ) DIALOGUE examples .
4
The boost in quality is naturally most pronounced when the HH DIALOGUE training set is small ( i.e. , where the learning curve is steepest ) , yielding an increase of up to 9.4 accuracy points , a 31 % improvement .
However , even when the entire PER -
We also calculated hit rates with 10,000 candidates ( instead of 20 ) , a setup more similar to the interactive setting where there may be many candidates that could be valid responses .
In that setting , models trained with the deployment examples continue to outperform their HH -only counterparts by significant margins ( see Appendix B ) .
On average , we found that adding 20 k FEED - BACK examples benefited the agent about as much as 60 k HB DIALOGUE examples .
5
This is somewhat surprising given the fact that nearly half of the FEEDBACK responses would not even be reasonable responses if used verbatim in a conversation ( instead being a list of options , a description of a response , etc. ) as shown in Table 2 .
Nevertheless , the tasks are related enough that the DI - ALOGUE task benefits from the MTL model 's improved skill on the FEEDBACK task .
And whereas HB DIALOGUE examples are based on conversations where the user appears to already be satisfied with the agent 's responses , each FEEDBACK example corresponds to a mistake made by the model , giving the latter dataset a more active role in improving quality .
Interestingly , our bestperforming model , which achieves 46.3 accuracy on DIALOGUE , scores 68.4 on FEEDBACK , suggesting that the auxiliary task is a simpler task overall .
When extracting HB DIALOGUE examples , we ignore human responses that the agent classifies as expressing dissatisfaction , since these turns do not represent typical conversation flow .
Including these responses in the 60 k HB dataset decreases hits @1 / 20 by 1.2 points and 0.6 points when added to 20k and 131 k HH DIALOGUE examples , respectively .
We also explored using chatbot responses with favorable satisfaction scores ( ? > t ) as new training examples , but found that our models performed better without them ( see Appendix D for details ) .
We also found that " fresher " feedback results in bigger gains .
We compared two models trained on 20 k HH DIALOGUE examples and 40 k FEED - BACK examples - the first collected all 40 k FEED - BACK examples at once , whereas the second was retrained with its first 20 k FEEDBACK examples before collecting the remaining 20k .
While the absolute improvement of the second model over the first was small ( 0.4 points ) , it was statistically significant ( p = 0.027 ) and reduced the gap to a model trained on fully supervised ( HH ) DIALOGUE examples by 17 % while modifying only 33 % of the training data .
6
Predicting User Satisfaction
For maximum efficiency , we aim to ask for feedback when it will most benefit our model .
The approach we chose ( classifying the tone of partner responses ) takes advantage of the fact that it is easier to recognize that a mistake has already been made than it is to avoid making that mistake ; or in other words , sentiment classification is generally an easier task than next utterance prediction .
We compare this to the approach of asking for feedback whenever the model is most uncertain what to say next .
This approach acts on the assumption that the model will be least confident when it is about to make a mistake , which we find very frequently to not be the case .
Not only is it difficult to recognize one 's own mistakes , but also there are often multiple valid responses to a given context ( e.g. , " Yes , I love seafood ! " or " Yuck , fish is gross . " ) - a lack of certainty about which to use does not necessarily suggest a poor model .
Table 4 reports the maximum F1 scores achieved by each method on the SATISFACTION test set .
For the model uncertainty approach , we tested two variants : ( a ) predict a mistake when the confidence in the top rated response is below some threshold t , and ( b ) predict a mistake when the gap between the top two rated responses is below the threshold t.
We used the best-performing standalone DIALOGUE model ( one trained on the full 131 k training examples ) for assessing uncertainty and tuned the thresholds to achieve maximum F1 score .
For the user satisfaction approach , we trained our dialogue agent on just the SATISFAC - TION task .
Finally , we also report the performance of a regular-expression - based method which we used during development , based on common ways of expressing dissatisfaction that we observed in our pilot studies , see Appendix F for details .
As shown by Table 4 , even with only 1 k training examples ( the amount used for the experiments in Section 5.1 ) , the trained classifier significantly outperforms both the uncertainty - based methods and our original regular expression , by as much as 0.28 and 0.42 F1 points , respectively .
Future Work
In this work we learned from dialogue using two types of self-feeding : imitation of satisfied user messages , and learning from the feedback of unsatisfied users .
In actuality , there are even more ways a model could learn to improve itself - for example , learning which question to ask in a given context to receive the most valuable feedback .
One could even use the flexible nature of dialogue to intermix data collection of more than one typesometimes requesting new FEEDBACK examples , and other times requesting new SATISFACTION examples ( e.g. , asking " Did my last response make sense ? " ) .
In this way , a dialogue agent could both improve its dialogue ability and its potential to improve further .
We leave exploration of this metalearning theme to future work .
A Data Collection Protocol
Here we report in greater detail the protocol we followed to collect the SATISFACTION , FEED - BACK , and HB DIALOGUE examples used in the experiments of Section 5 .
We first trained our dialogue agent on just the DIALOGUE task with 20 k HH examples .
This agent was deployed on a crowdsourcing platform using the interface shown in Appendix H.2 to collect 2.5 k SATISFACTION examples .
These were split into 1 k train , 500 validation , and 1 k test examples .
The agent was retrained using the 20k HH DIALOGUE examples and 1 k SATISFACTION examples , then deployed to collect the first batch of deployment examples .
We collected 40 k FEEDBACK examples ( feedback set A ) over the course of 17,250 conversations with 10 turns each ( 20 utterances , including the initial prompt ) .
We then retrained the agent on all three datasets , using the same 20 k
No filtering was performed on the crowdworker conversations .
Upon inspection after the fact , some workers did indeed give poor responses , make typographical mistakes , misunderstand the instructions , try to use the chatbot as a question answering interface , etc .
We assume however that similar types of noise will be present in most chatbot deployment environments and opted to maintain a workflow that truly does not require developer intervention to use the newly collected examples .
B Results with 10 k Candidates HH HB FB Hits@X/10,000 @1 @10 @ 100 20k
C PERSONACHAT Comparisons and Baselines
Our experiments use the PERSONACHAT distribution that was released as a part of the Con-vAI2 ( Burtsev et al. , 2018 ) challenge .
This distribution is slightly cleaner than the original PER -SONACHAT release and comes with a new crowdsourced test set .
In order to compare with the models and baselines used in the original PER -SONACHAT paper ( Zhang et al. , 2018 b ) , we report in this section the performance of our models on the original PERSONACHAT test set , not the ConvAI2 test set .
Note that empirically , near Hits@1 / 20 = 50 , each additional point of improvement corresponds to tens of thousands of fullysupervised Human-Human DIALOGUE examples .
All numbers reported here are for models that do not have access to the profiles that were used in the creation of the conversations ; models that do have access to this additional information tend to perform even better .
Model Hits@1/20
We also considered whether it was possible to consistently identify really good responses by the chatbot , rather than the really bad ones .
These could potentially be used as DIALOGUE examples along with the ones that have human responses as targets ( which we refer to as HH and HB in the paper ) .
To explore this question , we modified our SATISFACTION dataset so that contexts with a rating of 5 were the positive class and ones with ratings [ 1 , 2 , 3 ] were the negative class ( discarding ratings of 4 to increase the separation between classes ) .
The results were negative - even with a training set of over 34 k examples , the maximum precision we were able to achieve while maintaining at least 10 % recall was 0.70 , which is insufficient to improve performance on the DI - ALOGUE task .
Upon inspection , it appears that really good responses are hard to identify because most of the time they look like a normal human-tohuman conversation , and recognizing an appropriate next utterance is precisely the DIALOGUE task that we are trying to solve !
Negative responses , however , are much more semantically similar to one another , since most express one of a few common ideas such as asking for clarification or conveying confusion .
E The Effect of Data Freshness HH HB A HBB FB A FBB Total Hits@1/20
F SATISFACTION Regular Expressions
As described in Section 5.2 , before we trained a classifier on the SATISFACTION task , we used the union of the following six regular expressions ( using Python regular expression syntax ) to identify user dissatisfaction and trigger feedback requests : r" i
H Data Collection Interfaces H.1 Deployment Data Collection
We simulated a deployment environment on a crowdsourcing platform with the following interface .
Crowdworkers were told to " Talk as you would to a person you have just met . "
H.2 SATISFACTION Data Collection
The environment for collecting SATISFACTION ratings was very similar to the deployment environment , with the exception that workers were required to give a rating 1 - 5 on the quality of each response given by the chatbot at the end of their own responses .
FFigure 1 : 1 Figure 1 : As the self-feeding chatbot engages in dialogue , it estimates user satisfaction to know when to ask for feedback .
From the satisfied responses and feedback responses , new training examples are extracted for the DIALOGUE and FEEDBACK tasks , respectively , both of which improve the model 's dialogue abilities further .
Figure 2 : 2 Figure 2 : ( 1 ) The chatbot is first trained with any available supervised data ( boxed in red ) on the Human-Human ( HH ) DIALOGUE ( x , y ) HH and SATISFACTION ( x , s ) tasks .
( 2 ) During deployment , whenever the predicted satisfaction score of the current conversation x is above the threshold ( ? > t ) , a new Human-Bot ( HB ) DIALOGUE example ( x , y ) HB is extracted and the bot continues the conversation with its own response ?.
Otherwise , the chatbot requests feedback with question q and extracts a new FEEDBACK example ( x , f ) .
( 3 )
The chatbot is periodically retrained with the available examples from all four datasets , improving its DIALOGUE performance without collecting any new supervised examples .
Figure 3 , FEEDBACK ( FB ) examples collected from a more recently retrained model ( set B instead of set A ) are more valuable in terms of improving performance ; see Appendix A for details on how sets A and B were collected .
We did not observe the same trend for HB DIALOGUE examples .
We include the performance of models trained on only HH DIALOGUE examples in italics as reference points .
Figure 3 : 3 Figure 3 : The first 20k examples for all models are supervised DIALOGUE examples .
This model is deployed to collect 20 k FEEDBACK examples ( set A ) .
If the model is retrained before collecting the next 20k examples ( set B ) , the fresher feedback results in better performance ( p = 0.027 ) .
Shaded regions depict 95 % confidence intervals .
Table 1 : 1 , which consists of The number of examples used in our experiments by task and split .
Note that the HH DIALOGUE examples come from the PERSONACHAT dataset , HB DIALOGUE and FEEDBACK examples were collected during deployment , and an additional 40 k SATISFAC -TION training examples were collected for the analysis in Section 5.1 .
Task Train Valid Test Total DIALOGUE - HH ( HUMAN - HUMAN ) 131438 7801 6634 145873 - HB ( HUMAN - BOT ) 60000 0 0 60000 FEEDBACK 60000 1000 1000 62000 SATISFACTION 1000 500 1000 2500
? my favorite food is pizza ? no , i have never been to kansas ?
i like when its bright and sunny outside Suggestion 24.5 ? you could say hey , i'm 30 .
how old are you ??
yes , i play battlefield would have a been a great answer .?
you could have said " yes , I 'm happy it 's friday . "
Instructions 14.5 ? tell me what your favorite breakfast food is ?
answer the question about having children !
? tell me why your mom is baking bread Options 8.0 ? you could have said yes it really helps the environment or no its too costly ?
you could have said yes or no , or talked more about your mustang dream .
? you should have said new york , texas or maryland .
something like one of those .
Category % Feedback Examples Verbatim 53.0
Table 2 : 2 Examples of the types of feedback given to the dialogue agent , pulled from a random sample of 200 feedback responses .
Verbatim responses could be used directly in conversation , Suggestion responses contain a potential verbatim response in them somewhere , Instructions describe a response or tell the bot what to do , and Options make multiple suggestions .
Table 3 : 3 Accuracy ( hits @1/20 ) on the DIALOGUE task 's hidden test set by number of Human-Human ( HH ) DIA - LOGUE , Human-Bot ( HB ) DIALOGUE , and FEEDBACK examples , averaged over 20 runs , with standard deviations in parentheses .
For each column , the model using all three data types ( last row ) is significantly better than all the others , and the best model using only one type of self-feeding ( FEEDBACK examples or HB DIALOGUE examples ) is better than the supervised baseline in the first row ( p < 0.05 ) .
the learning curve .
It is interesting to note that the two types of deployment examples appear to provide complementary signal , with models performing best when they use both example types , despite them coming from the same conversations .
This improvement makes sense intuitively , since new FEEDBACK examples are Method Pr. Re. F1 Uncertainty Top 0.39 0.99 0.56 ( Pr. ? 0.5 ) 0.50 0.04 0.07 Uncertainty Gap 0.38 1.00 0.55 ( Pr. ? 0.5 ) 0.50 0.04 0.07 Satisfaction Regex 0.91 0.27 0.42 Satisfaction Classifier ( 1k ) 0.84 0.84 0.84 Satisfaction Classifier ( 2 k ) 0.89 0.84 0.87 Satisfaction Classifier ( 5 k ) 0.94 0.82 0.88 Satisfaction Classifier ( 20 k ) 0.96 0.84 0.89 Satisfaction Classifier ( 40 k ) 0.96 0.84 0.90
Table 4 : 4
The maximum F1 score ( with corresponding precision and recall ) obtained on the SATISFACTION task .
For the Uncertainty methods , we also report the maximum F1 score with the constraint that precision must be ? 0.5 .
The Satisfaction Classifier is reported with varying numbers of SATISFACTION training examples .
collected based on failure modes of the current model , making them potentially more efficient in a manner similar to new training examples selected via active learning .
It also suggests that the gains we observe in Table 3 might be further improved by ( a ) collecting FEEDBACK examples specific to each model ( rather than using the same 60 k FEED - BACK examples for all models ) , and ( b ) more fre- quently retraining the MTL model ( e.g. , every 5 k examples instead of every 20 k ) or updating it in an online manner .
We leave further exploration of this observation for future work .
The same experiment repeated for HB DIA - LOGUE examples found that fresher HB examples were no more valuable than stale ones , matching our intuition that HB DIALOGUE examples are less targeted at current model failure modes than FEEDBACK ones .
Table 6 : 6
The accuracy of various models and baselines on the original PERSONACHAT test set .
( Zhang et al. , 2018 b ) Seq2Seq 9.2 IR Baseline 21.4 Starspace 31.8 Profile Memory 31.8 KV Profile Memory 34.9 Ours Transformer 49.6 Self - Feeding 51.7 D Using Chatbot Responses as Targets HH BF BU Hits@1/20 20 k - - 30.3 20 k 32 k - 22.7 20 k - 33 k 19.3 131 k - - 44.7 131 k 32 k - 40.4 131 k - 33 k 39.0
Table 7 : 7 Both with few HH DIALOGUE examples ( 20 k ) and many ( 131 k ) , adding examples with bot utterances as the target decreased quality .
We explored using all bot responses ( Bot Unfiltered , or BU ) and only those responses with estimated satisfaction scores greater than the 0.5 ( Bot Filtered , or BF ) .
Table 8 : 8 As discussed in Section 5.1 and illustrated in 20 k - - - - 20k 30.3 20 k 40 k - - - 60 k 35.4 20 k 20 k 20 k - - 60 k 35.3 40 k - - - - 40 k 36.2 20 k - - 40 k - 60 k 36.7 20 k - - 20 k 20 k 60 k 37.1 60 k - - - - 60 k 39.1
Table 9 : 9
The hyperparameters used to obtain the results in Table3 .
Another option would be to have two thresholds - one for each example type - to decouple collection their rates .
Future work should examine how to ask different kinds of questions , depending on the context .
For comparisons with other models , see Appendix C .
The best existing score reported elsewhere on the PER -SONACHAT test set without using profiles is 34.9 .
Our baseline chatbot collected approximately one FEED - BACK example for every two HB DIALOGUE examples , but this ratio will vary by application based on the task difficulty , satisfaction threshold ( s ) , and current model quality .
Additional detail can be found in Appendix E.
