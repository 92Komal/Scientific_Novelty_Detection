title
Joint Turn and Dialogue level User Satisfaction Estimation on Multi-Domain Conversations
abstract
Dialogue level quality estimation is vital for optimizing data driven dialogue management .
Current automated methods to estimate turn and dialogue level user satisfaction employ hand -crafted features and rely on complex annotation schemes , which reduce the generalizability of the trained models .
We propose a novel user satisfaction estimation approach which minimizes an adaptive multi-task loss function in order to jointly predict turn- level Response Quality labels provided by experts and explicit dialogue - level ratings provided by end users .
The proposed BiLSTM based deep neural net model automatically weighs each turn 's contribution towards the estimated dialogue - level rating , implicitly encodes temporal dependencies , and removes the need to hand -craft features .
On dialogues sampled from 28 Alexa domains , two dialogue systems and three user groups , the joint dialogue -level satisfaction estimation model achieved up to an absolute 27 % ( 0.43 ? 0.70 ) and 7 % ( 0.63 ? 0.70 ) improvement in linear correlation performance over baseline deep neural net and benchmark Gradient boosting regression models , respectively .
Introduction Automatic turn and dialogue level quality evaluation of end user interactions with Spoken Dialogue Systems ( SDS ) is vital for identifying problematic conversations and for optimizing dialogue policy using a data driven approach , such as reinforcement learning .
One of the main requirements to designing data-driven policies is to automatically and accurately measure the success of an interaction .
Automated dialogue quality estimation approaches , such as Interaction Quality ( IQ ) ( Schmitt et al. , 2012 ) and recently Response Quality ( RQ ) * Currently at LinkedIn , but did this work at Amazon .
( Bodigutla et al. , 2019a ) were proposed to capture satisfaction at turn level from an end user perspective .
Automated models to estimate IQ ( Ultes et al. , 2014 ; Schmitt et al. , 2011 ; Asri et al. , 2014 ) used a variety of features derived from the dialogueturn , dialogue history , and output from three Spoken Language Understanding ( SLU ) components , namely : Automatic Speech Recognition ( ASR ) , Natural Language Understanding ( NLU ) , and the dialogue manager .
RQ prediction models ( Bodigutla et al. , 2019a ) further extended the feature sets with features derived from the dialogue -context , aggregate popularity and diversity of topics discussed within a dialogue-session .
Using automatically computed diverse feature sets and expert ratings to annotate turns overcame limitations suffered by earlier approaches to measure dialogue quality at turn-level , such as using sparse sentiment signal ( Shi and Yu , 2018 ) , intrusive solicitation of user feedback after each turn , and using manual feature extraction process to estimate turn - level ratings ( Engelbrecht et al. , 2009 ; Higashinaka et al. , 2010 ) .
For predicting user satisfaction at dialogue-level , IQ estimation approach was shown to generalize to dialogues from different domains ( Schmitt and Ultes , 2015 ) .
Using annotated user satisfaction ratings to estimate dialogue - level quality , overcame the limitation with using task success ( Schatzmann et al. , 2007 ) as dialogue evaluation criteria .
Task success metric does not capture frustration caused in intermediate turns and assumes the end user goal is known in advance .
However , IQ annotation approach to rate each turn incrementally , lowered Inter Annotator Agreement ( IAA ) for multi-domain dialogues ( Bodigutla et al. , 2019 b ) .
Multi-domain dialogues are conversations that span multiple domains ( Table 1 ) in a single dialogue-session .
On the contrary , RQ ratings were provided for each turn independently and were shown to be highly con-sistent , generalizable to multiple -domain conversations and were highly correlated with turn - level explicit user satisfaction ratings ( Bodigutla et al. , 2019 b ) .
Furthermore , using predicted turn- level RQ ratings as features , end-user explicit dialoguelevel ratings for complex multi-domain conversations were accurately predicted across dialogues from both new and seasoned user groups ( Bodigutla et al. , 2019 b ) .
Earlier widely used approach , such as PARADISE ( Walker et al. , 2000 ) , where the model is trained using noisy end dialogue ratings provided by users , did not generalize to diverse user population ( Deriu et al. , 2019 ) .
Despite generalizing to different user groups and domains , both turn and dialogue level quality estimation models trained using annotated RQ ratings ( Bodigutla et al. , 2019 a , b) used automated , yet hand -crafted features .
Modern day SDS support interoperability between different dialogue systems , such as " pipeline based modular " and " end - to - end neural " dialogue systems .
Hand-crafted features designed based on one system are not guaranteed to generalize to dialogues on a new system .
RQ based dialogue -level satisfaction estimation models ( Bodigutla et al. , 2019 b ) did not factor in noise in explicit user ratings and used average estimated turn - level RQ ratings as a feature to train the model .
Each turn 's success or failure was assumed to have an equal contribution to the overall dialogue rating .
However , a user might be dissatisfied even if most of the turns in the dialogue were successful ( example in Appendix Table 8 ) .
The LSTM ( Hochreiter and Schmidhuber , 1997 ) based IQ estimation approaches ( Pragst et al. , 2017 ; Rach et al. , 2017 ) were shown to encode temporal dependencies between turns implicitly .
Most recently , BiLSTMs ( Bi-directional LSTMs ) with self-attention mechanism ( Ultes , 2019 ) , which used only turn-level features achieved best performing IQ estimation performance .
In order to address the aforementioned limitations with using hand -crafted features , we propose a LSTM ( Hochreiter and Schmidhuber , 1997 ) based turn- level RQ estimation model , which implicitly encodes temporal dependencies and removes hand - crafting of turn and temporal features .
Along with turn- level features that are not dialoguesystem or user group specific , we use features derived from pre-trained Universal Sentence Encoder ( USE ) embeddings ( Cer et al. , 2018 ) of an utterance and system response texts to train the model .
Pre-trained sentence representations provided by USE Transformer model achieved excellent results on semantic relatedness and textual similarity tasks ( Perone et al. , 2018 ) .
Using an adaptive multi-task loss weighting technique ( Kendall et al. , 2017 ) and attention ( Vaswani et al. , 2017 ) over predicted turn- level ratings , we further extend the turn- level model to design a novel BiLSTM ( Graves et al. , 2013 ) based joint turn and dialogue - level quality estimation model .
To test the generalization performance of the proposed approaches , we estimate turn and dialoguelevel ratings on multi-turn 1 multi-domain conversations sampled from three user groups , spanning 28 domains ( e.g. , Music , Weather , Movie & Restaurant Booking ) across two different dialogue systems .
To the best of our knowledge , this is the first attempt to leverage noise adaptive multi-task deep learning approach to jointly estimate annotated turn - level RQ and user provided dialogue level ratings for multi-domain conversations from multiple user groups and dialogue systems .
The outline of the paper is as follows : Section 2 discusses the choice of RQ annotation .
Section 3 & 4 presents the novel approaches to estimate turn and dialogue level quality ratings .
Section 5 summarizes the turn and dialogue level data and presents our experimental setup .
Section 6 provides an empirical study of the models ' performance .
Section 7 concludes .
Response Quality for Turn and Dialogue level Quality Estimation Interaction Quality ( IQ ) ( Schmitt et al. , 2012 ) and and Task Success ( TS ) ( Schatzmann et al. , 2007 ) measures require an annotator to accurately determine the task that the user is aiming to accomplish through a dialogue , which is non-trivial for multidomain conversations ( Bodigutla et al. , 2019 b ) .
Both IQ and RQ ( Bodigutla et al. , 2019a ) 3 Turn-level Dialogue Quality Estimation
In this section we discuss previous turn-level satisfaction estimation models trained using RQ ratings , their limitations and our approach to overcome them . ) .
In addition to deriving domain-independent features from three SLU components , namely Automatic Speech Recognition ( ASR ) , Natural Language Understanding ( NLU ) , and the dialogue manager , five new feature sets were introduced by the authors to improve the performance of the turn-level satisfaction estimation model .
Features used in the model were automatically computed , yet they were carefully hand-engineered ( See Appendix Table 11 ) .
Features were handcrafted to identify and rank factors contributing to the predicted satisfaction rating , but these features do not generalize easily to different dialogue systems .
Introduced originally by authors of RQ , " un-actionable request " feature was computed by identifying the presence of particular key words ( e.g. , " sorry " , " i do n't know " ) in the system 's response .
This rule- based feature does not generalize to a system that uses different set of phrases to indicate its inability to satisfy user 's request .
Even temporal dialogue level features computed over turns ( t 1 :t n ) were also hand-crafted and computed by taking simple aggregate statistics ( e.g. , mean ) over turn level features .
LSTM - based Response Quality Estimation Models
In order to overcome the limitation of hand -crafting temporal features , we propose using a Long Short Term Memory ( LSTM ) ( Hochreiter and Schmidhuber , 1997 ) based model to estimate turn-level satisfaction ratings sequentially on a continuous [ 1?5 ] scale .
Rach et al. ( 2017 ) showed that by using only turn- level features , pre-computed temporal features were no longer required for estimating IQ using a LSTM network .
To keep the turn- level dialogue quality estimation system causal ( Li Tan , 2013 ) , where the output at the current time step only depends on current and previous steps , we do not introduce bi-directionality ( Graves et al. , 2013 ) into the network architecture ( See Figure 2 ) .
Unlike dialogue - level rating , which is computed at the end of a dialogue-session , only past dialogue -context is available to compute a turn 's quality rating .
Causality enables using turn-level model to optimize dialogue policies online .
Models encoding sentences into embedding vectors have been successfully used in transfer learning and performing several downstream Natural Language Processing tasks ( e.g. , Classification and semantic textual similarity detection ) .
Pre-trained sentence representations provided by Universal Sentence Encoder ( USE ) ( Cer et al. , 2018 ) model achieved excellent results on semantic relatedness and textual similarity tasks ( Perone et al. , 2018 ) .
To address the limitation with using features derived from hand -crafted rules , we use feature sets which are derived from USE pre-trained ( 512 dimensional ) embeddings from its transformer variant .
We introduce a set of five features derived from USE embeddings of user request and system response texts ( See Table 2 ) .
These features are then concatenated with turn-level features obtained from the SLU ( e.g. , ASR confidence score ) , dialogue manager ( e.g. , system response ) output and predicted intent and domain popularity statistics .
Concatenated features are passed as input to each time-step of the uni-directional turn-level satisfaction estimation deep LSTM network ( Figure 2 ) , that minimizes mean square error loss between actual and predicted turn - level RQ ratings .
Dialogue-level Quality Estimation
In this section we discuss the novel joint turn and dialogue quality estimation approach .
Joint Estimation of Turn and Dialogue Level Ratings Turn-level satisfaction estimation helps identify a particular turn 's success from an end user 's perspective .
In addition to predicting whether individual turn was successful , we need a dialogue level user satisfaction metric for learning dialogue policies that maximize end user satisfaction on the overall dialogue .
Dialogue -level metric also helps in identifying problematic dialogues which caused dissatisfaction to the end user .
We propose a novel approach ( Figure 3 ) to jointly predict turn and dialogue level satisfaction ratings for a given dialogue .
Unlike turn-level satisfaction estimation , we are not constrained to use only historical context of a dialogue to predict the dialogue - level ratings as entire context of the dialogue is available while predicting a dialogue level rating .
Hence instead of LSTMs we use deep BiLSTM ( Graves et al. , 2013 ) network for the dialogue -level satisfaction estimation task .
Ultes ( 2019 ) showed that BiLSTMs with self-attention ( Zheng et al. , 2018 ) model gave the best performance on the IQ prediction task and the model implicitly encoded temporal dependencies .
Feature inputs to the joint model are same as the ones we use for turn - level quality estimation in Section 3.1 .
Individual turn 's predicted RQ rating does not provide enough information to estimate whether an entire dialogue is satisfactory .
Bodigutla et al. ( 2019 b ) used average turn-level predicted RQ ratings as feature to estimate dialogue - level quality .
We hypothesize that users do not equally weigh each each turn 's success ( or failure ) while determining end dialogue rating ( Example conversation in Appendix Table 8 ) .
We apply attention ( Vaswani et al. , 2017 ) over turn- level ratings and concatenate the aggregate weighted turn - level rating with the entire dialogue 's representation ( hidden state h t N in Figure 3 ) before passing it through the sigmoid activation layer for dialogue rating prediction .
In the next section we describe the multi-task loss function we minimized for jointly estimating turn and dialogue - level quality ratings .
tive enough to provide correct feedback ( Su et al. , 2015 ) .
To address the difference in noisiness of labels provided for each task , we followed the approach by Kendall et al . ( 2017 ) to use homoscedastic ( task - dependent ) uncertainty to weigh losses from two tasks , where multi-task loss function is derived by maximizing Gaussian likelihood with homoscedastic uncertainty ( Equation 1 ) .
Sufficient statistics f W ( X ) is the output of a neural network with weight W on input X. Yt ( turn-ratings ) and Y d ( dialog-ratings ) are model outputs .
Multi-task p( Y t , Y d |f W ( X ) ) = p( Y t |f W ( X ) ) ? p( Y d |f W ( X ) ) = N ( Y t ; f W ( X ) , ? 2 t ) ? N ( Y d ; f W ( X ) , ? 2 d ) ( 1 ) L( W ) = ? log p( Y t , Y d |f W ( X ) ) ? 1 2 ?
2 t || Y t ? f W ( X ) || 2 + || Y d ? f W ( X ) || 2 + log ?t + log ? d = 1 2 ?
2 t Lt ( W ) + 1 2 ?
2 d L d ( W ) + log ?t + log ? d ( 2 ) Equation 2 shows the multi-task loss function L we minimize .
L t and L d are the mean square error losses computed on turn- level RQ ratings and dialogue - level user ratings respectively .
Minimizing the objective functions with respect to noise parameters ?
t and ?
d is interpreted as learning the weights for L t and L d adaptively from the data .
Higher the noise , lower is the weight of the corresponding loss .
This method to weigh the losses using learnt weights helps in bringing the losses from the two tasks on the same scale as well .
Data and Experimental Setup
This section describes our turn and dialogue -level datasets and explains our experimentation setup .
Dialogue Quality Data
In order to test the generalizability of the turn and dialogue level user satisfaction models across different domains , user groups and dialogue systems , we sampled 3,129 dialogue sessions ( 20,167 turns ) from 28 domains ( Table 3 ) .
These multi-domain dialogues ( Example goals user try to achieve in Appendix Table 10 ) are representative of end user interactions with Alexa and were randomly sampled from two dialogue systems .
Dialogue-system
A uses a pipelined modular dialogue agent comprising of ASR , NLU , State Tracker , Dialogue Policy and Natural Language Generation components ( Williams et al. , 2016 ) .
Dialogue-system
B is an end-to - end neural model ( Ritter et al. , 2011 ; Shah et al. , 2018 ) that shares only the ASR component with system A ( Fig. 4 ) .
Each turn was rated by expert RQ annotators 2 and Dialogue level ratings were provided by end users .
Users provided their satisfaction rating with the dialogue on a discrete [ 1 ? 5 ] scale at the end of each session , irrespective of the outcome .
Similar to Bodigutla et al. ( 2019 b ) the rating scale we asked the users to follow was 1 =Very dissatisfied , 2 =Dissatisfied , 3 =Moderately Satisfied ( or Slightly dissatisfied ) , 4 =Satisfied and 5 =Extremely Satisfied .
Since earlier attempts to estimate explicit dialogue -level satisfaction ratings did not generalize to different user population ( see section 1 ) , we collected dialogue ratings from users belonging to " novice " ( 15 % ) , " some experience " ( 33 % ) and " experienced " ( 52 % ) groups .
A novice user has minimal experience conversing with the SDS and he / she has never used the functionality provided by the 28 domains prior to the study .
A user with some experience has interacted with some ( but not all ) domains , whereas an experienced user is a seasoned user of Alexa and its domains .
Experimental Setup
This section describes the experimental setup we used for training and evaluating turn and dialogue level satisfaction estimation models .
Turn-level Dialogue Quality Estimation Similar to Bodigutla et al. ( 2019a ) , we considered regression models for experimentation to predict turn -level satisfaction rating on a continuous [ 1?5 ] scale .
We experimented with two variants of the turn-level satisfaction estimation model described in Section 3.1 .
In the first variant ( LST M embedding ) we passed concatenated pretrained USE sentence embeddings of the user request and system response as input to each time step of the LSTM based model .
In the second variant ( LST M embeddings f eatures ) we concatenate USE embeddings with rest of the 15 turn- level features mentioned in Table 2 .
We benchmarked the performance of the two LSTM models against the best performing ( Bodigutla et al. , 2019a ) turnlevel Gradient Boosting Regression model trained with 48 hand - crafted features ( Appendix Table 11 ) .
Dialogue-level Quality Estimation
We experimented with eight models to estimate dialogue level user satisfaction ratings .
Three out of the eight models were used as baseline models , which are : 1 ) Gradient Boosting Regression ( G.Boost ) model trained using features derived from the entire dialogue context ( t 1:N ) , including hand -crafted turn-level and temporal features ( See Appendix Table 11 ) ; 2 ) Two -layer BiL-STM model ( BiLST M f eatures ) trained with all turn-level features ( Table 2 ) , except for the embeddings themselves ; 3 ) BiLST
M f eatures model with self-attention mechanism ( BiLST M attn f eatures ) , which is also a variant of best performing IQ estimation model ( Ultes , 2019 ) .
For benchmarking we used best performing ( Bodigutla et al. , 2019 b ) G.Boost RQ dialogue - level quality estimation model , which used average predicted RQ rating as an additional feature to train the G.Boost model .
Remaining four models we experimented with comprised of two variants of our proposed BiLSTM based joint dialogue quality estimation model , that used attention over the predicted RQ ratings to predict dialogue level rating ( See Section 4.1 ) .
First variant used only USE embeddings as features ( Joint attn embeddings ) and the second one ( Joint attn embeddings f eatures ) used all the turn-level features mentioned in Table 2 .
To test whether including USE embeddings on user request and system response texts alone improved the performance of the baseline BiLST M f eatures and BiLST M attn f eatures models , we experimented with their respective counterparts BiLST M embeddings f eatures and BiLST M attn embeddings f eatures models that included USE embeddings as features .
The joint models minimized adaptive weighted loss ( Eq. 2 ) .
All the deep neural models we experimented with used Adam ( Kingma and Ba , 2014 ) optimizer with learning rate 0.0001 , mini- batch size of 64 and hidden vector size 512 .
We used early stopping criteria and ( 0.5 ) dropout ( Srivastava et al. , 2014 ) regularization techniques to avoid overfitting .
Hyper-parameter ranges we experimented with are in Appendix Table 12 .
For both and turn level quality estimation , dialogues were randomly split into training ( 80 % ) , validation ( 10 % ) and test ( 10 % ) sets , so that turns from the same dialogue do not appear in both test and training sets .
We trained and evaluated the performance of the turn and dialogue - level quality estimation models on dialogues from dialogue -system A and from both systems A & B combined 3 .
In the first case we used all turn-level features mentioned in Table 2 .
In the second case we excluded features derived from NLU as dialogue -system B did not use NLU output .
Evaluation Criteria
We used Pearson 's linear correlation coefficient ( r ) for evaluating each model 's 1 - 5 prediction performance .
For the use case to identify problematic turns from an end user 's perspective , it is sufficient to identify satisfactory ( rating ?3 ) and dissatisfactory ( rating < 3 ) interactions ( Bodigutla et al. , 2019 b ) .
We used F-score for the dissatisfactory class as the binary classification metric , as most turns and dialogues belong to the satisfactory class .
Dialogue -level ratings have a smoother distribution ( Pearson 's moment coefficient of skewness ?0.27 ) over turn- level RQ ratings ( skewness ?0.64 ) .
Results and Analysis
This section presents the turn and dialogue -level user satisfaction estimation results .
Turn-level satisfaction Estimation
As shown in Table 4 , our proposed LSTM based turn-level quality estimation model outperformed the benchmark Gradient Boosting regression model and removed the need to hand -craft features .
Even when NLU features were not used , on dialogues from both dialogue systems , the best- performing ( LST M embedding f eatures ) model achieved ?3 % relative improvement in correlation ( 0.74 ? 0.76 ) and statistically significant ( at 95 % boostrapconfidence interval ) relative improvement 8.3 % ( 0.72 ? 0.78 ) in F-score on dissatisfactory class performance , over the benchmark model .
Analysis of turn- level model 's performance on new domain
To further test the generalizability of the LST M embedding f eatures model to new domains , we wanted to verify that the model was not overfitting domain specific vocabulary .
To achieve this , we trained the turn- level model with varying percentage of dialogues from a new " movie reservation & recommendation " domain hosted on dialogue System A .
Training set consisted of dialogues from Sytems A&B and specified percentage of dialogues from the new domain 4 . Consistent with the results in ( Bodigutla et al. , 2019 b ) , the prediction performance dropped when no dialogues from the new domain were in the training set ( results in Appendix Table 14 ) .
However , when the model was trained with ( randomly sampled ) mere 10 % ( 9 % train , 1 % validation ) of dialogues ( ? 6 % slotvalue coverage 5 ) , the prediction performance on F ? dissatisf actory metric ( 0.75 ? 0.01 ) was at par ( difference not statistically significant ) with the overall performance achieved by the model when it was trained with 90 % ( 80 % train , 10 % valid ) dialogues ( Table 4 ) .
Performance parity in-terms of Correlation ( 0.74 ? 0.03 ) was achieved when LST M embedding f eatures model was trained with 60 % ( 54 % train 6 % validation ) of dialogues ( ? 50 % slot-value coverage ) .
These two observations imply that binary prediction performance improvement requires training with fewer dialogues in comparison to the number of dialogues required to accurately identify the degree of user ( dis ) satisfaction .
In order to further understand the relationship between slot types and annotated RQ labels we calculated the Pointwise Mutual Information ( PMI 6 ) score for the new domain , between its 8 slot-types and 5 RQ labels ( total 40 values ) .
Most of the dissatisfactory turns were associated with the system not interpreting the theater names ( slot-types ' theater ' ) and instructions containing numbers ( e.g. , " pick the fourth one " ) correctly .
Validating our hypothesis that users do not perceive all turns ' failures equally , based on the PMI scores , users seem more dissatisfied with system 's failure to identify " theater " ( RQ rating - 1 ) over failure in interpreting numeric instructions ( RQ rating - 2 ) 7 .
We calculated cosine similarity between the 40 dimensional PMI scores vector of ( Slot type , RQ labels ) in each selected training set , with PMI scores vector computed on entire set of dialogues in the new domain .
As shown in Appendix Table 14 , the turn- level model 's performance on new domain improves with the similarity score .
This observation suggests that the model is not overfitting to domain specific vocabulary ( e.g. , movie name ) , instead it learns the extent of user ( dis ) satisfaction to failures / success of different ( slot ) types of requests he / she makes .
Dialogue-level Satisfaction Estimation
As shown in Table 5 , on test sets from System A and System A & B combined , Joint attn embeddings f eatures model outperformed the seven other models we experimented with .
On test dialogues from System A & B , in comparison to the baseline BiLST M attn f eatures model , the Joint-model achieved statistically significant ( at 95 % confidence interval ) absolute 27 % ( 0.43 ? 0.70 ) improvement in correlation and 17 % ( 0.51 6 PMI of pair of outcomes ( x , y ) belonging to discrete random variables X , Y is log p( x , y ) p( x ) p( y ) .
7 Since RQ ratings are highly correlated with turn- level user satisfaction ratings ( Bodigutla et al. , 2019a ) . ? 0.68 ) in F-score on dissatisfactory class .
In comparison to benchmark G.Boost RQ model , the absolute improvement on the same metrics was 7 % and 5 % respectively .
Learnt noise ratio of 1.2 between the two learnt parameters ?
2 d and ?
2 t ( Eq. 2 ) , shows higher variance in dialogue - level ratings over turn - level labels .
Including USE embeddings as features improved the performance of the dialogue -level satisfaction estimation models .
Specifically on data from both systems , both BiLST M embeddings f eatures and BiLST M attn embeddings f eatures models achieved around absolute 15 % - 18 % significant improvement in both correlation and F-score on dissatisfactory class performance over their respective counterparts BiLST M f eatures and BiLST M attn f eatures .
Analysis of learnt Attention Weights
For the Joint attn embeddings f eatures model , Table 6 shows the attention weights learnt on predicted turn level ( RQ ) and true RQ ratings for each turn of a sample dialogue .
The joint model puts more weight on the dissatisfactory turns over the satisfactory ones and the dialogue was correctly identified as dissatisfactory .
Table 7 shows an example dialogue where a generous ( Kulikov et al. , 2018 ) user was satisfied ( dialogue-rating 4.0 ) even when the system did not offer any alternate time slots or restaurant suggestions when his / her initial request to book a table could not be fulfilled .
However , the model predicted the dialogue as dissatisfactory .
Conclusions
In this paper , we proposed a novel approach to use annotated consistent turn- level Response Quality ( RQ ) ratings for dialogue level user satisfac - 8 Results are not broken down further by domain , since a multi-domain conversation session comprises of turns which belong to ? one domain and context is shared between them .
9
This dialogue is not a real user conversation with live system .
Figure 3 : 3 Figure 3 : BiLSTM based joint turn and dialogue -level satisfaction estimation model .
Figure 4 : 4 Figure 4 : Dialogue systems
A & B with their own dialogue managers to process user request and generate Text To Speech ( TTS ) response once the shared ASR component does the speech to text translation .
Table 1 : 1 Example multi-domain dialogue a user has with the conversation agent to plan his / her evening .
The conversation spans 4 domains in a single dialogue session .
User utterance { Domain } System Response Movies playing near me { MovieBooking }
Avengers is playing at AMC
What are the showtimes ?
{ MovieBooking } 4 p.m and 7 p.m
Two tickets for 7 p.m show { MovieBooking }
Booking 2 tickets for 7 p.m ...
Is it raining today ?
{ Weather }
Yes there is a 60 % chance of rain Book cab to the theater at 6 p.m { CabBooking }
Ride for 2 costs $ 20 , should I book ?
Yes { CabBooking }
Sure , booked a cab for 2 to AMC Cancel 7 p.m appointment { Calendar } Canceled your 7 p.m park visit
Turn level features for step n Dialogue level features for step n Dialogue Session Dialogue Turn t 1 t n t n+1 t N 12/10/2019 use_fig1 ( t u 1 , t s 1 ) . . . ( t u n , t s n ) ( t u n+1 , t s n +1 ) . . . ( t u N , t s N ) Figure 1 : Dialogue and turn definitions for estimating user satisfaction rating on turn tn ( Bodigutla et al. , 2019a ) .
The solid blue and dotted red lines indicate the context used for generating turn and dialogue level features respectively .
Similar to Bodigutla et al. ( 2019a ) , we define a dialogue turn at time n as t n = ( t u n , t s n ) , where t u n and t s n represent the user request and system response on turn n respectively ( Figure 1 ) .
A dia- logue session of N turns is defined as ( t 1 :t N ) .
In experiments conducted by Bodigutla et al . ( 2019 a ) , Gradient Boosting Regression ( Friedman , 2001 ) model gave the best turn- level RQ prediction per- formance .
Features used to train the model were derived from current turn ( t n ) , dialogue history ( t 1:n?1 ) and next turn 's user request ( t u n+1 1/1
Figure2 : Uni-directional LSTM model to predict RQ ratings at each time -step to estimate dialogue quality at turn-level .
eusr , esys and turn ? f eatures are pre-trained Universal Sentence Encoder embeddings for user request , system response and rest of the features in Table2respectively .
R( t 1 ) R( t 2 ) R( t 3 ) R(t N ) fully connected fully connected fully connected fully connected linear layer linear layer linear layer linear layer sigmoid sigmoid sigmoid sigmoid LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM e usr e usr e usr e usr e sys e sys e sys e sys turn - turn - turn - turn - features features features features t 1 t2 t 3 t N Feature name Methodology used to compute the feature ASR Conf. score Available in the output of the ASR system NLU Conf. score Available in the output of the NLU system Barge - in Output from ASR USE embedding of user request USE embeddings of t u Syntactic resp .
repetition Jaccard sim .
between words in t s n & t s n?1 Length of User utterance Number of words in t u n Length of resp .
Number of words in t s n Duration between utterances Seconds elapsed between t u n & t u n+1 Domain popularity Avg. # of reqs .
per user for predicted NLU domain t u n Intent popularity Avg. # of reqs .
per user for predicted NLU intent t u 3/4 n USE embedding of system response USE embeddings of t s n NLU intent similarity Sim. between NLU predicted intents for t u n and t u n+1 Semantic paraphrase of user req .
Cosine sim .
between USE embeddings of t u n & t u n+1 Syntactic paraphrase of user req .
Jaccard sim .
between words in t u n & t u n+1 Semantic req .
& resp. coherence Cosine sim .
between USE embeddings of t u n & t s n Syntactic req .
& resp . coherence Jaccard sim .
between words in t u n & t s n+1 Semantic resp .
repetition Cosine sim .
between USE embeddings of t s n & t s n?1 n Table 2 : Turn level features for turn -tn and the methodology used to compute them .
In bold are features derived from USE embeddings that we introduced .
Rest of the turn- level features are similar to Bodigutla et al . ( 2019a ) ( Appendix Table 11 ) .
Note ?65 % relative drop in number of features ( 48 ? 17 ) . resp. , conf. , avg. , sim. , # , & req. indicate response , confidence , average , similarity , count and request respectively .
Loss Function for Joint Turn and Dialogue Quality Estimation attention ? att. h tN sigmoid fully connected linear layer R( t 1 ) R( t 2 ) R( t 3 ) R( tn ) R( t 1 ...t N ) fully connected fully connected fully connected fully connected linear layer linear layer linear layer linear layer sigmoid sigmoid sigmoid sigmoid BiLSTM BiLSTM BiLSTM BiLSTM BiLSTM h t( n - 1 ) h tn BiLSTM BiLSTM BiLSTM BiLSTM h t( n - 1 ) h tn e usr e usr e usr e usr e sys e sys e sys e sys turn - turn - turn - turn - features features features features t 1 t 2 t 3 t N RQ ratings provided by experts are reliable and consistent ( Bodigutla et al. , 2019a ) , however user ratings at the end of a dialogue in general are noisy and it is not clear if they would be coopera -
Table 3 : 3 Stats on dialogues from dialogue systems A & B Dialogue System # Domains # Dialogues # Turns Avg. # Turns per Dialogue A 24 2,133 10,774 5 B 4 996 9,393 9.5
Table 5 : 5 Performance of dialogue-level quality estimation models 8 . Each cell shows the mean and 95 % bootstrap confidence interval with the highest mean in bold .
and ?
indicate statistically significant performance in comparison to baseline BiLST M f eatures and BiLST M attn f eatures models respectively .
Compared to Table4 , wider confidence intervals are due to sparsity of dialogue - level ratings ( ? 15 % of turn- level ratings ) .
System A Systems A & B Model\Metric Correlation F ? dissatisf actory Correlation F ? dissatisf actory G.Boost 0.59 ? 0.06 0.61 ? 0.08 0.60 ? 0.05 ? 0.63 ? 0.04 ? G.BoostRQ 0.66 ? 0.05 ? 0.66 ? 0.06 0.63 ? 0.06 ? 0.63 ? 0.05 ? BiLST Mfeatures 0.54 ? 0.07 0.63 ? 0.08 0.48 ? 0.08 0.51 ? 0.06 BiLST Membeddings f eatures 0.62 ? 0.07 ? 0.60 ? 0.07 0.66 ? 0.06 ? 0.66 ? 0.05 ? BiLST M attn f eatures 0.44 ? 0.10 0.51 ? 0.09 0.43 ? 0.08 0.51 ? 0.06 BiLST
M attn embeddings f eatures 0.61 ? 0.08 0.64 ? 0.06 0.59 ? 0.06 ? 0.67 ? 0.04 ? Joint attn embeddings 0.68 ? 0.08 ? 0.65 ? 0.07 0.68 ? 0.06 ? 0.67 ?0.06 ? Joint attn embeddings f eatures 0.69 ? 0.07 ? 0.71 ? 0.07 ? 0.70 ? 0.06 ? 0.68 ? 0.05 ?
Open { Anonym .
movie app . }
System : Hey this is { application } { Wattn : 0.073 , RQ : 5.0 , RQ : 5.0 } User : tell me the plot of hustlers System :
Here is a summary of Hustlers ... { Wattn0.077 , RQ : 4.3 , RQ : 5.0 } User : Give me the whole cast System :
The main role was played by ... { Wattn : 0.087 , RQ : 1.8 , RQ : 2.0 } Utterance Attention weight and turn-level ratings User : User : What is it rated ?
System :
It has a rating of 6.6 on i.m.d. b { Wattn : 0.075 , RQ : 4.7 , RQ : 5.0 } User : Exit System : { Exits the app } { Wattn : 0.079 , RQ : 3.6 , RQ : 5.0 } System :
Heres a station for you User : play hustler soundtrack based on The Hustler by Willie ... { Wattn : 0.090 , RQ : 1.1 , RQ : 1.0 } User : Stop System : { Plays the station } { Wattn : 0.090 , RQ : 1.1 , RQ : 1.0 } User : Open { Anonym .
tkt -booking app } System : Welcome to { application } { Wattn : 0.078 , RQ : 4.0 , RQ : 5.0 } System :
You can go to AMC User :
Where is hustlers playing ?
Which theater would you like ?
{ Wattn : 0.087 , RQ : 1.6 , RQ : 2.0 } System : Today at AMC : User : AMC Joker ... which movie would you like to see ?
{ Wattn : 0.088 , RQ : 1 , 6 , RQ : 2.0 } System : Today at AMC : User : Is the hustlers playing there today Joker ... which movie would you like to see ?
{ Wattn : 0.087 , RQ : 1 , 7 , RQ : 2.0 } User : No System : { No response } { Wattn : 0.089 , RQ : 1 , 3 , RQ : 2.0 }
Table 6 : 6 Conversation 9 where the Joint attn embeddings f eatures correctly predicts the dialogue as dissatisfactory ( rating = 2.5 ) .
Wattn , RQ and true RQ ratings indicate learnt attention weights , predicted and true RQ ratings respectively .
Weights in bold indicate top 50 % of learnt attention weights .
User : Can you can you uh book a table at koreana for two ?
System : Sorry , this is not curr .
supported .
{ Wattn : 0.147 , RQ : 1.8 , RQ : 2.0 } User : Can you book a table at the cheesecake factory for three ?
System :
What time would you like to make the reservation for ?
{ Wattn : 0.137 , RQ : 3.2 , RQ : 4.0 } System :
What date would you User :
Three oclock like the reservation for ?
{ Wattn : 0.135 , RQ : 3.6 , RQ : 4.0 } System : Sorry .
No restaurants could User : For tomorrow be found that match your criteria .
UtteranceAttention weight and turn-level ratingsUser : Open { restaurant booking app . }
System : Hey this is { application } { Wattn : 0.126 , RQ : 5.0 , RQ : 5.0 } { Wattn : 0.152 , RQ : 1.1 , RQ : 2.0 } User : Ok System : { No response } { Wattn : 0.153 , RQ : 1.0 , RQ : 4.0 } User : Stop System : { No response } { Wattn : 0.149 , RQ : 1.5 , RQ : 4.0 }
Table 7 : 7 Conversation 9 where the model incorrectly predicts the dialogue as defective ( rating 2.3 ) .
User 's rating is 4.0. tion estimation on conversations which span three user groups , 28 domains and two dialogue systems .
With the help of pre-trained Universal Sentence Encoder ( USE ) embeddings , we removed the need to hand -craft features .
Leveraging noise adaptive weighting of multi-task loss technique and aggre - gating predicted RQ ratings using attention mecha- nism , we developed the BiLSTM based deep joint turn & dialogue level satisfaction estimation model .
The best- performing joint-model achieved up to 27 % absolute significant improvement in correla- tion ( Pearson ' s -r ) performance and 7 % absolute improvement over the baseline deep neural network and the benchmark G.Boost models , respectively .
Table 8 : 8 An example of a failed dialogue where the overall dialogue rating is 1.0.
However , average turn ratings 3.7 indicates a successful dialogue .
Rating Description
1 Terrible ( system fails to understand and fulfill user 's request ) 2 Bad ( understands the request but fails to satisfy it in any way ) 3 OK ( understands users request and either partially satisfies the request or provides information on how the request can be fulfilled )
Good ( understands and satisfies the user request , 4 but provides more information than what the user requested or takes extra turns before meeting the request ) 5 Excellent ( understands and satisfies user request completely and efficiently
Table 9 : 9 RQ rating guidelinesGoal DomainsGet ratings of movies directed by the director of a movie playing in theaters Movie recommendations and Reservations
Ask for a general type of recipe and then add the ingredients to the shopping list Recipe , Shopping Find out the weather in a location and book a ticket to a movie playing in theaters near by Weather , Location , Movie Recommendations and Reservations Playing sound track of a popular artist Knowledge and Music Book a cab and add a notification for the same Notifications and Cab booking Planning activities for eventing Weather , Restaurants and Cab booking
Table 10 : 10
Example goals users tried to achieve and their corresponding domains .
Turn ( s ) the fea- Index Feature set description ture is computed on 1 2 3 4 5 6 7 8 ASR Confidence NLU Confidence Barge - in Intent popularity computed on predicted NLU intent Domain popularity computed on predicted NLU intent NLU Intent similarity between consecutive turns Syntactic similarity between consecutive turns user utterances Syntactic similarity between user utterance & system response t u n t u n t u n t u n t u n t u n -t u n+1 t u n -t u n+1 t u n -t s n 9 Syntactic similarity between current response & previous turn 's system response t s n?1 -t s n 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34
Affirmation prompt in user request Negation prompt in user request Question prompt in user request Termination prompt in user request Next turn 's ASR Confidence Next turn 's NLU Confidence Next turn 's Barge - in indicator Affirmation prompt in next turn 's user request Negation prompt in next turn 's user request Question prompt in next turn 's user request Termination prompt in next turn 's user request Intent popularity computed on next turn 's predicted NLU intent Domain popularity computed on next turn 's predicted NLU intent Affirmation prompt in system response Negation prompt in system response Question prompt in system response Un-actionable user request # Un-actionable user request # Barge-ins # Question prompt in system response # Negation prompt in system response # Affirmation prompt in system response # Termination prompt in user request # Question prompt in user request # Negation prompt in user request t u n t u n t u n t u n t u n+1 t u n+1 t u n+1 t u n+1 t u n+1 t u n+1 t u n+1 t u n+1 t u n+1 t s n t s n t s n t s n t s 1 -t s n t u 1 -t u n t s n t s n t s n t u n t u n t s n
In single-turn conversations the entire context is expected to be present in the same turn .
In multi-turn case context from previous turns is carried to address user 's current request .
Expert RQ annotators consistently achieve a high agreement ( correlation >= 0.8 ) with other expert annotators and with explicit turn - level user ratings collected through user studies .
For completeness we evaluated dialogue-quality estimation results using train and test dialogues from System B ( Appendix Table 13 ) .
Due to limited data ( 96 test dialogues ) performance comparison between models is inconclusive and needs further experimentation .
Since System
B did not use NLU , it is not possible to train the model with utterances de-lexicalized using NLU output , such as predicted Intents and slots ( Tur and De Mori , 2011 ) .5
Slot-value coverage is the % of unique( slot- type , value ) pairs for the specific domain in the selected set of dialogues .
