title
Adaptive Parameterization for Neural Dialogue Generation
abstract
Neural conversation systems generate responses based on the sequence-to-sequence ( SEQ2SEQ ) paradigm .
Typically , the model is equipped with a single set of learned parameters to generate responses for given input contexts .
When confronting diverse conversations , its adaptability is rather limited and the model is hence prone to generate generic responses .
In this work , we propose an Adaptive Neural Dialogue generation model , ADAND , which manages various conversations with conversation -specific parameterization .
For each conversation , the model generates parameters of the encoder-decoder by referring to the input context .
In particular , we propose two adaptive parameterization mechanisms : a context- aware and a topic-aware parameterization mechanism .
The context - aware parameterization directly generates the parameters by capturing local semantics of the given context .
The topic-aware parameterization enables parameter sharing among conversations with similar topics by first inferring the latent topics of the given context and then generating the parameters with respect to the distributional topics .
Extensive experiments conducted on a large-scale real-world conversational dataset show that our model achieves superior performance in terms of both quantitative metrics and human evaluations .
Introduction Open-domain dialogue models , usually called chitchat systems , draw increasing attention from both academia and industry in recent years .
Building on the successful sequence-to-sequence ( SEQ2SEQ ) paradigm ( Sutskever et al. , 2014 ; Cho et al. , 2014 ; Bahdanau et al. , 2015 ) , contemporary mainstream open-domain dialogue generation models ( Shang et al. , 2015 ; Serban et al. , , 2017
Shen et al. , 2017 ; Clark and Cao , 2017 ; ; Xing * Work done at Data Science Lab , JD.com .
et al. , 2017 ; , trained on a large number of context-response pairs , attempt to generate an appropriate response for the given context based on a single set of the model parameters .
Because of its great potential in understanding and modeling conversations , SEQ2SEQ has been widely applied in different kinds of conversation scenarios including technical supporting , movie discussions , and social entertainment , etc .
However , when confronting conversations with diverse topics or themes , SEQ2SEQ is usually prone to make generic meaningless responses due to its oversimplified parameterization .
To tackle this issue , Xing et al . ( 2017 ) proposed a topic-aware response generation model , which utilizes a single encoderdecoder , augmented with topic information obtained from a pre-trained LDA model .
Though effective , the model heavily relies on the outsourcing topic information to capture the topic variations of different conversations .
Another approach , per-topic / theme encoder-decoder model ( Choudhary et al. , 2017 ) , uses separate encoder-decoder model for each topic or theme .
This method needs the preorganized topic / theme annotations for each conversation , which are prohibitively expensive to obtain .
Furthermore , building multiple separate topic / theme-specific encoder- decoders not only weakens the applicability and efficiency of the system , but also prevents parameter sharing across domains , which leads to overparameterization due to the excessive amount of model parameters .
To gather the benefits of both approaches , in this paper , we propose an adaptive neural dialogue generation model which utilizes a single encoderdecoder for diverse conversations , meanwhile , the encoder-decoder is specifically parameterized according to each conversation .
In particular , we propose two adaptive parameterization mechanisms : 1 ) context - aware parameterization directly generates parameters of the encoder-decoder model by capturing local semantics of the given context ; 2 ) topic-aware parameterization enables parameter sharing among conversations with similar topic distributions by first inferring the latent topics of the input context , and then generating the parameters with respect to the inferred latent topics .
Equipped with both the context-aware and topic-aware parameterization mechanisms , the model is capable of generating responses for diverse conversations with a single encoder-decoder through a more flexible and efficient approach .
Moreover , our model is trained in an end-to - end fashion without any costly external or labeled topic annotations .
We empirically evaluate our approach on a large scale real-world conversational dataset .
Extensive experiments show that our proposed ADAND outperforms existing dialogue generation models in terms of both the automatic evaluation metrics and human judgements .
Neural Dialogue Generation Model Conventionally , neural dialogue generation model follows the encoder-decoder paradigm .
Given the context x = {x 1 , x 2 , ? ? ? , x Tx } and the target response y = {y 1 , y 2 , ? ? ? , y
Ty } , the model learns to maximize the following conditional probability : p( y|x ) = Ty t=1 p(y t |y <t , x ) , ( 1 ) where y <t = y 1 ? ? ? y t?1 .
Typically , the probability p(y t |y <t , x ) is modeled as follows : p(y t |y <t , x ) = p(y t |s t?1 ) , ( 2 ) where s t?1 is the decoding hidden state up to time step t ?
1 , depending on y <t and x .
The s t can be defined as : s t = F(y t , s t?1 , G ( x ) ) , ( 3 ) where the encoder G and the decoder F can be implemented as recurrent neural networks such as LSTMs ( Hochreiter and Schmidhuber , 1997 ) or GRUs ( Cho et al. , 2014 ) , or the transformer ( Vaswani et al. , 2017 ) ( with the attention mechanism ( Bahdanau et al. , 2015 ) or selfattention mechanism ) .
In this work , we employ the LSTM - based encoder - decoder dialogue generation model .
The LSTM unit is formulated as : i t = W i h h t?1 + W i x x t + b i g t = W g h h t?1 + W g x x t + b g f t = W f h h t?1 + W f x x t + b f o t = W o h h t?1 + W o x x t + b o c t = ?( f t ) c t?1 + ?( i t ) tanh ( g t ) h t = ?( o t ) tanh ( c t ) , ( 4 ) where ? is the sigmoid operator and stands for Hadamard product .
h t?1 is the previous hidden state and x t is the input embedding at step t. W = { W i h , W g h , W f h , W o h , W i x , W g x , W f x , W o x } and b = {b i , b g , b f , b o } are the LSTM parameters .
The model parameters are tuned on the training corpus .
When testing , given the input context , it generates a response with the learned parameters .
This architecture shows great success in neural dialogue generation ( Shang et al. , 2015 ; Serban et al. , , 2017
Shen et al. , 2017 ; Clark and Cao , 2017 ; Xing et al. , 2017 ; Choudhary et al. , 2017 ; .
However , with a single set of model parameters and the oversimplified model architecture , the flexibility of the model is rather limited , especially when confronting conversations with diverse topics or themes .
Adaptive Neural Dialogue Generation Model
In this work , we propose an adaptive neural dialogue generation model which utilizes a single encoder-decoder model and a set of dynamical parameters to balance the model 's flexibility and efficiency .
As depicted in Figure 1 , we utilize the model adapter to parameterize the encoder-decoder for each conversation .
It takes the dialogue context as its input , and generates parameters of the encoder- decoder model through two adaptive parameterization mechanisms ; and then the resultant 7 ) ) .
encoder-decoder model generates the response with a specific set of model parameters .
The parameter adapter then generates the weights of LSTM units as : W = M [ 1 : Nr ] ? , ( 5 ) where Nr ] ? results in a weight matrix W , where each row is computed by one W ? R Nr?Nc and M [ 1 : Nr ] ? R Nr?Nc?N ? is a tensor .
N r = 4N h and N c = N h + N x , in which N h is the hidden size of the LSTM and N x is the embedding size .
? ? R N ? is the context representation .
The product M [ 1 : slice j = 1 , 2 , ? ? ? , N r of the tensor : W j = ( M j ? ) T .
Although such parameterization seems to be straightforward , due to the quadratic size of M [ 1 : Nr ] , the parameter size of such parameter adapter is N ? times larger than the basic encoder-decoder model .
Therefore , overfitting and expensive computational cost make it infeasible ( Bertinetto et al. , 2016 ; Ha et al. , 2017 ) . Following Flennerhag et al. ( 2018 ) , we reduce the parameter space by factorizing the weights as : W = A c ( ?
t , U , V ) ? t = ?(? , h t?1 ) , ( 6 ) where ? is implemented as a LSTM unit , h t?1 is the previous encoder or decoder hidden state , and ? t ?
R N ? is the context representation at time step t. A c denotes the context- aware parameterization function , defined as : A c ( ?
t , U , V ) = U ? t V T , ( 7 ) where U ? R Nr?N ? and V ? R Nc?N ? are learn - able weights .
The context - aware parameterization function A c is reminiscent of the Singular Value Decomposition ( SVD ) .
Here , A c composes a projection by adapting the dialogue context to the weight matrices and does not perform matrix decomposition actually .
The number of parameters in this formu- lation is L = N r ? N ? + N c ? N ? and the total parameter number of the model is linear with L , so that the total number of model parameters does not explode .
Topic-aware Parameterization Context - aware parameterization adapts the encoder-decoder parameters regarding each input context .
As a result , the adapted encoder-decoder is sensitive to the input context .
To enable the parameter sharing among similar topics , we further propose a topic-aware parameterization mechanism .
As shown in the Figure 3 , the topic inferrer first distills the topic distribution ? from the context ( Figure 3 ( a ) ) , and then the parameters of the encoder-decoder model are constructed upon ? ( Figure 3 ( b ) ) .
Latent Topic Inference
We introduce a variational topic inferrer to infer the topic distribution ? of the conversation .
Drawing inspirations from neural topic model ( Miao et al. , 2017 ) , as illustrated in Figure 3 ( a ) , the generation process of the variational topic inferrer is formulated as follows : ( i ) A latent variable ? is inferred to convey the underlying semantics of the given context .
( ii )
The topic distribution ? is constructed from the latent variable ? through a softmax function .
( iii )
The dialogue d , composing of a contextresponse pair ( x , y ) , is drawn from the topic distribution over words p( w i |? z i ) , where z i is a topic assignment sampled from a multinomial distribution parameterized by the topic distribution ? , and ?
z i is the topic-word distribution of topic assignment z i .
Given a context x , the latent variable ? is sampled from P ( ?|x ) = N (? prior , ? 2 prior ) , and N ( ? prior , ? 2 prior ) is the multivariate normal distribution with mean ? prior and diagonal covariance matrix ?
2 prior .
In practice , ? is reparameterized as : ? = ? prior + ? ? prior and is a standard Gaussian noise .
The Gaussian parameters ? prior and ?
2 prior are the outputs of multilayer perceptrons ( MLP ) given the bag-of-words ( BoW ) representation of the context as input .
To reduce the encoding noise of stop words , here we choose the BoW instead of LSTM for context representations , following Miao et al . ( 2017 ) .
To implement neural variational inference , we utilize an inference network Q ( ? | d ) = N (? posterior , ? 2 posterior ) to approximate the intractable true posterior p( ? |d ) , where ? posterior and ?
2 posterior are computed in the same way as the prior , taking the bag-of-words representation of dialogue d as input .
The dialogue d consists of the context- response pair ( x , y ) .
The topic distribution ? is constructed from the latent variable ?
through a softmax function : ? = g( ? ) = softmax (? ? W ? ) , ( 8 ) where W ? is a linear transformation and the bias terms are left out for brevity .
Then , the dialogue d is generated based on the topic distribution ?.
Given ? , the marginal likelihood of a dialogue d is formulated as : p( d ) = ? p( ? ) | d| i=1 z i p( w i |? z i ) p ( z i |?)d?. ( 9 ) In addition , the topic assignment z i can be integrated out and the log-likelihood of a word w i in dialogue d can be factorized as : log p( w i |? , ? ) = log z i [ p ( w i |? z i ) p ( z i | ? ) ] = log (? ? ? T ) . ( 10 )
The topic-word distribution ?
k is defined by : ? k = softmax ( ? ? T k ) , ( 11 ) where ?
R C?H is the topical word embedding matrix and ? ?
R K?H is the topic embedding matrix , K is the number of topics , C is the number of topical words , H is the embedding size and ? = {?
1 , ? 2 , ? ? ? , ? K }.
Parameterization with Latent Topics
We parameterize the encoder-decoder with the inferred topic distribution ?.
In context - aware parameterization , the parameters of the encoder-decoder are adapted dynamically at each time step , whereas in topic- aware parameterization , as illustrated in Figure 3 ( b ) , we generate only one set of parameters for each conversation .
Similar to the context- aware parameterization function in Eq. ( 7 ) , given the topic distribution ? , the topic-aware parameterization function A ? constructs the LSTM weight W as follows : W = A ? ( ? , U , V ) A ? ( ? , U , V ) = U ?V T , ( 12 ) where U ? R Nr?K and V ? R Nc?K are learnable parameters .
K is the number of latent topics .
Parameterization with Both Context and Topics Intuitively , context - aware parameterization is more adept at capturing local semantics of the input context while topic-aware parameterization enables parameter sharing between conversations with similar topic distributions .
To benefit the model parameterization with both the local and global information , we further adapt parameters of the encoder-decoder by utilizing both the context representations ?
t and the topic distribution ?.
In particular , the LSTM weight W at time step t is adapted as follows : W =? t A c ( ? t , U c , V c ) +
( 1 ? ? t ) A ? ( ? , U ? , V ? ) ? t =?(? t , ? ) , ( 13 ) where ?
t is the gating function deciding whether the parameterization relies more on the context or the topics .
U c , V c , U ? and V ? are learnable weights .
A c and A ? denote the context-aware and topicaware parameterization function respectively .
? is the sigmoid function .
Learning
To enable the joint optimization of latent topic inference , adaptive model parameterization , and response generation in ADAND , given the definitions in Eq. ( 9 ) , similar to Kingma and Welling ( 2014 ) and Miao et al . ( 2017 ) , we derive a variational lower bound for the generation likelihood : J =E Q ( ?|d ) [ | d| i=1 [ log z i [ p ( wi | ? z i ) p( zi | ? ) ] ] + log p( y| x ) ] ? DKL ( Q( ?|d ) | |P ( ?| x ) ) =E Q ( ?|d ) [ ? DKL ( Q( ?|d ) | |P ( ?| x ) ) , ( 14 ) where y = {y 1 , y 2 , ? ? ? , y Ty } , P ( ?|x ) is the prior estimation of the latent variable ? which approximates the posterior Q ( ?| d ) .
The prior P ( ?|x ) = P ( g( ? ) | x ) = P ( ?| x ) , and the posterior Q ( ?|d ) = Q ( g ( ? ) | d ) = Q ( ?|d ) .
The first term is the dialogue generation objective in the latent topic inferrer , the second term is the response generation objective , and the third term is the KL divergence between two Gaussian distributions .
All the parameters are learned by optimizing Eq. ( 14 ) and updated with back - propagation .
The following previously proposed strategies ( Bowman et al. , 2016 ; ) are adopted in training to alleviate the vanishing latent variable problem : ( 1 ) KL annealing : the weight of the KL divergence term is gradually increasing from 0 to 1 during training ; ( 2 ) Bag- of- words loss : the bag-of-words loss requires the latent variable ? , together with the dialogue context , to reconstruct the response bag-of-words representation y b .
Experiments
Dataset and Competitor Baselines
To ascertain the effectiveness of the proposed model , we construct an open-domain conversation corpus covering a broad range of resources including a movie discussions dataset collected from Reddit ( Dodge et al. , 2015 ) , an Ubuntu technical corpus ( Lowe et al. , 2015 ) , and a chitchat dataset ( Zhang et al. , 2018 ) . 87,468 contextresponse pairs were sampled for training , 4,460 for validation and 4,468 for testing .
The code and corpus are available at http:// github.com/hengyicai /AdaND .
The following state - of - the - art models are adopted as our comparison systems .
SEQ2SEQ
The attention - based sequence - tosequence model ( Bahdanau et al. , 2015 ) , which is a representative baseline .
CVAE
A latent variable conversation model in which it incorporates a latent variable at the sentence -level to inject stochasticity and diversity ( Clark and Cao , 2017 ; . LAED
A recurrent encoder-decoder conversation model using discrete latent actions for interpretable neural dialogue generation .
TA-SEQ2SEQ TA-SEQ2SEQ incorporates the outsourcing topic information into the response generation , where the topics are learned from a separate LDA model to enrich the context ( Xing et al. , 2017 ) . SEQ2SEQ models for response generation ( Choudhary et al. , 2017 ) .
DOM -SEQ2SEQ
A domain- aware conversation model consisting of multiple domain-targeted
Evaluation
Following the evaluation procedure in previous work ( Li et al. , 2016 ; Xing et al. , 2017 ; Chen et al. , 2018 ) , experimental results of all models are reported in terms of the relevance and informativeness .
To evaluate the semantic relevance between the generated response and the groundtruth response , we adopted the BLEU metric ( Papineni et al. , 2002 ) and three embedding - based similarity metrics proposed in Liu et al .
( 2016 ) : Embedding Average ( Average ) , Embedding Extrema ( Extrema ) and Embedding Greedy ( Greedy ) .
To measure informativeness and diversity of the response , we exploited the Distinct -1 , Distinct - 2 and Distinct - 3 metrics .
A higher ratio of distinct ngrams implies more informative and diverse responses .
Implementation and Reproducibility
We implemented our model with ParlAI ( Miller et al. , 2017 ) .
The sequence lengths are truncated at 50 .
We used Adam ( Kingma and Ba , 2014 ) with an initial learning rate of 0.001 to optimize the model .
For all the experiments , we employed a 2 - layer bidirectional LSTM as the encoder and a unidirectional one as the decoder .
The hidden size and the word embedding dimension are both set to 300 .
The latent variable size is set to 64 .
The topic number K in our model is set to 5 and the most frequent 3,159 words are taken as the topical words vocabulary by stemming , filtering stop-words from the training set .
The batch size is set to 128 for all models .
We trained a Twitter LDA model to obtain the topical words for TA-SEQ2SEQ and set its model-specific parameters following the original paper ( Xing et al. , 2017 ) .
For regularization and preventing over-fitting , a dropout of 0.1 is applied and the weight decay is set to 3 ? 10 ?5 .
We used the pretrained word embeddings ( Pennington et al. , 2014 ) of 300 dimensions , and the vocabulary size is set to 20,000 .
All models are trained with early stopping , i.e. , if the loss does not decrease after 10 validations .
The loss is computed on the validation set at every 0.5 epochs and we save the parameters for the top model on the validation set .
We finally report evaluation scores on the test set from the saved model .
Overall Performance
Table 1 lists the performance of our system and the comparison systems .
CVAE and LAED inject SEQ2SEQ with stochastic latent variable , resulting in more informative responses and better performance on Distinct - { 1 , 2 , 3 }. TA-SEQ2SEQ incorporates SEQ2SEQ with the outsourcing topic information from LDA .
It is not surprising that it performs much better on the response relevance ( BLEU , Average , Greedy , Extrema ) , while its improvements on the informativeness are limited .
DOM -SEQ2SEQ builds multiple domain-specific encoder-decoders .
It gains improvements on both the relevance metrics and informativeness metrics .
In general , with both the context- aware and topic-aware parameterization , our model outperforms all the competitive baselines in terms of the response relevance and informativeness .
Context -aware vs Topic-aware Parameterization Context - aware parameterization captures local semantics of the given context , while topic-aware parameterization enables parameters sharing among conversations with similar topics .
As shown in Ta- ble 1 , both parameterization mechanisms perform much better than the original SEQ2SEQ model , while context - aware parameterization is slightly better in terms of informativeness .
When jointly utilizing both the context-aware and topic-aware parameterization mechanisms , we observe the best performance , indicating that these two mechanisms are both beneficial and complementary .
Human Evaluation
We conducted human evaluations on the test set to further validate the effectiveness of the model .
We randomly selected 500 samples from the test set .
Three well - educated students were invited to conduct the evaluation .
For each case , we provided annotators with triplets ( sample , response 1 , response 2 ) whereby one response is generated by ADAND , and the other is generated by a competitor model .
The annotators , who have no knowledge about which system the response is from , are then required to independently rate among win ( response 1 is better ) , loss ( response 2 is better ) and tie ( they are equally good or bad ) , considering four factors : context relevance , logical consistency , fluency and informativeness .
Note that if annotators rate different options , this triplet will be counted as " tie " .
Table 2 reveals the results of subjective evaluation .
The kappa scores indicate that the annotators came to a fair agreement in the judgment .
As expected , ADAND outperforms the other baselines and enjoys a large margin over the existing models .
The relative performance of the competitors is consistent with the quantitative evaluation results , confirming the superior performance of our proposed method .
Speed Test
We conducted speed test to verify the efficiency of the ADAND model empirically in Table 3 .
Augmented with auxiliary components , all the extension models exhibit higher time cost than the original SEQ2SEQ model .
We observe that the decoding speeds of CVAE and LAED are relatively comparable with our model .
However , when comparing with TA-SEQ2SEQ and DOM -SEQ2SEQ that also elaborately and explicitly model conversations with diverse topics or themes , ADAND shows a clear superiority in decoding speed .
For TA-SEQ2SEQ , it relies on an outside LDA model to obtain the topic information .
The joint attention and copying mechanism also reduce its efficiency .
For DOM -SEQ2SEQ , it is not surprising that the time complexity of multiple topic / theme-specific encoderdecoders is much higher than all - other comparison models .
ADAND utilizes a single encoder-decoder and is parameterized dynamically regarding the input context , which ensures its flexibility and efficiency .
Analysis & Case Study
To get some insights of how topic-aware parameterization performs , we present the topics by the words ( ? in Eq. ( 10 ) ) with top - 10 highest probabilities in Table 4 .
The discernible clusters of the topical words ( in Eq . ( 11 ) ) are illustrated in Figure 4 .
These evidences demonstrate that the topic inferrer in topic-aware parameterization effectively distills the latent topic distribution of each conversation , which enables the parameter sharing among conversations with similar topics .
We also investigate the orthogonality of the learned U and V matrices in Eq. ( 7 ) and Eq. ( 12 ) .
We trained our model multiple times with differ - ent parameter initialization methods ( drawn values from normal distribution or uniform distribution ) .
We observe that U U T and V V T approximate identity matrices .
We conjecture that such SVD - alike parameterization implicitly enforces orthogonality during training .
We list several examples generated by different models in Table 5 .
The inferred latent topic distributions are also presented in the table .
It can be observed that responses generated by the original SEQ2SEQ model are more generic .
Latent variable conversation models ( CVAE and LAED ) generate more diverse but sometimes irrelevant responses , TA-SEQ2SEQ tends to produce short responses while DOM -SEQ2SEQ does not perform obviously better than TA - SEQ2SEQ .
The responses generated by ADAND are not only relevant but also informative .
Related Work
Our work is closely related to the research of dialogue generation in diverse conversations .
Previous work relies on external pre-organized topic information ( Xing et al. , 2017 ; Wang et al. , 2017 ) or predicted keywords ( Yao et al. , 2017 ; Wang et al. , 2018 ) to boost the response informativeness and coherence .
Choudhary et al. ( 2017 ) further leveraged the topic / theme annotations to build multiple separate encoder-decoder models for topic / themeaware response generation .
In contrast , we do not exploit any outsourcing or labeled topic information .
The proposed model directly infers the latent topics of each conversation and is trained in an end-to - end manner .
Another difference is that we Table 5 : Test samples of our model ( ADAND ) and the baselines .
The latent topic distributions inferred by ADAND are also presented .
The reference is the ground - truth response in the dataset .
maintain a single encoder-decoder for various conversations whereas the model is dynamically and specially parameterized .
The second line of related work is parameterization in NLP .
Ha et al. ( 2017 ) proposed to train a small network to generate the parameters for another larger network .
Such adaptive parameteriza-tion has been shown to be successful in many NLP tasks , including language modeling ( Suarez , 2017 ; Flennerhag et al. , 2018 ) , sequence generation ( Ha and Eck , 2018 ; Peng et al. , 2019 ) , and neural machine translation ( Platanios et al. , 2018 ) .
In our work , we parameterize the encoder-decoder with respect to both the context and the latent topics .
Regarding latent variable conversation models , prior researches strive to learn meaningful latent variables for dialogue systems , and reveal that latent variables befit the neural dialogue models with more diverse response generations ( Serban et al. , 2017 ; Clark and Cao , 2017 ; Chen et al. , 2018 ) and interpretable dialogue actions .
In our model , instead of directly injecting the latent variable into dialogue models , we distill the latent topics through neural variational inference , offering a more interpretable latent variable .
Moreover , we parameterize the encoder-decoder with the inferred latent topics , which allows parameter sharing among conversations with similar topics .
Conclusion
This paper presents an adaptive neural dialogue generation model - ADAND , which allows the dynamical parameterization of the model to each conversation and enables the generation of appropriate responses in diverse conversations .
Specially , we propose two adaptive parameterization approaches : context - aware parameterization which captures local semantics of the input context ; and topic-aware parameterization which enables parameter sharing by first inferring the latent topics of the given context and then generating the parameters with the inferred latent topics .
The proposed approaches are assessed on a large-scale conversational dataset and the results show that our model achieves superior performance and higher efficiency .
It should be noted that our approach is not isolated to only LSTMs .
We would like to explore the effectiveness of the approach regarding other structures in future work .
Figure 1 : 1 Figure 1 : General model architecture .
Black solid lines denote information flow , and the red dashed line indicates the adaptive parameterization operation .
