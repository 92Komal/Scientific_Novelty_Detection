title
Multi-Domain Goal-Oriented Dialogues ( MultiDoGO ) : Strategies toward Curating and Annotating Large Scale Dialogue Data
abstract
The need for high-quality , large-scale , goaloriented dialogue datasets continues to grow as virtual assistants become increasingly widespread .
However , publicly available datasets useful for this area are limited either in their size , linguistic diversity , domain coverage , or annotation granularity .
In this paper , we present strategies toward curating and annotating large scale goal oriented dialogue data .
We introduce the MultiDoGO dataset to overcome these limitations .
With a total of over 81 K dialogues harvested across six domains , MultiDoGO is over 8 times the size of MultiWOZ , the other largest comparable dialogue dataset currently available to the public .
Over 54 K of these harvested conversations are annotated for intent classes and slot labels .
We adopt a Wizard - of - Oz approach wherein a crowd-sourced worker ( the " customer " ) is paired with a trained annotator ( the " agent " ) .
The data curation process was controlled via biases to ensure a diversity in dialogue flows following variable dialogue policies .
We provide distinct class label tags for agents vs. customer utterances , along with applicable slot labels .
We also compare and contrast our strategies on annotation granularity , i.e. turn vs. sentence level .
Furthermore , we compare and contrast annotations curated by leveraging professional annotators vs the crowd .
We believe our strategies for eliciting and annotating such a dialogue dataset scales across modalities and domains and potentially languages in the future .
To demonstrate the efficacy of our devised strategies we establish neural baselines for classification on the agent and customer utterances as well as slot labeling for each domain .
Introduction Modern Natural Language Understanding ( NLU ) frameworks for dialogues are by definition data hungry .
They require large amounts of training data representative of goal oriented conversations reflecting both context and diversity .
But human responses in goal-oriented dialogues are less predictable than automated systems ( Bordes et al. , 2016 ) .
For example , " Please do this " cannot be interpreted without a broader context .
Only by seeing previous utterances , such as requests to book a flight on a specific day to a specific destination , can this task be performed .
Additionally , a single intent can be phrased in multiple ways depending on context ; " book my flight " , " finalize my reservation " , " Yes , the 6 pm one " may all be referring to a flight - booking intent .
Hence , entire conversations , rather than independent utterances , must be collected .
Such data is even more pertinent to modeling NLU and related tasks as they require large , varied , and ideally human- generated datasets .
Moreover , recent work ( Dong et al. , 2015 ; Devlin et al. , 2018 ) has shown the benefit of applying joint-training and transfer learning techniques to natural language processing tasks .
However , these approaches have yet to become widely used in dialogue tasks , due to a lack of largescale datasets .
Furthermore , the latest state of the art end-to - end neural approaches benefit from such training data even more so than past work on goal-oriented dialogues structured around slot filling ( Lemon et al. , 2006 ; Wang and Lemon , 2013 ) .
One way to simulate data- and not risk releasing personally identifying information - for a domain is to use a Wizard - of - Oz data gathering technique , which requires that participants in a conver -
Table 1 : A segment of a dialogue from the airline domain annotated at the turn level .
This data is annotated with agent dialogue acts ( DA ) , customer intent classes ( IC ) , and slot labels ( SL ) .
Roles C and A stand for " Customer " and " Agent " , respectively .
sation fulfill a role ( Kelley , 1984 ) .
This approach has been used in popular public goal-oriented datasets : DSTC and MultiWOZ ( Williams et al. , 2016 ; Budzianowski et al. , 2018 ) .
Conversations between people and automated systems occur with increasing frequency , especially in customer service .
Customers reach out to agents , which could be automated bots or real individuals , to achieve a domain-specific goal .
This creates a disparate conversation : agents are incentivized to operate within a set procedure and convey a patient and professional tone .
In contrast , customers do not have this incentive .
However , to date , the largest available multi-domain goal-oriented dialogue dataset assigns similar dialogue act annotations to both agents and customers ( Budzianowski et al. , 2018 ) .
To solve the aforementioned challenges , we present our efforts to curate , annotate , and evaluate a large scale multi-domain set of goal oriented dialogues .
The dataset is primarily gathered from workers in the crowd paired with professional annotators .
The dataset elicited , MultiDoGO , comprises over 86 K raw conversations of which 54,818 conversations are annotated at the turn level .
We investigate multiple levels of annotation granularity .
We annotate a subset of the data on both turn and sentence levels .
A turn is defined as a sequence of one or more speech / text sentences by a participant in a conversation .
A sentence is a period delimited sequence of words in a turn .
A turn may comprise one or more sentences .
We do use the term utterance to refer to a unit ( turn or sentence , spoken or written by a participant ) .
1 1 We acknowledge that the term utterance is controversial in the literature ( Pareti and Lando , 2018 )
In our devised annotation strategy , we distinguish between dialogue speech acts for agents vs. customers .
In MultiDoGO , the agents ' speech acts [ DA ] are annotated with generic class labels common across all domains , while customer speech acts are labeled with intent classes [ IC ] .
Moreover , we annotate customer utterances with the appropriate slot labels [ SL ] , which consist of the SL span and corresponding tokens with that SL tag .
We present the strategies we use to curate and annotate such data given its contextual setting .
We furthermore illustrate the efficacy of our devised approaches and annotation decisions against intrinsic metrics and via extrinsic evaluation , namely by applying neural baselines for DA , IC and SL classification leveraging joint models .
Existing Dialogue Datasets
There are multiple existing goal-oriented dialogue collections generated by humans through Wizardof - Oz techniques .
The Dialog State Tracking Challenge , aka Dialog Systems Technology Challenge , ( DSTC ) spans 8 iterations and entails the domains of bus timetables , restaurant reservations , and hotel bookings , travel , alarms , movies , etc .
( Williams et al. , 2016 ) . Frames ( Asri et al. , 2017 ) has 1369 dialogues about vacation packages .
MultiWOZ contains 10,438 dialogues about Cambridge hotels and restaurants ( Budzianowski et al. , 2018 ) .
There are several dialogue datasets that specialize in a single domain .
ATIS ( Hemphill et al. , 1990 ) comprises speech data about airlines structured around formal airline flight tables .
Similarly , the Google Airlines dataset purportedly contains 400,000 templated dialogues about airline reservations ( Wei et al. , 2018 ) .
2 The Ubuntu Dialogue Corpus has over a million dialogues about Ubuntu technical support ( Lowe et al. , 2015 ) .
On the other hand , Chit-chat style dialogues without goals have been popular since ELIZA and have been investigated with neural techniques ( Weizenbaum , 1966 ; Li et al. , 2016 Li et al. , , 2017 .
However , these datasets cannot be used for modeling goal-oriented tasks .
Related dialogue dataset collections used for Sequential Question Answering rely on dialogue to answer questions , but the task is notably different from our use case of modeling goal oriented conversational AI , hence leading to different evaluation considerations ( Reddy et al. , 2019 ; Choi et al. , 2018 ) . 3 MultiDoGO Dataset Curation
Data Collection Procedure
We employ both internal data associates , who we train , and crowd-sourced workers from Mechanical Turk ( MTurkers ) to generate conversational data using a Wizard - of - Oz approach .
In each conversation , the data associates assumes the role of an agent while the MTurkers act as customers .
In an effort to source competent MTurkers , we require that each MTurker have a Human Intelligence Task ( HIT ) accuracy minimum of 90 % , a location in the United States , and have completed a significant number of HITs in the past .
To facilitate goal-oriented conversations between the customer and agent , we give each agent a prompt listing the supported request types ( dialog acts ) and pieces of information ( slots ) needed to complete each request .
We also specify criteria such as minimal conversation length , number of goals , number of complex requests , etc , to increase conversation diversity .
See Figure 2 for an example prompt .
In addition , we explicitly request that neither agents nor customers use any personally identifiable information .
At an implementation level , we create a custom , web interface for the MTurkers and data associates that displays our instructions next to the current dialogue .
This allows each participant to quickly refer to our guidelines without stopping the conversation .
Despite following a familiar wizard - of- oz elicitation procedure , and curating data for multiple domains in a fashion similar to previous data collection efforts such as MultiWOZ , MultiDoGO comprises more varied domains , it is collected at an unprecedented scale , and it is curated with control over generating explicit biases in the conversations to allow for diverse conversation representation .
To our knowledge this is a novel collection strategy as we explicitly guide / prod the participants in a dialogue to engage in conversations with specific biases such as intent change , slot change , multi-intent , multiple slot values , slot overfilling and slot deletion .
For example , in the Fast Food domain , participants were instructed to pretend that they were ordering fast food from a drive-thru .
After making their initial order , they were instructed to change their mind about what they were ordering ( " I 'd like a burger .
No wait , can you make that a chicken sandwich ? " ) .
In the Financial domain , we asked participants to make sure that they requested multiple intents such as " I 'd like to find my routing number and check my balance . "
3
To that end , our collection procedure deliberately attempts to guide the dialogue flow to ensure diversity in dialogue policies .
Domain Selection
Our primary criteria for domain selection are twofold : covering a broad sweep of industries that use goal-oriented dialogues and selecting domains where conversational interfaces are already in use or likely to be implemented in the future .
This set of criteria is especially well matched with domains that frequently involve customer support .
Furthermore , there is a shortage of publicly available data in the domains we provide , such as Fast Food and Finance .
To fulfill both of these needs , we include multiple domains in the MultiDoGO dataset .
Ultimately , we curate conversations for six domains : Airline , Fast Food , Finance , Insurance , Media , and Software Support .
When considered independently , the corpus of dialogues for each of these domains is the largest collection of human-elicited dialogues available for financial advice and help , media support , enterprise software support ( nontechnical level support , unlike the Ubuntu forum dataset ( Lowe et al. , 2015 ) ) , fast food , and insurance .
Domains in our Data : 4 Airline domain dialogues focus on booking airline flights , select - ing or changing seat assignments , and requesting boarding passes ; Fast Food domain is the least similar to the others , as the intents primarily involve ordering food and the slots quantify their order .
For example , the OrderBurgerIntent contains slots for size , quantity , and ingredients ;
Finance domain simulates dialogues a customer may have with a bank .
These include opening a bank account , checking their balance , and reporting a lost credit card ;
Insurance domain simulates users calling about their insurance policy or requesting the fulfillment of a policy on their car or phone ; Media domain simulates dialogues a customer may have ordering a service or paying bills related to telecommunications .
This is our largest domain ; Software domain involves customers inquiring about software services : products , outages , promotions , and bills .
The majority of intents are domain specific .
Domain Schemata and Guidelines Prior to dialogue collection , we develop schemata for each domain .
These schemata are the set of slot labels , slot value types , and intents that pertain to the domain .
To determine which slots , values , intents , and dialog acts to include , we rely on real word reference points .
For instance , we populate slots for the Fast Food domain by identifying menu items , such as sodas , that are shared among popular fast food menus .
Using this schema , we then write two sets of instructions .
One set of instructions is for the " agents " and the other is for the " customers " .
The agents ' instructions are meticulously detailed as we expect them to " structure " the conversation and appropriately respond to out of domain requests .
Since our agents are trained for their role , we have high confidence in their ability to follow complex guidelines .
In con-trast , taking into consideration that the customer role is to be carried out by crowd-sourced workers , i.e. lay people , we create simplified instructions that are less detailed and shorter in length .
For each task , we provide an annotated conversation , explain each answer option , and ( most importantly ) provide examples .
Before scaling up our data collection , we run a pilot for each task and identify commonly missed questions .
We use this pilot process to revise the instructions and add relevant examples iteratively .
Data Annotation
Annotated Dialogue Tasks
Our dataset has three types of annotation : Agent dialogue acts [ DA ] , customer intent classes [ IC ] , and slot labels [ SL ] .
We intentionally decouple Agent and customer speech act tags into the categories DA and IC , respectively , to produce more fine - grained speech act tags than past iterations of dialog datasets .
Intuitively , agent DAs are consistent across domains and more abstract in nature , since agents have a standard form of response .
On the other hand , customer ICs are domain-specific and can entail reserving a hotel room or ordering a burger , depending on the domain .
A conversation example with annotations is provided in Table 1 . Agent Dialogue Acts ( DA ) Agent dialogue acts are the most straightforward of our annotation tasks .
There are eight possible DAs in all domains : ElicitGoal , ElicitSlot , ConfirmGoal , Con-firmSlot , EndGoal , Pleasantries ,
Other .
The names are self-explanatory .
Elicit Goal / Slot indicates that the agent is gathering information .
Confirm Goal / Slot indicates that the agent is confirming previously provided information .
The End-Goal and Pleasantries tags , identify non-task re-lated actions .
Other indicates that the selected utterance was not one of the other possible tags .
Agent dialogue acts are consistent across domains and are often abstract ( e.g. ElicitIntent , Confirm - Slot ) .
Customer Intent Classes ( IC ) : Unlike Agent DA , customer IC vary for each domain and are more concrete .
For example , the Airline domain has a " BookFlight " IC , Fast Food has an " Or- derMeal " IC , and Insurance has an " OrderPolicy " IC in our annotation schema .
Customer intents can overlap across domains ( e.g. OpeningGreeting , ClosingGreeting ) and other times be domain specific ( e.g. RequestCreditLimitIncrease , Order-Burger , BookFlight ) .
Slot Labels ( SL ) : Slot Labeling ( SL ) is a task contingent on Customer Intent Classes .
Certain intents require that additional information , namely slot values , be captured .
For instance , to open a bank account , one must solicit the customer 's social security number .
Slots can overlap across intents ( e.g. Name , SSN Number ) or they can be unique to a domain-specific intent ( e.g. CarPolicy ) .
Data Annotation Procedure
Our annotators utilize a web interface , depicted in Figure 1 , to select the appropriate intent class for an utterance out of a list of provided options .
To annotate slot labels , our annotators use their cursors to highlight slot value character spans within an utterance and then select the corresponding slot label from a list of options .
The output of this slot labeling process is a list of slot-label , slot-value , span triplets for each utterance .
Annotation Design Decisions Decoupled Agents and Customers Label sets : Agents and customers have notably different goals and styles of communication .
However , past dialogue datasets do not make this distinction at speech act schema level .
Specificity is important for handling unique customer requests , but a relatively formulaic approach is required of agents across different industries .
Our distinction between the customer and agent roles creates training data for a bot that explicitly simulates agents .
Annotation Unit Granularity : Sentence vs. Turn Level
An important decision , which is often under discussed , is the proper semantic unit of text to annotate in a dialogue .
Commonly , datasets provide annotations at the turn level ( Budzianowski et al. , 2018 ; Asri et al. , 2017 ; Mihail et al. , 2017 ) .
However , turn level annotations can introduce confusion for IC datasets , given multiple intents may be present in different sentences of a single turn .
For instance , consider the turn " I would like to book a flight to San Francisco .
Also , I want to cancel a flight to Austin . "
Here , the first sentence has the BookFlight intent and the second sentence has the CancelFlight intent .
An turn level annotation of this utterance would yield the multi-class intent ( BookFlight , CancelFlight ) .
In contrast , a sentence level annotation of this utterance identifies that the first sentence corresponds to BookFlight while the second corresponds to CancelFlight .
We annotate a subset our data , 2,500 conversation per domain for 15,000 conversations in total , at the sentence as well as turn level to access the impact of this design choice on downstream performance .
The remainder of our dataset is annotated only at the turn level .
Professional vs. Crowd-Sourced Workers for annotation For annotation , we compare and contrast professional annotators to crowd sourced annotators on a subset of data .
Professional annotators assign DA , IC , and SL tags to the 15,000 conversations annotated at both the turn and sentence level ; statistics for these conversations are given in table 6 .
In an effort to decrease annotation cost , we employ crowd source annotators via Mechanical Turk to label an additional 54,818 conversations rated as Good or Excellent quality during data collection .
We provide statistics for this set of crowd annotated data in Table 3 .
To compare the quality of crowd sourced annotations against professional annotations , we use both strategies to annotate a shared subset of 8,450 conversations .
We devise an Inter Source Annotation Agreement ( ISAA ) metric to quantify the agreement of these crowd sourced and professionally sourced annotations .
ISAA is a relaxation of Cohen Kappa , in-tended to count partial agreement of multi-tag labels .
ISAA defines two sets of tags , A and B , to be in agreement if there is at least one " shared " tag in both A and B .
A and B reflect the majority labels agreed upon per source ( professionals or crowd workers ) .
Using ISAA we find that crowd sourced and professional annotations have a substantial degree of shared annotations .
We report ISAA for the DA , IC , and SL tasks in Table 2 .
Quality Control
We institute three processes to enforce data quality .
During data collection , our data associates report on the quality of each conversation .
Specifically , the data associates grade the conversation on a scale from " Unusable " , " Poor " , " Good " , to " Excellent " .
They were provided with guidelines to help decide on the chosen rating such as coherence , whether the dialogue achieved the purported goal , etc .
To ensure high data quality we only utilize conversations with " Good " or " Excellent " ratings in subsequent annotation .
Secondly , for data annotation , each conversation is annotated at least twice .
We remove inconsistent annotations by selecting the annotation given by the majority of annotators per item .
We calculate inter-annotator agreement with Fleiss Kappa and find " substantial agreement " , according to the metric .
Our annotators must pass a qualification test as well as maintain an on-going level of accuracy in randomly distributed test questions throughout their annotation .
Third , we preprocess our data to remove issues such as duplicate conversations and improperly entered slot value spans .
We refer readers to our discussion of preprocessing in Section 5 for further detail .
Dataset Characterization and Statistics MultiDoGO dataset is more diverse by virtue of covering more domains , but more importantly , it is more controlled since it was curated rather than being scraped from existing data sources that are not necessarily synchronous ( Ubuntu ) .
Table 3 shows the statistics for MultiDoGO raw conversations harvested , rated as Excellent or Good , and annotated for DA , IC and SL .
Table 4 shows the number of conversations per domain reflecting the specific biases used .
MultiDoGO is several orders of magnitude larger than comparable datasets as reflected in nearly every dimension : the number of conversations , the length of the conversation , the number of
Agent Instructions
Imagine you work at a bank .
Customers may contact you about the following set of issues : checking account balances ( checking or savings ) , transferring money between accounts , and closing accounts .
GOAL : Answer the customer 's question ( s ) and complete their request ( s ) .
For any request , you will need to collect at least the following information to be able to identify the customer : name , account PIN * or * last 4 digits of SSN .
For giving information on balances , or for closing accounts , you will also need the last 4 digits of the account number .
For transferring money , you will also need : last 4 digits of account to move from , last 4 digits of account to move to , and the sum of money to be transferred .
Your customer may ask you to do only one thing ; that 's okay , but make sure you confirm you achieved everything the Customer wanted before completing the conversation .
Do n't forget to signal the end of the conversation ( see General guidelines )
Figure 2 : Agents are provided with explicit fulfillment instructions .
These are quick-reference instructions for the Finance domain .
Agents serve as one level of quality control by evaluating a conversation between Excellent and Unusable .
domains , and the diversity of the utterances used .
Table 5 illustrates a comparative statistics to existing data sets .
We provide summary statistics for the subset of our data annotated at both turn and sentence granularity in Table 6 .
This describes the total size of the data per domain in number of conversations , turns , the unique number of intents and slots , and inter-annotator agreement ( IAA ) for both turn and sentence level annotations .
It is worth observing that the DA annotations achieve a much higher IAA in Sentence level annotations compared to Turn level annotation , most notably in the Fast Food domain .
IC and SL annotations reflect a slightly higher IAA in Turn level annotation granularity compared to Sentence level .
Dialogue Classification Baselines
To establish baseline performance for the MultiDoGO dataset we pre-process , create dataset splits , and evaluate the performance of three baseline models for each domain .
Pre-processing :
We pre-process the corpus of dialogues for each domain to remove duplicate Table 5 : MultiDoGO is several times larger in nearly every dimension to the pertinent datasets as selected by Budzianowski et al . ( 2018 ) .
We provide counts for the training data , except for FRAMES , which does not have splits .
Our number of unique tokens and slots can be attributed to us not relying on carrier phrases .
conversations and utterances with inconsistent annotations .
The most common source of inconsistent annotations in our dataset is imprecise selection of slot label spans by annotators , which results in sub-token slot labels .
While much of this inconsistent data could likely be recovered by mapping each character span to the nearest token span , we drop these utterances to ensure these errors have no effect on our experimental results .
Our postprocessed data is pruned to approximately 90 % of the original size .
We form splits for each domain at the conversation level by randomly assigning 70 % of conversations to train , 10 % to development , and 20 % to test .
Conversation level splits enable the application of contextual models to our dataset , as each conversation is assigned to a single split .
However , our conversation level splits result in imbalanced intent and slot label distributions .
Models :
We evaluate the performance of two neural models on each domain .
The first is a bi-directional LSTM ( Hochreiter and Schmidhuber , 1997 ) with GloVe word embeddings , a hidden state of size 512 , and two fully connected output layers for slot labels and intent classes respectively .
The second model , ELMO , is similar to the LSTM architecture but it addiitonally uses pretrained ELMO ( Peters et al. , 2018 ) embeddings in addition to GloVe word embeddings , which are kept frozen during training .
We combine these ELMO and GloVe embeddings via concatenation .
As a sanity check , we also include a most frequent class ( MFC ) baseline .
The MFC baseline assigns the most frequent class label in the training split to every utterance u in the test split for both DA and IC tasks .
To adapt the MFC baseline to SL , we compute the most frequent slot label MFC ( w ) for each word type w in the training set .
Then given a test utterance u , we assign the pre-computed , most frequent slot MFC ( w ) to each word w ?
u if w is present in the training set .
If a given word w ?
u is not present the training set , we assign the other slot label , which denotes the absence of a slot , to w .
We utilize the AllenNLP ( Gardner et al. , 2017 ) library to implement these models and evaluate our performance .
We use the Adam optimizer ( Kingma and Ba , 2014 ) with a learning rate of 0.001 to train the LSTM and ELMO models for 50 epochs , using batch sizes 256 and 128 , respectively .
In addition , we employ early stopping on the validation loss with a tolerance of 10 epochs to prevent over fitting .
Evaluation Metrics :
We report micro F1 score to evaluate DA and IC performance of our models .
Similarly , we use a span based F1 score , implemented in the seqeval library 5 , to evaluate SL performance .
Results DA / IC / SL Results .
4 and the lowest IAA as per Table 6 . Sentence vs .
Turn Level Annotation Units .
Regarding the performance of the LSTM and ELMO models on sentence vs. turn level annotation units , our results suggest that turn level annotations increase the difficulty of the DA classification task .
This finding is evidenced by DA performance of our models on the Fast Food domain , for which F1 score is up to 25 F1 points lower for turn level annotations than sentence level annotations .
We believe the increased difficulty of turn level DA relative to sentence level DA is driven by a corresponding increase in the confusability of turn level dialogue acts .
This assertion of greater turn level DA confusability is supported by the lower inter annotator agreement ( IAA ) scores on turn level DA , which range from 0.314 to 0.521 , relative to IAA scores for sentence level DA , which range from 0.598 to 0.709 .
This experimental result highlights the importance of collecting sentence level annotations for conversational DA datasets .
Somewhat surprisingly , our models achieve similar IC F1 and SL F1 scores on turn and sentence level annotations .
We hypothesize that the choice of annotation unit has a lesser impact on the IC and SL tasks because customer utterances are more likely to focus on a single speech act , whereas Agent utterances may be more complex in comparison and include a greater number of speech acts .
Joint Training on Agent DA .
Agent DA clas-sification naturally lends itself to joint training , given agent DAs are shared among all domains .
To explore the benefits of multi-domain training , we jointly train an agent DA classification model on all domains and report test results for each domain separately .
These results are provided in Table 8 .
This straightforward technique leads to a consistent but less than one point improvement in F1 scores .
We expect that more sophisticated transfer learning methods Howard and Ruder , 2018 ) could generate larger improvements for these domains .
Overall , our results demonstrate that there is still headroom for performance improvement , especially for the SL task , across all domains .
Consequently , MultiDoGO should be a relevant benchmark for developing new state - of- theart NLU models for the foreseeable future .
Future Directions
The data collection and annotation methodology that we use to gather MultiDoGO can efficiently scale across languages .
Several pilot experiments aimed at collecting Spanish dialogues in the same domains have shown preliminary success in quality assessment .
The production of a NLU dataset with parallel data in multiple languages would be a boon to the cross-lingual research community .
To date , cross-lingual NLU research ( Upadhyay et al. , 2018 ; Schuster et al. , 2018 ) has relied on much smaller parallel corpora .
Conclusion
We present MultiDoGO , a new Wizard - of - Oz dialogue dataset that is the largest human-generated , multi-domain corpora of conversations to date .
The scale and range of this data provides a testbed for future work in joint training and transfer learning .
Moreover , our comparison of sentence and turn level annotations provides insight into the effect of annotation granularity on downstream model performance .
By pairing crowd-source labor with professional data annotators , we balance the cost , diversity , and quality of these conversations in a scalable manner .
We show that by adopting a modular annotation strategy , the crowds can reliably annotate dialogues at a level commensurate with trained professional annotators .
morning .
You 're connected to LMT Airways .
How may I help you ?
DA = { elicitgoal } C Hi , I wonder if you can confirm my seat assignment on my flight tomorrow ?
IC = { SeatAssignment } A Sure !
I 'd be glad to help you with that .
May I know your last name please ?
DA = { elicitslot } C My last name is Turker .
IC = { contentonly } , SL = { Name : Turker } A Alright Turker !
Could you please share the booking confirmation number
