title
Reading Turn by Turn : Hierarchical Attention Architecture for Spoken Dialogue Comprehension
abstract
Comprehending multi-turn spoken conversations is an emerging research area , presenting challenges different from reading comprehension of passages due to the interactive nature of information exchange from at least two speakers .
Unlike passages , where sentences are often the default semantic modeling unit , in multi-turn conversations , a turn is a topically coherent unit embodied with immediately relevant context , making it a linguistically intuitive segment for computationally modeling verbal interactions .
Therefore , in this work , we propose a hierarchical attention neural network architecture , combining turnlevel and word-level attention mechanisms , to improve spoken dialogue comprehension performance .
Experiments are conducted on a multi-turn conversation dataset , where nurses inquire and discuss symptom information with patients .
We empirically show that the proposed approach outperforms standard attention baselines , achieves more efficient learning outcomes , and is more robust to lengthy and out -of- distribution test samples .
Introduction
Reading comprehension has attracted much interest in the past couple years , fueled by avid neural modeling investigations .
Given a certain textual content , the goal is to answer a series of questions based on implicit semantic understanding .
Previous work has focused on passages like Wikipedia ( Rajpurkar et al. , 2016 ) or news articles ( Hermann et al. , 2015 ) .
Recently , dialogue comprehension in the form of cloze tests and multi-choice questions has also started to spur research interest ( Ma et al. , 2018 ; Sun et al. , 2019 ) .
Different from passages , human-to-human dialogues are a dynamic and interactive flow of information exchange , which are often informal , verbose and repetitive .
1
This leads to lower information density and more topic diffusion , since the spoken content of a conversation is determined by two speakers , each with his / her own thought process and potentially distracting and parallel streams of thoughts .
To address such challenges , we propose to utilize a hierarchical attention mechanism for dialogue comprehension , which has shown to be effective in various natural language processing tasks ( Yang et al. , 2016 ; Choi et al. , 2017 ; Hsu et al. , 2018 ) .
The hierarchical models successively capture contextual information at different levels of granularity , leveraging coarse-grained attention to reduce the potential distraction in finer- grained attention but at the same time exploit finer- grained attention to distill key information for downstream tasks more precisely and efficiently .
While in document tasks sentences are the default semantic modeling unit at the coarse- grained level , utterances might be a more suitable counterpart in spoken dialogues , as dialogues often consist of incomplete sentences .
However , a single utterance / sentence which usually implies information from one speaker is insufficient for grasping the full relevant context , as the interactive information from the interlocutor is often necessary .
In multi-turn dialogues , each turn is one round of information exchange between speakers , thus making it a linguistically intuitive segment for modeling verbal communications .
Thus , we postulate that for spoken dialogue comprehension , it is more effective to model conversations turn by turn using a multi-granularity design .
In this work , we introduce a hierarchical neu - ral attention architecture , integrating turn- level attention with word- level attention for multi-turn dialogue comprehension in a question - answering manner , where we evaluate performance on a corpus preserving linguistic features from real-world spoken conversation scenarios .
In particular , we examine how our approach is able to address challenges from limited training data scenarios and from lengthy and out -of- distribution test samples .
Hierarchical Attention Architecture
The proposed architecture of modeling multi-level attention for dialogue comprehension is shown in Figure 1 .
The model design is based on extractive question answering , and consists of the following layers : a sequence encoding layer , a questionaware modeling layer , a turn- level attention layer , a word- level attention layer , and an answer pointer layer .
We elaborate on the details below .
Sequence Encoding Layer Given a t-length sequence of word embedding vectors S = {w 0 , w 1 , ...w t } , a bi-directional long short-term memory ( Bi - LSTM ) layer ( Schuster and Paliwal , 1997 ) is used to encode S to a hidden representation , H = {h 0 , h 1 , ...h t } ?
R t?d , where d is the hidden dimension .
We obtain the content representation H c by encoding the dialogue sequence and concatenating the forward and backward information : h c i = [ LST M F orward w c i ; LST M Backward w c i ] ( 1 ) and extracting the last hidden state of question encoding as the question representation h q .
Question - Aware Modeling Layer
We concatenate each step of H c with the question h q as in aspect-modeling , then obtain the question - aware modeling H via a Bi-LSTM layer .
h i = [ LST M F orward [ h c i ;h q ] ; LST M Backward [ h c i ;h q ] ] ( 2 )
Turn -Level Attention Layer
We design the turn-level attention to score the dialogue turns explicitly , so the more salient turns will obtain higher scores , which is similar to ( Hsu et al. , 2018 ) .
However , instead of calculating the sentence - level attention using a separate recurrent component , we directly obtain the turn representations H turn by collecting hidden states from H with the turn - level segment position indices T turn = {t turn 0 , t turn 1 , ...t turn m } , where m is the turn number of the dialogue content .
More specifically , in a two -party conversation , each continuous utterance span between the speakers will be labeled as in one turn segment , and t turn i + 1 ? t turn i is the length of the ith turn .
Then the turn- level attentive score is calculated via a dense layer and softmax normalization : A turn = softmax ( W ? H turn + b ? ) ( 3 )
Word -Level Attention Layer
In our hierarchical architecture , to mitigate adverse effects of spurious word- level attention from words in less attended turns , we utilize turn - level salient scores to modulate word-level attentions .
Thus , we broadcast each a turn i in A turn with its turn length to obtain A in dialogue length , and then multiply H with A to obtain the contextual sequence C .
Then the word- level attention A word is calculated on C , and multiplied with H to obtain the contextual sequence C . A word = softmax ( W ? ( H * A ) + b ? ) ( 4 )
Answer Pointer Layer Contextual sequences C , C and question h q are concatenated together and fed to a LSTM modeling layer .
Then a dense layer with softmax normalization is applied for answer span prediction ( Wang and Jiang , 2016 ) .
M s/e = LST M [ C ; C ;h q ] ( 5 ) P s/e = softmax ( W ? M s/e + b ? ) ( 6 ) where each p s /p e indicates the probability of being the start / end position of the answer span .
Loss function Cross-entropy loss function is used as the metric between the predicted label and the ground -truth distribution .
The total loss L total contains the loss from answer span ( Wang and Jiang , 2016 ) and from turn - level attentive scoring similar to ( Hsu et al. , 2018 ) , with a weight ? ? [ 0 , 1 ] .
L total = L span + ?L turn attn ( 7 ) 3 Experiments
Corpus & Data Processing Dialogue Dataset :
We evaluated the proposed approach on a spoken dialogue comprehension dataset , consisting of nurse-to- patient symptom monitoring conversations .
This corpus was inspired by real dialogues in the clinical setting where nurses inquire about symptoms of patients ( Liu et al. , 2019 ) .
Linguistic structures at the semantic , syntactic , discourse and pragmatic levels were abstracted from these conversations to construct templates for simulating multi-turn dialogues .
The informal styles of expressions , including incomplete sentences , incorrect grammar and diffuse flow of topics were preserved .
A team of linguistically trained personnel refined , substantiated , and corrected the automatically simulated dialogues by enriching verbal expressions through different English speaking populations in Asia , Europe and the U.S. , validating
The default segmented turn is an adjacency pair of utterances from two speakers ( Yellow ) .
To ensure a turn spans across semantically congruent utterances , neighboring utterances could be merged according to a set of rules derived from spoken features , like n-gram repetition ( Green ) , back - channeling ( Blue ) , self - pause ( Red ) and interlocutor interruption ( Gray ) .
logical correctness through checking if the conversations were natural , reasonable and not disobeying common sense , and verifying the clinical content by consulting certified and registered nurses .
These conversations cover 9 topics / symptoms ( e.g. headache , cough ) .
For each conversation , the average word number is 255 and the average turn number is 15.5 .
Turn Segmentation :
In a smooth conversation , one turn is an adjacency pair of two utterances from two speakers ( Sacks et al. , 1974 ) .
However , in real scenarios , the conversation flow is often disrupted by verbal distractions such as interlocutor interruption , back - channeling , self - pause and repetition ( Schlangen , 2006 ) .
We thus annotated these verbal features from transcripts of the realworld dialogues and integrated them in the templates , which are used to generate the simulated dialogue data .
We subsequently merged the adjacent utterances from speakers considering the features and the intents to form turns ( see Figure 2 ) .
This procedure ensures semantic congruence of each turn .
Then the segment indices of turns were labeled for turn - level context collection .
Annotations for Question Answering :
For the comprehension task , questions were raised to query different attributes of a specified symptom ; e.g. ,
How frequently did you experience headaches ?
Answer spans in the dialogues were labeled with start and end indices , and turns containing the answer span were annotated for turnlevel attention training .
Baseline Models
We implemented the proposed turn - based hierarchical attention ( HA ) model , and compared it with several baselines : Pointer LSTM : We implemented a Pointer network for QA ( Vinyals et al. , 2015 ) .
The content and question embedding are concatenated and fed to a two -layer Bi-LSTM , then the answer span is predicted as in Section 2.5 .
Bi-DAF : We implemented the Bi-Directional Attention Flow network ( Seo et al. , 2017 )
Training Configuration Pre-trained word embeddings from Glove ( Pennington et al. , 2014 ) were utilized and fixed during training .
Out - of- vocabulary words were replaced with the [ unk ] token .
The hidden size and embedding dimension were set to 300 .
Adam optimizer ( Kingma and Ba , 2015 ) was used with batch size of 64 and learning rate of 0.001 .
For the modeling layers , dropout rate was set to 0.2 ( Srivastava et al. , 2014 ) .
The weight ? in the loss function was set to 1.0 .
During training , the validationbased early stop strategy was applied .
During prediction , we selected answer spans using the maximum product of p s and p e , with a constraint such that 0 ? e ? s ? 10 .
Evaluation : Comparison with Baselines Evaluation was conducted on the dialogue corpus described in Section 3.1 , where the training , validation and test sets were 40k , 3 k and 3 k samples of multi-turn dialogues , respectively .
We adopted Exact Match ( EM ) and F1 score in SQuAD as metrics ( Rajpurkar et al. , 2016 ) .
Results in Table 1 show that while the utterance - based HA network is on par with established baselines , the proposed turn - based HA model obtains more gains , achieving the best EM and F1 scores .
Evaluation in Low-Resource Scenarios Limited amount of training data is a major pain point for dialogue - based tasks , as it is timeconsuming and labor-intensive to collect and annotate natural dialogues at a large-scale .
We expect the hierarchical structure to result in more efficient learning capabilities .
We conducted experiments on a range of training sizes ( from 3 k to 40 k ) with a fixed - size test set ( 3 k samples ) .
As shown in Figure 3 , the turn - based HA model outperforms all other models significantly when the training set is smaller than 20k .
Lengthy Sample Evaluation
Spoken conversations are often verbose with low information density scattered with topics not central to the main dialogue theme , especially since speakers chit-chat and get distracted during taskoriented discussions .
To evaluate such scenarios , we adopted model- independent ADDSENT ( Jia and Liang , 2017 ) , where we randomly extracted sentences from SQuAD and inserted them before or after topically coherent segments .
The average length of the augmented test set ( 3 k samples ) , increased from 255 to 900 .
As shown in
Out-of-Distribution Evaluation
Another evaluation was performed on an augmented set of dialogue samples , by adding three out -of- distribution symptom entities ( bleeding , cold / flu , and sweating ) to the corresponding conversations ( 3 k samples ) .
This was conducted on the well -trained models in Section 3.4 .
As shown in Table 3 , the proposed turn - based HA model is the most robust in answering questions related to unseen symptoms / topics while till performing well on in- domain symptoms , thus showing potential generalization capabilities .
In summary , our overall experimental results demonstrate that the proposed hierarchical method achieves higher learning efficiency with robust performance .
Moreover , the turn- based model significantly outperforms the utterance - based one , empirically verifying that it is appropriate to use turns as the basic semantic unit in coarse-grained attention for modeling dialogues .
Related Work Machine comprehension of passages has achieved rapid progress lately , benefiting from large-scale datasets ( Rajpurkar et al. , 2016 ; Kocisky et al. , 2018 ) , semantic vector representations ( Pennington et al. , 2014 ; Peters et al. , 2018 ; Devlin et al. , 2019 ) , and end-to - end neural modeling ( Wang et al. , 2017 ; Hu et al. , 2018 ) .
The attention mechanism enables neural models to more flexibly focus on salient contextual segments ( Luong et al. , 2015 ; Vaswani et al. , 2017 ) , and is further im-proved by hierarchical designs for document processing tasks ( Yang et al. , 2016 ; Choi et al. , 2017 ) .
Multi-level attention could be fused in hidden representations ( Wang et al. , 2017 ) or calculated explicitly ( Hsu et al. , 2018 ) .
There is an established body of work studying how humans take turns speaking during conversations to better understand when and how to generate more natural dialogue responses ( Sacks et al. , 1974 ; Wilson et al. , 1984 ; Schlangen , 2006 ) .
Utterance - level attention has also been applied to context modeling for different dialogue tasks such as dialogue generation ( Serban et al. , 2016 ) and state tracking ( Dhingra et al. , 2017 ) .
Recently , there is emerging interest in machine comprehension of dialogue content ( Ma et al. , 2018 ; Sun et al. , 2019 ) .
To the best of our knowledge , our work is the first in exploiting turn -level attention in neural dialogue comprehension .
Conclusion
We proposed to comprehend dialogues by exploiting a hierarchical neural architecture through incorporating explicit turn-level attention scoring to complement word -level mechanisms .
We conducted experiments on a corpus embodying verbal distractors inspired from real-world spoken dialogues that interrupt the coherent flow of conversation topics .
Our model compares favorably to established baselines , performs better when there is limited training data , and is capable of addressing challenges from low information density of spoken dialogues and out-of- distribution samples .
Figure 1 : 1 Figure 1 : Turn-based hierarchical architecture for dialogue comprehension : tokens in purple are the indicators of dialogue turns , and their indices are used to select question - aware hidden states ( Green ) for turn - level attention calculation .
The turn with higher attentive score ( Yellow ) contributes more in scoring word-level attentions ( Red ) .
