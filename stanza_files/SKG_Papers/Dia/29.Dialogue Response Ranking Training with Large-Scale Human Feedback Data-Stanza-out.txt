title
Dialogue Response Ranking Training with Large-Scale Human Feedback Data
abstract
Existing open-domain dialog models are generally trained to minimize the perplexity of target human responses .
However , some human replies are more engaging than others , spawning more followup interactions .
Current conversational models are increasingly capable of producing turns that are context-relevant , but in order to produce compelling agents , these models need to be able to predict and optimize for turns that are genuinely engaging .
We leverage social media feedback data ( number of replies and upvotes ) to build a large-scale training dataset for feedback prediction .
To alleviate possible distortion between the feedback and engagingness , we convert the ranking problem to a comparison of response pairs which involve few confounding factors .
We trained DIALOGRPT , a set of GPT - 2 based models on 133 M pairs of human feedback data and the resulting ranker outperformed several baselines .
Particularly , our ranker outperforms the conventional dialog perplexity baseline with a large margin on predicting Reddit feedback .
We finally combine the feedback prediction models and a human-like scoring model to rank the machine - generated dialog responses .
Crowd - sourced human evaluation shows that our ranking method correlates better with real human preferences than baseline models .
1
Introduction
Conversing freely in natural language is one of the greatest challenges of artificial intelligence .
Endto-end open-domain dialog systems have become increasingly powerful , with advanced model architectures and large-scale training Adiwardana et al. , 2020 ; Roller et al. , 2020 ; Li et al. , 2020 ) .
In some settings , human annotators cannot reliably distinguish between human - and machine - generated responses .
Though surprisingly effective , the training objective for these models is conceptually simple : minimizing the perplexity of a reference response for a given context .
However , a meaningful evaluation of response generation must take into account more than whether a generated turn is relevant in context , or whether it " sounds human . "
Conventional neural conversation models often generate trivial or bland responses ( Li et al. , 2016 ; Zhao et al. , 2017 ) that are relevant to context but are not engaging .
Even human responses can vary dramatically in terms of tonal appropriateness and whether they are interesting enough to prompt a rich listener reaction .
A successful dialog turn must be proactive , engaging , and consistent with social norms ( Grice , 1975 ( Grice , , 1989 .
In this work , we move beyond simple prediction of response relevance , augmenting this with a prediction of how likely a response is to elicit a positive reaction from an interlocutor .
By incorporating a measure of engagingness into the response generation ranking algorithm , we hope to improve the overall behavior of data-driven conversational agents .
Existing methods are suboptimal for this ranking task .
Conventional perplexity based ranking methods ( Li et al. , 2016 ; Vijayakumar et al. , 2016 ) focus only on context- hypothesis relevancy .
Online conversational systems such as XiaoIce ( Zhou et al. , 2018 ) employ a manually - designed set of features to rank hypotheses , but the design of these rankers is not directly based on real-world human preferences or feedback in an end-to - end fashion .
Large-scale training data is necessary because of the one- to - many nature of dialog and the scope and complexity of human conversation .
However , labeling conversations at scale is too expensive and time - consuming for this purpose .
Labeling the " engagingness " of a response is not something a single annotator can do ; the task requires something more like a large-scale , collective vote .
And yet there is no obvious automated substitute for this kind of human labeling .
Conventional quality measurements such as reference - based similarity ( Papineni et al. , 2002 ) or lexical diversity ( Li et al. , 2016 ; Zhang et al. , 2018 b ) capture only limited aspects of response quality , and are not strongly predictive of human reactions : simply because a response is different from others does not necesarily mean that it will be perceived as " bad " .
Our solution involves leveraging existing human feedback data ( e.g. , number of replies and likes ) from online social communities .
While there is work in the field of social media on feedback prediction ( Sparling and Sen , 2011 ; Stoddard , 2015 ; Glenski and Weninger , 2017 ) , it has not previously been applied to dialog systems and response generation .
As illustrated in Figure 1 , each comment has its own number of replies and upvotes ( termed as " Likes " in some communities ) .
These can be used as engagingness labels after careful normalization and formulation .
There exist billions of online threads available and the number is growing fast , thus making it possible to build a large-scale training dataset .
However , the relation between feedback and quality may be distorted due to social influence and other confounding factors ( Salganik et al. , 2006 ) .
In order to ameliorate this problem , we propose a contrastive formulation , shifting from ranking to pairwise classification .
Using a dataset of 133 M pairs of human comments and their associated number of replies or up - / downvotes , we train a set of large-scale transformer - based feedback ranking models which outperform several baselines .
In particular , dialog perplexity shows little predictive power of human feedback .
We also show that a classifier trained on human- vs-artificial data can achieve good zero-shot relevancy prediction accuracy .
Finally , we describe an ensemble model that is capable of merging the predictive powers of all these models , tuned using human calibration .
Human evaluation shows that our ranking method outperforms the baselines in terms of correlation with actual human preferences .
Human Feedback Many social media platforms , such as Reddit , Twitter , and Facebook allow users to reply or upvote contents , leveraging that feedback to make decisions about what content to display , highlight , and hide .
These collective ratings are treated as a proxy for content engagingness .
In this section we discuss a few metrics of user vote data , along with some of the issues posed by its use .
Feedback metrics
As illustrated by Figure 1 , posts and comments typically form a tree structure .
Each comment branching from the root may have its own comment children .
We consider the path from the root to the parent node of a comment to be its context c , and the comment as a reply r.
For each dialog ( c , r ) , we consider the following feedback :
Width , the number of direct replies to r ; Depth , the maximum length of the dialog after this turn ; and Updown , the number of upvotes minus the number of downvotes .
For example , given the context c = u 0 , the reply u 1 gets three direct replies u 3 , u 4 , u 5 and the Width is thus 3 . u 3 continues the dialog with one more turn u 6 , thus the depth is 2 .
u 1 got 17 upvotes and 3 downvotes so its Updown is 14 .
In contrast , u 2 is for the same context , but its Width and Depth is only 0 , and Updown is 2 .
Though focused on different dimensions , both Width and Depth can be seen as measures of the number of replies , and are therefore often closely correlated , as shown in Table 1 using Reddit as an example .
They are less correlated with Updown .
Presumably , contributors may feel that an upvote is enough to express their agreement or appreciation , and so do not post a full reply .
Feedback and Engagingness
The feedback metrics defined above cannot be directly used as a measure of reply engagingness .
Stoddard ( 2015 ) shows that while popularity , measured by Updown , generally increases with quality , posts of similar quality can exhibit very different upvote counts .
This variability can be traced to several different factors .
As illustrated in Figure 2 , the distribution of feedback is long-tailed , with a small fraction of threads receiving most of the replies and likes .
Additionally , the popularity of the specific subreddit in which a comment occurs further confounds things : a relatively uninteresting comment in a very popular thread may get more feedback than an interesting comment in a less trafficked subreddit .
Feedback volume is also heavily dependent on the timing of a comment relative to other comments , with replies that come early in a thread being more likely to attract replies or likes .
This is shown in Figure 3 .
This may be tied to other factors such as social influence and disparities in comment visibility causing distortions in the relationship between comment engagingness and popularity ( Salganik et al. , 2006 ; Salganik and Watts , 2008 ; Gilbert , 2013 ) .
These findings imply that careful formulation and normalization should be applied before using feedback data as a training signal .
We present our approach to this in Section 3.1 .
Tasks Given a context and a list of responses , we consider the task of predicting a ranking based on the feedback they received , as measured by these three separate metrics : ( 1 ) Width , ( 2 ) Depth , and ( 3 ) Updown .
The gold label and training data is available for human response ranking , but in order to make this applicable to machine generated responses , we introduce another task : ( 4 ) human- vs-fake , which measures how human - like the response is .
We consider two modes of fake examples : random human responses and machine generated responses .
We will introduce an ensemble method in Section 3.2 for this last task .
The DIALOGRPT Method
In this section we introduce Dialog Ranking Pretrained Transformers ( DIALOGRPT ) .
Problem Formulation A Contrastive Learning approach .
Given the confounding factors affecting feedback mentioned above , we train the model on pairs of samples ( c , r + ) and ( c , r ? ) , rather than fitting it to score each dialog individually .
This follows the Contrastive Learning approach ( see Section 5 for a brief review ) .
The model is trained to predict a higher score for the positive sample r + ( i.e. the response with more feedback ) compared to the negative sample r ? . Besides ( 1 ) only comparing replies of the same context , we use the following criteria to construct pairs that minimize the effect of confounding factors : ( 2 ) the sequence of two replies , r + and r ? , must have been created within a brief time window ( no more than one hour ) , and ( 3 ) the feedback score of r + must exceed that of r ? by a specified threshold in order to make the label less noisy .
Due to the long-tailed distribution , we consider both an absolute- valued threshold and a percentage ranking threshold .
Furthermore , if a reply has more downvotes than upvotes , it will not be considered as a positive sample , but can be used as a negative sample .
Training objective .
The model should be able to output a score at testing time for a hypothesis r for a given context c.
At training time , as formulated in Section 3.1 , given two hypotheses for a context , the model should be able to identify which one has more feedback .
To connect these two requirements , the model outputs a scalar h , h( c , r ) = DIALOGRPT ( c , r ) ( 1 ) At inference time , we compute the score s ( r| c ) s ( r| c ) = Sigmoid ( h ( c , r ) ) ( 2 )
For training , the loss is designed to simultaneously maximize the positive sample score and minimize the negative sample score : L = ? i? batch log e h( c i , r + i ) e h( c i , r + i ) + e h( c i , r ? i ) ( 3 )
This can be interpreted as the cross entropy between the target distribution { P ( r + ) = 1 , P ( r ? ) = 0 } and the predicted distribution in Softmax form .
Note the contrastive form is crucial , given that a loss function only maximizing s( r + | c ) usually leads to a collapsed solution ( Hadsell et al. , 2006 ) .
Model ensemble
For machine generation .
The machine generation is required to be both human-like and preferred by human .
To rank the machine generations , we factorize the probability of a joint distribution as follows : P ( r = preferred , human-like | c ) = P ( r = preferred |r = human-like , c ) ?
P ( r = human-like | c ) ( 4 ) We estimate the first term with the models trained on a human-vs-human ranker on each feedback metric K ? { Width , Depth , Updown } P ( r = preferred K , human-like | c ) s K ( r|c ) ( 5 ) We denote the term P ( r = human-like |c ) as ? 0 ( r|c ) , and build a classifier to predict how human - like a response is ( see Section 3.3 for details ) .
P ( r = human-like |c ) ? 0 ( r|c ) ( 6 ) Both ? 0 ( r|c ) and s K ( r|c ) are scores defined in Eq. 2 interpreted as probability .
For overall preference .
In case only a simple human preference matters ( instead of separate Width , Depth , Updown metrics ) , we assume that a linear combination exists s Prefer ( r|c ) ? 0 ( r|c ) K w K s K ( r|c ) ( 7 ) Human calibration .
To estimate the correlation between the feedback score and human response preference , we present pairs of responses for the same context to a set of human annotators , asking them to select the response they would prefer to send or receive .
The annotation is conducted for machine - vs.- machine comparisons on 1 K pairs , and with 5 individual judges for each pair .
Through this controlled setup , we reduce confounding factors , such as social influence and disparities in visibility , that might exist even within the contrastive problem formulation .
The results are used as a proxy for s Prefer ( r|c ) , and can be used to estimate w K for the test set , though the optimal value may depend on the test set and the instructions the human annotators were given .
Note that the freedom of the system is now limited to a handful of hyper-parameters , limiting the need for large-scale human labeling to learn the model parameters .
Implementation details Model and training .
Our model is a 12 - layer transformer model based on GPT - 2 ( Radford et al. , 2019 ) architecture , and initialized with DialoGPTmedium model weights .
Di-aloGPT is a large-scale dialog response generation model , pre-trained on 147M Reddit conversations .
We use a linear layer to convert the final layer transformer output at the last token time step to a scalar For the human- like ( i.e. human-vs-fake ) task , we consider two representative negative modes : retrieval and generative dialog model generation .
For the former we simply construct negative examples by randomly sampling from the training data .
For the latter we use DialoGPT with top-k decoding .
Since DialoGPT is able to produce human-like responses in certain evaluation settings , we select only 5.3 M highly - rated human response as positive examples , instead of using all human responses .
Note that our method can be extended to include other negative modes such as perturbations and excessive repetition , similar to the synthetic example creation using BLEURT ( Sellam et al. , 2020 ) .
Baselines
We consider the following baselines : Dialog perplexity ( ppl . )
This metric is calculated for both the forward model ( i.e. , predict the response from the context ) and the reverse model ( i.e. predict the context from the response ) .
This ranking method was proposed by Li et al . ( 2016 ) and formulated to maximize mutual information ( MMI ) between the response and context .
We use DialoGPT and its reverse model to compute ppl .
BM25
This classic metric measures keywords similarity ( Robertson and Zaragoza , 2009 ) .
We use the inner product of the context BM25 vector and candidate response BM25 vector to rank candidates , similar to ( Henderson et al. , 2019a ) . ConveRT ( Henderson et al. , 2019 b ) is a transformer - based model pretrained on Reddit data .
It encodes context and candidate as vectors and compute their inner product as similarity used for ranking , achieved the existing state - of - the - art performance on several response matching test sets 2 . Bag of words ( BoW )
For each word , an average of rank - normalized feedback score 3 is calculated for replies that contain this word .
This is the score for this word .
Due to the long-tailed distribution of the absolute value of feedback items , we normalize them as the percentage ranking for their context .
Then we use the average of the scores of the words in a response as the score of this response .
Length
As shown in Figure 4 , feedback rank weakly correlates with response length .
We therefore use the average value of responses of the same length in training data as the predicted score for a hypothesis .
BoW and Length baselines are are intended to capture information about lexical patterns of hu-man feedback in the data and provide a preliminary analysis .
Results
Predicting Human Feedback Preliminary analysis
We first consider findings from the bag of words baseline .
As shown in Table 4 , responses that receive fewer replies or upvotes tend to be less contentful ( e.g. lol , awesome , wow , nice ) .
In contrast , comments that attract more feedback are typically different in character : for instance , questions ( indicated by ? , why , how , what , who ) often lead to longer conversation ( greater Depth ) .
Comments targeting a broad audience ( labeled by anyone , guys ) , tend to receive more direct replies ( greater Width ) than those aimed at a specific set of people .
A similar pattern is captured by DIALOGRPT , as shown in Table 3 . Given the context I love NLP ! , the relatively bland response
Me too !
gets the lowest scores for all three feedback measures .
Higher scores are obtained for Response B , where a justification is provided for the agreement ( useful , powerful ) .
Response C gets the highest Depth score , as it invites a discussion about how NLP works , something that is unlikely to be completed in one or two turns .
Response D , in contrast , can be answered in fewer turns but with potentially many valid answers , which explains its high Width score .
Finally , Response E receives the highest Updown score , probably because the model predicts that many people will upvote it to express gratitude for the useful resource pointer it provides ( textbook ) .
Removing the word ( URL ) from Response E causes the score to drop only slightly , indicating that the model is not simply sensitive to the post containing a web link .
Ranker evaluation
We evaluate ranker performance using two metrics .
First , we use pairwise accuracy , which measures accuracy in selecting the positive sample from a positive ( more feedback ) and negative ( less feedback ) pair for the same context .
This is consistent with the training objective .
Second , since the models will be used to rank hypotheses , we are also interested in the correlation between the model scorer rank and the the gold label rank .
We measure this correlation using Spearman 's ?.
As shown in Table 5 , DIALOGRPT shows the highest test performance on both measurements 4 Reverse dialog perplexity generally performs better than forward dialog perplexity .
However , as it is not trained with feedback labels , a simple BoW baseline outperforms the dialog models in this task .
We also evaluated performance on feedback data that the model had not been trained on , as shown in Table 6 . The model trained on Width data can perform reasonably well on Depth prediction , and vice versa , consistent with the high correlation between their labels as shown in Table 1 .
The Updown label is less correlated with these , and so the model trained on Updown data performs poorly on Width and Depth data .
This is in keeping with the complementary relationship between these models .
Human-like Classification Human-vs-Rand
We first evaluate performance on the task of selecting the gold response from a set of random distractor responses .
For each context , we randomly select n distractors .
Performance is evaluated using Hits@k , which is the ratio of the number of gold responses in the top-k ranked hypotheses .
Here , k is equal to the number of gold responses .
Although DIALOGRPT is trained solely on Human-vs - Rand Reddit data , we show in Table 7 that it performs well even when compared to baseline models on other data sources : Daily - Dialog ( Li et al. , 2017 ) and Twitter 5 PersonaChat 6 ( Zhang et al. , 2018a ) .
Such zero-shot performance indicate that the model generalize reasonably well on unseen datasets .
For the Reddit dataset , which has multiple gold replies , we also compare our method with reference - based similarity measurements , 7 including BLEU ( Papineni et al. , 2002 ) , BERTScore ( Zhang et al. , 2019a ) , and BLEURT ( Sellam et al. , 2020 ) .
These metrics are not applicable on -thefly , since references are not available , but they are commonly used as offline measures of dialog system quality .
As shown in Table 7 , although BLEU , BERTScore , and BLEURT take advantage of reference , which is unknown to DIALOGRPT , DIALO -4 Similar results are observed for the validation set .
5 https://github.com/Marsan-Ma/chat_ corpus / 6
The performance of IR Baseline , Starspace , and KV Profile Memory for Persona Chat are following Zhang et al . ( 2018a ) .
7 Following Galley et al. ( 2018 ) , for a gold hypothesis , we only use other k ?
1 gold hypotheses as references to avoid a similarity of 1 .
For each distractor response , we randomly pick k ?
1 references from k gold hypotheses .
GRPTshows higher accuracy measured by Hits@k .
Human-vs-Generated
We evaluate the model 's ability to discriminate between human and generated responses .
As shown in Table 6 , a model trained only on human- vs- rand data performs poorly on this task , indicating that the generated responses are sufficiently relevant to the context to yield a higher score than a random response .
This is consistent with the evaluation results reported by , which shows that DialoGPT receives higher relevancy score in a human evaluation .
However , the feedback prediction models , Width , Depth and Updown , show much higher accuracy in the human- vs- generated task , even though they were not trained on any generated responses .
This implies that the ranking models predict that DialoGPT 's generated responses may not be as proactive or as engaging as human responses .
Finally , the model trained with both random and generated responses perform well on both human - vs . - fake tasks , but not well on the humanvs .- human feedback ranking tasks .
This indicates that the models are complementary to each other , motivating us to build an ensemble model .
Ensembling Models Reddit test data .
The feedback and the humanlike models are combined following Eq. 7 and eval - uated using different test sets , as shown in Table 6 .
For testing on feedback K , where K is Width , Depth or Updown , we set w i = 1 if i = K and 0 otherwise .
For human vs. fake , we set w K = 1/3 for all three feedback models .
Although the ensemble model 's accuracy is not the highest for any of the test sets , it performs reasonably well on all of them .
Human overall preference .
We also test the correlation between the ensemble model and human overall preference , using the human annotations introduced in Section 3.2 .
As shown in Table 8 , adding the human- like model ?
0 improves the model performance , indicated by the comparison between the model ?
0 K w K s K and K w K s K .
Among the three feedback modes , human preference correlates best with Updown .
Presumably , Upvotes ( or " Likes " ) , is more directly tied to human preference than Width or Depth .
However , the other two metrics are useful as well .
The fitted coefficients of the K w K s K model implies the overall preference is a combination of these modes , favoring replies that can prolong a dialog session ( w Depth = 0.48 ) , that are likely to be upvoted ( w Updown = 1.0 ) and that do not target too ( Henderson et al. , 2019a ; Humeau et al. , 2019 ) encodes context and candidate as vectors and use their similarity for ranking .
Some systems ( Zhou et al. , 2018 ; employ a set of features to rank hypotheses , e.g. , local cohesion , global coherence , empathy matching , and retrieval matching .
Reference - based quality measure is also used to estimate the quality of response , although this is not applicable on - the-fly .
BLEU ( Papineni et al. , 2002 ) is a classic metric measuring the sentence similarity using ngram overlap .
BERTScore ( Zhang et al. , 2019a ) uses BERT contextualized word embeddings , instead of ngrams .
BLEURT ( Sellam et al. , 2020 ) directly measures sentencelevel similarity , initialized with BERT and then trained on millions of synthetic examples .
Contrastive Learning focuses on the relation between samples or labels .
Hadsell et al. ( 2006 ) learns representations using a contrastive loss function which pulls neighbors together and pushes apart non-neighbors in the learned space .
Gao et al. ( 2019a ) designed a loss function to reduce the distance between matched context and response in contrast to the random pairs .
Chen et al. ( 2020 ) proposed a contrastive learning framework , establishing a new state - of - the - art for image classification .
Social sciences and social-media NLP : Glenski and Weninger ( 2017 ) model each user separately and predict their interaction for a given post using features including existing upvotes / downvotes , rank , and bag of words .
Stoddard ( 2015 ) models upvotes as a time-series function of content quality , displaying position , age and score of the post and shows that popularity is positively correlated with quality , though articles of similar quality can have very different numbers of upvotes .
Lakkaraju et al. ( 2013 ) studied resubmissions to decompose article popularity into the quality of the content and the appeal of the title .
They find that textual features of the title significantly affect popularity .
Conclusion
We leverage Reddit human feedback data to build and release a large-scale training dataset for feedback prediction .
We trained GPT - 2 based models on 133 M pairs of human feedback data and demonstrate that these models outperform several standard baselines .
In particular , the conventional dialog perplexity baseline shows little predictive power on Reddit human feedback data .
We ensemble the feedback prediction models and a humanlike scoring model to rank the machine generated dialog responses .
Human evaluation shows that human preference is improved with our ranking method .
For the future work , we suggest to integrate the ranking models and generation model , e.g. , in beam search stage or reinforcement learning using ranking score as reward signal .
Figure 1 : 1 Figure 1 : For many online communities , posts and comments have a tree structure and user can upvote or downvote each node individually .
This allows us to define measures ( e.g. Width , Depth , and Updown ) of human feedback and build a large-scale training dataset for response quality prediction .
