title
Neural Dialogue State Tracking with Temporally Expressive Networks
abstract
Dialogue state tracking ( DST ) is an important part of a spoken dialogue system .
Existing DST models either ignore temporal feature dependencies across dialogue turns or fail to explicitly model temporal state dependencies in a dialogue .
In this work , we propose Temporally Expressive Networks ( TEN ) to jointly model the two types of temporal dependencies in DST .
The TEN model utilizes the power of recurrent networks and probabilistic graphical models .
Evaluating on standard datasets , TEN is demonstrated to improve the accuracy of turn- level-state prediction and the state aggregation .
Introduction Spoken dialogue systems ( SDS ) connect users and computer applications through human-machine conversations .
The users can achieve their goals , such as finding a restaurant , by interacting with a task - oriented SDS over multiple dialogue rounds or turns .
Dialogue state tracking ( DST ) is an important task in SDS and the key function is to maintain the state of the system so as to track the progress of the dialogue .
In the context of this work , a state ( or aggregated state ) is the user 's intention or interest accumulated from the conversation history , and the user 's intention or interest at each turn is referred to as turn-level state .
Many neural - network models have been successfully applied to DST .
These models usually solve the DST problem by two approaches , the Implicit Tracking and the Explicit Tracking .
As is shown in Figure 1 ( a ) , the Implicit Tracking models ( Henderson et al. , 2014 b , c ; Mrksic et al. , 2015 ; Ren et al. , 2018 ; employs recurrent networks to accumulate features extracted from historical system action and user * Corresponding author utterance pairs .
A classifier is then built upon these accumulated features for state prediction .
Although the Implicit Tracking captures temporal feature dependencies in recurrent - network cells , the state dependencies are not explicitly modeled .
Only considering temporal feature dependencies is insufficient for accurate state prediction .
This fact has been confirmed via an ablation study in our experiment .
Unlike the Implicit Tracking , the Explicit Tracking approaches , such as NBT and GLAD ( Zhong et al. , 2018 ) , model the state dependencies explicitly .
From the model structure in Figure 1 ( b ) , the Explicit Tracking approaches first build a classifier to predict the turn - level state of each turn and then utilize a state aggregator for state aggregation .
Despite achieving remarkable improvements upon the previous models , current Explicit Tracking models can be further improved in two aspects .
One is that the temporal feature dependencies should be considered in model design .
The Explicit Tracking models only extract features from the current system action and user utterance pair .
In practice , the slot-value pairs in different turns are highly dependent .
For example , if a user specifies ( FOOD , italian ) at the current turn , he or she will probably not express it again in the future turns .
For that reason , only extracting features from the current system action and user utterance pair is inadequate for turn - level state prediction .
The other is that the uncertainties in the state aggregation can be more expressively modeled .
The state- aggregation approaches in current Explicit Tracking models are sub-optimal .
The deterministic rule in GLAD will propagate errors to future turns and lead to incorrect state aggregation .
The heuristic aggregation in NBT needs further estimate the best configuration of its coefficient .
An approach that can both reduce the error propagation and require less parameter estimation is necessary for the state aggregation .
In this study , we propose a novel Temporally Expressive Networks ( TEN ) to jointly model the temporal feature dependencies and temporal state dependencies ( Figure 1 ( c ) ) .
Specifically , to improve the turn- level state prediction , we exploits hierarchical recurrent networks to capture temporal feature dependencies across dialogue turns .
Furthermore , to reduce state aggregation errors , we introduce factor graphs to formulate the state dependencies , and employ belief propagation to handle the uncertainties in state aggregation .
Evaluating on the DSTC2 , WOZ and MultiWoZ datasets , TEN is shown to improve the accuracy of the turn - level state prediction and the state aggregation .
The TEN model establishes itself as a new state - of - the - art model on the DSTC2 dataset and a state - of - the - art comparable model on the WOZ dataset .
Problem Statement
In a dialogue system , the state is represented as a set of slot-value pairs .
Let S denote the predefined set of slots .
For each slot s ?
S , let V ( s ) denote the set of all possible values associated with slot s .
We also include an additional token , unknown , as a legal value for all slots to represent their value is not determined .
And we define V * ( s ) := V ( s ) ? {unknown } V * := s?S V * ( s ) Let X denote the state space , and x ?
X be a state configuration .
Each state configuration x can be regarded as a function mapping x( s ) from S to V * .
For example , x( s ) = ? ? ? italian , s = FOOD moderate , s = PRICERANGE unknown , s = AREA ( 1 ) Let x t denotes the state configuration of the t th dialogue turn , u t denotes the user utterance of the t th turn and a t denotes the system action based on previous state x t?1 .
Let y t ?
X be the turn- level state , which is meant to capture the user intention of the current utterance .
The system computes the aggregated state x t through a deterministic procedure , according to y t and x t?1 .
We next describe this procedure .
For any given s , we define an operator on V * ( s ) as follows .
For any v , v ? V * ( s ) , v v := v , if v = unknown v , otherwise ( 2 ) We then extend the operator to any two elements x , y ?
X , where x y is also an element in X ( x y ) ( s ) := x( s ) y( s ) .
( 3 ) Using this notation , the aggregation of states is precisely according to x t = x t?1 y t . ( 4 ) For example , if x t?1 takes the configuration x in ( 1 ) and if y t is y t ( s ) = ? ? ? chinese , s = FOOD unknown , s = PRICERANGE unknown , s = AREA ( 5 )
The aggregated state x t is x t ( s ) = ? ? ? chinese , s = FOOD moderate , s = PRICERANGE unknown , s = AREA ( 6 )
The dialogue process can be characterized by a random process { ( X t , Y t , A t , U t ) : t = 1 , 2 , . . .) }.
In the DST problem , the probability measure P which defines the dialogue process is unknown .
We are however given a set R of realizations drawn from P , where each r ?
R is a dialogue , given in the form of { ( x ( r ) t , y ( r ) t , a ( r ) t , u ( r) t ) : t = 1 , 2 , . . .) }.
Let x <t denotes ( x 1 , x 2 , . . . , x t ) and assume similar notations for y <t , a < t etc .
The learning problem for DST then becomes estimating P ( x t | a <t , u <t ) for every t.
Model
This section introduces the proposed TEN model , which consists of Action - Utterance Encoder , Hierarchical Encoder , Turn-level State Predictor and State Aggregator .
X0 X1 Y1 H1 H0 Z1 A1 U1 X2 Y2 H2 Z2 A2 U2 X3 Y3 H3 Z3 A3 U3 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? Figure 2 : The probabilistic graphical model of TEN .
Model Structure
The overall model structure of TEN is shown in Figure 1 ( c ) .
we wish to express P( x t | a <t , u <t ) using a probabilistic graphical model .
For that purpose , we introduce two latent layers of random variables { H t } and { Z t } , together with { Y t } and { X t } , to form a Markov chain { ( A t , U t ) } ?
{ Z t } ?
{ H t } ?
{ Y t } ?
{ X t }. ( 7 )
Then we can express the TEN model as a probabilistic graphical model shown in Figure 2 .
In the probabilistic graphical model , the variable Z t is a matrix of size K Z ? | S | , each column of Z t ( s ) corresponds to a slot s ?
S. Obtained from ( A t , U t ) , Z t is referred to as the " action - utterance encoding " at turn t which has a dimension of K Z .
The variable H t is a matrix of size K H ? | S | , with each column H t ( s ) also corresponding to the slot s ?
S. Here the recurrent { H t } layer is used to capture temporal feature dependencies , and H t is referred to as the " hierarchical encoding " , which has a dimension of K H .
In state aggregation , we introduce the factor graphs to model the state dependencies .
The belief propagation is then employed to alleviate the error propagation .
It allows the soft-label of Y t and X t keeping modeled .
We next explain each module in detail .
Action - Utterance Encoder
This module 's function is to summarize the input system action and user utterance to a unified representation .
For later use , we first define a GRUattention encoder or abbreviated as GAE .
The GAE block first feeds an arbitrary - length sequence of word-embedding vectors ( w 1 , w 2 , ... , w n ) := w <n to a GRU encoder and obtains a hidden state vector d i at the i th time step , then weighted - combine all the hidden-state vectors using attention mechanism to construct the output vector o .
The computation process of the GAE block is d i = GRU ( d i?1 , w i ; W ) o = n i=1 exp d T i ? ? n j=1 exp d T j ? ? d i ( 8 ) Here W is the parameter of the GRU networks and ? is the learnable parameter of attention mechanism .
We simply introduce a notation GAE ( w <n ; W , ? ) to indicate the above computation process ( 8 ) of the GAE block .
Utterance Encoder .
Let w u, t <n denotes the wordembedding sequence of the t th user utterance u t .
A GAE block is then used to obtain the utterance encoder with input w u, t <n .
For each slot s ?
S , an utterance encoding u t ( s ) is computed by u t ( s ) = GAE ( w u, t <n ; W u , ? s ) ( 9 ) Note that the GAEs for different slot s share the same parameter W u , but they each have their own attention parameter ?
s . Action Encoder .
The system action at each turn may contain several phrases ( Zhong et al. , 2018 ) .
Suppose that action a t contains m phrases .
Each phrase b i t ?
a t is then taken as a word sequence , and let its word-embedding sequence be denoted as b i t .
For each i and each slot s , b i t is passed to a GAE block and the action - phrase vector c i t ( s ) is computed by c i t ( s ) = GAE ( b i t ; W a , ? s ) ( 10 )
Like utterance encoder , these | S | parallel GAE 's share the same GRU parameter W a but each has its own attention parameters ?
s .
Finally , we adopt the same approach proposed in ( Zhong et al. , 2018 ) , which combines the action- phrase vectors to a single vector by attention mechanism .
Specifically , the action encoding a t ( s ) is obtained by interacting with utterance encoding u t ( s ) , calculated as a t ( s ) = m i=1 exp u t ( s ) T ? c i t ( s ) m j=1 exp u t ( s ) T ? c j t ( s ) c i t ( s ) ( 11 ) Action -utterance Encoding .
The action- utterance encoding z t ( s ) is simply the concatenation of vectors u t ( s ) and a t ( s ) .
Hierarchical Encoder Instead of only utilizing the current actionutterance encoding for turn - level state prediction , in this module , we introduce the hierarchical recurrent networks to model the temporal feature dependencies across turns .
Specifically , upon the GAE blocks , we use | S | parallel GRU networks to obtain the hierarchical encoding {h t } from all the historical action -utterance encoding vectors .
The hierarchical encoding for each slot s is computed by h t ( s ) = GRU ( h t?1 ( s ) , z t ( s ) ; W h ) ( 12 ) where the parameter W h of these GRU networks , is shared across all slots .
Turn-level State Predictor
The Turn-level State Predictor is simply implemented by | S| softmax - classifiers , each for a slot s according to P ( y t ( s ) | a < t ( s ) , u <t ( s ) ) := smax ?
T s h t ( s ) ( 13 ) where smax denote the softmax function and ?
s with size K h ? | V * ( s ) | serves as the weight matrix of the classifiers .
We will denote this predictive distribution for turn - level state y t ( s ) computed by ( 13 ) as ?
s t .
State Aggregator
One of the insights in this work is that when a hard decision is made on the soft-label , the errors it creates may propagate to future turns , resulting in errors in future state aggregation .
We insist that the soft-label of Y t and X t should be maintained so that the uncertainties in state aggregation can be kept in modeling .
Thus we propose a state aggregator based on the factor graphs and handle these uncertainties using belief propagation .
Factor Graphs .
For utilizing the factor graphs in state aggregation , we first introduce an indicator function , denoted by g , according to the deterministic aggregation rule .
Specifically , for any v , v , v ? V * ( s ) , g( v , v , v ) := 1 , if v v = v 0 , otherwise ( 14 )
According to the probabilistic graphical model expressed in Figure 2 , it can be derived that P ( xt | a <t , u<t ) = x < t?1 y <t s?S ? s t ( yt ( s ) ) t ? = 1 g( x?1 ( s ) , y? ( s ) , x ? ( s ) ) = s?S x < t?1 ( s ) y <t ( s ) ? s t ( yt ( s ) ) t ? = 1 g( x?1 ( s ) , y? ( s ) , x ? ( s ) ) G( x <t ( s ) , y <t ( s ) ) Q s t ( x t ( s ) ) where the term Q s t ( x t ( s ) ) above is precisely P(x t ( s ) | a <t , u <t ) , a distribution on V * ( s ) .
It turns out that the term G ( x < t ( s ) , y <t ( s ) ) in the double summation of Q s t ( x t ( s ) ) , despite its complexity , can be expressed elegantly using a factor graph in Figure 3 . Belief Propagation .
Factor graphs are powered by a highly efficient algorithm , called the belief propagation or the sum-product algorithm , for computing the marginal distribution .
In particular , the algorithm executes by passing " messages " along the edges of the factor graph and the sent message is computed from all incoming messages on its " upstream " .
For a detailed description of message computation rules in belief propagation , the reader is referred to ( Kschischang et al. , 2001 ) . x0 ( s ) g y1 ( s ) ? s 1 x1 ( s ) g y2 ( s ) ? s 2 x2 ( s ) ? ? ? ? ? ? ? ? ? ? s 1 ? s 1 ? s 1 ? s 1 ? s 2 ? s 2 ? s Applying the principle of belief propagation , one can also efficiently express Q s t at each turn t for each slot s in terms of message passing .
We now describe this precisely .
Let T denote the total number of turns of the dialogue .
For each slot s , a factor graph representation G( x < T ( s ) , y < T ( s ) ) can be constructed .
For each t = 1 , . . . , T , let messages ?
s t , ?
s t and ?
s t be introduced on the edges of the factor graph as shown in Figure 3 and the computation of these messages are given below .
? ? ? ? ? ? s t := ? s t ? s t := ? s t?1 ? s t ( v ) : = ( v , v ) ?V * ( s ) ? V * ( s ) g( v , v , v ) ?
s t ( v ) ?
s t ( v ) ( 15 ) where ?
s 0 is defined by ? s 0 ( v) = 1 , if v = unknown 0 , otherwise .
According to message computation rule given in ( 15 ) , for each t ?
T and each slot s ?
S , ? s t = Q s t .
Recalling that Q s t is the predictive distribution for state x t ( s ) and ?
s t is the predictive distribution for turn - level state y t ( s ) , we have completed specifying how the factor graphs and the belief propagation are utilized for state aggregation .
Loss Function and Training Under the TEN model , the cross-entropy loss on the training set R follows the standard definition as below L TEN := r?R s?S T ( r) t=1 ? log Q s t ( x ( r ) t ( s ) ) ( 16 ) where the superscript " ( r ) " indexes a training dialogue in R .
It is worth noting that this loss function , involving the message computation rules , can be directly optimized by the stochastic gradient descent ( SGD ) method .
For ablation studies , we next present three ablated versions of the TEN model .
TEN -Y Model
In this model , we discard the { Y t } layer of TEN ( hence the name TEN - Y ) and conduct state aggregation using RNNs .
The model then turns to be an Implicit Tracking model .
The state distribution P(x t ( s ) | a <t , u <t ) is computed directly by the softmax - classifiers in ( 13 ) .
We will denote the state distribution computed this way by Q s t .
The cross-entropy loss is then defined as L TEN?Y := r?R s?S T ( r) t=1 ? log Q s t ( x ( r ) t ( s ) ) ( 17 ) TEN -X Model
In this model , instead of training against the state sequence { x t } , the training target is taken as the corresponding turn - level state sequence {y t }.
The computation of { x t } can be done through the operator : x t = x t?1 y t .
When using the turn- level state as training target , one discards the { X t } layer of TEN ( hence the name TEN - X ) .
The difference between TEN -X and TEN is that TEN -X aggregate states using the deterministic rule while TEN using the factor graphs .
The cross-entropy loss for TEN -X is naturally defined as L TEN?X := r?R s?S T ( r) t=1 ? log ? s t ( y ( r ) t ( s ) ) ( 18 ) TEN - XH Model
In this model , the Hierarchical Encoder layer { H t } is removed from TEN -X , and the model is reduced to an Explicit Tracking mode .
In this case , the computation of ? s t ( or P(y t ( s ) | a <t , u <t ) ) in ( 13 ) is done by replacing the input h t ( s ) with the action- utterance encoding z t ( s ) .
We will denote the ?
s t computed this way by ?
s t .
The TEN -XH and TEN -X models are different in whether the temporal feature dependencies are considered or not .
The cross-entropy loss for TEN - XH is L TEN?XH := r?R s?S T ( r) t=1 ? log ? s t ( y ( r ) t ( s ) ) ( 19 ) 4 Experiment
Datasets
The second Dialogue State Tracking Challenge dataset ( DSTC2 ) ( Henderson et al. , 2014a ) , the second version of the Wizard - of - Oz dataset ( WOZ ) ( Rojas - Barahona et al. , 2017 ) and MultiDomain Wizard - of - Oz dataset ( Multi- WOZ ) are used to evaluate the models .
Both the DSTC2 and WOZ datasets contain conversations between users and task - oriented dialogue systems about finding suitable restaurants around Cambridge .
The DSTC2 and WOZ datasets share the same ontology , which contain three informable slots : FOOD , AREA , PRICERANGE .
The official DSTC2 dataset contains some spelling errors in the user utterances , as is pointed out in .
Thus we use the manually corrected version provided by
Evaluation Metrics and Compared Models
In this work , we focus on the standard evaluation metrics , joint goal accuracy , which is described in ( Henderson et al. , 2014a ) .
The joint goal accuracy is the proportion of dialogue turns whose states are correctly predicted .
In addition , we also report the turn- level state accuracy of TEN - XH and TEN -X model for ablation studies .
The models used for comparison include NBT - DNN , NBT -CNN ( Mrksic et al. , 2017 ) , Scalable ( Rastogi et al. , 2017 ) , MemN2N ( Liu and Perez , 2017 ) , PtrNet ( Xu and Hu , 2018 ) , LargeScale ( Ramadan et al. , 2018 ) , GLAD ( Ramadan et al. , 2018 ) , GCE ( Nouri and Hosseini- Asl , 2018 ) , StateNetPSI ( Ren et al. , 2018 ) , SUMBT , HyST ( Goel et al. , 2019 ) , DSTRead+JST
( Gao et al. , 2019 ) , TRADE ( Wu et al. , 2019 ) , COMER ( Ren et al. , 2019 ) , DSTQA ( Zhou and Small , 2019 ) , MERET ( Huang et al. , 2020 ) and SST ( Chen et al. , 2020 ) .
Implementation
The proposed models are implemented using the Pytorch framework .
The code and data are released on the Github page 1 .
The word embedding is the concatenation of the pre-trained GloVe embeddings ( Pennington et al. , 2014 ) and the character n-gram embeddings ( Hashimoto et al. , 2017 ) .
We tune the hyper-parameters by grid search on the validation set .
The GAE block is implemented with bi-directional GRUs , and the hidden state dimension of the GAE is 50 .
The hidden state dimension of the GRU used in the Hierarchical Encoder module is 50 .
The fixed learning rate is 0.001 .
The Adam optimizer ( Kingma and Ba , 2015 ) with the default setting is used to optimize the models .
It is worth mentioning that the TEN model can be difficult to train with SGD from a cold start .
This is arguably due to the " hard " g function .
That is , the { 0 , 1 } - valued nature of g is expected to result in sharp barriers in the loss landscape , preventing gradient - based optimization to cross .
Thus when training TEN , we start with the parameters obtained from a pre-trained TEN -X model .
Evaluation Results
The joint goal accuracy results on the DSTC2 , WOZ and MultiWOZ datasets are shown in Table 1 .
From the table , we observe that the proposed TEN model outperforms previous models on both DSTC2 and WOZ datasets , except SUMBT , a model boosted with pre-trained BERT ( Devlin et al. , 2019 ) model .
It is worth noting that TEN , built upon attention - based GRU encoders , achieves comparable performance with SUMBT , without incorporating pre-trained language models .
This fact demonstrates that TEN is a strong model for DST .
Comparing to TEN - XH , the TEN -X model obtains impressive 2.7 % , 0.5 % and 4.3 % performance gains on the DSTC2 , WOZ and Multi-WOZ dataset respectively .
These performance gains demonstrate that the state estimation benefits from more accurate turn -level state prediction .
The TEN model further improves upon the TEN -X model by 1.1 % on the DSTC2 dataset , 1.5 % on the WOZ dataset and 0.3 % on the MultiWOZ dataset .
The TEN model achieves these improvements by modeling uncertainties with the belief propagation in the state aggregation .
Although both TEN -Y and TEN have modeled the temporal feature dependencies , TEN - Y performs much worse than TEN .
This fact indicates that only considering temporal feature dependencies is inadequate for DST .
Models relying on pre-defined ontologies ( including GLAD , GCE , SUMBT and TEN ) suffer from computational complexity when applying to multi-domain DST datasets with a large set of slots ( Ren et al. , 2019 ) , which leads to worse performance than recent generation - based models ( DSTRead + JST , TRADE , DSTQA , MERET and SST , specially designed for multi-domain DST ) on the MultiWOZ dataset .
Temporal Analysis
To analyze how the temporal dependencies influence the state tracking performance , we report the joint goal accuracy at each dialogue turn on the DSTC2 dataset .
As shown in Figure 4 , the joint goal accuracy of proposed models generally decrease at earlier turns and increase at later turns , as the turns increase .
This phenomenon can be explained by the fact : in the earlier stage of the dialogue , more slots are involved in the conversation as the dialogue progress ; thus more slot-value pairs need to be predicted in state estimation , making the state harder to calculate correctly ; in the later stage of dialogue , the state becomes fixed because the values for all slots are already determined , making the state easier to predict .
Another observation is that the gaps between TEN - XH and TEN generally increase as the turns increase , showing that modeling temporal dependencies reduces state estimation errors , especially when the dialogue is long .
By modeling temporal feature dependencies and temporal state dependencies respectively , TEN -Y and TEN -X also perform better than TEN - XH as the turns increase .
Effectiveness of the Hierarchical Encoder
To prove the effectiveness of the Hierarchical Encoder module , we report the turn- level state accuracy for TEN - XH and TEN -X on the DSTC2 dataset .
From the results in Figure 5 , we observe that TEN -X , with the Hierarchical Encoder module , achieves higher turn-level state accuracy than Table 2 : An example of dialogue state tracking .
We only report the results from turn 1 to turn 4 on slot s = FOOD and focus on dontcare ( dcr ) and unknown( unk ) value due to space limitation .
S and U represent the system utterance and the user utterance , respectively .
The boldface emphasizes the highest - probability value .
TEN - XH for all slots .
Recall that TEN -X achieves higher joint goal accuracy than TEN - XH , we could think that the performance gain for TEN - X is due to its improvement in turn - level state prediction .
This fact demonstrates the significance of considering temporal feature dependencies in turn-level state prediction and illustrates the effectiveness of the Hierarchical Encoder module in TEN -X .
Effectiveness of the Belief Propagation Table 2 is an example of dialogue state tracking selected from the test set of the DSTC2 dataset .
As we observe from the table , at turn 1 and turn 2 , the user does not specify any food type ; both TEN -X and TEN correctly predict the true value unknown .
At turn 3 , the user expresses that he or she does not care about the food type .
This time the turn- level state predictor gets an incorrect turn - level state value unknown , instead of the correct one dontcare .
Thus TEN -X gets a wrongly aggregated state value unknown with aggregating rule .
On the contrary , TEN can still correctly obtain the correct state with the belief propagation , in spite of the wrong turn - level state .
At turn 4 , the turn- level state predictor easily predicts the correct value unknown and TEN keeps the state correct .
But TEN -X fails to obtain the correct state again because of the wrong decision made at the last turn .
This example shows the effectiveness and robustness of the state aggregation approach equipped with the belief propagation .
Related Works Traditional works deal with the DST task using Spoken Language Understanding ( SLU ) , including ( Thomson and Young , 2010 ; Wang and Lemon , 2013 ; Liu and Perez , 2017 ; Jang et al. , 2016 ; Shi et al. , 2016 ; Vodol ?n et al. , 2017 ) .
Joint modeling of SLU and DST ( Henderson et al. , 2014c ; Zilka and Jurc?cek , 2015 ; Mrksic et al. , 2015 ) has also been presented and shown to outperform the separate SLU models .
Models like ( Sun et al. , 2014 ; Yu et al. , 2015 ) incorporate statistical semantic parser for modeling the dialogue context .
These models rely on hand-crafted features or delexicalisation strategies and are difficult to scale to realistic applications .
Recently , neural network models have been applied in the DST task , and there are mainly two model design approaches .
One approach aggregates the features extracted from previous turns of the dialogue using recurrent neural networks , including StateNet ( Ren et al. , 2018 ) , LargeScale ( Ramadan et al. , 2018 ) and SUMBT .
The other approach , like NBT and GLAD ( Zhong et al. , 2018 ) , build a model for predicting turn - level state , and estimate the state by accumulating all previous turn -level states .
The design of TEN integrates the advantages of both approaches .
Another topic related to our work is the Markov decision process ( MDP ) and the factor graphs .
Several works define a dialogue system as a partially observable Markov decision process ( POMDP ) , including ( Williams and Young , 2007 ; Thomson and Young , 2010 ; Gasic and Young , 2011 ; Yu et al. , 2015 ) .
In this paper , the definition of the dialogue process is related to the Markov decision process .
The factor graphs have been applied in many applications , such as social influence analysis ( Tang et al. , 2009 ) , knowledge base alignment ( Wang et al. , 2012 ) , entity linking ( Ran et al. , 2018 ) and visual dialog generation ( Schwartz et al. , 2019 ) .
The factor graphs in these applications are used to integrate different sources of features or repre-sentations into a unified probabilistic model .
In this paper , the factor graphs are naturally adopted to tackle the error propagation problem in state aggregation .
Concluding Remarks
Our inspiration for TEN comes from a careful study of the dialogue process .
This allows us to lay out the dependency structure of the network as in Figure 1 ( c ) , where the temporal feature dependencies and the temporal state dependencies are jointly modelled .
The application of the belief propagation in this model allows an elegant combination of graphical models with deep neural networks .
The proposed model may generalize to other sequence prediction tasks .
Figure 1 : 1 Figure 1 : The model structures of Implicit Tracking , Explicit Tracking and Joint model . ( a , u ) : the system action and user utterance .
z : features extracted from the ( a , u ) pair .
h:the hidden state of RNNs .
y : the turn- level state .
x : the aggregated state .
FE : Feature Extractor , such as CNNs , RNNs .
RC : Recurrent Cell , such as LSTM , GRU .
CL : Classifier .
SA : State Aggregator .
The dotted arrowed lines emphasize modeling temporal feature dependencies .
The dashed arrowed lines emphasize modeling temporal state dependencies .
Figure 3 : 3 Figure 3 : The factor graph for G ( x < t ( s ) , y <t ( s ) ) .
Figure 4 : 4 Figure 4 : Temporal analysis on the DSTC2 dataset .
Figure5 : turn-level state accuracy for TEN - XH and TEN -X on the DSTC2 dataset .
The PRICE indicates the PRICERANGE slot .
The all denotes the proportion of dialogue turns that the turn - level states for all slots are correctly predicted .
Tracking , Explicit Tracking and Joint model . ( a , u ) : the system action and user utterance .
z : features extracted from the ( a , u ) pair .
h:the hidden state of RNNs .
y : the turn- level state .
x : the aggregated state .
FE : Feature Extractor , such as CNNs , RNNs .
RC : Recurrent Cell , such as LSTM , GRU .
CL : Classifier .
SA : State Aggregator .
The dotted arrowed lines emphasize modeling temporal feature dependencies .
The dashed arrowed lines emphasize modeling temporal state dependencies .
.
This dataset consists of 3 , 235 dialogues with 25 , 501 turns .
There are 1 , 612 dialogues for training , 506 dialogues for validation and 1 , 117 dialogues for testing .
The average turns per dialogue is 14.49 .
In the WOZ dataset , there are 1 , 200 dialogues with 5 , 012 turns .
The number of dialogues used for training , validation and testing are 600 , 200 and 400 respectively .
The average turns per dialogue is 4 .
The MultiWOZ dataset is a large multi-domain dialogue state tracking dataset with 30 slots , collected from human-human conversations .
The training set contains 8 , 438 dialogues with 115 , 424 turns .
There are respectively 1 , 000 dialogues in validation and test set .
The average turns per dialogue is 13.68 .
Table 1 : 1 Joint goal accuracy on the DSTC2 , WOZ and MultiWOZ dataset .
Model DSTC2 WOZ MultiWOZ NBT -DNN 72.6 84.4 - NBT -CNN 73.4 84.2 - Scalable 70.3 - - MemN2N 74.0 - - PtrNet 72.1 - - LargeScale - 85.5 25.8 GLAD 74.5 88.1 35.6 GCE - 88.5 35.6 StateNetPSI 75.5 88.9 - SUMBT - 91.0 42.4 HyST - - 44.2 DSTRead+JST - - 47.3 TRADE - - 48.6 COMER - 88.6 45.7 DSTQA - - 51.4 MERET - - 50.9 SST - - 51.2 TEN -XH 73.5 88.8 42.0 TEN -Y 74.7 89.6 45.9 TEN -X 76.2 89.3 46.3 TEN 77.3 90.8 46.6
https://github.com/BDBC-KG-NLP/TEN EMNLP2020
