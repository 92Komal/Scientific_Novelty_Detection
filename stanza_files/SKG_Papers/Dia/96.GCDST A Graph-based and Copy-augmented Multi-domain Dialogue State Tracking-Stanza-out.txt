title
GCDST : A Graph-based and Copy-augmented Multi-domain Dialogue State Tracking
abstract
As an essential component of task - oriented dialogue systems , Dialogue State Tracking ( DST ) takes charge of estimating user intentions and requests in dialogue contexts and extracting substantial goals ( states ) from user utterances to help the downstream modules to determine the next actions of dialogue systems .
For practical usages , a major challenge to constructing a robust DST model is to process a conversation with multi-domain states .
However , most existing approaches trained DST on a single domain independently , ignoring the information across domains .
To tackle the multi-domain DST task , we first construct a dialogue state graph to transfer structured features among related domain-slot pairs across domains .
Then , we encode the graph information of dialogue states by graph convolutional networks and utilize a hard copy mechanism to directly copy historical states from the previous conversation .
Experimental results show that our model improves the performances of the multi-domain DST baseline ( TRADE ) with the absolute joint accuracy of 2.0 % and 1.0 % on the MultiWOZ 2.0 and 2.1 dialogue datasets , respectively .
Introduction
A task- oriented dialogue system provides fundamental technologies for continuous interactions with a human to accomplish predefined specific goals , such as taxi reservation or hotel booking .
Dialogue State Tracking ( DST ) is a crucial component in the task - oriented dialogue system .
Users ' intentions and goals are extracted from the current utterances and the conversation history .
Then , the DST model encodes the information as a set of states to help dialogue systems to determine which actions should be taken in next steps ( Young and Thomson , 2013 ) .
A dialogue state generally comprises an entity Usr : I 'm looking for an expensive restaurant in the centre of town .
Sys :
What about the Cambridge chop house ?
A British restaurant located in the centre of town .
Usr : I need to book a table for four people at 16:45 on Friday .
Sys : Booking was successful .
Reference number is 10p0 levh .
Anything else today ?
Usr : I 'm also looking for a place to stay .
Ideally a hotel with free WIFI that is also expensive .
Sys :
There is the Gonville hotel .
It has internet and is rated 3 stars .
Would you like to book ?
Usr : Great , can you book it for two people , for four nights starting Friday ?
Sys : Your booking was successful starting Friday for four nights .
Reference number is drw9 .
Usr : I also need a taxi to this hotel and leave at 21:45 .
Sys : Ok , what will your departure be ?
Usr : St. John's College Sys : Okay , I have booked a taxi for you , the number is 07240037071 .
Each tuple denotes a slot-value pair , and the lines between them represent that they have the same slot or the same value .
attribute ( slot ) and its corresponding value of a specific domain .
For example , there might be a slot-value pair ( book - day , Friday ) in the domain of restaurant .
In general , the dialogue states in DST are predefined by a single domain ontology .
However , as a real conversation is inherently complex and across multiple domains , modeling multidomain DST is of great practical application value in real-life situations .
As shown in Figure 1 , the conversation includes three domains ( restaurant , hotel , and taxi ) , in which some dialogue states and their expressions , such as the states connected with lines , are similar .
This paper focuses on multidomain DST .
To extract dialogue states from a conversation , there are generally two kinds of approaches .
One is utilizing delexicalization to get rephrasings of states by a semantic dictionary ( Zilka and Jurci-cek , 2015 ; Rastogi et al. , 2017 ) .
The other kind of DST models is based on neural networks , which uses word embeddings instead of delexicalization .
However , these approaches lack the capability of sharing and transferring information across domains , which causes low scalability in multi-domain settings .
Recently , proposed a generative multi-domain DST model ( TRADE ) based on a copy mechanism , which transfers state representations by sharing the parameters across domains .
Beyond that , one challenge is that , is there a more straightforward and explicit approach to encode the states between domains and to further improve the performance of multi-domain DST ?
Besides , a conversation often piles up long contexts 1 . Previous multi-domain DST systems often behave defectively in predicting dialogue states with such long contexts at the current turn , which shows another challenge of the multi-domain DST task .
To address the above issues , we come up with a more scalable multi-domain DST model .
In particular , to better represent the relationships between dialogue states , we first construct a state graph for each conversation .
Then , we introduce Graph Convolutional Networks ( GCN ) ( Kipf and Welling , 2017 ) to better encode the structured information into the representations of history state nodes .
For each node , GCN recursively aggregates neighbour information over the dialogue state graph via efficient graph convolution operations , then extracts state-centric representations to benefit the feature transferred across domains .
In addition , to avoid too much noise when generating states from longterm contexts , we utilize the previous states from dialogue history and propose a hard copy mechanism for the decoder to pick up the history states directly .
To verify the proposed approach , we combine it into an effective multi-domain DST framework .
The experiments are carried out on the Multi-WOZ 2.0 / 2.1 dialogue corpus Eric et al. , 2019 ) .
The results show that the proposed multi-domain DST approach improves 2.0 % / 1.0 % of joint accuracy over the baseline .
We also analyze our model from different perspectives to show the effectiveness of our approach .
The paper proceeds as follows .
First , we introduce the state graph- based multi-domain DST 1 65 % of conversations in MultiWOZ 2.0 are over 5 turns .
model ( ?2 ) .
Next , we describe the experimental results and analyze the effects of different settings and the case study ( ?3 ) .
Finally , we discuss the related work ( ?4 ) and conclude the study ( ?5 ) .
Method Figure 2 illustrates the encoder-decoder framework of our Graph- based and Copy-augmented multidomain Dialogue State Tracker ( GCDST ) .
Different from the previous work , we introduce state graph representations into both the encoder and the decoder to model the associated information between dialogue states across domains .
In addition , we propose a hard copy mechanism in dialogue decoder to get the history states from the last prediction .
The framework consists of four main components .
?
State graph representation extracts the graphstructured information of dialogue states in a conversation and provides the node representations using graph embeddings .
?
Dialogue encoder models history utterances and states of previous turns into a sequence of fixed - length vectors .
?
Dialogue decoder with copy mechanism predicts the current slot value by the historical states of the last turn .
Such a mechanism helps to decode a sequence of tokens from all possible domain-slot candidates effectively .
?
Slot gate , similar to the previous work , predicts ptr , none , and dontcare to filter some unrelated states .
State Graph Representation
A practical conversation usually contains dialogue states in more than one domain .
Different domains often have lots of same slots that might share the same values or have similar expressions and linguistic features .
As shown in Figure 1 , when a user books a restaurant , a hotel , and a taxi simultaneously in the conversation , the slot price- range exists in the restaurant domain and the hotel domain respectively .
Moreover , for the state expression , the value of hotel - name might be as same as that of taxi- destination , which means that after booking a hotel , the user will book a taxi to the hotel .
Thus , representing and transferring features between the same slots across domains or different domain-slot pairs that have the same values are imperative .
Graph Construction
The prior work mainly tracks slot information across domains by sharing parameters ( Zhong et al. , 2018 ; .
However , it is difficult to transfer the information between slots explicitly and directly .
Therefore , we come up with a graph structure to represent the relationship between dialogue states in a conversation .
Based on the graph , features learned from the states in one domain are able to directly transfer to other domains .
Formally , a dialogue state graph is denoted as G = { N , E} , where N = { ( d , s ) } stands for domain-slot tuple nodes and E represents undirected edges between nodes .
Considering two nodes N i = ( d i , s i ) and N j = ( d j , s j ) , we explore four ways of constructing of the edge adjacency matrix A : ? Domain-connection : if d i = d j , A ij = 1 ; ? Slot-connection : if s i = s j , A ij = 1 ; ? Value-connection 2 : ?v i , v j : if v i = v j , A ij = 1 where v i is one value of N i ; ?
Slot / value-connection : union of slot and value connection .
Graph Encoding
To propagate information among dialogue state nodes over the graph , we introduce the Graph Convolutional Networks ( GCN ) ( Kipf and Welling , 2017 ) to update structure - aware node representations by pooling features of their adjacent nodes .
In general , the input of GCN includes 1 ) the node embedding matrix H ? R | V |?d , where | V | is the number of nodes and d denotes the dimension of node embedding , and 2 ) the ad- jacency matrix A ? R |V |?|V | , where A ij = 1 if there is an edge between the node N i and the node N j , which represents the dialogue state graph structure .
In the dialogue state graph , the information propagation among nodes takes up at most two hops away .
Thus , we consider a two -layers GCN , in which every layer can be written as a non-linear function and a symmetric adjacency matrix : H 0 = I , H l+1 = ?( ?H l W l + b l ) , ( 1 ) where H l is the input node embedding matrix , H l+1 is the output node embedding matrix , and W l and b l are a parameter matrix and a bias vector for the l-th GCN layer , respectively . ?(? ) is a nonlinear activation function ( we use the ReLU ( ? ) in this paper ) .
Finally , we can obtain a | V | ? d nodelevel feature matrix E node = H l +1 .
In addition , the adjacency matrix A often adds self-loops to each node in the graph .
? = A + ?I , ( 2 ) where I is a | V | ?
| V | identity matrix .
As suggested in Kipf and Welling ( 2017 ) , we introduce the trade- off parameter ? , as the importance of self and neighboring node connections might be not equal .
Through the self-loop , the representation of each node can be affected by itself .
Dialogue Encoder Previous works ( Zhong et al. , 2018 ; only exploited utterances to encode the dialogue history .
However , the foregoing dialogue states are informative and related to the current state .
For instance , when a user inquiries the area or the number of people for a hotel , she is quite likely to have similar inquiries for other domains such as the restaurant in the following conversation .
Thus we propose an utterance encoder and a state encoder to encode history utterances and states respectively , by utilizing bi-directional gated recurrent units ( GRU ) ( Chung et al. , 2014 ) .
Specifically , the input of utterance encoder is the word sequence of history utterance {w 1 , w 2 , ... , w U } , where w i is the ith token of the sequence of the user utterances and the system responses of previous turns .
On the other hand , the input of state encoder is denoted as { ( d 1 s 1 , v 1 ) , ... , ( d M s M , v M ) } , where M is the max number of history state , and d j s j is the jth domain-slot pair .
For each domain slot pair , we use graph embedding to encode it .
In a few cases , the value of a domain-slot pair is a phrase that contains more than one word 3 .
For simplicity , we encode the value v j only according to its first word by a shared word embedding of the utterance encoder 4 . Finally , we concatenate d j s j and v j as the input representation and feed it into the state encoder .
During testing , we only use predicted state as input of state encoder , although there might be some errors in the predicted states .
In order to simulate this situation , we randomly replace , add , and delete some history states in the training step .
Specifically , for replacing or adding operation , only the states that have the same domain , slot , or value are selected as the candidates .
Dialogue decoder with copy mechanism
To predict the current state of a conversation , both the historical utterances and states can be taken into account .
Previous work applies a copy mechanism to copy the words from historical utterances , but as the dialogue goes on , the context will become longer .
In this case , RNN might lose much information of the states extracted from the first few turns .
To address the issue , we first propose a hard copy mechanism to copy the value from the previous state directly , because the history state as a summary of context is important for the current prediction .
Then we use a soft-gate to combine the probability based on vocabulary , utterances , and states .
In particular , we use a GRU to decode the value of each domain-slot pair and apply the node embedding E node ( d k s k ) to represent each dialogue state candidate .
When decoding the t-th word of d k s k , the GRU takes a word embedding from the previous step w t?1 , k as input .
The hidden state of GRU is denoted as h t , k .
For the first word we use h 0 , k = h enc u + h enc s and w 0 , k = E node ( d k s k ) to initialize its previous hidden state and word embedding , where h enc u and h enc s are the last hidden states of the utterance encoder and the state encoder , respectively .
The distributions over vocabulary and historical utterance are calculated by p vocab t , k = Softmax ( W 1 ? ( h t , k ) T ) p utter t , k = Softmax ( H utter ? ( h t , k ) T ) ( 3 ) where W 1 is a mapping matrix from hidden state size to vocabulary size and H utter is the history state from the dialogue utterance encoder .
As there might be many unchanged states in each dialogue turn , we try to refer to the history states predicted previously .
Thus , we explore two kinds of methods , a hard copy mechanism ( Eq . ( 4 ) ) and an attention - based method ( Eq . ( 5 ) ) , to get the distribution over the dialogue history state .
While the hard copy mechanism will generate a one- hot vector , the output of attention - based method is a distribution over the vocabulary , as below p state t , k = One-hot ( state t , k ) , ( 4 ) p state t , k = Softmax ( W 2 ? [ h enc u ; h dec t , k ; h enc s ] ) , ( 5 ) where state t , k is the t-th word of the domain-slot pair d k s k at the last turn .
If state t , k is not exist , we fill it by padding .
W 2 is a mapping matrix for training .
The final output distribution is a weighted sum of the mentioned three distributions .
Table 1 : Statistics on MultiWOZ 2.0 and 2.1 .
Note that one turn consists of one user utterance and its corresponding system response , which is different from the previous works . p t , k = ( 1 ? ? ) ? [? ? p utter t , k + ( 1 ? ? ) ? p vocab t , k ] + ? ? p state t , k ( 6 ) The parameters ? and ? are trainable gates , computed by ? = Sigmoid( W 3 ? [ h t , k ; w t , k ; c uttr t , k ; c state t , k ] ) , ? = Sigmoid( W 4 ? [ h t , k ; w t , k ; c uttr t , k ; c state t , k ] ) , c uttr t , k = p uttr t , k ?
H uttr , c state t , k = Softmax ( H state ? h t , k ) ?
H state , ( 7 ) where W 3 and W 4 are trainable matrices , and c uttr t , k and c state t , k are context vectors of utterances and states , respectively .
By Eq. ( 6 ) , we are able to copy the states from p state t , k directly .
Slot Gate Similar with G k = Sof tmax ( W 5 ? [ c uttr 1 , k ; c state 1 , k ] ) ( 8 ) where W 5 is a trainable matrix , c uttr 1 , k and c state 1 , k are the context vectors computed by Eq. ( 7 ) .
Optimization
During training , we optimize the sum of crossentropy loss L v of the decoder and L g of the slot gate , We choose the best model on the development sets and evaluate the performances on the test sets of both MultiWOZ 2.0 and MultiWOZ 2.1 .
Figure 3 demonstrates the distributions of numbers of dialogues and turns of the five domains on the training set , the development set , and the test set , respectively .
Note that the total amount of dialogues in all five domains is larger than that in Table 1 because a dialogue often spans over multiple domains in practice .
L = L g + L v . ( 9 Metric
To evaluate the multi-domain DST models , we employ joint goal accuracy as the evaluation metric .
Joint accuracy assesses the predictive capability of the DST model on turn-level .
A result is correct only if all of the predicted values exactly match the ground truth in a dialogue turn .
This evaluation metric measures the capability of identifying the completed user goals on multiple domains in a turn , which is of paramount importance for multi-domain DST assessment .
Hyper-parameters
The word embeddings are initialized with 400 - dimensional pre-trained em-beddings which concatenated the Glove embeddings ( Pennington et al. , 2014 ) and the character n-gram embeddings ( Hashimoto et al. , 2017 ) .
For the two -layer graph convolutional networks , the dimension of the hidden units for the first layer is set to 512 , and the dimension of the node embeddings is set to 400 .
We initialize the input adjacency matrix A by row-normalization .
We use the node embeddings to convert dialogue states into 400 - dimensional vector representations .
During training , we set the dropout with 0.2 ratio .
The ? in Eq. ( 2 ) is set to 2 .
The model is trained by using the Adam optimizer ( Kingma and Ba , 2015 ) with a batch size of 32 .
We apply early stopping based on the joint goal accuracy .
In this paper , GCDST refers to the model proposed at Section 2 with slotconnection and hard copy mechanism if not clearly stated .
Baselines
We compare with the following models for multi-domain DST .
? MDBT : It leverages semantic interactions between dialogue utterances and ontology terms to learn the shared representations between slots across domains ( Ramadan et al. , 2018 ) .
? GLAD : By utilizing system actions and user utterances , this model builds global modules to share parameters among slot-value pairs and local modules to learn slot-specific features ( Zhong et al. , 2018 ) . ? GCE : Based on GLAD , this model replaces the slot-dependent RNN with a global conditioning encoder .
It is the state- of- the - art model of single- domain DST ( Nouri and Hosseiniasl , 2018 ) . ?
SpanPtr :
This model uses pointer networks to generate both start and end positions to perform index - based copying ( Xu and Hu , 2018 ) . ? TRADE : This model utilizes a copy mechanism that shares parameters across domains , to generate dialogue states from user utterances .
Experimental Results
We compare our GCDST with the previous work in outperforming the baseline ( TRADE ) with absolute improvements about 2 % on MultiWOZ 2.0 and 1 % on MultiWOZ 2.1 , respectively .
Different from existing multi-domain DST models , we do not use complex decoding algorithms ( GLAD ) and parameter - sharing mechanism ( TRADE ) .
We attribute the performance improvements to the straightforward graph structures , by which it could represent and transfer information among dialogue state nodes via GCN effectively .
Moreover , the hard copy mechanism copies the values from previous predicted states directly , which maintains the consistency of the predicted states .
The results demonstrate the effectiveness of GCDST on capturing information on multiple domain-slot pairs from dialogues and utilizing the states from historical turns .
Analysis and Discussion Effects of edge connection In Section 2.1 , we propose four types of edge connection for state graph construction , including slot-connection , value-connection , slot / value-connection , and domain-connection .
As shown in Table 3 , GCDST with slot-connection achieves the best performance on both MultiWOZ 2.0 and 2.1 .
In addition , the other two connection types by value ( valueconnection and slot / value-connection ) achieve comparative performances .
We argue that similar contextualized representations exist between dialogue states that have the same slot or value .
For instance , in Figure 1 , the states restaurant - book people and hotel - book people have the same slot book people , so the information between them can be transmitted via the state graph .
There are similar effects on the value connection - based graph .
However , domain-connection obtains worse performances .
It makes sense that different types of states are difficult to share expressions even in the same domain .
Effects of decoder for history states
To consult the historical states directly , we introduce two kinds of state encoders , hard copy mechanism and attention - based method , to predict the current states ( Section 2.3 ) .
Table 4 shows the performances of GCDST with different state encoders .
We observe that 1 ) both of the state encoders improve GCDST , and 2 ) the GCDST model with hard copy mechanism is slightly better than that with attention - based method .
We argue that the hard copy mechanism directly copies the states without considering the hidden states of the utterance encoder and the state encoder , which increases the learning burden of the decoder .
Effects of hyper-parameter ?
According to Kipf and Welling ( 2017 ) , we introduce a tradeoff parameter ? into Eq. ( 2 ) , which balances the impacts between self-loops and neighboring node connections by the adjacent matrix of GCN .
To evaluate its effects , we verify GCDST on Multi- WOZ 2.1 by varying ? in range [ 0 , 5 ] .
As shown in Figure 4 , the joint accuracy suffers from a significant decrease when ? = 1 , which indicates that there is not equal importance between selfconnections and edges to neighboring nodes .
We also find that the performances become stable when ? ? 1.4 . Actually , Kipf and Welling ( 2017 ) consider that the ?
plays a similar role as the trade- off parameter between supervised and unsupervised loss in the typical semi-supervised setting .
We will try to find the reason for this interesting phenomenon in future work .
Effects of context length Figure 5 illustrates how the performances of DST models change with respect to the context length ( turns of dialogue history ) on MultiWOZ 2.1 .
We can see a consistent trend of both the baseline and GCDST : 1 ) As the conversation progresses through more turns , the performances of both GCDST and the baseline decrease , which suffers from predicting dialogue states for longer context obviously .
2 ) GCDST and the baseline achieve comparable performance with short dialogue history ( turn ? 6 ) .
As the conversation goes on , GCDST performs better with longer dialogue contexts .
We argue that the baseline encodes the previous utterances by only RNN , which might lose some useful information in context .
By contrast , GCDST uses an extra encoder to model the previous states and exploits hard copy mechanism to duplicate words from historical state , which can alleviate the forgetting problem to some extent .
3 ) According to statistics , to the cases that GCDST correctly predicts while the baseline fails to , the average length is 4.48 turns .
On the contrary , the average length of the cases only predicted by the baseline is 3.96 turns .
It indicates that GCDST is good at dealing with long contexts .
Case Study
We list three examples of the results on MultiWOZ 2.1 , as shown in Table 5 . For Case 1 , the value moderate of the slot attraction - area is mentioned at the 2nd turn in the conversation .
After the 8th turn , the baseline cannot correctly predict the value for the state due to the long context , while GCDST still predicts it correctly at the 10th turn .
It indicates that GCDST can process the longer context , because this model copies values turn by turn by copy mechanisms .
For Case 2 , the expressions of the slot hotel - name are different between the 4th turn ( a and b guest house ) and the 6th turn ( a&b guesthouse ) .
The baseline can predict correctly in the 4th turn but come to nothing in the 6th turn , which might be due to the misleading by the distinct utterance .
In the same case , GCDST gets the correct value by copying it from the previous dialogue state .
It indicates that our proposed model can address the issue of expression diversity to some extent .
For Case 3 , however , the copy mechanisms also might copy an incorrect state from the previous , when the model predicts by mistake .
Related Work Dialogue State Tracking Early research on dialogue state tracking mainly adopted various kinds of natural language understanding modules to extract semantic features from user utterances ( Williams and Young , 2007 ; Thomson and Young , 2010 ; Henderson et al. , 2012 ; Wang and Lemon , 2013 ; Williams , 2014 ) .
These feature - engineering based approaches heavily rely on hand-crafted complex features which are domain-specific and easily give rise to error propagation .
Then , a class of typical methods directly infer dialogue states by semantic dictionaries and delexicalization with the conversation history and the user utterances ( Henderson et al. , 2014 b ; Zilka and Jurcicek , 2015 ; Mrk?i? et al. , 2015 ) .
Although these models possess generalization capability to some extent , it is difficult to obtain a relatively full dictionary .
Meanwhile the number of slot value candidates could be large and variable .
With the increasing technological sophistication of neural networks , the mainstream DST approaches turn to neural - based representation learning models , which represent a dialogue state as a distribution over all slot value candidates that are defined in the ontology .
Amongst these , neural belief tracker ) is a typical CNNbased DST model which regards DST as a binary classification task to determine whether each slotvalue pair in the predefined ontology is represented in the conversation .
There are lots of alternative neural - based frameworks presenting to the DST task Lei et al. , 2018 ; Xu and Hu , 2018 ) .
However , the aforementioned approaches only focus on the single- domain DST task , which is difficult to extend and scale from one domain to another .
Multi-domain DST
In recent years , more and more researchers are devoted to multi-domain DST .
Rastogi et al. ( 2017 ) adopted bi-directional GRUs to share parameters across slots and transfer the parameters to a previously unseen domain .
Ramadan et al. ( 2018 ) estimated the semantic similarities and modeled the interactions between user utterances and the ontology terms to determine which information could be transferred across domains .
Zhong et al. ( 2018 ) presented global modules to share parameters between slots .
Based on their work , Nouri and Hosseiniasl ( 2018 ) introduced recurrent networks to further improve performance .
proposed a copy mechanism to generate dialogue states from user utterances and system responses .
Such mechanism ensures the knowledge transfer when predicting the unseen ( domain , slot , value ) triples .
Le et al. ( 2020 ) introduced a non-autoregressive method into dialogue state tracking to accelerate the state decoding .
Recently , many studies have proposed effective solutions to the multi-domain DST task from various aspects , including the dual strategy model ( Zhang et al. , 2019 ) , the QA - based model ( Zhou and Small , 2019 ) , the memory - based model ( Kim et al. , 2020 ) , the multi-attention - based model ( Budzianowski et al. , 2020 ) , the copy strategy model ( Heck et al. , 2020 ) , and the graph attention neural networks ( Chen et al. , 2020 ) .
Although there is still a performance gap between the proposed model and some of the above models , we argue that the principal motivation of this paper is to verify the effectiveness of graph neural networks and copy mechanisms on multi-domain DST , but not more complicated settings or techniques .
Graph Convolutional Networks for NLP Recently , Graph Convolutional Networks ( GCN ) ( Kipf and Welling , 2017 ) , one typical variant of Graph Neural Networks ( GNN ) ( Cai et al. , 2018 ; Zhou et al. , 2018 ) , has been receiving a considerable amount of attention and been widely applied to many NLP tasks such as semantic role labeling ( Marcheggiani and Titov , 2017 ) , relation extraction Sun et al. , 2019 ) , and question answering ( Tu et al. , 2019 ; De Cao et al. , 2019 ) .
In this paper , we utilize GCN to encode structured information into state node representations .
Copy mechanism
It is a useful way to keep the context consistent in sequence - to-sequence frame-works ( Zeng et al. , 2016 ; Eric and Manning , 2017 ; Song et al. , 2018 ) .
In text summarization , Gu et al . ( 2016 ) first introduced copying into a sequenceto-sequence framework to copy a word from the source passage .
In machine translation , copy mechanisms often copy rare words ( Luong et al. , 2015 ; Gulcehre et al. , 2016 ) .
Similar to previous studies , we use copy mechanisms to extract the state from the last turn to keep the dialogue state consistent .
Conclusion
This paper presents a graph- based and copyaugmented multi-domain DST model ( GCDST ) .
In particular , GCDST constructs a graph to transfer knowledge among states with the same slots or values across domains by GCN and encodes the history utterances and states by two independent encoders .
Furthermore , we add the hard copy mechanism to directly copy states from the last turn for the decoder .
Empirical studies on the MultiWOZ 2.0 / 2.1 dialogue datasets suggest that GCDST outperforms previous systems substantially for the multi-domain DST task .
Further analysis demonstrates the positive effects of graph representations for information transferring across domains and advantages of copy mechanisms for state tracking of long-distance dialogue history .
Figure 1 : 1 Figure1 : A conversation with dialogue states ( solid box ) of three domains ( dashed boxes ) of MultiWOZ 2.0 .
The colored slots in states are corresponding to their values with the same color in the conversation .
Each tuple denotes a slot-value pair , and the lines between them represent that they have the same slot or the same value .
