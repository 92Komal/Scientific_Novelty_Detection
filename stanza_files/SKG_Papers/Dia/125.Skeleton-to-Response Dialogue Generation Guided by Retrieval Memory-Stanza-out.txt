title
Skeleton-to- Response : Dialogue Generation Guided by Retrieval Memory
abstract
Traditional generative dialogue models generate responses solely from input queries .
Such information is insufficient for generating a specific response since a certain query could be answered in multiple ways .
Recently , researchers have attempted to fill the information gap by exploiting information retrieval techniques .
For a given query , similar dialogues are retrieved from the entire training data and considered as an additional knowledge source .
While the use of retrieval may harvest extensive information , the generative models could be overwhelmed , leading to unsatisfactory performance .
In this paper , we propose a new framework which exploits retrieval results via a skeleton- to- response paradigm .
At first , a skeleton is extracted from the retrieved dialogues .
Then , both the generated skeleton and the original query are used for response generation via a novel response generator .
Experimental results show that our approach significantly improves the informativeness of the generated responses .
Introduction
This paper focuses on tackling the challenges to develop a chit-chat style dialogue system ( also known as chatbot ) .
Chit - chat style dialogue system aims at giving meaningful and coherent responses given a dialogue query in the open domain .
Most modern chit-chat systems can be categorized into two categories , namely , information retrieval - based ( IR ) models and generative models .
The IR - based models ( Ji et al. , 2014 ; Hu et al. , 2014 ) directly copy an existing response from a training corpus when receiving a response request .
Since the training corpus is usually collected from real-world conversations and possibly post-edited by a human , the retrieved responses are informative and grammatical .
However , the performance of such systems drops when a given dialogue history is substantially different from those in the training corpus .
The generative models ( Shang et al. , 2015 ; Vinyals and Le , 2015 ; Li et al. , 2016a ) , on the other hand , generate a new utterance from scratch .
While those generative models have better generalization capacity in rare dialogue contexts , the generated responses tend to be universal and noninformative ( e.g. , " I do n't know " , " I think so " etc . ) ( Li et al. , 2016 a ) .
It is partly due to the diversity of possible responses to a single query ( i.e. , the one-to- many problem ) .
The dialogue query alone cannot decide a meaningful and specific response .
Thus a well - trained model tends to generate the most frequent ( safe but boring ) responses instead .
To summarize , IR - based models may give informative but inappropriate responses while generative models often do the opposite .
It is desirable to combine both merits .
Song et al. ( 2016 ) used an extra encoder for the retrieved response .
The resulted dense representation , together with the original query , is used to feed the decoder in a standard SEQ2SEQ model ( Bahdanau et al. , 2014 ) . Weston et al. ( 2018 ) used a single encoder that takes the concatenation of the original query and the retrieved as input .
Wu et al . ( 2019 ) noted that the retrieved information should be used in awareness of the context difference , and further proposed to construct an edit vector by explicitly encoding the lexical differences between the input query and the retrieved query .
However , in our preliminary experiments , we found that the IR - guided models are inclined to degenerate into a copy mechanism , in which the generative models simply repeat the retrieved response without necessary modifications .
Sharp performance drop is caused when the retrieved re-sponse is irrelevant to the input query .
A possible reason is that both useful and useless information is mixed in the dense vector space , which is uninterpretable and uncontrollable .
To address the above issue , we propose a new framework , skeleton- to- response , for response generation .
Our motivations are two folds : ( 1 )
The guidance from IR results should only specify a response aspect or pattern , but leave the queryspecific details to be elaborated by the generative model itself ; ( 2 ) The retrieval results typically contain excessive information , such as inappropriate words or entities .
It is necessary to filter out irrelevant words and derive a useful skeleton before use .
Our approach consists of two components : a skeleton generator and a response generator .
The skeleton generator extracts a response skeleton by detecting and removing unwanted words in a retrieved response .
The response generator is responsible for adding query -specific details to the generated skeleton for query - to- response generation .
A dialogue example illustrating our idea is shown in Fig. 1 . Due to the discrete choice of skeleton words , the gradient in the training process is no longer differentiable from the response to the skeleton generator .
Two techniques are proposed to solve this issue .
The first technique is to employ the policy gradient method for rewarding the output of the skeleton generator based on the feedback from a pre-trained critic .
An alternative technique is to solve both the skeleton generation and the response generation in a multi-task learning fashion .
Our contributions are summarized as below : ( 1 ) We develop a novel framework to inject the power of IR results into generative response models by introducing the idea of skeleton generation ; ( 2 ) Our approach generates response skeletons by detecting and removing unnecessary words , which facilitates the generation of specific responses while not spoiling the generalization ability of the underlying generative models ; ( 3 ) Experimental results show that our approach significantly outperforms other compared methods , resulting in more informative and specific responses .
Models
In this work , we propose to construct a response skeleton based on the results of IR systems for guiding the response generation .
The skeleton- to- response paradigm helps reduce the search space of possible responses and provides useful elements missing in the given query .
Our model consists of two components , namely , the skeleton generator and the response generator .
These components are parameterized by the above two probabilistic models , denoted by ? ske and ? res respectively .
Fig. 2 depicts the overall architecture of our proposed framework .
Skeleton Generator
The skeleton generator transforms a retrieved response into a skeleton by explicitly removing inappropriate or useless information regarding the input query q.
We consider this procedure as a series of word - level masking actions .
Following Wu et al. ( 2019 ) , we first construct an edit vector by comparing the difference between the original query q and the retrieved query q .
In ( Wu et al. , 2019 ) the edit vector is used to guide the response generation directly .
In our model , the edit vector is used to estimate the probability of being reserved or being masked for every word in a sentence .
We define two word sets , namely insertion words I and deletion words D .
The insertion words include words that are in the original query q , but not in the retrieved query q , while the deletion words do the opposite .
The two bags of words highlight the changes in the dialogue context , corresponding to the changes in the response .
The edit vector z is thus defined as the concatenation of the representations of the two bags of words .
We use the weighted sum of the word embeddings to get the dense representations of I and D .
The edit vector is computed as : z = w 1 ?I ? w 1 ?( w 1 ) ? w 2 ?D ? w 2 ?( w 2 ) , ( 1 ) where ? is the concatenation operation .
? maps a word to its corresponding embedding vector , ? w 1 and ?
w 2 are the weights of an insertion word w 1 and a deletion word w 2 respectively .
The weights of different words are derived by an attention mechanism ( Luong et al. , 2015 ) .
Formally , the retrieved response r = ( r 1 , r 2 . . . , r |r | ) is processed by a bidirectional GRU network ( biGRU ) .
We denote the states of the biGRU ( i.e. concatenation of forward and backward GRU states ) as ( h 1 , h 2 , . . . , h |r | ) .
The weight ?
w 1 is calculated by : ? w 1 = exp( s w 1 ) w?
I exp( s w ) , s w 1 = v I tanh ( W I [?( w 1 ) ? h |r | ] ) , ( 2 ) where v I and W I are learnable parameters .
The weight ?
w 2 is obtained in a similar way with another set of parameters v D and W D .
After acquiring the edit vector , we transform the prototype response r to a skeleton t by the follow-ing equations : t = ( ?( r 1 , h 1 , z ) , ?( r 2 , h 2 , z ) , ? ? ? , ?( r |r | , h |r | , z ) ) , ?( r i , h i , z ) = < blank > if mi = 0 , r i else , ( 3 ) where mi is the indicator and equals 0 if r i is replaced with a placeholder " < blank > " and 1 otherwise .
The probability of mi = 1 is computed by P ( mi = 1 ) = sigmoid ( W m [ h i ? z ] + b m ) . ( 4 )
Response Generator
The response generator can be implemented using most existing IR - augmented models ( Song et al. , 2016 ; Weston et al. , 2018 ; Pandey et al. , 2018 ) , just by replacing the retrieved response input with the corresponding skeleton .
We discuss our choices below .
Encoders
Two separate bidirectional LSTM ( biLSTM ) networks are used to obtain the distributed representations of the query memories and the skeleton memories , respectively .
For biLSTM , the concatenation of the forward and the backward hidden states at each token position is considered a memory slot , producing two memory pools : M q = {h 1 , h 2 , . . . , h |q| } for the input query , and M t = {h 1 , h 2 , . . . , h | t| } for the skeleton .
1 Decoder During the generation process , our decoder reads information from both the query and the skeleton using attention mechanism ( Bahdanau et al. , 2014 ; Luong et al. , 2015 ) .
To query the memory pools , the decoder uses the hidden state s t of itself as the searching key .
The matching score function is implemented by bilinear functions : ?( h k , s t ) = h k T W q s t ; ?( h k , s t ) = h k T W t s t , ( 5 ) where W q and W t are trainable parameters .
A query context vector c t is then computed as a weighted sum of all memory slots in M q , where the weight for a memory slot h k is exp ( ?( h k , s t ) ) / ( | q| i=1 exp ( ?( h i , s t ) ) ) .
A skele - ton context vector c t is computed in a similar spirit by using ?( h k , s t ) 's .
The probability of generating the next word r t is then jointly determined by the decoder 's state s t , the query context c t and the skeleton context c t .
We first fuse the information of s t and c t by a linear transformation .
For c t , a gating mechanism is additionally introduced to control the information flow from skeleton memories .
Formally , the probability of the next token r t is estimated by y t followed by a softmax function over the vocabulary : y t = ( W c [ s t ? c t ] ) ? g t + c t ? ( 1 ? g t ) , ( 6 ) where g t = f g ( s t , c t , c t ) is implemented by a single layer neural network with sigmoid output layer .
Learning Given that our skeleton generator performs nondifferentiable hard masking , the overall model cannot be trained end-to - end using the standard maximum likelihood estimate ( MLE ) .
A possible solution that circumvents this problem is to treat the skeleton generation and the response generation as two parallel tasks and solve them jointly in a multi-task learning fashion .
An alternative is to bridge the skeleton generator and the final response output using reinforcement learning ( RL ) methods , which can exclusively inform the skeleton generator with the ultimate goal .
The latter option is referred as cascaded integration while the former is called joint integration .
Recall that we have formulated the skeleton generation as a series of binary classifications .
Nevertheless , most of the dialogue datasets are end-to - end query - response pairs without explicit skeletons .
Hence , we propose to construct proxy skeletons to facilitate the training .
Definition 1 Proxy Skeleton : Given a training quadruplet ( q , q , r , r ) and a stop word list S , the proxy skeleton for r is generated by replacing some tokens in r with a placeholder " < blank > " .
A token r i is kept if and only if it meets the following conditions 1 . r i / ? S 2 . r i is a part of the longest common subsequence ( LCS ) ( Wagner and Fischer , 1974 ) of r and r .
The proxy skeletons are used in different manners according to the integration method , which we will introduce below .
Joint Integration
To avoid breaking the differentiable computation , we connect the skeleton generator and the response generator via a shared network architecture rather than by passing the discrete skeletons .
Concretely , the last hidden states in our skeleton generator ( i.e , the hidden states that are utilized to make the masking decisions ) are used as the skeleton memories in response generation .
The training objective is the sum of the proxy skeleton labels likelihood L (? ske ) and the response likelihood L (? res ) : L (? res ? ? ske ) = L (? res ) + ?L ( ? ske ) , ( 7 ) where ? is a harmonic weight , and it is set as 1.0 in our experiments .
Cascaded Integration Policy gradient methods ( Williams , 1992 ) can be applied to optimize the full model while keeping it running as cascaded process .
We regard the skeleton generator as the first RL agent , and the response generator as the second one .
The final output generated by the pipeline process and the intermediate skeleton are denoted by r and t respectively .
Given the original query q and the generated response r , a reward R( q , r ) for generating r is calculated .
All network parameters are then optimized to maximize the expected reward by the policy gradient .
The reward function R should convey both the naturalness of the generated response and its relevance to the given query q.
A pre-trained critic is utilized to make the judgment .
Inspired by comparative adversarial learning in , we design the critic as a classifier that receives four inputs every time : the query q , a human-written response r , a machine - generated response r and a random response r ( yet written by human ) .
The critic is trained to pick the human-written response r among others correctly .
Formally , the following objective is maximized : log D( r|q , r , r , r ) = log exp ( h r T M D h q ) x?{r,r, r} exp( h x T M D h q ) , ( 8 ) where h x is a vector representation of x , produced by a bidirectional LSTM ( the last hidden state ) , and M D is a trainable matrix .
2
Related Work Multi-source Dialogue Generation Chit-chat style dialogue system dates back to ELIZA ( Weizenbaum , 1966 ) .
Early work uses handcrafted rules , while modern systems usually use data-driven approaches , e.g. , information retrieval techniques .
Recently , end-to - end neural approaches ( Vinyals and Le , 2015 ; Serban et al. , 2016 ; Li et al. , 2016a ; Sordoni et al. , 2015 ) have attracted increasing interest .
For those generative models , a notorious problem is the " safe response " problem : the generated responses are dull and generic , which may attribute to the lack of sufficient input information .
The query alone cannot specify an informative response .
To mitigate the issue , many research efforts have been paid to introducing other information source , such as unsupervised latent variable ( Serban et al. , 2017 ; Cao and Clark , 2017 ; Shen et al. , 2017 ) , discourse- level variations , topic information , speaker personality ( Li et al. , 2016 b ) and knowl-edge base ( Ghazvininejad et al. , 2018 ; Zhou et al. , 2018 ) .
Our work follows the similar motivation and uses the output of IR systems as the additional knowledge source .
Combination of IR and Generative models
To combine IR and generative models , early work ( Qiu et al. , 2017 ) tried to re-rank the output from both models .
However , the performance of such models is limited by the capacity of individual methods .
Most related to our work , Song et al . ( 2016 ) ; Weston et al. ( 2018 ) and Wu et al . ( 2019 ) encoded the retrieved result into distributed representation and used it as the additional conditionals along with the standard query representation .
While the former two only used the target side of the retrieved pairs , the latter took advantages of both sides .
In a closed domain conversation setting , Pandey et al . ( 2018 ) further proposed to weight different training instances by context similarity .
Our model differs from them in that we take an extra intermediate step for skeleton generation to filter the retrieval information before use , which shows the effectiveness in avoiding the erroneous copy in our experiments .
Multi-step Language Generation
Our work is also inspired by the recent success of decomposing an end-to - end language generation task into several sequential sub-tasks .
For document summarization , Chen and Bansal ( 2018 ) first select salient sentences and then rewrite them in parallel .
For sentiment - to-sentiment translation , first use a neutralization module to remove emotional words and then add sentiment to the neutralized content .
Not only does their decomposition improve the overall performance , but also makes the whole generation process more interpretable .
Our skeleton- to- response framework also sheds some light on the use of retrieval memories .
Experiments
Data
We use the preprocessed data in ( Wu et al. , 2019 ) as our test bed .
The total dataset consists of about 20 million single - turn query - response pairs collected from Douban Group 3 .
Since similar contexts may correspond to totally different responses , the training quadruples ( q , r , q , r ) for IR - augmented models are constructed based on response similarity .
All response are indexed by Lucene .
4 For each ( q , r ) pair , top 30 similar responses with their corresponding contexts are retrieved {( q i , r i ) } 30 i=1 .
However , only those satisfying 0.3 ? Jaccard(r , r i ) ? 0.7 are leveraged for training , where Jaccard measures the Jaccard distance .
The reason for the data filter is that nearly identical responses drive the model to do simple copy while distantly different responses make the model ignore the retrieval input .
About 42 million quadruples are obtained afterward .
For computational efficiency , we randomly sample 5 million quadruples as training data for all experiments .
The test set consists of 1,000 randomly selected queries that are not in our training data .
5
For a fair comparison , when training a generative model without the help of IR , the quadruples are split into pairs .
Model Details
We implement the skeleton generator based on a bidirectional recurrent neural network with 500 LSTM units .
We concatenate the hidden states from both directions .
The word embedding size is set to 300 .
For the response generator , the encoder for queries , the encoder for skeletons and the decoder are three two -layer recurrent neural networks with 500 LSTM units , where both encoders are bidirectional .
We use dropout ( Srivastava et al. , 2014 ) to alleviate overfitting .
The dropout rate is set to 0.3 across different layers .
The same architecture for the encoders and the decoder is shared across the following baseline models , if applicable .
Compared Methods ?
Seq2Seq the standard attention - based RNN encoder-decoder model ( Bahdanau et al. , 2014 ) . ? MMI SEQ2SEQ with Maximum Mutual Information ( MMI ) objective in decoding ( Li et al. , 2016 a ) .
In practice , an inverse ( response-to-query ) SEQ2SEQ model is used to rerank the N - best hypothesizes from the standard SEQ2SEQ model ( N equals ?
EditVec the model proposed by Wu et al . ( 2019 ) , where the edit vector z is used directly at each decoding step by concatenating it to the word embeddings . ?
IR the Lucene system is also used a benchmark .
6 ? IR + rerank rerank the results of IR by MMI .
Besides ,
We use JNT to denote our model with joint integration , and CAS for our model with cascaded integration .
To validate the usefulness of the proposed skeletons .
We design a response generator that takes an intact retrieval response as its skeleton input ( i.e. , to completely skip the skeleton generation step ) , denoted by SKP .
7
Evaluation Metrics
Our method is designed to improve the informativeness of the generative model and alleviate the inappropriateness problem of the retrieval model .
To measure the performance effectively , we use human evaluation along with two automatic evaluation metrics .
?
Human evaluation
We asked three experienced annotators to score the group of responses ( the best output of each model ) for 300 test queries .
The responses are rated on a five-point scale .
A response should be scored 1 if it can hardly be considered a valid response , 3 if it is a valid but not informative response , 5 if it is an informative response , which can deepen the discussion of the current topic or lead to a new topic .
2 and 4 are for decision ? dist - 1 & dist - 2
It is defined as the number of unique uni-grams ( dist - 1 ) or bi-grams ( dist - 2 ) dividing by the total number of tokens , measuring the diversity of the generated responses ( Li et al. , 2016 a ) .
Note the two metrics do not necessarily reflect the response quality as the target queries are not taken into consideration .
Response Generation Results
The results are depicted in Table 1 . Overall , both of our models surpass all other methods , and our cascaded model ( CAS ) gives the best performance according to human evaluation .
The contrast with the SKP model illustrates that the use of skeletons brings a significant performance gain .
According to the dist - 1&2 metrics , the generative models achieve significantly better diversity by the use of retrieval results .
The retrieval method yields the highest diversity , which is consistent with our intuition that the retrieval responses typically contain a large amount of information though they are not necessarily appropriate .
The model of MMI also gives strong diversity , yet we find that it tends to merely repeat the words in queries .
By removing the words in queries , the dist - 2 of MMI and CAS become 0.710 and 0.751 respectively .
It indicates our models are better at generating new words .
To further reveal the source of performance gain , we study the relation between response quality and query similarity ( measured by the Jaccard similarity between the input query and the retrieved query ) .
Our best model ( CAS ) is compared with the strong IR system ( IR - rerank ) and the previous state - of- the- art ( EditVec ) in Fig. 3 .
The CAS model significantly boosts the performance when query similarity is relatively low , which indicates that introducing skeletons can alleviate erroneous copy and keep a strong generalization ability of the underlying generative model .
More Analysis of Our Framework
Here , we present further discussions and empirical analysis of our framework .
Generated Skeletons
Although generating skeletons is not our primary goal , it is interesting to assess the skeleton generation .
The word- level precision ( P ) , recall ( R ) , F 1 score ( F 1 ) and accuracy ( Acc. ) of the well - trained skeleton generators are reported in Table 2 , taking the proxy skeletons as golden references .
Table 3 shows some skeleton- to- response examples of the CAS model and a case study among different models .
In the leftmost example in Table 3 , the MMI and the EditVec simply repeat the query while the retrieved response is weakly related to the query .
Our CAS model extracts a useful word ' boy ' from the retrieved response and generates a more interesting response .
In the middle example , the MMI response makes less sense , and some private information is included in the retrieved response .
Our CAS model removes the privacy without the loss of informativeness , while the outputs by other models are less informative .
The rightmost case shows that our response generator is able to recover the possible mistakes made by the skeleton generator .
Retrieved Response v.s. Generated Response
To measure the extent that the generative models are copying the retrieval , we compute the edit distances between generated responses and retrieved responses .
As shown in Fig. 4 , in the comparison between the SKP and other models , the use of skeletons makes the generated response deviate more from its prototype response .
Ideally , when the retrieved context is very similar to the input query , the changes between the generated response and the response be minor .
Conversely , the changes should be drastic .
Fig. 4 also shows that our models can learn this intuition .
Single v.s. Multiple Retrieval Pair( s )
For a given query q , the retrieval pair set R q could contain multiple query - response pairs .
We investigate two ways of using it under the CAS setting .
?
Single
For each query- response pair ( q i , r i ) ?
R q , a response ri is generated solely based on q , and ( q i , r i ) .
The resulted responses are re-ranked by generation probability .
?
Multiple
The whole retrieval set R q is used in a single run .
Multiple skeletons are generated and concatenated in the response generation stage .
The results are shown in
Conclusion
In this paper , we proposed a new methodology to enhance generative models with information retrieval technologies for dialogue response generation .
Given a dialogue context , our methods generate a skeleton based on historical responses that respond to a similar context .
The skeleton serves as an additional knowledge source that helps specify the response direction and complement the response content .
Experiments on real world data validated the effectiveness of our method for more informative and appropriate responses .
Figure 2 : 2 Figure2 : The architecture of our framework .
Given a query " Do you like banana " , a similar historical query " Do you like apple " is retrieved along with its response , i.e. , " Yes , apple is my favorite " .
Upper : The skeleton generator removes inappropriate words and extracts a response skeleton .
Lower :
The response generator generates a response based on both the skeleton and the query .
