title
A Long Short-Term Memory Framework for Predicting Humor in Dialogues
abstract
We propose a first-ever attempt to employ a Long Short - Term memory based framework to predict humor in dialogues .
We analyze data from a popular TV - sitcom , whose canned laughters give an indication of when the audience would react .
We model the setuppunchline relation of conversational humor with a Long Short - Term Memory , with utterance encodings obtained from a Convolutional Neural Network .
Out neural network framework is able to improve the F-score of 8 % over a Conditional Random Field baseline .
We show how the LSTM effectively models the setup-punchline relation reducing the number of false positives and increasing the recall .
We aim to employ our humor prediction model to build effective empathetic machine able to understand jokes .
Introduction
There has been many recent attempts to detect and understand humor , irony and sarcasm from sentences , usually taken from Twitter ( Reyes et al. , 2013 ; Barbieri and Saggion , 2014 ; Riloff et al. , 2013 ; Joshi et al. , 2015 ) , customer reviews ( Reyes and Rosso , 2012 ) or generic canned jokes ( Yang et al. , 2015 ) . Bamman and Smith ( 2015 ) and Karoui et al . ( 2015 ) included the surrounding context .
Our work has a different focus from the above .
We analyze transcripts of funny dialogues , a genre somehow neglected but important for human-robot interaction .
Laughter is the natural reaction of people to a verbal or textual humorous stimulus .
We want to predict when the audience would laugh .
Compared to a typical canned joke or a sarcastic Tweet , a dialog utterance is perceived as funny only in relation to the dialog context and the past history .
In a spontaneous setting a funny dialog is usually built through a setup which prepares the audience to receive the humorous discourse stimuli , followed by a punchline which releases the tension and triggers the laughter reaction ( Attardo , 1997 ; Taylor and Mazlack , 2005 ) .
Automatic understanding of a humorous dialog is a first step to build an effective empathetic machine fully able to react to the user 's humor and to other discourse stimuli .
We are ultimately interested in developing robots that can bond with humans better ( Devillers et al. , 2015 ) .
As a source of situational humor we study a popular TV sitcom : " The Big Bang Theory " .
The domain of sitcoms is of interest as it provides a full dialog setting , together with an indication of when the audience is expected to laugh , given by the background canned laughters .
An example of dialog from this sitcom , as well as of the setup-punchline schema , is shown below ( punchlines in bold ) : LAUGH
He started America on a path to the metric system but then just gave up .
LAUGH
The utterances before the punchline are the setup .
Without them , the punchlines may not be perceived as humorous ( the last utterance , out of context , may be a political complaint ) , only with proper setup a laughter would be triggered .
The humorous intent is also strengthen by the fact the dialog takes place in a bar ( evident from the previous and following utterances ) , where a request of 40 ml of " Ethyl Alcohol " is unusual and weird .
Our previous attempts on the same corpus ( Bertero and Fung , 2016 b ; Bertero and Fung , 2016a ) showed that using a bag-of-ngram representation over a sliding window or a simple RNN to capture the contextual information of the setup was not ideal .
For this reason we propose a method based on a Long Short - Term Memory network ( Hochreiter and Schmidhuber , 1997 ) , where we encode each sentence through a Convolutional Neural Network ( Collobert et al. , 2011 ) . LSTM is successfully used in context-dependent sequential classification tasks such as speech recognition ( Graves et al. , 2013 ) , dependency parsing ( Dyer et al. , 2015 ) and conversation modelling ( Shang et al. , 2015 ) .
This is also to our knowledge the first-ever attempt that a LSTM is applied to humor response prediction or general humor detection tasks .
Methodology
We employ a supervised classification method to detect when punchlines occur .
The bulk of our classifier is made of a concatenation of a Convolutional Neural Network ( Collobert et al. , 2011 ) to encode each utterance , followed by a Long Short - Term Memory ( Hochreiter and Schmidhuber , 1997 ) to model the sequential context of the dialog .
Before the output softmax layer we add a vector of higher level syntactic , structural and sentiment features .
A framework diagram is shown in Figure 1 .
Convolutional Neural Network for each utterance
The first stage of our classifier is represented by a Convolutional Neural Network ( Collobert et al. , 2011 ) .
Low-level , high- dimensional input feature vectors are fed into a first embedding layer to obtain a low dimensional dense vector .
A sliding window is lt are the high level feature vectors , and yt the outputs for each utterance .
then moved on these vectors and another layer is applied to each group of token vectors , in order to capture the local context of each token .
A max-pooling operation is then applied to extract the most salient features of all the tokens into a single vector for the whole utterance .
An additional layer is used to generalize and distribute each feature to its full range before obtaining the final utterance vector .
In our task we use three input features : 1 . Word tokens : each utterance token is represented as a one-hot vector .
This feature models how much each word is likely to trigger humor in the specific corpus .
2 . Character trigrams : each token is represented as a bag-of- character - trigrams vector .
The feature models the role of the word signifier and removes the influence of the word stems .
3 . Word2Vec : we extract for each token a word vector from word2vec ( Mikolov et al. , 2013 ) , trained on the text9 Wikipedia corpus 1 .
This representation models the general semantic meanings , and matches words that do not appear to others similar in meaning .
The convolution and max-pooling operation is applied individually to each feature , and the three vectors obtained are then concatenated together and fed to the final sentence encoding layer , which combines all the contributions .
Long / Short Term Memory for the utterance sequence
The LSTM is an improvement over the Recurrent Neural Network aimed to improve its memory capabilities .
In a standard RNN the hidden memory layer is updated through a function of the input and the hidden layer at the previous time instant : h t = tanh ( W x x t + W h h t?1 + b ) ( 1 ) where x is the network input and b the bias term .
This kind of connection is not very effective to maintain the information stored for long time instants , as well as it does not allow to forget unneeded information between two time steps .
The LSTM enhances the RNN with a series of three multiplicative gates .
The structure is the following : i t = ?( W ix x t + W i h h t?1 + b i ) ( 2 ) f t = ?( W fx x t + W f h h t?1 + b f ) ( 3 ) o t = ?( W ox x t + W o h h t?1 + b o ) ( 4 ) s t = tanh ( W sx x t + W s h h t?1 + b h ) ( 5 ) c t = f t c t?1 + i t s t ( 6 ) h t = tanh ( c t ) o t ( 7 ) where is the element-wise product .
Each gate factor is able to let through or suppress a specific update contribution , thus allowing a selective information retaining .
The input gate i is applied to the cell input s , the forget gate f to the cell value at the previous time step c t?1 , and the output gate o to the cell output for the current time instant h t .
In this way a cell value can be retained for multiple time steps when i = 0 , ignored in the output when o = 0 , and forgotten when f = 0 .
As dialog utterances are sequential , we feed all utterance vectors of a sitcom scene in sequence into a Long Short - Term Memory block to incorporate contextual information .
The memory unit of the LSTM keeps track of the context in each scene , and mimics human memory to accumulate the setup that may trigger a punchline .
Before the output we incorporate a set of high level features from our previous work ( Bertero and Fung , 2016 b ) and past literature ( Reyes et al. , 2013 ; Barbieri and Saggion , 2014 ) .
They include : ?
Structural features : average word length , sentence length , difference in sentence length with the five previous utterances .
?
Part of speech proportion : noun , verbs , adjectives and adverbs .
?
Antonyms : proportion of antonym words with the previous utterance ( from WordNet ( Miller , 1995 ) ) . ?
Sentiment : positive , negative and average sentiment score among all words ( from Senti-WordNet ( Esuli and Sebastiani , 2006 ) ) . ?
Speaker and turn : speaker character identity and utterance position in the turn ( beginning , middle , end , isolated ) .
?
Speaking rate : time duration of the utterance from the subtitle files , divided by the sentence length .
All these features are concatenated to the LSTM output , and a softmax layer is applied to get the final output probabilities .
Experiments
Corpus
We built a corpus from the popular TV - sitcom " The Big Bang Theory " , seasons 1 to 6 .
We downloaded the subtitle files ( annotated with the timestamps of each utterance ) and the scripts 2 , used to segment all the episodes into scenes and get the speaker identity of each utterance .
We extracted the audio track of each episode in order to retrieve the canned laughters timestamps , with a vocal removal tool followed by a silence / sound detector .
We then annotated each utterance as a punchline in case it was followed by a laughter within 1s , assuming that utterances not followed by a laughter would be the setup for the punchline .
We obtained a total of 135 episodes , 1589 overall scenes , 42.8 % of punchlines , and an average interval between two punchlines of 2.2 utterances .
We built a training set of 80 % of the overall episodes , and a development and test set of 10 % each .
The episodes were drawn from all the seasons with the same proportion .
The total number of utterances is 35865 for the training set , 3904 for the development set and 3903 for the test set .
Experimental setup and baseline
In the neural network we set the size to 100 for all the hidden layers of the CNN and the LSTM , and 5 to the convolutional window .
We applied a dropout regularization layer ( Srivastava et al. , 2014 ) after the output of the LSTM , and L2 regularization on the softmax output layer .
The network was trained with standard backpropagation , using each scene as a training unit .
The development set was used to tune the hyperparameters , and to determine the early stopping condition .
When the error on the development set began to increase for the first time we kept training only the final softmax layer , this improved the overall results .
The neural network was implemented with THEANO toolkit ( Bergstra et al. , 2010 ) .
We ran experiments with and without the extra high - level feature vector .
As a baseline for comparison we used an implementation of the Conditional Random Field ( Lafferty et al. , 2001 ) from CRFSuite ( Okazaki , 2007 ) , with L2 regularization .
We ran experiments using the same high level feature vector added at the end of the neural network , 1 - 2 - 3 gram features of a window made by the utterance and the four previous , and the two feature sets combined .
We also compared the overall system where we replace the CNN with an LSTM sentence encoder ( Li et al. , 2015 ) , where we kept the same input features .
Results and discussion Results of our system and our baselines are shown in table 1 .
The LSTM with the aid of the high level feature vector generally outperformed all the CRF baselines with the highest accuracy of 70.0 % and the highest F-score of 62.9 % .
The biggest improvement of the LSTM is the improvement of the recall without affecting too much the precision .
Lexical features given by n-gram from a context window are very useful to recognize more punchlines in our baseline experiment , but they also yield many false positives , when the same n-gram is used in different contexts .
A CNN -LSTM network seems to overcome this issue as the CNN stage is better in modeling the lexical and semantic content of the utterance , as the LSTM allows to put each utterance in relation with the past context , filtering out many false positives from wrong contexts .
The choice of the CNN is further justified by the results obtained from the comparison between the CNN and the LSTM sentence encoding input , shown in table 2 .
The CNN is more effective , obtaining a recall of 10 % higher and 6 % more in Fscore .
The CNN is a simpler model that might benefit more of a small - size corpus .
It also required a much shorter training time compared to the LSTM .
We may consider in the future to use more data , and try other sentence input encoders , including deeper or bi-directional LSTMs , to find the most effective one .
Predicting humor response from the canned laughters is a particularly challenging task .
In some cases canned laughters are inserted by the show producers with the purpose of solicit response to weak jokes , where otherwise people would not laugh .
The audience must also be kept constantly amused , extra canned laughters may help in scenes where fewer jokes are used .
Conclusion and future work We proposed a Long Short - Term Memory based framework to predict punchlines in a humorous dialog .
We showed that our neural network is particularly effective in increasing the F-score to 62.9 % over a Conditional Random Field baseline of 58.1 % .
We furthermore showed that the LSTM is more effective in obtaining an higher recall with fewer false positives compared to simple n-gram shifting context window features .
As future work we plan to use a virtual agent system to collect a set of human-robot humorous interactions , and adapt our model to predict humor from them .
Figure 1 : 1 Figure 1 : Framework diagram .
at , bt and ct are the CNN three input features ( words , word2vec and character trigrams ) .
