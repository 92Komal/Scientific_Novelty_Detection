title
Hidden Softmax Sequence Model for Dialogue Structure Analysis
abstract
We propose a new unsupervised learning model , hidden softmax sequence model ( HSSM ) , based on Boltzmann machine for dialogue structure analysis .
The model employs three types of units in the hidden layer to discovery dialogue latent structures : softmax units which represent latent states of utterances ; binary units which represent latent topics specified by dialogues ; and a binary unit that represents the global general topic shared across the whole dialogue corpus .
In addition , the model contains extra connections between adjacent hidden softmax units to formulate the dependency between latent states .
Two different kinds of real world dialogue corpora , Twitter - Post and AirTicketBooking , are utilized for extensive comparing experiments , and the results illustrate that the proposed model outperforms sate- ofthe - art popular approaches .
Introduction Dialogue structure analysis is an important and fundamental task in the natural language processing domain .
The technology provides essential clues for solving real-world problems , such as producing dialogue summaries ( Murray et al. , 2006 ; Liu et al. , 2010 ) , controlling conversational agents ( Wilks , 2006 ) , and designing interactive dialogue systems ( Young , 2006 ; Allen et al. , 2007 ) etc .
The study of modeling dialogues always assumes that for each dialogue there exists an unique latent structure ( namely dialogue structure ) , which consists of a series of latent states .
1 1 Also called dialogue acts or speech acts in some past work .
In this paper , for simplicity we will only use the term " latent state " to describe the sequential dialogue structure .
Some past works mainly rely on supervised or semi-supervised learning , which always involve extensive human efforts to manually construct latent state inventory and to label training samples .
Cohen et al. ( 2004 ) developed an inventory of latent states specific to E-mail in an office domain by inspecting a large corpus of e-mail .
Jeong et al. ( 2009 ) employed semi-supervised learning to transfer latent states from labeled speech corpora to the Internet media and e-mail .
Involving extensive human efforts constrains scaling the training sample size ( which is essential to supervised learning ) and application domains .
In recent years , there has been some work on modeling dialogues with unsupervised learning methods which operate only on unlabeled observed data .
Crook et al. ( 2009 ) employed Dirichlet process mixture clustering models to recognize latent states for each utterance in dialogues from a travel - planning domain , but they do not inspect dialogues ' sequential structure .
Chotimongkol ( 2008 ) proposed a hidden Markov model ( HMM ) based dialogue analysis model to study structures of task - oriented conversations from indomain dialogue corpus .
More recently , Ritter et al. ( 2010 ) extended the HMM based conversation model by introducing additional word sources for topic learning process .
Zhai et al. ( 2014 ) assumed words in an utterance are emitted from topic models under HMM framework , and topics were shared across all latent states .
All these dialogue structure analysis models are directed generative models , in which the HMMs , language models and topic models are combined together .
In this study , we attempt to develop a Boltzmann machine based undirected generative model for dialogue structure analysis .
As for the document modeling using undirected generative model , Hinton and Salakhutdinov ( 2009 ) proposed a general framework , replicated soft-max model ( RSM ) , for topic modeling based on restricted Boltzmann machine ( RBM ) .
The model focuses on the document- level topic analysis , it cannot be applied for the structure analysis .
We propose a hidden softmax sequence model ( HSSM ) for the dialogue modeling and structure analysis .
HSSM is a two -layer special Boltzmann machine .
The visible layer contains softmax units used to model words in a dialogue , which are the same with the visible layer in RSM ( Hinton and Salakhutdinov , 2009 ) .
However , the hidden layer has completely different design .
There are three kinds of hidden units : softmax hidden units , which is utilized for representing latent states of dialogues ; binary units used for representing dialogue specific topics ; and a special binary unit used for representing the general topic of the dialogue corpus .
Moreover , unlike RSM whose hidden binary units are conditionally independent when visible units are given , HSSM has extra connections utilized to formulate the dependency between adjacent softmax units in the hidden layer .
The connections are the latent states of two adjacent utterances .
Therefore , HSSM can be considered as a special Boltzmann machine .
The remainder of this paper is organized as follows .
Section 2 introduces two real world dialogue corpora utilized in our experiments .
Section 3 describes the proposed hidden softmax sequence model .
Experimental results and discussions are presented in Section 4 .
Finally , Section 5 presents our conclusions .
Data Set
Two different datasets are utilized to test the effectiveness of our proposed model : a corpus of post conversations drawn from Twitter ( Twitter - Post ) , and a corpus of task - oriented human-human dialogues in the airline ticket booking domain ( AirTicketBooking ) .
Twitter - Post Conversations in Twitter are carried out by replying or responding to specific posts with short 140 - character messages .
The post length restriction makes
Twitter keep more chat -like interactions than blog posts .
The style of writing used on Twitter is widely varied , highly ungrammatical , and often with spelling errors .
For example , the terms " be4 " , " b4 " , and " bef4 " are always appeared in the Twitter posts to represent the word " before " .
Here , we totally collected about 900 , 000 raw Twitter dialogue sessions .
The majority of conversation sessions are very short ; and the frequencies of conversation session lengths follow a power law relationship as described in ( Ritter et al. , 2010 ) .
For simplicity , in the data preprocessing stage non-English sentences were dropped ; and non-English characters , punctuation marks , and some non-meaning tokens ( such as " & " ) were also filtered from dialogues .
We filtered short Twitter dialogue sessions and randomly sampled 5,000 dialogues ( the numbers of utterances in dialogues rang from 5 to 25 ) to build the Twitter - Post dataset .
AirTicketBooking
The AirTicketBooking corpus consists of a set of task - oriented human-human mandarin dialogues from an airline ticket booking service center .
The manual transcripts of the speech dialogues are utilized in our experiments .
In the dataset , there is always a relative clear structure underlying each dialogue .
A dialogue often begins with a customer 's request about airline ticket issues .
And the service agent always firstly checks the client 's personal information , such as name , phone number and credit card numberm , etc .
Then the agent starts to deal with the client 's request .
We totally collected 1,890 text - based dialogue sessions obtaining about 40,000 conversation utterances with length ranging from 15 to 100 .
3 Dialogue Structure Analysis
We design an undirected generative model based on Boltzmann machine .
As we known , dialogue structure analysis models are always based on an underlying assumption : each utterance in the dialogues is generated from one latent state , which has a causal effect on the words .
For instance , an utterance in AirTicketBooking dataset , " Tomorrow afternoon , about 3 o'clock " corre-sponds to the latent state " Time Information " .
However , by carefully examining words in dialogues we can observe that not all words are generated from the latent states ( Ritter et al. , 2010 ; Zhai and Williams , 2014 ) .
There are some words relevant to a global or background topic shared across dialogues .
For example , " about " and " that " belong to a global ( general English ) topic .
Some other words in a dialogue may be strongly related to the dialogue specific topic .
For example , " cake " , " toast " and " pizza " may appear in a Twitter dialogue with respect to a specific topic , " food " .
From the perspective of generative model , we can also consider that words in a dialogue are generated by the mixture model of latent states , a global / background topic , and a dialogue specific topic .
Therefore , there are three kinds of units in the hidden layer of our proposed model , which are displayed in Figure 1 . h ? is a softmax unit , which indicates the latent state for a utterance .
h ? and h ? represent the general topic , and the dialogue specific topic , respectively .
For the visible layer , we utilize the softmax units to model words in each utterance , which is the same with the approach in RSM ( Hinton and Salakhutdinov , 2009 ) .
In Section 3.2 , We propose a basic model based on Boltzmann machine to formulate each word in utterances of dialogues .
A dialogue can be abstractly viewed as a sequence of latent states in a certain reasonable order .
Therefore , formulating the dependency between latent states is another import issue for dialogue structure analysis .
In our model , we assume that each utterance 's latent state is dependent on its two neighbours .
So there exist connections between each pair of adjacent hidden softmax units in the hidden layer .
The details of the model will be presented in Section 3.3 .
HSM : Hidden Softmax Model ? a ? bias term of h ? a ? bias terms of h ?
W ? weights connecting h ? to V W ? weights connecting h ? to V W ? weights connecting h ? to V F , F s , F e weights between hidden softmax units Table 1 summarizes important notations utilized in this paper .
Before introducing the ultimate learning model for dialogue structure analysis , we firstly discuss a simplified version , Hidden Softmax Model ( HSM ) , which is based on Boltzmann machine and assumes that the latent variables are independent given visible units .
HSM has a twolayer architecture as shown in Figure 2 .
The energy of the state { V , h ? , h ? , h ?
} is defined as follows : E( V , h ? , h ? , h ? ) = ?
( V , h ? ) + ? ( V , h ? ) + ? ( V , h ? ) + C ( V ) , ( 1 ) where ?
( V , h ? ) , ? ( V , h ? ) and ?
( V , h ? ) are sub-energy functions related to hidden variables h ? , h ? , and h ? , respectively .
C( V ) is the shared visible units bias term .
Suppose K is the dictionary size , D r is the r th utterance size ( i.e. the number of words in the r th utterance ) , and R is the number of utterances in the a dialogue .
For each utterance v r ( r = 1 , .. , R ) in the dialogue session we have a hidden variable vector h ?
r ( with size of J ) as a latent state of the utterance , the sub-energy function ?
( V , h ? ) is defined by ?
( V , h ? ) = ?
R r=1 J j=1 Dr i=1 K k=1 h ? rj W ? rjik v rik ?
R r=1 J j=1 h ? rj a ? rj , ( 2 ) where v rik = 1 means the i th visible unit v ri in the r th utterance takes on k th value , h ? rj = 1 means the r th softmax hidden units takes on j th value , and a ? rj is the corresponding bias .
W ? rjik is a symmetric interaction term between visible unit v ri that takes on k th value and hidden variable h ?
r that takes on j th value .
The sub-energy function ? ( V , h ? ) , related to the global general topic of the corpus , is defined by ?
( V , h ? ) = ?
R r=1 Dr i=1 K k=1 h ?
W ? rik v rik ? h ? a ? . ( 3 ) The sub-energy function ?
( V , h ? ) corresponds to the dialogue specific topic , and is defined by ?
( V , h ? ) = ?
R r=1 Dr i=1 K k=1 h ?
W ? rik v rik ? h ? a ? . ( 4 ) W ? rik in Eq. ( 3 ) and W ? rik in Eq. ( 4 ) are two symmetric interaction terms between visible units and the corresponding hidden units , which are similar to W ? rjik in ( 2 ) ; a ? and a ? are the corresponding biases .
C( V ) is defined by C( V ) = ?
R r=1 Dr i=1 K k=1 v rik b rik , ( 5 ) where b rik is the corresponding bias .
The probability that the model assigns to a visible binary matrix V = {v 1 , v 2 , ... , v D } ( where D = R r=1 D r is the dialogue session size ) is P ( V ) = 1 Z h ? , h ? ,h ? exp ( ?E( V , h ? , h ? , h ? ) ) Z = V h ? , h ? ,h ? exp ( ?E( V , h ? , h ? , h ? ) , ( 6 ) where Z is known as the partition function or normalizing constant .
In our proposed model , for each word in the document we use a softmax unit to represent it .
For the sake of simplicity , assume that the order of words in an utterance is ignored .
Therefore , all of these softmax units can share the same set of weights that connect them to hidden units , thus the visible bias term C( V ) and the sub-energy functions ?
( V , h ? ) , ? ( V , h ? ) and ? ( V , h ? ) in Eq. ( 1 ) can be redefined as follows : ? ( V , h ? ) = ?
R r=1 J j=1 K k=1 h ? rj W ? jk vrk ?
R r=1 ( Dr J j=1 h ? rj a ? j ) ( 7 ) ? ( V , h ? ) = ?
K k=1 h ?
W ? k vk ?
Dh ? a ? ( 8 ) ? ( V , h ? ) = ?
K k=1 h ? W ? k vk ? Dh ? a ? ( 9 ) C ( V ) = ?
K k=1 vk b k , ( 10 ) where vrk = Dr i=1 v rik denotes the count for the k th word in the r th utterance of the dialogue , vk = R r=1 vrk is the count for the k th word in whole dialogue session .
D r and D ( D = R r=1 D r ) are employed as the scaling parameters , which can make hidden units behave sensibly when dealing with dialogues of different lengths ( Hinton and Salakhutdinov , 2009 ) .
The conditional distributions are given by softmax and logistic functions : P ( h ? rj = 1| V ) = exp ( K k=1 W ? jk vrk + Dra ? j ) J j =1 exp ( K k=1 W ? j k vrk + Dra ? j ) ( 11 ) P ( h ? = 1 | V ) = ?(
K k=1 W ? k vk + Da ? ) ( 12 ) P ( h ? = 1 | V ) = ?(
K k=1 W ? k vk + Da ? ) ( 13 ) P ( v rik = 1| h ? , h ? , h ? ) = exp ( J j=1 h ? rj W ? jk + h ?
W ? k + h ?
W ? k + b k ) K k =1 exp ( J j=1 h ? rj W ? jk + h ?
W ? k + h ?
W ? k + b k ) , ( 14 ) where ?( x ) = 1 / ( 1 + exp ( ? x ) ) is the logistic function .
HSSM : Hidden Softmax Sequence Model
In this section , we consider the dependency between the adjacent latent states of utterances , and extend the HSM to hidden softmax sequence model ( HSSM ) , which is displayed in Figure 3 .
We define the energy of the state { V , h ? , h ? , h ? } in HSSM as follows : E( V , h ? , h ? , h ? ) = ?
( V , h ? ) + ? ( V , h ? ) + ? ( V , h ? ) + C ( V ) + ?( h ? , h ? ) , ( 15 ) where C( V ) , ? ( V , h ? ) , ? ( V , h ? ) and ?
( V , h ? ) are the same with that in HSM .
The last term ? ( h ? , h ? ) is utilized to formulate the dependency between latent variables h ? , which is defined as follows : ?( h ? , h ? ) = ?
J q=1 h ? s F s q h ?
1q ? J q=1 h ? Rq F e q h ? e ? R?1 r=1 J j=1 J q=1 h ? rj Fjqh ? r+1 , q , ( 16 ) where h ? s and h ?
e are two constant scalar variables ( h ? s ?
1 , h ? e ? 1 ) , which represent the virtual beginning state unit and ending state unit of a dialogue .
F s is a vector with size J , and its elements measure the dependency between h ? s and the latent softmax units of the first utterance .
F e also contains J elements , and in contrast to F s , F e represents the dependency measure between h ?
e and the latent softmax units of the last utterance .
F is a symmetric matrix for formulating dependency between each two adjacent hidden units pair ( h ? r , h ? r+1 ) , r = 1 , ... , R ? 1 . Utterance 1 Utterance 2 Utterance 3 Figure 3 : Hidden softmax sequence model .
A connection between each pair of adjacent hidden softmax units is added to formulate the dependency between the two corresponding latent states .
Parameter Learning Exact maximum likelihood learning in the proposed model is intractable .
" Contrastive Divergence " ( Hinton , 2002 ) can be used for HSM 's learning , however , it can not be utilized for HSSM , because the hidden- to -hidden interaction term , { F , F s , F e } , result in the intractability when obtaining exact samples from the conditional distribution P ( h ? rj = 1 | V ) , r = [ 1 , R ] , j ? [ 1 , J ] .
We use the mean-field variational inference ( Hinton and Zemel , 1994 ; Neal and Hinton , 1998 ; Jordan et al. , 1999 ) and a stochastic approximation procedure ( SAP ) ( Tieleman , 2008 ) to estimate HSSM 's parameters .
The variational learning is utilized to get the data-dependent expectations , and SAP is utilized to estimate the model 's expectation .
The log-likelihood of the HSSM has the following variational lower bound : log P ( V ; ? ) ? h Q( h ) log P ( V , h ; ? ) + H ( Q ) . ( 17 ) Q( h ) can be any distribution of h in theory .
? = { W ? , W ? , W ? , F , F s , F e } ( the bias terms are omitted for clarity ) are the model parameters .
h = {h ? , h ? , h ?
} represent all the hidden variables . H ( ? ) is the entropy functional .
In variational learning , we try to find parameters that minimize the Kullback - Leibler divergences between Q( h ) and the true posterior P ( h|V ; ? ) .
A naive mean-field approach can be chosen to obtain a fully factorized distribution for Q ( h ) : Q( h ) = R r=1 q( h ? ) q(h ? ) q(h ? ) , ( 18 ) where q(h ? rj = 1 ) = ? ? rj , q( h ? = 1 ) = ? ? , q( h ? = 1 ) = ? ? . ? = {? ? , ? ? , ? ? } are the parameters of Q ( h ) .
Then the lower bound on the log-probability log P ( V ; ? ) has the form : log P ( V ; ? ) ? ? ? ( V , ? ? ) ? ? ( V , ? ? ) ? ? ( V , ? ? ) ? C( V ) ? ?(? ? , ? ? ) ? log Z , ( 19 ) where ?
( V , ? ? ) , ? ( V , ? ? ) , ? ( V , ? ? ) , and ? (? ? , ? ? ) have the same forms , by replacing ? with h , as Eqs. ( 7 ) , ( 8 ) , ( 9 ) , and ( 16 ) , respectively .
We can maximize this lower bound with respect to parameters ? for fixed ? , and obtain the meanfield fixed - point equations : ? ? rj = exp ( K k=1 W ? jk vrk + Dra ? j + D j prev + D j next ?
1 ) J j = 1 exp ( K k=1 W ? j k vrk + Dra ? j + D j prev + D j next ? 1 ) , ( 20 ) ? ? = ?(
K k=1 W ? k vk + Da ? ) ( 21 ) ? ? = ?(
K k=1 W ? k vk + Da ? ) , ( 22 ) where D j prev and D j next are two terms relevant to the derivative of the RHS of Eq. ( 19 ) with respect to ? ? rj , defined by D j prev = F s j , r = 1 J q=1 ? ? r?1 , q Fqj , r > 1 D j next = J q=1 Fjq ? ? r+1 , q , r < R. F e j , r = R
The updating of ? can be carried out iteratively until convergence .
Then , ( V , ? ) can be considered as a special " state " of HSSM , thus the SAP can be applied to update the model 's parameters , ? , for fixed ( V , ? ) .
Experiments and Discussions
It 's not easy to evaluate the performance of a dialogue structure analysis model .
In this study , we examined our model via qualitative visualization and quantitative analysis as done in ( Ritter et al. , 2010 ; Zhai and Williams , 2014 ) .
We implemented five conventional models to conduct an extensive comparing study on the two corpora : Twitter -Post and AirTicketBooking .
Conventional models include : LMHMM ( Chotimongkol , 2008 ) , LMH -MMS ( Ritter et al. , 2010 ) , TMHMM , TMHMMS , and TMHMMSS ( Zhai and Williams , 2014 ) .
In our experiments , for each corpus we randomly select 80 % dialogues for training , and use the rest 20 % for testing .
We select three different number ( 10 , 20 and 30 ) of latent states to evaluate all the models .
In TMHMM , TMHMMS and TMH - MMSS , the number of " topics " in the latent states and a dialogue is a hyper-parameter .
We conducted a series of experiments with varying numbers of topics , and the results illustrated that 20 is the best choice on the two corpora .
So , for all the following experimental results of TMHMM , TMHMMS and TMHMMSS , the corresponding topic configurations are set to 20 .
The number of estimation iterations for all the models on training sets is set to 10,000 ; and on held - out test sets , the numver of iterations for inference is set to 1000 .
In order to speed - up the learning of HSSM , datasets are divided into minibatches , each has 15 dialogues .
In addition , the learning rate and momentum are set to 0.1 and 0.9 , respectively .
Qualitative Evaluation Dialogues in Twitter - Post always begin with three latent states : broadcasting what they ( Twitter users ) are doing now ( " Status " ) , broadcasting an interesting link or quote to their followers ( " Reference Broadcast " ) , or asking a question to their followers ( " Question to Followers " ) .
2
We find that structures discoverd by HSSM and LMHMMS with 10 latent states are most reasonable to interpret .
For example , after the initiating state ( " Status " , " Reference Broadcast " , or " Question to Followers " ) , it was often followed a " Reaction " to " Reference Broadcast " ( or " Status " ) , or a " Comment " to " Status " , or a " Question " to " Status " ( " Reference Broadcast " , or " Question to Followers " ' ) etc.
Compared with LMHMMS , besides obtaining similar latent states , HSSM exhibits powerful ability in learning sequential dependency relationship between latent states .
Take the following simple Twitter dialogue session as an example : : rt i like katy perry lt lt we see tht lol : lol gd morning : lol gd morning how u : i'm gr8 n urself : i'm good gettin ready to head out : oh ok well ur day n up its cold out here ... LMHMMS labelled the second utterance ( " lol gd morning " ) and the third utterance ( " lol good morning how u " ) into the same latent state , while HSSM treats them as two different latent states ( Though they both have almost the same words ) .
The result is reasonable : the first " gd morning " is a greeting , while the second " gd morning " is a response .
For AirTicketBooking dataset , the statetransition diagram generated with our model under the setting of 10 latent states is presented in Figure 4 .
And several utterance examples corresponding to the latent staes are also showed in Table 2 .
In general , conversations begin with sever agent 's short greeting , such as " Hi , very glad to be of service . " , and then transit to checking the passenger 's identity information or inquiring the passenger 's air ticket demand ; or it 's directly interrupted by the passenger with booking demand which is always associated with place information .
After that , conversations are carried out with other booking related issues , such as checking ticket price or flight time .
The flowchart produced by HSSM can be reasonably interpreted with knowledge of air ticket booking domain , and it most consistent with the agent 's real workflow of the Ticket Booking Corporation 3 compared with other models .
We notice that conventional models can not clearly distinguish some relevant latent states from each other .
For example , these baseline models always confound the latent state " Price Info " with the latent state " Reservation " , due to certain words assigned large weights in the two states , such as " ? ( discount ) " , and " ? ( credit card ) " etc .
Furthermore , Only HSSM and LMHMMS have dialogue specific topics , and experimental results illustrate that HSSM can learn much better than LMHMMS which always mis-recognize corpus general words as belonging to dialogue specific topic ( An example is presented in Table 3 ) .
Please Waiting
Confirmation
Quantitative Evaluation
For quantitative evaluation , we examine HSSM and traditional models with log likelihood and an ordering task on the held - out test set of Twitter - Post and AirTicketBooking .
3
We hide the corporation 's real name for privacy reasons .
Log Likelihood
The likelihood metric measures the probability of generating the test set using a specified model .
The likelihood of LMHMM and TMHMM can be directed computed with the forward algorithm .
However , since likelihoods of LMHMMS , TMHMMS and TMHMMSS are intractable to compute due to the local dependencies with respect to certain latent variables , Chibstyle estimating algorithms ( Wallach et al. , 2009 ) are employed in our experiments .
For HSSM , the partition function is a key problem for calculating the likelihood , and it can be effectively estimated by Annealed Importance Sampling ( AIS ) ( Neal , 2001 ; Salakhutdinov and Murray , 2008 ) .
Figure 5 presents the likelihood of different models on the two held - out datasets .
We can observe that HSSM achieves better performance on likelihood than all the other models under different number of latent states .
On Twitter - Post dataset our model slightly surpasses LMHMMS , and it performs much better than all traditional models on AirTicketBooking dataset .
Ordering Test Following previous work ( Barzilay and Lee , 2004 ; Ritter et al. , 2010 ; Zhai and Williams , 2014 ) , we utilize Kendall 's ?
( Kendall , 1938 ) as evaluation metric , which measures the similarity between any two sequential data and ranges from ?1 ( indicating a reverse ordering ) to + 1 ( indicating an identical J = 10 J = 20 J = 30 ordering ) .
This is the basic idea : for each dialogue session with n utterances in the test set , we firstly generate all n! permutations of the utterances ; then evaluate the probability of each permutation , and measure the similarity , i.e. Kendall's ? , between the max-probability permutation and the original order ; finally , we average ?
values for all dialogue sessions as the model 's ordering test score .
As pointed out by Zhai et al . ( 2014 ) , it 's however infeasible to enumerate all possible permutations of dialogue sessions when the number of utterances in large .
In experiments , we employ the incrementally adding permutation strategy , as used by Zhai et al . ( 2014 ) , to build up the permutation set .
The results of ordering test are presented in Figure 6 .
We can see that HSSM exhibits better performance than all the other models .
For the conventional models , it is interesting that LMHMMS , TMHMMS and TMHMMSS achieve worse performances than LMHMM and TMHMM .
This is likely because the latter two models allow words to be emitted only from latent states ( Zhai and Williams , 2014 ) , while the former three models allow words to be generated from additional sources .
This also implies HSSM 's effectiveness of modeling distinct information uderlying dialogues .
Discussion
The expermental results illustrate the effectiveness of the proposed undirected dialogue structure analysis model based on Boltzmann machine .
The conducted experiments also demonstrate that undirected models have three main merits for text modeling , which are also demonstrated by Hinton and Salakhutdinov ( 2009 ) , Srivastava et al . ( 2013 ) through other tasks .
Boltzmann machine based undirected models are able to generalize much better than traditional directed generative model ; and model learning is more stable .
Besides , an undirected model is more suitable for describing complex dependencies between different kinds of variables .
We also notice that all the models can , to some degree , capture the sequential structure in the dialogues , however , each model has a special characteristic which makes itself fit a certain kind of dataset better .
HSSM and LMHMMS are more appropriate for modeling the open domain dataset , such as Twitter - Post used in this paper , and the task - oriented domain dataset with one relatively concentrated topic in the corpus and special information for each dialogue , such as AirTicket -Booking .
As we known , dialogue specific topics in HSSM or LMHMMS are used and trained only within corresponding dialogues .
They are crucial for absorbing certain words that have important meaning but do not belongs to latent states .
In addition , for differet dataset , dialogue specific topics may have different effect to the modeling .
Take the Twitter - Post for an example , dialogue specific topics formulate actual themes of dialogues , such as a pop song , a sport news .
As for the AirTicketBooking dataset , dialogue specific topics always represent some special information , such as the personal information , including name , phone number , birthday , etc .
In summary , each dialogue specific topic reflects special information which is different from other dialogues .
The three models , TMHMM , TMHMMS and TMHMMSS , which do not include dialogue specific topics , should be utilized on the task - oriented domain dataset , in which each dialogue has little special or personnal information .
For example , the three models perform well on the the BusTime and TechSupport datasets ( Zhai and Williams , 2014 ) , in which name entities are all replaced by different semantic types ( e.g. phone numbers are replaced by " < phone > " , E-mail addresses are replaced by " < email > " , etc ) .
Conclusions
We develope an undirected generative model , HSSM , for dialogue structure analysis , and examine the effectiveness of our model on two different datasets , Twitter posts occurred in open-domain and task - oriented dialogues from airline ticket booking domain .
Qualitative evaluations and quantitative experimental results demonstrate that the proposed model achieves better performance than state - of - the - art approaches .
Compared with traditional models , the proposed HSSM has more powerful ability of discovering structures of latent states and modeling different word sources , including latent states , dialogue specific topics and global general topic .
According to recent study ( Srivastava et al. , 2013 ) , a deep network model exhibits much benefits for latent variable learning .
A dialogue may actually have a hierarchy structure of latent states , therefore the proposed model can be extended to a deep model to capture more complex structures .
Another possible way to extend the model is to consider modeling long distance dependency between latent states .
This may further improve the model 's performance .
Figure 1 : 1 Figure 1 : Hidden layer that consists of different types of latent variables
Figure 2 : 2 Figure 2 : Hidden Softmax Model .
The bottom layer are softmax visible units and the top layer consists of three types of hidden units : softmax hidden units used for representing latent states , a binary stochastic hidden unit used for representing the dialogue specific topic , and a special binary stochastic hidden unit used for representing corpus general topic .
Upper : The model for a dialogue session containing three utterances .
Connection lines in the same color related to a latent state represent the same weight matrix .
Lower : A different interpretation of the Hidden Softmax Model , in which D r visible softmax units in the r th utterance are replaced by one single multinomial unit which is sampled D r times .
Figure 4 : 4 Figure 4 : Transitions between latent states on AirTicketBooking generated by our HSSM model under the setting of J = 10 latent states .
Transition probability cut-off is 0.10 .
Figure 5 : 5 Figure 5 : Negative log likelihood ( smaller is better ) on held - out datasets of Twitter - Post ( upper ) and AirTicketBooking ( lower ) under different number of latent states J.
Figure 6 : 6 Figure 6 : Average Kendall's ? measure ( larger is better ) on held - out datasets of Twitter - Post ( upper ) and AirTicketBooking ( lower ) under different number of latent states J.
Table 1 : 1 Definition of notations .
Table 2 : 2 Utterance examples of latent states discovered by our model .
Model Top Words HSSM ? , ? , ? , ? , ? , ... ten o'clock , Dong Li ( name ) , Fuzhou ( city ) , Xiamen ( city ) , Shanghai Airlines , ... LMHMMS ? , ? , ? , ? , ? , ... have , ten o'clock , er , Dong Li ( name ) , reserve , ...
Table 3 : 3
One example of dialogue specific topic learned on the same dialogue session with HSSM and LMHMMS , respectively .
For simplicity and readability in consistent , we follow the same latent state names used in ( Ritter et al. , 2010 )
