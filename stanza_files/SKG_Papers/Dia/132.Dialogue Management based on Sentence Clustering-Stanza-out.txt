title
Dialogue Management based on Sentence Clustering
abstract
Dialogue Management ( DM ) is a key issue in Spoken Dialogue System ( SDS ) .
Most of the existing studies on DM use Dialogue Act ( DA ) to represent semantic information of sentence , which might not represent the nuanced meaning sometimes .
In this paper , we model DM based on sentence clusters which have more powerful semantic representation ability than DAs .
Firstly , sentences are clustered not only based on the internal information such as words and sentence structures , but also based on the external information such as context in dialogue via Recurrent Neural Networks .
Additionally , the DM problem is modeled as a Partially Observable Markov Decision Processes ( POMD -P ) with sentence clusters .
Finally , experimental results illustrate that the proposed DM scheme is superior to the existing one .
Introduction Dialogue Management ( DM ) is an important issue in Spoken Dialogue Systems ( SDS ) .
( Paek et al. , 2008 )
Most of the existing studies on DM use the abstract semantic representation such as Dialogue Act ( DA ) to represent the sentence intention .
In ( Bohus et al. , 2009 ) , authors propose a planbased , task - independent DM framework , called RavenClaw , which isolates the domain-specific aspects of the dialogue control logic from domainindependent conversational skills .
( Daubigney et al. , 2010 ) proposes a Kalman Temporal Differences based algorithm to learn efficiently in an offpolicy manner a strategy for a large scale dialogue system .
In ( Emmanuel et al. , 2013 ) , authors propose a scheme to utilize a socially - based reward function for reinforcement learning and use it to fit the user adaptation issue for DM .
( Young et al. , 2013 ) provides an overview of the current state of the art in the development of POMDP - based spoken dialog systems .
( Hao et al. , 2014 ) presents a dialog manager based on a log-linear probabilistic model and uses context -free grammars to impart hierarchical structure to variables and features .
As we know , sentences in human-human dialogues are extremely complicated .
The sentences labeled with the same DA might contain different extra meanings .
Thus , it is difficult for DA to represent the nuanced meaning of sentence in dialogue .
In this paper , we propose a novel DM scheme based on sentence clustering .
The contributions of this work are as follows .
?
Semantic representation of sentence in dialogue is defined as sentence cluster which could represent more nuanced semantic information than DA .
Sentence similarity for clustering is calculated via internal information such as words and sentence structures and external information such as the distributed representation of sentence ( vector ) from Recurrent Neural Networks ( RNN ) .
?
The DM problem is modeled as a POMD -P , where state is defined as sequence of sentence clusters , reward is defined as slot-filling efficiency and sentence popularity , and state transition probability is calculated by the prediction model based on RNN , considering historical dialogue information sufficiently .
The rest of this paper is organized as follows .
In Section 2 , system model is introduced .
Section 3 describes sentence clustering and prediction model based on RNN , and Section 4 models the DM problem as a POMDP .
Extensive experimental results are provided in Section 5 to illustrate the performance comparison , and Section 6 concludes this study .
In this paper , we establish a SDS via humanhuman dialogue corpus , where sentence cluster rather than DA is utilized to represent sentence intention due to its ability of catching finer - grained semantic information .
For example , Fig. 1 shows some dialogue segments in hotel reservation .
Both A1 and A2 could be labeled with " request ( client quantity ) " , because the aims of them are requesting the quantity of clients .
However , A1 has an extra meaning that it is a necessity for the reception to record the quantity of clients , while A2 not , which might lead to different evolutions of dialogues .
Probably , we could add this necessity to the DA corresponding to A1 manually , but it is infeasible for all the sentences to distinguish the fine- grained semantic information by adding abstract symbol to DA .
Thus , in this paper , we automatically cluster all the sentences in dialogues , and utilize sentence clusters to represent sentence intentions , which has more powerful capability to capture semantic information .
The SDS based on sentence clustering could be divided into offline stage and online stage , illustrated in Fig. 2 .
In offline stage : Sentence Clustering :
The sentence similarity is calculated based on not only internal information such as words and sentence structure , but also external information such as the distributed representation from RNN .
And then the sentences in dialogue corpus are clustered into different tiny groups , which will be discussed in section 3 .
We label the dialogues in corpus with the sentence clusters generated in the previous process .
Thus , these labeled dialogues could be utilized to train the optimal dialogue policy with Reinforcement Learning , which will be introduced in section 4 .
In online stage : Automatic Speech Recognition ( ASR ) :
When receiving user voice , ASR module transforms it into text ( Vinyals et al. , 2012 ) .
As there might be ambiguity and errors in ASR , it is difficult to obtain the exact text corresponding to the input voice .
Thus , the distribution over possible texts is used to represent the result of ASR .
Sentence Matching ( SM ) : the function of SM is to establish a mapping from the distribution over possible texts to the distribution over possible sentence clusters .
DM : Based on the distribution of clusters , D- M model updates the belief state in POMDP and selects the optimal action , namely the optimal machine sentence cluster , according to the dialogue policy .
The relevant slots are also filled based on the user and machine sentence clusters .
Sentence Selection :
This module selects the most appropriate sentence from the output machine sentence cluster according to the user profile such as personality character ( Ball et al. , 2000 ) .
Text To Speech ( TTS ) :
This model transforms the selected sentence text into the output voice as a response ( Zen et al. , 2007 ) . Fig. 3 is a human-machine dialogue example in online stage .
and sentence parsing might be used for this calculation . )
Additionally , for DM - based sentence clustering , the sentences that we intend to put into the same cluster are not only the sentences with similar surface meaning , but also the sentences with similar intention ( Semantics or Pragmatics ) , even if they might be different in surface meaning sometimes .
For example , illustrated in Fig. 4 , B4 and B6 are different in surface meaning , but they have similar intention , namely he or she might not provide his or her phone number right now .
Thus , in the sentence clustering for DM modeling , they should be clustered into the same group .
It is difficult to give a high similarity score between B4 and B6 only according to the internal information , but we could observe that the sentences around them in the context are similar .
Thus , external information is also important to the sentence clustering for DM .
In the following , we will discuss the clustering process .
We denote the sentence cluster set as C k = { c k 1 , c k 2 , ? ? ? , c k N k C } , and the dialogue set as D k = { d k 1 , d k 2 , ? ? ? , d k N k D } in the k-th iteration .
Thus , the steps of sentence clustering are : Step 1 : Initially , we only utilize the internal information to cluster the sentences via Affinity Propagation ( AP ) algorithm ( Brendan et al. , 2007 ) and denote the clustering result as C 0 .
If C 0 is used to label the sentences in dialogues , the j-th dialogue could be denoted as a sequence of clusters , namely d 0 j = { c 0 1 , c 0 2 , ? ? ? , c 0 N d j } .
Step 2 : In the k-th iteration , we use cluster set C k to label dialogue set D k .
Step 3 : We utilize RNN to obtain the distributed representation of sentence , illustrated in Fig. 5 .
The input of RNN is sentence cluster in each turn , namely c k t .
The input layer I ( t ) is the one- hot representation of c k t .
( Turian et al. , 2010 )
( The size of I ( t ) is equivalent to C k .
There is only one 1 in I ( t ) corresponding to the c k t position , and other elements are zeros . ) H ( t ) is defined as the hidden layer .
The output layer O ( t ) is the distribution over possible c k t+1 , which could be calculated as O t H t 1 H t 2 H t I t 1 I t 2 I t k t c 1 k t c 2 k t c U V W U U W Figure 5 : RNN for sentence clustering follow .
( Mikolov et al. , 2010 ) { H ( t ) = f ( UI ( t ) + WH ( t ? 1 ) ) O ( t ) = g ( VH ( t ) ) ( 1 ) where f ( x ) = 1 / ( 1 + e ?x ) and g ( x i ) = e x i / ?
Ne i=1 e x i .
The parameters of this RNN could be trained by the Back Propagation Through Time ( BPTT ) algorithm .
( Mikolov , 2012 ) From RNN , we could obtain two significant results : one is the distributed representation ( vectors ) of the sentence clusters ( U ) , which is used for sentence clustering ; the other is the prediction model for sentence clusters , which is used for DM .
Step 4 : we calculate the sentence similarity based on vectors obtained in Step 3 , and combine it with the sentence similarity from internal information ( weighted mean ) , in order to cluster the set C k via AP algorithm , which is denoted as C k+1 .
Step 5 : NC = ? k+1 i=k? k th +2 N i C is defined as the average number of clusters in the last k th iteration .
If ? k+1 i=k? k th +2 N i C ? NC < N th , stop the iteration of clustering , or go to Step 2 , where N th is the variation threshold of quantity of clusters .
Thus , in the last iteration , we get the cluster set C k = { c k 1 , c k 2 , ? ? ? , c k N k C } and prediction model for these sentence clusters .
We divide all the sentences in dialogue corpus into the sentence set spoken by customers and the sentence set spoken by customer service representatives , and then utilize C k to label them respectively , which is denoted as C u = { c u 1 , c u 2 , ? ? ? , c u Nu } , namely the clusters of user sentences , and C m = { c m 1 , c m 2 , ? ? ? , c m
Nm } , namely the clusters of machine sentences .
DM based on Sentence Clustering
The dialogue process mentioned in section 2 could be formulized as follows , illustrated in Fig. 6 .
It is defined X = {x 1 , ? ? ? , x T } as inner ( or exact ) sentence cluster corresponding to the user input in each turn , which is unobservable and x t ?
C u . E = {e 1 , ? ? ? , e T } is defined as the input voice , which is observable to infer x t in each turn .
Y = {y 1 , ? ? ? , y T } is defined as the output cluster of machine , where y t ?
C m .
Thus , the DM problem is to find out the optimal y t according to {e 1 , y 1 , ? ? ? , e t }.
In the following , the DM problem is modeled as a POMDP .
State in the t-th epoch is defined as the sequence of clusters , namely ( Kaelbling et al. , 1998 ) , the belief state updating could be represented as s t = {x t? , y t? , ? ? ? , x t?1 , y t?1 , x t } b t+1 ( s t + 1 ) = Pr {o t +1 |s t+1 , a t } p s t+1 Pr {o t + 1 | b t , a t } ( 3 ) where p s t+1 = ? st ?S Pr {s t+1 |s t , a t } b t ( s t ) .
According to Fig. 5 , Pr {o t +1 |s t+ 1 , a t } could be shown as Pr {o t + 1 |s t+1 , a t } = Pr { o t+1 |s t+1 } = Pr {e t? + 1 , ? ? ? , e t+1 |x t? + 1 , ? ? ? , y t , x t+1 } = Pr {e t? + 1 , ? ? ? , e t+1 |x t? + 1 , ? ? ? , x t+1 } = t+1 ? i=t? +1 Pr {e i |x i } ( 4 ) However , it is difficult to obtain the probability Pr {e t |x t } , as different people have different habits of expression and pronunciation .
Fortunately , Pr {x t |e t } could be estimated based on ASR and SM .
Thus , based on Bayes Rules , we have the following equation .
Pr {e i |x i } = Pr {x i |e i } Pr {e i } Pr {x i } ( 5 ) where Pr {x t } is the prior distribution of x t and could be counted by corpus .
With ( 4 ) and ( 5 ) , ( 3 ) could be rewritten as b t+1 ( s t + 1 ) = ? ? p s t +1 ? t+ 1 ? i=t? +1 Pr {x i |e i } t+1 ? i=t? +1 Pr {x i } ( 6 ) where ? = ? t+ 1 i=t ?
+1 Pr {e i } / Pr {o t+1 | b t , a t } ( 7 ) is a normalization constant .
The reward function is defined as r t ( s t , a t , s t +1 ) = ?
f r f ( st , at , s t + 1 ) + ? p r p ( st , at , s t + 1 ) ( 8 ) where ? f + ? p = 1 and r t ( s t , a t , s t +1 ) ? R. Firstly , r f ( st , at , s t + 1 ) stands for the number of unfilled slots that are filled by the sequence of sentence clusters corresponding to ( s t , a t , s t + 1 ) .
This slot-filling process could be achieved by a classifier trained by the dialogues labeled with sentence clusters and slot-filling information .
( Inputs are cluster sequences , and outputs are filled slots . )
Additionally , r p ( st , at , s t + 1 ) is defined as the normalized quantity of s t+1 conditioned by s t and a t , which could be counted in corpus and stands for the popularity features of human-human dialogues .
Thus , for the belief state , the reward function could be represented as r t ( b t , a t ) = ? s t+1 ?S ? st ?S r t ( s t , a t , s t +1 ) ? Pr ( s t+1 |s t , a t ) b t ( s t ) ( 9 )
Therefore , if we define the policy as a mapping from belief state to action , namely ? ? Z : B ?
A , the POMDP - based DM problem is shown as max ?S E ? [ T ? t=1 ?r t ( b t , a t ) ] s.t. b t+1 ( s t +1 ) = ? t+ 1 ? i=t? +1 Pr{x i |e i } t+1 ? i=t? +1 Pr{x i } ? ? st?S Pr {s t+1 |s t , a t } b t ( s t ) ( 10 ) where ? is the time discount factor and 0 < ? < 1 . This problem is a MDP problem with continuous states , which could be solved by the Natural Actor and Critic algorithm ( Peters et al. , 2008 ) .
Experimental Results
In this section , we compare the performances of the proposed Sentence Clustering based Dialogue Management ( SCDM ) scheme and the existing D-M scheme .
The existing scheme is designed according to ( Young et al. , 2013 ) , where DA is utilized to represent the semantic information of sentence and the dialogue policy is trained via Reinforcement Learning .
It is also an extrinsic ( or endto-end ) evaluation to compare the semantic representation ability between sentence cluster and DA .
In order to compare the performances of the DM schemes , we collect 171 human-human dialogues in hotel reservation and utilize 100 dialogues of them to establish a SDS .
The residual 71 dialogues are used to establish a simulated user for testing ( Schatzmann et al. , 2006 ) .
We define the slots requested from machine to user as " room type " , " room quantity " , " checkin time " , " checkout time " , " client name " and " client phone " .
We also define the slots requested from users to machine as " hotel address = No.95 East St. " , " room type set = single room , double room , and deluxe room " , " single room price = $ 80 " , " double room price = $ 100 " , " deluxe room price = $ 150 " .
The hotel reservation task could be considered as a process of exchanging the slot information between machine and user to some extent .
Fig. 7 illustrates the dialogue turn in the DM schemes , using different training corpus .
Here , we vary the size of training corpus from 10 dialogues to 100 dialogues and define average turn as the average dialogue turn cost to complete the task .
From this picture , we find out that the SCD - M scheme has lower average turn than the existing scheme , partly because the sentence are automatically clustered into many small groups that could represent more nuanced semantic information than DAs , partly because RNN could estimate next sentence cluster according to the vector in hidden layer that contains abundant historical dialogue information .
As the number of sentence clusters is greater than number of DAs , RNN could also solve the scarcity problem and smoothing problem in the predicting process .
Additionally , with the increment of training dialogue size , the average turn
Conclusion
In this paper , we focused on the DM scheme based on sentence clustering .
Firstly , sentence cluster is defined as the semantic representation of sentence in dialogue , which could describe more naunced sentence intention than DA .
Secondly , RNN is established for sentence clustering , where sentence similarity is calculated not only based on the internal information such as words and sentence structure , but also based on the external information such as context in dialogue .
Thirdly , the DM problem is modeled as a POMDP , where the state is defined as the sequence of sentence clusters and the state transition probability is estimated by RN -N , considering the whole information of historical dialogue .
Finally , the experimental results illustrated that the proposed DM scheme is superior to the existing one .
Figure 1 : sentence cluster vs. DA
