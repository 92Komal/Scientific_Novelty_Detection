title
Queens are Powerful too : Mitigating Gender Bias in Dialogue Generation
abstract
Social biases present in data are often directly reflected in the predictions of models trained on that data .
We analyze gender bias in dialogue data , and examine how this bias is not only replicated , but is also amplified in subsequent generative chit- chat dialogue models .
We measure gender bias in six existing dialogue datasets before selecting the most biased one , the multi-player textbased fantasy adventure dataset LIGHT ( Urbanek et al. , 2019 ) , as a testbed for bias mitigation techniques .
We consider three techniques to mitigate gender bias : counterfactual data augmentation , targeted data collection , and bias controlled training .
We show that our proposed techniques mitigate gender bias by balancing the genderedness of generated dialogue utterances , and find that they are particularly effective in combination .
We evaluate model performance with a variety of quantitative methods - including the quantity of gendered words , a dialogue safety classifier , and human assessments - all of which show that our models generate less gendered , but equally engaging chit- chat responses .
Introduction
Machine learning algorithms learn to model patterns present in training datasets .
In particular , they make predictions that directly reflect the harmful societal biases present in training datasets , such as racial bias in sports reports ( Merullo et al. , 2019 ) and political bias in news data .
Such biases are rife in NLP , for example , in learned word embeddings ( Bolukbasi et al. , 2016 ; Brunet et al. , 2018 ; , visual semantic role labeling ( Zhao et al. , 2017 ) , natural language inference ( He et al. , 2019 ) , abusive language classification ( Park et al. , 2018 )
Table 1 : Counts of gendered words in several dialogue datasets .
We report the percent of gendered words ( % gend. words ) as well as the percentage of male - gendered words out of all gendered words ( % male bias ) .
Datasets are arranged in descending order with respect to % male bias .
LIGHT has the most % male bias ; thus we chose it as our main testbed .
coreference resolution ( Zhao et al. , 2018a ) .
Although research into bias in NLP writ large is maturing , bias in dialogue utterances has received somewhat less attention ( Liu et al. , 2019 ; Henderson et al. , 2018 ) .
As realworld use-cases for dialogue agents , such as interactive assistants , are rapidly developing , bias in dialogue models has the very real potential to invade downstream systems and exacerbate existing social biases .
Thus , dialogue debiasing is becoming an increasingly important problem in NLP .
In this work , we foreground dataset bias as a crucial cause of gender bias in dialogue models , and explore ways to address it .
Gender bias has been found in many machine learning datasets , in both images and text ( Stock and Ciss ? , 2017 ; Zhao et al. , 2017 ) .
Here , we analyze several existing dialogue datasets for gender bias ( see Table 1 , and ?3 for more discussion ) for the purpose of finding a good testbed for a deeper dive .
Our analysis revealed that the dataset from the LIGHT text adventure world ( Urbanek et al. , 2019 ) was the most biased in our sample .
LIGHT is also an interesting dataset for measur -Persona Example ( Original LIGHT Dataset ) daughter : I spend most of my time doing household chores .
I want to find meaning in life .
I am energetic and happy .
chief wife : I am the king 's chief wife .
Of all the women that he has married , or who are his concubines , I am the principal one .
I represent the kingdom of my father , who is the king 's biggest ally .
My sons are the ones who will most likely become the king after the death of my husband .
women : I live with my husband and 4 children in the village .
I spend my days washing clothing and cleaning our home .
My husband works for the royal army defending out town .
farmer Bob 's wife : I am farmer Bob 's wife .
I like to take care of all our animals .
I help Farmer Bob everyday on the farm .
mother : I am a mother of eight children .
I live with my family in a cottage in the countryside .
I spend every day tending to the needs of all of my little ones which can be overwhelming , but I always manage to maintain a pleasing disposition and a happy smile .
wife : I am the wife of a farmer .
While I may not be the most attractive woman ever , I am loyal and loving .
My husband is a good man , but only seems to stay with me out of duty .
This room is going to take a while to clean .
You might want to come back later .
merchant :
It is fine I can set my booth up here .
wife : With all the foot traffic ?
merchant :
Yes it should be ok .
wife :
It does n't appear that anyone ever comes up here !
merchant :
Well they will when they know I am here .
wife : I have my doubts but I 'll just go about my cleaning .
merchant : Yea sounds like a good idea .
wife : What is that supposed to mean ?
merchant : I am saying we should both do our jobs .
wife : Do n't take that tone with me !
ing and mitigating gender bias for three additional reasons : first , it has multiple potential sources of bias - not just dialogues , but also characters and personas-second , it was crowdsourced , and thus susceptible to reflecting the gender biases known to be present in crowdworkers ' annotations ( Otterbacher et al. , 2018 ; Barbosa and Chen , 2019 ) , and third , LIGHT 's medieval , fantasy setting might encourage crowdworkers to impart text with their gender biases .
After selecting LIGHT for particular scrutiny , we then explore three bias mitigation techniques , one of which is wholly novel , and another which is novel in its application to dialogue : ( i ) Counterfactual Data Augmentation ( CDA ) ( Hall Maudslay et al. , 2019 ; Zmigrod et al. , 2019 ) , ( ii ) a targeted data collection method , which we refer to as Positive - Bias Data collection , and ( iii ) Bias Controlled text generation .
We show that these techniques are most effective in combination , resulting in dialogue models that produce engaging responses with measurably less gender bias and offensive content ( see ?5 ) .
Models and code are released at https://parl.ai/projects/genderation_ bias / .
Related Work Recently , the NLP community has focused on exploring gender bias in NLP systems ( Sun et al. , 2019 ) , uncovering many gender disparities and harmful biases in algorithms and text ( Cao and Chang and McKeown 2019 ; Costa-juss ?
2019 ; Du et al. 2019 ; Emami et al. 2019 ; Garimella et al. 2019 ; Gaut et al. 2020 ; Habash et al. 2019 ; Hashempour 2019 ; Hoyle et al. 2019 ; Lee et al. 2019a ; Lepp 2019 ; Qian 2019 ; Sharifirad and Matwin 2019 ; Stanovsky et al. 2019 ; O'Neil 2016 ; Blodgett et al. 2020 ; Nangia et al. 2020 ) .
Particular attention has been paid to uncovering , analyzing , and removing gender biases in word embeddings ( Basta et al. , 2019 ; Kaneko and Bollegala , 2019 ; Zhao et al. , , 2018 b Bolukbasi et al. , 2016 ) .
This word embedding work has even extended to multilingual work on gender-marking Williams et al. , 2019 ; Zhou et al. , 2019 ; . Despite these efforts , many methods for debiasing embeddings have only succeeded in hiding word embedding biases as opposed to removing them - making gender debiasing still an open area of research .
Despite the relatively ample literature on gender debiasing for word-level representations , very little work has focused on sentence representations ( Liang et al. , 2020 ; Liu et al. , 2019 ; Lee et al. , 2019 b ) .
Until this point , most debiasing work on sentences mainly focus on measuring bias ( Lee et al. , 2019 b ; . Very few foreground the contribution of training data to gender bias in model outputs .
For example , Kang et al.
collect a corpus of text that is parallel across multiple stylistic categories , one of which is gender .
Closer to our work , Liu et al .
present a test dataset for dialogue and find that models can produce less diverse dialogues when prompted with sentences containing words describing individuals from underrepresented groups .
Still , it differs from our work in that the data was created by combining templates and hand -created lists of wordpairs , rather than using real dialogue data .
Liu et al .
also proposes two methods for debiasing , one of which we also employ ( i.e. , CDA ) , and the other of which extends to sentences a word-embedding post-processing method ( Bolukbasi et al. , 2016 ) that has been shown to be ineffective at removing gender bias ( Gonen and Goldberg 2019 , but see for a more recent , perhaps more effective attempt ) .
Finally - and as a direct extension of this work - Dinan et al . ( 2020 ) decomposes gender bias along three semantic -pragmatic dimensions , and show that train more fine- grained classifiers allow for more accurate classification of dataset gender biases .
The novelty of the present contribution lies in how we measure bias , and in the joint application of our three gender debiasing methods .
Measuring Bias
Before one can mitigate bias , one must first measure it .
As a first pass , we measured the counts of gendered words used ( using a word list from Zhao et al. 2018 b ) , and the percent of those which referred to male characters for six datasets ( Table 1 ) .
We count the number of male and female gendered words in the training sets of several datasets ( LIGHT , ConvAI2 , Reddit , Wizard of Wikipedia , Daily Dialog , Empathetic Dialogues , and ConvAI2 ) .
We use this to calculate the percentage of gendered words out of all words , and the % male bias , that is the percentage of male gendered words among all gendered words in a dialogue .
We find that LIGHT is the most gender imbalanced dataset among all datasets in this table , with a % male bias of 73 % , although others , like Reddit , are close behind .
Since LIGHT was found to be the most gender biased , we qualitatively examine it more closely , and find many biased utterances present in the training data .
For example , the queen persona adheres to negatively stereotyped gender roles when uttering the line I spend my days doing embroidery and having a talk with the ladies .
Another character admires a sultry wench with fire in her eyes .
We conclude from examples like this that presenting crowdworkers with gender biased personas often leads them to create even more gender biased dialogues ( see Table 3 ) : for example , a wife persona contains the text I spend my days cooking and cleaning so my husband will have something to eat when he returns from his work ... , and , in dialogue with a merchant , discusses only her cleaning duties .
The merchant even derisively refers to cleaning as the wife 's job .
This could be an effect of gender stereotype priming ( Blair and Banaji , 1996 ; Steele and Ambady , 2006 ; Oswald , 2008 ; Derks et al. , 2011 ; Verhaeghen et al. , 2011 ) .
Given this , we wonder how much biased character names and personas themselves lead to LIGHT dialogues being more biased than the others .
Thus , we focus on persona- based dialogue text in particular for the remainder of the paper .
Dialogue research has found that , while incorporating personas increases engagingness and improves consistency ( Zhang et al. , 2018 ; Shuster et al. , 2018 ; Mazar ?
et al. , 2018 ; Olabiyi et al. , 2018 ; Li et al. , 2016 b ) , they can also crystallize gender bias ( Clark et al. , 2019 ; Henderson et al. , 2018 ) .
Such bias propagates to subsequently generated conversations .
Crowdworkers in particular might imbue their annotations with their particular gender biases at every stage of dataset creation .
For example , LIGHT ( Urbanek et al. , 2019 ) was created by crowdworkers in stages : crowdworkers were first assigned a character ( with previously crowdsourced names such as " farmer " or " witch " ) , as well as a previously crowdsourced persona , or short textual description of the char- acter .
Then , they were paired up , and tasked with generating a dialogue as those characters .
To determine with more granularity precisely how bias manifests in persona- based dialogue datasets , we investigate the text for ( i ) characters such as fisherman ( Table 1 ) , and ( ii ) personas such as I love fishing ( Table 2 ) .
We ask : ( i ) do crowdworkers generate male and female characters at an equal rate , ( ii ) do they imbue characters ' personas with sexism or undesirable gender biases ?
Bias in Number of Characters .
We first determine whether crowdworkers create an equal number of male and female characters .
To quantify this , we asked annotators on Amazon Mechanical Turk to label the gender of each character name based on its persona description ( choosing neutral if the gender was not explicit ) .
This annotation is possible because many personas include text such as I am a young woman .
1 Since this measurement requires personas , we consider the two personabased dialogue datasets in our sample : LIGHT and ConvAI2 ( Zhang et al. , 2018 ) . LIGHT is highly gender imbalanced : there are over 1.6 times as many male characters as female ones 2 . LIGHT is also considerably less gender- balanced than Conv - AI2 , which has a nearly equal number of male and female gendered personas ( see Table 4 ) .
Bias in Personas .
In addition to the stark underrepresentation of female characters , the medieval setting in LIGHT is likely to encourage crowdworkers to generate dialogues accentuating historical biases and inequalities of the time period ( Bowman , 2010 ; Garcia , 2017 ) .
We investigate the number of references to men or women in the text of personas , as another source of bias .
Take for example , a female persona that contains a gendered reference such as I want to follow in my father 's footsteps rather than in my mother 's .
Although using gendered relational nouns ( Barker , 1992 ; Williams , 2018 ) , such as father , does n't always signal sexism , if female characters are predominantly defined in reference to male characters , it becomes a problem .
We count the appearance of gendered words in personas using the list compiled by Zhao et al . ( 2018 b ) , and find that men are disproportionately referred to in the personas : there are nearly 3x as many mentions of men than women , which suggests that a large number of characters are defined by their relationships to men ( see Table 2 for examples , and Table 4 for counts ) .
Gender bias and sexism are clearly present in many dialogue datasets ( Henderson et al. , 2018 ) , but finding a clear way to define these terms ( and others that categorize unsafe text ) , let alone measure their effects at scale , is very challenging .
For example , the persona for the character girl contains the line I regularly clean and cook dinner ( see Table 2 for more examples ) , which strikes us as stereotypical and sexist , but it might not be noticed by others .
In this paper , we rely on each annotator 's own , subjective , definition ( s ) of the term but aggregate multiple opinions .
Three na?ve annotators examined each persona for unsafe content .
If annotators detected content was ' offensive ' or ' maybe offensive ' , they were asked to select one of four categories - racist , sexist , classist , otherand to provide a reason for their response .
Just over 2 % of personas were flagged by at least one annotator , and these personas and their resulting dialogues were removed .
Mitigating Bias in Generative Dialogue
In this section , we present a general framework for mitigating bias in generative dialogue .
More specifically , we explore data augmentation and other algorithmic methods to mitigate bias in generative Transformer models .
We ( i ) extend counterfactual data augmentation to dialogue
We split test set across the four genderedness bins : F 0 / + M 0 / + . X 0 indicates there are no X-gendered words in the gold response , while X + indicates that there is at least one .
We measure the percent of gendered words generated in the dialogue ( % gend. words ) and the percent of male bias ( % male bias ) , i.e. the percent of malegendered words out of all generated gendered words .
While each of these methods yield some improvement , combining them yields the best control over the genderedness of the utterances while improving the F1 -score .
The orange outline represents the best performing model .
For % Gendered words , lower is better .
For % Male Bias , closer to 50 is better .
For F1 Score , higher is better .
( Hall Maudslay et al. , 2019 ; Zmigrod et al. , 2019 ) following ( Liu et al. , 2019 ) , ( ii ) perform positive data collection by augmenting the existing dataset via targeted data collection with crowdworkers , and lastly , ( iii ) apply controllable generation techniques to gender bias to control how many male and female gendered words models produce .
Counterfactual Data Augmentation
A straightforward solution for gender bias in embeddings is Counterfactual Data Augmentation ( CDA ) ( Hall Maudslay et al. , 2019 ; Zmigrod et al. , 2019 ; Liu et al. , 2019 ) . CDA swaps , say , all instances of grandmother with grandfather , she with he , etc .
We apply this word - based augmentation to dialogue by first copying every dialogue , then swapping all gendered words with their counterpart from the paired list in Zhao et al . ( 2018 b ) .
The augmentation is limited to words on the list , and the swapping is performed automatically .
The model is then retrained on the augmented data .
While CDA is somewhat effective strategy for mitigating bias in word embeddings , this method has several pitfalls : it may result in ungrammatical sentences , and it relies on existing ( and perhaps incomplete ) lists to determine and swap gender .
Positive -Bias Data Collection
To resolve the issues with CDA , we use humans to collect additional dialogue data via a two -pronged Positive -Bias Data Collection ( Pos. Data ) strategy .
We first collect additional personas by having humans ( i ) manually swap the gender of the character name and all gendered references in the character 's persona text ( rather than relying on brittle word lists ) and ( ii ) write additional , diversified personas .
We then use these personas to seed the collection of additional , positively biased dialogue data , which we refer to as Pos.
Data throughout .
New Characters & Personas .
When a dataset contains more male characters and references to male characters than it contains female characters and references to female characters ( see Table 4 ) , we balance existing characters and personas with gender swapping .
For every gendered characterpersona pairing , annotators create a new oppositegendered character - persona pairing for which animate nouns or pronouns are changed , but the rest of the persona remains unchanged .
For example , for every persona describing a male character like a king , annotators will create a new one describing a female character like a queen .
Annotators are instructed to swap the gender ( s ) of other animate references in the text ( e.g. , if an original persona describes a woman in relation to her father , the new male persona will describe a man in relation to his mother ) .
This method ensures that the created sentences will be grammatical , unlike heuristic data augmentation .
However , simply balancing references to men and women is insufficient , as female characters might be specifically described in sexist ways ( see ?3 ) .
As detecting sexism is challenging ( also see ?3 ) , we take our qualitative analysis to be sufficient motivation , and moved to further offset the bias by collecting a new set of interesting and independent female characters .
We primed workers by showing examples of gender underspecified character names like adventurer with personas like I am a woman passionate about exploring a world I have not yet seen .
I embark on ambitious adventures .
We also provided crowdworkers with additional instruction to encourage them to create diverse characters :
We 're looking for strong and diverse descriptions .
Avoid descriptions that could be considered hateful , offensive , or stereotypical .
Even with explicit instruction , annotators created 3 times as many male characters as female characters , revealing the stubbornness of the inherent gender biases of the available crowdworker pool .
We ultimately exclude all male- gendered personas created in this fashion from the new dataset , as including them would worsen the gender balance of the dataset .
Our new dataset is approximately balanced then in the number of male or female characters and in the number of references to male or female characters ( see Table 4 ) .
In total , we add 2,629 new characters and release the data for optional inclusion in the LIGHT dataset .
New Dialogues .
After gender - balancing the personas , we moved on to using the gender- balanced personas to crowdsource additional , hopefully gender - balanced , dialogues .
We selected more female- gendered characters for new dialogue collection , and explicitly instructed annotators to be mindful of gender bias .
In particular , we encouraged them to assume equality -social , economic , political , or otherwise - between genders ( Note : this is uniquely possible with a dataset like LIGHT , which is situated in a fully fictional world ) .
We collected a total of 507 new dialogues containing 6,658 utterances ( approximately 6 % of the original dataset size ) .
We refer to this additional dialogue data as Pos. Data .
Bias Controlled Training Gender bias in dialogue can take the form of imbalanced use of gendered words .
To create dialogue models that can generate an equal number of gendered words , we control model output with We apply conditional training techniques to control gender bias in generative dialogue by learning to associate control tokens with properties of gender bias .
Any general function that takes as input a dialogue utterance and outputs a continuous or discrete value that provides information about gender bias could be used as a control variable .
In our case , prior to training , each dialogue response is binned into one of four bins - F 0 / + M 0 / + - where X 0 indicates that there are zero X-gendered words in the response .
X + indicates the presence of one or more X-gendered word .
The percentage of test set examples that fall into each bin is in Table 5 .
Nouns and adjectives are binned into gendered bins via an aggregation of existing gendered word lists ( Zhao et al. , 2018 b , a ; Hoyle et al. , 2019 ) .
Note that other functions could be used as well , such as a bias classifier ( Dinan et al. , 2020 ) . F 0 M 0 F 0 M + F + M 0 F + M + % of We append a special token to the input that indicates which bin the response falls into .
During Bias Ctrl training , the model should learn to associate the special token with the genderedness of the dialogue response , such that at inference time , we could append different special tokens to control the genderedness of the model output .
For example , a model trained with multiple gender control bins could be set to the gender neutral ( in this case , F 0 M 0 ) setting at inference time , to produce a response containing few ( or no ) gendered words .
Implementation Details Following Urbanek et al. ( 2019 ) , we fine- tune a large , pre-trained Transformer encoder-decoder on the dialogues in the LIGHT dataset for all generation experiments .
Following , we pre-trained on Reddit conversations extracted and obtained by a third party , and made avail - able on pushshift .io .
During pre-training , models learned to generate a comment conditioned on the preceding conversation thread .
All comments that contained URLs or were shorter than 5 characters long were removed , along with child comments , resulting in approximately 2.2 billion training examples .
Similarly during fine-tuning , models were conditioned on the full preceding dialogue history .
All models are 8 - layer encoders , 8 - layer decoders , with 512 dimensional embeddings and 16 attention heads based on the ParlAI transformer implementation ( Miller et al. , 2017 ) .
We decode with a beam search size of 5 .
Results
We train five Transformer models : one baseline trained only on original LIGHT without any mitigation techniques , one Transformer for each of our three methods ( see ?4.1 for CDA , ?4.2 for Positive - Bias Data Collection , and ?4.3 for Bias Control ) , and a final one combining all three methods ( ALL ) that achieves the best results .
Bias is Amplified in Generation .
Figure 1 compares the performance of the various techniques .
We compare our methods to the gold labels from the test set and to the baseline .
To do this , we divide the test set into four genderedness bins ( as defined in ?4.3 ) - F 0 M 0 , F 0 M + , F + M 0 , and F + M + - and calculate : ( i ) the F1 word overlap with the gold response , ( ii ) the percentage of gendered words generated ( % gend. words ) , and ( iii ) the percentage of male - gendered words generated ( relative to the sum total of gendered words generated by the model ) .
We find that Transformer models not only reflect dataset biases , but also they amplify them .
When the model produces gendered words , it generates male - gendered words the vast majority of the time .
Even when the gold label only contains female- gendered words ( F + M 0 ) , it still generates male- gendered words nearly 78 % of the time .
Comparing Debiasing Methods
As shown in Figure 1 , each method improves on the metrics - % gendered words , % male bias , and F1 - over the baseline Transformer , but we find that combining all methods ( ALL ) is most advantageous .
While ALL has more data than CDA and Bias Ctrl , more data alone is not enough - the Positive - Bias Data Collection model does not achieve as strong results as ALL despite also having more data .
Both the Bias Ctrl and ALL models benefit from knowing the data split ( F 0 M 0 , for example ) , and both yield a gender ratio closest to ground truth .
Bias Controlled Training Controls Gendered Words .
Our Bias Ctrl method can control the number of gendered words in generated dialogues ( Figure 2 ) .
We examine the effect of Bias Ctrl by generating responses conditioning the ALL model on each bin .
We observe that changing the bin radically changes the genderedness of generated text with only small differences in overall F1 , which shows that the Bias Ctrl method is efficacious .
Examples of generated text from both the baseline and the ALL model are shown in Table 6 . Further examples are provided in the Appendix in Table 12 .
The baseline model generates malegendered words when the gold response contains no gendered words or only female- gendered words , even generating unlikely sequences such as my name is abigail .
i am the king of this kingdom .
For various methods , we compute the top 20 words generated on the test set ( after removing stop words ) , shown in Appendix Table 8 .
We denote gendered nouns using an asterisk .
Among the top 20 words generated by the baseline , there are only two gendered nouns -knight and king - both male- gendered .
The ALL model generates similar words , but also features queen in its top 20 , another indication that gender is more balanced .
Safety of Generated Text
To further evaluate our techniques , we investigate whether the ALL model generates fewer offensive utterances than ( i ) the baseline , and ( ii ) the human- generated gold labels .
Our bias mitigation techniques have the ancillary benefit of producing models that generate proportionately fewer offensive utterances ; see Table 7 for results .
We use a Transformer - based dialogue safety classifier to classify model- generated utterances as offensive or safe following Liu et al . ( 2019 ) .
The classifier was fine-tuned on an offensive language classification task , and achieves state - of - the - art results .
We apply this classifier to each utterance generated by the ALL model and baseline models on the test set , in addition to the gold ( human generated ) labels from the test set .
The dialogue safety classifier rates our proposed ALL model as less offensive than both the baseline model and the ground truth ( gold ) labels , which argues in favor of the efficacy of our debiasing methods .
Human Evaluation : Bias and Quality
We compare the quality of our debiasing methods using human evaluation .
One might hypothesize that some gender debiasing methods work by replacing contentful words ( e.g. , witch ) with bleached or uninteresting ones ( e.g. , person , thing ) , effectively trading off gender bias with engagingness .
Generative models in particular are well -known to produce generic text ( Li et al. , 2016a ; Fan et al. , 2018 b ) , which is often less engaging .
Overreliance on generic text might increase the chances of biases such as androcentrism , or the propensity of societies to consider men central but women peripheral ( Bem , 1993 ; Bailey et al. , 2020 ) ; in language , male- gendered words often act as a gender-neutral standard ( Bailey et al. , 2019 ) , as in Neil Armstrong 's 1969 quote " one small step for a man , one giant leap for mankind " .
We use the dialogue evaluation system Acute-Eval ( Li et al. , 2019 ) to ask evaluators to compare pairs of conversations from models and decide which model generates ( i ) more biased dialogues and ( ii ) more engaging dialogues .
We collect 100 model conversations with crowdworkers per method .
Then , we compare conversations between a human and the baseline model to conversations between a human and the ALL model with all generations set to the F 0 M 0 gender-neutral control bin .
We found that asking for predictions of speaker gender was more effective than asking about sexism directly .
As shown in Figure 3 , predicting the gender accurately of ALL model generations is more challenging ( significant at p < 0.01 with a t-test ) , but the responses are just as engaging according to human evaluators .
We conclude our proposed methods are able to help mitigate gender bias without degrading dialogue quality .
Conclusion
We analyze gender bias in dialogue data and resulting model generations for models trained on dialogue data .
We propose general purpose techniques for reducing gender bias in generated text .
The methods described in this paper combine data augmentation , positive - bias data collection , and bias controlled training .
We note that our results show that data collection techniques help mitigate issues , so when it is possible , bias should be considered at the earliest stages of a project .
Newly collected or constructed datasets should consider how to carefully craft the collection to mitigate bias issues from the very start .
When this is not possible , however , such as in the case of using real-world data or a dataset that already exists , the techniques presented in this paper are shown to be effective at reducing gender bias .
They are especially effective when combined , producing less gendered , more balanced , safer utterances that maintain the engagingness of the dialogue .
