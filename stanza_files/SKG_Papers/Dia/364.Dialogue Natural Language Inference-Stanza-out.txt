title
Dialogue Natural Language Inference
abstract
Consistency is a long standing issue faced by dialogue models .
In this paper , we frame the consistency of dialogue agents as natural language inference ( NLI ) and create a new natural language inference dataset called Dialogue NLI .
We propose a method which demonstrates that a model trained on Dialogue NLI can be used to improve the consistency of a dialogue model , and evaluate the method with human evaluation and with automatic metrics on a suite of evaluation sets designed to measure a dialogue model 's consistency .
Introduction
A long standing issue faced by dialogue models is consistency ( Li et al. , 2016 ; Vinyals et al. , 2015 ; Zhang et al. , 2018 ) .
An example from ( Vinyals et al. , 2015 ) shows a two -round dialogue in which their neural sequence model first responds to what is your job ?
with i'm a lawyer , then responds to what do you do ?
with i'm a doctor .
Even when inconsistencies are relatively rare and semantically plausible , they are jarring , and because semantic plausibility is not enough to root them out , preventing them is challenging .
One approach to increasing the consistency of a chit-chat dialogue model was proposed in ( Zhang et al. , 2018 ) , where the dialogue agent was given a set of personal facts describing its character ( a persona ) and produces utterances that reflect the persona .
The intended outcome is that the agent produces utterances consistent with its given persona .
However , these models still face the consistency issue , as shown in Figure 1 . Separately , the framework of Natural Language Inference ( NLI ) ( Bowman et al. , 2015 ; Dagan et al. , 2006 ; Maccartney and Manning , 2009 ) involves learning a mapping between a sentence pair and an entailment category .
It is hypothesized that the NLI task is a proxy for general goals in natural language processing , such as language understanding ( Bowman et al. , 2015 ; Williams et al. , 2018 ) .
Thus , the NLI task has been used for learning general sentence representations ( Conneau et al. , 2017 ) and for evaluating NLP models ( Poliak et al. , 2018a ; Wang et al. , 2018 ) , with the expectation that such models will be useful in downstream tasks .
Despite this expectation , leveraging an NLI model for a downstream task remains an underexplored research direction .
An NLI model may improve downstream task performance if properly used , while downstream tasks may yield new datasets or identify issues with existing NLI models , thus expanding the NLI research domain .
In this paper , we reduce the problem of consistency in dialogue to natural language inference .
We first create a dataset , Dialogue NLI , 1 which contains sentence pairs labeled as entailment , neutral , or contradiction .
Then , we demonstrate that NLI can be used to improve the consistency of dialogue models using a simple method where utterances are re-ranked using a NLI model trained on Dialogue NLI .
The method results in fewer persona contradictions on three evaluation sets .
The evaluation sets can be used independently to automatically evaluate a dialogue model 's persona consistency , reducing the need for human evaluation .
We discuss several future research directions involving this approach .
Dialogue Consistency and Natural Language Inference First , we review the dialogue generation and natural language inference problems as well as the notions of consistency used throughout .
Dialogue Generation Dialogue generation can be framed as next utterance prediction , in which an utterance ( a sequence of tokens representing a sentence ) u t+1 is predicted given a conversation prefix u ?t .
A sequence of utterances is interpreted as a dialogue between agents .
For instance , an alternating two -agent dialogue which starts with agent A and ends with agent B is written as u A 1 , u B 2 , u A 3 , u B 4 , ... , u B T . Persona- Based Dialogue
In persona- based dialogue , each agent is associated with a persona , P A and P B .
An utterance is now predicted using the conversation prefix u ?t and the agents own persona , e.g. P A for agent A .
It is assumed that an agent 's utterances are conditionally dependent on its persona , which can be interpreted as the utterances being representative of , or reflecting , the persona .
A typical approach for representing the persona is to use a set of sentences P = {p 1 , ... , p m }.
Consistency
A consistency error , or contradiction , occurs when an agent produces an utterance that contradicts one of their previous utterances .
Similarly , a persona consistency error , or persona contradiction , occurs when an agent produces an utterance that contradicts a subset of its persona .
A contradiction may be a clear logical contradiction , e.g. I have a dog vs .
I do not have a dog , but in general is less clearly defined .
As a result , in addition to logical contradictions , we interpret a consistency error as being two utterances not likely to be said by the same persona .
For instance , " i'm looking forward to going to the basketball game this weekend ! " vs. " i do n't like attending sporting events " , as well as " i'm a lawyer " vs. " i'm a doctor " would be viewed here as con-tradictions , although they are not strict logical inconsistencies .
Similarly , a persona consistency error is interpreted here as an utterance which is not likely to be said given a persona described by a given set of persona sentences , in addition to logical contradictions .
Natural Language Inference Natural Language Inference ( NLI ) assumes a dataset D = {( s 1 , s 2 ) i , y i } N i=1 which associates an input pair ( s 1 , s 2 ) to one of three classes y ?
{entailment , neutral , contradiction} .
Each input item s j comes from an input space S j , which in typical NLI tasks is the space of natural language sentences , i.e. s j is a sequence of words ( w 1 , ... , w K ) where each word w k is from a vocabulary V .
The input ( s 1 , s 2 ) are referred to as the premise and hypothesis , respectively , and each label is interpreted as meaning the premise entails the hypothesis , the premise is neutral with respect to the hypothesis , or the premise contradicts the hypothesis .
The problem is to learn a function f NLI ( s 1 , s 2 ) ?
{ E , N , C} which generalizes to new input pairs .
Reducing Dialogue Consistency to NLI Identifying utterances which contradict previous utterances or an agent 's persona can be reduced to natural language inference by assuming that contradictions are contained in a sentence pair .
That is , given a persona P A = {p A 1 , ... , p A m } for agent A and a length - T dialogue u A 1 , u B 2 , ...u
A T ?1 , u B T , it is assumed that a dialogue contradiction for agent A is contained in an utterance pair ( u A i , u A j ) , and a persona contradiction is contained in a pair ( u A i , p A k ) .
Similarly , we assume that entailments and neutral interactions , defined in Section 3 , are contained in sentence pairs .
We do not consider relationships which require more than two sentences to express .
Under this assumption , we can use a natural language inference model f NLI to identify entailing , neutral , or contradicting utterances .
Section 3 proposes a dialogue-derived dataset for training f NLI , and Section 4 proposes a method which incorporates f NLI with a dialogue model for next utterance prediction .
Dialogue NLI Dataset
The Dialogue NLI dataset consists of sentence pairs labeled as entailment ( E ) , neutral ( N ) , or contradiction ( C ) .
Sentences Sentences originate from a two -agent persona- based dialogue dataset .
A dialogue between agents A and B consists of a sequence of utterances u A 1 , u B 2 , u A 3 , u B 4 , ... , u B T , and each agent has a persona represented by a set of persona sentences {p A 1 , ... , p A m A } and {p B 1 , ... , p B m B }.
The Dialogue NLI dataset consists of ( u i , p j ) and ( p i , p j ) pairs 2 from the Persona - Chat dataset ( Zhang et al. , 2018 ) 3 .
Triple Generation
In order to determine labels for our dataset , we require human annotation of the utterances and persona sentences in PersonaChat , as the original dataset does not contain this information .
We perform such annotation by first associating a human-labeled triple ( e 1 , r , e 2 ) with each persona sentence , and a subset of all the utterances , detailed in 3.2 .
Each triple contains the main fact conveyed by a persona sentence , such as ( i , have pet , dog ) for the persona sentence I have a pet dog , or a fact mentioned in an utterance , such as No , but my dog sometimes does .
Persona sentences and utterances are grouped by their triple ( e.g. see Figure 2 ) , and pairs ( u , p ) and ( p , p ) are defined as entailment , neutral , or contradiction based on their triple according to the criteria below .
For examples and summary , we refer readers to Tables 1 - 2 . Entailment
Each unique pair of sentences that share the same triple are labeled as entailment .
Neutral Neutral pairs are obtained with three different methods .
First , a miscellaneous utterance is a ( u , p ) pair of which u is not associated with any triple .
This includes greetings ( how are you today ? ) and sentences unrelated to a persona sentence ( the weather is ok today ) , so such utterances are assumed to be neutral with respect to persona sentences .
The second method , persona pairing , takes advantage of the fact that each ground - truth persona is typically neither redundant nor contradictory .
A persona sentence pair ( p , p ) is first selected from a persona if p and p do not share the same triple .
Then each sentence associated with the same triple as p is paired with each sentence associated with the same triple as p .
Lastly , we specify relation swaps ( r , r ) for certain relations ( see Appendix A.2 ) whose triples are assumed to represent independent facts , such as have vehicle and have pet .
A sentence pair , whose first sentence is associated with a triple ( ? , r , ? ) and whose second sentence has triple ( ? , r , ? ) , is labeled as neutral .
See Table 1 for an example .
Contradiction
We obtain contradictions using three methods .
See Figure 2 for an example .
First , the relation swap method is used by specifying contradicting relation pairs ( r , r ) ( see Appendix A.2 ) , such as ( like activity , dislike ) , then pairing each sentence associated with the triple ( e 1 , r , e 2 ) with each sentence associated with ( e 1 , r , e 2 ) .
Similarly , an entity swap consists of specifying relations , e.g. , physical attribute , that would yield a contradiction when the value of e 2 is changed to a different value e 2 , e.g. , short ? tall ( see Appendix A.3 ) .
Sentences associated with ( e 1 , r , e 2 ) are then paired with sentences associated with ( e 1 , r , e 2 ) .
Finally , a numeric contradiction is obtained by first selecting a sentence which contains a number that appears in the associated triple ( see Table 1 ) .
A contradicting sentence is generated by replacing the sentence 's numeric surface form with a different randomly sampled integer in the number or text form .
Triple Annotation
Each persona sentence is annotated with a triple ( e 1 , r , e 2 ) using Amazon Mechanical Turk task .
We first define a schema consisting of category relation category rules , such as person have pet animal , where the relation comes from a fixed set of relation types R , listed in Appendix A.1 .
Given a sentence , the annotator selects a relation r from a drop-down populated with the values in R .
The annotator then selects the categories and values of the entities e 1 and e 2 using drop-downs that are populated based on the schema rules .
An optional drop-down contains numeric values for annotating entity quantities ( e.g. , 3 brothers ) .
If selected , the numeric value is concatenated to the front of the entity value .
The annotator can alternatively input an out-of-schema entity value in a text-box .
Using this method , each of the 10,832 persona sentences is annotated with a triple ( e 1 , r , e 2 ) , where r ? R , e 1 ? E 1 , and e 2 ? E 2 .
Here E 1 is the set of all annotated e 1 from the drop-downs or the text- box , and E 2 is similarly defined .
Finally , utterances are associated with a triple as follows .
Let p be a persona sentence with triple ( e 1 , r , e 2 ) .
We start with all utterances , U , from agents that have p in their persona .
An utterance u ?
U is then associated with the triple ( e 1 , r , e 2 ) and persona sentence p when e 2 is a sub-string of u , or word similarity 4 sim ( u , p ) ? ? is suitably large .
Statistics
Table 2 summarizes the dataset and its underlying data types .
The label , triple , and data type are supplied as annotations for each sentence pair .
We additionally create a gold-standard test set ( Test Gold ) by crowdsourcing three label annotations for each example in the test set .
We keep each test example for which two or more annotators agreed with its dataset label .
All sentences in Dialogue NLI were generated by humans during the crowdsourced dialogue collection process of the Persona - Chat dataset ( Zhang et al. , 2018 ) .
The resulting sentence pairs are thus drawn from a natural dialogue domain that differs from existing NLI datasets , which are either drawn from different domains such as image captions or created using synthetic templates ( Bowman et al. , 2015 ; Demszky et al. , 2018 ; Khot et al. , 2018 ; Marelli et al. , 2014 ; Poliak et al. , 2018 b ; Wang et al. , 2018 ; Williams et al. , 2018 ) .
Consistent Dialogue Agents via Natural Language Inference
We now present a method which demonstrates that natural language inference can be used to improve the consistency of dialogue agents .
Candidate utterances are re-ranked based on whether the candidate is predicted to contradict a persona sentence .
If the NLI model predicts that a candidate contradicts a persona sentence , the candidate 's score is penalized , with the penalty weighted by the NLI model 's confidence 5 scaled by a constant .
Specifically , assume a dialogue model f dialogue ( P , u ?t , U ) ? ( s 1 , s 2 , ... , s | U | ) and a Dialogue NLI model f NLI ( u , p ) ? { E , N , C}. Given a persona P = {p 1 , ... , p m } , previous utterances u ?t , and a set of candidate nextutterances U , the dialogue model outputs a ranked list of scores s 1 , s 2 , ... , s | U | corresponding to next-utterance candidates u 1 , u 2 , ... , u | U | .
The NLI model is then run on each ( u i , p j ) pair , predicting a label y i , j ? { E , N , C} with confidence c i , j .
A contradiction score is computed for each candidate as : New candidate scores are then computed as s contradict i = ? ? ? 0 , if y i , j = C ? p j ?
P max j:y i , j =C c i , j , s re-rank i = s i ? ?( s 1 ? s k ) s contradict i ( 1 ) and the candidates are sorted according to s re-rank .
Hyper-parameters ? and k control the NLI model 's influence in re-ranking .
For example , if the top candidate has a contradiction score of 1.0 , then with ? = 1 , it will be moved to the k'th position in the ranking .
? = 0 corresponds to no re-ranking .
Experiments 5.1 Experiment 1 : NLI Models
Many recently proposed NLI models can be categorized into sentence encoding based methods of the form f MLP ( g enc ( s 1 ) , g enc ( s 2 ) ) , and attention - based methods of the form f MLP ( g attn ( s 1 , s 2 ) ) ( Lan and Xu , 2018 ) .
We thus choose and train representative models of each type which have achieved competitive performance on existing NLI benchmark datasets .
For the sentence encoding method , we use In-ferSent ( Conneau et al. , 2017 ) , which encodes a sentence using a bidirectional LSTM followed by max-pooling over the output states .
As the representative attention - based method we use the enhanced sequential inference model ( ESIM , ( Chen et al. , 2017 ) ) , which computes an attention score for each word pair .
We also report results from a model trained and evaluated using the hypothesis sentence only ( In-ferSent Hyp .
Only ) ( Gururangan et al. , 2018 ; Poliak et al. , 2018 c ) on Dialogue NLI ( InferSent SNLI ) , and a model which returns the most common class from the Dialogue NLI training set ( Most Common Class ) .
Results
Table 3 shows the performance of the two NLI models and three baselines on the Dialogue NLI validation and test sets .
The test performance of ESIM ( 88.2 % ) and InferSent ( 85.68 % ) is similar to the performance reported on the existing SNLI dataset ( 88.0 % ( Chen et al. , 2017 ) and 85.5 % ( Conneau et al. , 2017 ) , respectively ) , while the results on the Dialogue NLI gold test set ( 92.45 % , 89.96 % ) are higher .
As in Table 3 , however , an InferSent model trained on SNLI performs poorly when evaluated on the proposed Dialogue NLI ( 47.03 % ) .
This is likely due to a mismatch in sentence distributions between SNLI , which is derived from image captions , and Dialogue NLI , whose sentences more closely resemble downstream dialogue applications .
The hypothesisonly performance ( 51.52 % ) is lower than the hypothesis-only baseline for SNLI ( 69.00 % ( Poliak et al. , 2018 c ) ) , and shows that using information from both the utterance and persona sentence is necessary to achieve good performance on Dialogue NLI .
ESIM 's reasonably strong performance on Dialogue NLI suggests that the model may be useful in a downstream task - a claim which we verify in Experiment 5.1 .
However , there is also room for improvement .
In particular , we report the performance of a model which takes the ground -truth triples as input instead of sentences .
As shown in the last row of Table 3 , each sentence 's underlying triple contains sufficient information to achieve near-perfect accuracy ( 99.69 % ) .
We also show ESIM 's accuracy by data type on Test Gold in Table 5 , along with example mispredictions in Table 4 .
The accuracies and examples suggest that the NLI model could be improved further .
Experiment 2 : Consistency in Dialogue
This experiment evaluates the effect of the reranking method from Section 4 on the dialogue model 's persona consistency .
model and the Dialogue NLI model .
Experiment Setup
For the dialogue model we train a key-value memory network ( Zhang et al. , 2018 ) on the Persona - Chat dataset , which uses persona sentences and the conversation prefix as context .
This model achieved the best performance on Persona - Chat in ( Zhang et al. , 2018 ) .
We train the model using ParlAI ( Miller et al. , 2017 ) on the personachat : self original task , using the hyper-parameters given for the KVMemnnAgent in the ConvAI2 competition .
For the NLI model we use the ESIM model trained on Dialogue NLI , based on the results of Experiment 5 .
To study the effect of re-ranking on persona consistency , we form evaluation sets which contain next-utterances which are likely to yield persona contradiction or entailment , as follows .
Evaluation Sets
Each example is formed by first finding a next-utterance u t+1 in the Persona - Chat validation set which has an associated triple ( e 1 , r , e 2 ) of interest , e.g. ( i , like music , country ) .
If a sentence in the agent 's profile P has triple ( e 1 , r , e 2 ) , we form the validation example ( P , u ?t , u t + 1 ) .
Figure 3 shows an example .
Each example is associated with candidates U , consisting of the ground - truth utterance u t+1 , 10 entailment candidates with the same triple as u t+1 , 10 contradicting candidates with a different triple than that of u t+1 , and 10 random candidates .
The dialogue model must avoid ranking a contradicting candidate highly .
Specifically , suppose the ground - truth nextutterance u t+1 is associated with triple ( e 1 , r , e 2 ) , e.g. , ( i , have pet , dog ) .
Entailment candidates are utterances u from the validation or training sets such that u is associated with triple ( e 1 , r , e 2 ) .
Since by construction a sentence in the profile also has triple ( e 1 , r , e 2 ) , these candidates entail a profile sentence .
A contradicting candidate is an utterance associated with a specified contradicting triple ( e 1 , r , e 2 ) , e.g. , ( i , not have , dog ) .
We construct three evaluation sets , Haves , Likes , and Attributes using this process .
Metrics
We introduce variants of the ranking metric Hits@k , called Contradict@k and Entail@k .
Contradict@k measures the proportion of top-k candidates returned by the model which contradict candidates , averaged over examples .
This measures the propensity of a model to highly rank contradictions .
Contradiction@1 is the proportion of consistency errors made by the model .
For this metric lower values are better , in contrast to Hits@k .
Entail@k measures the ment candidates share the same underlying triple as the ground - truth next utterance , so this metric rewards highly ranked candidates that convey similar meaning and logic to the ground -truth utterance .
Thus it can be interpreted as a more permissive version of Hits@k .
Results
Table 6 shows re-ranking results on the three evaluation sets ( ? = 1.0 , k = 10 ) .
The NLI re-ranking improves all three metrics on all the evaluation sets .
Overall dialogue performance improves , as measured by Hits@1 .
The NLI reranking substantially reduces the number of contradicting utterances predicted by the model , and increases the number of utterances which entail a profile sentence , as seen in the Contradict@1 and Entail@1 scores .
Figure 3 shows an example dialogue with candidates , contradictions predicted by the NLI model , and the corresponding re-ranked candidates .
Experiment 3 : Human Evaluation
This experiment evaluates the effect of the proposed NLI re-ranking method on a dialogue model 's consistency , where consistency is judged by human annotators in an interactive personabased dialogue setting .
Experiment Setup
We use ParlAI ( Miller et al. , 2017 ) which integrates with Amazon Mechanical Turk for human evaluation .
A human annotator is paired with a model , and each is randomly assigned a persona from 1,155 persona sets .
The human and model are then asked to make a conversation of at least either five or six turns ( randomly decided ) .
After the conversation , the annotator assigns three scores to the conversation , described below .
Each annotator is allowed to participate in at most ten conversations per model , and we collect 100 conversations per model .
Two models are evaluated : the same key -value memory network used in Experiment 5.1 without re-ranking ( KV - Mem ) , and with re-ranking ( KV - Mem + NLI ) .
Scoring and Calibration Following a conversation , an annotator is shown the conversation and the model 's persona , and assigns three scores : an overall score of how well the model represented its persona ( { 1,2,3,4,5 } ) , a marking of each model utterance that was consistent with the model 's persona ( { 0,1} ) , and a marking of each model utterance that contradicted a previous utterance or the model 's persona ( { 0,1 } ) .
We use Bayesian calibration to adjust for annotator bias , following ( Kulikov et al. , 2018 ) .
We assume a model with observed scores S ij and latent variables M i for the unobserved score of model i and B j for the bias of annotator j .
We then estimate the posterior mean and variance for the unobserved scores given the observed scores .
We use Pyro ( Bingham et al. , 2018 ) and the no-u-turn sampler ( Hoffman and Gelman , 2014 ) for posterior inference .
See Appendix C for details .
Results
Table 7 shows the human evaluation results .
The natural language inference re-ranking improves all the metrics , notably the fine- grained consistency score ( 0.27 vs. 0.35 ) and contradiction score ( 0.25 vs. 0.16 ) .
The results are consistent with the conclusions from the automatic evaluation in Experiment 5.1 .
Conclusion
In this paper , we demonstrated that natural language inference can be used to improve performance on a downstream dialogue task .
To do so , we created a new dialogue - derived dataset called Dialogue NLI , a re-ranking method for incorporating a Dialogue NLI model into a dialogue task , and an evaluation set which measures a model 's persona consistency .
The dataset offers a new domain for natural language inference models , and suggests avenues such as devising alternative methods for using natural language inference components in downstream tasks .
Future work may also incorporate contradiction information into the dialogue model itself , and extend to generic contradictions .
tity swap was done using all WordNet antonym pairs in the personality trait and person attribute entity categories , as well as the swaps ( { blonde} , { brunette} ) , ( { large} , { tiny } ) , ( { carnivore , om-nivore} , { vegan , vegetarian} ) , ( { depressed } , { happy , cheerful} ) , ( { clean} , { dirty } ) where each entity in the left set is swapped with each entity in the right set .
