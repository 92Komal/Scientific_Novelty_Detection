title
Do n't Say That !
Making Inconsistent Dialogue Unlikely with Unlikelihood Training
abstract
Generative dialogue models currently suffer from a number of problems which standard maximum likelihood training does not address .
They tend to produce generations that ( i ) rely too much on copying from the context , ( ii ) contain repetitions within utterances , ( iii ) overuse frequent words , and ( iv ) at a deeper level , contain logical flaws .
In this work we show how all of these problems can be addressed by extending the recently introduced unlikelihood loss ( Welleck et al. , 2019a ) to these cases .
We show that appropriate loss functions which regularize generated outputs to match human distributions are effective for the first three issues .
For the last important general issue , we show applying unlikelihood to collected data of what a model should not do is effective for improving logical consistency , potentially paving the way to generative models with greater reasoning ability .
We demonstrate the efficacy of our approach across several dialogue tasks .
Introduction
Open-ended tasks such as dialogue reveal a number of issues with current neural text generation methods .
In more strongly grounded tasks such as machine translation and image captioning , current encoder-decoder architectures provide strong performance , where mostly word - level decisions are often taken correctly by the model .
However , critical failings are exposed in less constrained generation : reliance on repetitive copying and overuse of frequent words , and an inability to maintain logical coherence .
The former shows the learning objective is faulty in that it cannot match simple statistics of the training data , while the latter touches more to the heart of artificial intelligence :
Work done while at Facebook AI Research ( FAIR ) .
these models do not understand what they are saying .
For example , Figure 1 shows how the 345 Mparameter GPT2 model ( Radford et al. , 2019 ) can give high probability to contradictory generations .
In this work , we show how the recently introduced unlikelihood objective ( Welleck et al. , 2019a ) can be generalized to remedy these problems .
Unlikelihood is a technique developed for removal of repetition in language model completions , and works by adding an extra term to the objective that forces repetitions to have low probability , alleviating the degenerative problems highlighted in .
In fact , unlikelihood can be seen as a much more general framework , as we will see .
We first generalize unlikelihood to a different domain : dialogue , where we measure statistics of the training distribution in terms of contextual copies , within -utterance repeats , and vocabulary usage .
We then develop loss functions that control these statistics , providing improved metrics on several tasks .
Secondly , we show how the same tools can be used to address deeper semantic issues in such models .
By leveraging existing natural language inference ( NLI ) data ( Welleck et al. , 2019 b ) as supervision against poor quality generations , we train models that assign low probability to generating incoherent and contradictory text .
Overall , our approach yields more consistent dialogue models across several axes , and provides a promising framework for further advances .
Code and pre-trained models will be made available .
?
Dialogue Unlikelihood Training Dialogue Generation Dialogue generation consists in predicting an utterance y = ( y 1 , . . . , y |y | ) given a context x = {s 1 , . . . , s k , u 1 , . . . , u t } that consists of initial context sentences s 1:k ( e.g. , scenario , knowledge , personas , etc. ) followed by dialogue history utterances u 1:t from speakers who take consecutive turns .
Likelihood Training Given a dataset D = { ( x ( i ) , y ( i ) ) } derived from a collection of humanhuman interactions , the standard approach to generative training for dialogue tasks is maximum likelihood estimation ( MLE ) , that minimizes : L ( i ) MLE ( p ? , x ( i ) , y ( i ) ) = ?
|y ( i ) | t=1 log p ? ( y ( i ) t |x ( i ) , y ( i ) <t ) , where x ( i ) is a gold context ( dialogue history and initial context sentences ) and y ( i ) is a gold nextutterance , and y ( i ) t is the t-th token of y ( i ) . Likelihood - based ( greedy or beam ) decoding applied after training a model with this objective yields sequences with statistics that do not match the original human training sequence distribution .
Unlikelihood Training
To control for such distribution mismatches , we employ the unlikelihood loss ( Welleck et al. , 2019a ) , generalizing it to our setting , and developing a particular form of the loss function for each type of mismatch .
The general form of the unlikelihood loss penalizes a set of tokens C t at each time-step , L ( i ) UL ( p ? , C 1:T , x , y ) = ? | y | t=1 yc?
Ct ?(y c ) log ( 1 ? p ? ( y c |x , y < t ) ) , where C t ?
V is a subset of the vocabulary , and ?(y c ) is a candidate - dependent scale that controls how much the candidate token should be penalized .
The overall objective in unlikelihood training then consists of mixing the likelihood and unlikelihood losses , L ( i ) ULE = L ( i ) MLE + ?L ( i) UL , ( 1 ) ?
https://parl.ai/projects/dialogue_ unlikelihood / where ? ?
R is the mixing hyper-parameter .
Likelihood tries to model the overall sequence probability distribution , while unlikelihood corrects for known biases .
It does this via the set of negative candidates C t calculated at each step t , where we are free to select candidate generation functions depending on the biases to be mitigated .
Likelihood pushes up the probability of a gold token y ( i ) t while unlikelihood pushes down the probability of negative candidate tokens y c ?
C t .
In Welleck et al. ( 2019a ) the context x consists of a ground - truth sequence ( x = x ( i ) ) , the target y is either a ground - truth sequence ( y = y ( i ) ) or a model- generated sequence ( y = ? ) , and the pertoken scale parameter ?( y c ) is 1 .
In this paper , we demonstrate how unlikelihood can be used as a general framework by applying it to the dialogue domain .
We show how varying the contexts x , targets y , candidates C and scaling ?
can be used to improve the coherence and language modeling quality of dialogue models .
To do this , we now consider the different biases we wish to mitigate , and construct a specific unlikelihood loss for each in turn .
Repetition and Copying Generative dialogue models are known to both ( i ) rely too much on copying existing context knowledge or dialogue history ; and ( ii ) repeat themselves within individual utterances .
To address this with unlikelihood , we define two types of negative candidate tokens which either appear in a repeating n-gram from the context or from the generated label itself , C context- copy t = {y t } y t ?
repeat context n-gram ? otherwise , C label- repeat t = {y t } y t ?
repeating label n-gram ?
otherwise , where y t is a token in a repeating context n-gram when y t is part of an n-gram that already appeared in the context tokens x , and is in a repeating label n-gram when y t is part of an n-gram that already appeared in y <t .
Given a ground - truth context x ( i ) , we apply these two forms of unlikelihood to a model- generated sequence ?( i ) .
In summary , we either apply the per-example loss L ( i ) UL ( p ? , C context-copy 1:|y | , x ( i ) , ?( i ) ) for controlling context copies , or L ( i ) UL ( p ? , C label- repeat 1:|y | , x ( i ) , ?( i ) ) . for controlling label repeats .
We also consider mixing the two losses to mitigate both issues .
Vocabulary Usage Neural sequence models trained with maximum likelihood generate sequences with token distributions that differ from those of human text ( Dinan et al. , 2020 ; .
In particular , these models tend to produce high frequency tokens too often and low frequency tokens too rarely , where frequency is defined by the human token distribution .
We address this with unlikelihood by penalizing tokens according to the mismatch between the model and ground -truth unigram distributions .
Specifically , we first maintain an empirical estimate of the model 's unigram distribution p model ( y t ) and the human distribution p * ( y t ) : p model ( y t ) = count (y t ) | Y | , where Y is a collection of token predictions on a subset of training data D ( e.g. the preceding k = 256 batches ) , and count(y t ) is the number of occurrences of y t in Y .
This is computed using model sequences ( y = ? ) , defining Y as the collection of all tokens in all ?.
We wish to push down the probability of tokens appearing too often , i.e. when p model ( y t ) > p * ( y t ) .
For the unlikelihood loss , each step 's candidate is thus the current token , C identity t = {y t } , and each token 's unlikelihood loss is scaled according to the mismatch between the approximated model and human distributions , ?(y c ) = p model( yc ) log p model ( y c ) p * ( y c ) .
The unlikelihood loss for a token y c is non-zero when the token occurs more often in the model 's estimated unigram distribution .
In summary , the resulting per-example loss is L ( i ) UL ( p ? , C identity 1:|y | , x ( i ) , y ) where y is a model- generated sequence .
Contradictions Neural generation models appear fluent , especially when pre-trained on large datasets , but are still poor at understanding the language they produce .
That is , they can produce logically or factually inaccurate , or contradicting statements ( Welleck et al. , 2019 b ; Zhang et al. , 2018 ; Hayashi et al. , 2019 ; Petroni et al. , 2019 ) .
Here , we show how the unlikelihood objective can be used to train such models to assign low probability to inconsistent and contradictory utterances .
To do so , we assume the existence of training data of both positive and negative examples of coherent behavior .
There is a raft of recent largescale , high quality data that can be massaged into this form , from natural language inference ( NLI ) tasks ( Bowman et al. , 2015 ; Williams et al. , 2018 ; Welleck et al. , 2019 b ) to commonsense reasoning tasks ( Zellers et al. , 2019 ; Qin et al. , 2019 ) .
Two collections of data can be derived from the labels of such a supervised task : D + = { ( x ( i ) , y ( i ) + ) } , D ? = { ( x ( i ) , y ( i ) ? ) } , where D + is coherent behavior , e.g. neutral or entailing data in NLI , and D ? is incoherent behavior , e.g. contradictions .
In general , many forms of this type of data can be collected , not just NLI , and it is also not necessary for the contexts x ( i ) to overlap as we have written here .
Standard likelihood training can then be performed on coherent data D + , while the unlikelihood objective is applied to D ? as we wish to push down the probability of generating the incoherent response y ? given a context x .
That is , given an incoherent pair ( x , y ? ) we use the loss L UL ( p ? , C identity 1:|y | , x , y ? ) , where we penalize each token in the target ( C identity t = {y ? t } ) .
Hence , the loss makes generating the contradicting sentences less likely .
Related Work
Our work provides new applications of unlikelihood training ( Welleck et al. , 2019a ) , showing that unlikelihood offers a general framework for improving generative models , and in particular dialogue models .
Outside of that work , the use of negative training in dialogue retrieval , rather than generation , has been previously extensively studied , see e.g. ( Humeau et al. , 2019 ; Nugmanova et al. , 2019 ) .
In the area of generative dialogue , a number of works have focused on improving the standard likelihood training approach .
Closer to our work is that of He and Glass ( 2019 ) which developed the approach of negative training to prevent generic and malicious responses in dialogue models .
In terms of improving repetition and specificity , a recent alternative approach is that of control ( Fan et al. , 2018 ; Ficler and Goldberg , 2017 ; Ghazvininejad et al. , 2017 ; See et al. , 2019 ) .
Nucleus sampling can help to remove generic or repetitive utterances at the expense of accuracy , but was shown to be inferior to beam blocking , which in turn was shown to be inferior to unlikelihood in Welleck et al . ( 2019 a ) .
In terms of dialogue coherence , Welleck et al . ( 2019 b ) showed that retrieval , but not generative models , could be improved with NLI as a rescorer , while Yang et al . ( 2018 ) multi-tasked with NLI .
The work of Gabriel et al . ( 2019 ) has also studied improving narrative flow with a discriminative rescorer , but in that case for generated language .
In our work , the improvements are tightly integrated into the training of the model itself .
Experiments
In all of our experiments we employ a large pre-trained seq2seq Transformer ( Vaswani et al. , 2017 ) as our base model , which we then fine-tune for particular tasks with the objectives outlined in Section 2 and specified in each experiment below .
Following previous work ( Humeau et al. , 2019 ) , we pre-train our model on dialogue data , using a previously existing Reddit dataset extracted and obtained by a third party and made available on pushshift . io , training to generate a comment conditioned on the full thread leading up to the comment , spanning ?
2200 M training examples .
Our Transformer model consists of an 8 layer encoder , 8 layer decoder with 512 - dimensional embeddings and 16 attention heads , and is based on the ParlAI implementation of Miller et al . ( 2017 ) .
The model was trained with a batch size of 3072 sequences for approximately 3 M updates using a learning rate of 5e - 4 , and an inverse square root scheduler .
This pre-training took approximately two weeks using 64 NVIDIA V100s .
Repetition and Copying We use the ConvAI2 persona- based dialogue ( Zhang et al. , 2018 ) knowledge - grounded dialogue ( Dinan et al. , 2019 ) and ELI5 long-form question answering datasets to evaluate the effect of using unlikelihood to reduce copying and repetition in model generated utterances .
On each dataset , we fine - tune the pre-trained pushshift .
io Reddit model , then evaluate by generating nextutterances for dialogue contexts from the test set ( or validation in ConvAI2 , as the test set is hidden ) .
We use greedy decoding in our main experiments for simplicity and scalability , but we also obtained similar results with beam search , shown in Appendix A .
To measure label repetition in a sequence y , we use the portion of duplicate n-grams : and report the metric averaged over the examples .
Context repetition increases when the model ' copies ' n-grams from the context .
To quantify language modeling quality , we use standard perplexity and F1 metrics .
We use the pre-trained model fine-tuned with MLE as the baseline , and compare it against the pre-trained model fine-tuned with copy and repetition unlikelihood ( ?2.1 ) .
Results Results for ConvAI2 are shown in Table 1 .
We see that training unlikelihood using only -contexts or only - labels reduces their corresponding metrics dramatically compared to the MLE baseline .
Training with both context - and label- repetition unlikelihood reduced both context repetitions ( by 69 % , .0352 vs. .1131 ) and label repetitions ( by 89 % , .0023 vs .0210 ) compared to the MLE baseline , much closer to human levels , while keeping perplexity essentially constant .
Comparatively , the Wizard of Wikipedia MLE baseline experiences a much larger problem with context repetition , due to its tendency to copy grounded knowledge verbatim ( Table 2 ) .
Results for ELI5 , shown in Table 3 , show that it has an especially large problem with label repetition , and that label - unlikelihood is able to reduce the repetitions by 91 % ( .055 vs .617 ) , while significantly boosting F1 ( .130 to .182 ) .
Figures 2 and 3 show perplexity as a function of label and context repeats respectively using unlikelihood on ELI5 .
The parameter ?
can clearly control repeats smoothly , with only very high values resulting in increased perplexity .
Human Evaluation Finally , we perform a human evaluation using the same pairwise evaluation scheme as performed on ELI5 , comparing the MLE baseline to UL ( Label only ) which asks :
Which response answers the question better ?
The evaluators are asked to consider both the readability and accuracy of the answer .
Results are given in Figure 4 ( left ) , showing a statistically significant improvement over the baseline ( 150 trials , two tailed binomial test , p < 0.01 ) .
Further details are given in Appendix C.
Vocabulary Usage
We evaluate the ability of vocabulary unlikelihood ( ?2.2 ) to reduce the mismatch between model and human token distributions .
We use the ConvAI2 dataset , where our baseline is again trained using maximum likelihood .
Starting with the baseline model , we then fine-tune several models using vocab unlikelihood at logarithmically interpolated values of ? ? [ 1 , 1000 ] .
We partition the vocabulary into ' frequent ' , ' medium ' , ' rare ' , and ' rarest ' using the human unigram distribution computed with the ConvAI2 training set , corresponding to the sorted token sets whose cumulative mass accounts for the top 40 % , the next 30 % , the next 20 % and the final 10 % of usage , respectively .
We evaluate a model by generating utterances given contexts from the Con-vAI2 validation set , and compute the fraction of tokens within each class .
Results
Figure 5 shows how the vocabulary distribution obtained after unlikelihood training is affected by the choice of mixing hyperparameter ? ( Eq. 1 ) : it can smoothly transition between the human training distribution and the MLE trained distribution ( ' Baseline ' ) , which is far from the human one .
Table 4 compares the MLE baseline with unlikelihood with increasing ?
values in terms of distribution and F1 score .
The vocabulary unlikelihood fine-tuning shifts probability mass from the over-represented frequent words towards underrepresented medium and rare words , with the effect strengthening as ? increases .
At a small cost to perplexity and F1 , the unlikelihood tuning reduced the overuse of common tokens by 9 points , matching the human rate , while improving the production of rare tokens by 3 percentage points .
Human Evaluation Finally , we perform a human evaluation using the ACUTE - EVAL framework ( Li et al. , 2019 ) , comparing the MLE baseline to UL for various ?.
First , 252 human- bot conversations ( 8 turns each ) are collected , and then models are compared pairwise by asking the question :
Who would you prefer to talk to for a long conversation ?
For these experiments we compare with both methods generating using beam with context blocking of trigrams .
Results are given in Figure 4 ( right ) , showing a statistically significant improvement over the baseline according to humans ( two tailed binomial test , p < 0.01 ) .
Further details are given in Appendix C.
Contradictions
We use the dialogue natural language inference ( NLI ) task of Welleck et al . ( 2019 b ) to obtain labeled non-contradicting and contradicting dialogue sentence pairs to use in unlikelihood training ( ?2.3 ) .
Dialogue NLI contains utterances labeled as entailing ( E ) , neutral ( N ) or contradiction ( C ) , given a premise that is either a persona sentence ( an initial context sentence describing a dialogue agent 's personality ) or another dialogue utterance from the Persona - Chat dialogue task ( Zhang et al. , 2018 ) .
We show examples from Dialogue NLI in Figure 6 .
The original data consists of sentence pairs ( s 1 , s 2 ) along with a label ( E , N , or C ) , and was constructed by developing a schema and employing crowdworkers to label utterances with relation triples .
The labels are then inferred from the triple representation .
We first transform the original classification dataset into a form useful for unlikelihood training of a generative dialogue model .
We consider two setups : ( i ) a two utterance generation task ; and ( ii ) a full dialogue generation task .
Two Utterance Generation Task
We adapt the initial dialogue NLI dataset by using entailing and neutral training sentence pairs as plausible positive utterances , and contradicting pairs as negatives .
That is , if a pair ( s 1 , s 2 ) from Dialogue NLI has label E or N , the example ( x , y ) = ( s 1 , s 2 ) is added to D + , otherwise ( label C ) it is added to D ? .
We consider two types of entailment : entailing sentence pairs that appear together in a dialogue in the original Persona - Chat dataset and are therefore natural ( ' entailment ' ) , and those that only entail via their triple relations ( ' triple-entailment ' ) .
The latter are more challenging , noisier targets .
Evaluation is performed by measuring the test set perplexity over the four target label types , where contradictions should have relatively higher perplexity .
We additionally evaluate a selection accuracy task , where for each test example there are two candidate responses : a positive and a negative ( contradicting ) statement .
The candidate response with the lowest perplexity is considered to be the model 's selection , and we measure the selection success rate .
Evaluation is broken down by positive type ( entailment , triple- entailment , neutral ) .
Dataset statistics are given in Table 5 . Full Dialogue Task
To evaluate in a more realistic setup that involves full dialogue rather than a single utterance , we take full Persona - Chat dialogues ( Zhang et al. , 2018 ) similar to Figure 6 , and map back the dialogue NLI data to provide positive and negative continuations of the dialogue .
We consider continuations as either triple entailing utterances , neutral utterances or contradictions - where the relation triple is used to match the existing persona or dialogue turns by the same speaker to induce the label .
That is , an example ( x , y ) consists of a dialogue history x = {p 1 , . . . , p k , u 1 , . . . , u t } and utterance y = s 2 , where ( s 1 , s 2 ) is a sentence pair from Dialogue NLI , and at least one sentence in x has the same relation triple as s 1 .
When the pair ( s 1 , s 2 ) is labeled as E or N in Dialogue NLI , the example ( x , y ) is added to D + , and otherwise it is added to D ? .
Results
Our MLE baseline obtains a perplexity of 11.4 , in line with current best systems on this task .
Unfortunately , despite being good on such standard metrics , our baseline models fail at our coherence task .
As seen in Table 6 for the two utterance task , the perplexity of contradicting utterances ( 12.5 ) is on average lower than for neutral ( 36.7 ) or triple -entailing utterances ( 17.5 ) , although it is higher than entailing utterances .
We believe this is due to contradicting utterances having high word overlap with the premise utterance , coupled with an inability to judge incoherence .
Viewed as a selection task between utterances , picking the utterance with the lowest perplexity , this means the selection rates of non-contradicting utterances are very low , e.g. picking neutral utterances over contradicting utterances only 18 % of the time .
Even fully entailing utterances are only picked 73 % of the time .
Similar results are found on the full dialogue task as well , see Table 7 .
Unlikelihood training brings large improvements in coherence metrics , whilst minimally impacting overall dialogue perplexity .
After applying unlikelihood , perplexity for contradicting utterances has a clear signature , with very large av- Neutral ) . Selection
Accuracy measures how often the model assigns lower perplexity to the positive candidate than to the negative candidate in the pair .
Top two rows : for standard maximum likelihood models , the perplexity of contradicting utterances is lower compared to neutral or triple-entailing utterances ( albeit higher compared to entailing utterances ) , showing partial failure at the coherence task .
Bottom row : NLI Unlikelihood training yields large improvements on all coherence metrics , while minimally increasing overall perplexity .
We did too but working in real estate for 12 years .
( E ) I have been working as a real estate sucked up a lot of time agent for the past 12 years .
3.9 3.8 ( C ) We did too but working in real estate for fifteen years sucked up a lot of time .
3.1 17.6 Figure 7 : Example perplexities of a baseline maximum likelihood model ( L MLE ) and our unlikelihood trained model ( L UL ) when generating the provided hypotheses , given the premise .
The maximum likelihood trained model assigns high probability ( low perplexity ) to contradictory generations , while unlikelihood does not .
erage values compared to entailing or neutral utterances , e.g. 248.9 vs. 9.1 for contradict vs. entail on the two utterance task .
This converts to corresponding large increases in selection accuracy across all types on both tasks , e.g. , an increase from 18 % to 78 % on neutral statements on the two utterance task , and from 37.4 % to 69.8 % on the full dialogue task .
Some example model predictions are given in Figure 7 , comparing the MLE baseline and unlikelihood model perplexities of generating the given hypotheses .
The likelihood model cannot differentiate between contradicting and entailing statements easily , while there are large perplexity differences for the unlikelihood model in these cases .
Conclusion Generating consistent and coherent human-like dialogue is a core goal of natural language research .
We studied several aspects that contribute to that goal , defined metrics to measure them , and proposed algorithms that improve them , mitigating some of the failings of maximum likelihood training , the current dominant approach .
Our method defines objective functions under the umbrella of unlikelihood : during training , we wish to make inconsistent dialogue unlikely by lowering the probability of such events occurring .
This makes generative models repeat themselves less , copy the context less , and use more rare words from the vocabulary - closer to matching human statistics .
Further , utilizing supervised datasets with labeled coherent and incoherent utterances and applying unlikelihood yields measurably improved levels of coherence with respect to the aspect measured , in this case contradiction .
Future work could apply this same technique with other supervised data , e.g. correcting causal or commonsense reasoning errors ( Zellers et al. , 2019 ; Qin et al. , 2019 ) .
