title
Efficient Context and Schema Fusion Networks for Multi-Domain Dialogue State Tracking
abstract
Dialogue state tracking ( DST ) aims at estimating the current dialogue state given all the preceding conversation .
For multi-domain DST , the data sparsity problem is a major obstacle due to increased numbers of state candidates and dialogue lengths .
To encode the dialogue context efficiently , we utilize the previous dialogue state ( predicted ) and the current dialogue utterance as the input for DST .
To consider relations among different domain-slots , the schema graph involving prior knowledge is exploited .
In this paper , a novel context and schema fusion network is proposed to encode the dialogue context and schema graph by using internal and external attention mechanisms .
Experiment results show that our approach can outperform strong baselines , and the previous state - of - the - art method ( SOM - DST ) can also be improved by our proposed schema graph .
Introduction Dialogue state tracking ( DST ) is a key component in task - oriented dialogue systems which cover certain narrow domains ( e.g. , booking hotel and travel planning ) .
As a kind of context- aware language understanding task , DST aims to extract user goals or intents hidden in human-machine conversation and represent them as a compact dialogue state , i.e. , a set of slots and their corresponding values .
For example , as illustrated in Fig. 1 , ( slot , value ) pairs like ( name , huntingdon marriott hotel ) are extracted from the dialogue .
It is essential to build an accurate DST for dialogue management ( Young et al. , 2013 ) , where dialogue state determines the next machine action and response .
Recently , motivated by the tremendous growth of commercial dialogue systems like Apple Siri , Microsoft Cortana , Amazon Alexa , or Google Assistant , multi-domain DST becomes crucial to help Figure 1 : An example of multi-domain dialogues .
Utterances at the left side are from the system agent , and utterances at the right side are from a user .
The dialogue state of each domain is represented as a set of ( slot , value ) pairs .
users across different domains Eric et al. , 2019 ) .
As shown in Fig. 1 , the dialogue covers three domains ( i.e. , Hotel , Attraction and Taxi ) .
The goal of multidomain DST is to predict the value ( including NONE ) for each domain-slot pair based on all the preceding dialogue utterances .
However , due to increasing numbers of dialogue turns and domainslot pairs , the data sparsity problem becomes the main issue in this field .
To tackle the above problem , we emphasize that DST models should support open-vocabulary based value decoding , encode context efficiently and incorporate domain-slot relations :
1 . Open-vocabulary DST is essential for realworld applications Ren et al. , 2019 ) , since value sets for some slots can be very huge and variable ( e.g. , song names ) .
2 . To encode the dialogue context efficiently , we attempt to get context representation from the previous ( predicted ) dialogue state and the current turn dialogue utterance , while not concatenating all the preceding dialogue utterances .
3 . To consider relations among domains and slots , we introduce the schema graph which contains domain , slot , domain-slot nodes and their relationships .
It is a kind of prior knowledge and may help alleviate the data imbalance problem .
To this end , we propose a multi-domain dialogue state tracker with context and schema fusion networks ( CSFN - DST ) .
The fusion network is exploited to jointly encode the previous dialogue state , the current turn dialogue and the schema graph by internal and external attention mechanisms .
After multiple layers of attention networks , the final representation of each domain-slot node is utilized to predict the corresponding value , involving context and schema information .
For the value prediction , a slot gate classifier is applied to decide whether a domain-slot is mentioned in the conversation , and then an RNN - based value decoder is exploited to generate the corresponding value .
Our proposed CSFN - DST is evaluated on Mul-tiWOZ 2.0 and MultiWOZ 2.1 benchmarks .
Ablation study on each component further reveals that both context and schema are essential .
Contributions in this work are summarized as : ?
To alleviate the data sparsity problem and enhance the context encoding , we propose exploiting domain-slot relations within the schema graph for open-vocabulary DST .
?
To fully encode the schema graph and dialogue context , fusion networks are introduced with graph - based , internal and external attention mechanisms .
?
Experimental results show that our approach surpasses strong baselines , and the previous state - of - the - art method ( SOM - DST ) can also be improved by our proposed schema graph .
Related Work Traditional DST models rely on semantics extracted by natural language understanding to predict the current dialogue states ( Young et al. , 2013 ; Henderson et al. , 2014d ; Sun et al. , 2014 b , a ; Yu et al. , 2015 ) , or jointly learn language understanding in an end-to- end way ( Henderson et al. , 2014 b , c ) .
These methods heavily rely on hand-crafted features and complex domain-specific lexicons for delexicalization , which are difficult to extend to new domains .
Recently , most works about DST focus on encoding dialogue context with deep neural networks ( such as CNN , RNN , LSTM - RNN , etc. ) and predicting a value for each possible slot ( Mrk ?i? et al. , 2017 ; Xu and Hu , 2018 ; Zhong et al. , 2018 ; Ren et al. , 2018 ) . Multi-domain DST
Most traditional state tracking approaches focus on a single domain , which extract value for each slot in the domain Henderson et al. , 2014a ) .
They can be directly adapted to multi / mixed -domain conversations by replacing slots in a single domain with domain-slot pairs ( i.e. domain-specific slots )
Zhang et al. , 2019 ; . Despite its simplicity , this approach for multi-domain DST extracts value for each domain-slot independently , which may fail to capture features from slot co-occurrences .
For example , hotels with higher stars are usually more expensive ( price range ) .
Predefined ontology - based DST Most of the previous works assume that a predefined ontology is provided in advance , i.e. , all slots and their values of each domain are known and fixed ( Williams , 2012 ; Henderson et al. , 2014a ) .
Predefined ontology - based DST can be simplified into a value classification task for each slot ( Henderson et al. , 2014 c ; Mrk?i? et al. , 2017 ; Zhong et al. , 2018 ; Ren et al. , 2018 ; . It has the advantage of access to the known candidate set of each slot , but these approaches may not be applicable in the real scenario .
Since a full ontology is hard to obtain in advance ( Xu and Hu , 2018 ) , and the number of possible slot values could be substantial and variable ( e.g. , song names ) , even if a full ontology exists .
Open-vocabulary DST
Without a predefined ontology , some works choose to directly generate or extract values for each slot from the dialogue context , by using the encoder- decoder architecture or the pointer network Ren et al. , 2019 ; Le et al. , 2020 ) .
They can improve the scalability and robustness to unseen slot values , while most of them are not efficient in context encoding since they encode all the previous utterances at each dialogue turn .
Notably , a multi-domain dialogue could involve quite a long history , e.g. , MultiWOZ dataset contains about 13 turns per dialogue on average .
Graph Neural Network Graph Neural Network ( GNN ) approaches ( Scarselli et al. , 2009 ; Veli?kovi ?
et al. , 2018 ) aggregate information from graph structure and encode node features , which can learn to reason and introduce structure information .
Many GNN variants are proposed and also applied in various NLP tasks , such as text classification , machine translation ( Marcheggiani et al. , 2018 ) , dialogue policy optimization etc .
We introduce graph- based multi-head attention and fusion networks for encoding the schema graph .
Problem Formulation
In a multi-domain dialogue state tracking problem , we assume that there are M domains ( e.g. taxi , hotel ) involved , D = {d 1 , d 2 , ? ? ? , d M }. Slots included in each domain d ?
D are denoted as a set S d = {s d 1 , s d 2 , ? ? ? , s d | S d | }. 1
Thus , there are J possible domain-slot pairs totally , O = { O 1 , O 2 , ? ? ? , O J } , where J = M m=1 | S dm |.
Since different domains may contain a same slot , we denote all distinct N slots as S = {s 1 , s 2 , ? ? ? , s N } , where N ? J. A dialogue can be formally represented as { ( A 1 , U 1 , B 1 ) , ( A 2 , U 2 , B 2 ) , ? ? ? , ( A T , U T , B T ) } , where A t is what the agent says at the t-th turn , U t is the user utterance at t turn , and B t denotes the corresponding dialogue state .
A t and U t are word sequences , while B t is a set of domain-slot-value triplets , e.g. , ( hotel , price range , expensive ) .
Value v tj is a word sequence for j-th domain-slot pair at the t-th turn .
The goal of DST is to correctly predict the value for each domain-slot pair , given the dialogue history .
Most of the previous works choose to concatenate all words in the dialogue history , [ A 1 , U 1 , A 2 , U 2 , ? ? ? , A t , U t ] ,
Context and Schema Fusion Networks for Multi-domain DST
In this section , we will introduce our approach for multi-domain DST , which jointly encodes the current dialogue turn ( A t and U t ) , the previous dialogue state B t?1 and the schema graph G by fusion networks .
After that , we can obtain contextaware and schema- aware node embeddings for all J domain-slot pairs .
Finally , a slot-gate classifier and RNN - based value decoder are exploited to extract the value for each domain-slot pair .
The architecture of CSFN - DST is illustrated in Fig. 3 , which consists of input embeddings , context schema fusion network and state prediction modules .
Input Embeddings
Besides token and position embeddings for encoding literal information , segment embeddings are also exploited to discriminate different types of input tokens .
( 1 ) Dialogue Utterance
We denote the representation of the dialogue utterances at t-th turn as a joint sequence , X t = [ CLS ] ?
A t ? ; ?U t ? [ SEP ] , where [ CLS ] and [ SEP ] are auxiliary tokens for separation , ? is the operation of sequence concatenation .
As [ CLS ] is designed to capture the sequence embedding , it has a different segment type with the other tokens .
The input embeddings of X t are the sum of the token embeddings , the segmentation embeddings and the position embeddings ( Vaswani et al. , 2017 ) , as shown in Fig. 3 . ( 2 ) Previous Dialogue State
As mentioned before , a dialogue state is a set of domain-slot-value triplets with a mentioned value ( not NONE ) .
Therefore , we denote the previous dialogue state as B t?1 = [ CLS ] ? R 1 t?1 ? ? ? ? ? R K t?1 , where K is the number of triplets in B t?1 .
Each triplet d-s-v is denoted as a sub-sequence , i.e. , R = d ? -? s ? -? v. The domain and slot names are tokenized , e.g. , price range is replaced with " price range " .
The value is also represented as a token sequence .
For the special value DONTCARE which means users do not care the value , it would be replaced with " dont care " .
The input embeddings of B t?1 are the sum of the token , segmentation and position embeddings .
Positions are re-enumerated for different triplets .
( 3 ) Schema Graph
As mentioned before , the schema graph G is comprised of M domain nodes , N slot nodes and J domain-slot nodes .
These nodes are arranged as G = d 1 ? ? ? ? ? d M ? s 1 ? ? ? ?s N ?o 1 ? ? ?o J . Each node embedding is initialized by averaging embeddings of tokens in the corresponding domain / slot / domain-slot .
Positions embeddings are omitted in the graph .
The edges of the graph are represented as an adjacency matrix A G whose items are either one or zero , which would be used in the fusion network .
To emphasize edges between different types of nodes can be different in the computation , we exploit node types to get segment embeddings .
Context and Schema Fusion Network
At this point , we have input representations H G 0 ? R |G|?dm , H Xt 0 ? R | Xt|?dm , H B t?1 0 ? R | B t?1 |?dm , where |.| gets the token or node number .
The context and schema fusion network ( CSFN ) is utilized to compute hidden states for tokens or nodes in X t , B t?1 and G layer by layer .
We then apply a stack of L context - and schema-aware self-attention layers to get final hidden states , H G L , H Xt L , H B t?1 L .
The i-th layer ( 0 ? i < L ) can be formulated as : H G i+1 , H Xt i + 1 , H B t?1 i+ 1 = CSFNLayer i ( H G i , H Xt i , H B t?1 i ) 4.2.1 Multi-head Attention
Before describing the fusion network , we first introduce the multi-head attention ( Vaswani et al. , 2017 ) where z i ?
R 1?d model and Z ? R | Z|?d model .
For each vector y i , we can compute an attention vector c i over Z by using H heads as follows : e ( h ) ij = ( y i W ( h ) Q ) ( zjW ( h ) K ) dmodel / H ; a ( h ) ij = exp ( e ( h ) ij ) | Z| l=1 exp ( e ( h ) il ) c ( h ) i = | Z | j=1 a ( h ) ij ( zjW ( h ) V ) ; ci = Concat ( c ( 1 ) i , ? ? ? , c ( H ) i ) WO where 1 ? h ?
H , W O ? R d model ?d model , and W ( h ) Q , W ( h ) K , W ( h ) V ? R d model ?( d model / H ) .
We can compute c i for every y i and get a transformed matrix C ? R | Y |?d model .
The entire process is denoted as a mapping MultiHead ? : C = MultiHead ? ( Y , Z ) ( 1 ) Graph - based Multi-head Attention
To apply the multi-head attention on a graph , the graph adjacency matrix A ? R | Y |?|Z | is involved to mask nodes / tokens unrelated , where A ij ? { 0 , 1 } .
Thus , e ( h ) ij is changed as : e ( h ) ij = ? ? ? ( y i W ( h ) Q ) ( z j W ( h ) K ) ?
d model / H , if A ij = 1 ? , otherwise and Eqn. ( 1 ) is modified as : C = GraphMultiHead ? ( Y , Z , A ) ( 2 ) Eqn. ( 1 ) , can be treated as a special case of Eqn. ( 2 ) that the graph is fully connected , i.e. , A = 1 .
Context - and Schema- Aware Encoding Each layer of CSFN consists of internal and external attentions to incorporate different types of inputs .
The hidden states of the schema graph G at the i-the layer are updated as follows : I GG = GraphMultiHead ? GG ( H G i , H G i , A G ) E GX = MultiHead ? GX ( H G i , H Xt i ) E GB = MultiHead ? GB ( H G i , H B t?1 i ) C G = LayerNorm ( H G i + I GG + E GX + E GB ) H G i+1 = LayerNorm ( C G + FFN ( C G ) ) where A G is the adjacency matrix of the schema graph and LayerNorm ( . ) is layer normalization function ( Ba et al. , 2016 ) . FFN ( x ) is a feedforward network ( FFN ) function with two fullyconnected layer and an ReLU activation in between , i.e. , FFN ( x ) = max ( 0 , xW 1 + b 1 ) W 2 + b 2 . Similarly , more details about updating H
Xt i , H B t?1 i are described in Appendix A .
The context and schema- aware encoding can also be simply implemented as the original transformer ( Vaswani et al. , 2017 ) with graph - based multi-head attentions .
State Prediction
The goal of state prediction is to produce the next dialogue state B t , which is formulated as two stages : 1 ) We first apply a slot-gate classifier for each domain-slot node .
The classifier makes a decision among { NONE , DONTCARE , PTR } , where NONE denotes that a domain-slot pair is not mentioned at this turn , DONTCARE implies that the user can accept any values for this slot , and PTR represents that the slot should be processed with a value .
2 ) For domain-slot pairs tagged with PTR , we further introduced an RNN - based value decoder to generate token sequences of their values .
Slot-gate Classification
We utilize the final hidden vector of j-th domainslot node in G for the slot-gate classification , and the probability for the j-th domain-slot pair at the t-th turn is calculated as : P gate tj = softmax ( FFN ( H G L , M +N +j ) )
The loss for slot gate classification is L gate = ?
T t=1 J j=1 log ( P gate tj ?
( y gate tj ) ) where y gate tj is the one- hot gate label for the j-th domain -slot pair at turn t.
RNN - based Value Decoder
After the slot-gate classification , there are J domain-slot pairs tagged with PTR class which indicates the domain-slot should take a real value .
They are denoted as C t = { j|argmax ( P gate tj ) = PTR } , and J = | C t |.
We use Gated Recurrent Unit ( GRU ) decoder like and the soft copy mechanism ( See et al. , 2017 ) to get the final output distribution P value , k tj over all candidate tokens at the k-th step .
More details are illustrated in Appendix B .
The loss function for value decoder is L value = ?
T t=1 j?Ct k log ( P value , k tj ?
( y value , k tj ) ) where y value , k tj is the one- hot token label for the j-th domain -slot pair at k-th step .
During training process , the above modules can be jointly trained and optimized by the summations of different losses as : L total = L gate + L value 5 Experiment
Datasets
We use MultiWOZ 2.0 and MultiWOZ 2.1 ( Eric et al. , 2019 ) to evaluate our approach .
MultiWOZ 2.0 is a task - oriented dataset of human-human written conversations spanning over seven domains , consists of 10348 multi-turn dialogues .
MultiWOZ 2.1 is a revised version of MultiWOZ 2.0 , which is re-annotated with a different set of inter-annotators and also canonicalized entity names .
According to the work of Eric et al . ( 2019 ) , about 32 % of the state annotations is corrected so that the effect of noise is counteracted .
Note that hospital and police are excluded since they appear in training set with a very low frequency , and they do not even appear in the test set .
To this end , five domains ( restaurant , train , hotel , taxi , attraction ) are involved in the experiments with 17 distinct slots and 30 domain-slot pairs .
We follow similar data pre-processing procedures as on both MultiWOZ 2.0 and 2.1 .
2
The resulting corpus includes 8,438 multi-turn dialogues in training set with an average of 13.5 turns per dialogue .
Data statistics of MultiWOZ 2.1 is shown in Table 1 .
The adjacency matrix A G of MultiWOZ 2.0 and 2.1 datasets is shown in Figure 4 of Appendix , while domain-slot pairs are omitted due to space limitations .
Experiment Settings
We set the hidden size of CSFN , d model , as 400 with 4 heads .
Following , the token embeddings with 400 dimensions are initialized by concatenating Glove embeddings ( Pennington et al. , 2014 )
In the inference , the predicted dialogue state of the last turn is applied , and we use a greedy search strategy in the decoding process of the value decoder .
Baseline Models
We make a comparison with the following existing models , which are
Main Results
Joint goal accuracy is the evaluation metric in our experiments , which is represented as the ratio of turns whose predicted dialogue states are entirely consistent with the ground truth in the test set .
Table 2 illustrates that the joint goal accuracy of CSFN - DST and other baselines on the test set of MultiWOZ 2.0 and MultiWOZ 2.1 datasets .
As shown in the table , our proposed CSFN - DST can outperform other models except for SOM - DST .
By combining our schema graphs with SOM - DST , we can achieve state - of - the - art performances on both MultiWOZ 2.0 and 2.1 in the open-vocabulary setting .
Additionally , our method using BERT ( Bert- base-uncased ) can obtain very competitive performance with the best systems in the predefined ontology - based setting .
When a BERT is exploited , we initialize all parameters of CSFN with the BERT encoder 's and initialize the token / position embeddings with the BERT 's .
Analysis
In this subsection , we will conduct some ablation studies to figure out the potential factors for the improvement of our method .
(
Effect of context information Context information consists of the previous dialogue state or the current dialogue utterance , which are definitely key for the encoder .
It would be interesting to know whether the two kinds of context information are also essential for the RNN - based value decoder .
As shown in Table 3 , we choose to omit the top hidden states of the previous dialogue state ( H B t?1 L ) or the current utterance ( H Xt L ) in the RNN - based value decoder .
The results show both of them are crucial for generating real values .
Do we need more context ?
Only the current dialogue utterance is utilized in our model , which would be more efficient than the previous methods involving all the preceding dialogue utterance .
However , we want to ask whether the performance will be improved when more context is used .
In Table 3 , it shows that incorporating the previous dialogue utterance X t?1 gives no improvement , which implies that jointly encoding the current utterance and the previous dialogue state is effective as well as efficient .
Effect of the schema graph
In CSFN - DST , the schema graph with domain-slot relations is exploited .
To check the effectiveness of the schema graph used , we remove knowledgeaware domain-slot relations by replacing the adjacency matrix A G as a fully connected one 1 or node-independent one I. Results in Table 4 show that joint goal accuracies of models without the schema graph are decreased similarly when BERT is either used or not .
To reveal why the schema graph with domain- slot relations is essential for joint accuracy , we further make analysis on domain-specific and turnspecific results .
As shown in Table 5 , the schema graph can benefit almost all domains except for Attaction ( Attr . ) .
As illustrated in Table 1 , the Attaction domain contains only three slots , which should be much simpler than the other domains .
Therefore , we may say that the schema graph can help complicated domains .
The turn-specific results are shown in Table 6 , where joint goal accuracies over different dialogue turns are calculated .
From the table , we can see that data proportion of larger turn number becomes smaller while the larger turn number refers to more challenging conversation .
From the results of the table , we can find the schema graph can make improvements over most dialogue turns .
Oracle experiments
The predicted dialogue state at the last turn is utilized in the inference stage , which is mismatched with the training stage .
An oracle experiment is conducted to show the impact of training - inference mismatching , where ground truth of the previous dialogue state is fed into CSFN - DST .
The results in Table 4 show that joint accuracy can be nearly 80 % with ground truth of the previous dialogue state .
Other oracle experiments with ground truth slot-gate classification and ground truth value generation are also conducted , as shown in Table 4 .
Slot- gate classification
We conduct experiments to evaluate our model performance on the slot-gate classification task .
Table 7 shows F1 scores of the three slot gates , i.e. , { NONE , DONTCARE , PTR } .
It seems that the pretrained BERT model helps a lot in detecting slots of which the user does n't care about values .
The F1 score of DONTCARE is much lower than the others ' , which implies that detecting DONTCARE is a much challenging sub-task .
Discussion
The main contributions of this work may focus on exploiting the schema graph with graph - based attention networks .
Slot-relations are also utilized in DSTQA ( Zhou and Small , 2019 ) .
However , DSTQA uses a dynamically - evolving knowledge graph for the dialogue context , and we use a static schema graph .
We absorb the dialogue context by using the previous ( predicted ) dialogue state as another input .
We believe that the two different usages of the slot relation graph can be complementary .
Moreover , these two methods are different in value prediction that DSTQA exploits a hybrid of value classifier and span prediction layer , which relies on a predefined ontology .
SOM - DST is very similar to our proposed CSFN - DST with BERT .
The main difference between SOM - DST and CSFN - DST is how to exploit the previous dialogue state .
For the previous dialogue state , SOM - DST considers all domain-slot pairs and their values ( if a domainslot pair contains an empty value , a special token NONE is used ) , while CSFN - DST only considers the domain-slot pairs with a non-empty value .
Thus , SOM - DST knows which domain-slot pairs are empty and would like to be filled with a value .
We think that it is the strength of SOM - DST .
However , we choose to omit the domain-slot pairs with an empty value for a lower computation burden , which is proved in Table 8 .
As shown in the last two rows of Table 2 , the schema graph can also improve SOM - DST , which achieves 52.23 % and 53.19 % joint accuracies on MultiWOZ 2.0 and 2.1 , respectively .
Appendix E shows how to exploit schema graph in SOM - DST .
Conclusion and Future Work
We introduce a multi-domain dialogue state tracker with context and schema fusion networks , which involves slot relations and learns deep representations for each domain-slot pair dependently .
Slots from different domains and their relations are organized as a schema graph .
Our approach outperforms strong baselines on both MultiWOZ 2.0 and 2.1 benchmarks .
Ablation studies also show that the effectiveness of the schema graph .
It will be a future work to incorporate relations among dialogue states , utterances and domain schemata .
To further mitigate the data sparsity problem of multi-domain DST , it would be also interesting to incorporate data augmentations ( Zhao et al. , 2019 ) and semi-supervised learnings ( Lan et al. , 2018 ; Cao et al. , 2019 ) . g k tj = GRU ( g k?1 tj , e k tj ) GRU is initialized with g 0 tj = H Xt L,0 + H B t?1 L,0 and e 0 tj = H G L , M +N +j .
The value generator transforms the hidden state to the probability distribution over the token vocabulary at the k-th step , which consists of two parts : 1 ) distribution over all input tokens , 2 ) distribution over the input vocabulary .
The first part is computed as P ctx , k tj = softmax ( ATT ( g k tj , [ H Xt L ; H B t?1 L ] ) ) where ) , and ATT ( . , .) is a function to get attention weights with more details shown in Appendix B.1 .
The second part is calculated as P ctx , k tj ? R 1 ? ( | Xt | +| B t?1 | c k tj = P ctx , k tj [ H Xt L ; H B t?1 L ] P vocab , k tj = softmax ( [ g k tj ; c k tj ] W proj E ) where P vocab , k tj ?
R 1?d vocab , c k tj ?
R 1?d model is a context vector , W proj ?
R 2d model ?d model is a trainable parameter , and E ?
R d vocab ?d model is the token embedding matrix shared across the encoder and the decoder .
We use the soft copy mechanism ( See et al. , 2017 ) to get the final output distribution over all candidate tokens : where W gen ?
R 3d model ?1 is a trainable parameter .
The loss function for value decoder is L value = ?
T t=1 j?Ct k log ( P value , k tj ?
( y value , k tj ) ) where y value , k tj is the one- hot token label for the j-th domain -slot pair at k-th step .
