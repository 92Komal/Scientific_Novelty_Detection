title
A Semi-Supervised Stable Variational Network for Promoting Replier-Consistency in Dialogue Generation
abstract
Neural sequence - to-sequence models for dialog systems suffer from the problem of favoring uninformative and non replier-specific responses due to lacking of the global and relevant information guidance .
The existing methods model the generation process by leveraging the neural variational network with simple Gaussian .
However , the sampled information from latent space usually becomes useless due to the KL divergence vanishing issue , and the highly abstractive global variables easily dilute the personal features of replier , leading to a non replier-specific response .
Therefore , a novel Semi-Supervised Stable Variational Network ( SSVN ) is proposed to address these issues .
We use a unit hypersperical distribution , namely the von Mises - Fisher ( vMF ) , as the latent space of a semi-supervised model , which can obtain the stable KL performance by setting a fixed variance and hence enhance the global information representation .
Meanwhile , an unsupervised extractor is introduced to automatically distill the replier-tailored feature which is then injected into a supervised generator to encourage the replier-consistency .
Experimental results on two large conversation datasets show that our model outperforms the competitive baseline models significantly , and can generate diverse and replier-specific responses .
Introduction Dialog systems , aiming at generating relevant and fluent responses in the replier-consistent way , have received increasing attention due to its numerous applications ( Grosz , 2016 ; Chen et al. , 2017a ) .
Recently , Seq2Seq neural networks ( Sutskever et al. , 2014 ) have demonstrated excellent results on open-domain conversation ( Shang et al. , 2015 ; Sordoni et al. , 2015 ; Vinyals and V. Le , 2015 ; Yao et al. , 2015 ) .
However , due to lacking of the global and relevant information guidance , they inherently tend to generate trivial and uninformative responses ( e.g. , " I do n't know " ) , rather than meaningful and replier-specific ones Li et al. , 2016 ) .
The existing methods based on neural variational methods with Gaussian , are proposed to use a latent variable as the global information in decoder to strengthen the generation ( Serban et al. , 2017 ; Zhao et al. , 2017 ; Chen et al. , 2018 ) .
However , they face the problems of ( 1 ) latent space futility and ( 2 ) replier -consistency decay .
( 1 ) The model tends to select more gain from a lower Kullback - Leibler ( KL ) divergence during training , which encourages the approximate posterior close to Gaussian prior , rendering the latent space of the former unused .
Thus , the latent variables on this space become worthless global guidance for decoder .
To address this issue , most previous work ( Xie et al. , 2017 ; Yang et al. , 2017 ; Chen et al. , 2017 ) has suggested a weaker decoder to match the Gaussian samples , which essentially sacrifice the generative capacity .
( 2 ) The speakers in a dyadic conversation have different linguistic characteristics , sentiments and personalities .
However , the latent variable is learned conditioned on the holistic context without any distinction between speakers , especially the replier .
This will dilute the personal features of replier and lead to a decrease in replier-consistency .
Current methods ( Li et al. , 2016 ; Zhang et al. , 2018 ) normally recur to artificially scheduled personal information to promote the replier-consistency , but they cannot be migrated to the other datasets .
Inspired by the effectiveness of vMF distribution in solving the KL - vanishing in the unsupervised scene ( Xu and Durrett , 2018 ) and the suc-cess of Variational Auto-Encoder ( VAE ) in capturing latent feature of the real data ( Davidson et al. , 2018 ) , we propose a Semi-Supervised Stable Variational Network ( SSVN ) framework to address the above issues .
It consists of an unsupervised personal feature extractor ( a VAE with vMF ) and a supervised information - enhanced generator ( a CVAE with vMF ) .
To maintain the consistency of replier features , the extractor only encodes the previous utterances from the replier and produces a personally tailored latent variable .
On the top of this , the generator fuses the replier-tailored latent variable and the self vMF distributed global information to facilitate the diverse and replier-specific responses .
In general , our contributions are as follows : ?
A semi-supervised stable variational network is proposed to solve the latent space futility issue and promote the replier-consistency .
?
To the best of our knowledge , our model is the first to use the vMF distribution in a semisupervised framework for dialogue generation , which can enhance the global information by alleviating the KL divergence vanishing problem .
?
An unsupervised personal feature extractor is designed to acquire the replier-specific features automatically .
?
The experimental results on two large conversation datasets validate the effectiveness of our model . ?
It is shown that the different roles of vMF on extractor and generator .
We suprisingly find that the extractor can alleviate the KLvanishing to some extent .
2 Related Work
Neural Variational Network Variational autoencoder ( VAE ) is one of the most popular generative models .
The principle idea is to encode the data x to learn a probability distribution z , then sample the latent variables from z and inject them into a directed decoder network to reconstruct x .
The model parameters are optimized by maximizing a reparameterized variational lower bound .
Based on this process , the conditional VAE ( CVAE ) ( Sohn et al. , 2015 ) can be conditioned on certain attributes to improve diversity .
In diaog generation task , Serban et al . ( 2017 ) employs the CVAE to acquire a global latent variable as a holistic representation in a hierarchical setting .
Zhao et al. ( 2017 ) regards the latent variable as a global dialog act information and directly feed it to the decoder to control the dialog act of a response .
To maintain the long-term memory of the previous utterances , Chen et al . ( 2018 ) utilizes the higher - level abstract variable to retrieve and update memory cells .
Latent Space Futility
As for the latent space futility issue , also called KL - vanishing in Shen et al . ( 2018 ) , most previous work has suggested a weaker decoder to encourage the simple Gaussian samples to be leveraged , such as a word drop-out technique in decoder ( Xie et al. , 2017 ; Zhao et al. , 2017 ) or a practice of replacing RNN decoder with a CNN counterpart ( Yang et al. , 2017 ; Chen et al. , 2017 ; Semeniuta et al. , 2017 ) .
These methods are contrary to our origiral intention due to generation capacity descending .
Other efforts focus on changing prior and posterior : Rezende and Mohamed ( 2015 ) and Kingma et al . ( 2016 ) utilize a normalizing flow to transform the sampled variables ;
Shen et al . ( 2018 ) introduces an AE module to complicate the quondam distribution .
Extending the latter direction yet without increasing the model complexity , we only leverage the vMF distribution instead of the simple Gaussian to strengthen the KL term .
Unlike the single vMF - based VAEs implemented in other cases ( Davidson et al. , 2018 ; Guu et al. ; Xu and Durrett , 2018 ) , we apply vMF into a semisupervised dialog model to generate diverse and replier-specific responses .
Replier-Consistency Decay
In order to emphasize the replier-consistency , Li et al . ( 2016 ) 3 Model
Task Description Given a series of dialogue context utterances ( x 1 , x 2 , ... , x n ) , where x i = ( w i,1 , w i, 2 , ... , w i , N i ) , our task is to generate a response y = ( w y,1 , w y,2 , ... , w y, Ny ) that not only rely on the global information but also consider the personally special features from the replier .
In this paper , we employ the vMF distribution to stimulate the potential of latent space , impelling the extractor to condense a feature - augmented individual information and the generator to generalize a useful global guidance .
The overview of SSVN is illustrated in Figure 1 .
von Mises - Fisher
The von Mises - Fisher ( vMF ) places a distribution over points on the unit hypersphere , parameterized by a direction vector ? ?
R d indicating the mean direction and a concentration parameter ? ? R ?0 . The PDF of the vMF distribution for the unit vector z ?
R d is defined as : f d ( x ; ? , ? ) = C d ( ? ) exp (? T x ) ( 1 ) C d ( ? ) = ? d/2?1 ( 2 ? ) d/2 I d/2?1 ( ? ) ( 2 ) where ? = 1 , C d is the normalization constant , and I ? stands for the modified Bessel function of the first kind at order ?.
Personal Feature Extractor
To enhance the replier-consistency , the personal feature extractor , implemented by a VAE with vMF , encodes rustically the context utterances from replier x r = ( x r 1 , x r 2 , ... , x r l ) ( e.g. , 1 into a random latent variable z r , based on which the decoder reconstructs x r .
Due to an intractable highdimensional integral problem over the latent variable z r , we set a recognition network q ?e ( z r |x r ) as a variational approximation to the true posterior p( z r |x r ) , then apply variational inference to optimize the evidence lower bound ( ELBO ) as : x r = ( x r 1 , x r 2 ) = ( x 2 , x 4 ) in Figure L E = ?KL ( q ?e ( z r |x r ) p ?e ( z r ) ) + E q ?e ( zr|x r ) log p ?e ( x r | z r ) ? log zr p ?e ( z r ) p ( x r | z r ) dz r = log p( x r ) ( 3 ) Utterance & Local Context Encoder Concretely , we employ a hierarchical encoder to encode x r : the utterance encoder based on bidirectional RNN ( Schuster and Paliwal , 1997 ) determinitically reads each utterance x r i and output a sizefixed real- valued h u i = [ ? ? h u i , ? ? h u i ] , which the local context encoder takes as input to obtain the final hidden state h r l as the summary of x r . Prior / Posterior Distribution
Since we assume the latent space follows vMF distribution , the prior p ?e ( ( z r ) ? vM F ( ? , ? e prior = 0 ) and the variational posterior q ?e ( z r |x r ) ? vM F (?
e pos , ?
e pos ) where ?
e pos is the output of the recognition net-work and ?
e pos is set to a constant . ( 5 ) where f e pos ( ? ) is a linear transformation , and ? stands for 2 - norm to ensure the normalization .
With the uniform distribution as our prior , the KL divergence can be computed as : KL ( vM F (?
e pos , ? e pos ) vM F ( ? , 0 ) ) = 1 ? d 2 log 2 ? log I d/2?1 ( ? e pos ) ? log ? d 2 + ? e pos I d/2 ( ? e pos ) I d/2?1 ( ?
e pos ) + d 2 ? 1 log ?
e pos ( 6 ) Since Eq. 6 only depends on fixed constant ?
e pos , not on ?
e pos , this term can resolve the latent space futility problem by averting the KL -zeroing .
Replier Decoder During reconstruction , the decoder receives the concatenation of replier 's context h r l and personal latent variable z r as the initial hidden state , then generates tokens sequentially under the following probability distribution : p ?e ( x r | z r ) = l i=1 N i j=1 p( w i , j |x r < i , w i , <j ) ( 7 ) where l is the number of turns of the replier 's context x r ; N i is the length of the i-th utterance ( x r i ) in x r ; w i , j is the j-th token in x r i .
Information - Enhanced Generator Similar to the extractor , the information - enhanced generator based on CVAE also employs a recognition network q ?g ( z|x , y ) to approximate the true posterior p( z|x , y ) , correspondingly , its ELBO can be calculated as : log p( y | x ) = log z p( y|x , z ) p( z| x ) dz ? ?KL ( q ?g ( z|x , y ) p ?g ( z| x ) ) + E q ?g ( z| x , y ) log p ?g ( y|x , z ) ( 8 ) when considering an external personal feature z r , the ELBO in generator would be rewritten as : L G = ?KL ( q ?g ( z|x , y , z r ) p ?g ( z|x , z r ) ) + E q ?g ( z| x , y , zr ) log p ?g ( y|x , z , z r ) ? log z p( y|x , z , z r ) p ( z|x , z r ) dz = log p( y|x , z r ) ( 9 ) Notice that z r only participates in the generation process p ?g ( y|x , z , z r ) , the approximate posterior q ?g ( z|x , y , z r ) ? vM F (? g pos , ? g pos ) is conditioned on dialog context x and the corresponding response y , and the prior p ?g ( z|x , z r ) ? vM F (? g prior , ? g prior ) depends on x 1 . Utterance & Golbal Context Encoder
The hierarchical encoder in this part utilizes the shared utterance encoder from extractor to encode utterances x 1 , x 2 , ... , x n , y into the corresponding representations h u 1 , h u 2 , ... , h u n , h u y orderly .
Thereafter , the utterance vectors h u 1 , h u 2 , ... , h u n are fed to the global context encoder to compute the representation of the whole dialog context h c n .
Based on these , the approximate posterior and prior can be determined by the following operations : ? g pos = f g pos ( [ h c n , h u y ] ) ( 10 ) ? g pos = ?
g pos / ? g pos ( 11 ) ?g prior = f g prior ( h c n ) ( 12 ) ? g prior = ?g prior / ?g prior ( 13 ) where f g pos ( ? ) and f g prior ( ? ) are both linear transformations .
? g pos and ?
g prior in both distributions are the constants with equal values .
Prior / Posterior Distribution Without vM F ( ? , 0 ) as the prior , we require to recalculate the KL term in generator as : KL ( vM F (? g pos , ? g pos ) vM F (? g prior , ? g prior ) ) = ( d/2 ? 1 ) log ?
g pos ? g prior + log I d/2?1 ( ? g prior ) I d/2?1 ( ? g pos ) ? ? g prior ?
g prior ?
g pos ?1 I d/2 ( ? g pos ) I d/2?1 ( ? g pos ) + ? g pos I d/2 ( ? g pos ) I d/2?1 ( ? g pos ) ( 14 ) Response Decoder
We employ a RNN decoder similar to the one in extractor , extending it to condition on a personal feature z r by concatenating z r to the input of the decoder at each time step .
The concrete generative process is as follows : s R t = ?(s R t?1 , [ e w y,t? 1 , z r ] ) ( 15 ) p vocab = sof tmax ( V s R t + b ) ( 16 ) where ? is the sigmoid function ; e w y, i is the word embedding of the i-th word in response y ; s R t denotes the hidden state at the time step t ; V and b are learnable parameters ; p vocab stands for the probability distribution over the vocabulary .
Then the objective function of the decoder is given by : p ?g ( y|x , z , z r ) =
Ny i=1 p vocab ( w y , i ) ( 17 ) where p vocab ( w y , i ) is the probability to generate the word w y, i ; N y is the length of the response y .
Training Objective
The entire SSVN model integrates two modules in Figure 1 , i.e. , the unsupervised extractor and the supervised generator , which can be optimized simultaneously in one framework .
Thus , the overall objective function of SSVN is to maximize : L = ?L G + ( 1 ? ?) L E ( 18 ) where we have a hyperparameter ? to control the balance between response generation ( generator ) and personality reconstruction ( extractor ) .
Sampling Techique for vMF Similar to Xu and Durrett ( 2018 ) , we utilize the rejection sampling scheme of Wood ( 1994 ) to sample a value w ? [ ? 1 , 1 ] , then derive a random unit vector tangent ? on the hypersphere at the mean vector ?.
Based on these , our latent variable z can be given by z = w ?
+ ? ? 1 ? w 2 .
Experiments
Datasets
The proposed model is evaluated on two datasets .
The first corpus is Cornell Movie Dialogs Corpus 2 ( Danescu-Niculescu -Mizil and Lee , 2011 ) that contains more than 80,000 imagined movie conversations .
To normalize the length ( turns ) of the dialogs , we divide the original conversations into consecutive 3 - 10 utterances .
Our second dataset is Ubuntu Dialogue Corpus 3 ( Lowe et al. , 2015 ) .
It contains about 500,000 multi-turn dialogues collected from the Ubuntu Internet Relayed Chat channel , each of which starts with a Ubunturelated technical problem and follows by the corresponding responses about solutions .
In the above two datasets , the last utterance in a conversation is regarded as the response and the remaining ones are the input context .
The detailed statistical information is shown in Table 1 .
Baselines
We compare SSVN with the following models : S2SA : the standard Seq2Seq model with the attention mechanism ( Vinyals and V. Le , 2015 ) . HRED : a hierarchical encoder framework to model multi-turn dialogs .
VHRED : a hierarchical encoder-decoder with latent stochastic variable ( Serban et al. , 2017 ) . HVMN : an encoder-decoder network containing the hierarchical structure and the variational memory ( Chen et al. , 2018 ) .
Experimental Details
Our model is implemented using the Tensorflow framework ( Abadi et al. , 2016 ) with the following parameter settings :
We set word embeddings to size of 200 and initialize them randomly .
The shared utterance encoder is a 2 - layer bidirectional GRU structure with 600 hidden neurons for each layer , while the both context encoders and the both decoders are the unidirectional ones with hidden size of 600 .
The dimensions of the latent variable z and z r are both set to 50 .
We use the Adam algorithm ( Kingma and Ba , 2014 ) to update the parameters with an initial learning rate of 0.001 .
In the training , we employ the early - stop strategy ( Caruana et al. , 2000 ) to select the best models using the variational lower - bound on the validation set .
Evaluation Metrics
We use both automatic and human evaluations to analyze the model 's performance .
Automatic Evaluation Metrics
In our experiment , three embedding - based metrics ( Average , Greedy , Extreme ) 4 ( Liu et al. , 2016 ) are employed to measure the semantic relevance between generated responses and ground truths .
Besides , we also adopt Distinct - 1 and Distinct - 2 ( Li et al. , 2016 ) to evaluate the diversity of responses .
Human Evaluation
In order to assess how well the models can maintain the replier 's consistency , we conduct a human evaluation .
Specifically , we randomly sample 300 context from the test set and apply 5 models to generate responses for each context .
For each response , three annotators are re- cruited to give a 4 - graded judgement with the following criteria : 1 : the response is ungrammatical or semantically irrelevant ; or inconsistent with replier 's features ( e.g. , linguistic characteristics , sentiments and personalities ) ; or has wrong logic ; 2 : the response is semantically weak related , but it is too trivial ( e.g. , " I do n't know " ) ; 3 : the response is semantically relevant and informative , but has no obvious consistency about the replier 's personal features ; 4 : the response is not only semantically related and informative , but also consistent with the individual features of replier .
Evaluation Results
Automatic Evaluation
The metric-based evaluation results are shown in Table 2 .
From the results , we can observe that : ( 1 ) HRED performs better than S2SA , indicating that the hierarchical structure is benefical .
( 2 ) VHRED outperforms HRED on all metrics on Cornell , which demonstrates that the latent variables are the useful global guidance information .
Inversely , VHRED has a worse performance than HRED in terms of three embedding - based metrics on Ubuntu , which is consistent with Chen et al . ( 2018 ) due to the domain specific dataset .
( 3 ) On the top of VHRED , HVMN introduces the memory network to enhance the long-term memory , and obtains the best performance among the baseline models .
( 4 ) Compared with all the baselines , our SSVN model achieves the highest scores in terms of all metrics on two datasets , indicating that SSVN can best fit the ground truth semantically and generate more informative responses .
Meanwhile , the sign tests show that the improvements of SSVN are statistically significant ( p- value < 0.01 ) .
( 5 ) Noticeably , the models trained on Ubuntu consistently have more distinct n-grams than the same models trained on Cornell , while the distinct ratios do not differ much .
The reason is that Ubuntu dataset has more words averagely per utterance than Cornell data ( as the statistical details shown in Table 1 ) , which forces the models to produce longer responses .
Human Evaluation
The human evaluation results on Cornell data are shown in Table 4 , in which the score distribution values represent the percentages of responses belonging to each grade , and Fleiss ' kappa ( Fleiss and Cohen , 1973 ) is employed to evaluate the inter-annotator agreement .
From the results , we have the following observations : ( 1 ) The percentage of replier-specific responses ( i.e. , the grade ' 4 ' ) of SSVN model is 22.69 % , which is much higher than that of baselines , indicating that the personal feature extractor can effectively capture the personal feature of replier .
( 2 ) SSVN model generates much more informative responses ( i.e. , 71.56 % labeled as ' 3 + 4 ' ) and much less generic responses ( i.e. , 20.28 % labeled as ' 2 ' ) than all the baselines .
The results are in line with the above results of metric-based evaluation .
( 3 ) Kappa scores of the models are all higher than 0.4 , demonstrating that the annotators come to a fair agreement .
Meanwhile , the sign tests also show that the human evaluation improvements of SSVN to baselines are significant on Cornell dataset ( p- value < 0.01 ) .
The percentages are calculated by combining the judgements from three annotators together . ' 3 + 4 ' stands for the sum of percentages of the grade ' 3 ' and ' 4 ' ( i.e. , the ratio of informative responses ) .
Discussions Model Ablation
To investigate the effect of different parts , we conduct a set of experiments on Cornell by removing the extractor or modifying the distribution of extractor and generator .
From the results listed in Table 3 , we can observe that : ( 1 ) Removing the extractor ( denoted as SVN ) makes the distinct ratios and numbers drop dramatically , while the embedding - based metric scores are only slightly lower than that of SSVN .
This indicates the personal features learned by the extractor not only maintain the replierconsistency , but also improve the diversity of responses .
In addition , SSVN Gau ?E ( replacing the vMF distribution with a Gaussian in extractor ) has a better performance than SVN , but a worse one than SSVN , demonstrating the vMF - Extractor is more effective than Gau-Extractor .
( 2 ) As for the generator , when setting the Gaussian as the latent space ( denoted as SSVN Gau ? G ) , the embedding - based performance deteriorates dramatically whereas the distinct numbers decrease slightly , indicating that the vMF - Generator is more capable of facilitating the generated responses semantically close to the ground truth than Gau-Generator .
Notably , the distinct ratios in SSVN Gau ?
G rise unexpectedly , which will be investigated in ( 3 ) .
( 3 ) To figure out this special phenomenon , we conduct an experiment on SSVN Gau composed by Gau-Extractor and Gau-Generator .
We can see that the SSVN Gau ?G and SSVN Gau obtain the best distinct ratios among the ablation models , but their distinct numbers are not the highest .
The results indicate that , whatever the latent space of extractor follows , the Gau-Generator always tends to produce informative but very short responses .
Meanwhile , their worst embeddingbased scores show that the responses generated by Gau -Generator semantically deviate from the ground truth significantly .
The Effect of vMF on KL Besides the metricbased performance , we also evaluate the effectiveness of different settings in sloving the latent space futility problem .
Figure 2 visualizes the evolution of the KL loss in both extractor and generator parts .
We can see that : ( 1 ) In Extractor KL , Gau-Extractors ( i.e. , SSVN Gau and SSVN Gau ?E ) have a KL cost close to 0 at the begining and never recover , while the vMF - Extractors ( i.e. , SSVN Gau ? G and SSVN ) can keep a constant KL value as evidenced by Eq . ( 6 ) .
The results indicate that the vMF in extractor can mitigate KL - vanishing and capture the more meaningful personal features .
( 2 ) Surprisingly , in Generator KL , the KL loss presents an upward trend in Gau-Generators ( i.e. , SSVN Gau and SSVN Gau ? G ) .
The reason is that , the personal features from the extractor can effectively strengthen the expressiveness of the latent space in the generator , thus the response decoder is encouraged to exploit the latent variables and the latent space futility problem is alleviated .
( 3 ) Compared with the Gau-Generators in Generator KL , the vMF - Generators ( i.e. , SVN , SSVN Gau ?E and SSVN ) have the much higher KL values , indicating that the vMF is a better selection than Gaussian to solve the KL - vanishing problem .
Meanwhile , the KL values are relatively stable , which experimentally demonstrates the KL cost mainly depends on the last term in Eq. ( 14 ) and the variable term has little effect on it .
Last but not least , KL cost can explictly be changed by setting different kappa values .
Impact of the Coefficient ? Recall that Eq. ( 18 ) shows the capacity of SSVN in balancing response generation and personality reconstruction .
Here we analyze the effects of different coefficient ? on the quality of responses .
Figure 3 shows the performances given varying ?.
Notably , the performances of embedding - based metrics are changing in a similar trend , as the same case in Distinct - 1 and Distinct - 2 , thus we only consider Average and Distinct - 1 as the major analysis items .
As we can see , the evolutions of Average and Distinct - 1 in Figure 3 can be broadly into three stages : generation adynamic stage , mutual promotion stage and reconstruction rout stage .
( 1 ) The first stage shows that as ? increases , the Average monotonically increases while diversity decreases .
This is because the lower ?
gives the model less incentive to optimize the generator , which makes the response decoder incapable of utilizing the higher -quality personal features , resulting in the diverse but semantically inappropriate responses .
( 2 ) When ? moves to the second stage , the performances of the Average and diversity improve simultaneously , implying that the response gener-ation and personality reconstruction achieve en expected balance .
( 3 ) For the reconstruction rout stage , although the model focuses on response generation , the larger ? does not bring an improvement of Average , but instead increases diversity .
The result indicates that the unprofitable personal features in thwarted personality reconstruction part , as a random disturbance , can increase the diversity of the responses , but severely bias the generation of response decoder semantically .
Observed from the curves of all metrics , the best performance of embedding - based metrics is achieved at ? = 0.7 , while the diversity reaches the peak in the mutual promotion stage .
Thus , we set ? to 0.7 in all previous experiments .
Case Study Besides the quantitative analysis , we also organize some examples ( seen in Table 5 ) from different models to analyze the performances of the methods qualitatively .
They are chosen randomly from the responses produced by the proposed model , and showed together with the corresponding contexts and the outputs of the baselines .
From the case 1 , we can observe that the SSVN can extract the personal feature of the replier that the speaker A prefers to acquire further information from others , which guides the generator to produce an interrogative response to promote the replier-consistency .
Meanwhile , the SSVN can also extract the firm attitude of the replier in case 2 and the pleading tone of the replier in case 3 .
By contrast , the baselines favor to produce the bad responses , such as containing more ' unk ' .
Error Analysis
To improve the performance of SSVN in the future , we take the worse cases ( i.e. , the grade ' 1 ' and ' 2 ' ) in human judgement as an example to analyze our errors .
Specifically , we divide the errors with the grade ' 1 ' into grammatical error , replier-nonconsistency and logic contradiction , which occupy 19.79 % , 31.49 % and 48.72 % , respectively .
We can find that 1 ) logic condradiction scenes make up most of the errors as SSVN pays little attention to this issue .
2 ) although considering the personal features from replier , there still exists 31.49 % replier-inconsistent cases , incidating that only strengthening the VAE with the vMF distribution may not be a perfect approach for personal feature extraction .
As for grade ' 2 ' , the consistency of replier 's features improve the of response diversity significantly , but the model still has " safe response " problem as the baseline .
The above analysis sheds light on our future directions : 1 ) modeling the logic consistency between the context and response ; 2 ) exploring advanced methods for extracting personal features ; 3 ) improving the response diversity .
Conclusion and Future Work
In this work , we propose a semi-supervised stable variational network for addressing the latent space futility and replier-consistency decay issues .
Different from the traditional variational dialog models , the proposed model selects the vMF as the prior and posterior to resolve the latent space futility issue , and then integrates a unsupervised extractor to obtain the replier-tailored personal features to ensure the replier-consistency .
Experimental results on two dialog datasets demonstrate the effectiveness of our model , especially on replier -consistency in terms of human evaluation .
However , the error analysis shows that there are still challenges in dialogue generation , which we would like to explore in the future .
Figure 1 : 1 Figure 1 : The SSVN framework .
