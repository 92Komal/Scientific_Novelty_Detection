title
Summarization of Multi-Document Topic Hierarchies using Submodular Mixtures
abstract
We study the problem of summarizing DAG - structured topic hierarchies over a given set of documents .
Example applications include automatically generating Wikipedia disambiguation pages for a set of articles , and generating candidate multi-labels for preparing machine learning datasets ( e.g. , for text classification , functional genomics , and image classification ) .
Unlike previous work , which focuses on clustering the set of documents using the topic hierarchy as features , we directly pose the problem as a submodular optimization problem on a topic hierarchy using the documents as features .
Desirable properties of the chosen topics include document coverage , specificity , topic diversity , and topic homogeneity , each of which , we show , is naturally modeled by a submodular function .
Other information , provided say by unsupervised approaches such as LDA and its variants , can also be utilized by defining a submodular function that expresses coherence between the chosen topics and this information .
We use a large-margin framework to learn convex mixtures over the set of submodular components .
We empirically evaluate our method on the problem of automatically generating Wikipedia disambiguation pages using human generated clusterings as ground truth .
We find that our framework improves upon several baselines according to a variety of standard evaluation metrics including the Jaccard Index , F1 score and NMI , and moreover , can be scaled to extremely large scale problems .
Introduction Several real world machine learning applications involve hierarchy based categorization of topics for a set of objects .
Objects could be , e.g. , a set of documents for text classification , a set of genes in functional genomics , or a set of images in computer vision .
One can often define a natural topic hierarchy to categorize these objects .
For example , in text and image classification problems , each document or image is assigned a hierarchy of labels - a baseball page is assigned the labels " baseball " and " sports . "
Moreover , many of these applications , naturally have an existing topic hierarchy generated on the entire set of objects ( Rousu et al. , 2006 ; Barutcuoglu et al. , 2006 ; ling Zhang and hua Zhou , 2007 ; Silla and Freitas , 2011 ; Tsoumakas et al. , 2010 ) .
Given a DAG - structured topic hierarchy and a subset of objects , we investigate the problem of finding a subset of DAG - structured topics that are induced by that subset ( of objects ) .
This problem arises naturally in several real world applications .
For example , consider the problem of identifying appropriate label sets for a collection of articles .
Several existing text collection datasets such as 20 Newsgroup 1 , Reuters - 21578 2 work with a predefined set of topics .
We observe that these topic names are highly abstract 3 for the articles categorized under them .
On the other hand , techniques proposed by systems such as Wikipedia Miner ( Milne , 2009 ) and TAGME ( Ferragina and Scaiella , 2010 ) generate several labels for each article in the dataset that are highly specific to the article .
Collating all labels from all articles to create a label set for the dataset can result in a large number of labels and become unmanageable .
Our proposed techniques can summarize such large sets of labels into a smaller and more meaningful label sets using a DAG - structured topic hierarchy .
This also holds for image classification problems and datasets like ImageNet ( Deng et al. , 2009 ) .
We use the term summarize to highlight the fact that the smaller label set semantically covers the larger label set .
For example , the topics Physics , Chemistry , and Mathematics can be summarized into a topic Science .
A particularly important application of our work ( and the one we use for our evaluations in Section 4 ) is the following :
Given a collection of articles spanning different topics , but with similar titles , automatically generate a disambiguation page for those titles using the Wikipedia category hierarchy 4 as a topic DAG .
Disambiguation pages 5 on Wikipedia are used to resolve conflicts in article titles that occur when a title is naturally associated with multiple articles on distinct topics .
Each disambiguation page organizes articles into several groups , where the articles in each group pertain only to a specific topic .
Disambiguations may be seen as paths in a hierarchy leading to different articles that arguably could have the same title .
For example , the title Apple 6 can refer to a plant , a company , a film , a 4 http://en.wikipedia.org/wiki/Help:Categories 5 http://en.wikipedia.org/wiki/Wikipedia:Disambiguation 6 http://en.wikipedia.org/wiki/Apple_ ( disambiguation ) television show , a place , a technology , an album , a record label , and a newspaper daily .
The problem then , is to organize the articles into multiple groups where each group contains articles of similar nature ( topics ) and has an appropriately discerned group heading .
Figure 1 describes the topic summarization process for creation of the disambiguation page for " Apple " .
All the above mentioned problems can be modeled as the problem of finding the most representative subset of topic nodes from a DAG - Structured topic hierarchy .
We argue that many formulations of this problem are natural instances of submodular maximization , and provide a learning framework to create submodular mixtures to solve this problem .
A set function f ( . ) is said to be submodular if for any element v and sets A ? B ?
V \ { v} , where V represents the ground set of elements , f ( A ? { v} ) ? f ( A ) ? f ( B ? {v} ) ? f ( B ) .
This is called the diminishing returns property and states , informally , that adding an element to a smaller set increases the function value more than adding that element to a larger set .
Submodular functions naturally model notions of coverage and diversity in applications , and therefore , a number of machine learning problems can be modeled as forms of submodular optimization ( Kempe et al. , 2003 ; Krause and Guestrin , 2005 ; Narasimhan and Bilmes , 2004 ; Iyer et al. , 2013 ; Lin and Bilmes , 2012 ; Lin and Bilmes , 2010 ) .
In this paper , we investigate structured prediction methods for learn-ing weighted mixtures of submodular functions to summarize topics for a collection of objects using DAG - structured topic hierarchies .
Throughout this paper we use the terms " topic " and " category " interchangeably .
Related Work
To the best of our knowledge , the specific problem we consider here is new .
Previous work on identifying topics can be broadly categorized into one of the following types : a ) cluster the objects and then identify names for the clusters ; or b ) dynamically identify topics ( including hierarchical ) for a set of objects .
LDA ( Blei et al. , 2003 ) clusters the documents and simultaneously produces a set of topics into which the documents are clustered .
In LDA , each document may be viewed as a mixture of various topics and the topic distribution is assumed to have a Dirichlet prior .
LDA associates a group of high probability words to each identified topic .
A name can be assigned to a topic by manually inspecting the words or using additional algorithms like ( Mei et al. , 2007 ; Maiya et al. , 2013 ) . LDA does not make use of existing topic hierarchies and correlation between topics .
The Correlated Topic Model ( Blei and Lafferty , 2006 ) induces a correlation structure between topics by using the logistic normal distribution instead of the Dirichlet .
Another extension is the hierarchical LDA ( Blei et al. , 2004 ) , where topics are joined together in a hierarchy by using the nested Chinese restaurant process .
Nonparametric extensions of LDA include the Hierarchical Dirichlet Process ( Teh et al. , 2006 ) mixture model , which allows the number of topics to be unbounded and learnt from data and the Nested Chinese Restaurant Process which allows topics to be arranged in a hierarchy whose structure is learnt from data .
In each of these approaches , unlike our proposed approach , an existing topic hierarchy is not used , nor is any additional objecttopic information leveraged .
The pachinko allocation model ( PAM ) ( Li and McCallum , 2006 ) captures arbitrary , nested , and possibly sparse correlations between topics using a DAG .
The leaves of the DAG represent individual words in the vocabulary , while each interior node represents a correlation among its children , which may be words or other interior nodes ( topics ) .
PAM learns the probability distributions of words in a topic , subtopics in a topic , and topics in a document .
We cannot , however , generate a subset of topics from a large existing topic DAG that can act as summary topics , using PAM .
HSLDA ( Perotte et al. , 2011 ) introduces a hierar-chically supervised LDA model to infer hierarchical labels for a document .
It assumes an existing label hierarchy in the form of a tree .
The model infers one or more labels such that , if a label l is inferred as relevant to a document , then all the labels from l to the root of the tree are also inferred as relevant to the document .
Our approach differs from HSLDA since : ( 1 ) we use the label hierarchy to infer a set of labels for a group of documents ; ( 2 ) we do not enforce the label hierarchy to be a tree as it can be a DAG ; and ( 3 ) generalizing HSLDA to use a DAG structured hierarchy and infer labels for a group of documents ( e.g. , combining into one big document ) also may not help in solving our problem .
HSLDA will apply all the relevant labels to the documents as per the classifier that it learns for every label .
Moreover , the " root " label is always applied and it is very likely that many labels near the top level of the label hierarchy are also classified as relevant to the group of documents .
Wei and James ( Bi and Kwok , 2011 ) present a hierarchical multi-label classification algorithm that can be used on both tree and DAG structured hierarchies .
They formulate a search for the optimal consistent multi-label as the finding of the best subgraph in a tree / DAG .
In our approach , we assume , individual documents are already associated with one or more topics and we find a consistent label set for a group of documents using the DAG structured topic hierarchy .
Medelyan et al .
( Medelyan et al. , 2008 ) and Ferragina et al .
( Ferragina and Scaiella , 2010 ) detect topics for a document using Wikipedia article names and category names as the topic vocabulary .
These systems are able to extract signals from a text document and identify Wikipedia articles and / or categories that optimally match the document and assign those article / category names as topics for the document .
When run on a large collection of documents , these approaches generate enormous numbers of topics , a problem our proposed approach addresses .
Our Contributions
While most prior work discussed above focuses on the underlying set of documents , ( e.g. , by clustering documents ) , we focus directly on the topics .
In particular , we formulate the problem as subset selection on the set of topics within a DAG while simultaneously considering the documents to be categorized .
Our method can scale to the colossal size of the DAG ( 1 million topics and 3 million correlation links between topics in Wikipedia ) .
Moreover , our approach can naturally incorporate outputs from many of the aforementioned algorithms .
Our approach is based on submodular maximization and mixture learning , which has been successfully used in applications such as document summarization ( Lin , 2012 ) and image summarization ( Tschiatschek et al. , 2014 ) , but has never been applied to topic identification tasks or , more generally , DAG summarization .
We introduce a family of submodular functions to identify an appropriate set of topics from a DAG structured hierarchy of topics for a group of documents .
We characterize this topic appropriateness through a set of desirable properties such as coverage , diversity , specificity , clarity , and relevance .
Each of the submodular function components we consider are monotone , thereby ensuring a near optimal performance obtainable via a simple greedy algorithm for optimization .
7 . We also show how our technique naturally embodies outputs of other algorithms such as LDA , clustering , and classifications .
Finally , we utilize a large margin formulation for learning mixtures of these submodular functions , and show how we can optimally learn them from training data .
Our approach demonstrates how to utilize the features collectively in the document space and the topic space to infer a set of topics .
From an empirical perspective , we introduce and evaluate our approach on a dataset of around 8000 disambiguations that was extracted from Wikipedia and subsequently cleaned using the methods described in the experimentation section .
We show that our learning framework outperforms many of the baselines , and is practical enough to be used on large corpora .
Problem Formulation Let G ( V , E ) be the DAG structured topic hierarchy with V topics .
These topics are observed to have a parent child ( isa ) relationship forming a DAG .
Let D be the set of documents that are associated with one or more of these topics .
The middle portion of Figure 1 depicts a topic hierarchy with associated documents .
The association links between the documents and topics can be hard or soft .
In case of a hard link , a document is attached to a set of topics .
Examples include multi-labeled documents .
In case of a soft link , a document is associated with a topic with some degree of confidence ( or probability ) .
Furthermore , if a document is attached to a topic t , we assume that all the ancestor topics of t are also relevant for that document .
This assumption has been employed in earlier works ( Blei et al. , 2004 ; Bi and Kwok , 2011 ; Rousu et al. , 2006 ) as well .
Given a budget of K , our objective is to choose a set of K topics from V , which best describe the documents in D .
The notion of best describing topics is characterized through a set of desirable properties - coverage , diversity , specificity , clarity , relevance and fidelity - that K topics have to satisfy .
The submodular functions that we introduce in the next section ensure these properties are satisfied .
Formally , we solve the following discrete optimization problem : S * ? argmax S?V :|S|?
K
i w i f i ( S ) ( 1 ) where , f i are monotone submodular mixture components and w i ?
0 are the weights associated with those mixture components .
Set S * is the summary topics scored best .
It is easy to find massive ( i.e. , size in the order of million ) DAG structured topic hierarchies in practice .
Wikipedia 's category hierarchy consists of more than 1 M categories ( topics ) arranged hierarchically .
In fact , they form a cyclic graph ( Zesch and Gurevych , 2007 ) .
However , we can convert it to a DAG by eliminating the cycles as described in the supplementary material .
YAGO ( Suchanek et al. , 2007 ) and Freebase ( Bollacker et al. , 2008 ) are other instances of massive topic hierarchies .
The association of the documents with the existing topic hierarchy is also well studied .
Systems such as WikipediaMiner ( Milne , 2009 ) , TAGME ( Ferragina and Scaiella , 2010 ) and several annotation systems such as ( Dill et al. , 2003 ; Mihalcea and Csomai , 2007 ; Bunescu and Pasca , 2006 ) attach topics from Wikipedia ( and other catalogs ) to the documents by establishing the hard or soft links mentioned above .
Our goal is the following : Given a ( ground set ) collection
V of topics organized in a pre-existing hierarchical DAG structure , and a collection D of documents , chose a size K ? Z + representative subset of topics .
Our approach is distinct from earlier work ( e.g. , ( Kanungo et al. , 2002 ; Blei et al. , 2003 ) ) where typically only a set of documents is classified and categorized in some way .
We next provide a few definitions needed later in the paper .
Definition 1 : Transitive Cover ? ) :
A topic t is said to cover a set of documents ?( t ) , called the transitive cover of the topic t , if for all documents i ? ?( t ) , either i is associated directly with topic t or with any of the descendant topics of t in the topic DAG .
A natural extension of this definition to a set of topics T is defined as ?( T ) = ? t?T ?( t ) .
Definition 2 : Truncated Transitive Cover ( ? ? ) :
This is a transitive cover of topic t , but with the limitation that the path length between a document and the topic t is not more than ?.
Hence , |? ? ( t ) | ? |?( t ) | .
While our problem is closely related to clustering approaches , which consider the set of documents directly , there are some crucial differences .
In particular , we focus on producing a clustering of documents where clusters are encouraged to honor a pre-defined DAG structured topic hierarchy .
Existing agglomerative clustering algorithms focusing on the coverage of documents may not produce the desired clustering .
To understand this , consider six documents d1 , d2 . . . d6 to be grouped into three clusters .
There may be multiple ways to do this depending upon multiple aggregation paths present in the topic DAG : ( ( d1 , d2 ) , ( d3 , d4 ) , ( d5 , d6 ) ) or ( ( d1 , d2 , d3 ) , ( d4 , d5 ) , ( d6 ) ) or ( ( d1 , d2 , d3 , d4 ) , ( d5 ) , ( d6 ) ) or something else .
Hence , we need more stringent measures to prefer one clustering over the others .
Our work addresses this with a variety of quality criteria ( coverage , diversity , specificity , clarity , relevance and fidelity , which are explained later in this paper ) that are organically derived from well established submodular functions .
And , most importantly , we learn the right mixture of these qualities to be enforced from the data itself .
Furthermore , our approach also generalizes these clustering approaches , since one of the components in our mixture of submodular functions is defined via these unsupervised approaches , and maps a given clustering to a set of topics in the hierarchy .
Submodular Components and Learning Summarization is the task of extracting information from a source that is both small in size but still representative .
Our problem is different from traditional summarization tasks since we have an underlying DAG as a topic hierarchy that we wish to summarize in response to a subset of documents .
Thus , a critical part of our problem is to take the graph structure into account while creating the summaries .
Below , we identify properties we wish our summaries to posses .
Coverage : A summary set of topics should cover most of the documents .
A document is said to be covered by a topic if there exists a path from the topic , going through intermediary descendant topics , to the document , i.e. , the document is within the transitive cover of the topic .
Diversity : Summaries should be as diverse as possible , i.e. , each summary topic should cover a unique set of documents .
When a document is covered by more than one topic , that document is redundantly covered , e.g. , " Finance " and " Banking " would be unlikely members of the same summary .
Summary qualities also involve " quality " notions , including : Specificity / Clarity / Relevance / Coherence :
These quality measures help us choose a set of topics that are neither too abstract nor overly specific .
They ensure that the topics are clear and relevant to the documents that they represent .
When additional information such as clustering ( from LDA or other sources ) and tagging ( manual ) documents is available , these quality criteria encourage the chosen topics to show resemblance ( coherence ) to those clustering / tagging in terms of transitive cover of documents they produce .
In the below , we define a variety of submodular functions that capture the above properties , and we then describe a large margin learning framework for learning convex mixtures of such components .
Submodular Components
Coverage Based Functions
Coverage components capture " coverage " of a set of documents .
Weighted Set Cover Function : Given a set of categories , S ? V , define ?( S ) as the set of documents covered - for each topic s ? S , ?( s ) ?
D represents the documents covered by topic s and ?( S ) = ? s?S ?( s ) .
The weighted set cover function , defined as f ( S ) = d? ( S ) w d = w ( ?( S ) ) , assigns weights to the documents based on their relative importance ( e.g. , in Wikipedia disambiguation , the different documents could be ranked based on their priority ) .
Feature - based Functions :
This class of function represents coverage in feature space .
Given a set of categories S ? V , and a set of features U , define m u ( S ) as the score associated with the set of categories S for feature u ?
U .
The feature set could represent , for example , the documents , in which case m u ( S ) represents the number of times document u is covered by the set S. U could also represent more complicated features .
For example , in the context of Wikipedia disambiguation , U could represent TFIDF features over the documents .
Feature based are then defined as f ( S ) = u?U ?( m u ( S ) ) , where ? is a concave ( e.g. , the square root ) function .
This function class has been successfully used in several applications ( Kirchhoff and Bilmes , 2014 ; Wei et al. , 2014a ; Wei et al. , 2014 b ) .
Similarity based Functions Similarity functions are defined through a similarity matrix S = {s ij } i , j?V .
Given categories i , j ?
V , similarity s ij in our case can be defined as s ij = |?( i ) ? ( j ) | , i.e the number of documents commonly covered by both i and j. Facility Location :
The facility location function , defined as f ( S ) = i?V max j?S s ij , is a natural model for k-medoids and exemplar based clustering , and has been used in several summarization problems ( Tschiatschek et al. , 2014 ; Wei et al. , 2014a ) .
Penalty based diversity : A similarity matrix may be used to express a form of coverage of a set S but that is then penalized with a redundancy term , as in the following difference : ( Lin and Bilmes , 2011 ) ) .
Here ? ? [ 0 , 1 ] .
This function is submodular , but is not in general monotone , and has been used in document summarization ( Lin and Bilmes , 2011 ) , as a dispersion function ( Borodin et al. , 2012 ) , and in image summarization ( Tschiatschek et al. , 2014 ) . f ( S ) = i? V , j?S s ij ? ? i?S j?S , s i , j
Quality Control ( QC ) Functions QC functions ensure a quality criteria is met by a set S of topics .
We define the quality score of the set S as F q ( S ) = s?S f q ( s ) , where f q ( s ) is the quality score of topic s for quality q.
Therefore , F q ( S ) is a modular function in S .
We investigate three types of quality control functions : Topic Specificity , Topic Clarity , and Topic Relevance .
Topic Specificity :
The farther a topic is from the root of the DAG , the more specific it becomes .
Topics higher up in the hierarchy are abstract and less specific .
We therefore prefer topics low in the DAG , but lower topics also have less coverage .
We define f specificity ( s ) = s h where s h is the height of topic s in the DAG .
The root topic has height zero and the " height " increases as we move down the DAG in Figure 1 . Topic Clarity : Topic clarity is the fraction of descendant topics that cover one or more documents .
If a topic has many descendant topics that do not cover any documents , it has less clarity .
Formally , f clarity ( s ) = t?descendants ( s ) ?( t ) >0
| descendants ( s ) | , where is the indicator function .
Topic Relevance :
A topic is considered to be better related to a document if the number of hops needed to reach the document from that topic is lower .
Given any set A ?
D of document , and any topic s ?
V , we can define f relevance ( s|A ) = argmin ? {? : A ? ? ? ( s ) }.
QC Functions As Barrier Modular Mixtures :
We introduce a modular function for every QC function as follows f ? specificity ( s ) = 1 if the height of topic s is at least ?
0 otherwise for every possible value of ?.
This creates a submodular mixture with as many components as the number of possible values of ?.
In our experiments with Wikipedia , we had ?
varying from 1 to 120 stepping by 1 , adding 120 modular mixture components .
Similarly , we define , f ? clarity ( s ) = 1 if the clarity of topic s is at least ?
0 otherwise for every possible ( discretized to make it countably finite ) value of ?.
And , f ? relevance ( s ) = f cov ( s |? ? ( s ) ) , where f cov ( ) is the coverage submodular function and s|X indicates coverage of a topic s over a set of documents X . All these functions ( modular and submodular terms ) are added as mixture components in our learning framework to learn suitable weights for them .
We then use these weights in our inference procedure to obtain a subset of topics as described in 3.2 .
We show from our experiments that this approach performs better than all other approaches and baselines .
Fidelity Functions
A function representing the fidelity of a set S to another reference set R is one that gets a large value when the set S represents the set R. Such a function scores inferred topics high when it resembles a reference set of topics and / or item clusters .
The reference set in this case can be produced from other algorithms such as k-means , LDA and its variants or from a manually tagged corpus .
Next we describe one such fidelity function .
Topic Coherence :
This function scores a set of topics S high when the transitive cover ( Definition 1 ) produced by the topics in S resembles the clusters of documents produced by an external source ( k- means , LDA or manual ) .
Given an external source that clusters the documents , producing T clusters L 1 , L 2 , ... , L T ( for T topics ) , topic coherence is defined as : f ( S ) = t?T max k?S w k, t where w k, t = harmonic mean( w p k , t , w r k , t ) and w p k , t = |?( k ) ? Lt| |?(k ) | and w r k , t = |?( k ) ?
Lt| | Lt | .
Note that , w p k, t ?
0 and w r k, t ?
0 are the precision on recall of the resemblance and w k,t is the F1 measure .
If the transitive cover of topics in S resembles the reference clusters L t exactly , we attain maximum coherence ( or fidelity ) .
As the resemblance diminishes , the score decreases .
The above function f ( S ) is monotone submodular .
Mixture of Submodular Components :
Given the different classes of submodular functions above , we construct our submodular scoring functions F w ( ? ) as a convex combinations of these different submodular functions f 1 , f 2 , . . . , f m , above .
In other words , F w ( S ) = m i=1 w i f i ( S ) , ( 2 ) where w = ( w 1 , . . . , w m ) , w i ?
0 , i w i = 1 .
The components f i are submodular and assumed to be normalized : i.e. , f i ( ? ) = 0 , and f i ( V ) = 1 for monotone functions and max A?V f i ( A ) ?
1 for non-monotone functions .
A simple way to normalize a monotone submodular function is to define the component as f i ( S ) / f i ( V ) .
This ensures that the components are compatible with each other .
Obviously , the merit of the scoring function F w ( ? ) depends on the selection of the components .
Large Margin Learning
We optimize the weights w of the scoring function F w ( ? ) in a large-margin structured prediction framework .
In this setting , we assume we have training data in the form of pairs of a set of documents , and a human generated summary as a set of topics .
For example , in the case of Wikipedia disambiguation , we use the human generated disambiguation pages as the ground truth summary .
We represent the set of ground - truth summaries as S = { S 1 , S 2 , ? ? ? , S N }. In large margin training , the weights are optimized such that ground - truth summaries S are separated from competitor summaries by a loss-dependent margin : Fw ( S ) ? Fw ( S ) + L( S ) , ?S ? S , S ? Y \ S , ( 3 ) where L ( ? ) is the loss function , and where Y is a structured output space ( for example Y is the set of summaries that satisfy a certain budget B , i.e. , Y = { S ? V : | S | ? B} ) .
We assume the loss to be normalized , 0 ? L( S ) ? 1 , ?S ? V , to ensure that mixture and loss are calibrated .
Equation ( 3 ) can be stated as F w ( S ) ? max S ?Y [ F w ( S ) + L ( S ) ] , ?S ?
S which is called loss-augmented inference .
We introduce slack variables and minimize the regularized sum of slacks ( Lin and Bilmes , 2012 ) : min w?0 , w 1 =1 S?S max S ?Y Fw ( S ) + L( S ) ? Fw ( S ) + ? 2 w 2 2 , ( 4 ) where the non-negative orthant constraint , w ?
0 , ensures that the final mixture is submodular .
Note a 2 - norm regularizer is used on top of a 1 - norm constraint w 1 = 1 which we interpret as a prior to encourage higher entropy , and thus more diverse mixture distributions .
Tractability depends on the choice of the loss function .
The parameters w are learnt using stochastic gradient descent as in ( Tschiatschek et al. , 2014 ) .
Loss Functions
A natural choice of loss functions for our case can be derived from cluster evaluation metrics .
Every inferred topic s induces a subset of documents , namely the transitive cover ? ( s ) of s .
We compare these clusters with the clusters induced from the true topics in the training set and compute the loss .
In this paper , we use the Jaccard Index ( JI ) as a loss function .
Let S be the inferred topics and T be the true topics .
The Jaccard loss is defined as L jaccard ( S , T ) = 1 ? 1 k s?S max t?T |?(s ) ? ( t ) | |?(s ) ? ( t ) | , where k = | S | = |T | is the number of topics .
When the clustering produced by the inferred and the true topics are similar , Jaccard loss is 0 .
When they are completely dissimilar , the loss is maximum , i.e. , 1 . Jaccard loss is a modular function .
Inference Algorithm : Greedy
Having learnt the weights for the mixture components , the resulting function F w ( S ) = m i=1 w i f i ( S ) is a submodular function .
In the case when the individual components are themselves monotone ( all our functions in fact are ) , F w ( S ) can be optimized by the accelerated greedy algorithm ( Minoux , 1978 ) .
Thanks to submodularity , we can obtain near optimal solutions very efficiently .
In case the functions are all monotone submodular , we can guarantee that the solution is within 1 ? 1/e factor from the optimal solution .
Experimental Results
To validate our approach , we make use of Wikipedia category structure as a topic DAG and apply our technique to the task of automatic generation of Wikipedia disambiguation pages .
We pre-processed the category graph to eliminate the cycles in order to make it a DAG .
Each Wikipedia disambiguation page is manually created by Wikipedia editors by grouping a collection of Wikipedia articles into several groups .
Each group is then assigned a name , which serves as a topic for the group .
Typically , a disambiguation page segregates around 20 - 30 articles into 5 - 6 groups .
Our goal is to measure how accurately we can recreate the groups for a disambiguation page and label them , given only the collection of articles mentioned in that disambiguation page ( when actual groupings and labels are hidden ) .
Datasets
We parsed the contents of Wikipedia disambiguation pages and extracted disambiguation page names , article groups and group names .
We collected about 8000 disambiguation pages that had at least four groups on them .
Wikipedia category structure is used as the topic DAG .
We eliminated few administrative categories such as " Hidden Categories " , " Articles needing cleanup " , and the like .
The final DAG had about 1 M topics and 3 M links .
Evaluation Metrics
Every group of articles on the Wikipedia disambiguation page is assigned a name by the editors .
Unfortunately , these names may not correspond to the Wikipedia category ( topic ) names .
For example , one of the groups on the " Matrix " disambiguation page has a name " Business and government " and there is no Wikipedia category by that name .
However , the group names generated by our ( and baseline ) method are from the Wikipedia categories ( which forms our topic DAG ) .
In addition , there can be multiple relevant names for a group .
For example , a group on a disambiguation page may be called " Calculus " , but an algorithm may rightly generate " Vector Calculus " .
Hence we cannot evaluate the accuracy of an algorithm just by matching the generated group names to those on the disambiguation page .
To alleviate this problem , we adopt cluster - based evaluation metrics .
We treat every group of articles generated by an algorithm under a topic for a disambiguation page as a cluster of articles .
These are considered as inferred clusters for a disambiguation page .
We compare them against the actual grouping of articles on the Wikipedia disambiguation page by treating those groups as true clusters .
We can now adopt Jaccard Index , F1 - measure , and NMI ( Normalized Mutual Information ) based cluster evaluation metrics described in ( Manning et al. , 2008 ) .
For each disambiguation page in the test set , we compute every metric score and then average it over all the disambiguation pages .
Methods Compared
We validated our approach by comparing against several baselines described below .
We also compared two variations of our approach as described next .
For each of these cases ( baselines and two variations ) we generated and compared the metrics ( Jaccard Index , F1 - measure and NMI ) as described in the previous section .
KM docs : K- Means algorithm run on articles as TF - IDF vectors of words .
The number of clusters K is set to the number of true clusters on the Wikipedia disambiguation page .
KMed docs : K- Medoids algorithm with articles as TF - IDF vectors of words .
The number of clusters are set as in KM docs .
KMed topics : K- Medoids run on topics as TF - IDF vectors of words .
The words for each topic is taken from the articles that are in the transitive cover of the topic .
LDA docs : LDA algorithm with the number of topics set to the number of true clusters on the Wikipedia disambiguation page .
Each article is then grouped under the highest probability topic .
SMML cov :
This is the submodular mixture learning case explained in section 3.1.5 .
Here we consider a mixture of all the submodular functions governing coverage , diversity , fidelity and QC functions .
However , we exclude the similarity based functions described in section 3.1.2 .
Coverage based functions have a time complexity of O ( n ) whereas similarity based functions are O n 2 .
By excluding similarity based functions , we can compare the quality of the results with and without O(n 2 ) functions .
We learn the mixture weights from the training set and use them during inference on the test set to subset K topics through the submodular maximization ( Equation 1 ) .
SMML cov+sim :
This case is similar to SMML cov except that , we include similarity based submodular mixture components .
This makes the inference time complexity O n 2 .
We do not compare against HSLDA , PAM and few other techniques cited in the related work sections because they do not produce a subset of K summary topics - these are not directly comparable with our work .
Evaluation Results
We show that the submodular mixture learning and maximization approaches , i.e. , SMML cov and SMML cov+sim outperform other approaches in various metrics .
In all these experiments , we performed 5 fold cross validation to learn the parameters from 80 % of the disambiguation pages and evaluated on the rest of the 20 % , in each fold .
In Figure 2a we summarize the results of the comparison of the methods mentioned above on Jaccard Index , F1 measure and NMI .
Our proposed techniques SMML cov and SMML cov+sim outperform other techniques consistently .
In Figures 2 b and 2 c we measure the number of test instances ( i.e. , disambiguation queries ) in which each of the algorithms dominate ( win ) in evaluation metrics .
In 60 % of the disambiguation queries , SMML cov and SMML cov+sim approaches From Figures 2 b and 2 c it is clear that O ( n ) time complexity based submodular mixture functions ( SMML cov ) perform on par with O n 2 based functions ( SMML cov+sim ) , but at a greatly reduced execution time , demonstrating the sufficiency of O ( n ) functions for our task .
On the average , for each disambiguation query , SMML cov took around 40 seconds ( over 1 M topics and 3 M edges DAG ) to infer the topics , whereas SMML cov+sim took around 35 minutes .
Both these experiments were carried on a machine with 32 GB RAM , Eight - Core AMD Opteron ( tm ) Processor 2427 .
Conclusions
We investigated a problem of summarizing topics over a massive topic DAG such that the summary set of topics produced represents the objects in the collection .
This representation is characterized through various classes of submodular ( and monotone ) functions that captured coverage , similarity , diversity , specificity , clarity , relevance and fidelity of the topics .
Currently we assume that the number of topics K is given as an input to our algorithm .
It would be an interesting future problem to estimate the value of K automatically in our setting .
As future work , we also plan to extend our techniques to produce a hierarchical summary of topics and scale it across heterogeneous collection of objects ( from different domains ) to bring all of them under the same topic DAG and investigate interesting cases thereon .
Figure 1 : 1 Figure1 : Topic Summarization overview .
On the left , we show many documents related to Apple .
In the middle , a Wikipedia category hierarchy shown as a topic DAG , links these documents at the leaf level .
On the right , we show the output of our summarization process , which creates a set of summary topics ( Plants , Technology , Companies , Films , Music and Places in this example ) with the input documents classified under them .
Figure 2 : 2 Figure 2 : Comparison of techniques
http://qwone.com/ ?jason/20Newsgroups / 2 http://www.daviddlewis.com/resources/ testcollections / reuters21578 / 3 Topic Concept is more abstract than the topic Science which is more abstract than the topicChemistry
A simple greedy algorithm ( Nemhauser et al. , 1978 ) obtains a 1 ? 1/e approximation guarantee for monotone submodular function maximization
