title
SupMMD : A Sentence Importance Model for Extractive Summarization using Maximum Mean Discrepancy
abstract
Most work on multi-document summarization has focused on generic summarization of information present in each individual document set .
However , the under-explored setting of update summarization , where the goal is to identify the new information present in each set , is of equal practical interest ( e.g. , presenting readers with updates on an evolving news topic ) .
In this work , we present SupMMD , a novel technique for generic and update summarization based on the maximum mean discrepancy from kernel two -sample testing .
SupMMD combines both supervised learning for salience and unsupervised learning for coverage and diversity .
Further , we adapt multiple kernel learning to make use of similarity across multiple information sources ( e.g. , text features and knowledge based concepts ) .
We show the efficacy of SupMMD in both generic and update summarization tasks by meeting or exceeding the current state - of- the - art on the DUC - 2004 and TAC - 2009 datasets .
Introduction Multi-document summarization is the problem of producing condensed digests of salient information from multiple sources , such as articles .
Concretely , suppose we are given two sets of articles ( denoted set A and set B ) on a related topic ( e.g. , climate change , the COVID-19 pandemic ) , separated by publication timestamp or geographic region .
We may then identify three possible instantiations of multi-document summarization ( see Figure 1 ) : ( i ) generic summarization , where the goal is to summarize a set ( A or B ) individually .
( ii ) comparative summarization , where the goal is to summarize a set ( B ) against another set ( A ) while highlighting the differences .
( iii ) update summarization , where the goal is both generic summarization of set A and comparative summarization of set B versus A. Most existing work on this topic has focused on the generic summarization task .
However , update summarization is of equal practical interest .
Intuitively , the comparative aspect of this setting aims to inform a user of new information on a topic they are already familiar with .
Multi-document extractive summarization methods can be unsupervised or supervised .
Unsupervised methods typically define salience ( or coverage ) using a global model of sentence -sentence similarity .
Methods based on retrieval ( Goldstein et al. , 1999 ) , centroids , graph centrality ( Erkan and Radev , 2004 ) , or utility maximization ( Lin and Bilmes , 2010 , 2011 ; have been well explored .
However , sentence salience also depends on surface features ( e.g. , position , length , presence of cue words ) ; effectively capturing these requires supervised models specific to the dataset and task .
A body of work has incorporated such information through supervised learning , for example based on point processes ( Kulesza and Taskar , 2012 ) , learning important words , graph neural networks ( Yasunaga et al. , 2017 ) , and support vector regression ( Varma et al. , 2009 ) .
These supervised methods have either a separate model for learning and inference , leading to a disconnect between learning sentence salience and sentence selection ( Varma et al. , 2009 ; Yasunaga et al. , 2017 ; , or are designed specifically for generic summarization ( Kulesza and Taskar , 2012 ) .
In this work , we propose SupMMD , which has a single model of learning salience and inference and can be applied to generic and comparative summarization .
We make the following contributions : ( 1 ) We present SupMMD , a novel technique for both generic and update summarization that combines supervised learning for salience and unsupervised learning for coverage and diversity .
SupMMD has a single model for learning and inference .
( 2 ) We adapt multiple kernel learning ( Cortes et al. , 2010 ) into our model , which allows similarity across multiple information sources ( e.g. , text features and knowledge based concepts ) to be used .
( 3 ) We show that SupMMD meets or exceeds the state - of - the - art in generic and update summarization on the DUC - 2004 and TAC - 2009 datasets .
Literature Review Multi-document summarization can be extractive , where salient pieces of the original text such as sentences are selected to form the summary ; or abstractive , where a new text is generated by paraphrasing important information .
The former is popular as it often creates semantically and grammatically correct summaries ( Nallapati et al. , 2017 ) .
In this work , we focus on generic and update multidocument summarization in the extractive setting .
Most extractive summarizers have two components : sentence scoring and selection .
A variety of unsupervised and supervised methods have been developed for the former .
Unsupervised sentence scorers are based on centroids , graph centrality ( Erkan and Radev , 2004 ) , retrieval relevance ( Goldstein et al. , 1999 ) , word statistics ( Nenkova and Vanderwende , 2005 ) , topic models ( Haghighi and Vanderwende , 2009 ) , or concept coverage Lin and Bilmes , 2011 ) .
Supervised techniques include : using a graph - based neural network ( Yasunaga et al. , 2017 ) , learning sentence quality from point processes ( Kulesza and Taskar , 2012 ) , combining word importances , combining sentence and phrase importances ( Cao et al. , 2015 ) , or employing a mixture of submodular functions ( Lin and Bilmes , 2012 ) .
Sentence selection methods can be broadly categorized as greedy methods ( Goldstein et al. , 1999 ; Erkan and Radev , 2004 ; Nenkova and Vanderwende , 2005 ; Cao et al. , 2015 ; Haghighi and Vanderwende , 2009 ; Kulesza and Taskar , 2012 ; Cao et al. , 2015 ; Varma et al. , 2009 ) , which produce approximate solutions by iteratively selecting the sentences with the maximal score , or exact integer linear programming ( ILP ) based methods Cao et al. , 2015 ) .
Some greedy methods use an objective which belongs to a special class of set functions called submodular functions ( Lin and Bilmes , 2010 Kulesza and Taskar , 2012 ) , which have good approximation guarantees under greedy optimization ( Nemhauser et al. , 1978 ) .
There has been limited research into update and comparative summarization .
Notable prior work includes maximizing concept coverage using ILP , learning sentence scores using a support vector regressor ( Varma et al. , 2009 ) , and temporal content filtering .
Bista et al. ( 2019 ) cast the comparative summarization problem as classification , and use MMD ( Gretton et al. , 2012 ) .
In this work , we adapt their method to learn sentence importances driven by surface features .
Summarization as Classification
We review a perspective introduced by Bista et al . ( 2019 ) , where summarization is viewed as classification , and provide a brief introduction to Maximum Mean Discrepancy ( MMD ) .
Both these ideas form the basis of our subsequent method .
Generic Summarization as Classification Let { V t } T t=1 be T topics of articles that we wish to summarize .
For a topic t , we wish to select summary sentences S t . Bista et al. ( 2019 ) formulated summarization as selecting prototypes that minimize the accuracy of a powerful classifier between sentences in the input and summary .
The intuition is that a powerful classifier should not be able to distinguish between the sentences from articles and summary sentences .
Formally , we pick S t = argmax S?S t ? Acc( V t , S ) , ( 3.1 ) where S t ?
2 V t : ? S ?S s?S len(s ) ?
L comprise subsets of V t with upto L words , and Acc ( X , Y ) is the accuracy of the best possible classifier that distinguishes between elements in sets X and Y ; we shall shortly realize this using MMD .
Comparative Summarization as Competing Binary Classification
For comparative summarization between two sets A and B , Bista et al. ( 2019 ) introduced an additional term into ( 3.1 ) , giving rise to competing goals for the classifier : it should not be able to distinguish between the summaries and sentences from set B , but should be able to distinguish between the summaries and sentences from set A. Formally , let V t B be the set of sentences in set B , V t A be the sentences in set to compare ( set A ) .
Then , for suitable ? > 0 , we seek S t , the summary sentences of set B , S t = argmax S?S t ?Acc ( V t B , S ) + ? ? Acc( V t A , S ) . ( 3.2 )
The hyperparameter ?
controls the relative importance of accurately representing articles in set B , versus not representing the articles in set A .
Maximum Mean Discrepancy ( MMD )
The MMD is a kernel - based measure of the distance between two distributions .
More formally : Definition 3.1 . Let H be a Reproducing Kernel Hilbert Space ( RKHS ) with associated kernel k.
Let F be the set of functions h : X ?
R in the unit ball of H , where X is a topological space .
Then , the MMD between distributions p , q is the maximal difference in expectations of functions from F under p , q ( Gretton et al. , 2012 ) : MMD F ( p , q ) = sup h?F E x?p [ h ( x ) ] ? E y?q [ h ( y ) ] . ( 3.3 ) A small MMD value indicates that p , q are similar .
Given finite samples X ? p n and Y ? q m , an empirical estimate of the MMD , denoted as MMD 2 F ( X , Y ) , can be computed as : 1 n 2 x , x k( x , x ) + 1 m 2 y,y k(y , y ) ?
2 n ? m x,y k( x , y ) . ( 3.4 )
MMD for Summarization
The MMD corresponds to the minimal achievable loss of a centroi-based kernel classifier ( Sriperumbudur et al. , 2009 ) .
Consequently , we use MMD 2 F ( V , S ) to approximate the Acc ( V , S ) in ( 3.1 ) and ( 3.2 ) , using a suitable kernel k that measures the similarity of two sentences .
Intuitively , this selects summaries S which best represent the distribution of original sentences V . Note that if we expand MMD 2 F ( V , S ) as per ( 3.4 ) and later in ?4.6 , the first term is irrelevant for optimization .
The second and third term capture the coverage and diversity of the summary sentences without any supervision .
Hence , this is an unsupervised summarization .
The SupMMD Method
We start by developing a technique for incorporating sentence importance into MMD for the purpose of generic multi-document extractive summarization .
We then extend this method to comparative summarization , and incorporate multiple different kernels to use a diverse sets of features .
From MMD to Weighted MMD Unsupervised MMD ( Bista et al. , 2019 ) selects representative sentences that cover relevant concepts while retaining diversity .
The notion of representativeness is based on a global model of sentencesentence similarity ; however , this notion of representativeness is not necessarily well matched to the selection of salient information .
Salience of a sentence may be determined by surface features such as position in the article , or number of words .
For example , news articles are often written such that sentences at the start of a article have the characteristics of a summary ( Kedzie et al. , 2018 ) .
Learning a notion of salience that is specific to the summarization task and dataset requires supervised training .
Thus , we extend the MMD model by incorporating supervised sentence importance weighting .
Let v , s ?
X be independent samples drawn from the distributions of article sentences p and summary sentences q on the space of all sentences X . We define non-negative importance functions f p ? , f q ? parameterized by learnable parameters ?.
We restrict these functions so that E p f p ? ( v ) = 1 and E q f q ? ( s ) = 1 .
Equipped with f ? , we may modify MMD such that the importance of sentences which are good summary candidates is increased .
Definition 4.1 .
The weighted MMD MMD F ( p , q , ? ) between p , q is sup h?F E p f p ? ( v ) ? h( v ) ?
E q f q ? ( s ) ? h( s ) ( 4.1 )
Note that classic MMD ( 3.3 ) is a special case of ( 4.1 ) where f ? ? 1 . In practice , the supremum over all h is impossible to compute directly .
We thus derive an alternative form for Equation 4 .1 . Lemma 4.1 . For h H ? 1 , ( 4.1 ) is equivalently E p [ f p ? ( v ) ? ?( v ) ] ? E q [ f q ? ( s ) ? ?( s ) ]
H . ( 4.2 )
In the above , ? : X ?
F is a canonical feature mapping of sentences and summaries from X to RKHS .
The derivation , which mirrors a similar derivation for MMD ( Gretton et al. , 2012 ) , is given in the Appendix .
Importance Function
We use log-linear models as importance functions , as they are a common choice of sentence importance ( Kulesza and Taskar , 2012 ) and easy to fit when training data is scarce .
Formally , the log-linear importance function is : f ? ( v ) = exp ( ? , ?( v ) ) , where ?( v ) is the surface features of sentence v.
We can define the empirical estimates f nt ? ( v ) , f mt ? ( s ) of the importance functions f p ? ( v ) and f q ? ( s ) as : f nt ?
( v ) = f ? ( v ) v ?V t f ? ( v ) ?
n t f mt ?
( s ) = f ? ( s ) s ?S t f ? ( s ) ? m t ( 4.3 ) where nt = | V t | is the number of sentences and mt = | S t | is the number of summary sentences in topic t.
Training : Generic Summarization
The parameters ? of the log-linear importance function must be learned from data , so we define a loss function based on weighted MMD .
Let {( V t , S t ) }
T t=1 be the T training tuples .
Then , the loss of topic t is the square of importance weighted empirical MMD between sentences and summary sentences from within the topic : L t ( V t , S t , ? ) = MMD 2 F ( V t , S t , ? ) ( 4.4 ) where MMD 2 F ( V t , S t , ? ) is an empirical estimate of the weighted MMD 2 F ( p , q , ? ) .
Applying the kernel trick to Equation 4.4 gives ( see Appendix ) : the parameters ? by minimizing an importance weighted distance between sentences and ground truth summary sentences over all topics .
Training : Comparative Summarization
We now extend the learning task to comparative summarization using the competing binary classifiers idea of Bista et al . ( 2019 ) ( cf. ?3.2 ) .
Specifically , we replace the accuracy terms in Equation 3 .2 with the square of weighted MMD .
Given the T comparative training tuples {( V t B , V t A , S t ) }
T t=1 , then the objective is to minimize : min ? B , ?
A 1 T t L t ( V t B , S t , ? B ) ? ? ? L t ( V t A , S t , ? A ) ( 4.6 )
Note there are two sets of importance parameters ?
B , ?
A one for each of the two document sets .
Multiple Kernel Learning
We employ Multiple Kernel Learning ( MKL ) to make use of data from multiple sources in our MMD summarization framework .
We adapt two stage kernel learning ( Cortes et al. , 2010 ) , where different kernels are linearly combined to maximize the alignment with the target kernel of the classification problem .
Since MMD can be interpreted as classifiability ( Sriperumbudur et al. , 2009 ) MKL fits neatly into our MMD based summarization objective .
Intuitively , MKL should identify a good combination of kernels for building a classifier that separates summary and non-summary sentences .
Let { k i } p i=1 be p kernel functions .
For topic t , let K t i be the kernel matrix according to kernel function k i , and K t i = U nt K t i U nt be the centered kernel matrix , with U nt = I ? 11 T /n t .
Let y t = { ?1 } n t be the ground truth summary labels with y t i = + 1 iff i ?
S t .
The target kernel y t ( y t ) T represents the ideal notion of similarity between sentences .
The non-negative kernel weights w which lead to the optimal alignment with the target kernel are given by ( Cortes et al. , 2010 ) min w?0 w T ( M t ) T w ? 2w T a t , ( 4.7 ) where M t ?
R p?p has M t rs = K r , K s F and a t ?
R p has a i = K t i , y t ( y t ) T F .
The kernel function must be characteristic for MMD to be a valid metric ( Muandet et al. , 2017 ) .
Most popular kernels used for bag of words like text features ( including TF - IDF ) , the linear kernel ( k( x , y ) = x , y ) and the cosine kernel ( k( x , y ) = x,y x y ) , are not characteristic ( Sriperumbudur et al. , 2010 ) .
Fortunately , the exponential kernel , k( x , y ) = exp ( ? k ( x , y ) ) , ? > 0 , is characteristic for any kernel k ( Steinwart , 2001 ) .
Hence , we use the normalized exponential kernel combined with the cosine kernel , k( x , y ) = exp ( ? ) exp ? p i=1 w i ? cos x ( i ) , y ( i ) .
Inference Given a learned importance function f ? , we may find the best set of summary sentences
St for generic summarization via :
St = argmax S?S t ?
L t ( V t , S t , ? ) ( 4.8 )
Similarly , for the comparative task , with learned importance functions , we seek St as : argmax S?S t ( ? L t ( V t B , S t , ? B ) + ?L t ( V t A , S t , ? A ) ) ( 4.9 )
Both these inference problems are budgeted maximization problems , which are often solved by greedy algorithms ( Lin and Bilmes , 2010 ) .
The generic unsupervised summarization task is submodular and monotone under certain conditions ( Kim et al. , 2016 ) , so greedy algorithms have good theoretical guarantees ( Nemhauser et al. , 1978 ) .
While our supervised variants do not have these guarantees , we find that greedy optimization nonetheless leads to good solutions .
Experimental Setup
We include guidance on applying SupMMD and the details required to reproduce our experiments .
Datasets
We use four standard multi-document summarization benchmark datasets : DUC -
Data Preprocessing and Preparation
The DUC and TAC datasets are provided as collections of XML documents , so it is necessary to extract relevant text and then perform sentence and word tokenization .
For DUC we clean the text using various regular expressions the details of which are provided in our code release .
We train PunktSentenceTokenizer to detect sentence boundaries , and use the standard NLTK ( Bird , 2006 ) word tokenizer .
For the TAC dataset , we use the preprocessing pipeline employed by 2 .
This enables a cleaner comparison with the state - of- the - art ICSI method on the TAC dataset .
For all datasets , we keep the sentences between 8 and 55 words per Yasunaga et al . ( 2017 ) .
Feature Representations
Our method requires two different sets of sentence features : text features , which are used to compute the sentence -sentence similarity as part of the kernel ; and surface features , which are used in learning the sentence importance model .
Text Features
Each sentence has three different feature representations : unigrams , bigrams and entities .
The unigrams are stemmed words , with stop words from the NLTK english list removed .
The bigrams are a combination of stemmed unigrams and bigrams .
The entities are DBPedia concepts extracted using DBPedia Spotlight ( Mendes et al. , 2011 ) .
We use a Term Frequency Inverse Sentence Frequency ( TF - ISF ) ( Neto et al. , 2000 ) representation for all text features .
TF - ISF has been used extensively in multi-document summarization ( Dias et al. , 2007 ; Alguliev et al. , 2011 ; Wan et al. , 2007 ) .
Surface Features
We use 10 surface features for the DUC dataset , and 12 for the TAC dataset : position :
There are five position features .
Four indicators denote the 1 st , 2 nd , 3 rd or a later position of the sentence in the article .
The final feature gives the position relative to the length of the article .
counts :
There are two count features : the number of words and number of nouns .
We use the spaCy 3 part of speech tagging to find nouns .
tfisf :
This is the sum of the TS - ISF scores for unigrams composing the sentence .
For sentence
We report the number of topics in each dataset , along with the number of sentences after preprocessing .
We show the ROUGE scores of our oracle method and the one by Liu and Lapata ( 2019 ) with average number of sentence in summary from each method .
s , this is w?s is f ( w ) ? tf ( w , s ) , where is f ( w ) is the inverse sentence frequency of unigram w , and tf (w , s ) is the term frequency of w in s. btfisf :
The boosted sum of TS - ISF scores for unigrams composing the sentence .
Specifically , we compute w?s is f ( w ) ? b( w ) ? tf ( w , s ) , where we boost the score of unigrams w that appear in the first sentence of the article as b ( w ) .
In the generic summarization b( w ) = 2 , for comparative summarization b( w ) = 3 , as used by .
Unigrams that do not appear in the first sentence of the article have b( w ) = 1 . lexrank The LexRank score ( Erkan and Radev , 2004 ) computed on the bigrams ' cosine similarity .
For the TAC datasets , we additionally use : par_start :
An indicator whether the sentence begins a paragraph .
This is provided by the preprocessing pipeline from ICSI . qsim :
The fraction of topic description unigrams present in each sentence ; these topic descriptions are only available for TAC .
Oracle Extraction Both DUC and TAC provide four human written summaries for each topic .
Since our goal is extractive summarization with supervised training , we need to know which sentences in the articles could be used to construct the summaries in the training set .
The article sentences that best match the abstractive summaries are called the oracles ( S t ) .
Algorithm 1 Oracle extraction 1 : function EXTRACTORACLE ( ? , V t , H t , r , L ) 2 : S t ? ? 3 : while s?S t len(s ) ?
L do 4 : s * ? argmax s?V t \S t ?( S t ?{ s} , H t ) ?( S t , H t ) len( s ) r 5 : S t ?
S t ? {s * } return S t Our extraction algorithm ( Algorithm 1 ) , is inspired by Liu and Lapata ( 2019 ) .
We greedily select sentences ( s ) which provide the maximum gain in extraction score ?( S t , H t ) against the human summaries ( H t ) until a word budget ( L ) is reached .
We only include sentences between 8 to 55 words as suggested by Yasunaga et al . ( 2017 ) , and set a budget of 104 words to ensure our oracle summaries are within 100 ? 4 words , consistent with the evaluation ( ?5.6 ) .
In contrast to Liu and Lapata ( 2019 ) which uses only ROUGE - 2 recall score ( Lin , 2004 ) , our method balances both ROUGE - 1 and ROUGE - 2 recall scores using the harmonic mean and explicitly accounts for sentence length .
Grid search on the validation sets shows that the optimal value for r is 0.4 across different datasets and summarization tasks .
As reported in Table 1 , on average our method produces oracles consisting of more sentences and with higher ROUGE - 1 and ROUGE - 2 scores compared to oracles from Liu and Lapata ( 2019 ) .
This is consistent across all datasets .
Implementation Details Supervised variants use an 2 regularized log-linear model of importance ( ?4.2 ) trained using the oracles ( ?5.4 ) as ground truth .
We selected the number of training epochs using 5 - fold cross validation .
We then tune the other hyperparameters on the training set .
The hyperparameters of the generic summarization task are : ? , a parameter of the kernel ; ? , the 2 regularization weight for the log-linear importance function ; and r , which defines the length dependent scaling factor in greedy selection ( Lin and Bilmes , 2010 ) .
The comparative objective ( 4.6 ) has an additional hyperparameter ? , which controls the comparativeness .
More implementation details are provided in the Appendix .
We will make implementation publicly available 4 .
Evaluation Settings
To evaluate our methods we use the ROUGE ( Lin , 2004 ) metric , the de facto choice for evaluating both generic summarization Cho et al. , 2019 ; Yasunaga et al. , 2017 ; Kulesza and Taskar , 2012 ) , and update summarization ( Varma et al. , 2009 ; Li et al. , 2009 ) . ROUGE metrics have been shown to correlate with human judgments ( Lin , 2004 ) in generic summarization task .
Our recent work ( Bista et al. , 2019 ) shows that human judgments are consistent with the automatic metrics for evaluating comparative summaries .
Both DUC and TAC evaluations use the first 100 words of the generated summary .
Our DUC - 2004 evaluation setup mirrors .
This allows us to compare performance with the stateof - the - art methods they reported and other works also evaluated using this setup 5 .
As is standard for the DUC - 2004 datasets , we report ROUGE - 1 and ROUGE - 2 recall scores .
For TAC - 2009 datasets ( both set A and B ) , we adopt the evaluation settings from the TAC - 2009 competition 6 so we can compare against the three best performing systems in the competition 7 .
As is standard for the TAC - 2009 dataset , we report ROUGE - 2 and ROUGE - SU4 recall scores .
Baselines DUC -2004 : We select the top performing methods from a recent benchmark paper to serve as baselines and report ROUGE scores from the benchmark paper .
They are : ICSI : an integer linear programming method that maximizes coverage , DPP : a determinantal point process method that learns sentence quality and maximizes diversity ( Kulesza and Taskar , 2012 ) , Submodular : a method based on a learned mixture of submodular functions ( Lin and Bilmes , 2012 ) , OCCAMS_V : a method base on topic modeling ( Conroy et al. , 2013 ) , Regsum : a method that focuses on learning word importance , Lexrank : a popular graph based sentence scoring method ( Erkan and Radev , 2004 ) .
We also include recent deep learning methods evaluated using the same setup as and report ROUGE scores from the individual papers : DPPSim : an extension to the DPP model which learns the sentence -sentence similarity using a capsule network ( Cho et al. , 2019 ) , HiMAP : a recurrent neural model that employs a modified pointer - generator component ( Fabbri et al. , 2019 ) , and GRU + GCN : a model that uses a graph convolution network combined with a recurrent neural network to learn sentence saliency ( Yasunaga et al. , 2017 ) . TAC - 2009 : As baselines for the TAC - 2009 dataset we use the top three systems in the TAC - 2009 competition for each task , resulting in four systems altogether .
To the best of our knowledge these systems are the current state - of- the- art .
We report the ROUGE scores from the competition .
The systems are : ICSI : with two variants :
Sys.34 uses integer linear programming to maximize coverage of concepts , and Sys.40 , which additionally uses sentence compression to generate new candidate sentences , IIT : uses a support vector regressor to predict sentence ROUGE scores ( Varma et al. , 2009 ) , ICTCAS : a temporal content filtering method , and ICL : a manifold ranking based method ( Li et al. , 2009 ) .
Experimental Results
We compare our methods with the baselines on the DUC -2004 , TAC - 2009 -A and TAC - 2009
We present several variants of our method to analyze the effects of different components and modeling choices .
We report the performance of unsupervised MMD ( UnsupMMD ) which does not explicitly consider sentence importance .
For our supervised method SupMMD , we report the performance with a bigram kernel ( SupMMD ) and combined kernels ( SupMMD + MKL ) .
We also evaluated the impact of our oracle extraction method by replacing it with the extraction method suggested by Liu and Lapata ( 2019 ) in SupMMD + alt oracles .
Meanwhile , SupMMD + MKL + compress presents the result of applying sentence compression to our model .
Generic Summarization
The performance of our methods on the DUC - 2004 generic summarization task are shown in Table 2 .
DUC-2004 R1 R2 ICSI 38.41 9.78 DPP ( Kulesza and Taskar , 2012 ) 39.79 9.62 Submodular ( Lin and Bilmes , 2012 ) 39.18 9.35 OCCAMS_V ( Conroy et al. , 2013 ) 38.50 9.76 Regsum Nenkova , 2014 ) 38.57 9.75 Lexrank Erkan and 35.95 7.47 DPP - Sim ( Cho et al. , 2019 ) 39.35 10.14 HiMAP ( Fabbri et al. , 2019 ) 35.78 8.90 GRU +GCN ( Yasunaga et al. , 2017 ) Supervised Modeling : Models using supervised training to identify important sentences substantially outperform the unsupervised method Un-supMMD .
In fact , UnsupMMD is the lowest scoring method across all metrics and datasets .
This strongly indicates that a degree of supervision is essential to perform well in this task , and that the importance function is a suitable way to adapt the Un-supMMD model to supervised training .
Moreover , we observe a strong correlation between the the relative position of a sentence and the score given by SupMMD .
This observation is consistent with previous works ( Kedzie et al. , 2018 ) , and demonstrates that SupMMD has learned to use the surface features to capture salience .
Further details of feature correlations are provided in the Appendix .
TAC-2009 -A R2 RSU4 ICSI ( Sys.34 ) 12.10 15.09 ICSI ( Sys.40 ) 12.16 15.03 IIIT ( Sys.35 ) ( Varma et al. , 2009 ) 10.89 14.49 ICTCAS ( Sys.45 )
Oracle extraction :
Our oracle extraction technique for transforming abstractive training data to extractive training data helps SupMMD methods achieve higher ROUGE performance .
An alternative technique developed by Liu and Lapata ( 2019 ) and implemented in SupMMD ( alt oracle ) gives lower performance than our technique .
For example , on DUC - 2004 SupMMD ( alt oracle ) has a ROUGE - 1 of 39.02 and ROUGE - 2 of 10.22 , while SupMMD has a ROUGE - 1 of 39.36 and a ROUGE - 2 of 10.31 .
Thus , the advantages of our proposed oracle extraction method are substantial and consistent across multiple datasets and evaluation metrics .
Multiple Kernel Learning :
We observe that combining multiple kernels helps the performance of SupMMD models on the generic summarization task .
SupMMD + MKL which combines both bigram and entity kernels has a ROUGE - 2 of 10.54 on DUC - 2004 , while SupMMD only uses the bigrams kernel and scores 10.31 in ROUGE - 2 .
Multiple kernels show even clearer gains in the TAC - 2009 - A dataset .
Sentence compression incorporated into the post-processing steps of SupMMD + MKL + compress does not clearly improve the results over SupMMD + MKL .
On TAC - 2009 -A , compression clearly reduces performance , and on DUC - 2004 SupMMD + MKL + compress has a higher ROUGE - 1 score but a lower ROUGE - 2 score than SupMMD + MKL .
Incorporating compression into the summarization pipeline is an appealing direction for future work .
Comparative Summarization
The results for the comparative summarization task on the TAC - 2009 - B dataset are shown in Table 4 . Our supervised MMD variants SupMMD and Sup-MMD + MKL both outperform the state- of- theart baseline ICSI in ROUGE - SU4 but fall short in ROUGE -2 .
It would be hard to claim that either method is superior in this instance ; however , it does show that SupMMD - which uses a substantially different approach to that of ICSI - provides an alternative state - of - the - art .
Thus SupMMD further maps out the set of techniques that are useful for comparative summarization .
As per the generic summarization task , both our supervised training method and oracle extraction method are essential for achieving good performance in ROUGE - 2 and ROUGE - SU4 .
We also identify sentence position and btfisf as important features for sentence salience ( see the Appendix ) .
Multiple kernels as in SupMMD + MKL has relatively little effect , reducing the ROUGE - 2 score to 10.24 from the slightly higher 10.28 achieved by SupMMD .
A similar small decrease is seen for ROUGE - SU4 .
Manual inspection shows that the summaries from SupMMD and SupMMD + MKL methods are largely identical with differences primarily on topic D0908 , which covers political movements in Nepal .
The key entities in this topic are not resolved accurately by DBPedia Spotlight , contributing additional noise and affecting the MKL approach .
Model variants :
We have tested an additional variant of our model for comparative summarization , SupMMD 2 , which defines two different importance functions : one for each of the two document sets - A and B ( See ?4.4 for details ) .
In contrast , SupMMD has a single importance function shared between document sets , i.e. , in Equation ( 4.6 ) , ? A = ? B . SupMMD 2 performed substantially worse than SupMMD in both metrics , for example , SupMMD has a ROUGE - 2 of 10.28 while SupMMD 2 has a ROUGE - 2 of 9.94 .
We conjecture that a single importance function performs better when training data is relatively scarce because it reduces the number of parameters and simplifies the learning problem .
Techniques for tying together the parameters for both importance functions , such as with a hierarchical Bayesian model , are left as future work .
Conclusion
In this work , we present SupMMD , a novel technique for update summarization based on the maximum mean discrepancy .
SupMMD combines supervised learning for salience , and unsupervised learning for coverage and diversity .
Further , we adapt multiple kernel learning to exploit multiple sources of similarity ( e.g. , text features and knowledge based concepts ) .
We show the efficacy of Sup-MMD in both generic and update summarization tasks on two standard datasets , when compared to the existing approaches .
We also show that the importance model we introduce on top of our existing unsupervised MMD ( Bista et al. , 2019 ) improves the summarization performance substantially on both generic and comparative summarization tasks .
For future work , we leave the task of incorporating embeddings features such as BERT ( Devlin et al. , 2019 ) , and evaluating with large generic multi-document summarization dataset MultiNews ( Fabbri et al. , 2019 ) .
