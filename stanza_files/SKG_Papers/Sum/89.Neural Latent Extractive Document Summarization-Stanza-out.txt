title
Neural Latent Extractive Document Summarization
abstract
Extractive summarization models require sentence - level labels , which are usually created heuristically ( e.g. , with rule- based methods ) given that most summarization datasets only have document-summary pairs .
Since these labels might be suboptimal , we propose a latent variable extractive model where sentences are viewed as latent variables and sentences with activated variables are used to infer gold summaries .
During training the loss comes directly from gold summaries .
Experiments on the CNN / Dailymail dataset show that our model improves over a strong extractive baseline trained on heuristically approximated labels and also performs competitively to several recent models .
Introduction Document summarization aims to automatically rewrite a document into a shorter version while retaining its most important content .
Of the many summarization paradigms that have been identified over the years ( see Mani 2001 and Nenkova and McKeown 2011 for comprehensive overviews ) , two have consistently attracted attention : extractive approaches generate summaries by copying parts of the source document ( usually whole sentences ) , while abstractive methods may generate new words or phrases which are not in the document .
A great deal of previous work has focused on extractive summarization which is usually modeled as a sentence ranking or binary classification problem ( i.e. , sentences which are top ranked or predicted as True are selected as summaries ) .
Early attempts mostly leverage human-engineered features ( Filatova and Hatzivassiloglou , 2004 ) coupled with binary classifiers ( Kupiec et al. , 1995 ) , hidden Markov models ( Conroy and O'leary , 2001 ) , graph based methods ( Mihalcea , 2005 ) , and integer linear programming ( Woodsend and Lapata , 2010 ) .
The successful application of neural network models to a variety of NLP tasks and the availability of large scale summarization datasets ( Hermann et al. , 2015 ; Nallapati et al. , 2016 ) has provided strong impetus to develop data-driven approaches which take advantage of continuousspace representations .
Cheng and Lapata ( 2016 ) propose a hierarchical long short - term memory network ( LSTM ; Hochreiter and Schmidhuber 1997 ) to learn context dependent sentence representations for a document and then use yet another LSTM decoder to predict a binary label for each sentence .
Nallapati et al. ( 2017 ) adopt a similar approach , they differ in their neural architecture for sentence encoding and the features used during label prediction , while Narayan et al . ( 2018 ) equip the same architecture with a training algorithm based on reinforcement learning .
Abstractive models ( Nallapati et al. , 2016 ; See et al. , 2017 ; Paulus et al. , 2017 ) are based on sequenceto-sequence learning Bahdanau et al. , 2015 ) , however , most of them underperform or are on par with the baseline of simply selecting the leading sentences in the document as summaries ( but see Paulus et al. 2017 and Celikyilmaz et al. 2018 for exceptions ) .
Although seemingly more successful than their abstractive counterparts , extractive models require sentence - level labels , which are not included in most summarization datasets ( only document and gold summary pairs are available ) .
Sentence labels are usually obtained by rule- based methods ( Cheng and Lapata , 2016 ) or by maximizing the ROUGE score ( Lin , 2004 ) between a subset of sentences and the human written summaries ( Nallapati et al. , 2017 ) .
These methods do not fully exploit the human summaries , they only create True / False labels which might be suboptimal .
In this paper we propose a latent variable extractive model and view labels of sentences in a document as binary latent variables ( i.e. , zeros and ones ) .
Instead of maximizing the likelihood of " gold " standard labels , the latent model directly maximizes the likelihood of human summaries given selected sentences .
Experiments on the CNN / Dailymail dataset ( Hermann et al. , 2015 ) show that our latent extractive model improves upon a strong extractive baseline trained on rulebased labels and also performs competitively to several recent models .
Model
We first introduce the neural extractive summarization model upon which our latent model is based on .
We then describe a sentence compression model which is used in our latent model and finally move on to present the latent model itself .
Neural Extractive Summarization
In extractive summarization , a subset of sentences in a document is selected as its summary .
We model this problem as an instance of sequence labeling .
Specifically , a document is viewed as a sequence of sentences and the model is expected to predict a True or False label for each sentence , where True indicates that the sentence should be included in the summary .
It is assumed that during training sentences and their labels in each docu-ment are given ( methods for obtaining these labels are discussed in Section 3 ) .
As shown in the lower part of Figure 1 , our extractive model has three parts : a sentence encoder to convert each sentence into a vector , a document encoder to learn sentence representations given surrounding sentences as context , and a document decoder to predict sentence labels based on representations learned by the document encoder .
Let D = ( S 1 , S 2 , . . . , S | D| ) denote a document and S i = ( w i 1 , w i 2 , . . . , w i | S i | ) a sen- tence in D ( where w i j is a word in S i ) .
Let Y = ( y 1 , . . . , y | D| ) denote sentence labels .
The sentence encoder first transforms S i into a list of hidden states ( h i 1 , h i 2 , . . . , h i | S i | ) using a Bidirectional Long Short -Term Memory Network ( Bi-LSTM ; Hochreiter and Schmidhuber 1997 ; Schuster and Paliwal 1997 ) .
Then , the sentence encoder yields v i , the representation of S i , by averaging these hidden states ( also see Figure 1 ) : v i = 1 | S i | j h i j ( 1 )
In analogy to the sentence encoder , the document encoder is another Bi-LSTM but applies on the sentence level .
After running the Bi-LSTM on a sequence of sentence representations ( v 1 , v 2 , . . . , v | D | ) , we obtain context dependent sentence representations ( h E 1 , h E 2 , . . . , h E | D | ) .
The document decoder is also an LSTM which predicts sentence labels .
At each time step , it takes the context dependent sentence representation of S i produced by the document encoder as well as the prediction in the previous time step : h D i = LSTM ( h D i?1 , W e e(y i?1 ) h E i ) ( 2 ) where W e ?
R d?2 is the label embedding matrix ( d is the hidden dimension for the document decoder LSTM ) and y i?1 is the prediction at time step i ?
1 ; the predicted label distribution for y i is : p(y i |y 1:i?1 , h D i?1 ) = softmax ( W o h D i ) ( 3 ) where W o ? R 2?d .
The model described above is usually trained by minimizing the negative log-likelihood of sentence labels in training documents ; it is almost identical to Cheng and Lapata ( 2016 ) except that we use a word- level long short - term memory network coupled with mean pooling to learn sentence representations , while they use convolutional neural network coupled with max pooling ( Kim et al. , 2016 ) .
Sentence Compression
We train a sentence compression model to map a sentence selected by the extractive model to a sentence in the summary .
The model can be used to evaluate the quality of a selected sentence with respect to the summary ( i.e. , the degree to which it is similar ) or rewrite an extracted sentence according to the style of the summary .
For our compression model we adopt a standard attention - based sequence - to-sequence architecture ( Bahdanau et al. , 2015 ; Rush et al. , 2015 ) .
The training set for this model is generated from the same summarization dataset used to train the exractive model .
Let D = ( S 1 , S 2 , . . . , S | D| ) denote a document and H = ( H 1 , H 2 , . . . , H | H| ) its summary .
We view each sentence H i in the summary as a target sentence and assume that its corresponding source is a sentence in D most similar to it .
We measure the similarity between source sentences and candidate targets using ROUGE , i.e. , S j = argmax S j ROUGE ( S j , H i ) and S j , H i is a training instance for the compression model .
The probability of a sentence ? i being the compression of ?j ( i.e. , p s2s ( ?i | ?j ) ) can be estimated with a trained compression model .
Latent Extractive Summarization
Training the extractive model described in Section 2.1 requires sentence - level labels which are obtained heuristically ( Cheng and Lapata , 2016 ; Nallapati et al. , 2017 ) .
Our latent variable model views sentences in a document as binary variables ( i.e. , zeros and ones ) and uses sentences with activated latent variables ( i.e. , ones ) to infer gold summaries .
The latent variables are predicted with an extractive model and the loss during training comes from gold summaries directly .
Let D = ( S 1 , S 2 , . . . , S | D| ) denote a document and H = ( H 1 , H 2 , . . . , H | H| ) its human summary ( H k is a sentence in H ) .
We assume that there is a latent variable z i ? { 0 , 1 } for each sentence S i indicating whether S i should be selected , and z i = 1 entails it should .
We use the extractive model from Section 2.1 to produce probability distributions for latent variables ( see Equation ( 3 ) ) and obtain them by sampling z i ? p( z i |z 1:i?1 , h D i?1 ) ( see Figure 1 ) .
C = { S i |z i = 1 } , the set of sentences whose latent variables equal to one , are our current extractive summaries .
Without loss of generality , we denote C = ( C 1 , . . . , C | C | ) .
Then , we estimate how likely it is to infer the human summary H from C .
We estimate the likelihood of summary sentence H l given document sentence C k with the compression model introduced in Section 2.2 and calculate the normalized 1 probability s kl : s kl = exp 1 | H l | log p s2s ( H l | C k ) ( 4 )
The score R p measures the extent to which H can be inferred from C : R p ( C , H ) = 1 | C | | C| k=1 | H | max l=1 s kl ( 5 ) For simplicity , we assume one document sentence can only find one summary sentence to explain it .
Therefore , for all H l , we only retain the most evident s kl .
R p ( C , H ) can be viewed as the " precision " of document sentences with regard to summary sentences .
Analogously , we also define R r , which indicates the extent to which H can be covered by C : R r ( C , H ) = 1 | H| | H| l=1 | C | max k=1 s kl ( 6 ) R r ( C , H ) can be viewed as the " recall " of document sentences with regard to summary sentences .
The final score R ( C , H ) is the weighted sum of the two : R ( C , H ) = ?
R p ( C , H ) + ( 1 ? ? ) R r ( C , H ) ( 7 )
Our use of the terms " precision " and " recall " is reminiscent of relevance and coverage in other summarization work ( Carbonell and Goldstein , 1998 ; Lin and Bilmes , 2010 ; See et al. , 2017 ) .
We train the model by minimizing the negative expected R ( C , H ) : L ( ? ) = ?E ( z 1 , ... , z | D| ) ?p( ? |D ) [ R ( C , H ) ] ( 8 ) where p(? |D ) is the distribution produced by the neural extractive model ( see Equation ( 3 ) ) .
Unfortunately , computing the expectation term is prohibitive , since the possible latent variable combinations are exponential .
In practice , we approximate this expectation with a single sample from the distribution of p ( ? | D ) .
We use the REIN - FORCE algorithm ( Williams , 1992 ) to approximate the gradient of L ( ? ) : ?L ( ? ) ? | D| i=1 ? log p( z i |z 1:i?1 , h D i?1 ) [ R ( C , H ) ? b i ]
Note that the model described above can be viewed as a reinforcement learning model , where R ( C , H ) is the reward .
To reduce the variance of gradients , we also introduce a baseline linear regression 2 model b i ( Ranzato et al. , 2016 ) to estimate the expected value of R ( C , H ) .
To avoid random label sequences during sampling , we use a pre-trained extractive model to initialize our latent model .
Experiments Dataset and Evaluation
We conducted experiments on the CNN / Dailymail dataset ( Hermann et al. , 2015 ; See et al. , 2017 ) .
We followed the same pre-processing steps as in See et al . ( 2017 ) .
The resulting dataset contains 287,226 document - summary pairs for training , 13,368 for validation and 11,490 for test .
To create sentence level labels , we used a strategy similar to Nallapati et al . ( 2017 ) .
We label the subset of sentences in a document that maximizes ROUGE ( against the human summary ) as True and all other sentences as False .
Using the method described in Section 2.2 , we created a compression dataset with 1,045,492 sentence pairs for training , 53,434 for validation and 43,382 for testing .
We evaluated our models using full length F1 ROUGE ( Lin , 2004 ) and the official ROUGE -1.5.5.pl script .
We report ROUGE -1 , ROUGE - 2 , and ROUGE -L.
Implementation
We trained our extractive model on an Nvidia K80 GPU card with a batch size of 32 .
Model parameters were uniformly initialized to [ ?
1 ? c , 1 ? c ] ( c is the number of columns in a weight matrix ) .
We used Adam ( Kingma and Ba , 2014 ) to optimize our models with a learning rate of 0.001 , ? 1 = 0.9 , and ? 2 = 0.999 .
We trained our extractive model for 10 epochs and selected the model with the highest ROUGE on the validation set .
We rescaled the gradient when its norm exceeded 5 ( Pascanu et al. , 2013 )
Results of different models on the CNN / Dailymail test set using full- length F1 ROUGE - 1 ( R - 1 ) , ROUGE - 2 ( R - 2 ) , and ROUGE -L ( R- L ) .
regularized all LSTMs with a dropout rate of 0.3 ( Srivastava et al. , 2014 ; Zaremba et al. , 2014 ) .
We also applied word dropout ( Iyyer et al. , 2015 ) at rate 0.2 .
We set the hidden unit size d = 300 for both word-level and sentence - level LSTMs and all LSTMs had one layer .
We used 300 dimensional pre-trained FastText vectors ( Joulin et al. , 2017 ) to initialize our word embeddings .
The latent model was initialized from the extractive model ( thus both models have the same size ) and we set the weight in Equation ( 7 ) to ? = 0.5 .
The latent model was trained with SGD , with learning rate 0.01 for 5 epochs .
During inference , for both extractive and latent models , we rank sentences with p(y i = True|y 1:i?1 , D ) and select the top three as summary ( see also Equation ( 3 ) ) .
Comparison Systems
We compared our model against LEAD3 , which selects the first three leading sentences in a document as the summary and a variety of abstractive and extractive models .
Abstractive models include a sequence-tosequence architecture ( Nallapati et al. , 2016 ) ; abstract ) , its pointer generator variant ( See et al. 2017 ; pointer+coverage ) , and two reinforcement learning - based models ( Paulus et al. 2017 ; abstract - RL and abstract - ML +RL ) .
We also compared our approach against an extractive model based on hierarchical recurrent neural networks ( Nallapati et al. 2017 ; SummaRuNNer ) , the model described in Section 2.1 ( EXTRACT ) which encodes sentences using LSTMs , a variant which employs CNNs instead ( Cheng and Lapata 2016 ; EXTRACT - CNN ) , as well as a similar system based on reinforcement learning ( Narayan et al. 2018 ; REFRESH ) .
Results
As shown in Table 1 , EXTRACT , our extractive model outperforms LEAD3 by a wide margin .
EXTRACT also outperforms previously published extractive models ( i.e. , SummaRuNNer , EXTRACT - CNN , and REFRESH ) .
However , note that SummaRuNNer generates anonymized summaries ( Nallapati et al. , 2017 ) while our models generate non-anonymized ones , and therefore the results of EXTRACT and SummaRuNNer are not strictly comparable ( also note that LEAD3 results are different in Table 1 ) .
Nevertheless , EXTRACT exceeds LEAD3 by + 0.75 ROUGE - 2 points and + 0.57 in terms of ROUGE -L , while Sum-maRuNNer exceeds LEAD3 by + 0.50 ROUGE - 2 points and is worse by ?0.20 points in terms of ROUGE -L .
We thus conclude that EXTRACT is better when evaluated with ROUGE - 2 and ROUGE -L. EXTRACT outperforms all abstractive models except for abstract - RL. ROUGE - 2 is lower for abstract - RL which is more competitive when evaluated against ROUGE - 1 and ROUGE -L .
Our latent variable model ( LATENT ; Section 2.3 ) outperforms EXTRACT , despite being a strong baseline , which indicates that training with a loss directly based on gold summaries is useful .
Differences among LEAD3 , EXTRACT , and LATENT are all significant with a 0.95 confidence interval ( estimated with the ROUGE script ) .
Interestingly , when applying the compression model from Section 2.2 to the output of our latent model ( LATENT + COMPRESS ) , performance drops considerably .
This may be because the compression model is a sentence level model and it removes phrases that are important for creating the document - level summaries .
Conclusions
We proposed a latent variable extractive summarization model which leverages human summaries directly with the help of a sentence compression model .
Experimental results show that the proposed model can indeed improve over a strong extractive model while application of the compression model to the output of our extractive system leads to inferior output .
In the future , we plan to explore ways to train compression models tailored to our summarization task .
Table 1 : 1 and Model R-1 R-2 R-L LEAD3 40.34 17.70 36.57 LEAD3 ( Nallapati et al. , 2017 ) 39.20 15.70 35.50 abstract 35.46 13.30 32.65 pointer+coverage 39.53 17.28 36.38 abstract - RL 41.16 15.75 39.08 abstract-ML+RL 39.87 15.82 36.90 SummaRuNNer 39.60 16.20 35.30 EXTRACT -CNN 40.11 17.52 36.39 REFRESH ( Narayan et al. , 2018 ) 40.00 18.20 36.60 EXTRACT 40.62 18.45 37.14 LATENT 41.05 18.77 37.54 LATENT + COMPRESS 36.69 15.43 34.33
We also experimented with unnormalized probabilities ( i.e. , excluding the exp in Equation ( 4 ) ) , however we obtained inferior results .
The linear regression model bt is trained by minimizing the mean squared error between the prediction of bt and R ( C , H ) .
