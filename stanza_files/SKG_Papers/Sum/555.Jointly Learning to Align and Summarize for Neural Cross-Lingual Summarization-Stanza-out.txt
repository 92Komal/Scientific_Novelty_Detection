title
Jointly Learning to Align and Summarize for Neural Cross-Lingual Summarization
abstract
Cross-lingual summarization is the task of generating a summary in one language given a text in a different language .
Previous works on cross-lingual summarization mainly focus on using pipeline methods or training an endto-end model using the translated parallel data .
However , it is a big challenge for the model to directly learn cross-lingual summarization as it requires learning to understand different languages and learning how to summarize at the same time .
In this paper , we propose to ease the cross-lingual summarization training by jointly learning to align and summarize .
We design relevant loss functions to train this framework and propose several methods to enhance the isomorphism and cross-lingual transfer between languages .
Experimental results show that our model can outperform competitive models in most cases .
In addition , we show that our model even has the ability to generate cross-lingual summaries without access to any cross-lingual corpus .
Introduction Neural abstractive summarization has witnessed rapid growth in recent years .
Variants of sequenceto-sequence models have shown to obtain promising results on English ( See et al. , 2017 ) or Chinese summarization datasets .
However , Cross-lingual summarization , which aims at generating a summary in one language from input text in a different language , has been rarely studied because of the lack of parallel corpora .
Early researches on cross-lingual abstractive summarization are mainly based on the summarization - translation or translationsummarization pipeline paradigm and adopt different strategies to incorporate bilingual features ( Leuski et al. , 2003 ; Orasan and Chiorean , 2008 ; Wan et al. , 2010 ; Wan , 2011 ) into the pipeline model .
Recently , Shen et al. ( 2018 ) first propose a neural cross-lingual summarization system based on a large-scale corpus .
They first translate the texts automatically from the source language into the target language and then use the teacher -student framework to train a cross-lingual summarization model .
Duan et al. ( 2019 ) further improve this teacher -student framework by using genuine summaries paired with the translated pseudo source sentences to train the cross-lingual summarization model .
Zhu et al . ( 2019 ) propose a multi-task learning framework to train a neural cross-lingual summarization model .
Cross-lingual summarization is a challenging task as it requires learning to understand different languages and learning how to summarize at the same time .
It would be difficult for the model to directly learn cross-lingual summarization .
In this paper , we explore this question : can we ease the training and enhance the cross-lingual summarization by establishing alignment of context representations between two languages ?
Learning cross-lingual representations has been proven a beneficial method for cross-lingual transfer for some downstream tasks ( Klementiev et al. , 2012 ; Artetxe et al. , 2018 ; Ahmad et al. , 2019 ; .
The underlying idea is to learn a shared embedding space for two languages to improve the model 's ability for cross-lingual transfer .
Recently , it has been shown that this method can also be applied to context representations ( Aldarmaki and Diab , 2019 ; Schuster et al. , 2019 ) .
In this paper , we show that the learning of cross-lingual representations is also beneficial for neural crosslingual summarization models .
We propose a multi-task framework that jointly learns to summarize and align context- level representations .
Concretely , we first integrate monolingual summarization models and cross-lingual summarization models into one unified model and then build two linear mappings to project the context representation from one language to the other .
We then design several relevant loss functions to learn the mappers and facilitate the cross-lingual summarization .
In addition , we propose some methods to enhance the isomorphism and cross-lingual transfer between different languages .
We also show that the learning of aligned representation enables our model to generate cross-lingual summaries even in a fully unsupervised way where no parallel crosslingual data is required .
We conduct experiments on several public crosslingual summarization datasets .
Experiment results show that our proposed model outperforms competitive models in most cases , and our model also works on the unsupervised setting .
To the best of our knowledge , we are the first to propose an unsupervised framework for learning neural crosslingual summarization .
In summary , our primary contributions are as follow : ?
We propose a framework that jointly learns to align and summarize for neural cross-lingual summarization and design relevant loss functions to train our system .
?
We propose a procedure to train our crosslingual summarization model in an unsupervised way .
?
The experimental results show that our model outperforms competitive models in most cases , and our model has the ability to generate cross-lingual summarization even without any cross-lingual corpus .
Overview
We show the overall framework of our proposed model in Figure 1 .
Our model consists of two encoders , two decoders , two linear mappers , and two discriminators .
Suppose we have an English source text x = {x 1 , . . . , x m } and a Chinese source text y = {y 1 , . . . , y n } , which consist of m and n words , respectively .
The English encoder ?
E X ( res.
Chinese encoder ?
E Y ) transforms x ( res. y ) into its context representation z x ( res. z y ) , and the decoder ?
D X ( res. ? D Y ) reads the memory z x ( res. z y ) and generates the corresponding English summary x ( res. Chinese summary ? ) .
The mappers M X : Z x ? Z y and M Y : Z y ?
Z x are used for transformations between z x and Taking English - to - Chinese summarization for example , our model generates cross-lingual summaries as follows :
First we use the English encoder to get the English context representations , then we use the mapper to map English representations into Chinese space .
Lastly the Chinese decoder is used to generate Chinese summaries .
In Section 3 , we describe the techniques we adopt to enhance the cross-lingual transferability of the model .
In Section 4 and Section 5 , we describe the unsupervised training objective and supervised training objective for cross-lingual summarization , respectively .
3 Model Adjustment for Cross-Lingual Transfer
Normalizing the Representations
In our model , we adopt Transformer ( Vaswani et al. , 2017 ) as our encoder and decoder , which is the same with previous works ( Duan et al. , 2019 ; Zhu et al. , 2019 ) .
The encoder and decoder are connected via cross-attention .
The cross-attention is implemented as the following dot-product attention module : Attention ( S , T ) = softmax T S ? d k S ( 1 ) where S is the packed encoder-side contextual representation , T is the packed decoder-side contextual representation and d k is the model size .
In the dot-product module , it would be beneficial if the contextual representations of the encoder and decoder have the same distributions .
However , in the cross-lingual setting , the encoder and decoder deal with different languages and thus the distributions of the learned contextual representations may be inconsistent .
This motivates us to explicitly learn alignment relationships between languages .
To make the contextual representations of two languages easier to be aligned , we introduce the normalization technique into the transformer model .
Normalizing the word representations has been proved an effective technique on word alignment ( Xing et al. , 2015 ) .
After normalization , two sets of embeddings are both located on a unit hypersphere , which makes them easier to be aligned .
We achieve this by introducing the prenormalization technique and replacing the LayerNorm with ScaleNorm ( Nguyen and Salazar , 2019 ) : o +1 = LayerNorm ( o + F ( o ) ) ?
o +1 = o + F ( ScaleNorm ( o ) ) where F is the - th layer and o is its input .
The formula for calculating ScaleNorm is : ScaleNorm ( x ; g ) = g ? x / x ( 2 ) where g is a hyper-parameter .
An additional benefit of ScaleNorm is that after being normalized , the dot-product of two vectors u v is equivalent to their cosine distance u v u v , which may benefit the attention module in Transformer .
We will conduct experiments to verify this .
Enhancing the Isomorphism A key assumption of aligning the representations of two languages is the isomorphism of learned monolingual representations .
Some researchers show that the isomorphism assumption weakens when two languages are etymologically distant ( S?gaard et al. , 2018 ; Patra et al. , 2019 ) .
However , Ormazabal et al. ( 2019 ) show that this limitation is due to the independent training of two separate monolingual embeddings , and they suggest to jointly learn cross-lingual representations on monolingual corpora .
Inspired by Ormazabal et al. ( 2019 ) , we take the following approaches to address the isomorphism problem .
First , we combine the English and Chinese summarization corpora and build a unified vocabulary .
Second , we share encoders and decoders in our model .
Sharing encoders and decoders can also enforce the model to learn shared contextual representations across languages .
For the shared decoder , to indicate the target language , we set the first token of the decoder to specify the language the module is operating with .
Third , we train several monolingual summarization steps before cross-lingual training , as shown in the first line in Alg .
1 . The pre-trained monolingual summarization steps also allow the model to learn easier monolingual summarization first , then further learn cross-lingual summarization , which may reduce the training difficulty .
Unsupervised Training Objective
We describe the objective of unsupervised crosslingual summarization in this section .
The whole training procedure can be found in Alg.
1 . Summarization Loss Given an English textsummary pair x and x , we use the encoder ?
E X and the decoder ?
D X to generate the hypothetical English summary x that maximizes the output summary probability given the source text : x = arg max x P ( x | x ) .
We adopt maximum loglikelihood training with cross-entropy loss between hypothetical summary x and gold summary x : z x = ? E X ( x ) , x = ? D X ( z x ) L summ X ( x , x ) = ?
T t=1 log P x t | x<t , z x ( 3 ) where T is the length of x .
The Chinese summarization loss L summ Y is similarly defined for the Chinese encoder ?
E Y and decoder ?
D Y .
Generative and Discriminative Loss Given an English source text x and a Chinese source text y , we use the encoder ?
E X and ?
E Y to obtain the contextual representations z x = { z x 1 , . . . , z xm } and z y = { z y 1 , . . . , z yn } , respectively .
For Zhto - En summarization , we use the mapper M Y to map z y into the English context space : z y?x = M Y ( z y ) .
We hope the mapped distribution z y?x and the real English distribution z x could be as similar as possible such that the English decoder can deal with cross-lingual summarization just like dealing with monolingual summarization .
To learn this mapping , we introduce two discriminators and adopt the adversarial training ( Goodfellow et al. , 2014 ) technique .
We optimize the mappers at the sentence - level 1 rather than wordlevel , which is inspired by Aldarmaki and Diab ( 2019 ) where they found learning the aggregate mapping can yield a more optimal solution compared to word-level mapping .
Concretely , we first average the contextual representations : zy?x = 1 n n i=1 ( z y?x ) i , zx = 1 m m i=1 z x i ( 4 )
Then we train the discriminator D X to discriminate between zy ?x and zx using the following discriminative loss : L dis X ( z y?x , zx ) = ? log P D X ( src = 0 | z y?x ) ? log P D X ( src = 1 | z x ) ( 5 ) where P D X ( src |z ) is the predicted probability of D X to distinguish whether z is coming from the real English representation ( src = 1 ) or from the mapper M Y ( src = 0 ) .
In our framework , the encoder ?
E X and mapper M Y together make up the generator .
The generator tries to generate representations which would confuse the discriminator , so its objective is to maximize the discriminative loss in Eq.
5 .
Alternatively , we train the generator to minimize the following generative loss : L gen Y ( z y?x , zx ) = ? log P D X ( src = 1 | z y?x ) ? log P D X ( src = 0 | z x ) ( 6 )
The discriminative loss L dis Y ( z x?y , zy ) for D Y , generative loss L gen X ( z x?y , zy ) for ?
E Y and M X are similarly defined .
Notice that since we use vector averaging and adopt the linear transformation , it does not matter whether we apply the linear mapping before or after averaging the contextual representations , and the learned sentence - level mappers can be directly applied to word -level mappings .
Cycle Reconstruction Loss
Theoretically , if we do not add additional constraints , there exist infinite mappings that can align the distribution of zx and zy , and thus the learned mappers may be invalid .
In order to learn better mappings , we introduce the cycle reconstruction loss and back -translation loss to enhance them .
Given z x , we first use M X to map it to the Chinese space , and then use M Y to map it back : z x?y = M X ( z x ) , ?x = M Y ( z x?y ) ( 7 ) We force z x and ?x to be consistent , constrained by the following cycle reconstruction loss : L cyc X ( z x , ?x ) = z x ? ?x ( 8 )
The cycle reconstruction loss L cyc Y for z y and ?y is similarly defined .
Back -Translation Loss
The cycle-reconstructed representation ?x in Eq. 8 can be regarded as augmented data to train the decoder , which is similar to the back -translation in the Neural Machine Translation area .
Concretely , we use the decoder ?
D X to read ?x and generate the hypothetical summary x .
The back - translation loss is defined as the cross-entropy loss between x and gold summary x : x = ? D X ( ? x ) L back X ( ? x ) = ?
T t=1 log P x t | x<t , ?x ( 9 )
The back - translation loss enhances not only the generation ability of the decoder but also the effectiveness of the mapper .
The back - translation loss L back Y for ?y is similarly defined .
Total Loss
The total loss for optimizing the encoder , decoder , and mapper of the English side is weighted sum of the above losses : L X = L summ X + ?
1 L gen X + ? 2 L cyc X + ? 3 L back X ( 10 ) where ?
1 , ? 2 , and ?
3 is the weighted hyperparameters .
The total loss of the Chinese side is similarly defined , and the complete loss of our model is the sum of English loss and Chinese loss : L = L X + L Y ( 11 )
The total loss for optimizing the discriminators is : L dis = L dis X + L dis Y ( 12 ) 5 Supervised Training Objective
The supervised training objective contains the same summarization loss in unsupervised training objective ( Eq. 3 ) .
In addition , it has X-summarization loss and reconstruction loss .
Algorithm 1 Cross-lingual summarization Input : English summarization data X and Chinese summarization data Y. 1 : Pre-train English and Chinese monolingual summarization several epochs on X and Y. 2 : for i = 0 to max iters do on L gen in Eq. 6 . 11 : on L rec in Eq. 14 . ( c ) Update ? E X , ? E Y , M X , X- Summarization Loss Given a parallel English source text x and Chinese summary y .
We use ?
E X , M X , and ?
D Y to generate the hypothetical Chinese summary ? , then train them with crossentropy loss : z x = ? E X ( x ) , z x?y = M X ( z x ) , ? = ? D Y ( z x?y ) L xsumm X ( x , y ) = ?
T t=1 log P y t | ?<t , x ( 13 )
The X-summarization loss for a Chinese text y and English summary x is similarly defined .
Reconstruction Loss
Since the cross-lingual summarization corpora are constructed by translating the texts to the other language , the English texts and the Chinese texts are parallel to each other .
We can build a reconstruction loss to align the sentence representation for the parallel English and Chinese texts .
Specifically , supposing x and y are parallel source English and Chinese texts , we first use ?
E X and ?
E Y to obtain contextual representations z x and z y , respectively .
Then we average the contextual representations to get their sentence representations and use the mappers to map them into the other language .
Since the English and Chinese texts are translations to each other , the semantics of their sentence representations should be the same .
Thus we design the following reconstruction loss : zx = 1 m m i=1 z x i , zy ?x = 1 n n i=1 ( z y?x ) i L rec X ( z x , zy ?x ) = zx ? zy?x ( 14 ) and L rec Y is similarly defined .
Notice that the generative and discriminative loss , cycle-construction loss , and back - translation loss are unnecessary here because we can directly use aligned source text with objective 14 to align the context representations .
Total Loss
The total loss for training the English side is : L X = L xsumm X + ?
1 L summ X + ? 2 L rec X ( 15 ) where ?
1 and ?
2 is the weighted hyper-parameters .
The total loss of the Chinese side is similarly defined .
Experiments
Experiment Settings
We conduct experiments on English- to - Chinese ( En-to - Zh ) and Chinese-to- English ( Zh-to - En ) summarizations .
Following Duan et al. ( 2019 ) , we translate the source texts to the other language to form the ( pseudo ) parallel corpus .
Since they do not release their training data , we translate the source text ourselves through the Google translation service .
Notice that Zhu et al . ( 2019 ) 2019 ) are unprocessed , therefore we have to process the test samples they provided ourselves .
Dataset Gigaword English Gigaword corpus ( Napoles et al. , 2012 ) contains 3.80 M training pairs , 2 K validation pairs , and 1,951 test pairs .
We use the human-translated Chinese source sentences provided by ( Duan et al. , 2019 ) to do Zh-to - En tests .
DUC2004 DUC2004 corpus only contains test sets .
We use the model trained on gigaword corpus to generate summaries on DUC2004 test sets .
We use the 500 human-translated test samples provided by ( Duan et al. , 2019 ) to do Zh-to - En tests .
LCSTS LCSTS ( Hu et al. , 2015 ) is a Chinese summarization corpus , which contains 2.40 M training pairs , 10,666 validation pairs , and 725 test pairs .
We use 3 K cross-lingual test samples provided by Zhu et al . ( 2019 ) to do Zh-to - En tests .
CNN / DM CNN / DM ( Hermann et al. , 2015 ) contains 287.2 K training pairs , 13.3 K validation pairs , and 11.5 K test pairs .
We use the 3K cross-lingual test samples provided by Zhu et al . ( 2019 ) to do En-to - Zh cross-lingual tests .
Evaluation Metrics
We use ROUGE - 1 ( unigram ) , ROUGE - 2 ( bigram ) , and ROUGE -L ( LCS ) F1 scores as the evaluation metrics , which are most commonly used evaluation metrics in the summarization task .
Competitive Models
For unsupervised cross-lingual summarization , we set the following baselines : ? Unified
It jointly trains English and Chinese monolingual summarizations in a unified model and uses the first token of the decoder to control whether it generates Chinese or English summaries .
?
Unified+CLWE
It builds a unified model and adopts pre-trained unsupervised cross-lingual word embeddings .
The cross-lingual word embeddings are obtained via projecting embeddings from source language to target language .
We use Vecmap 2 to learn the cross-lingual word embeddings .
For supervised cross-lingual summarization , we compare our model with ( Shen et al. , 2018 ) , ( Duan et al. , 2019 ) , and Zhu et al . ( 2019 ) .
We also consider the following baselines for comparison : ? Pipe-TS
The Pipe -TS baseline first uses a Transformer - based translation model to translate the source text to the other language , then uses a monolingual summarization model to generate summaries .
To make this baseline stronger , we replace the translation model with the Google translation system and name it as Pipe -TS *. ? Pipe-ST
The Pipe -ST baseline first uses a monolingual summarization model to generate the summaries , then uses a translation model to translate the summaries to the other language .
We replace the translation model with the Google translation system as Pipe-ST *. ?
Pseudo
The Pseudo baseline directly trains a cross-lingual summarization model by using the pseudo parallel cross-lingual summarization data .
? XLM Pretraining
This method is proposed by Lample and Conneau ( 2019 ) , where they pretrain the encoder and decoder on largescale multilingual text using causal language modeling ( CLM ) , masked language modeling ( MLM ) , and translation language modeling ( TLM ) tasks .
3
Implementation Details
For transformer architectures , we use the same configuration as Vaswani et al . ( 2017 ) , where the number of layers , model hidden size , feed - forward hidden size , and the number of heads are 6 , 512 , 1024 , and 8 , respectively .
We set g = ?
d model = ? 512 in ScaleNorm .
The mapper is a linear layer with a hidden size of 512 , and the discriminator is a two -layer linear layer with a hidden size of 2048 .
We use the NLTK 4 tool to process English texts and use jieba 5 tool to process Chinese texts .
The vocabulary size of English words and Chinese words are 50,000 and 80,000 respectively .
We set ?
1 = 1 , ? 2 = 5 , ? 3 = 2 in unsupervised training and ?
1 = 0.5 , ? 2 = 5 in supervised training according to the performance of the validation set .
We set dis iters = 5 in Alg .
1 . We use Adam optimizer ( Kingma and Ba , 2014 ) with ? = ( 0.9 , 0.98 ) for optimization .
We set the learning rate to 3e ? 4 and adopt the warm - up learning rate ( Goyal et al. , 2017 ) for the first 2,000 steps , the initial warm - up learning is set to 1e ? 7 .
We adopt the dropout technique and set the dropout rate to 0.2 .
Results and Analysis
Unsupervised Cross-Lingual Summarization
The experiment results of unsupervised crosslingual summarization are shown in Table 2 , and it can be seen that our model significantly outperforms all baselines by a large margin .
By training a unified model of all languages , the model 's crosslingual transferability is still poor , especially for the gigaword dataset .
Incorporating cross-lingual word embeddings into the unified model can improve the performance , but the improvement is limited .
We think this is due to that the cross-lingual word embeddings learned by Vecmap cannot leverage the contextual information .
Due to space limitations , we present case studies in the Appendix .
After checking the generated summaries of the two baseline models , we find that they can generate readable texts , but the generated texts are far away from the theme of the source text .
This indicates that the encoder and decoder of these baselines have a large gap , such that the decoder cannot understand the output of the encoder .
We also find that summaries generated by our model are obviously more relevant , demonstrating that aligned representations between languages are helpful .
But we can also see that there is still a gap be - tween our unsupervised results ( Table 2 ) and supervised results ( Table 1 ) , indicating that our model has room for improvement .
Supervised Cross-Lingual Summarization
The experiment results of supervised cross-lingual summarization are shown in Table 1 . Due to the lack of corpus for training Chinese long document summarization model , we do not experiment with the Pipe -TS model on the CNN / DM dataset .
By comparing our results with pipeline - based or pseudo baselines , we can find that our model outperforms all these baselines in all cases .
Our model achieves an improvement of 0?3
Rouge scores over the Pseudo model trained directly with translated parallel cross-lingual corpus , and 1.5? 4 Rouge - 1 scores over those pipeline models .
We also observe that models using the Google translation system all perform better than models using the Transformer - based translation system .
This may because the Transformer - based translation system will bring some " UNK " tokens , and the transformer - based translation system trained by ourselves does not perform as well as the Google translation system .
In addition , Pipe -ST models perform better than Pipe -TS models , which is con- sistent with the conclusions of previous work .
This is because ( 1 ) the translation process may discard some informative clauses , ( 2 ) the domain of the translation corpus is different from the domain of summarization corpus , which will bring the domain discrepancy problem to the translation process , and ( 3 ) the translated texts are often " translationese " .
The Pseudo model performs better than Pipe -TS models but performs similarly as Pipe -ST models .
By comparing our results with others , we can find that our model outperforms Shen et al . ( 2018 ) and Duan et al . ( 2019 ) on both gigaword and DUC2004 test sets , and it outperforms Zhu et al . ( 2019 ) on the LCSTS dataset .
But our Rouge scores are lower than Zhu et al . ( 2019 ) on the CNN / DM dataset , especially the Rouge - 2 score .
However , our model performs worse than pretrained models .
Human Evaluation
The human evaluation was also performed .
Since we cannot get the summaries generated by other models , we only compare with our baselines in the human evaluation .
We randomly sample 50 examples from the gigaword ( Zh- to - En ) test set and 20 examples from the CNN / DM ( En-to - Zh ) test set .
We ask five volunteers to evaluate the quality of the generated summaries from the following three aspects : ( 1 ) Informative : how much does the generated summaries cover the key content of the source text ?
( 2 ) Conciseness : how concise are the generated summaries ?
( 3 ) Fluency : how fluent are the generated summaries ?
The scores are between 1 - 5 , with 5 being the best .
We average the scores and show the results in Table 3 and Table 4 .
Our model exceeds all baselines in informative and conciseness scores , but get a slightly lower fluency score than Pipe -ST * .
We think this is because the Google translation system has the ability to identify grammatical errors and generate fluent sentences .
Ablation Tests
To study the importance of different components of our model , we also test some variants of our model .
For supervised training , we set variants of ( 1 ) without ( monolingual ) summarization loss , ( 2 ) without mappers 6 , ( 3 ) replace ScaleNorm with LayerNorm , ( 4 ) without pre-trained monolingual steps , and ( 5 ) unshare the encoder and decoder .
For unsupervised training , we additionally set variants without cyc-reconstruction loss or back -translation loss .
The results of ablation tests of supervised and unsupervised cross-lingual summarization are shown in Table 5 and Table 6 , respectively .
It seems that the role of mappers does not seem obvious in the case of supervised training .
We speculate that this may be due to the joint training of monolingual and cross-lingual summarizations , and directly constraining the context representations before mapping can also yield shared ( aligned ) representations .
But mappers are crucial for unsupervised cross-lingual summarization .
For supervised cross-lingual summarization , except for mappers , all components contribute to the improvement of the performance .
The performance decreases after removing any of the components .
For unsupervised cross-lingual summarization , all components contribute to the improvement of the performance and the mappers and shared encoder / decoder are key components .
Early researches on cross-lingual abstractive summarization are mainly based on the monolingual summarization methods and adopt different strategies to incorporate bilingual information into the pipeline model ( Leuski et al. , 2003 ; Orasan and Chiorean , 2008 ; Wan et al. , 2010 ; Wan , 2011 ; Yao et al. , 2015 ) .
Recently , some neural cross-lingual summarization systems have been proposed for cross-lingual summarization ( Shen et al. , 2018 ; Duan et al. , 2019 ; Zhu et al. , 2019 ) .
The first neural - based crosslingual summarization system was proposed by Shen et al . ( 2018 ) , where they first translate the source texts from the source language to the target language to form the pseudo training samples .
A teacher -student framework is adopted to achieve end-to - end cross-lingual summarization .
Duan et al. ( 2019 ) adopt a similar framework to train the cross-lingual summarization model , but they translate the summaries rather than source texts to strengthen the teacher network .
Zhu et al . ( 2019 ) propose a multi-task learning framework by jointly training cross-lingual summarization and monolingual summarization ( or machine translation ) .
They also released an English - Chinese cross-lingual summarization corpus with the aid of online translation services .
Learning Cross-Lingual Representations
Learning cross-lingual representations is a beneficial method for cross-lingual transfer .
Conneau et al. ( 2017 ) use adversarial networks to learn mappings between languages without supervision .
They show that their method works very well for word translation , even for some distant language pairs like English - Chinese .
Lample et al. ( 2018 ) learn word mappings between languages to build an initial unsupervised machine translation model , and then perform iterative backtranslation to fine - tune the model .
Aldarmaki and Diab ( 2019 ) propose to directly map the averaged embeddings of aligned sentences in a parallel corpus , and achieve better performances than wordlevel mapping in some cases .
Conclusions
In this paper , we propose a framework that jointly learns to align and summarize for neural crosslingual summarization .
We design training objectives for supervised and unsupervised cross-lingual summarizations , respectively .
We also propose methods to enhance the isomorphism and crosslingual transfer between languages .
Experimental results show that our model outperforms supervised baselines in most cases and outperforms unsupervised baselines in all cases .
Figure 1 : 1 Figure 1 : The overall framework of our proposed model .
D X and D Y on L dis in Eq. 5.7 : ( a) Update ? E X , ? E Y , ? D X , and ?
D Y Update ? E X , ? E Y , M X , and M Y 10 :
and M Y Update M X , M Y , ? D X , and ?
D Y Upate ? E X , ? E Y , ? D X , and ?
D Y Update ? E X , ? E Y , ? D X , and ?
D Y Update ? E X , ? E Y , M X , and M Y 12 : on L cyc in Eq. 8. 13 : ( d ) 14 : on L back in Eq. 9. 15 : else if supervised then 16 : ( a ) 17 : on L summ in Eq. 3 . 18 : ( b) 19 : on L xsumm in Eq. 13 . 20 : ( c ) 21 :
Table 1 : 1 Rouge F1 scores ( % ) on cross-lingual summarization tests .
" XLM Pretraining " and " Zhu et al . ( 2019 ) w/ LDC " use additional training data .
Our model significantly ( p < 0.01 ) outperforms all pipeline methods and pseudo-based methods .
Zh-to-En En-to-Zh Method Gigaword DUC2004 LCSTS CNN / DM R1 R2 RL R1 R2 RL R1 R2 RL R1 R2 RL Pipe-TS 22.27 6.58 20.53 21.29 5.96 17.99 27.26 10.41 21.72 - - - Pipe-ST 28.27 11.90 26.50 25.73 8.19 21.60 36.48 18.87 31.44 25.95 11.01 23.29 Pipe-TS * 22.52 6.67 20.76 21.83 6.11 18.42 29.29 11.09 23.18 - - - Pipe-ST * 29.56 12.50 26.42 26.66 8.51 22.37 38.26 19.56 32.93 27.82 11.78 24.97 Pseudo * 30.93 13.25 27.29 27.03 8.49 23.08 38.61 19.76 34.63 35.81 14.96 32.07
( Shen et al. , 2018 ) 21.5 6.6 19.6 19.3 4.3 17.0 - - - - - - ( Duan et al. , 2019 ) 30.1 12.2 27.7 26.0 8.0 23.1 - - - - - - ( Zhu et al. , 2019 ) - - - - - - 40.34 22.65 36.39 38.25 20.20 34.76 ( Zhu et al. , 2019 ) w/ LDC - - - - - - 40.25 22.58 36.21 40.23 22.32 36.59 XLM Pretraining 32.28 14.03 28.19 28.27 9.40 23.78 42.75 22.80 38.73 39.11 17.57 34.14 Ours 32.04 13.60 27.91 27.25 8.71 23.36 40.97 23.20 36.96 38.12 16.76 33.86
Table 2 : 2 Rouge F1 scores ( % ) on unsupervised crosslingual summarization tests .
Our model outperforms all baselines significantly ( p < 0.01 ) .
Method R1 LCSTS R2 RL R1 Gigaword R2 RL Unified 13.52 1.35 10.02 5.25 0.87 2.09 Unified +CLWE 14.02 1.49 12.10 6.51 1.07 2.92 Ours 20.11 5.46 16.07 13.75 4.29 11.82
Table 3 : 3 Results of the human evaluation on the gigaword dataset .
Method Info . ? Con. ? Flu. ? Reference 3.60 3.50 3.80 PipeST * 3.56 3.51 4.00 PipeTS * 3.37 3.80 3.81 Pseudo 3.27 3.81 3.89 Ours ( supervised ) 3.56 3.93 3.94 Ours ( unsupervised ) 2.18 3.34 2.87 Method Info . ? Con. ? Flu. ? Reference 3.58 3.57 4.21 PipeST * 3.38 3.45 4.13 PipeTS * 3.38 3.93 3.78 Pseudo 3.46 3.90 4.05 Ours ( supervised ) 3.55 4.03 4.13
Table 4 : 4 Results of the human evaluation on the CNN / DM dataset .
Table 5 : 5 32.04 13.60 27.91 38.12 16.76 33.86 w/o summ . loss 30.36*12.84*26.41*36.37*15.97*32.11 * w/o mappers 31.95 13.46 27.88 38.28 16.73 33.93 w/o ScaleNorm 31.27 * 13.29 27.22*37.01*16.30*32.87 * w/o pre. steps 31.33 * 13.30 27.35*37.23 * 16.39 33.01 *
Unshare enc/ dec30.10*12.71*26.28*35.93*15.86*31.82 *
Results of ablation tests in supervised setting .
Statistically significant improvement ( p < 0.01 ) over the complete model are marked with *.
Method R1 Gigaword R2 RL R1 CNN / DM R2 RL Ours ( supervised )
Table 6 : 6 Results of the ablation tests of unsupervised cross-lingual summarization .
Statistically significant improvement ( p < 0.01 ) over the complete model are marked with *.
Method LCSTS R1 R2 RL Gigaword R1 R2 RL Ours ( unsupervised ) 20.10 5.46 16.07 13.75 4.29 11.82 w/o mappers 14.79 * 2.29 * 12.36 * 6.26 * 1.02 * 3.11 * w/o cyc. loss 17.51 * 4.70 * 13.95 * 7.21 * 1.31 * 4.04 * w/o back .
loss 19.37 5.23 15.44 13.20 4.11 11.27 w/o ScaleNorm 19.24 * 5.21 15.37 * 13.15 * 4.08 11.21 w/o pre. steps 19.70 5.24 15.72 13.13 4.10 10.91 Unshare enc/ dec 12.28 * 0.97 * 10.37 * 4.88 * 0.82 * 1.91 *
8 Related Work 8.1 Cross-Lingual Summarization
The " sentence " in this paper can refer to the sequence containing multiple sentences .
https://github.com/artetxem/vecmap
This baseline was suggested by the reviewers , and the results are only for reference since it additionally uses a lot of pre-training text . 4
https://github.com/nltk/nltk 5 https://github.com/fxsjy/jieba
In this case , we directly constrain the parallel zx and zy to be the same .
