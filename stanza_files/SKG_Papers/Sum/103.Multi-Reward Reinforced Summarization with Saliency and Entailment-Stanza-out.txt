title
Multi-Reward Reinforced Summarization with Saliency and Entailment
abstract
Abstractive text summarization is the task of compressing and rewriting a long document into a short summary while maintaining saliency , directed logical entailment , and non-redundancy .
In this work , we address these three important aspects of a good summary via a reinforcement learning approach with two novel reward functions : ROUGE - Sal and Entail , on top of a coverage - based baseline .
The ROUGESal reward modifies the ROUGE metric by up-weighting the salient phrases / words detected via a keyphrase classifier .
The Entail reward gives high ( lengthnormalized ) scores to logically - entailed summaries using an entailment classifier .
Further , we show superior performance improvement when these rewards are combined with traditional metric ( ROUGE ) based rewards , via our novel and effective multi-reward approach of optimizing multiple rewards simultaneously in alternate mini-batches .
Our method achieves the new state - of- the - art results on CNN / Daily Mail dataset as well as strong improvements in a test-only transfer setup on DUC - 2002 .
Introduction Abstractive summarization , the task of generating a natural short summary of a long document , is more challenging than the extractive paradigm , which only involves selection of important sentences or grammatical sub-sentences ( Jing , 2000 ; Knight and Marcu , 2002 ; Clarke and Lapata , 2008 ; Filippova et al. , 2015 ) .
Advent of sequence - to-sequence deep neural networks and large human summarization datasets ( Hermann et al. , 2015 ; Nallapati et al. , 2016 ) made the abstractive summarization task more feasible and accurate , with recent ideas ranging from copypointer mechanism and redundancy coverage , to metric reward based reinforcement learning ( Rush et al. , 2015 ; Chopra et al. , 2016 ; Ranzato et al. , 2015 ; Nallapati et al. , 2016 ; See et al. , 2017 ) .
A good abstractive summary requires several important properties , e.g. , it should choose the most salient information from the input document , be logically entailed by it , and avoid redundancy .
Coverage - based models address the latter redundancy issue ( Suzuki and Nagata , 2016 ; Nallapati et al. , 2016 ; See et al. , 2017 ) , but there is still a lot of scope to teach current state - of - the - art models about saliency and logical entailment .
Towards this goal , we improve the task of abstractive summarization via a reinforcement learning approach with the introduction of two novel rewards : ' ROUGESal ' and ' Entail ' , and also demonstrate that these saliency and entailment skills allow for better generalizability and transfer .
Our ROUGESal reward gives higher weight to the important , salient words in the summary , in contrast to the traditional ROUGE metric which gives equal weight to all tokens .
These weights are obtained from a novel saliency scorer , which is trained on a reading comprehension dataset 's answer spans to give a saliency - based probability score to every token in the sentence .
Our Entail reward gives higher weight to summaries whose sentences logically follow from the ground -truth summary .
Further , we also add a length normalization constraint to our Entail reward , to importantly avoid misleadingly high entailment scores to very short sentences .
Empirically , we show that our new rewards with policy gradient approaches perform significantly better than a cross-entropy based state - of - the - art pointer - coverage baseline .
We show further performance improvements by combining these rewards via our novel multi-reward optimization approach , where we optimize multiple rewards simultaneously in alternate mini-batches ( hence avoiding complex scaling and weighting issues in reward combination ) , inspired from how humans take multiple concurrent types of rewards ( feedback ) to learn a task .
Overall , our methods achieve the new state - of- the - art on the CNN / Daily Mail dataset as well as strong improvements in a testonly transfer setup on DUC - 2002 .
Lastly , we present several analyses of our model 's saliency , entailment , and abstractiveness skills .
Related Work Earlier summarization work was based on extraction and compression - based approaches ( Jing , 2000 ; Knight and Marcu , 2002 ; Clarke and Lapata , 2008 ; Filippova et al. , 2015 ) , with more focus on graph- based ( Giannakopoulos , 2009 ; Ganesan et al. , 2010 ) and discourse tree- based ( Gerani et al. , 2014 ) models .
Recent focus has shifted towards abstractive , rewriting - based summarization based on parse trees ( Cheung and Penn , 2014 ; Wang et al. , 2016 ) , Abstract Meaning Representations ( Liu et al. , 2015 ; Dohare and Karnick , 2017 ) , and neural network models with pointercopy mechanism and coverage ( Rush et al. , 2015 ; Chopra et al. , 2016 ; Nallapati et al. , 2016 ; See et al. , 2017 ) , as well as reinforcebased metric rewards ( Ranzato et al. , 2015 ; Paulus et al. , 2017 ) .
We also use reinforce - based models , but with novel reward functions and better simultaneous multi-reward optimization methods .
Recognizing Textual Entailment ( RTE ) , the task of classifying two sentences as entailment , contradiction , or neutral , has been used for Q&A and IE tasks ( Harabagiu and Hickl , 2006 ; Dagan et al. , 2006 ; Lai and Hockenmaier , 2014 ; Jimenez et al. , 2014 ) .
Recent neural network models and large datasets ( Bowman et al. , 2015 ; Williams et al. , 2017 ) enabled stronger accuracies .
Some previous work ( Mehdad et al. , 2013 ; Gupta et al. , 2014 ) has explored the use of RTE by modeling graphbased relationships between sentences to select the most non-redundant sentences for summarization .
Recently , Pasunuru and Bansal ( 2017 ) improved video captioning with entailment -corrected rewards .
We instead directly use multi-sentence entailment knowledge ( with additional length constraints ) as a separate RL reward to improve abstractive summarization , while avoiding their penalty hyperparameter tuning .
For our saliency prediction model , we make use of the SQuAD reading comprehension dataset ( Rajpurkar et al. , 2016 ) , where the answer spans annotated by humans for important questions , serve as an interesting and effective proxy for keyphrase -style salient information in summarization .
Some related previous work has incorporated document topic / subject classification ( Isonuma et al. , 2017 ) and webpage keyphrase extraction ( Zhang et al. , 2004 ) to improve saliency in summarization .
Some recent work Subramanian et al. ( 2017 ) has also used answer probabilities in a document to improve question generation .
Models
Baseline Sequence-to-Sequence Model
Our abstractive text summarization model is a simple sequence - to-sequence single - layer bidirectional encoder and unidirectional decoder LSTM - RNN , with attention ( Bahdanau et al. , 2015 ) , pointer-copy , and coverage mechanism - please refer to See et al . ( 2017 ) for details .
Policy Gradient Reinforce Traditional cross-entropy loss optimization for sequence generation has an exposure bias issue and the model is not optimized for the evaluated metrics ( Ranzato et al. , 2015 ) .
Reinforce - based policy gradient approach addresses both of these issues by using its own distribution during training and by directly optimizing the non-differentiable evaluation metrics as rewards .
We use the RE-INFORCE algorithm ( Williams , 1992 ; Zaremba and Sutskever , 2015 ) to learn a policy p ? defined by the model parameters ? to predict the next action ( word ) and update its internal ( LSTM ) states .
We minimize the loss function L RL = ?E w s ?p ? [ r ( w s ) ] , where w s is the sequence of sampled words with w s t sampled at time step t of the decoder .
The derivative of this loss function with approximation using a single sample along with variance reduction with a bias estimator is : ? ? L RL = ?( r( w s ) ? b e ) ?
? log p ? ( w s ) ( 1 )
There are several ways to calculate the baseline estimator ; we employ the effective SCST approach ( Rennie et al. , 2016 ) , as depicted in Fig. 1 , where b e = r(w a ) , is based on the reward obtained by the current model using the test time inference algorithm , i.e. , choosing the arg-max word w a t of the final vocabulary distribution at each time step t of the decoder .
We use the joint cross-entropy and reinforce loss so as to optimize the non-differentiable evaluation metric as reward while also maintaining the readability of the generated sentence ( Wu et al. , 2016 ; Paulus et al. , 2017 ; Pasunuru and Bansal , 2017 ) , which is defined as L Mixed = ?L RL + ( 1 ? ?) L XE , where ? is a tunable hyperparameter .
Multi-reward Optimization
Optimizing multiple rewards at the same time is important and desired for many language generation tasks .
One approach would be to use a weighted combination of these rewards , but this has the issue of finding the complex scaling and weight balance among these reward combinations .
To address this issue , we instead introduce a simple multi-reward optimization approach inspired from multi-task learning , where we have different tasks , and all of them share all the model parameters while having their own optimization function ( different reward functions in this case ) .
If r 1 and r 2 are two reward functions that we want to optimize simultaneously , then we train the two loss functions of Eqn. 2 in alternate mini-batches .
L RL 1 = ?( r 1 ( w s ) ? r 1 ( w a ) ) ?
? log p ? ( w s ) L RL 2 = ?( r 2 ( w s ) ? r 2 ( w a ) ) ?
? log p ? ( w s ) ( 2 ) 4 Rewards
ROUGE Reward
The first basic reward is based on the primary summarization metric of ROUGE package ( Lin , 2004 )
To learn these saliency weights , we train our saliency predictor on sentence and answer spans pairs from the popular SQuAD reading comprehension dataset ( Rajpurkar et al. , 2016 ) ) ( Wikipedia domain ) , where we treat the humanannotated answer spans ( avg. span length 3.2 ) for important questions as representative salient information in the document .
As shown in Fig. 2 , given a sentence as input , the predictor assigns a saliency probability to every token , using a simple bidirectional encoder with a softmax layer at every time step of the encoder hidden states to classify the token as salient or not .
Finally , we use the probabilities given by this saliency prediction model as weights in the ROUGE matching formulation to achieve the final ROUGESal score ( see appendix for details about our ROUGESal weighted precision , recall , and F-1 formulations ) .
Entailment Rewards
A good summary should also be logically entailed by the given source document , i.e. , contain no contradictory or unrelated information .
Pasunuru and Bansal ( 2017 ) used entailment -corrected phrase -matching metrics ( CIDEnt ) to improve the task of video captioning ; we instead directly use the entailment knowledge from an entailment scorer and its multisentence , length - normalized extension as our ' Entail ' reward , to improve the task of abstractive text summarization .
We train the entailment classifier ( Parikh et al. , 2016 ) ( Rajpurkar et al. , 2016 ) .
All dataset splits and other training details ( dimension sizes , learning rates , etc. ) for reproducibility are in appendix .
Evaluation Metrics
We use the standard ROUGE package ( Lin , 2004 ) and Meteor package ( Denkowski and Lavie , 2014 ) for reporting the results on all of our summarization models .
Following previous work ( Chopra et al. , 2016 ; Nallapati et al. , 2016 ; See et al. , 2017 ) , we use the ROUGE full- length F1 variant .
Results Baseline Cross-entropy Model
Our abstractive summarization model has attention , pointer-copy , and coverage mechanism .
First , we apply crossentropy optimization and achieve comparable re-2
Since the GT summary is correctly entailed by the source document , we directly ( by transitivity ) use this GT as premise for easier ( shorter ) encoding .
We also tried using the full input document as premise but this did n't perform as well ( most likely because the entailment classifiers are not trained on such long premises ; and the problem with the sentence - tosentence avg .
scoring approach is discussed below ) .
We also tried summary - to-summary entailment scoring ( similar to ROUGE -L ) as well as pairwise sentence - to-sentence avg .
scoring , but we found that avg .
scoring of groundtruth summary ( as premise ) w.r.t. each generated summary 's sentence ( as hypothesis ) works better ( intuitive because each sentence in generated summary might be a compression of multiple sentences of GT summary or source document ) .
ROUGE metrics .
Finally , we combined our two rewards ROUGESal + Entail to incorporate both saliency and entailment knowledge , and it gives the best results overall ( p < 0.001 in all metrics w.r.t. both baseline and ROUGE - reward models ) , setting the new state - of- the- art .
5 Test- Only Transfer ( DUC - 2002 )
Results
Finally , we also tested our model 's generalizability / transfer skills , where we take the models trained on CNN / Daily Mail and directly test them on DUC - 2002 in a test-only setup .
As shown in Table 2 , our final ROUGESal + Entail multi-reward RL model is statistically significantly better than both the cross-entropy ( pointer - generator + coverage ) baseline as well as ROUGE reward RL model , in terms of all 4 metrics with a large margin ( with p < 0.001 ) .
This demonstrates that our ROUGESal + Entail model learned better transferable and generalizable skills of saliency and logical entailment .
Output Analysis Saliency Analysis
We analyzed the output summaries generated by See et al . ( 2017 ) , and our baseline , ROUGE - reward and ROUGESal - reward models , using our saliency prediction model ( Sec. 4 ) , and the scores are 27.95 % , 28.00 % , 28.80 % , and 30.86 % .
We also used the original CNN / Daily Mail Cloze Q&A setup ( Hermann et al. , 2015 ) with the fill - in- the - blank answers treated as salient information , and the results are 60.66 % , 59.36 % , 60.67 % , and 64.66 % for the four models .
Both these experiments illustrate that our ROUGESal reward model is stat .
signif . better in saliency than the See et al . ( 2017 ) , our baseline , and ROUGE - reward models ( p < 0.001 ) .
Entailment Analysis
We also analyzed the entailment scores of the generated summaries from See et al . ( 2017 ) , and our baseline , ROUGEreward , and Entail - reward models , and the results are 27.33 % , 27.21 % , 28.23 % , and 28.98 % .
We observe that our Entail- reward model achieves stat .
significant entailment scores ( p < 0.001 ) w.r.t. all the other three models .
Abstractiveness Analysis
In order to measure the abstractiveness of our models , we followed the ' novel n-gram overlap ' approach suggested in See et al . ( 2017 ) .
First , we found that all our rewardbased RL models have significantly ( p < 0.01 ) more novel n-grams than our cross-entropy baseline ( see Table 3 ) .
Next , the Entail- reward model ' maintains ' stat .
equal abstractiveness as the ROUGE - reward model , likely because it encourages rewriting to create logical subsets of information , while the ROUGESal - reward model does a bit worse , probably because it focuses on copying more salient information ( e.g. , names ) .
Compared to previous work ( See et al. , 2017 ) , our Entailreward and ROUGE - reward models achieve statistically significant improvement ( p < 0.01 ) while ROUGESal is comparable .
Conclusion
We presented a summarization model trained with novel RL reward functions to improve the saliency and directed logical entailment aspects of a good summary .
Further , we introduced the novel and effective multi-reward approach of optimizing multiple rewards simultaneously in alternate minibatches .
We achieve the new state- of- the- art on CNN / Daily Mail and also strong test-only improvements on a DUC - 2002 transfer setup .
