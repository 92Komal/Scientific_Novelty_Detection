title
Mixed -Lingual Pre-training for Cross-lingual Summarization
abstract
Cross-lingual Summarization ( CLS ) aims at producing a summary in the target language for an article in the source language .
Traditional solutions employ a twostep approach , i.e. translate ?
summarize or summarize ? translate .
Recently , end- to - end models have achieved better results , but these approaches are mostly limited by their dependence on large-scale labeled data .
We propose a solution based on mixed - lingual pretraining that leverages both cross-lingual tasks such as translation and monolingual tasks like masked language models .
Thus , our model can leverage the massive monolingual data to enhance its modeling of language .
Moreover , the architecture has no task -specific components , which saves memory and increases optimization efficiency .
We show in experiments that this pre-training scheme can effectively boost the performance of cross-lingual summarization .
In Neural Cross-Lingual Summarization ( NCLS ) ( Zhu et al. , 2019 b ) dataset , our model achieves an improvement of 2.82 ( English to Chinese ) and 1.15 ( Chinese to English ) ROUGE - 1 scores over state - of - the - art results .
Introduction
Text summarization can facilitate the propagation of information by providing an abridged version for long articles and documents .
Meanwhile , the globalization progress has prompted a high demand of information dissemination across language barriers .
Thus , the cross-lingual summarization ( CLS ) task emerges to provide accurate gist of articles in a foreign language .
Traditionally , most CLS methods follow the twostep pipeline approach : either translate the article into the target language and then summarize it ( Leuski et al. , 2003 ) , or summarize the article in the source language and then translate it ( Wan * Equal contribution et al. , 2010 ) .
Although this method can leverage off - the-shelf summarization and MT models , it suffers from error accumulation from two independent subtasks .
Therefore , several end-to - end approaches have been proposed recently ( Zhu et al. , 2019 b ; Ouyang et al. , 2019 ; , which conduct both translation and summarization simultaneously .
Easy to optimize as these methods are , they typically require a large amount of cross-lingual summarization data , which may not be available especially for low-resource languages .
For instance , NCLS ( Zhu et al. , 2019 b ) proposes to co-train on monolingual summarization ( MS ) and machine translation ( MT ) tasks , both of which require tremendous labeling efforts .
On the other hand , the pre-training strategy has proved to be very effective for language understanding ( Devlin et al. , 2018 ; Holtzman et al. , 2019 ) and cross-lingual learning ( Lample and Conneau , 2019 ; Chi et al. , 2019 ) .
One of the advantages of pre-training is that many associated tasks are selflearning by nature , which means no labeled data is required .
This greatly increases the amount of training data exposed to the model , thereby enhancing its performance on downstream tasks .
Therefore , we leverage large-scale pre-training to improve the quality of cross-lingual summarization .
Built upon a transformer - based encoderdecoder architecture ( Vaswani et al. , 2017 ) , our model is pre-trained on both monolingual tasks including masked language model ( MLM ) , denoising autoencoder ( DAE ) and monolingual summarization ( MS ) , and cross-lingual tasks such as crosslingual masked language model ( CMLM ) and machine translation ( MT ) .
This mixed - lingual pretraining scheme can take advantage of massive unlabeled monolingual data to improve the model 's language modeling capability , and leverage crosslingual tasks to improve the model 's cross-lingual representation .
We then finetune the model on the downstream cross-lingual summarization task .
Furthermore , based on a shared multi-lingual vocabulary , our model has a shared encoder-decoder architecture for all pre-training and finetuning tasks , whereas NCLS ( Zhu et al. , 2019 b ) sets aside taskspecific decoders for machine translation , monolingual summarization , and cross-lingual summarization .
In the experiments , our model outperforms various baseline systems on the benchmark dataset NCLS ( Zhu et al .
, 2019 b ) .
For example , our model achieves 3.27 higher ROUGE - 1 score in Chinese to English summarization than the state - of - the - art result and 1.28 higher ROUGE - 1 score in English to Chinese summarization .
We further conduct an ablation study to show that each pretraining task contributes to the performance , especially our proposed unsupervised pretraining tasks .
Related Work
Pre-training Pre-training language models ( Devlin et al. , 2018 ; have been widely used in NLP applications such as question answering ( Zhu et al. , 2018 ) , sentiment analysis ( Peters et al. , 2018 ) , and summarization ( Zhu et al. , 2019a ; Yang et al. , 2020 ) .
In multi-lingual scenarios , recent works take input from multiple languages and shows great improvements on cross-lingual classification ( Lample and Conneau , 2019 ; Pires et al. , 2019 ; and unsupervised machine translation ( Liu et al. , 2020 ) . Artetxe and Schwenk ( 2019 ) employs the sequence encoder from a machine translation model to produce cross-lingual sentence embeddings .
Chi et al. ( 2019 ) uses multi-lingual pre-training to improve cross-lingual question generation and zero-shot cross-lingual summarization .
Their model trained on articles and summaries in one language is directly used to produce summaries for articles in another language , which is different from our task of producing summaries of one language for an article from a foreign language .
Cross-lingual Summarization
Early literatures on cross-lingual summarization focus on the two-step approach involving machine translation and summarization ( Leuski et al. , 2003 ; Wan et al. , 2010 ) , which often suffer from error propagation issues due to the imperfect modular systems .
Recent end-to - end deep learning models have greatly enhanced the performance .
Shen et al. ( 2018 ) presents a solution to zero-shot cross-lingual headline generation by using machine translation and summarization datasets .
leverages monolingual abstractive summarization to achieve zero-shot cross-lingual abstractive sentence summarization .
NCLS ( Zhu et al. , 2019 b ) proposes a cross-lingual summarization system for large-scale datasets for the first time .
It uses multitask supervised learning and shares the encoder for monolingual summarization , cross-lingual summarization , and machine translation .
However , each of these tasks requires a separate decoder .
In comparison , our model shares the entire encoder-decoder architecture among all pre-training and finetuning tasks , and leverages unlabeled data for monolingual masked language model training .
A concurrent work by improves the performance by combining the neural model with an external probabilistic bilingual lexicon .
Method
Pre-training Objectives
We propose a set of multi-task pre-training objectives on both monolingual and cross-lingual corpus .
For monolingual corpus , we use the masked language model ( MLM ) from Raffel et al . ( 2019 ) .
The input is the original sentence masked by sentinel tokens , and the target is the sequence consists of each sentinel token followed by the corresponding masked token .
The other monolingual task is the denoising auto-encoder ( DAE ) , where the corrupted input is constructed by randomly dropping , masking , and shuffling a sentence and the target is the original sentence .
Since our final task is summarization , we also include monolingual summarization ( MS ) as a pre-training task .
To leverage cross-lingual parallel corpus , we introduce the cross-lingual masked language model ( CMLM ) .
CMLM is an extension of MLM on the parallel corpus .
The input is the concatenation of a sentence in language A and its translation in language B .
We then randomly select one sentence and mask some of its tokens by sentinels .
The target is to predict the masked tokens in the same way as MLM .
Different from MLM , the masked tokens in CMLM are predicted not only from the context within the same language but also from their translations in another language , which encourages the model to learn language - invariant representations .
Note that CMLM is similar to the Translation Language Model ( TLM ) loss proposed in Lample and Conneau ( 2019 ) .
The key differences are : 1 ) TLM randomly masks tokens in sentences from both languages , while CMLM only masks tokens from one language ; 2 ) TLM is applied on encoderonly networks while we employ CMLM on the encoder-decoder network .
In addition to CMLM , we also include standard machine translation ( MT ) objective , in which the input and output are the unchanged source and target sentences , respectively .
The examples of inputs and targets used by our pre-training objectives are shown in Table 1 .
Unified Model for Pre-training and Finetuning While NCLS ( Zhu et al. , 2019 b ) uses different decoders for various pre-training objectives , we employ a unified Transformer ( Vaswani et al. , 2017 ) encoder-decoder model for all pre-training and finetuning tasks .
This makes our model learn a crosslingual representation efficiently .
A shared dictionary across all languages is used .
To accommodate multi-task and multilingual objectives , we introduce language id symbols to indicate the target language , and task symbols to indicate the target task .
For instance , for the CMLM objective where the target language is Chinese , the decoder takes < cmlm > and < zh > as the first two input tokens .
We empirically find that our model does not suffer from the phenomenon of forgetting target language controllability as in Chi et al . ( 2019 ) , which requires manual freezing of encoder or decoder during finetuning .
After pretraining , we conduct finetuning on cross-lingual summarization data .
Experiments
Dataset
We conduct our experiment on NCLS dataset ( Zhu et al. , 2019 b ) , which contains paired data of English articles with Chinese summaries , and Chinese articles with English summaries .
The cross-lingual training data is automatically generated by a machine translation model .
For finetuning and testing , we followed the same train / valid / test split of the original dataset .
We refer readers to Table 1 in Zhu et al . ( 2019 b ) for detailed statistics of the dataset .
For pre-training , we obtain monolingual data for English and Chinese from the corresponding Wikipedia dump .
There are 83 million sentences for English monolingual corpus and 20 million sentences for Chinese corpus .
For parallel data between English and Chinese , we use the parallel corpus from Lample and Conneau ( 2019 ) , which contains 9.6 million paired sentences .
For monolingual summarization objective , we use CNN / DailyMail dataset ( Nallapati et al. , 2016 ) for English summarization and LCSTS dataset ( Hu et al. , 2015 ) for Chinese summarization .
Implementation Details
Our transformer model has 6 layers and 8 heads in attention .
The input and output dimensions d model for all transformer blocks are 512 and the inner dimension d f f is 2048 .
We use a dropout probability of 0.1 on all layers .
We build a shared SentencePiece ( Kudo and Richardson , 2018 )
Baselines
We first include a set of pipeline methods from Zhu et al . ( 2019 b ) which combines monolingual summarization and machine translation .
TETran first translates the source document and then uses LexRank ( Erkan and Radev , 2004 ) to summarize the translated document .
TLTran first summarizes the source document and then translates the summary .
GETran and GLTran replace the translation model in TETran and TLTran with Google Translator 1 respectively .
We also include three strong baselines from Zhu et al . ( 2019 b ) : NCLS , NCLS - MS and NCLS -MT .
We finetune XNLG model from Chi et al . ( 2019 ) on the same cross-lingual summarization data .
We finetune all layers of XNLG in the same way as our pretrained model .
Finally , we include the result of ATS from the concurrent work of .
Results
Table 2 shows the ROUGE scores of generated summaries in English - to - Chinese and Chinese-to - English summarization .
As shown , pipeline models , although incorporating state - of - the - art machine translation systems , achieve sub-optimal performance in both directions , proving the advantages of end-to - end models .
Our model outperforms all baseline models in all metrics except for ROUGE -L in English - to - Chinese .
For instance , our model achieves 2.82 higher ROUGE - 1 score in Chinese to English summarization than the previously best result and 1.15 higher ROUGE - 1 score in English to Chinese summarization , which shows the effectiveness of utilizing multilingual and multi-task data to improve cross-lingual summarization .
from the pre-training objectives i ) all monolingual unsupervised tasks ( MLM , DAE ) , ii ) machine translation ( MT ) , iii ) monolingual summarization ( MS ) , and iv ) all the objectives .
Note that " - All Pretraining " and NCLS both only train on the cross-lingual summarization data .
The performance difference between the two is most likely due to the difference in model size , vocabulary , and other hyperparameters .
Ablation Study
As shown , the pre-training can improve ROUGE -1 , ROUGE - 2 , and ROUGE -L by 2.38 , 1.74 , and 1.13 points respectively on Chinese-to - English summarization .
Moreover , all pre-training objectives have various degrees of contribution to the results , and the monolingual unsupervised objectives ( MLM and DAE ) are relatively the most important .
This verifies the effectiveness of leveraging unsupervised data in the pre-training .
Low-resource scenario .
We sample subsets of size 1 K and 10K from the training data of crosslingual summarization and finetune our pre-trained model on those subsets .
Figure 1 shows the the performance of the pre-trained model and the model trained from scratch on the same subsets .
As shown , the gain from pre-training is larger when the size of training data is relatively small .
This proves the effectiveness of our approach to deal with low-resource language in cross-lingual summarization .
Conclusion
We present a mix-lingual pre-training model for cross-lingual summarization .
We optimize a shared encoder-decoder architecture for multi-lingual and multi-task objectives .
Experiments on a benchmark dataset show that our model outperforms pipelinebased and other end-to - end baselines .
Through an ablation study , we show that all pretraining objectives contribute to the model 's performance .
1 https://translate.google.com/ NCLS trains a standard Transformer model on the cross-lingual summarization dataset .
NCLS - MS and NCLS - MT both use one encoder and multiple decoders for multi-task scenarios .
NCLS - MS combines the cross-lingual summarization task with monolingual summarization while NCLS - MT combines it with machine translation .
