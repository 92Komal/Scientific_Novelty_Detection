title
Intrinsic Evaluation of Summarization Datasets
abstract
High quality data forms the bedrock for building meaningful statistical models in NLP .
Consequently , data quality must be evaluated either during dataset construction or post hoc .
Almost all popular summarization datasets are drawn from natural sources and do not come with inherent quality assurance guarantees .
In spite of this , data quality has gone largely unquestioned for many recent summarization datasets .
We perform the first large-scale evaluation of summarization datasets by introducing 5 intrinsic metrics and applying them to 10 popular datasets .
We find that data usage in recent summarization research is sometimes inconsistent with the underlying properties of the datasets employed .
Further , we discover that our metrics can serve the additional purpose of being inexpensive heuristics for detecting generically low quality examples .
Introduction Data understanding is fundamentally important in natural language processing ( NLP ) ; for data-driven learning - based methods ( e.g. neural networks ) , the quality of the training data bounds the quality of models learned using it .
Therefore , understanding this data is necessary in order to ensure that models learn to perform a given task correctly .
Understanding data is a multidimensional problem .
One line of inquiry has demonstrated why prominent datasets are insufficiently challenging : many data examples can be solved by alternative heuristics that do not encode an approach that is faithful to the task ( McCoy et al. , 2019 ) .
From the perspective of datasets , several works have shown that standard datasets in areas such as visual question answering Kafle and Kanan , 2017 ) , natural language inference ( Gururangan et al. , 2018 ; Poliak et al. , 2018 ) , and reading comprehension ( Kaushik and Lipton , 2018 ) contain annotation artifacts that often give rise to these spurious correlations or reasoning shortcuts .
Data understanding can also inform scientific and ethical decision-making ( Bender and Friedman , 2018 ; Gebru et al. , 2018 ; Mitchell et al. , 2019 ) with recent work studying how social biases encoded in training data propagate to learned models ( Zhao et al. , 2019 ; Tan and Celis , 2019 ) .
In this work , we extend these efforts towards the setting of summarization .
We find this to be particularly timely since several summarization datasets have been released in recent years with little discussion of data quality .
While prior work on evaluating NLP datasets has focused on their difficulty , transparency , or bias , we consider broadly the overall quality of the dataset - in our case , for the task of summarization .
1
Our central insight is that desirable properties of a summary can be readily estimated by adapting and applying existing NLP methods .
With this in mind , we present a multiaspect large-scale study of summarization datasets that dissects summarization into 5 properties that are evaluated across 10 datasets spanning multiple summarization domains .
Our analysis reveals that our metrics can serve as lightweight detectors of generically low quality examples .
Most strikingly , we show that quantifiable aspects of summarization datasets are inconsistent with their use by the NLP community in several instances .
Motivation Quality assurance for data .
Nuanced understanding of data is requisite for drawing sound scientific conclusions .
In particular , without evaluating for the quality and accuracy of data used to test models , it is impossible to be certain that progress is being made and that successive iterations of models truly make progress on the underlying task or linguistic phenomena of interest .
Within NLP , iconic datasets such as the Penn Treebank ( Marcus et al. , 1993 ) have sustained subareas such as language modelling , part- of-speech tagging , and syntactic parsing for years due to the painstaking annotation efforts put into making these high-fidelity resources .
And in the context of summarization , initial datasets , such as those produced during the Document Understanding Conference ( DUC ) and Text Analysis Conference ( TAC ) evaluations , implemented fine - grained verification of data quality .
2
In part due to the emergence of data-hungry modelling techniques , the demands for larger datasets often render quality assurance procedures of this standard to be impractical and infeasible .
Nonetheless , several recent natural language understanding datasets ( Bowman et al. , 2015 ; Rajpurkar et al. , 2016 ; Suhr et al. , 2017 ) institute explicit qualitycontrol procedures in crowd-sourcing dataset construction ( Zaidan and Callison - Burch , 2011 ; Yan et al. , 2014 ; Callison - Burch et al. , 2015 ) , such as using additional annotators to validate annotations ( c.f. Geva et al. , 2019 ) .
In the sibling subfield of machine translation , which often shares similar modelling challenges and evaluation regimes as summarization due to the shared nature of being sequence - to-sequence natural language generation tasks , the annual WMT conference 3 consistently furnishes high quality data .
In summary , ensuring data quality is both crucial and challenging .
And in comparison with other subareas of NLP , we argue that summarization has lagged behind in rigorously ensuring the quality of widely - used datasets .
Relating data quality and model quality .
The correctness and quality of data inherently bounds what can be learned from the data about the task of interest .
From an information - theoretic perspective , this can be made fully formal as follows : 4 I ( S ; M ) training data P , and the model 's architecture A. For fully learning - based methods , especially those with weak / minimal inductive biases such as neural networks , I ( S ; A ) is approximately zero .
While I ( S ; P ) may be greater than zero ( e.g. language modelling pretraining provides statistical information that may facilitate a model to avoid a priori unlikely summaries ) , standard pretraining regimes such as large-scale language modelling over generic text corpora ( Devlin et al. , 2019 ; Raffel et al. , 2019 ) are likely insufficient to meaningfully learn to summarize .
Under these assumptions , the mutual information between S and M is critically upper-bounded in terms of I ( S ; T ) .
We hypothesize that the quality of the training dataset T is highly correlated with its mutual information with respect to the summarization task S , I ( S ; T ) .
One size does not fit all .
Sp?rck Jones ( 1999 ) famously argued that summarization systems should be understood conditional on the context in which they will be used .
In recent years , the field has significantly departed from this perspective and primarily studied " general - purpose summarization " ( Kryscinski et al. , 2019 ) , which she denounced as ignis fatuus .
With our work , we adopt the perspective that for all datasets it is strictly preferable to have all properties quantified ; it is the responsibility of practitioners building summarization systems to accurately weight different metrics based on their ultimate goals and use cases .
As such , we refrain from providing prescriptive domain-agnostic or context -agnostic notions of summarization .
Metrics
In this work , we evaluate the quality of a dataset by aggregating scores for each example in the dataset .
We conjecture that for many NLP tasks , estimating the quality of a particular data example is of similar complexity as correctly performing the task on the example .
5 Nevertheless , for summarization , our insight is that various aspects of a summarization example ( a document - summary pair ) can be reliably estimated by re-purposing existing NLP methods .
We are guided by pioneering work ( Luhn , 1958 ; Edmundson , 1969 ; Mani , 1999 ) that defined core properties of summarization systems and influential sub-sequent work ( Radev et al. , 2002 ; Nenkova , 2006 ; Nenkova and McKeown , 2012 ; Peyrard , 2019a ) that refined and extended these properties .
From this literature , we specifically study compression , topic similarity , abstractivity , redundancy , and semantic coherence as these properties are of recurring and sustained interest .
6
For each abstract property , numerous concrete methods can be proposed to quantify it .
In Appendix
A , we describe alternatives we considered and detail how we decided which methods performed best .
We restrict discussion to the bestperforming approaches in the main paper .
Notation .
Our metrics will assume indexed sets D , S such that summary S i ?
S summarizes document D i ?
D . The length in words of a sequence s is | s | and the length in sentences is s .
Each metric assigns a value x ? [ 0 , 1 ] to every ( D i , S i ) where 1 is the maximal score and example - level scores are averaged to yield a dataset - level score .
Compression .
We quantify compression at the word ( w ) and sentence ( s ) levels : CMP w ( D i , S i ) = 1 ? | S i | | D i | ( 1 ) CMP s ( D i , S i ) = 1 ? S i D i ( 2 ) Topic Similarity .
We learn a topic model M on training corpus T with k topics using LDA ( Blei et al. , 2003 ) and quantify topic similarity by comparing the inferred topic distributions ?
D i | M , ?
S i | M for a given summary and document : TS ( D i , S i ) = 1 ? JS ( ?
D i | M , ? S i |M ) ( 3 ) where JS is the Jensen -Shannon distance .
We set k = 20 and T = D. Abstractivity .
Grusky et al. ( 2018 ) introduced fragments F( D i , S i ) , which are greedily - matched spans shared between D i and S i .
We quantify abstractivity as a normalized function of the aggregate fragment length ; our definition generalizes the definition of Grusky et al . ( 2018 ) .
We set p = 1 . ABS p ( D i , S i ) = 1 ? f ?F
( D i , S i ) |f | p | S i | p ( 4 ) Redundancy .
ROUGE ( Lin , 2004 ) implicitly penalizes redundancy but underestimates its detrimental impacts ( Chaganty et al. , 2018 ) .
However , we find that ROUGE is effective for identifying redundancy given the definitional focus on overlapping spans .
We quantify redundancy as the average ROUGE -L 6 Different names and interpretations have been given for these properties in the literature .
We revisit this in Appendix A in discussing alternate metrics .
F -score for all pairs of distinct sentences in the summary .
RED ( S i ) = mean ( x, y ) ?
S i ?S i , x =y ROUGE ( x , y ) ( 5 ) Semantic Coherence .
We evaluate the semantic coherence of multi-sentence summaries by predicting the probability of each successive sentence conditioned on the previous one using a powerful language model , BERT ( Devlin et al. , 2019 ) , pretrained with precisely this objective .
SC ( S i ) = ||S | | j=2 1 BERT ( S j i | S j?1 i ) || S i || ? 1 ( 6 ) 4 Data
We study the following 10 summarization datasets that have been frequently used in recent years .
GW ( Graff and Cieri , 2003 ) is the Gigaword headline generation dataset that some refer to as a summarization dataset ( Rush et al. , 2015 ; Chopra et al. , 2016 ) .
Examples in the dataset are drawn from seven news sources and are the article prefix paired with its headline .
XSum ( Narayan et al. , 2018 ) is an extreme summarization dataset where BBC articles are paired with single-sentence summaries written generally by the author of the article that tries to motivate the BBC audience to read the article by answering " What is the article about ? " .
PeerRead ( Kang et al. , 2018 ) is a dataset of paper drafts from top-tier computer science venues as well as arXiv .
8 Consistent with its use in the summarization community , we consider the full introduction to be the source document and the ab-stract to be the target summary .
PubMed ( Cohan et al. , 2018 ) is a dataset of papers drawn from the biomedical and life sciences .
Unlike PeerRead , the full paper is taken as the document but the summary is still specified as the abstract .
TL ; DR ( V?lske et al. , 2017 ) is a dataset of userwritten articles from the social media platform Reddit along with the author-provided courtesy summaries that tend to be multi-sentence .
Results and Analysis Compression scores quantitatively disambiguate summarization tasks .
Concretely , we observe GW has the lowest compression scores and while GW is sometimes described as a summarization dataset ( Rush et al. , 2015 ; Chopra et al. , 2016 ) , it is better seen as a headline generation dataset that is more in the style of sentence compression ( as is suggested by S i = D i = 1 ) .
Conversely , AMI and Movi-eScript achieve the highest scores by a substantial margin and are long-document summarization datasets .
Classifying new summarization datasets accurately may prove useful given that successful methods from one domain often do not extend to another and this shortcoming in generalization can be attributed to the differences in compression requirements ( Cohan et al. , 2018 ) .
Given the goals stated in the XSum dataset paper , TL ; DR may be a better choice than XSum .
In particular , Narayan et al . ( 2018 ) introduce XSum as a large dataset that legitimately requires abstraction .
While XSum is more abstractive than other News datasets ( barring GW ) and is relatively large , TL ;DR displays greater abstractivity , similar length summaries , and is 15 times larger .
That said , Narayan et al . ( 2018 ) explore topic-oriented strategies in their work and such methods may be better suited to XSum given the TS scores .
CNN - DM and NYT are suboptimal for studying abstractive / extractive systems respectively .
Several recent works ( See et al. , 2017 ; Paulus et al. , 2018 ; Li et al. , 2018 ) have used CNN - DM to build and evaluate abstractive systems .
Conversely , NYT has been used to build extractive systems ( Hong and Nenkova , 2014 ; Li et al. , 2016 ) .
Given our findings , we find both of these trends to be inconsistent with dataset properties and suboptimal given other preferable datasets for these purposes : CNN - DM is one of the least abstractive datasets and there are larger and more extractive alternatives to NYT such as NWS .
Especially in the case of CNN - DM , we note that training learning - based systems ( e.g. neural methods ) using data with limited abstractivity implies the resulting summarizers will be limited in their ability to generate genuinely abstractive text .
This is validated by empirical findings as both See et al . ( 2017 ) and Zhang et al . ( 2018 ) observe limited abstractivity in abstractive systems trained on CNN -DM .
In light of this , we argue systems should be characterized as abstractive or not based on their empirical behavior rather than their theoretical capability .
9 CNN - DM is not a representative benchmark for summarization as a whole .
Recent work ( Kryscinski et al. , 2019 ; Raffel et al. , 2019 ) has explicitly portrayed CNN - DM as the benchmark dataset for summarization ; the field has implicitly done this for several years ( Kryscinski et al. , 2019 ) .
While there is clear value in evaluating pretrained representations on summarization datasets , we caution against using CNN - DM as a stand - in for the entire summarization subfield .
Instead , we suggest using a diverse group of datasets and not reducing a highly heterogeneous subfield to a single dataset .
While this adds additional overhead , this cost is necessary to draw meaningful conclusions about the impact of advances on summarization broadly given the pronounced diversity in summarization datasets ( Table 1 ) .
Post- processing methods for mitigating redundancy may be needed for practical systems .
While evaluation on standard datasets using may not penalize for this , redundancy is clearly undesirable ( Carbonell and Goldstein , 1998 ; Peyrard , 2019a ) and existing datasets ( and thereby systems learned using that data ) display significant amounts of redundancy in their gold -standard summaries ( exceptions are datasets with short summaries where cross-sentence redundancy is constrained to be low ) .
Specifically , Nenkova ( 2006 ) argues that redundancy is a clear inhibitor for practical application of summarization systems .
Consequently , post hoc methods that reduce redundancy after initial evaluation may be useful in generating summaries that are suitable for human users .
Semantic coherence captures observable variation in summary coherence .
We observe that the Scientific summaries ( which are abstracts of published papers ) are clearly more coherent than the author- generated summaries in TL ; DR , the fragmented summaries in AMI , and the concatenated bullet-point summaries in CNN -DM .
We find that this distinction is captured by the SC measure using BERT .
Quantifying semantic coherence is especially important given that the coherence of reference summaries will inform the coherence of system summaries , especially for learning - based approaches .
Akin to what we discuss for abstractivity , See et al . ( 2017 ) and Paulus et al . ( 2018 ) both demonstrate that neural summarizers generate incoherent summaries despite achieving high ROUGE scores .
Pairwise Correlations
While the properties we evaluate for do not exhaust all aspects of summarization that may be of interest , it is unclear to what extent different measures overlap in judgments .
To quantify this , in we report pairwise correlations for every pair of metrics .
In each case , the value reported is the Spearman rank correlation coefficient ? computed between the length 10 vectors containing the scores for each dataset .
10 ? = 1 indicates perfect positive correlation ( which is why we see this for all diagonal entries ) and ? < 0 indicates the metrics are anti-correlated .
Unsurprisingly , the compression metrics are strongly correlated with each other .
We further observe that redundancy and topic similarity are correlated whereas abstractivity is anti-correlated with both .
In particular , when summaries are considerably redundant , we qualitatively observe that the repeated content in the summary was both important and repeated in the context of the reference document .
As a result , this may explain why redundancy and abstractivity are anti-correlated as this would suggest that highly redundant summaries are highly extractive .
Additionally , since we measure topic similarity using LDA and unigram count statistics , it is not surprising that extractions may correlate with high topic similarity .
In part , this may suggest a deficiency of our measure of topic similarity to accurately consider references to the same topic using substantially different words .
We also observe that semantic coherence patterns similarly to redundancy .
In particular , while we find the semantic coherence scores are appropriate for most examples we manually inspected , this suggests that BERT relies upon word- level overlaps in making next-sentence judgments ( similar to behaviors seen in other sentence - pair tasks such as natural language inference , c.f Gururangan et al. , 2018 )
Detecting Low Quality Examples
To complement our quantitative dataset - level analysis , we conduct a qualitative study of individual examples by examining outliers .
For each ( dataset , metric ) pair , we sample 10 examples from both the top and bottom 10 % of examples for that metric and in that dataset .
Since manually considering all of the 1080 examples was not feasible , we began by examining the sampled examples for topic similarity , redundancy , and semantic coherence .
Our hypothesis was that example quality would positively correlate with coherence and topic similarity and negatively correlate with redundancy .
We found this hypothesis to be validated by our observations as we found that examples with low coherence , low topic similarity , or high redundancy scores were generally low quality examples .
Every example which we judged to be low quality demonstrated at least one of the following defects : ?
The summary contains critical disfluencies that severely hinder accurate processing .
11 ?
The summary excludes unambiguously critical information from the reference document .
?
Crucial information in the summary does not appear in the reference document and is not general knowledge .
?
Substantial fractions of the summary involve entities , relations , or events that are ambiguous and that we could not resolve from the 11 We invoked this condition fairly judiciously as we observed that the domain of summaries also could influence the fluency of summaries in terms of grammaticality .
In particular , we unsurprisingly found that academic papers in the Science domain generally have highly grammatical summaries whereas the bullet-point summaries in CNN - DM and the author-written summaries in TL ; DR often were ungrammatical but still sufficiently clear to be interpreted correctly .
summary alone .
In particular , accurate interpretation of the summary would require also reading the reference document to resolve various coreferring expressions ; the summary is not self-contained .
12 ?
The summary is entirely inappropriate as a summary of the reference document .
For example , the summary only discusses an event with no obvious relationship to the contents of the reference document . ?
The summary includes an entire sentence or long phrase describing something that appears in the main document but that is clearly an auxiliary detail .
We flagged examples as low quality due to this condition quite conservatively , only using it when we could come to no basis for why the sentence / phrase should appear in the summary .
On the other hand , we did not find any systematic defects in examples with high coherence , high topic similarity , or low redundancy scores .
Instead , almost all of these examples were satisfactory .
For the remaining two properties ( compression measured by CMP w , abstractivity measured by ABS 1 ) , we analyzed all of the associated 400 examples .
What we observed is that many of these examples tended to be generically low quality and we quantify this in Table 3 .
Since this analysis may be difficult to replicate and involves subjective decisions about example quality , we comprehensively enumerate all example IDs we use in Table 8 . usually attention - grabbers , and in NYT are nearexact duplicates of reference documents , which themselves are letters to the editor .
Abstractivity .
Manual inspection reveals highly abstractive summaries in NYT and NWS generally are exceedingly vague or are entirely unrelated to the original document .
Highly abstractive summaries in PeerRead are often translated to English from the reference document 's language and discuss results that do not appear in the introduction but likely appear later in the paper .
Conversely , extremely extractive summaries in NWS and NYT often are just the lede and cannot be understood without the reference document .
However , in most other instances , the lede is an effective summary for examples drawn from the News domain .
Within the context of our sample of examples , we find that eight of the ten summarization datasets ( all but AMI , MovieScript ) contain at least 8 % low quality examples , the majority contain at least 14 % low quality examples , and that these low quality examples can be detected using our compression and abstractivity metrics .
For the worst-offending TL ; DR dataset , we conservatively estimate at least 20 % of examples are of substantially subpar quality .
In general , we find that the low quality TL ; DR " summaries " we detect often serve a different rhetorical purpose than summarization ( e.g. attention grabbing , responding to a previous post that is not available in the dataset , sarcasm / humor ) .
Related Work Dataset Analysis .
As an alternative to automated evaluation , Chen et al . ( 2016 ) and Yatskar ( 2019 ) conduct human evaluations of standard datasets in reading comprehension and question answering .
In some cases , dataset creators perform manual analyses of the data they introduce ( e.g. Sandhaus ( 2008 ) and Grusky et al . ( 2018 ) for the NYT and Newsroom corpora , respectively ) .
Automated and human evaluation provide complementary benefits with respect to their scalability and reliability .
Even in the context of human evaluations , we advocate that automatic metrics can be useful in guiding the exploration of data and informing subsampling procedures that provide fine - grained insights .
Quality Estimation .
Our work bears resemblance both in name and structure to work on quality estimation .
Quality estimation , often centered on natural language generation , is the task of measuring system-generated output quality ( Paetzold and Specia , 2016 ; Yuan and Sharoff , 2020 ) .
It is closely related to work on unsupervised or reference - free evaluation ( Napoles et al. , 2016 ; Ethayarajh and Sadigh , 2020 ) .
Within the context of summarization , the special case of quality estimation regarding factual consistency / faithfulness has been of recent interest ( Wang et al. , 2020 ; Maynez et al. , 2020 ; Durmus et al. , 2020 ) since neural abstractive summarizers have been shown to hallucinate / misrepresent facts ( See et al. , 2017 ) .
In comparison to these settings , our metrics make no use of labelled data ( even in training ) and are entirely intrinsic / unsupervised .
Summarization Practices .
Several analyses and critiques exist for different aspects of the summarization pipeline .
From a modelling perspective , Zhang et al . ( 2018 ) assess whether abstractive systems are truly abstractive , Kedzie et al . ( 2018 ) evaluate content selection policies in a variety of methods , and Mao et al . ( 2020 ) assess the facetlevel performance of extractive summarizers .
From an evaluation perspective , several works have discussed the shortcomings of ROUGE / automated evaluation ( Liu and Liu , 2008 ; Chaganty et al. , 2018 ; Hashimoto et al. , 2019 ; Peyrard , 2019 b ) as well proposed alternative metrics for summarization or natural language generation more broadly ( Clark et al. , 2019 ; Zhang et al. , 2020 ; Sellam et al. , 2020 ) .
Two recent works are highly related to our own .
Kryscinski et al. ( 2019 ) provide a critical reevaluation of summarization research .
Most relevant to our work , they show that web-scraped datasets , specifically CNN - DM and NWS , contain a nontrivial fraction of examples ( approx . 3.5 % ) with HTML artifacts ( which can be easily detected / removed ) .
Jung et al. ( 2019 ) provide an aspect-level evaluation of both summarization datasets and systems .
In their work , the dataset analyses center on biases in the data ( e.g. positional biases , which are often seen in news summarization ) , which is reminiscent of the annotation artifacts seen in other NLP tasks ( Gururangan et al. , 2018 ; Niven and Kao , 2019 ) .
Discussion Open Problems and Future Directions .
Our results demonstrate that a sizeable fraction of examples in most summarization datasets are low quality .
However , it remains open whether modellers should simply prune these examples , manually / automatically attempt to correct them , or model them without change .
We do note that research in the machine learning and learning theory communities shows that models both theoretically and empirically do substantially worse when trained using low quality examples , even when the examples are not strictly adversarially chosen ( Klivans et al. , 2009 ; Biggio et al. , 2012 ; Koh et al. , 2018 ) .
These concerns are further compounded by the evidence of Belinkov and Bisk ( 2018 ) that neural models for natural language generation are not robust to naturally noisy data .
Our metrics may be repurposed to rank examples in designing curricula for curriculum learning ap-proaches ( Bengio et al. , 2009 ) .
Alternatively , they can serve as additional metrics for the ( possibly unsupervised ) evaluation of summarization systems , potentially mitigating deficiencies in standard metrics , such as ROUGE , by directly penalizing redundancy and semantic incoherence .
Limitations .
In this work , we restrict ourselves to single-document single-reference English language summarization datasets .
While the datasets we study constitute a considerable fraction of dataset usage in the summarization community , several multi-document summarization datasets have been introduced ( e.g. Fabbri et al. , 2019 ; Antognini and Faltings , 2020 ) and multi-reference summarization datasets have often been argued to be desirable due to under-constrained nature of the summarization task ( Kryscinski et al. , 2019 ) and the ideal evaluation paradigm for ROUGE ( Lin , 2004 ) .
Beyond English , both large summarization datasets ( Nguyen and Daum ?
III , 2019 ; Varab and Schluter , 2020 ) and more general language resources / technologies ( Joshi et al. , 2020 ) are less available , which may heighten the need for data quality assurance .
More broadly , the measures that we introduce are automated , and therefore non-human , judgments of the quality of summarization data .
Therefore , we only envision these measures to be useful as inexpensive first-order approximations of aspectlevel summary quality rather than bona fide replacements for human evaluation .
Additionally , since we principally envision applying these metrics to datasets , we make no efforts to make these metrics robust to adversarially - crafted data and they are likely quite susceptible to adversarial attack .
Conclusion
In this work , we demonstrate that various aspects of summarization datasets can be intrinsically evaluated for .
We specifically show this for 5 properties across 10 popular datasets , uncovering that dataset use is sometimes incongruous with the attributes of the underlying data .
We also find that some aspectlevel estimators may be surprisingly effective at detecting low quality dataset examples .
Our findings suggest that more intentional and deliberate decisions should be made in selecting summarization datasets for downstream modelling research and that further scrutiny should be placed upon summarization datasets released in the future .
