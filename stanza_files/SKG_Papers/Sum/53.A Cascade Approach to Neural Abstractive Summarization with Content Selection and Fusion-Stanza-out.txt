title
A Cascade Approach to Neural Abstractive Summarization with Content Selection and Fusion
abstract
We present an empirical study in favor of a cascade architecture to neural text summarization .
Summarization practices vary widely but few other than news summarization can provide a sufficient amount of training data enough to meet the requirement of end-to - end neural abstractive systems which perform content selection and surface realization jointly to generate abstracts .
Such systems also pose a challenge to summarization evaluation , as they force content selection to be evaluated along with text generation , yet evaluation of the latter remains an unsolved problem .
In this paper , we present empirical results showing that the performance of a cascaded pipeline that separately identifies important content pieces and stitches them together into a coherent text is comparable to or outranks that of end-to - end systems , whereas a pipeline architecture allows for flexible content selection .
We finally discuss how we can take advantage of a cascaded pipeline in neural text summarization and shed light on important directions for future research .
Introduction
There is a variety of successful summarization applications but few can afford to have a large number of annotated examples that are sufficient to meet the requirement of end-to - end neural abstractive summarization .
Examples range from summarizing radiology reports ( Jing et al. , 2019 ; Zhang et al. , 2020 ) to congressional bills ( Kornilova and Eidelman , 2019 ) and meeting conversations ( Mehdad et al. , 2013 ; Koay et al. , 2020 ) .
The lack of annotated resources suggests that end-toend systems may not be a " one-size-fits - all " solution to neural text summarization .
There is an increasing need to develop cascaded architectures to allow for customized content selectors to be combined with general - purpose neural text generators to realize the full potential of neural abstractive summarization .
We advocate for explicit content selection as it allows for a rigorous evaluation and visualization of intermediate results of such a module , rather than associating it with text generation .
Existing neural abstractive systems can perform content selection implicitly using end-to - end models ( See et al. , 2017 ; Celikyilmaz et al. , 2018 ; Raffel et al. , 2019 ; Lewis et al. , 2020 ) , or more explicitly , with an external module to select important sentences or words to aid generation ( Tan et al. , 2017 ; Gehrmann et al. , 2018 ; Chen and Bansal , 2018 ; Kry?ci?ski et al. , 2018 ; Hsu et al. , 2018 ; Lebanoff et al. , 2018 Lebanoff et al. , , 2019 b Liu and Lapata , 2019 ) .
However , content selection concerns not only the selection of important segments from a document , but also the cohesiveness of selected segments and the amount of text to be selected in order for a neural text generator to produce a summary .
In this paper , we aim to investigate the feasibility of a cascade approach to neural text summarization .
We explore a constrained summarization task , where an abstract is created one sentence at a time through a cascaded pipeline .
Our pipeline architecture chooses one or two sentences from the source document , then highlights their summary - worthy segments and uses those as a basis for composing a summary sentence .
When a pair of sentences are selected , it is important to ensure that they are fusible - there exists cohesive devices that tie the two sentences together into a coherent text - to avoid generating nonsensical outputs ( Geva et al. , 2019 ; Lebanoff et al. , 2020 ) .
Highlighting sentence segments allows us to perform fine- grained content selection that guides the neural text generator to stitch selected segments into a coherent sentence .
The contributions of this work are summarized as follows .
?
We present an empirical study in favor of a cascade architecture for neural text summarization .
Our cascaded pipeline chooses one or two sentences from the document and highlights their important segments ; these segments are passed to a neural generator to produce a summary sentence .
Sent ?
Our quantitative results show that the performance of a cascaded pipeline is comparable to or outranks that of end-to - end systems , with added benefit of flexible content selection .
We discuss how we can take advantage of a cascade architecture and shed light on important directions for future research .
1
A Cascade Approach Our cascaded summarization approach focuses on shallow abstraction .
It makes use of text transformations such as sentence shortening , paraphrasing and fusion ( Jing and McKeown , 2000 ) and is in contrast to deep abstraction , where a full semantic analysis of the document is often required .
A shallow approach helps produce abstracts that convey important information while , crucially , remaining faithful to the original .
In what follows , we describe our approach to select single sentences and sentence pairs from the document , highlight summary - worthy segments and perform summary generation conditioned on highlights .
Selection of Singletons and Pairs
Our approach iteratively selects one or two sentences from the input document ; they serve as the basis for composing a single summary sentence .
Previous research suggests that 60 - 85 % of human-written summary sentences are created by shortening a single sentence or merging a pair of sentences ( Lebanoff et al. , 2019 b ) .
We adopt this setting and present a coarse- to - fine strategy for content selection .
Our strategy begins with selecting sentence singletons and pairs , followed by highlighting important segments of the sentences .
Importantly , the strategy allows us to control which segments will be combined into a summary sentence -" compatible " segments come from either a single document sentence or a pair of fusible sentences .
In contrast , when all important segments of the document are provided to a neural generator all at once ( Gehrmann et al. , 2018 ) , it can happen that the generator arbitrarily stitches together text segments from unrelated sentences , yielding a summary that contains hallucinated content and fails to retain the meaning of the original document ( Falke et al. , 2019 ; Lebanoff et al. , 2019a ; Kryscinski et al. , 2019 ) .
We expect a sentence singleton or pair to be selected from the document if it contains salient content .
Moreover , a pair of sentences should contain content that is compatible with each other .
Given a sentence or pair of sentences from the document , our model predicts whether it is a valid instance to be compressed or merged to form a summary sentence .
We follow ( Lebanoff et al. , 2019 b ) to use BERT ( Devlin et al. , 2019 ) to perform the classification .
BERT is a natural choice since it takes one or two sentences and generates a classification prediction .
It treats an input singleton or pair of sentences as a sequence of tokens .
The tokens are fed to a series of Transformer block layers , consisting of multi-head self-attention modules .
The first Transformer layer creates a contextual representation for each token , and each successive layer further refines those representations .
An additional [ CLS ] token is added to contain the sentence representation .
BERT is fine- tuned for our task by adding an output layer on top of the final layer representation h L [ CLS ] for sequence s , as seen in Eq. ( 1 ) .
p sent ( s ) = ?( u h L [ CLS ] ) ( 1 ) where u is a vector of weights and ? is the sigmoid function .
The model predicts p sent - whether the sentence singleton or pair is an appropriate one based on the [ CLS ] token representation .
We describe the training data for this task in ?3 .
Fine - Grained Content Selection
It is interesting to note that the previous architecture can be naturally extended to perform fine - grained content selection by highlighting important words of sentences .
When two sentences are selected to generate a fusion sentence , it is desirable to identify segments of text from these sentences that are potentially compatible with each other .
The coarse-tofine method allows us to examine the intermediate results and compare them with ground -truth .
Concretely , we add a classification layer to the final layer representation h L i for each token w i ( Eq. ( 2 ) ) .
The per-target - word loss is then interpolated with instance prediction ( one or two sentences ) loss using a coefficient ?.
Such a multi-task learning objective has been shown to improve performance on a number of tasks ( Guo et al. , 2019 ) . p highlight ( w i ) = ?( v h L i ) ( 2 ) where v is a vector of weights and ? is the sigmoid function .
The model predicts p highlight for each token - whether the token should be included in the output fusion , calculated based on the given token 's representation .
Information Fusion Given one or two sentences taken from a document and their fine- grained highlights , we proceed by describing a fusion process that generates a summary sentence from the selected content .
Our model employs an encoderdecoder architecture based on pointer - generator networks that has shown strong performance on its own and with adaptations ( See et al. , 2017 ; Gehrmann et al. , 2018 ) .
We feed the sentence singleton or pair to the encoder along with highlights derived by the fine- grained content selector , the latter come in the form of binary tags .
The tags are transformed to a " highlight - on " embedding for each token if it is chosen by the content selector , and a " highlight - off " embedding for each token not chosen .
The highlight-on / off embeddings are added to token embeddings in an element - wise manner ; both highlight and token embeddings are learned .
An illustration is shown in Figure 1 .
Highlights provide a valuable intermediate representation suitable for shallow abstraction .
Our approach thus provides an alternative to methods that use more sophisticated representations such as syntactic / semantic graphs ( Filippova and Strube , 2008 ; Banarescu et al. , 2013 ; Liu et al. , 2015 ) .
It is more straightforward to incorporate highlights into an encoder-decoder fusion model , and obtaining highlights through sequence tagging can be potentially adapted to new domains .
Experimental Results Data and Annotation
To enable direct comparison with end-to - end systems , we conduct experiments on the widely used CNN / DM dataset ( See et al. , 2017 ) to report results of our cascade approach .
We use the procedure described in Lebanoff et al . ( 2019 b ) to create training instances for the sentence selector and fine- grained content selector .
Our training data contains 1,053,993 instances ; every instance contains one or two candidate sentences .
It is a positive instance if a groundtruth summary sentence can be formed by compressing or merging sentences of the instance , negative otherwise .
For positive instances , we highlight all lemmatized unigrams appearing in the summary , excluding punctuation .
We further add smoothing to the labels by highlighting single words that con - System R - 1 R - 2 R-L SumBasic ( Vanderwende et al. , 2007 ) 34.11 11.13 31.14 LexRank ( Erkan and Radev , 2004 ) 35.34 13.31 31.93 Pointer -Generator ( See et al. , 2017 ) 39.53 17.28 36.38 FastAbsSum ( Chen and Bansal , 2018 ) 40.88 17.80 38.54 BERT - Extr ( Lebanoff et al. , 2019 b ) 41.13 18.68 37.75 BottomUp ( Gehrmann et al. , 2018 ) 41 ( SYSTEM SENTS ) A Duke student has admitted to hanging a noose made of rope from a tree near a student union , university officials said Thursday .
The student was identified during an investigation by campus police and the office of student affairs and admitted to placing the noose on the tree early Wednesday , the university said .
( CASCADE - FUSION ) A Duke student was identified during an investigation by campus police and the office of student affairs and admitted to placing the noose on the tree early Wednesday .
( GT SENTS )
In a news release , it said the student was no longer on campus and will face student conduct review .
Duke University is a private college with about 15,000 students in Durham , North Carolina .
( GT SENTS + FUSION ) Duke University student was no longer on campus and will face student conduct review .
( REFERENCE ) Student is no longer on Duke University campus and will face disciplinary review .
Table 1 : ( LEFT ) Summarization results on CNN / DM test set .
Our cascade approach performs comparable to strong extractive and abstractive baselines ; oracle models using ground -truth sentences and segment highlights perform the best .
( RIGHT ) Example source sentences and their fusions .
Dark highlighting is content taken from the first sentence , and light highlighting comes from the second .
Our Cascade-Fusion approach effectively performs entity replacement by replacing " student " in the second sentence with " a Duke student " from the first sentence .
nect two highlighted phrases and by dehighlighting isolated stopwords .
At test time , four highestscored instances are selected per document ; their important segments are highlighted by content selector then passed to the fusion step to produce a summary sentence each .
The hyperparameter ? for weighing the per-target - word loss is set to 0.2 and highlighting threshold value is 0.15 .
The model hyperparameters are tuned on the validation split .
Summarization Results
We show experimental results on the standard test set and evaluated by ROUGE metrics ( Lin , 2004 ) in Table 1 .
The performance of our cascade approaches , Cascade-Fusion and Cascade -Tag , is comparable to or outranks a number of extractive and abstractive baselines .
Particularly , Cascade - Tag does not use a fusion step ( ?2 ) and is the output of fine- grained content selection .
Cascade-Fusion provides a direct comparison against BERT - Abs ( Lebanoff et al. , 2019 b ) that uses sentence selection and fusion but lacks a fine-grained content selector .
Our results suggest that a coarse- to-fine content selection strategy remains necessary to guide the fusion model to produce informative sentences .
We observe that the addition of the fusion model has only a moderate impact on ROUGE scores , but the fusion process can reorder text segments to create true and grammatical sentences , as shown in Table 1 .
We analyze the performance of a number of oracle models that use ground - truth sentence selection ( GT - Sent ) and tagging ( GT - Tag ) .
When given ground - truth sentences as input , our cascade models achieve ?10 points of improvement in all ROUGE metrics .
When the models are also given ground - truth highlights , they achieve an additional 20 points of improvement .
In a preliminary examination , we observe that not all highlights are included in the summary during fusion , indicating there is space for improvement .
These results show that cascade architectures have great potential to generate shallow abstracts and future emphasis may be placed on accurate content selection .
How much should we highlight ?
It is important to quantify the amount of highlighting required for generating a summary sentence .
Highlighting too much or too little can be unhelpful .
We experiment with three methods to determine the appropriate amount of words to highlight .
Probability
Thresholding chooses a set threshold whereby all words that have a probability higher than the threshold are highlighted .
When Proportional to Input is used , the highest probability words are iteratively highlighted until a target rate is reached .
The amount of highlighting can be proportional to the total number of words per instance ( one or two sentences ) or per document , containing all sentences selected for the document .
We investigate the effect of varying the amount of highlighting in Figure 2 .
Among the three methods , probability thresholding performs the best , as it gives more freedom to content selection .
If the model scores all of the words in sentences highly , then we should correspondingly highlight all of the words .
If only very few words score highly , then we should only pick those few .
Highlighting a certain percentage of words tend to perform less well .
On our dataset , a threshold value of 0.15 - 0.20 produces the best ROUGE scores .
Interestingly , these thresholds end up highlighting 58 - 78 % of the words of each sentence .
Compared to what the generator was trained on , which had a median of 31 % of each sentence highlighted , the system 's rate of highlighting is higher .
If the model 's highlighting rate is set to be similar to that of the ground - truth , it yields much lower ROUGE scores ( cf. threshold value of 0.3 in Figure 2 ) .
This observation suggests that the amount of highlighting can be related to the effectiveness of content selector and it may be better to highlight more than less .
Conclusion
We present a cascade approach to neural abstractive summarization that separates content selection from surface realization .
Importantly , our approach makes use of text highlights as intermediate representation ; they are derived from one or two sentences using a coarse- to - fine content selection strategy , then passed to a neural text generator to compose a summary sentence .
A successful cascade approach is expected to accurately select sentences and highlight an appropriate amount of text , both can be customized for domain-specific tasks .
Figure 1 : 1 Figure 1 : Model architecture .
We divide the task between two main components : the first component performs sentence selection and fine- grained content selection , which are posed as a classification problem and a sequencetagging problem , respectively .
The second component receives the first component 's outputs as supplementary information to generate the summary .
A cascade architecture provides the necessary flexibility to separate content selection from surface realization in abstractive summarization .
