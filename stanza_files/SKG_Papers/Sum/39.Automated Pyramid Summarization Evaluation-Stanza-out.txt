title
Automated Pyramid Summarization Evaluation
abstract
Pyramid evaluation was developed to assess the content of paragraph length summaries of source texts .
A pyramid lists the distinct units of content found in several reference summaries , weights content units by how many reference summaries they occur in , and produces three scores based on the weighted content of new summaries .
We present an automated method that is more efficient , more transparent , and more complete than previous automated pyramid methods .
It is tested on a new dataset of student summaries , and historical NIST data from extractive summarizers .
Introduction
During the 70's and 80's , educational pychologists studied human summarization skills , and their development throughout secondary school and beyond .
Three separate skills are acquired in the following order : selection of important information , abstraction through vocabulary generalization and sentence fusion , and integration with background knowledge ( van Dijk and Kintsch , 1977 ; Brown and Day , 1983 ) .
A recent comparison of summaries from human experts versus extractive summarizers on forty - six topics from the TAC 2010 summarization challenge used automatic caseframe analysis , and found essentially these same properties in the human summaries , and not in the extractive ones ( Cheung and Penn , 2013 ) .
Abstractive summarizers , however , are beginning to replicate the first two of these behaviors , as illustrated in many published examples based on encoder-decoder and pointergenerator neural architectures ( Nallapati et al. , 2016 ; See et al. , 2017 ; Hsu et al. , 2018 ; Guo et al. , 2018 ) . Summarization evaluation relies almost exclusively on ROUGE ( Lin , 2004 ) , an automated tool that cannot directly assess importance of summary content , or novel wording for the same infor -
Scotland have made enquiries to allow payment from customers using cryptoc .
Match to a student summary that used synomyms : a craftsmanship exhibition alongside a Scottish inn have plans for their clients to pay in digital currencies to a manual SCU of weight 4 from a dataset of student summaries .
The manual and automated SCUs express the same content , and their weights differ only by one .
For each of five reference summaries ( RSUM1 - RSUM5 ) , exact matches of words between the PyrEval and manual contributor are in bold , text in plain font ( RSUM2 , RSUM4 ) appears only in the manual version , and text in italics appears only in the PyrEval version .
Paraphrases of the same content from RSUM4 were identified by human annotators ( plain font ) and PyrEval ( italics ) .
Also shown is a matching segment from a student summary , where the student used synonyms of some of the words in the reference summaries .
mation .
We present an automated method to assess the importance of summary content , independent of wording , based on a widely used manual evaluation called pyramid ( Nenkova et al. , 2007 ) .
The pyramid method and ROUGE both use multiple summaries written by humans as references to assess new summaries .
The manual pyramid method requires human annotators to identify Summary Content Units ( SCUs ) by grouping phrases from different reference summaries into the same SCU if they express the same propositional content .
Figure 1 illustrates an SCU from a manual pyramid applied to college student summaries of articles on cryptocurrency , with contributions from four of the five reference summaries ( RSUM1 - RSUM4 ) .
It is aligned to a nearly identical SCU constructed by PyrEval , with a contribution from the fifth reference ( RSUM5 ) .
Previous work has shown that these kinds of discrepancies occur between human annotators , and have little effect on interannotator agreement or rankings of summarizers ( Passonneau , 2010 ) .
The importance of an SCU increases with the number of reference summaries that express it , as indicated by its weight .
If an evaluation summary expresses the same content as an SCU , its score is increased by the SCU weight ( details below ) .
ROUGE allows the user to select among numerous ways to measure ngram overlap of a new summary to the references , e.g. , for different ngram sizes with or without skips , and with or without stemming .
ROUGE does not , however , consider the relative importance of content , or account for synonyms of words that appear in the reference summaries .
We present PyrEval , 1 which outperforms previous work on automated pyramid in accuracy and efficiency .
It produces human-readable pyramids , and prints matches between SCUs and evaluation summaries , which can support feedback for educational applications .
PyrEval performs well on a new dataset of student summaries , where we applied the pyramid annotation .
We also present results for TAC 2010 automated summaries , one of the more recent years where NIST applied pyramid evaluation .
While ROUGE - 2 more accurately identifies system differences than PyrEval , its performance is more sensitive to different topics .
Pyramid Content Analysis
The challenge in evaluation of summary content is that different equally good human summaries have only partial content overlap .
van Halteren and Teufel ( 2003 ) annotated factoids ( similar to FOL propositions , and to SCUs ) for fifty summaries of a Dutch news article , and found a Zipfian distribution of factoid frequency : a small number of factoids represent 80 % of the content in summaries , but a very long tail of rare content accounts for 20 % .
Pyramid annotation of ten summaries for a 1 Available at https://github.com/serenayj/
PyrEval few DUC 2003 topics had a similar a Zipfian distribution ( Nenkova and Passonneau , 2004 ) .
Pyramid has had extensive reliability testing .
A sensitivity analysis showed four reference summaries were sufficient for score reliability , and with probability of misranking errors below 0.01 % ( Nenkova and Passonneau , 2004 ; Nenkova et al. , 2007 ) .
Interannotator agreement using Krippendorff 's alpha on ten pyramids ranged from 0.61 to 0.89 , and averaged 0.78 on matching new summaries to pyramids for 16 systems on 3 topics each ( Passonneau , 2010 ) .
Comparison of two manual pyramid evaluations from distinct annotators showed that different pyramids for the same topic yield the same system rankings , even though SCUs from different pyramids typically do not align exactly ( Passonneau , 2010 ) .
The default size of a phrase that contributes to an SCU is a simple clause , but if it is clear from the context that a summary essentially expresses the same content expressed in other reference summaries , it is said to contribute to the same SCU , and the annotator must select at least a few contributing words .
SCU weights reflect how many of N reference summaries express the SCU content .
As such , SCUs are constrained to have no more than one contributor phrase from each reference summary .
If a summary repeats the same information , the repetition will increment the count of total SCUs within one summary , but cannot be a distinct contributor .
For example , the paraphrases from RSUM4 shown in Figure 1 add two to the total SCU size of the summary , but can only be used once to increment an SCU weight .
Simple clauses in an evaluation summary that do not match pyramid SCUs add to the summary 's SCU count , but have zero weight .
Related Work Summarization is an important component of strategy instruction in reading and writing skills ( Graham and Perin , 2007 ) , but is used less than it could be due to the demands of manual grading and feedback .
However , integration of NLP with rubric-based assessment has received increasing attention .
and Gerard and Linn ( 2016 ) applied automated assessment using rubrics to successfully identify students who need the most help , and facilitate and meaningful classroom interactions .
Agejev and ?najder ( 2017 ) ROUGE is the most prevalent method to assess automated summarization .
In 39 long papers on summarization in ACL conferences from 2013 through 2018 ( mostly abstractive ) , 87 % used ROUGE -1 , ROUGE - 2 or other variants such as LCS ( longest common subsequence ) .
A few used POURPRE ( question answering ) ( Lin and Demner-Fushman , 2006 ) , or METEOR ( MT ) ( Denkowski and Lavie , 2014 ) to investigate scores based on weighted content or synonomy .
POURPRE relies on string matching against reference units called answer facts , weighting matches by inverse document frequency .
METEOR aligns words between reference and candidate , and can use relaxed word matching , such as WordNet synonymy .
Despite its dominant use in previous work , Graham ( 2015 ) noted that the large range of ROUGE variants causes inconvenience and instability in evaluating performance .
Graham 's results from testing the 192 variants on DUC2004 data suggest that the ROUGE variants that correlate best with human evaluations are not often used .
PyrEval differs from other automated pyramid tools in its focus on accurately isolating and weighting the distinct SCUs in the reference summaries .
Three previous semi-automated pyramid tools used dynamic programming to score summaries , given a manual pyramid ( Harnly et al. , 2005 ; Passonneau et al. , 2013 Passonneau et al. , , 2018 .
The first of these used unigram overlap to compare summaries to a pyramid .
Absolute scores were much lower than ground truth , but average system rankings across multiple tasks were accurate .
A subsequent extension that used cosine similarity of latent vector representations of ngrams and SCUs , based on ( Guo and Diab , 2012 ) , had much better performance ( Passonneau et al. , 2013 ) .
This was extended further through use of a weighted set cover algorithm for scoring ( Passonneau et al. , 2018 ) . PEAK was the first fully automated approach to construct a pyramid and score summaries ( Yang et al. , 2016 ) .
It uses OpenIE to extract subjectpredicate -object triples from references , then con-structs a hypergraph with triples as hyperedges .
Semantic similarity between nodes from distinct hyperedges is measured using ADW 's random walks over WordNet ( Pilehvar et al. , 2013 ) , to assign weights to triples .
On a small set of summaries used here in Table 1 , PEAK raw scores had a high correlation with a manual summary rubric .
PEAK was also tested on a single DUC 2006 topic , where the input text was first manually altered .
Because PEAK is slow , Peyrard and Eckle-Kohler ( 2017 ) reimplemented it 's use of the Hungarian algorithm to optimize their summarizer .
Because PEAK produces many noisy copies of the same SCU , its output cannot be used to justify scores based on the unique matches or misses of a student summary to SCUs .
Its score normalizations are inaccurate , and the un-normalized scores are impractical for general - purpose evaluation .
PyrEval System
To construct a pyramid , humans identify contributor segments 2 and group them into SCUs .
Evaluating a summary is a simpler process of matching phrases to existing SCUs .
PyrEval performs analogous steps , as shown in Figure 2 .
It first decomposes sentences of reference summaries ( RSUM ) into segments ( DECOMP PARSE ) and converts them into semantic vectors ( LATENT SEM ) .
It then applies EDUA to group the segment vectors into SCUs .
EDUA ( see below ) is a novel restricted set partition algorithm that maximizes the semantic similarity within SCUs , subject to SCU constraints .
Evaluation summaries ( ESUM ) are preprocessed in a similar fashion to convert them to segments represented as vectors .
As in ( Passonneau et al. , 2018 ) , PyrEval applies WMIN ( Sakai et al. , 2003 ) to find the optimal set of matches with pyramid SCUs .
The remainder of this section describes each step .
Sentence Decomposition
The decomposition parser takes as input a phrase structure parse and dependency parse for each sentence , using Stanford CoreNLP .
Every tensed verb phrase ( VP ) from the phrase structure parse initializes a new segment .
The head verbs of tensed VPs are aligned to the dependency parse , and their dependent subjects are then attached to the segments .
Words other than those in the VP and subject are reinserted in their original order .
Every sentence has at least one default segmentation corresponding to the full sentence , possibly with one or more alternative segmentations of at least two segments each .
It performs well for most cases apart from sentences with coordinate structures , which are notoriously difficult for conventional parsers .
Figure 3 illustrates a sentence segmentation , with three alternatives .
3
Semantic Vectors for Segments
The second PyrEval preprocessing step converts segments to semantic vectors .
We chose to avoid semantic representation that requires training , to make PyrEval a lightweight , standalone tool .
Although recent contextualized representations perform very well on a variety of NLP tasks , they are typically intended as the basis for a transfer learning approach , or to initialize further task-specific Christopher Shake , the director of the London art gallery , suggests that this is not the case and that many different companies of different natures not just technology related are getting involved with cryptocurrencies .
1.6.1.0 that this is not the case 1.6.1.1 Christopher
Shake , the director of the London art gallery , suggests and .
1.6.1.2 that many different companies of different natures not just technology related are getting involved with cryptocurrencies 1.6.2.0 Christopher
Shake , the director of the London art gallery , suggests that this is not the case and .
1.6.2.1 that many different companies of different natures not just technology related are getting involved with cryptocurrencies 1.6.3.0 Christopher Shake , the director of the London art gallery , suggests and that many different companies of different natures not just technology related are getting involved with cryptocurrencies .
1.6.3.1 that this is not the case Figure 3 : Segmentation output for a sentence from a reference summary for the " CryptoCurrencies " topic of our student summaries .
neural training ( e.g. , ( Pagliardini et al. , 2018 ; Peters et al. , 2018 ; Devlin et al. , 2018 ; Vaswani et al. , 2017 ) ) .
The most practical way to rely on completely pre-trained representations is to use word embeddings along with a method to combine them into phrase embeddings .
Here we report on a comparison of ELMo ( Peters et al. , 2018 ) and the Universal Sentence Encoder for English ( USE ) ( Cer et al. , 2018 ) with two conventional word embedding methods , GloVe ( Pennington et al. , 2014 ) and WTMF ( Guo and Diab , 2012 ) .
4 ELMo is character - based rather than wordbased , relies on a many - layered bidirectional LSTM , and incorporates word sequence ( language model ) information .
It was trained on billions of tokens of Wikipedia and news text .
To create meaning vectors for strings of words , we use pretrained ELMo vectors , taking the weighted sum of 3 output layers as the word embeddings , then applying mean pooling .
5 USE is intended for transfer learning tasks , based on Transformer ( Vaswani et al. , 2017 ) or the ( Iyyer et al. , 2015 ) deep averaging network ( DAN ) .
We create meaning vectors for word strings with the USE - DAN pretrained encoder .
6
We use the GloVe download for 100D vec-Figure 4 : Part of an EDUA solution graph .
Each vertex is a segment vector from a reference summary , indexed by Summary .
ID ( si ) , Sentence .ID ( sij ) , Segmentation .ID ( s ijk ) , Segment .ID ( s ijkm ) .
All segments of all reference summaries have a corresponding node .
All edges connect segments from different summaries with similarity ?
t edge .
This schematic representation of a partial solution contains three fully connected subgraphs with attraction edges ( solid lines ) , each representing an SCU , whose weight is the number of vertices ( segments ) .
tors trained on the 840B Common Crawl .
7
To combine the GloVe word vectors into a phrase vector , we use the weighted averaging method from ( Arora et al. , 2016 ) . WTMF is a matrix factorization method .
We use WTMF matrices trained on the Guo and Diab ( 2012 ) corpus ( 393 K sentences , 81 K vocabulary size ) that consists of WordNet , Wiktionary , and the Brown corpus .
We compare the four embedding methods on three datasets .
Because our goal is to select a method that performs well on pyramid annotation , the first two datasets are human and machine summaries with manual pyramid annotations , with correlation of the manual pyramid and PyrEval scores as the metric .
WIM ( for What is Matter ) is a dataset of student summaries with pyramid annotation from ( Passonneau et al. , 2018 ) with 20 student summaries on one topic .
Note that PyrEval achieved a correlation of 0.85 on this data , compared with 0.82 for PEAK ( Passonneau et al. , 2018 ) .
We also use a subset of data from the NIST TAC 2009 summarizer challenge .
We use summaries from all 54 peer systems on 14 of the 44 topics .
We also use the STS - 14 benchmark dataset of semantic similarity judgements ( 3750 sentence pairs ) , as in ( Guo and Diab , 2012 ) .
Table 1 shows WTMF to perform best on the 1 .
Given a set of n reference summaries R , a preprocessing function ( described in subsections 4.1- 4.2 ) SEG returns segments as vectors : ? Ri ? R , SEGS ( Ri ) = { seg ij k1 , seg ijk2 , . . . , seg ijkm } where seg ijkm is the mth segment of the kth segmentation of the jth sentence in the ith summary .
2 . A graph G is constructed from SEGS ( Ri ) , where an edge connects segments seg ijkm , seg i j k m if ( i = i , seg ijkm = seg i j k m , cosine ( seg ijkm , seg i j k m ) ? t edge ) .
Every fully connected subgraph ( clique ) is a candidate scu whose size is the number of nodes , which has a maximum of n.
3 .
The attraction score of an scu z , AS ( scu z ) = 1 ( | scu z | 2 ) seg ijkm , seg i j k m ? scu z , seg ijkm =seg i j k m cosine ( seg ijkm , seg i j k m ) if z > 1 , else = 1 .
4 . A candidate pyramid P is a set of equivalence classes SCU x that is a covering of all sentences in R ( meaning only one segmentation per sentence belongs to any P ) , ? x ? [ 1 , n ] : (?
SCU x ? P ) ? ( x ? [ 1 , n ] , ?scu z ? SCU x , x = z ) .
An SCU x has an attraction class score AC ( SCU x ) = 1 | SCU x | scu z ?SCU x AS ( SCU z ) .
5 . Finally , a pyramid P has an attraction score AP ( P ) = SCU x ?P AS ( SCU x ) .
6 . The optimal pyramid ( R ) = P that maximizes AP .
Figure 5 : Formal specification of EDUA 's input graph G consisting of all segments from all segmentations of reference summary sentences ( item 2 ) , the objective ( item 6 ) , and three scores for defining the objective function that are assigned to candidate SCUs ( item 3 ) , sets of SCUs of the same weight ( item 4 ) , and a candidate pyramid ( item 5 ) .
three tasks by a large margin .
We speculate this results from two factors .
The lower dimensionality of WTMF vectors compared to ELMo or USE - DAN leads to higher maximum cosine values , thus better contrast between similar and dissimilar pairs .
WTMF differs from similar matrix reduction methods in assigning a small weight to non-context words , which improves robustness for short phrases ( fewer context words ) Guo and Diab ( 2012 ) .
The authors also claimed that a training corpus largely consisting of definitional statements leads to co-occurrence data that is less noisy than sentences found in the wild .
EDUA EDUA ( Emergent Discovery of Units of Attraction ) is a restricted set partition algorithm .
It constructs an optimal pyramid to achieve the highest attraction ( semantic similarity of segments ) in all SCUs .
Figure 4 schematically represents the input graph to EDUA ( see also item 1 in Fig-ure 5 ) , whose nodes consist of the segment vectors described in the preceding section , and whose edges connect segments from different summaries whose cosine similarity ?
t edge .
8 A candidate SCU is a fully connected subgraph ( clique ; item 2 in Figure 5 ) .
Every candidate SCU has an attraction score AS equal to the average of the edge scores ( item 3 in Figure 5 ) .
A candidate pyramid is a set of SCUs that constitute a covering of all the sentences in the input reference summaries , with all segments for a given sentence coming from only one of its segmentations .
The SCU weights for a pyramid , which are in [ 1 , n] for n reference summaries , form a partition over its segments , and each equivalence class ( all SCUs of the same weight ) has a score AC that is the average of its SCU scores ( item 4 in Figure 5 ) .
The score for a candidate pyramid AP is the sum of its AC scores ( item 5 in Figure 5 ) .
We use the sum rather than the average for AP to favor the equivalence classes for higher weight SCUs .
The optimal pyramid maximizes AP ( item 6 in Figure 5 ) .
EDUA has two implementations .
EDUA - C implements a complete solution based on depth first search ( DFS ) of candidate SCUs that guarantees the global optimum ( max AP ) .
EDUA -G is a greedy approximation .
9
EDUA -C EDUA -C constructs an adjacency list that for each clique ( candidate SCU ) in the input graph , identifies all the other SCUs that satisfy two constraints : 1 ) for any given sentence , all SCUs reference the same segmentation ; 2 ) all segments in all SCUs are distinct .
DFS search proceeds through the adjacency list , ordering the SCUs by weight , until a path is found through all SCUs that meets the constraints .
The solution has the highest AP , or in the case of ties , the path found first .
Figure 6 illustrates a toy EDUA -C DFS tree .
Each node depicts a candidate SCU clique , labelled by the number of nodes in the clique ( SCU weight ) .
No child node has a higher weight than its parent nodes .
A child node is added to a search path ( solid nodes ) if it violates no constraints .
Each of the six paths in the figure would receive an AP score .
After DFS finds all legal paths , the one with highest AP is selected as the solution .
Comparison of EDUA variants
Table 2 compares the distribution of SCUs by weight of the two EDUA variants with manual pyramids on the student summary dataset discussed in the next section .
EDUA - C produces a more skewed distribution than EDUA -G .
Both variants suffer from the coarse-grained segmentation output from the decomposition parser , but EDUA -G compensates by enforcing the Zipfian distribution observed in most pyramids ( see appendix A for details ) .
To evaluate speed , we tested both variants on datasets with different numbers and lengths of reference summaries .
TAC 2010 reference summaries ( 4 per topic ) have on average 46 segments each , and 321 candidate SCUs .
Pyramid construction for TAC 2010 takes less than 10 seconds with either variant on an Ubuntu machine with 4 Intel i5- 6600 CPUs .
EDUA - G's greater efficiency is more apparent for larger input .
DUC 2005 has seven reference summaries per topic , and longer summaries than in TAC 2010 ; on five , EDUA - C takes 211 seconds , while EDUA - G is still only about ten seconds ; on six , EDUA - C takes 20 minutes , compared to 5 minutes for EDUA -G .
WMIN Scoring For automatic matching of phrases in evaluation summaries to SCUs in a manual pyramid , Passonneau et al . ( 2018 ) found good performance with WMIN ( Sakai et al. , 2003 ) , a greedy maximum weighted independent set algorithm .
Because EDUA pyramids are analogous to manual pyramids , PyrEval also uses WMIN .
The input to WMIN is a graph where each node is a tuple of a segmentation of an ESUM sentence with the sets of SCUs that give the highest average cosine similarity for that sentence .
The node weight is the sum of SCU weights .
Graph edges enforce constraints that only one segmentation for a sentence can be selected , and each pyramid SCU can be matched to an ESUM sentence at most once .
WMIN selects the nodes that result in the maximum sum of SCU weights for the ESUM .
Score computation is a function of the matched SCUs , as illustrated by the ESUM in the lower right of Figure 2 .
This ESUM has five SCUS : two of weight 5 , one of weight 4 , one of weight 2 , and one that does not match the pyramid ( zero weight ) .
The sum of SCU weights is 16 .
The original pyramid score , a precision analog , normalizes the raw sum by the maximum sum for the same SCU count given by the pyramid -( 3 ? 5 ) + ( 2 ? 4 ) - indicating the degree to which the summary SCUs are as high weighted as possible .
Following ( Passonneau et al. , 2018 ) , we use the term quality score .
The average number of SCUs in the reference summaries is 15 , whose maximum weight from this pyramid is 53 .
Normalizing the raw sum by 53 gives a coverage score of 0.30 ( a recall analog ) .
The harmonic mean of these scores gives an F score analog referred to as a comprehensive score .
Student Summaries
As part of a collaboration with a researcher in educational technology , we collected a new data set of student summaries that were assigned in fall 2018 to computer science freshman in a university in the United Kingdom ( Gao et al. , 2019 ) .
Our immediate goal is to see how PyrEval could support instructors who assign summaries by providing an automated assessment that could be later corrected , but which provides scores and score justifications .
PyrEval scores correlate well with manual pyramid scores on content , and the log output it produces provides a clear trace of score computation ( see below ) .
The class was an academic skills class that included instruction in academic reading and writing .
For one assignment , they were instructed to select one of two current technology topics ( three readings per topic ) , then to summarize it in 150 to 250 to words .
The two topics are shown below , with the number of student summaries per reading , and average number of words .
1 . Autonomous Vehicles ( AV ) : 42 summaries , average words = 237.76 2 . Cryptocurrency ( CC ) : 37 summaries , average words = 245.84
To write reference summaries for both topics , the instructor recruited advanced students who had done well in her academic skills class in previous years .
Three trained annotators applied manual pyramid annotation to the student summaries .
As noted in section 2 , pyramid annotation is highly reliable .
Annotations of the student summaries were performed in two passes by different annotators .
Table 3 reports the correlation between the manual pyramid scores and the PyrEval scores on the two sets of student summaries .
For both AV and CC , EDUA -G performs better than EDUA -C and ROUGE - 2 , the best ROUGE variant on TAC10 ( see below ) , and ROUGE - 2 performs better than EDUA -C .
We attribute the lower correlations on the quality score , and the lower performance on this dataset compared to WIM ( see Table 1 ) , to the greater challenges of the new dataset .
WIM students read a single , middle school text , and average summmary length was 109.02 words .
For the new dataset , students read three advanced texts , and produced summaries that were over twice the length ( see above ) .
Error analysis shows complex sentence structure for the AV and CC data , with many constructions such as conjunctions and lists , that the decomposition parser cannot handle .
As noted above , EDUA -G compensates due to a Zipfian constraint on the pyramid shape .
Figure 1 compares a PyrEval SCU with a manual one for the cryptocurrency topic , and Single PyrEval SCU ( W=3 ) about the relation of " car accidents " to " insurance cost " RSUM1 Also , as most collisions are due to human error , costs of insurance for self driving cars could fall by up to < NUM >.
RSUM2
The cars themselves would also reduce insurance premiums ; < NUM > percent of road accidents are caused by human error RSUM3 Shankleman does well to balance out the positives such as lower insurance , reduced traffic , savings on mechanical costs and lower chance of road accidents .
Single manual SCU ( W=4 ) on " high car accidents " Single manual SCU ( W=4 ) on " lower insurance " RSUM1 Also , as most collisions are due to human costs of insurance for self- driving cars could fall by error up to 50 % RSUM2 90 percent of road accidents are caused by human
The cars themselves would also reduce insurance error premiums RSUM3 . . . lower chance of road accidents . . . . . . lower insurance . . . RSUM4 . . . he claims that over 90 percent of road traffic this would result in lower insurance premium for accidents occur as a result of human error owners of autonomous vehicles by up to 50 percent .
Student IDs Segments correctly matching this PyrEval SCU to students ' summaries ( from PyrEval log output ) A
The insurance industry is also going to experience great changes as the director insurer of AXA SA explains that more than < NUM > percent of road accidents are caused by human error .
B as < NUM > of the accidents are caused by human errors , also reducing the number of human drivers will contribute to cheap insurance premiums and efficient transport C Shankleman explains how problems with modern day transport such as high crash statistics and extortionate insurance costs will be eradicated with such computing capabilities .
also illustrates issues that might explain the relatively poorer performance of ROUGE .
We show a phrase that both the manual annotator and PyrEval matched to the SCU from one of the student summaries , where the student used near synonyms for terms in the articles and reference summaries : craftmanship exhibition for art gallery , and inn for hotel .
ROUGE cannot match synonyms , and does not distinguish differences in content importance .
Figure 7 shows an excerpt from PyrEval 's log output on autonomous vehicle to illustrate the alignment of an SCU to three student summaries and comparison to two manual SCUs .
10 The PyrEval SCU captures a causal relation between " car accidents due to human error " and " lower insurance costs . "
The two manual SCUs , however , show that the human annotators split this content into two SCUs , because the content is expressed in distinct clauses in RSUM1 and RSUM2 .
The same content is supported by the implicit contexts for the shorter RSUM3 contributing phrases .
The RSUM4 contributor in the manual SCU about " lower insurance " illustrates another issue that PyrEval preprocessing cannot handle : resolution of the deictic pronoun subject in " this would result . . . " .
10 Preprocessing replaces numeric character strings with tags .
TAC 2010 Summaries NIST summarization challenges dealt exclusively with news , which is also the most prevalent genre for automated summarizers in our survey of 2013 - 2018 ACL publications ( 23/39 summarizers ; see above ) .
To evaluate ROUGE , NIST used two human gold standards in yearly challenges from 2005 through 2011 , one of which was manual pyramid .
Annotation was performed by volunteers among the challenge participants , using guidelines developed for DUC 2006 .
11
In this section , we apply a method NIST helped develop to evaluate ROUGE against manual pyramid in an evaluation of PyrEval against manual pyramid .
We selected TAC 2010 because summarizer performance was less good in the earlier years .
TAC 2010 had two 100 - word summarization tasks on 10 documents for 46 topics .
Task A summarization was guided by a query .
Task B was an update to A , based on additional input .
On inspection of the 92 pyramids ( 46 each for Tasks A and B ) , we found that roughly 27 % had poor quality pyramids that did not follow the guidelines mentioned above .
We assembled a team of five people familiar with manual pyramid to manually redo the twelve pyramids that were independently identified as the lowest quality .
12 Tests of the correlation of human scores as- signed to automated summaries with ROUGE ( and other automated metrics ) were found to be unreliable , because of high score variance resulting as much from properties of the input texts as from differences in summarization systems ( Nenkova , 2005 ; Nenkova and Louis , 2008 ) .
Analyses of over a decade of NIST data from automated summarizers that evaluate ROUGE against manual pyramid and another manual score led to a solution to this problem ( Rankel et al. , 2013 ; Owczarzak et al. , 2012a , b ; Rankel et al. , 2011 ) .
The solution is to use Wilcoxon signed rank tests , so that pairs of systems are compared on matched input in a way that tests for statistical significance .
The outcome is either that one of the systems is significantly better than the other , or that the difference between them is not statistically significant .
To determine if the automated metric accurately reflects the gold standard scores , the same Wilcoxon tests are performed using the manually assigned scores on all pairs of systems , matching each pair on the same inputs .
A given automated metric is then compared to the human gold standard to determine how accurately the automated metric leads to the same set of significant differences between all pairs of systems .
Table 4 presents bootstrapped accuracy results for ROUGE and PyrEval using 41 topics per bootstrap sample , along with absolute accuracy on all 46 topics .
Each selection of 41 topics gives a gold standard set of system differences against which to compare a given metric .
ROUGE 2 has the highest average accuracy on both Task A and B. ROUGE 1 performs nearly as well on Task A. PyrEval performs less well on average accuracy for all tasks , but similarly to ROUGE 1 in Task B. ROUGE - 2 has greater sensitivity to topics , as shown by the higher deltas between the bootstrapped accuracy on 41 topics versus the accuracy on all 46 .
The differences in Table 4 between the bootstrapped summaries .
12
We plan to ask NIST if we can make this data available through them .
averages across 41 topics , and the accuracy scores on all 46 topics , confirms the sensitivity of evaluation results to topics noted in ( Nenkova , 2005 ; Nenkova and Louis , 2008 ) .
Conclusion PyrEval outperforms previous automated pyramid methods in accuracy , efficiency , score normalization , and interpretability .
It correlates with manual pyramid better than ROUGE on a new dataset of student summaries , and produces output that helps justify the scores ( similar to the examples for Figures 1 and 7 ) .
While it does not perform as well as ROUGE on extractive summarization , we speculate it would outperform ROUGE on abstractive summarizers .
It relies on EDUA , a novel restricted set partition algorithm , that expects semantic vectors of sentence segments as input .
The current rule- based method that identifies sentence substrings ( the decomposition parser ) is limited by the output of the constituency and dependency parsers it relies on .
We are currently working on a neural architecture that simultaneously identifies simple clauses and produces semantic representations that could provide better input for both EDUA and WMIN , and thus improve PyrEval .
