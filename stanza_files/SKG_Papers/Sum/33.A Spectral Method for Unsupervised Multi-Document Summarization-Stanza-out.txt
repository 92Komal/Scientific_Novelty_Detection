title
A Spectral Method for Unsupervised Multi-Document Summarization
abstract
Multi-document summarization ( MDS ) aims at producing a good-quality summary for several related documents .
In this paper , we propose a spectral - based hypothesis , which states that the goodness of summary candidate is closely linked to its so-called spectral impact .
Here spectral impact considers the perturbation to the dominant eigenvalue of affinity matrix when dropping the summary candidate from the document cluster .
The hypothesis is validated by three theoretical perspectives : semantic scaling , propagation dynamics and matrix perturbation .
According to the hypothesis , we formulate the MDS task as the combinatorial optimization of spectral impact and propose an accelerated greedy solution based on a surrogate of spectral impact .
The evaluation results on various datasets demonstrate : ( 1 )
The performance of the summary candidate is positively correlated with its spectral impact , which accords with our hypothesis ; ( 2 ) Our spectral - based method has a competitive result as compared to state - of - the - art MDS systems .
Introduction Given a cluster of documents related to the same topic or event , the task of multi-document summarization ( MDS ) centers on a brief summary of the cluster .
As emphasized by Lebanoff et al . ( 2018 ) , for this task , the labeled training data ( i.e. clustersummary pairs ) are scarce .
Hence dealing with it in an unsupervised paradigm becomes a reasonable choice .
For the unsupervised MDS task , the automatic summarizer is required to discover the main content of the document cluster without the guidance of golden summaries .
To preserve the fluency and grammaticality of summary , we mainly focus on the extractive method in which summary sentences are extracted from the original document cluster .
In this paper , we propose a novel spectral - based hypothesis for the unsupervised MDS task .
The hypothesis states that the goodness ( or effectiveness ) of any summary candidate is closely linked with its spectral impact on the document cluster .
The spectral impact of a summary candidate quantifies the perturbation to the dominant eigenvalue ( in modulus ) of affinity matrix when dropping the candidate from the document cluster .
In other words , the hypothesis points out the spectral impact as an indicator of the MDS task , which is the first attempt to characterize MDS from a spectral viewpoint explicitly .
As a representation of the document cluster , the affinity matrix supports the definition of spectral impact .
Adjusting the building of the affinity matrix can bring out the best in the hypothesis .
To validate the proposed hypothesis , we provide both theoretical explanations and empirical evidence .
Theoretically , the spectral impact caused by dropping a summary from the cluster can be characterized from three different perspectives ( see ?2.4 ) .
Empirically , for any summary candidate , the real dataset witnesses a positive correlation between its performance and computed spectral impact .
For a particular MDS task , applying the hypothesis leads to a constrained optimization problem where the objective function is spectral impact .
Our summarizer utilizes an accelerated greedy algorithm based on a surrogate of spectral impact .
The competitive results of our summarizer have been obtained on various datasets .
The differences between prior works and our method are clarified for unsupervised MDS : ( 1 ) Underlying hypothesis .
The hypothesis indicates the mechanism for the summarization .
For instance , manifold - ranking - based methods share the hypothesis that a good summary sentence has a high ranking on the low-dimensional manifold that documents reside in ( Wan et al. , 2007 ; Cheng et al. , 2011 ; . However , the reasonableness of this manifold hypothesis has not been directly evaluated .
Another hypothesis in the sparse-codingbased methods ( Li et al. , 2015 b ; Liu et al. , 2015 ; Yao et al. , 2015 ) regards the original sentences as a linear combination of summary sentences .
This leads to an intuitive reconstruction , whereas linear combination is more a simplification than a necessity .
Our proposed hypothesis offers a spectral viewpoint and will be explicitly validated on the real dataset .
( 2 ) Optimization objective .
Multi-criteria optimization is suitable for MDS as various criteria ( goals ) exist in the task , such as relevancy criterion and non-redundancy criterion .
For instance , Lin and Bilmes ( 2011 ) is a bi-criteria case that imposes the submodularity constraint on each criterion .
Multi-criteria loss functions in neuralnetwork - based methods ( Ma et al. , 2016 ; Chu and Liu , 2019 ; Zheng et al. , 2019 ) include the reconstruction errors from different spaces .
In the above cases , the overall objective functions used include some hyperparameters for gluing singletons .
Comparatively , our proposed objective ( spectral impact ) has a compact form .
It avoids the hyperparameter setting and simulates the non-separable processing of multiple MDS criteria by human beings .
( 3 ) Model complexity .
There is a trade- off between model complexity and model interpretability .
For instance , the reported performance of the aforementioned deep-neural - based models is elusive , and there exists no general principle to further improve them .
Our summarizer realizes the interpretable behavior based on verified hypothesis while preserving enough model complexity by the flexible affinity matrix ( as a plug-in ) .
Our main contributions are twofold : ( 1 ) A novel spectral - based hypothesis for unsupervised MDS , which gains support from both theoretical and empirical sides ; ( 2 ) An accelerated greedy algorithm for solving the hypothesis-driven optimization problem .
The rest of the paper is organized as follows .
?2 gives the details of our method , including the spectral - based MDS hypothesis and the greedy algorithm to solve the spectral optimization problem .
Evaluation results , related work and conclusions are covered in ?3 , ?4 and ?5 respectively .
Spectral- based MDS
What role does the summary play in the process of MDS ?
Our proposed hypothesis offers a spectral insight and brings out a workable formulation of MDS .
Notations
We use calligraphic fonts for sets , capital bold letters for matrices and lower - case bold letters for vectors .
The universal set C is formed by splitting and gathering the sentences from document cluster , i.e.
C = {s 1 , s 2 , ... , s n } ( s i represents the i-th sentence and n is the total number of sentences ) .
Each sentence has its ordinal number , e.g. o i of sentence s i indicates it is the ( o i ) - th sentence in the document that s i belongs to .
The summary candidate ( subset of C ) is denoted as S .
We represent the affinity matrix of document cluster as : A = { a ij } n?n .
In addition , the dominant eigenvalue ( in modulus ) and the corresponding eigenvector of A are denoted as ?( A ) and v , respectively .
Dropping a set from a matrix : emptying all the rows and columns whose indexes occur in the set .
Consider the operation of dropping S from A .
If we denote the operation itself and the resultant matrix as A\S , then A\S = 0 , s i ?
S or s j ?
S , a ij , otherwise .
Spectral Hypothesis
When representing the document cluster as a matrix , the matrix spectrum ( i.e. a collection of eigenvalues ) can uncover its different facets .
Note that the dominant eigenvalue especially corresponds to the key facet , which gives a clue as to the main content that the summarizer needs to discover .
For the extractive MDS , we propose the spectral - based hypothesis : GIVEN : Affinity matrix
A , the matrix representation of document cluster ; set S , any summary candidate including some original sentences .
DEFINITION : Spectral impact of S is the perturbation to dominant eigenvalue of A when dropping S from A , i.e. ?( S ) ?( A ) ? ( A\S ) .
HYPOTHESIS : Goodness of S as a summary has a close link with its spectral impact ?( S ) .
The above hypothesis tells us that the goodness ( effectiveness ) of any summary candidate can be determined by the proposed spectral impact , which reflects the change of dominant eigenvalue when the summary candidate S is left out .
Affinity matrix
A supports the definition of spectral impact , which
The hypothesis suggests using spectral impact to judge whether a summary candidate is good or not .
? ? Affinity Matrix stores the pairwise affinity of sentences as the name suggests .
For a specific MDS task , applying the hypothesis leads to an optimization formulation as follows : S * = arg max S?C ?( S ) , s.t. | S| k. ( 1 )
The set C denotes the universal set and the number k specifies the maximum capacity of candidate S .
The above formulation sets the spectral impact to be the objective function .
The inherent rationality can be verified partially by these properties : ( a ) monotonicity : ?( S 1 ) ?( S 2 ) for any S 1 ? S 2 ( see Li et al. , 2012 , Theorem 1 ) ; ( b ) normalization : ?(? ) = 0 , ?( C ) = ?( A ) ( ? denotes empty set ) .
In the context of MDS , property ( a ) points out that a whole summary has more goodness than its components and property ( b ) regulates a reasonable range of the goodness of any summary candidate .
An overview of our hypothesis can be found in Figure 2 .1 .
The document cluster with its matrix representation
A is depicted in the left part , while the right part gives the dropping operation and the computation of spectral impact for two summary candidates S p and S q .
The goodness of each candidate is judged by their spectral impacts , and the winner S * stands out with the largest spectral impact .
Notice that cardinality constraint is adopted in Problem ( 1 ) to specify the length limit of summary .
Other reasonable constraints are also available , such as the knapsack constraint and the nonuniform matroid constraint ( Welsh , 1976 ) .
The relevant conclusions and solutions discussed in the following sections continue to be applicable for those constraints .
Affinity Matrix
Many prior works have adopted A for the MDS task , such as Yang et al . ( 2018 ) and Yang et al . ( 2019 ) .
Each element in the affinity matrix A is a pairwise affinity of two different sentences .
Since our hypothesis depends on A , a better MDS performance can be expected by adjusting the building of A.
Sentence embeddings play a vital role in the process of building A , since affinity a ij can be set to be the cosine similarity of the embeddings of sentences s i and s j ( i.e. a ij = a ji and a ii = 0 ) .
For comparison purposes , we consider the following three strategies of building sentence embeddings .
Tf -isf : the simple tf - idf model with a finer granularity .
More details can be found in Wan et al . ( 2007 ) and Wang et al . ( 2017 ) . ESE : the enhanced feature embedding model ( Yang et al. , 2019 ) .
The embedding of each sentence is the concatenation of all components : paragraph vector , positional embedding and three feature embeddings ( namely word - part - of-speech , bigram and trigram ) .
BERT : the sentence encoder that learns vector representations by pre-training a deep bi-directional Transformer network ( Devlin et al. , 2019 ) .
The advantage is that BERT is context-sensitive when considering the word embedding .
Notice that the leading sentences in each document should have priority in the summary extraction .
For injecting this knowledge , a ij is multiplied by the average positional weight 1 / ( o i +o j ) .
This can differentiate the sentences across documents and preserve the symmetry of A .
Justifications of Hypothesis
We validate our spectral - based hypothesis by the following three complementary perspectives : Semantic scaling : dominant eigenvalue of affinity matrix determines the vector scaling in semantic space .
The n-dimensional semantic space is constructed as follows :
Each sentence in the document cluster represents a different dimension and the i-th axis of the space corresponds to the i-th sentence .
Then the affinity matrix A n?n can be seen as a linear operator on this semantic space and the pairwise affinity a ij regulates the transformation between the i-th axis and j-th axis .
Given an arbitrary nonzero vector x in the space , the transformed vector is Ax .
Then the property holds : || Ax || ?( A ) ||x||.
( 2 ) Notation || ? || denotes the Euclidean norm of vector .
This is a sharp bound as equality holds only if x is the dominant eigenvector of operator A .
The property shows that the scaling up of any vector ( namely | | Ax | |/||x|| ) is not larger than ?( A ) .
Hence , the dominant eigenvalue ?( A ) characterizes the ability of operator A to scale up any vector in the semantic space that document cluster resides in .
When dropping the summary candidate S , the transformations to and from all axes covered by S will no longer exist for operator A .
In other words , there is no contribution for scaling up vectors from the i-th axis for any sentence s i ?
S. When the bestquality summary is dropped , the main components of operator A are emptied , which causes the largest reduction of its ability to scale up vectors .
Therefore , the dominant eigenvalue , indicator of this ability , can be used to locate the multi-document summary , as proposed in our hypothesis .
Propagation dynamics : isolating the summary blocks the information dynamics .
In this perspective , there is a spread of information over the document cluster according to the underlying network specified by matrix A .
The pairwise affinity a ij indicates the propagation rate between sentences s i and s j ( more similar they are , more rapid the propagation occurs ) .
The question that arises here is whether the information propagated from a few seed sentences will form a pandemic or become extinct in the long term .
In epidemiology , the virus ( information ) will form a pandemic only if the basic reproduction number R 0 of this virus is larger than 1 ( Jones , 2007 ) .
For instance , R 0 of COVID -19 is about 3.28 ( > 1 ) ( Liu et al. , 2020 ) , which uncovers the inevitable propagation of this virus .
Many works ( Wang et al. , 2003 ; Prakash et al. , 2012 ; Chen , 2018 ) have found out that R 0 is proportional to the dominant eigenvalue of the underlying information network .
Thus a small dominant eigenvalue corresponds to a small value of R 0 , which hinders the information propagation .
For the MDS task , when isolating the best-quality summary ( i.e. A\S ) , the remainder of the document cluster will become the hardest for information propagation .
Our hypothesis is consistent with this finding as the summary S found by solving Problem ( 1 ) is able to reduce ?( A\S ) the most .
Matrix perturbation : spectral impact considers both the relevancy and non-redundancy goal of MDS .
For analyzing the behavior of spectral impact , we expand it using first-order matrix perturbation theory ( Stewart , 1990 ) as follows : ?( S ) = u Eu + O ( ||E || 2 ) = 2 s i ?S ui 2 ?( A ) ?
s i , s j ?S uiaijuj + O ( | | E | | 2 ) , ( 3 ) where E = A ? A\S , Au = ?( A ) u , ||u|| = 1 . Let us analyze each term of the expansion shown in Eq. ( 3 ) .
The first sums up the score of 2 u i 2 ?( A ) for any sentence s i ?
S . The value of u i is a measure for the relevancy of sentence s i , since eigenvector centrality has been typically used for ranking sentences ( Erkan and Radev , 2004 ; Bellaachia and Al - Dhelaan , 2014 ; Al - Dhelaan , 2015 ) .
Hence , the first term is an indicator for the relevancy of summary S.
The second term is a penalty that considers every pair of summary sentences .
Specifically , the penalty is u i a ij u j for sentences s i and s j .
When a ij is large ( sentences are redundant ) , the penalty becomes prominent .
Thus the second term measures the non-redundancy of summary S. The third term O ( | | E | | 2 ) is relatively small compared to the preceding two because matrix E is nearly dominated by zeros ( | S| << n ) .
Hence the third term will not change the main behavior of ?( S ) .
Algorithm
The naive idea for solving Problem ( 1 ) is to enumerate all possible combinations and find the best summary .
The time complexity is n k n 2 if it takes O(n 2 ) time to compute the dominant eigenvalue of matrix ( say using the method proposed by Lanczos ( 1950 ) ) .
This exact enumeration algorithm is infeasible even when n is 500 and k is 5 .
Theoretically , Problem ( 1 ) falls into spectral optimization that has been proved to be NP - hard in many cases ( Van Mieghem et al. , 2011 ) .
To avoid the time - consuming eigen- decomposition , some works resort to the QR decomposition of matrix ( Li et al. , 2015a ; Chen et al. , 2018 ) .
However , to actually compute QR decomposition , they depend on the Gram-Schmidt process that is inherently numerically unstable , which impedes the optimization process .
In this paper , we bypass all these matrix decomposition and propose a straightforward surrogate for spectral impact , which is both effective and efficient .
Based on a bound for dominant eigenvalue of A\S ( Theorem 2.14 in Stevanovic ( 2014 ) ) , the surrogate is proposed as follows :
Surrogate of spectral impact : ?( S ) ? s i ?S v 2 i ? ? s i , s j ?S v i a ij v j s i ?
C v 2 i ? s i ?S v 2 i where ? is the dominant eigenvalue ?( A ) and v 1 is its corresponding eigenvector of matrix A .
By using the surrogate for acceleration , we consider a greedy strategy to iteratively select S , which is listed in Alg. 2.1 .
First , we compute the dominant eigenvalue ? and eigenvector v of A ( line 1 ) .
At each iteration ( lines 3 to 7 ) , the sentence s ? maximizing the marginal gain of ?( S ) is extracted based on the previously selected set S ( i.e. maximizing ?( S ?{s j } ) ?( S ) , line 4 ) and added to S ( line 5 ) .
Also , the auxiliary vector w and scalar x should be updated according to the numerator and denominator of the surrogate ( lines 6 , 7 ) .
The operator ' ' and ' ? ' , for any two vectors , are their Hadamard product and inner product , respectively .
The lemma below demonstrates that Alg. 2.1 has a quadratic time complexity , which is evidently better than the exponential one of naive enumeration .
Lemma 2.1 .
The time complexity of Alg. 2.1 is O( n 2 + kn ) .
Proof .
Computing the dominant eigen- pair of matrix A takes O( n 2 ) time .
The initializations of the vector w and scalar x are both linear time operations , i.e. O( n ) .
At each iteration , all n sentences 1
The eigenvector v can be of arbitrary length , which differs from the normalized vector u in Eq. ( 3 ) .
Update : w j w j ?2 v j a j? v ? for all s j / ?
S ; 7 Update : x x ? v 2 ? ; 8 return S need to be traversed for extracting s ? and updating w .
Thus the total complexity is O( n 2 + kn ) .
To get the final summary , we reorder the summary sentences returned by Alg. 2.1 according to their positions in the corresponding document .
Experiments
Datasets
Three datasets are selected in the following experiments to provide a complete evaluation of our method .
Two domains have been taken into account : news ( DUC and Multi- News ) and business reviews ( Yelp ) .
Table 3 .1 lists some key characteristics of these datasets .
DUC 2004 2 ( task 2 ) : the DUC task that contains a benchmark dataset .
There are 50 document clusters , each of which includes 10 documents about the same news event .
In addition , four human-written summaries are offered for each cluster to be the reference ( golden ) summary .
Yelp 3 : an all- purpose dataset that can be utilized for MDS .
We only use the subset that has the reference summary ( the test split offered by Chu and Liu ( 2019 ) ) : 100 businesses ( document clusters ) , each of which includes 8 reviews ( documents ) .
One reference summary was collected for each cluster using crowdsourcing .
More details of building the dataset can be found in Chu and Liu ( 2019 ) . Multi-News 4 : a large-scale dataset collected from news aggregator ( Fabbri et al. , 2019 ) .
It has 5622 document clusters ( in the test split offered by the original paper ) , and multiple documents are present in each cluster .
Furthermore , each cluster is attached with one human-written reference summary .
Experimental Details
For the extractive MDS method ( including ours ) , the pre-processing includes paragraph splitting , sentence splitting and word tokenization .
In our method , all the splitted sentences are gathered in set C .
The input of Alg. 2.1 includes the affinity matrix A which is built according to the strategies stated in ?2.3 .
Specifically , the strategy utilizing tf -isf vectors has a word bag that contains all the stemmed words found in the dataset ( word stemming using Porter 's stemmer 5 ) .
For the strategy ESE , we pre-trained all different sentence embeddings on Daily Mail dataset ( Hermann et al. , 2015 ) by following the guideline of Yang et al . ( 2019 ) ( the dimension of concatenated embedding is 800 ) .
For the strategy BERT , we used the uncased BERT - Base model 6 pre-trained on Wikipedia , through bert - as-service 7 to obtain the sentence embedding of 768 dimensions .
All the experiments are performed on a machine with two CPUs ( 3.5 GHz ) and one GPU ( 16G memory ) .
The extractive MDS methods need a length limit of summary to terminate the extraction of summary sentences .
We adopt 100 words as the length limit in the DUC dataset , instead of 665 bytes specified by the official task .
The change has also been made to provide the same setting for evaluating various methods in Hong et al . ( 2014 ) and Zheng et al . ( 2019 ) .
For the Yelp dataset , we set the limit to be the 99.5 th percentile less than the maximum length of any document ; for Multi- News , the limit is set as 300 words .
The same settings have been adopted in Chu and Liu ( 2019 ) and Fabbri et al . ( 2019 ) , respectively .
Evaluation Metrics
We adopt ROUGE ( Lin , 2004 ) as the automatic metric , which has been observed in a good agreement with human judgment ( Owczarzak et al. , 2012 ) .
It measures the overlap of N - grams ( R - N ) and skipbigrams with a maximum distance of four words ( R - SU4 ) .
Also , it can be computed based on the longest common subsequence ( R - L ) .
Each version of ROUGE has their scores oriented to recall , precision and F1 .
In the experiments , we report the different combinations of ROUGE scores for each dataset , which have been recommended and adopted by previous works .
Specifically , the recall scores of R -1,2,4 will be reported for the DUC 2004 dataset according to Hong et al . ( 2014 ) , Wang et al. ( 2017 ) and Zheng et al . ( 2019 ) ; the F1 scores of R -1,2 , L will be reported for Yelp as in Chu and Liu ( 2019 ) ; the F1 scores of R -1,2 , SU4 will be reported for Multi-News as in Fabbri et al . ( 2019 ) .
The toolkit for computing ROUGE metrics is ROUGE - 1.5.5 8 and its option is set to be '-m -c 95 -r 1000 -f A -p 0.5 -t 0 ' .
Comparing Methods
We compare our method with both traditional and state - of- the - art MDS methods .
Lead :
The documents in a cluster are randomly shuffled , and the first sentence of the document is added to the summary until the length limit is reached .
LexRank ( Erkan and Radev , 2004 ) :
It performs the sentence relevancy estimation by the random walk process on the sentence graph .
CLASSY04 ( Conroy et al. , 2004 ) :
It ranked first in the official evaluation of DUC 2004 .
As a supervised method , it uses a Hidden Markov Model to rank sentences and a QR decomposition to produce the summary .
C-Attention ( Li et al. , 2017a ) :
The cascaded attention based auto-encoder is proposed for estimating the relevancy of words and sentences .
GRU -GCN ( Yasunaga et al. , 2017 ) :
It is a supervised method that employs a Graph Convolutional Network on sentence graph .
The sentence embedding obtained from a Recurrent Neural Network serves as the input node feature .
ParaFuse ( Nayeem et al. , 2018 ) : MDS is formulated as multi-sentence compression .
As the state- of- the- art on DUC 2004 , however , it needs some extra resource and toolkit , such as paraphrase bank and keyword extractor .
Best Review ( Chu and Liu , 2019 ) :
A simple baseline selecting the best document to be summary based on word overlap .
Centroid ( Rossiello et al. , 2017 ) :
Word embeddings are exploited to boost the performance of centroid-based methods .
MeanSum ( Chu and Liu , 2019 ) :
An end-to - end neural model is put forward to implement the abstractive summarization of business review documents .
The summary is decoded from the mean of the representations of input reviews .
PG ( See et al. , 2017 ) :
It introduces a pointergenerator ( PG ) network that motivates the summarizer to copy original words from input via pointing , while preserving the ability to generate new words .
Hi-MAP ( Fabbri et al. , 2019 ) :
It proposes the integration of sentence - level MMR scores into the PG network in order to adapt the attention weights on a word-level .
The MMR score is computed by the Maximal Marginal Relevance algorithm ( Carbonell and Goldstein , 1998 ) , which gives the goodness of the available sentence given already selected ones .
Our method Spectral :
This is our spectral - based method specified in Alg. 2.1 .
According to different building strategies in ?2.3 , it has three versions : Spectral-tfisf , Spectral -ESE and Spectral - BERT .
Notice that on DUC 2004 , the supervised methods are trained on datasets of earlier DUC evaluations or CNN and Daily Mail datasets ( Hermann et al. , 2015 ) according to their original papers .
On Multi-News , they are both trained and tested on the dataset itself .
Main Results
Table 3 .2 demonstrates the ROUGE results of various methods on the DUC dataset .
The method Para- Fuse is previously state - of - the - art on this dataset .
From the table , our method Spectral - BERT outperforms ParaFuse by 1.2 percent in R - 1 score and has a slightly lower R - 2 and R - 4 score ( still ranking second ) .
Notice that ParaFuse , as mentioned in ?3.4 , is not exactly a self-contained system .
Compared with CLASSY04 ( winner of the official evaluation ) , Spectral - BERT has an enormous advantage ( say 3.7 % and 2.7 % higher R - 1 and R - 2 score , respectively ) .
Notice that all supervised methods have a relatively low performance , since they are trained on datasets different from DUC 2004 .
This ob- servation is consistent with the results reported in Fabbri et al . ( 2019 ) .
Table 3 .3 and Table 3 .4 show ROUGE results on Yelp and Multi-News , respectively .
Our method Spectral - BERT has also beaten other unsupervised methods by a wide margin ( say 1.3 % higher R-L score than MeanSum and 2.1 % higher R - SU4 score than C-Attention ) .
Compared with the state-ofthe - art supervised system on Multi-News ( namely Hi- MAP ) , Spectral - BERT cannot rival its performance .
However , Spectral - BERT has beaten the other supervised system ( i.e. PG ) according to R - 2 and R - SU4 score .
We observe that a better matrix building strategy ( stated in ?2.3 ) has led to a considerable improvement of our method on all three datasets .
Specifically , the BERT encoder brings about one percent improvement in R - 2 score as compared with the tfisf model .
It proves that our method is flexible and can benefit from recent off - the-shelf pre-training techniques ( Devlin et al. , 2019 ; Clark et al. , 2019 ) .
Linguistic Quality
To further assess the linguistic quality of different summaries , we employ Amazon Mechanical Turk 9 workers to judge the performance of three summarizers on a random sample of Multi-News ( 200 document clusters ) .
A worker is asked to rate each summary on a scale of 1 ( poor ) to 5 ( excellent ) according to three criteria : relevancy ( does the summary cover all the key information of document cluster ? ) and two criteria adopted by DUC 2005 evaluation ( non- redundancy and grammaticality ) ( Dang , 2005 ) .
Table 3 .5 shows the results .
We observe that our method Spectral has the highest relevancy and non-redundancy score .
Abstractive method C-Attention has a relatively low score of grammaticality .
Notice that the non-redundancy scores of all summarizers are generally low , which shows that humans are more sensitive to the redundancy existing in the summary .
Table 3 .5 : Linguistic quality on Multi-News .
Hypothesis Validation
We provide the empirical evidence of our proposed spectral - based hypothesis .
For each cluster of documents on DUC 2004 , we construct a sample set of 500 summary candidates S , each of which contains 3 original sentences selected randomly from the documents .
The Pearson correlation coefficient 9 https://www.mturk.com/ ( denoted by r ) of spectral impact and the candidate goodness as a summary , when applied to the sample set , is computed and the derived histogram is shown in Figure 3 .1 .
Each correlation coefficient falls into their corresponding bins .
In the figure , our method Spectral - BERT and the ROUGE metrics are utilized because : ( 1 ) Spectral - BERT has been reported with a better empirical performance in ?3.5 ; ( 2 )
The goodness of summary candidate in this scenario can be measured by the precisionoriented ROUGE scores , esp. R - 1 and R - 2 , in that the word count of candidate S is varied in the sample .
We note that there are no bins corresponding to negative correlation coefficients ( r ranges from - 1 to 1 ) , and quite a few r's have a large score beyond 0.5 ( the widely accepted threshold of a large r recommended by Cohen ( 2013 ) ) .
This demonstrates that the two variables have a positive linear correlation , which supports our hypothesis .
Similar results can be obtained when S contains a different number of sentences .
Related Work Unsupervised MDS .
There are a bunch of works working on different hypotheses and models in this field .
PageRank alike algorithms ( Erkan and Radev , 2004 ; Mei et al. , 2010 ; Wang et al. , 2017 ) utilize random walks with some redundancy avoiding measures .
Regarding the document cluster as a manifold structure , ( Wan et al. , 2007 ; Cheng et al. , 2011 ; use the manifold ranking process on data .
There are also quite a few neural architecture based models for a hidden semantic representation of sentences , documents or subtopics , such as ( Ma et al. , 2016 ; Li et al. , 2017 a , b ; Zheng et al. , 2019 ) . Spectral optimization .
Optimizing eigen-related metrics often leads to a specific collective optimization problem , which is believed to be hard in nature unless P = NP ( Cook , 1971 ) .
Some particular examples ( Van Mieghem et al. , 2011 ; Chen et al. , 2016 ) have been proved NP - hard .
The typical solvers adopt the heuristics based on either perturbation theory ( Chen et al. , 2016 ) or QR decomposition ( Li et al. , 2015a ; Chen et al. , 2018 ) .
Conclusion
We propose a novel hypothesis-driven method for unsupervised MDS , where the goodness of any summary candidate can be determined from a spectral perspective when dropping it from the document cluster .
Various MDS tasks of different sizes and domains show a promising result of our method .
Extending our method to an abstractive setting is meaningful future work .
Figure 2 . 1 : 21 Figure 2.1 : An example depicting the spectral - based hypothesis for the task of unsupervised MDS ( n = 5 , k = 2 ) .
The hypothesis suggests using spectral impact to judge whether a summary candidate is good or not .
