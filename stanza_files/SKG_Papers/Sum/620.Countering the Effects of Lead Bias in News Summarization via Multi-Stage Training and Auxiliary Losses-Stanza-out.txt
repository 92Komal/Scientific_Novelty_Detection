title
Countering the Effects of Lead Bias in News Summarization via Multi-Stage Training and Auxiliary Losses
abstract
Sentence position is a strong feature for news summarization , since the lead often ( but not always ) summarizes the key points of the article .
In this paper , we show that recent neural systems excessively exploit this trend , which although powerful for many inputs , is also detrimental when summarizing documents where important content should be extracted from later parts of the article .
We propose two techniques to make systems sensitive to the importance of content in different parts of the article .
The first technique employs ' unbiased ' data ; i.e. , randomly shuffled sentences of the source document , to pretrain the model .
The second technique uses an auxiliary ROUGEbased loss that encourages the model to distribute importance scores throughout a document by mimicking sentence - level ROUGE scores on the training data .
We show that these techniques significantly improve the performance of a competitive reinforcement learning based extractive system , with the auxiliary loss being more powerful than pretraining .
Introduction Extractive summarization remains a simple and fast approach to produce summaries which are grammatical and accurately represent the source text .
In the news domain , these systems are able to use a dominant signal : the position of a sentence in the source document .
Due to journalistic conventions which place important information early in the articles , the lead sentences often contain key information .
In this paper , we explore how systems can look beyond this simple trend .
Naturally , automatic systems have all along exploited position cues in news as key indicators of important content ( Schiffman et al. , 2002 ; Hong and Nenkova , 2014 ; Liu , 2019 ) .
The ' lead ' base - * Equal contribution .
Lead - 3 : Bangladesh beat fellow World Cup quarterfinalists Pakistan by 79 runs in the first one - day international in Dhaka .
Tamim Iqbal and Mushfiqur Rahim scored centuries as Bangladesh made 329 for six and Pakistan could only muster 250 in reply .
Pakistan will have the chance to level the three - match series on Sunday when the second odi takes place in Mirpur .
Reference : Bangladesh beat fellow World Cup quarterfinalists Pakistan by 79 runs .
Tamim Iqbal and Mushfiqur Rahim scored centuries for Bangladesh .
Bangladesh made 329 for six and Pakistan could only muster 250 in reply .
Pakistan will have the chance to level the threematch series on Sunday .
Lead - 3 : Standing up for what you believe .
What does it cost you ?
What do you gain ?
Reference : Indiana town 's Memories Pizza is shut down after online threat .
Its owners say they 'd refuse to cater a same - sex couple 's wedding .
line is rather strong in single-document news summarization ( Brandow et al. , 1995 ; Nenkova , 2005 ) , with automatic systems only modestly improving the results .
Nevertheless , more than 20 - 30 % of summary - worthy sentences come from the second half of news documents ( Nallapati et al. , 2016 ; Kedzie et al. , 2018 ) , and the lead baseline , as shown in Table 1 , does not always produce convincing summaries .
So , systems must balance the position bias with representations of the semantic content throughout the document .
Alas , preliminary studies ( Kedzie et al. , 2018 ) suggest that even the most recent neural methods predominantly pick sentences from the lead , and that their content selection performance drops greatly when the position cues are withheld .
In this paper , we verify that sentence position and lead bias dominate the learning signal for state - of - the - art neural extractive summarizers in the news domain .
We then present techniques to improve content selection in the face of this bias .
The first technique makes use of ' unbiased data ' created by permuting the order of sentences in the training articles .
We use this shuffled dataset for pre-training , followed by training on the original ( unshuffled ) articles .
The second method introduces an auxiliary loss which encourages the model 's scores for sentences to mimic an estimated score distribution over the sentences , the latter computed using ROUGE overlap with the gold standard .
We implement these techniques for two recent reinforcement learning based systems , RNES ( Wu and Hu , 2018 ) and BanditSum ( Dong et al. , 2018 ) , and evaluate them on the CNN / Daily Mail dataset ( Hermann et al. , 2015 ) .
We find that our auxiliary loss achieves significantly better ROUGE scores compared to the base systems , and that the improvement is even more pronounced when the true best sentences appear later in the article .
On the other hand , the pretraining approach produces mixed results .
We also confirm that when summary - worthy sentences appear late , there is a large performance discrepancy between the oracle summary and state - of - the - art summarizers , indicating that learning to balance lead bias with other features of news text is a noteworthy issue to tackle .
Related Work Modern summarization methods for news are typically based on neural network - based sequenceto-sequence learning ( Kalchbrenner et al. , 2014 ; Kim , 2014 ; Chung et al. , 2014 ; Yin and Pei , 2015 ; Cao et al. , 2015 ; Cheng and Lapata , 2016 ; Nallapati et al. , 2017 ; Narayan et al. , 2018a ; .
In MLE - based training , extractive summarizers are trained with gradient ascent to maximize the likelihood of heuristically - generated groundtruth binary labels ( Nallapati et al. , 2017 ) .
Many MLE - based models do not perform as well as their reinforcement learning - based ( RL ) competitors that directly optimize ROUGE ( Paulus et al. , 2018 ; Narayan et al. , 2018 b ; Dong et al. , 2018 ; Wu and Hu , 2018 ) .
As RL - based models represent the state of the art for extractive summarization , we analyze them in this paper .
The closest work to ours is a recent study by Kedzie et al . ( 2018 ) which showed that MLEbased models learn a significant bias for selecting early sentences when trained on news articles as opposed to other domains .
As much as 58 % of selected summary sentences come directly from the lead .
Moreover , when these models are trained on articles whose sentences are randomly shuffled , the performance drops considerably for news domain only .
While this drop could be due to the destruction of position cues , it may also arise because the article 's coherence and context were lost .
In this paper , we employ finer control on the distortion of sentence position , coherence , and context , and confirm that performance drops are mainly due to the lack of position cues .
We also propose the first techniques to counter the effects of lead bias in neural extractive systems .
Base Models for Extractive Summarization
In supervised systems , given a document D = {s 1 , ... , s n } with n sentences , a summary can be seen as set of binary labels y 1 , . . . , y n ? { 0 , 1 } , where y i = 1 indicates that the i-th sentence is included in the summary .
We choose to experiment with two state- ofthe- art RL - based extractive models : RNES ( Wu and Hu , 2018 ) and BanditSum ( Dong et al. , 2018 ) .
Both employ an encoder-decoder structure , where the encoder extracts sentence features into fixed - dimensional vector representations h 1 , . . . , h n , and a decoder produces the labels y 1 , . . . , y n based on these sentence representations .
RNES uses a CNN + bi-GRU encoder , and BanditSum a hierarchical bi-LSTM .
RNES 's decoder is auto-regressive , meaning it predicts the current sentence 's label based on decisions made on previous sentences ; i.e. , y t = f ( D , h t , y 1:t? 1 ) .
In BanditSum , there is no such dependence : it produces affinity scores for each sentence and the top scoring sentences are then selected .
Lead Bias of News Systems First , we investigate the impact of sentence position on our models .
We manipulate the original CNN / Daily Mail dataset to preserve sentence position information at different levels .
In the random setting , sentences are shuffled randomly ; in reverse , they are in reverse order ; in insert-lead and insert-lead3 , we insert an out- of- document sentence ( chosen randomly from the corpus ) as the first sentence or randomly as one of the first three sentences , respectively .
In worse when tested on a mismatched data perturbation .
Even when the distortion is at a single lead position in insert-lead and insert-lead3 , the performance on the original data is significantly lower than when trained without the distortion .
These results corroborate Kedzie et al . ( 2018 ) 's findings for RL - based systems .
Interestingly , the random model has the best mean performance and the lowest variation indicating that completely removing the position bias may allow a model to focus on learning robust sentence semantics .
Learning to Counter Position Bias
We present two methods which encourage models to locate key phrases at diverse parts of the article .
Multi-Stage Training
This technique is inspired by the robust results from the random model in section 4 .
We implement a multi-stage training method for both Ban-ditSum and RNES where in the first few epochs , we train on an ' unbiased ' dataset where the sentences in every training document are randomly shuffled .
We then fine - tune the models by training on the original training articles .
The goal is to prime the model to learn sentence semantics independently of position , and then introduce the task of balancing semantics and positional cues .
ROUGE - based Auxiliary Loss
We observed that BanditSum tends to converge to a low-entropy policy , in the sense that the model 's affinity scores are either 1 or 0 at the end of training .
Furthermore , over 68 % of its selections are from the three leading sentences of the source .
Regularizing low-entropy policies can increase a model 's propensity to explore potentially good states or stay close to a known good policy ( Nachum et al. , 2017 ; Galashov et al. , 2019 ) .
We extend this idea to summarization by introducing a ROUGE - based loss which regularizes the model policy using an estimate of the value of individual sentences .
These sentence - level estimates are computed as a distribution P R : P R ( x = i ) = r( s i , G ) n j=1 r(s j , G ) , ( 1 ) where r is the average of ROUGE - 1 , - 2 and - L F 1 scores between sentence s i in the article and the reference summary G .
We would like the model 's predictive distribution P M to approximately match P R .
To compute P M , we normalize the predicted scores from a non-auto- regressive model .
In an auto-regressive model such as RNES , the decision of including a sentence depends on those selected so far .
So a straightforward KL objective is hard to implement , and we use this technique for BanditSum only .
Our auxiliary loss is defined as the KL divergence : L KL = D KL ( P R P M ) .
The update rule then becomes : ? ( t + 1 ) = ? ( t ) + ? ?L M ( ? ( t ) ) + ?L KL ( ? ( t ) ) ( 2 ) where ? ( t ) represents the model 's parameters at time step t , L M is the original model 's loss function , and ? is a hyperparameter .
Experimental Setup
We use the CNN / Daily Mail dataset ( Hermann et al. , 2015 ) with the standard train / dev/ test splits of 287,227/13,368/11,490 .
To avoid inconsistencies , we built on top of the author-provided implementations for BanditSum and our faithful reimplementation of RNES .
To reduce training time , we pre-compute and store the average of ROUGE - 1 , - 2 , and - L for every sentence triplet of each article , using a HDF5 table and PyTables ( PyTables Developers Team , 2002 - 2019
The HDF Group , 1997 - 2019 .
This allows for a considerable increase in training speed .
We limit the maximum number of sentences considered in an article to the first 100 .
All the models were trained for 4 epochs .
For the multi-stage training , we pretrain for 2 epochs , then train on the original articles for 2 epochs .
We set the auxiliary loss hyperparameters ? = 1e ? 4 and ? = 0.0095 in eq. 2 based on a grid search using the Tune library ( Liaw et al. , 2018 ) .
We also train a baseline entropy model by replacing L KL with the negated entropy of P M in eq .
2 . This loss penalizes low entropy , helping the model explore , but it is ' undirected ' compared to our proposed method .
We present the results of Lead - 3 baseline ( first 3 sentences ) , and two other competitive models - Refresh 2 ( Narayan et al. , 2018a ) and NeuSum .
Lastly , we include results from an oracle summarizer , computed as the triplet of source sentences with the highest average of ROUGE - 1 , - 2 and - L scores against the abstractive gold standard .
Results and Discussion Table 3 reports the F1 scores for ROUGE - 1 , -2 and - L ( Lin , 2004 ) .
We use the pyrouge 3 wrapper library to evaluate the final models , while training with a faster Python-only implementation 4 .
We test for significance between the baseline models and our proposed techniques using the bootstrap method .
This method was first recommended for testing significance in ROUGE scores by Lin ( 2004 ) , and has subsequently been advocated as an appropriate measure in works such as Dror et al . ( 2018 ) and Berg-Kirkpatrick et al . ( 2012 ) .
The simple entropy regularizer has a small but not significant improvement , and pretraining has a similar improvement only for RNES .
But the auxiliary ROUGE loss significantly ( p < 0.001 ) improves over BanditSum , obtaining an extra 0.15 ROUGE points on average .
The last column reports the percentage of summary sentences which overlap with the lead .
The auxiliary loss leads to a 4.7 % absolute decrease in such selections compared to the base system , while also reaching a better ROUGE score .
Figure 1 shows that the reward ( average ROUGE - 1 , - 2 , -L ) for the auxiliary loss model is consistently above the base .
We also examined the auxiliary loss model on documents where the summary is mostly comprised of lead sentences D early , mostly sentences much later in the article D late , and a dataset at the midway point , D med .
To create these sets , we rank test articles using the average index of its summary sentences in the source document .
The 100 test articles with lowest average index are D early , the 100 with highest value are D late and the 100 closest to the median are D med .
In Table 4 , we can see that the auxiliary loss model 's improvements are even more amplified on D med and D late .
On the other hand , our pretraining results are mixed .
We hope to employ more controlled multitasking methods ( Kiperwasser and Ballesteros , 2018 ) in the future to deal with the issue .
The second line in Table 4 reports the oracle ROUGE scores of the best possible extractive summary .
While all systems are quite close to the oracle on D early they only reach half the performance on D late .
This gap indicates that our improvements only scratch the surface , but also that this problem is worthy and challenging to explore .
It is worth noting that we have attempted to build a single model which can summarize both lead-biased articles and those whose information is spread throughout .
Our aim was to encourage the model to explore useful regions as a way of learning better document semantics .
But we hypothesize that our models can be further improved by learning to automatically predict when the lead paragraph suffices as a summary , and when the model should look further in the document .
Conclusion
In this paper , we have presented the first approaches for learning a summarization system by countering the strong effect of summary - worthy lead sentences .
We demonstrate that recent summarization systems over-exploit the inherent lead bias present in news articles , to the detriment of their summarization capabilities .
We explore two techniques aimed at learning to better balance positional cues with semantic ones .
While our auxiliary loss method achieves significant improvement , we note that there is a large gap which better methods can hope to bridge in the future .
One approach , building on ours , is to examine other ways to combine loss signals ( Finn et al. , 2017 ) , and to encourage exploration ( Haarnoja et al. , 2018 ) .
We will also carry out deeper study of the properties of D early and D late type documents and use them to inform new solutions .
On cursory analysis , the most frequent terms in D early tend to be about UK politics , while in D late they are often related to British soccer .
Figure 1 : 1 Figure 1 : Training curves for BanditSum based models .
Average ROUGE is the average of ROUGE -1 , - 2 and -L F1 .
