title
Minimum Imputed Risk : Unsupervised Discriminative Training for Machine Translation
abstract
Discriminative training for machine translation has been well studied in the recent past .
A limitation of the work to date is that it relies on the availability of high-quality in - domain bilingual text for supervised training .
We present an unsupervised discriminative training framework to incorporate the usually plentiful target - language monolingual data by using a rough " reverse " translation system .
Intuitively , our method strives to ensure that probabilistic " round-trip " translation from a targetlanguage sentence to the source - language and back will have low expected loss .
Theoretically , this may be justified as ( discriminatively ) minimizing an imputed empirical risk .
Empirically , we demonstrate that augmenting supervised training with unsupervised data improves translation performance over the supervised case for both IWSLT and NIST tasks .
Introduction
Missing data is a common problem in statistics when fitting the parameters ? of a model .
A common strategy is to attempt to impute , or " fill in , " the missing data ( Little and Rubin , 1987 ) , as typified by the EM algorithm .
In this paper we develop imputation techniques when ? is to be trained discriminatively .
We focus on machine translation ( MT ) as our example application .
A Chinese-to- English machine translation system is given a Chinese sentence x and asked to predict its English translation y .
This system employs statistical models p ? ( y | x ) whose parameters ? are discriminatively trained using bilingual sentence pairs ( x , y ) .
But bilingual data for such supervised training may be relatively scarce for a particular language pair ( e.g. , Urdu-English ) , especially for some topics ( e.g. , technical manuals ) or genres ( e.g. , blogs ) .
So systems seek to exploit additional monolingual data , i.e. , a corpus of English sentences y with no corresponding source - language sentences x , to improve estimation of ?.
This is our missing data scenario .
1 Discriminative training of the parameters ? of p ? ( y | x ) using monolingual English data is a curious idea , since there is no Chinese input x to translate .
We propose an unsupervised training approach , called minimum imputed risk training , which is conceptually straightforward : First guess x ( probabilistically ) from the observed y using a reverse Englishto - Chinese translation model p ? ( x | y ) .
Then train the discriminative Chinese-to- English model p ? ( y | x ) to do a good job at translating this imputed x back to y , as measured by a given performance metric .
Intuitively , our method strives to ensure that probabilistic " round-trip " translation from a targetlanguage sentence to the source - language and back again will have low expected loss .
Our approach can be applied in an application scenario where we have ( 1 ) enough out - of- domain bilingual data to build two baseline translation systems , with parameters ? for the forward direction , and ? for the reverse direction ; ( 2 ) a small amount of in-domain bilingual development data to discriminatively tune a small number of parameters in ? ; and ( 3 ) a large amount of in- domain English monolingual data .
The novelty here is to exploit ( 3 ) to discriminatively tune the parameters ? of all translation model components , 2 p ? ( y|x ) and p ? ( y ) , not merely train a generative language model p ? ( y ) , as is the norm .
Following the theoretical development below , the empirical effectiveness of our approach is demonstrated by replacing a key supervised discriminative training step in the development of large MT systems - learning the log-linear combination of several component model scores ( viewed as features ) to optimize a performance metric ( e.g. BLEU ) on a set of ( x , y ) pairs - with our unsupervised discriminative training using only y .
One may hence contrast our approach with the traditional supervised methods applied to the MT task such as minimum error rate training ( Och , 2003 ; , the averaged Perceptron ( Liang et al. , 2006 ) , maximum conditional likelihood ( Blunsom et al. , 2008 ) , minimum risk ( Smith and Eisner , 2006 ; Li and Eisner , 2009 ) , and MIRA ( Watanabe et al. , 2007 ; Chiang et al. , 2009 ) .
We perform experiments using the open-source MT toolkit Joshua ( Li et al. , 2009a ) , and show that adding unsupervised data to the traditional supervised training setup improves performance .
Supervised Discriminative Training via Minimization of Empirical Risk
Let us first review discriminative training in the supervised setting - as used in MERT ( Och , 2003 ) and subsequent work .
One wishes to tune the parameters ? of some complex translation system ? ? ( x ) .
The function ? ? , which translates Chinese x to English y = ? ? ( x ) need not be probabilistic .
For example , ? may be the parameters of a scoring function used by ? , along with pruning and decoding heuristics , for extracting a high-scoring translation of x .
The goal of discriminative training is to minimize the expected loss of ? ? ( ? ) , under a given taskspecific loss function L(y , y ) that measures how L (? ? ( x i ) , y i ) .
( 2 ) The search for ?
* typically requires the use of numerical methods and some regularization .
5 3 Unsupervised Discriminative Training with Missing Inputs
Minimization of Imputed Risk
We now turn to the unsupervised case , where we have training examples {y i } but not their corresponding inputs {x i } .
We cannot compute the summand L (? ? ( x i ) , y i ) for such i in ( 2 ) , since ? ? ( x i ) requires to know x i .
So we propose to replace 3
This goal is different from the minimum risk training of Li and Eisner ( 2009 ) in a subtle but important way .
In both cases , ? * minimizes risk or expected loss , but the expectation is w.r.t. different distributions : the expectation in Li and Eisner ( 2009 ) is under the conditional distribution p(y | x ) , while the expectation in ( 1 ) is under the joint distribution p( x , y ) .
4
In the terminology of statistical decision theory , p( x , y ) is a distribution over states of nature .
We seek a decision rule ? ? ( x ) that will incur low expected loss on observations x that are generated from unseen states of nature .
5
To compensate for the shortcut of using the unsmoothed empirical distribution rather than a posterior estimate of p( x , y ) ( Minka , 2000 ) , it is common to add a regularization term ||?|| 2 2 in the objective of ( 2 ) .
The regularization term can prevent overfitting to a training set that is not large enough to learn all parameters .
L (? ? ( x i ) , y i ) with the expectation x p ? ( x | y i ) L (? ? ( x ) , y i ) , ( 3 ) where p ? ( ? | ? ) is a " reverse prediction model " that attempts to impute the missing x i data .
We call the resulting variant of ( 2 ) the minimization of imputed empirical risk , and say that ? * = argmin ?
1 N N i=1 x p ? ( x | y i ) L (? ? ( x ) , y i ) ( 4 ) is the estimate with the minimum imputed risk 6 .
The minimum imputed risk objective of ( 4 ) could be evaluated by brute force as follows .
1 . For each unsupervised example y i , use the reverse prediction model p ? (? | y i ) to impute possible reverse translations X i = {x i1 , x i2 , . . .} , and add each ( x ij , y i ) pair , weighted by p ? ( x ij | y i ) ? 1 , to an imputed training set .
2 . Perform the supervised training of ( 2 ) on the imputed and weighted training data .
The second step means that we must use ? ? to forward - translate each imputed x ij , evaluate the loss of the translations y ij against the corresponding true translation y i , and choose the ? that minimizes the weighted sum of these losses ( i.e. , the empirical risk when the empirical distribution p( x , y ) is derived from the imputed training set ) .
Specific to our MT task , this tries to ensure that probabilistic " roundtrip " translation , from the target - language sentence y i to the source - language and back again , will have a low expected loss .
7
The trouble with this method is that the reverse model p ? generates a weighted lattice or hypergraph X i encoding exponentially many translations of y i , and it is computationally infeasible to forwardtranslate each x ij ?
X i .
We therefore investigate several approximations to ( 4 ) in Section 3.4 .
6
One may exploit both supervised data { ( xi , yi ) } and unsupervised data { yj } to perform semi-supervised training via an interpolation of ( 2 ) and ( 4 ) .
We will do so in our experiments .
7
Our approach may be applied to other tasks as well .
For example , in a speech recognition task , ? ? is a speech recognizer that produces text , whereas p ? is a speech synthesizer that must produce a distribution over audio ( or at least over acoustic features or phone sequences ) ( Huang et al. , 2010 ) .
The Reverse Prediction Model p ?
A crucial ingredient in ( 4 ) is the reverse prediction model p ? ( ?|? ) that attempts to impute the missing x i .
We will train this model in advance , doing the best job we can from available data , including any outof-domain bilingual data as well as any in- domain monolingual data 8 x .
In the MT setting , ? ? and p ? may have similar parameterization .
One translates Chinese to English ; the other translates English to Chinese .
Yet the setup is not quite symmetric .
Whereas ? ? is a translation system that aims to produce a single , low-loss translation , the reverse version p ? is rather a probabilistic model .
It is supposed to give an accurate probability distribution over possible values x ij of the missing input sentence x i .
All of these values are taken into account in ( 4 ) , regardless of the loss that they would incur if they were evaluated for translation quality relative to the missing x i .
Thus , ? does not need to be trained to minimize the risk itself ( so there is no circularity ) .
Ideally , it should be trained to match the underlying conditional distribution of x given y , by achieving a low conditional cross-entropy H( X | Y ) = ? x,y p( x , y ) log p ? ( x | y ) .
( 5 ) In practice , ? is trained by ( empirically ) minimiz - ing ?
1 M N j=1 log p ? ( x j | y j ) + 1 2 ? 2 ? 2 2 on some bilingual data , with the regularization coefficient ?
2 tuned on held out data .
It may be tolerable for p ? to impute mediocre translations x ij .
All that is necessary is that the ( forward ) translations generated from the imputed x ij " simulate " the competing hypotheses that we would see when translating the correct Chinese input x i .
The Forward Translation System ? ? and The Loss Function L (? ? ( x i ) , y i )
The minimum empirical risk objective of ( 2 ) is quite general and various popular supervised training methods ( Lafferty et al. , 2001 ; Collins , 2002 ; Och , 2003 ; Crammer et al. , 2006 ; Smith and Eisner , 2006 ) can be formalized in this framework by choosing different functions for ? ? and L (? ? ( x i ) , y i ) .
The generality of ( 2 ) extends to our minimum imputed risk objective of ( 4 ) .
Below , we specify the ? ? and L (? ? ( x i ) , y i ) we considered in our investigation .
Deterministic Decoding
A simple translation rule would define ? ? ( x ) = argmax y p ? ( y | x ) ( 6 ) If this ? ? ( x ) is used together with a loss function L ( ? ? ( x i ) , y i ) that is the negated BLEU score 9 , our minimum imputed risk objective of ( 4 ) is equivalent to MERT ( Och , 2003 ) on the imputed training data .
However , this would not yield a differentiable objective function .
Infinitesimal changes to ?
could result in discrete changes to the winning output string ? ? ( x ) in ( 6 ) , and hence to the loss L (? ? ( x ) , y i ) .
Och ( 2003 ) developed a specialized line search to perform the optimization , which is not scalable when the number of model parameters ? is large .
Randomized Decoding Instead of using the argmax of ( 6 ) , we assume during training that ? ? ( x ) is itself random , i.e. the MT system randomly outputs a translation y with probability p ? ( y | x ) .
As a result , we will modify our objective function of ( 4 ) to take yet another expectation over the unknown y.
Specifically , we will replace L (? ? ( x ) , y i ) in ( 4 ) with y p ? ( y | x ) L(y , y i ) .
( 7 ) Now , the minimum imputed empirical risk objective of ( 4 ) becomes ? * = argmin ? 1 N N i=1 x, y p ? ( x | y i ) p ? ( y | x ) L(y , y i ) ( 8 ) If the loss function L(y , y i ) is a negated BLEU , this is equivalent to performing minimum-risk training described by ( Smith and Eisner , 2006 ; Li and Eisner , 2009 ) on the imputed data .
10 9 One can manipulate the loss function to support other methods that use deterministic decoding , such as Perceptron ( Collins , 2002 ) and MIRA ( Crammer et al. , 2006 ) .
10 Again , one may manipulate the loss function to support other probabilistic methods that use randomized decoding , such as CRFs ( Lafferty et al. , 2001 ) .
The objective function in ( 8 ) is now differentiable , since each coefficient p ? ( y | x ) is a differentiable function of ? , and thus amenable to optimization by gradient - based methods ; we use the L-BFGS algorithm ( Liu et al. , 1989 ) in our experiments .
We perform experiments with the syntax - based MT system Joshua ( Li et al. , 2009a ) , which implements dynamic programming algorithms for second-order expectation semirings ( Li and Eisner , 2009 ) to efficiently compute the gradients needed for optimizing ( 8 ) .
Approximating p ? ( x | y i )
As mentioned at the end of Section 3.1 , it is computationally infeasible to forward - translate each of the imputed reverse translations x ij .
We propose four approximations that are computationally feasible .
Each may be regarded as a different approximation of p ? ( x | y i ) in equations ( 4 ) or ( 8 ) .
k-best .
For each y i , add to the imputed training set only the k most probable translations { x i1 , . . . x ik } according to p ? ( x | y i ) .
( These can be extracted from X i using standard algorithms ( Huang and Chiang , 2005 ) .)
Rescale their probabilities to sum to 1 .
Sampling .
For each y i , add to the training set k independent samples { x i1 , . . . x ik } from the distribution p ? ( x | y i ) , each with weight 1/k .
( These can be sampled from X i using standard algorithms ( Johnson et al. , 2007 ) .)
This method is known in the literature as multiple imputation ( Rubin , 1987 ) . Lattice .
11 Under certain special cases it is be possible to compute the expected loss in ( 3 ) exactly via dynamic programming .
Although X i does contain exponentially many translations , it may use a " packed " representation in which these translations share structure .
This representation may furthermore enable sharing work in forward - translation , so as to efficiently translate the entire set X i and obtain a distribution over translations y.
Finally , the expected loss under that distribution , as required by equation ( 3 ) , may also be efficiently computable .
All this turns out to be possible if ( a ) the posterior distribution p ? ( x | y i ) is represented by an un-ambiguous weighted finite -state automaton X i , ( b ) the forward translation system ? ? is structured in a certain way as a weighted synchronous context- free grammar , and ( c ) the loss function decomposes in a certain way .
We omit the details of the construction as beyond the scope of this paper .
In our experimental setting described below , ( b ) is true ( using Joshua ) , and ( c ) is true ( since we use a loss function presented by Tromble et al . ( 2008 ) that is an approximation to BLEU and is decomposable ) .
While ( a ) is not true in our setting because X i is a hypergraph ( which is ambiguous ) , Li et al . ( 2009 b ) show how to approximate a hypergraph representation of p ? ( x | y i ) by an unambiguous WFSA .
One could then apply the construction to this WFSA 12 , obtaining an approximation to ( 3 ) .
Rule-level Composition .
Intuitively , the reason why the structure - sharing in the hypergraph X i ( generated by the reverse system ) cannot be exploited during forward translating is that when the forward Hiero system translates a string x i ?
X i , it must parse it into recursive phrases .
But the structure - sharing within the hypergraph of X i has already parsed x i into recursive phrases , in a way determined by the reverse Hiero system ; each translation phrase ( or rule ) corresponding to a hyperedge .
To exploit structure - sharing , we can use a forward translation system that decomposes according to that existing parse of x i .
We can do that by considering only forward translations that respect the hypergraph structure of X i .
The simplest way to do this is to require complete isomorphism of the SCFG trees used for the reverse and forward translations .
In other words , this does round-trip imputation ( i.e. , from y to x , and then to y ) at the rule level .
This is essentially the approach taken by .
The Log-Linear Model p ?
We have not yet specified the form of p ? .
Following much work in MT , we begin with a linear model score ( x , y ) = ? ? f ( x , y ) = k ? k f k ( x , y ) ( 9 ) where f ( x , y ) is a feature vector indexed by k .
Our deterministic test-time translation system ? ? simply outputs the highest - scoring y for fixed x .
At training time , our randomized decoder ( Section 3.3.2 ) uses the Boltzmann distribution ( here a log-linear model ) ( x , y ) y e ?score ( x , y ) ( 10 )
The scaling factor ?
controls the sharpness of the training - time distribution , i.e. , the degree to which the randomized decoder favors the highest - scoring y.
For large ? , our training objective approaches the imputed risk of the deterministic test-time system while remaining differentiable .
p ? ( y | x ) = e ?score ( x , y ) Z( x ) = e ?score
In a task like MT , in addition to the input x and output y , we often need to introduce a latent variable d to represent the hidden derivation that relates x to y .
A derivation d represents a particular phrase segmentation in a phrase - based MT system ( Koehn et al. , 2003 ) and a derivation tree in a typical syntaxbased system ( Galley et al. , 2006 ; Chiang , 2007 ) .
We change our model to assign scores not to an ( x , y ) pair but to the detailed derivation d ; in particular , now the function f that extracts a feature vector can look at all of d .
We replace y by d in ( 9 ) -( 10 ) , and finally define p ? ( y|x ) by marginalizing out d , p ? ( y | x ) = d?D( x , y ) p ? ( d | x ) ( 11 ) where D( x , y ) represents the set of derivations that yield x and y .
Minimum Imputed Risk vs. EM
The notion of imputing missing data is familiar from other settings ( Little and Rubin , 1987 ) , particularly the expectation maximization ( EM ) algorithm , a widely used generative approach .
So it is instructive to compare EM with minimum imputed risk .
One can estimate ? by maximizing the loglikelihood of the data {( x i , y i ) , i = 1 , . . . , N } as argmax ?
1 N N i=1 log p ? ( x i , y i ) .
( 12 ) If the x i 's are missing , EM tries to iteratively maximize the marginal probability : argmax ?
1 N N i=1 log x p ? ( x , y i ) .
( 13 ) The E-step of each iteration comprises computing x p ?t ( x | y i ) log p ? ( x , y i ) , the expected loglikelihood of the complete data , where p ?t ( x | y i ) is the conditional part of p ?t ( x , y i ) under the current iterate ?
t , and the M-step comprises maximizing it : ?
t+ 1 = argmax ? 1 N N i=1 x p ?t ( x | y i ) log p ? ( x , y i ) .
( 14 ) Notice that if we replace p ?t ( x|y i ) with p ? ( x | y i ) in the equation above , and admit negated loglikelihood as a loss function , then the EM update ( 14 ) becomes identical to ( 4 ) .
In other words , the minimum imputed risk approach of Section 3.1 differs from EM in ( i ) using an externally - provided and static p ? , instead of refining it at each iteration based on the current p ?t , and ( ii ) using a specific loss function , namely negated log-likelihood .
So why not simply use the maximum-likelihood ( EM ) training procedure for MT ?
One reason is that it is not discriminative : the loss function ( e.g. negated BLEU ) is ignored during training .
A second reason is that training good joint models p ? ( x , y ) is computationally expensive .
Contemporary MT makes heavy use of log-linear probability models , which allow the system designer to inject phrase tables , linguistic intuitions , or prior knowledge through a careful choice of features .
Computing the objective function of ( 14 ) in closed form is difficult if p ? is an arbitrary log-linear model , because the joint probability p ? ( x i , y i ) is then defined as a ratio whose denominator Z ? involves a sum over all possible sentence pairs ( x , y ) of any length .
By contrast , our discriminative framework will only require us to work with conditional models .
While conditional probabilities such as p ? ( x | y ) and p ? ( y | x ) are also ratios , computing their denominators only requires us to sum over a packed forest of possible translations of a given y or x .
13
In summary , EM would impute missing data using p ? ( x | y ) and predict outputs using p ? ( y | x ) , both being conditional forms of the same joint model p ? ( x , y ) .
Our minimum imputed risk training method is similar , but it instead uses a pair of separately parameterized , separately trained models p ? ( x | y ) and p ? ( y | x ) .
By sticking to conditional models , we can efficiently use more sophisticated model features , and we can incorporate the loss function when we train ? , which should improve both efficiency and accuracy at test time .
Experimental Results
We report results on Chinese-to - English translation tasks using Joshua ( Li et al. , 2009a ) , an open-source implementation of Hiero ( Chiang , 2007 ) .
Baseline Systems
IWSLT Task
We train both reverse and forward baseline systems .
The translation models are built using the corpus for the IWSLT 2005 Chinese to English translation task ( Eck and Hori , 2005 ) , which comprises 40,000 pairs of transcribed utterances in the travel domain .
We use a 5 - gram language model with modified Kneser - Ney smoothing ( Chen and Goodman , 1998 ) , trained on the English ( resp. Chinese ) side of the bitext .
We use a standard training pipeline and pruning settings recommended by ( Chiang , 2007 ) .
NIST Task
For the NIST task , the TM is trained on about 1 M parallel sentence pairs ( about 28 M words in each language ) , which are sub-sampled from corpora distributed by LDC for the NIST MT evaluation using a sampling method implemented in Joshua .
We also used a 5 - gram language model , trained on a data set consisting of a 130M words in English Gigaword ( LDC2007T07 ) and the bitext 's English side .
Feature Functions
We use two classes of features f k for discriminative training of p ? as defined in ( 9 ) .
Regular Hiero Features
We include ten features that are standard in Hiero ( Chiang , 2007 ) .
In particular , these include one baseline language model feature , three baseline translation models , one word penalty feature , three features to count how many rules with an arity of zero / one / two are used in a derivation , and two features to count how many times the unary and binary glue rules in Hiero are used in a derivation .
Target -rule Bigram Features
In this paper , we do not attempt to discriminatively tune a separate parameter for each bilingual rule in the Hiero grammar .
Instead , we train several hundred features that generalize across these rules .
For each bilingual rule , we extract bigram features over the target - side symbols ( including nonterminals and terminals ) .
For example , if a bilingual rule 's target - side is " on the X 1 issue of X 2 " where X 1 and X 2 are non-terminals ( with a position index ) , we extract the bigram features on the , the X , X issue , issue of , and of X .
( Note that the position index of a non-terminal is ignored in the feature . )
Moreover , for the terminal symbols , we will use their dominant POS tags ( instead of the symbol itself ) .
For example , the feature the X becomes DT X .
We use 541 such bigram features for IWSLT task ( and 1023 such features for NIST task ) that fire frequently .
Data Sets for Discriminative Training
IWSLT Task
In addition to the 40,000 sentence pairs used to train the baseline generative models ( which are used to compute the features f k ) , we use three bilingual data sets listed in Table 1 , also from IWSLT , for discriminative training : one to train the reverse model p ?
( which uses only the 10 standard Hiero features as described in Section 5.2.1 ) , 14 one to train the forward model ? ? ( which uses both classes of features described in Section 5.2 , i.e. , 551 features in total ) , and one for test .
Note that the reverse model ? is always trained using the supervised data of Dev ? , while the forward model ? may be trained in a supervised or semisupervised manner , as we will show below .
In all three data sets , each Chinese sentence x i has 16 English reference translations , so each y i is actually a set of 16 translations .
When we impute data from y i ( in the semi-supervised scenario ) , we actually impute 16 different values of x i , by using p ? to separately reverse translate each sentence in y i .
This effectively adds 16 pairs of the form ( x i , y i ) to the training set ( see section 3.4 ) , where each x i is a different input sentence ( imputed ) in each case , but y i is always the original set of 16 references .
NIST Task
For the NIST task , we use MT03 set ( having 919 sentences ) to tune the component parameters in both the forward and reverse baseline systems .
Additionally , we use the English side of MT04 ( having 1788 sentences ) to perform semi-supervised tuning of the forward model .
The test sets are MT05 and MT06 ( having 1082 and 1099 sentences , respectively ) .
In all the data sets , each source sentence has four reference translations .
Main Results
We compare two training scenarios : supervised and semi-supervised .
The supervised system ( " Sup " ) carries out discriminative training on a bilingual data set .
The semi-supervised system ( " + Unsup " ) additionally uses some monolingual English text for discriminative training ( where we impute one Chinese translation per English sentence ) .
Tables 2 and 3 report the results for the two tasks under two training scenarios .
Clearly , adding unsupervised data improves over the supervised case , by at least 1.3 BLEU points in IWSLT and 0.5 BLEU in NIST .
Results for Analysis Purposes Below , we will present more results on the IWSLT data set to help us understand the behavior of the for each English sentence we impute the 1 - best Chinese translation .
" WLM " means a Chinese language model is used in the reverse system , while " NLM " means no Chinese language model is used .
In addition to reporting the BLEU score on Eval ? , we also report " Imputed - CN BLEU " , the BLEU score of the imputed Chinese sentences against their corresponding Chinese reference sentences .
BLEU point in the forward translations .
Still , even with the worse imputation ( in the case of " NLM " ) , our forward translations improve as we add more monolingual data .
Imputation with Different k-best Sizes
In all the experiments so far , we used the reverse translation system to impute only a single Chinese translation for each English monolingual sentence .
This is the 1 - best approximation of section 3.4 .
Table 5 shows ( in the fully unsupervised case ) that the performance does not change much as k increases .
16
This may be because that the 5 - best sentences are likely to be quite similar to one another ( May and Knight , 2006 ) .
Imputing a longer k-best list , a sample , or a lattice for x i ( see section 3.4 ) might achieve more diversity in the training inputs , which might make the system more robust .
Conclusions
In this paper , we present an unsupervised discriminative training method that works with missing inputs .
The key idea in our method is to use a reverse model to impute the missing input from the observed output .
The training will then forward translate the imputed input , and choose the parameters of the forward model such that the imputed risk ( i.e. , the expected loss of the forward translations with respect to the observed output ) is minimized .
This matches the intuition that the probabilistic " roundtrip " translation from the target - language sentence to the source - language and back should have low expected loss .
We applied our method to two Chinese to English machine translation tasks ( i.e. IWSLT and NIST ) .
We showed that augmenting supervised data with unsupervised data improved performance over the supervised case ( for both tasks ) .
Our discriminative model used only a small amount of training data and relatively few features .
In future work , we plan to test our method in settings where there are large amounts of monolingual training data ( enabling many discriminative features ) .
Also , our experiments here were performed on a language pair ( i.e. , Chinese to English ) that has quite rich bilingual resources in the domain of the test data .
In future work , we plan to consider lowresource test domains and language pairs like Urdu-English , where bilingual data for novel domains is sparse .
Table 1 : 1 IWSLT Data sets used for discriminative training / test .
Dev ? is used for discriminatively training of the reverse model ? , Dev ? is for the forward model , and Eval ? is for testing .
The star * for Dev ?
emphasizes that some of its Chinese side will not be used in the training ( see Table2 for details ) .
Data set Purpose # of sentences Chinese English Dev ? training ? 503 503?16 Dev ? Eval ? training ? testing 503 * 506 503?16 506?16
Table 4 : 4 BLEU scores for unsupervised training with / without using a language model in the reverse system .
A data size of 101 means that we use only the English sentences from a subset of Dev ? containing 101 Chinese sentences and 101?16 English translations ; Data size Imputed -CN BLEU Test-EN BLEU WLM NLM WLM NLM 101 11.8 3.0 48.5 46.7 202 11.7 3.2 48.9 47.6 303 13.4 3.5 48.8 47.9
Table 5 : 5 BLEU scores for unsupervised training with different k-best sizes .
We use 101?16 monolingual English sentences , and for each English sentence we impute the k-best Chinese translations using the reverse system .
Training scenario Test BLEU Unsup , k=1 48.5 Unsup , k=2 48.4 Unsup , k=3 48.9 Unsup , k=4 48.5 Unsup , k=5 48.4
Contrast this with traditional semi-supervised training that looks to exploit " unlabeled " inputs x , with missing outputs y.
Note that the extra monolingual data is used only for tuning the model weights , but not for inducing new phrases or rules .
In a translation task from x to y , one usually does not make use of in- domain monolingual data x .
But we can exploit x to train a language model p ? ( x ) for the reverse translation system , which will make the imputed xij look like true Chinese inputs .
The lattice approximation is presented here as a theoretical contribution , and we do not empirically evaluate it since its implementation requires extensive engineering effort that is beyond the main scope of this paper .
Note that the forward translation of a WFSA is tractable by using a lattice - based decoder such as that by Dyer et al . ( 2008 ) .
Analogously , discriminative CRFs have become more popular than generative HMMs because they permit efficient training even with a wide variety of log-linear features ( Lafferty et al. , 2001 ) .
Ideally , we should train ? to minimize the conditional cross-entropy ( 5 ) as suggested in section 3.2 .
In the present results , we trained ?
discriminatively to minimize risk , purely for ease of implementation using well versed steps .
The BLEU scores are low even with the language model because only one Chinese reference is available for scoring .
In the present experiments , however , we simply weighted all k imputed translations equally , rather than in proportion to their posterior probabilities as suggested in Section 3.4 .
