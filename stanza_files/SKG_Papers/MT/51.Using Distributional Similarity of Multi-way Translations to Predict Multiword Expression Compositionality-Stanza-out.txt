title
Using Distributional Similarity of Multi-way Translations to Predict Multiword Expression Compositionality
abstract
We predict the compositionality of multiword expressions using distributional similarity between each component word and the overall expression , based on translations into multiple languages .
We evaluate the method over English noun compounds , English verb particle constructions and German noun compounds .
We show that the estimation of compositionality is improved when using translations into multiple languages , as compared to simply using distributional similarity in the source language .
We further find that string similarity complements distributional similarity .
Compositionality of MWEs Multiword expressions ( hereafter MWEs ) are combinations of words which are lexically , syntactically , semantically or statistically idiosyncratic ( Sag et al. , 2002 ; Baldwin and Kim , 2009 ) .
Much research has been carried out on the extraction and identification of MWEs 1 in English ( Schone and Jurafsky , 2001 ; Pecina , 2008 ; Fazly et al. , 2009 ) and other languages ( Dias , 2003 ; Evert and Krenn , 2005 ; Salehi et al. , 2012 ) .
However , considerably less work has addressed the task of predicting the meaning of MWEs , especially in non-English languages .
As a step in this direction , the focus of this study is on predicting the compositionality of MWEs .
An MWE is fully compositional if its meaning is predictable from its component words , and it is non-compositional ( or idiomatic ) if not .
For example , stand up " rise to one 's feet " is composi-tional , because its meaning is clear from the meaning of the components stand and up .
However , the meaning of strike up " to start playing " is largely unpredictable from the component words strike and up .
In this study , following McCarthy et al. ( 2003 ) and Reddy et al . ( 2011 ) , we consider compositionality to be graded , and aim to predict the degree of compositionality .
For example , in the dataset of Reddy et al . ( 2011 ) , climate change is judged to be 99 % compositional , while silver screen is 48 % compositional and ivory tower is 9 % compositional .
Formally , we model compositionality prediction as a regression task .
An explicit handling of MWEs has been shown to be useful in NLP applications ( Ramisch , 2012 ) .
As an example , Carpuat and Diab ( 2010 ) proposed two strategies for integrating MWEs into statistical machine translation .
They show that even a large scale bilingual corpus cannot capture all the necessary information to translate MWEs , and that in adding the facility to model the compositionality of MWEs into their system , they could improve translation quality .
Acosta et al. ( 2011 ) showed that treating non-compositional MWEs as a single unit in information retrieval improves retrieval effectiveness .
For example , while searching for documents related to ivory tower , we are almost certainly not interested in documents relating to elephant tusks .
Our approach is to use a large-scale multi-way translation lexicon to source translations of MWEs and their component words , and then model the relative similarity between each of the component words and the MWE , using distributional similarity based on monolingual corpora for the source language and each of the target languages .
Our hypothesis is that using distributional similarity in more than one language will improve the prediction of compositionality .
Importantly , in order to make the method as language - independent and broadly - applicable as possible , we make no use of corpus preprocessing such as lemmatisation , and rely only on the availability of a translation dictionary and monolingual corpora .
Our results confirm our hypothesis that distributional similarity over the source language in addition to multiple target languages improves the quality of compositionality prediction .
We also show that our method can be complemented with string similarity ( Salehi and Cook , 2013 ) to further improve compositionality prediction .
We achieve state - of - the - art results over two datasets .
Related Work
Most recent work on predicting the compositionality of MWEs can be divided into two categories : language / construction -specific and general- purpose .
This can be at either the tokenlevel ( over token occurrences of an MWE in a corpus ) or type-level ( over the MWE string , independent of usage ) .
The bulk of work on compositionality has been language / construction - specific and operated at the token - level , using dedicated methods to identify instances of a given MWE , and specific properties of the MWE in that language to predict compositionality ( Lin , 1999 ; Kim and Baldwin , 2007 ; Fazly et al. , 2009 ) .
General - purpose token - level approaches such as distributional similarity have been commonly applied to infer the semantics of a word / MWE ( Schone and Jurafsky , 2001 ; Reddy et al. , 2011 ) .
These techniques are based on the assumption that the meaning of a word is predictable from its context of use , via the neighbouring words of token - level occurrences of the MWE .
In order to predict the compositionality of a given MWE using distributional similarity , the different contexts of the MWE are compared with the contexts of its components , and the MWE is considered to be compositional if the MWE and component words occur in similar contexts .
Identifying token instances of MWEs is not always easy , especially when the component words do not occur sequentially .
For example consider put on in put your jacket on , and put your jacket on the chair .
In the first example put on is an MWE while in the second example , put on is a simple verb with prepositional phrase and not an instance of an MWE .
Moreover , if we adopt a conservative identification method , the number of token occurrences will be limited and the distribu-tional scores may not be reliable .
Additionally , for morphologically - rich languages , it can be difficult to predict the different word forms a given MWE type will occur across , posing a challenge for our requirement of no language -specific preprocessing .
Pichotta and DeNero ( 2013 ) proposed a tokenbased method for identifying English phrasal verbs based on parallel corpora for 50 languages .
They show that they can identify phrasal verbs better when they combine information from multiple languages , in addition to the information they get from a monolingual corpus .
This finding lends weight to our hypothesis that using translation data and distributional similarity from each of a range of target languages , can improve compositionality prediction .
Having said that , the general applicability of the method is questionable - there are many parallel corpora involving English , but for other languages , this tends not to be the case .
Salehi and Cook ( 2013 ) proposed a generalpurpose type-based approach using translation data from multiple languages , and string similarity between the MWE and each of the component words .
They use training data to identify the best - 10 languages for a given family of MWEs , on which to base the string similarity , and once again find that translation data improves their results substantially .
Among the four string similarity measures they experimented with , longest common substring was found to perform best .
Their proposed method is general and applicable to different families of MWEs in different languages .
In this paper , we reimplement the method of Salehi and Cook ( 2013 ) using longest common substring ( LCS ) , and both benchmark against this method and combine it with our distributional similaritybased method .
Our Approach
To predict the compositionality of a given MWE , we first measure the semantic similarity between the MWE and each of its component words 2 using distributional similarity based on a monolingual corpus in the source language .
We then repeat the process for translations of the MWE and its component words into each of a range of target languages , calculating distributional similarity using Figure 1 : Outline of our approach to computing the distributional similarity ( DS ) of translations of an MWE with each of its component words , for a given target language .
score 1 and score 2 are the similarity for the first and second components , respectively .
We obtain translations from Panlex , and use Wikipedia as our corpus for each language .
a monolingual corpus in the target language ( Figure 1 ) .
We additionally use supervised learning to identify which target languages ( or what weights for each language ) optimise the prediction of compositionality ( Figure 2 ) .
We hypothesise that by using multiple translations - rather than only information from the source language - we will be able to better predict compositionality .
We optionally combine our proposed approach with string similarity , calculated based on the method of Salehi and Cook ( 2013 ) , using LCS .
Below , we detail our method for calculating distributional similarity in a given language , the different methods for combining distributional similarity scores into a single estimate of compositionality , and finally the method for selecting the target languages to use in calculating compositionality .
Calculating Distributional Similarity
In order to be consistent across all languages and be as language - independent as possible , we calcu -
CS method CS method Score 1 for each language Score 2 for each language 2 1 ) 1 ( s s ? ? ? ?
Compositionality score s 1 s 2 Figure 2 : Outline of the method for combining distributional similarity scores from multiple languages , across the components of the MWE .
CS method refers to one of the methods described in Section 3.2 for calculating compositionality .
late distributional similarity in the following manner for a given language .
Tokenisation is based on whitespace delimiters and punctuation ; no lemmatisation or case-folding is carried out .
Token instances of a given MWE or component word are identified by full-token ngram matching over the token stream .
We assume that all full stops and equivalent characters for other orthographies are sentence boundaries , and chunk the corpora into ( pseudo - ) sentences on the basis of them .
For each language , we identify the 51st - 1050th most frequent words , and consider them to be content - bearing words , in the manner of Sch?tze ( 1997 ) .
This is based on the assumption that the top - 50 most frequent words are stop words , and not a good choice of word for calculating distributional similarity over .
That is not to say that we ca n't calculate the distributional similarity for stop words , however ( as we will for the verb particle construction dataset - see Section 4.3.2 ) they are simply not used as the dimensions in our calculation of distributional similarity .
We form a vector of content - bearing words across all token occurrences of the target word , on the basis of these content - bearing words .
Distributional similarity is calculated over these context vectors using cosine similarity .
According to Weeds ( 2003 ) , using dependency relations with the neighbouring words of the target word can better predict the meaning of the target word .
However , in line with our assumption of no language -specific preprocessing , we just use word co-occurrence .
Calculating Compositionality First , we need to calculate a combined compositionality score from the individual distributional similarities between each component word and the MWE .
Following Reddy et al. ( 2011 ) , we combine the component scores using the weighted mean ( as shown in Figure 2 ) : comp = ?s 1 + ( 1 ? ?) s 2 ( 1 ) where s 1 and s 2 are the scores for the first and the second component , respectively .
We use different ? settings for each dataset , as detailed in Section 4.3 .
We experiment with a range of methods for calculating compositionality , as follows : CS L1 : calculate distributional similarity using only distributional similarity in the source language corpus ( This is the approach used by Reddy et al . ( 2011 ) , as discussed in Section 2 ) .
CS L2N : exclude the source language , and compute the mean of the distributional similarity scores for the best - N target languages .
The value of N is selected according to training data , as detailed in Section 3.3 .
CS L1 + L2N : calculate distributional similarity over both the source language ( CS L1 ) and the mean of the best - N languages ( CS L2N ) , and combine via the arithmetic mean .
3
This is to examine the hypothesis that using multiple target languages is better than just using the source language .
CS SVR ( L1 + L2 ) : train a support vector regressor ( SVR : Smola and Sch?lkopf ( 2004 ) ) over the distributional similarities for all 52 languages ( source and target languages ) .
CS string : calculate string similarity using the LCS - based method of Salehi and Cook ( 2013 ) .
4 CS string + L1 : calculate the mean of the string similarity ( CS string ) and distributional similarity in the source language ( Salehi and Cook , 2013 ) .
CS all : calculate the mean of the string similarity ( CS string ) and distributional similarity scores ( CS L1 and CS L2N ) .
Selecting Target Languages
We experiment with two approaches for combining the compositionality scores from multiple target languages .
First , in CS L2N ( and CS L1 + L2N and CS all that build off it ) , we use training data to rank the target languages according to Pearson 's correlation between the predicted compositionality scores and the gold -standard compositionality judgements .
Based on this ranking , we take the best - N languages , and combine the individual compositionality scores by taking the arithmetic mean .
We select N by determining the value that optimises the correlation over the training data .
In other words , the selection of N and accordingly the best - N languages are based on nested cross-validation over training data , independently of the test data for that iteration of cross-validation .
Second in CS SVR ( L1 + L2 ) , we combine the compositionality scores from the source and all 51 target languages into a feature vector , and train an SVR over the data using LIBSVM .
5
Resources
In this section , we describe the resources required by our method , and also the datasets used to evaluate our method .
Monolingual Corpora for Different Languages
We collected monolingual corpora for each of 52 languages ( 51 target languages + 1 source language ) from XML dumps of Wikipedia .
These languages are based on the 54 target languages used by Salehi and Cook ( 2013 ) , excluding Spanish because we happened not to have a dump of Spanish Wikipedia , and also Chinese and Japanese because of the need for a language -specific word tokeniser .
The raw corpora were preprocessed using the WP2TXT toolbox 6 to eliminate XML tags , HTML tags and hyperlinks , and then tokenisation based on whitespace and punctuation was performed .
The corpora vary in size from roughly 750 M tokens for English , to roughly 640 K tokens for Marathi .
Multilingual Dictionary
To translate the MWEs and their components , we follow Salehi and Cook ( 2013 ) in using Panlex ( Baldwin et al. , 2010 ) .
This online dictionary is massively multilingual , covering more than 1353 languages .
For each MWE dataset ( see Section 4.3 ) , we translate the MWE and component words from the source language into each of the 51 languages .
In instances where there is no direct translation in a given language for a term , we use a pivot language to find translation ( s ) in the target language .
For example , the English noun compound silver screen has direct translations in only 13 languages in Panlex , including Vietnamese ( m?n bac ) but not French .
There is , however , a translation of m?n bac into French ( cin?ma ) , allowing us to infer an indirect translation between silver screen and cin ?ma .
In this way , if there are no direct translations into a particular target language , we search for a single-pivot translation via each of our other target languages , and combine them all together as our set of translations for the target language of interest .
In the case that no translation ( direct or indirect ) can be found for a given source language term into a particular target language , the compositionality score for that target language is set to the average across all target languages for which scores can be calculated for the given term .
If no translations are available for any target language ( e.g. the term is not in Panlex ) the compositionality score for each target language is set to the average score for that target language across all other source language terms .
Datasets
We evaluate our proposed method over three datasets ( two English , one German ) , as described below .
English Noun Compounds ( ENC )
Our first dataset is made up of 90 binary English noun compounds , from the work of Reddy et al . ( 2011 ) .
Each noun compound was annotated by multiple annotators using the integer scale 0 ( fully non-compositional ) to 5 ( fully compositional ) .
A final compositionality score was then calculated as the mean of the scores from the annotators .
If we simplistically consider 2.5 as the threshold for compositionality , the dataset is relatively well balanced , containing 48 % compositional and 52 % non-compositional noun compounds .
Following Reddy et al. ( 2011 ) , in combining the componentwise distributional similarities for this dataset , we weight the first component in Equation 1 higher than the second ( ? = 0.7 ) .
English Verb Particle Constructions ( EVPC )
The second dataset contains 160 English verb particle constructions ( VPCs ) , from the work of Bannard ( 2006 ) .
In this dataset , a verb particle construction consists of a verb ( the head ) and a prepositional particle ( e.g. hand in , look up or battle on ) .
For each component word ( the verb and particle , respectively ) , multiple annotators were asked whether the VPC entails the component word .
In order to translate the dataset into a regression task , we calculate the overall compositionality as the number of annotations of entailment for the verb , divided by the total number of verb annotations for that VPC .
That is , following , we only consider the compositionality of the verb component in our experiments ( and as such ? = 1 in Equation 1 ) .
One area of particular interest with this dataset will be the robustness of the method to function words ( the particles ) , both under translation and in terms of calculating distributional similarity , although the findings of Baldwin ( 2006 ) for English prepositions are at least encouraging in this respect .
Additionally , English VPCs can occur in " split " form ( e.g. put your jacket on , from our earlier example ) , which will complicate identification , and the verb component will often be inflected and thus not match under our identification strategy ( for both VPCs and the component verbs ) . ( 1977 ) .
German Noun Compounds ( GNC )
Our final dataset is made up of 246 German noun compounds ( von der Heide and Borgwaldt , 2009 ; Schulte im Walde et al. , 2013 ) .
Multiple annotators were asked to rate the compositionality of each German noun compound on an integer scale of 1 ( non-compositional ) to 7 ( compositional ) .
The overall compositionality score is then calculated as the mean across the annotators .
Note that the component words are provided as part of the dataset , and that there is no need to perform decompounding .
Following Schulte im Walde et al. ( 2013 ) , we weight the first component higher in Equation 1 ( ? = 0.8 ) when calculating the overall compositionality score .
This dataset is significant in being non-English , and also in that German has relatively rich morphology , which we expect to impact on the identification of both the MWE and the component words .
Results
All experiments are carried out using 10 iterations of 10 - fold cross validation , randomly partitioning the data independently on each of the 10 iterations , and averaging across all 100 test partitions in our presented results .
In the case of CS L2N and other methods that make use of it ( i.e. CS L1 + L2N and CS all ) , the languages selected for a given training fold are then used to compute the compositionality scores for the instances in the test set .
Figures 3a , 3 b and 3c are histograms of the number of times each N is selected over 100 folds on ENC , EVPC and GNC datasets , respectively .
From the histograms , N = 6 , N = 15 and N = 2 are the most commonly selected settings for ENC , EVPC and GNC , respectively .
That is , multiple languages are generally used , but more languages are used for English VPCs than either of the compound noun datasets .
The 5 most-selected languages for ENC , EVPC and GNC are shown in Table 1 .
As we can see , there are some languages which are always selected for a given dataset , but equally the commonly -selected languages vary considerably between datasets .
Further analysis reveals that 32 ( 63 % ) target languages for ENC , 25 ( 49 % ) target languages for EVPC , and only 5 ( 10 % ) target languages for GNC have a correlation of r ? 0.1 with goldstandard compositionality judgements .
On the other hand , 8 ( 16 % ) target languages for ENC , 2 ( 4 % ) target languages for EVPC , and no target languages for GNC have a correlation of r ? ?0.1 .
ENC Results English noun compounds are relatively easy to identify in a corpus , 7 because the components occur sequentially , and the only morphological variation is in noun number ( singular vs. plural ) .
In other words , the precision for our token matching method is very high , and the recall is also acceptably high .
Partly as a result of the ease of identification , we get a high correlation of r = 0.700 for CS L1 ( using only source language data ) .
Using only target languages ( CS L2N ) , the results drop to r = 0.434 , but when we combine the two ( CS L1 + L2N ) , the correlation is higher than using only source or target language data , at r = 0.725 .
When we combine all languages using SVR , the results rise slightly higher again to r = 0.744 , which is slightly above the correlation of the state - of - the - art method of Salehi and Cook ( 2013 ) , which combines their method with the method of Reddy et al . ( 2011 ) ( CS string + L1 ) .
These last two results support our hypothesis that using translation data can improve the prediction of compositionality .
The results for string similarity on its own ( CS string , r = 0.644 ) are slightly lower than those using only source language distributional similarity , but when combined with Figure 3 : Histograms displaying how many times a given N is selected as the best number of languages over each dataset .
For example , according to the GNC chart , there is a peak for N = 2 , which shows that over 100 folds , the best - 2 languages achieved the highest correlation on 18 folds . ) there is a slight rise in correlation ( from r = 0.725 to r = 0.732 ) .
Method
EVPC Results English VPCs are hard to identify .
As discussed in Section 2 , VPC components may not occur sequentially , and even when they do occur sequentially , they may not be a VPC .
As such , our simplistic identification method has low precision and recall ( hand analysis of 927 identified VPC instances would suggest a precision of around 74 % ) .
There is no question that this is a contributor to the low correlation for the source language method ( CS L1 ; r = 0.177 ) .
When we use target languages instead of the source language ( CS L2N ) , the correlation jumps substantially to r = 0.398 .
When we combine English and the target lan-guages ( CS L1 + L2N ) , the results are actually lower than just using the target languages , because of the high weight on the target language , which is not desirable for VPCs , based on the source language results .
Even for CS SVR ( L1 + L2 ) , the results ( r = 0.389 ) are slightly below the target language -only results .
This suggests that when predicting the compositionality of MWEs which are hard to identify in the source language , it may actually be better to use target languages only .
The results for string similarity ( CS string : r = 0.385 ) are similar to those for CS L2N .
However , as with the ENC dataset , when we combine string similarity and distributional similarity ( CS all ) , the results improve , and we achieve the state - of - the - art for the dataset .
In et al . ( 2003 ) .
Our method achieves state - of - the - art results in terms of overall F-score and accuracy .
GNC Results German is a morphologically - rich language , with marking of number and case on nouns .
Given that we do not perform any lemmatization or other language -specific preprocessing , we inevitably achieve low recall for the identification of noun compound tokens , although the precision should be nearly 100 % .
Partly because of the resultant sparseness in the distributional similarity method , the results for CS L1 are low ( r = 0.141 ) , although they are lower again when using target languages ( r = 0.113 ) .
However , when we combine the source and target languages ( CS L1 + L2N ) the results improve to r = 0.178 .
The results for CS SVR ( L1 + L2 ) , on the other hand , are very low ( r = 0.085 ) .
Ultimately , simple string similarity achieves the best results for the dataset ( r = 0.372 ) , and this result actually drops slightly when combined with the distributional similarities .
To better understand the reason for the lacklustre results using SVR , we carried out error analysis and found that , unlike the other two datasets , about half of the target languages return scores which correlate negatively with the human judgements .
When we filter these languages from the data , the score for SVR improves appreciably .
For example , over the best - 3 languages overall , we get a correlation score of r = 0.179 , which is slightly higher than CS L1 + L2N .
We further investigated the reason for getting very low and sometimes negative correlations with many of our target languages .
We noted that about 24 % of the German noun compounds in the dataset do not have entries in Panlex .
This contrasts with ENC where only one instance does not have an entry in Panlex , and EVPC where all VPCs have translations in at least one language in Panlex .
We experimented with using string similarity scores in the case of such missing transla-tions , as opposed to the strategy described in Section 4.2 .
The results for CS SVR ( L1 + L2 ) rose to r = 0.269 , although this is still below the correlation for just using string similarity .
Our results on the GNC dataset using string similarity are competitive with the state - of - the - art results ( r = 0.45 ) using a window - based distributional similarity approach over monolingual German data ( Schulte im Walde et al. , 2013 ) .
Note , however , that their method used part- of-speech information and lemmatisation , where ours does not , in keeping with the language - independent philosophy of this research .
Conclusion and Future Work
In this study , we proposed a method to predict the compositionality of MWEs based on monolingual distributional similarity between the MWE and each of its component words , under translation into multiple target languages .
We showed that using translation and multiple target languages enhances compositionality modelling , and also that there is strong complementarity between our approach and an approach based on string similarity .
In future work , we hope to address the question of translation sparseness , as observed for the GNC dataset .
We also plan to experiment with unsupervised morphological analysis methods to improve identification recall , and explore the impact of tokenization .
Furthermore , we would like to investigate the optimal number of stop words and content - bearing words for each language , and to look into the development of general unsupervised methods for compositionality prediction .
Table 1 : 1 The 5 best languages for the ENC , EVPC and GNC datasets .
The language family is based on Voegelin and Voegelin Dataset Language Frequency Family Italian 100 Romance French 99 Romance ENC German 86 Germanic Vietnamese 83 Viet - Muong Portuguese 62 Romance Bulgarian 100 Slavic Breton 100 Celtic EVPC Occitan 100 Romance Indonesian 100 Indonesian Slovenian 100 Slavic Polish 100 Slavic Lithuanian 99 Baltic GNC Finnish 74 Uralic Bulgarian 72 Slavic Czech 40 Slavic
