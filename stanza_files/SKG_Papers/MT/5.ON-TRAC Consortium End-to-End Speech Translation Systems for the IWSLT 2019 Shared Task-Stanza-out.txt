title
ON -TRAC Consortium End-to- End Speech Translation Systems for the IWSLT 2019 Shared Task
abstract
This paper describes the ON - TRAC Consortium translation systems developed for the end-to - end model task of IWSLT Evaluation 2019 for the English ?
Portuguese language pair .
ON - TRAC Consortium is composed of researchers from three French academic laboratories : LIA
Introduction Previous automatic speech - to- text translation ( AST ) systems operate in two steps : source language speech recognition ( ASR ) and source - to - target text translation ( MT ) .
However , recent works have attempted to build end-to - end AST without using source language transcription during learning or decoding [ 1 , 2 ] or using it at training time only [ 3 ] .
Very recently several extensions of these pioneering works were introduced : low-resource AST [ 4 ] , unsupervised AST [ 5 ] , end-to - end speech - to-speech translation ( Translatotron ) [ 6 ] .
Improvements of end-to- end AST were also proposed using weakly supervised data [ 7 ] , or by adding a second attention mechanism [ 8 ] .
This paper describes the ON - TRAC consortium automatic speech translation ( AST ) systems for the IWSLT 2019 Shared Task .
ON -TRAC Consortium is composed of researchers from three French academic laboratories : LIA ( Avignon Universit ? ) , LIG ( Universit ? Grenoble Alpes ) , and LIUM ( Le Mans Universit ? ) .
We participated to the end-to- end model English - to - Portuguese AST task on How2 [ 9 ] and MuST - C [ 10 ] datasets .
We notably try to answer to the following questions : ? Question 1 : does pooling heterogenous corpora ( How2 and MuST - C ) help the AST training ?
?
Question 2 : what is the better tokenization unit on the target side ( BPE or characters ) ?
?
Question 3 : considering that segmentation is an important challenge of AST , what is the optimal way to segment the speech input ?
?
Question 4 : does fine-tuning increase the system 's performance ?
?
Question 5 : is our end-to- end AST model better than an ASR + MT pipeline ?
This paper is organized as follows : after briefly presenting the data in Section 2 and after detailing our investigation on automatic speech segmentation in Section 3 , we present the end-to - end speech translation systems submitted by our ON - TRAC consortium in Section 4 .
Section 5 summarizes what we learned from this evaluation and Section 6 concludes this work .
Data
The corpora used in this work are the How2 [ 9 ] and MuST - C [ 10 ] corpora .
Since we focus on English-to - Portuguese AST tasks , only the English - Portuguese portion of MuST - C corpus is used .
The statistics of these two corpora , along with the corresponding provided evaluation data , can be found in Table 1 .
In order to answer to the first scientific question mentioned in Section 1 , we pool these two corpora together to create a merged corpus whose details can also be found in the same table .
Note that the statistics for the How2 training set might slightly differ from that of other participants since the original audio files for the How2 corpus are not officially available .
Since we wanted to apply our own feature extraction , instead of using the one shared by the How2 authors , we have to download the original video files from Youtube , 1 extract the audio from these downloaded video files .
One issue with this approach is that the final corpus content will depend on the availability of audio files on Youtube at the downloading date .
On July 12th , when our version of the corpus was downloaded , 21 ( out of 13,472 ) video files were missing .
We consider this as a minor loss with regard to the possibility it gives us to extract our own acoustic features .
Speech segmentation
While How2 evaluation data is distributed with a predefined segmentation , this information is not provided for the TED talks evaluation data .
In this context , we explore two different approaches to segment the MuST - C ( TED talks ) audio stream .
The first one is based on the use of the well known LIUM SpkDiarization toolkit [ 11 ] , which is an open source toolkit for speaker diarization ( we used the default configuration ) .
The second approach is based on the use of an Automatic Speech Recognition system ( ASR ) as a speech segmenter : we transcribe automatically and without segmentation all the validation and evaluation datasets with a Kaldi- based ASR system [ 12 ] trained on TEDLIUM 3 [ 13 ] .
2
We did not try to optimize the ASR system on our data .
This ASR system produces recognized words with timecodes ( start time and duration for each word ) .
Thanks to this temporal information , we are able to measure silence duration between two words when silence ( or non speech event ) exists .
When a silence between two words is higher than 0.65 seconds , we split the audio file .
When the number of words in the current speech segment exceeds 40 , this threshold is reduced to 0.15 seconds , in order to avoid exceedingly long segments .
These thresholds have been tuned in order to get a segment duration distribution in the evaluation data close to the one observed in the training data .
Table 2 summarizes statistics about segment duration on training data ( with the segmentation provided by the organizers ) and evaluation data ( ASRbased segmentation vs. speaker diarization toolkit ) .
In order to choose the segmentation process for our primary system among these two approaches , we carried out experiments on the tst-COMMON data from the MuST - C corpus .
For these experiments , we applied a preliminary version of our end-to - end system , trained on the MuST - C training data to translate speech into lower - case text .
Then , we used the mwerSegmenter tool 3 to realign our translations to the reference segmentation of the tst -COMMON data , in order to evaluate translation quality .
Those preliminary results show that ASR - based segmentation leads to better speech translation performance than the speaker diarization approach .
However , we observe that manual segmentation ( 25.50 ) still outperforms our best automatic segmentation ( 22.03 ) .
This shows that automatic segmentation of the audio stream is an important issue to address for the speech translation task .
Finally , based on these findings we decided to use the ASR - based approach for our primary system applied to the TED talks ( MuST - C ) evaluation data ( for which we do not possess manual segmentation ) .
For the How2 evaluation data , we use the manual segmentation provided by the organizers .
Speech translation systems
In this work , several speech translation systems were developed for translating English speech into Portuguese text ( EN - PT ) .
End-to - end speech translation
In this section we detail our end-to - end architecture .
All the experiments presented are conducted using the ESPnet [ 14 ] end-to - end speech processing toolkit .
Speech features .
For all models , 80 - dimensional Mel filter - bank features , concatenated with 3 - dimensional pitch features , 4 are used for training .
Features are extracted using 25 ms windows with a frame shift of 10 ms .
Cepstral mean and variance normalization is computed on the training set .
Data augmentation , based on speed perturbation with factors of 0.9 , 1.0 , and 1.1 , is applied to the training data [ 16 ] .
Text preprocessing .
Following the ESPnet speech translation recipe , we normalize punctuation , and tokenize all the Portuguese text using Moses .
5 Texts are case-sensitive and contain punctuation .
Moreover , the texts of the MuST - C corpus contain ' Laughter ' , ' Applause ' marks .
These are kept for training the model which uses only MuST - C data , but they are removed from the texts when training the models on the combination of both corpora to ensure consistency .
The development sets are generated by randomly sampling 2,000 , 2,000 , and 4,000 sentences from MuST -C , How2 and the merged corpus respectively .
These sentences are removed from the corresponding training sets .
Furthermore , to make the training feasible with our limited computational resources , training and development sentences longer than 3 , 000 frames ( ? 30s ) or 400 characters are removed .
This results in 6 % , 8 % and 7 % speech data loss for How2 , MuST - C and the merged corpus respectively .
The summarization of the training data after preprocessing can be found in Table 4 . Architecture .
We use an attention - based encoderdecoder architecture , whose encoder has two VGG - like [ 17 ]
CNN blocks followed by five stacked 1024 - dimensional BLSTM layers ( see Figure 1 ) .
The decoder has two 1024 dimensional LSTM layers .
Each VGG block contains two 2D - convolution layers followed by a 2D - maxpooling layer whose aim is to reduce both time ( T ) and frequency dimension ( D ) of the input speech features by a factor of 2 .
These two VGG blocks transform input speech features ' shape from ( T ? D ) to ( T /4 ? D/4 ) .
Bahdanau 's attention mechanism [ 18 ] is used in all our experiments .
Set
Pipeline approach ( baseline )
In this section we describe the pipeline approach for speech translation .
ASR system .
Kaldi speech recognition tookit [ 19 ] was used for this purpose .
The system used in the pipeline is close to the tedlium / s5 r3 recipe .
6
The acoustic model is trained on TEDLIUM - 3 and a subset of MuST - C corpus .
We use TDNN -F ( 11 TDNN -F layers ) structures for acoustic modeling with 40 - dimensional MFCC features .
A simple 3 - gram language model ( LM ) is trained using TEDLIUM -3 , MuST - C and How2 corpus , with SRILM toolkit [ 20 ] .
The ASR system achieved a case-insensitive Word Error Rate ( WER ) of 21.71 % and 26.89 % on Must - C tst-COMMON and How2 val sets respectively .
MT system .
we used the Transformer [ 21 ] sequence - tosequence model as implemented in fairseq [ 22 ] .
Transformer is the state of the art NMT model .
In this architecture , scaled - dot- product attention between keys , values and query vectors in multiple dimensions ( or heads ) is computed .
This is done both within encoder and decoder stacks ( multihead self attention ) and between encoder and decoder stacks ( multi-head encoder- decoder attention ) .
Our models are based on the small transformer settings using 6 stacks ( layers ) for encoder and decoder networks with an embedding layer of size 512 , a feed-forward layer with an inner dimension of 1024 , and 4 heads for the multihead attention layers .
We train the NMT system using the merged corpora ( Table 1 ) with a vocabulary of 30 K units based on a joint source and target byte pair encoding ( BPE ) [ 23 ] .
Results of the pipeline speech translation system are reported in the last line of Table 5 .
Evaluation set ASR Ref How2 val 34.23 51.37 MuST - C tst-COMMON 22.14 28.34 Table 5 : Detokenized Case-sensitive BLEU scores for different evaluation sets when translating the automatic ( ASR ) and human ( Ref ) transcription .
Experiments and lessons learned
In order to answer the scientific questions introduced in Section 1 , we conducted a series of experiments whose results are presented in Table 6 .
Question 1 : choosing the training corpus
We train three end-to - end models with the architecture described in Section 4 using three different training corpora : ( 1 ) MuST -C , ( 2 ) How2 , and ( 3 ) the merged version of the two corpora .
The target tokens are characters .
These models are then evaluated on the tst - COMMON ( MuST - C ) , and val ( How2 ) datasets , and the results are reported in the first three lines of Table 6 .
We can observe that the model trained on the merged corpora outperforms the ones trained on MuST - C ( difference of 3.32 ) and How2 ( difference of 3.11 ) .
This model ( line # 3 of the table ) is used for our IWSLT primary system submission for both evaluation datasets .
Question 2 : choosing the tokenization units
In this series of experiments , we investigate the impact of the tokenization units on the performance of the translation system .
We investigated two types of tokenization units : characters and subword units based on byte-pair encoding ( BPE ) [ 23 ] .
Using BPE units , we train four models with different vocabulary sizes : 400 , 2,000 , 5,000 and 8,000 .
Results for the models are given in Table 6 , lines # 4 - 7 , in which we observe that having fewer output tokens on the decoder side is beneficial .
We conclude that characters seem to be the best tokenization units on the MuST -C , and BPE 400 units provides the best results for the How2 task .
7
Question 3 : segmentation
We have seen in Section 3 that our ASR - based segmentation leads to better BLEU scores than using off - the-shelf speaker 7 However , since the bpe-400 result for How2 was obtained after the evaluation deadline , our official submission uses characters for both datasets ) .
diarization .
Our primary system used the ASR - based segmentation to process TED talks , while a contrastive system used speaker diarization .
We expect that the final campaign results will confirm our preliminary conclusion .
8
No
Question 4 : fine-tuning impact
We also investigate fine-tuning .
For instance , training for one more epoch on the target corpus might help to improve translation performance .
In order to verify this , we extend the training of the model which uses the merged corpora ( line # 3 in Table 6 ) for one more epoch on the How2 corpus only ( our evaluation target ) .
We investigated ( 1 ) fine -tuning both encoder and decoder ( Unfreeze , line # 8 ) and ( 2 ) fine -tuning the decoder only ( Freeze , line # 9 ) .
Results are presented at the last two lines of Table 6 .
We observe a slight but not significant gain with fine-tuning and no difference between Freeze and Unfreeze options .
Question 5 : pipeline or end-to-end
The pipeline results for both corpora are available in the last line ( # 10 ) of Table 6 .
We verify that our best end-to - end speech translation results ( lines # 3 and # 4 ) outperform this baseline model by a difference of 4.77 points for TED talks and 9.59 points for How2 .
While it is important to mention that we did not fully optimize ASR , NMT systems and their combination , 9 we find that these results highlight the performance of our end-to - end speech translation systems .
Conclusion
This paper described the ON - TRAC consortium submission to the end-to - end speech translation systems for the IWSLT 2019 shared task .
Our primary end-to- end translation model used in the IWSLT - 2019 , evaluated on the development datasets , scores the following results for case-sensitive BLEU score : 26.91 on TED talks task and 43.02 on How2 .
For the How2 task , we verified ( after the evaluation campaign deadline ) that it is possible to obtain a better result by using the model with 400 BPE units .
Figure 1 : 1 Figure 1 : Architecture of the speech encoder : a stack of two VGG blocks followed by 5 BLSTM layers .
