title
Referential Translation Machines for Predicting Translation Quality and Related Statistics
abstract
We use referential translation machines ( RTMs ) for predicting translation performance .
RTMs pioneer a language independent approach to all similarity tasks and remove the need to access any task or domain specific information or resource .
We improve our RTM models with the ParFDA instance selection model , with additional features for predicting the translation performance , and with improved learning models .
We develop RTM models for each WMT15 QET ( QET15 ) subtask and obtain improvements over QET14 results .
RTMs achieve top performance in QET15 ranking 1st in document - and sentence - level prediction tasks and 2nd in word - level prediction task .
Referential Translation Machine ( RTM ) Referential translation machines are a computational model effectively judging monolingual and bilingual similarity while identifying translation acts between any two data sets with respect to interpretants .
RTMs achieve top performance in automatic , accurate , and language independent prediction of machine translation performance and reduce our dependence on any task dependent resource .
Prediction of translation performance can help in estimating the effort required for correcting the translations during post-editing by human translators .
We improve our RTM models ( Bic ?ici and Way , 2014 ) : ? by using improved ParFDA instance selection model allowing better language models ( LM ) in which similarity judgments are made to be built with improved optimization and selection of the LM data , ? by selecting TreeF features over source and translation data jointly instead of taking their intersection , ? with extended learning models including bayesian ridge regression ( Tan et al. , 2015 ) , which did not obtain better performance than support vector regression in training results ( Section 2.2 ) .
We present top results with Referential Translation Machines ( Bic ?ici , 2015 ; Bic ?ici and Way , 2014 ) at quality estimation task ( QET15 ) in WMT15 ( Bojar et al. , 2015 ) . RTMs pioneer a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training data ( Bic ?ici and Yuret , 2015 ) as interpretants for reaching shared semantics .
RTMs use Machine Translation Performance Prediction ( MTPP ) System Bic ?ici , 2015 ) , which is a state - of - the - art performance predictor of translation even without using the translation by using only the source .
We use ParFDA for selecting the interpretants Bic ?ici and Yuret , 2015 ) and build an MTPP model .
MTPP derives indicators of the closeness of test sentences to the available training data , the difficulty of translating the sentence , and the presence of acts of translation for data transformation .
We view that acts of translation are ubiquitously used during communication :
Every act of communication is an act of translation ( Bliss , 2012 ) .
Figure 1 depicts RTM .
Our encouraging results in QET provides a greater understanding of the acts of translation we ubiquitously use and how they can be used to predict the performance of translation .
RTMs are powerful enough to be applicable in different domains and tasks while achieving top performance .
Table 1 : Number of sentences in different tasks .
RTM in the Quality Estimation Task
We participate in all of the three subtasks of the quality estimation task ( QET ) ( Bojar et al. , 2015 ) , which include English to Spanish ( en-es ) , English to German ( en-de ) , and German to English ( deen ) translation directions .
There are three subtasks : sentence - level prediction ( Task 1 ) , wordlevel prediction ( Task 2 ) , and document- level prediction ( Task 3 ) .
Task 1 is about predicting HTER ( human- targeted translation edit rate ) ( Snover et al. , 2006 ) scores of sentence translations , Task 2 is about binary classification of word-level quality , and Task 3 is about predicting METEOR ( Lavie and Agarwal , 2007 ) scores of document translations .
Instance selection for the training set and the language model ( LM ) corpus is handled by ParFDA , whose parameters are optimized for each translation task .
LM are trained using SRILM ( Stolcke , 2002 ) .
We tokenize and truecase all of the corpora using code released with Moses ( Koehn et al. , 2007 ) 1 . Table 1 lists the number of sentences in the training and test sets for each task .
1 mosesdecoder / scripts /
RTM Prediction Models and Optimization
We present results using support vector regression ( SVR ) with RBF ( radial basis functions ) kernel ( Smola and Sch?lkopf , 2004 ) for sentence and document translation prediction tasks and Global Linear Models ( GLM ) ( Collins , 2002 ) with dynamic learning ( GLMd ) ( Bic ?ici , 2013 ; Bic ?ici and Way , 2014 ) for word -level translation performance prediction .
We also use these learning models after a feature subset selection ( FS ) with recursive feature elimination ( RFE ) ( Guyon et al. , 2002 ) or a dimensionality reduction and mapping step using partial least squares ( PLS ) ( Specia et al. , 2009 ) , or PLS after FS ( FS + PLS ) .
GLM relies on Viterbi decoding , perceptron learning , and flexible feature definitions .
GLMd extends the GLM framework by parallel perceptron training ( McDonald et al. , 2010 ) and dynamic learning with adaptive weight updates in the perceptron learning algorithm : w = w + ? (?( x i , y i ) ? ?( x i , ? ) ) , ( 1 ) where ?
returns a global representation for instance i and the weights are updated by ? , which dynamically decays the amount of the change during weight updates at later stages and prevents large fluctuations with updates .
The learning rate updates the weight values with weights in the range [ a , b] using the following function taking error rate as the input : f ( x ) = ( log a b ?
1 ) x 2 + 1 ( 2 ) Learning rate curve for a = 0.5 and b = 1.0 is provided in Figure 2 :
Training Results
We use mean absolute error ( MAE ) , relative absolute error ( RAE ) , root mean squared error ( RMSE ) , and correlation ( r ) as well as relative MAE ( MAER ) and relative RAE ( MRAER ) to evaluate ( Bic ?ici , 2015 ; Bic ?ici , 2013 ) .
MAER is mean absolute error relative to the magnitude of the target and MRAER is mean absolute error relative to the absolute error of a predictor always predicting the target mean assuming that target mean is known ( Bic ?ici , 2015 ) 2012 ) calculates the average quality difference between the top n?1 quartiles and the overall quality for the test set .
Table 2 presents the training results for Task 1 and Task 3 .
Table 3 presents
Task 2 training results .
We refer to GLMd parallelized over 4 splits as GLMd s4 and GLMd with 5 splits as GLMd s5 .
Test Results Task 1 : Predicting the HTER for Sentence Translations
The results on the test set are given in Table 4 . Rank lists the overall ranking in the task out of about 9 submissions .
We obtain the rankings by sorting according to the predicted scores and randomly assigning ranks in case of ties .
RTMs with FS followed by PLS and learning with SVR is able to achieve the top rank in this task .
Task 2 : Prediction of Word-level Translation Quality
Task 2 is about binary classification of word-level quality .
We develop individual RTM models for each subtask and use GLMd model ( Bic ?ici , 2013 ; Bic ?ici and Way , 2014 ) , for predicting the quality at the word-level .
The results on the test set are in Table 5 where the ranks are out of about 17 submissions .
RTMs with GLMd becomes the second best system this task .
Task 3 : Predicting METEOR of Document Translations
Task 3 is about predicting ME-TEOR ( Lavie and Agarwal , 2007 ) and their ranking .
The results on the test set are given in Table 4 where the ranks are out of about 6 submissions using wF 1 . RTMs achieve top rankings in this task .
RTMs Across Tasks and Years
We compare the difficulty of tasks according to MRAER levels achieved .
In Table 6 , we list the RTM test results for tasks and subtasks that predict HTER or METEOR from QET15 , QET14 ( Bic ?ici and Way , 2014 ) , and QET13 ( Bic ?ici , 2013 ) .
The best results when predicting HTER are obtained this year .
Conclusion Referential translation machines achieve top performance in automatic , accurate , and language independent prediction of document - , sentence - , and word - level statistical machine translation ( SMT ) performance .
RTMs remove the need to access any SMT system specific information or prior knowledge of the training data or models used when generating the translations .
RTMs achieve top performance when predicting translation performance .
( Bic ?ici and Way , 2014 ) and QET13 ( Bic ?ici , 2013 ) . Figure 1 : 1 Figure 1 : RTM depiction .
