title
Samsung R&D Institute Poland submission to WMT20 News Translation Task
abstract
This paper describes the submission to the WMT20 shared news translation task by Samsung R&D Institute Poland .
We submitted systems for six language directions : English to Czech , Czech to English , English to Polish , Polish to English , English to Inuktitut and Inuktitut to English .
For each , we trained a single-direction model .
However , directions including English , Polish and Czech were derived from a common multilingual base , which was later fine -tuned on each particular direction .
For all the translation directions , we used a similar training regime , with iterative training corpora improvement through backtranslation and model ensembling .
For the En ?
Cs direction , we additionally leveraged document-level information by re-ranking the beam output with a separate model .
Introduction Since the Transformer architecture became the standard model in Neural Machine Translation , recent advancements in the field have come from two techniques .
The first one is deepening the model by adding more layers , mainly in the encoder part , in order to model more complex dependencies ( Raganato and Tiedemann , 2018 ; Wang et al. , 2019a ; Wu et al. , 2019 ) .
This , however , poses problems during the training - too deep models are much harder to train due to the gradient vanishing problem ( Zhang et al. , 2019 ) .
The second technique consists in improving the quality of training data by removing spurious translations ( Koehn et al. , 2019 ) and making the data easier to learn through the teacher -student methodology ( Hinton et al. , 2015 ; Kim and Rush , 2016 ; .
In this submission , we decided to leverage both techniques .
We deepened the model with a lexicalshortcuts transformer modification .
We also iteratively improved the synthetic corpora by train-ing better and better translation models , backtranslating and distilling the data in each step .
The remainder of this paper is structured as follows :
Section 2 introduces the data used for training , Section 3 shows baseline NMT models and our experiments .
In Section 4 we describe our training regime and results .
Section 5 is for conclusions .
Data
Data Filtering
We used all the parallel data available in the constrained settings .
We filtered the parallel data with a two-step process .
First , we used a simple heuristics for general clean- up : ? remove pairs where any of the sentences is longer than 1500 characters ?
remove sentences with characters not in the Unicode range specific to a given language pair ?
remove pairs based on a length- ratio threshold .
We then de-duplicated the data and used the fastalign 1 tool to filter out pairs basing on the alignment probability between the source and the target ( Table 1 ) .
For monolingual data , we used only the general clean - up procedure .
Data Pre-Processing
We used the normalize - punctuation .
perl 2 script from the Moses package on all the training data .
For the En ?
Iu directions , we used the alignment provided by the organizers , and decided to stick to the Inuktitut syllabics , without romanization .
For tokenization and segmentation , we used Sen-tencePiece 3 ( Kudo and Richardson , 2018 ) .
For the En ? Cs and En ?
Pl directions , we started with a multilingual translation model that was later specialized towards each direction separately .
For these 3 languages , we had to use a single , joint vocabulary with 32 , 000 pieces and a unigram language model ( ULM ) tokenization scheme .
For the En ?
Iu directions , we used a joint vocabulary with the ULM tokenization scheme and 16 , 000 pieces .
NMT System Overwiev
All of our systems are trained with the Marian NMT 4 ( Junczys - Dowmunt et al. , 2018 ) framework .
Baseline systems for En ? Cs and En ? Pl
We started with strong baselines , i.e. transformer models ( Vaswani et al. , 2017 ) , which we will now refer to as transformer - big .
This model consists of 6 encoder layers , 6 decoder layers , 16 heads , a model / embedding dimension of 1024 and a feedforward layer dimension of 4096 .
The model is regularized with a dropout between transformer layers of 0.2 and a label smoothing of 0.1 .
We also used layer normalization ( Lei Ba et al. , 2016 ) and tied the weights of the target - side embedding and the transpose of the output weight matrix , as well as source - and target - side embeddings ( Press and Wolf , 2017 ) .
Optimizer delay was used to simulate bigger batches , updating weights every 16 batches , Adam ( Kingma and Ba , 2015 ) was used as an optimizer , parametrized with a learning rate of 0.0003 and linear warm - up for the initial 32 , 000 updates with subsequent inverted squared decay .
For each language pair , we trained both uni-and bi-directional models .
We also examined the effect of using multilingual data to train a quadrodirectional model on concatenated En ? Cs and En ?
Pl corpora .
The En ?
Pl corpora were upsampled 5 times to match size .
< 2 XX > tokens were appended to each sentence to indicate the target language .
The results on newsdev2020 are presented in Table 2 .
Baseline system for En ?
Iu
As the parallel corpora for En ?
Iu are significantly smaller than for the other pairs , we decided to start with a transformer model with a smaller number of parameters i.e. transformer - base .
All our base models were bi-directional .
The model consists of 6 encoder layers , 6 decoder layers , 8 heads , a model / embedding dimension of 512 and a feed-forward layer dimension of 2048 .
We examined the effect of vocabulary size on the model quality , and obtained the best results for the vocabulary size of 16 , 000 ( Table 4 ) .
Basing on our previous experience , we also examined an During training , we randomly cropped up to 25 % tokens from each sentence , and taught the model to predict the original sequence .
We used the same architecture as in baseline trainings .
Next , we used the best checkpoint to warm-start training on the parallel data .
Table 3 presents our results for varying sizes of the training corpus ( the smaller corpus is a random subset of the parallel data ) .
We observe that , although our implementation works well for low-resource setting , it leads to quality drop when all the parallel data is used .
Accordlingly , we used this pre-training method only for the En ?
Iu directions .
Lexical Shortcuts
Since our quadro-directional model showed promising results , we decided to try to examine the effect of deepening and enlarging the model .
We increased the feed -forward layer dimension to 8192 , and the number of encoder layers to 12 .
The rest of the parameters is the same as in transformer - big .
coding for back - translation much faster .
To help with gradient propagation , we implemented Lexical Shortcuts ( Emelin et al. , 2019 ) in the encoder .
We used the feature-fusion version of the gating mechanism .
The results are summarized in the Quadro-huge column in Table 2 .
This model outperformed the baseline in all the directions , except one .
We decided to use this system in further trainings .
Back -Translation with Language Model Back-translation ( Sennrich et al. , 2016 ) is a common strategy of utilizing monolingual data in training NMT systems .
For English and Czech , the amount of monolingual in-domain data in the Newscrawl data set is big enough , so for this language pair we used only the monolingual set .
Yet for Polish , the Newscrawl is very limited in size , hence we decided to use Moore - Lewis filtering ( Moore and Lewis , 2010 ) to extract in- domain data from CommonCrawl .
With this additional monolingual corpus , we had over 80 M in- domain news sentences for each language ( Table 5 ) .
We used those monolingual datasets to train an in- domain RNN - style language model for each of the three languages , using the same common vocabulary as the one in the translation models .
This allowed us to easily ensemble this language model with a translation model during decoding , as described in Gulcehre et al . ( 2015 ) .
For each iteration of the back - translation , we used an ensemble of the top 4 NMT models available w.r.t. the dev-set score for the particular direction and the in-domain language model .
The weights of the models were optimized through a grid-search .
Noisy Channel Model Reranking Re-ranking the beam output is a method used to improve translation quality by the re-scoring hypothesis from a forward model .
The noisy channel model approach was used with success by Facebook in their submission to the WMT19 news translation task ( Ng et al. , 2019 ) . Based on the Bayes ' rule , given a target sequence y and a source sentence x , for every hypothesis from the beam output , we calculate log P ( y | x ) + ?
1 log P ( x | y ) + ? 2 log P ( y ) and use this score to re-rank the beam outputs .
We model P ( y | x ) with the forward model , P ( x | y ) with the backward model and P ( y ) with the language domain model .
The weights ?
1 and ?
2 are tuned on the dev-set .
When we used this method for our baseline unidirectional systems , we noticed significant BLEU improvements : 26.9 ( + 0.8 ) on newsdev2020 for the En ?
Pl direction .
However , there was no improvement when applied to translations produced with strong ensembles of both the domain language models and the translation models , trained on the back - translated data .
In our final submission , we used this method only for the Iu ?
En and En ?
Iu directions .
Multi-Agent Dual Learning
In our submission , we used the simplified version of Multi-Agent Dual Learning ( MADL ) ( Wang et al. , 2019 b ) , proposed in Kim et al . ( 2019 ) , to generate additional training data from the parallel corpus .
We generated n-best translations of both the source and the target sides of the parallel data , with strong ensembles of , respectively , the forward and the backward models .
Next , we picked the best translation from among n candidates w.r.t. the sentence - level BLEU score .
Thanks to these steps , we tripled the number of sentences by combining three types of datasets : 1. original source - original target , 2 . original source - synthetic target , 3 . synthetic source - original target , where the synthetic target is the translation of the original source with the forward model , and the synthetic source is the translation of the original target with the backward model .
Document Level Reranking For the En ?
Cs translation directions , the training data is aligned on the document level .
To make use of this information , we implemented the method presented in Voita et al . ( 2019 ) .
The method assumes one has access to consecutive tuples of sentences in the target language .
Using the backward and forward models , one should translate the tuples with the sentence - level based systems , and then train the model to predict the original tuple , basing on the two -way translated data .
As we already have access to the document-level aligned translations from the CzEng 2.0 corpus ( Kocmi et al. , 2020 ) , we could do the translation just once .
We experimented only with the En ?
Cs direction .
We selected tuples of 4 consecutive sentences in English , translated each sentence independently , and glued the translations back together .
We used a special token to indicate the end of the sentence .
See Table 9 in the Appendix for an example of the training data .
However , when we utilized this model to " repair " the newsdev2020 dev-set translations , we noticed a quality drop .
We decided to try a different approach , and used the document - level repair model to re-rank the beam output .
The procedure is similar to a greedy search for the best path through n-best lists of forward model translations .
It is described with Algorithm 1 . i += 1
Algorithm 1 Document Level Reranking 12 : end for 13 : end for Although on the dev-set we did n't see much difference in the BLEU score , manual inspection showed some promising results .
We decided to apply this method to our best-scoring system and saw a 0.1 improvement in the BLEU score on the test-set .
Post-Processing
For all the translation directions we participated in , we normalized the system outputs with a series of regular expressions : ? substitute English quotation marks ( " ... " ) with Czech / Polish ones ( , , ... " ) , ? if a source starts / ends with a quotation mark , we make sure so does the translation , ? remove word repetitions , ? replace consecutive sequences of whitespaces with a single one , ? if a source ends with a punctuation mark ( e.g. ?.! ) , we substitute the last character of the translation with it , ? replace three consecutive dots with an ellipsis , ? replace hyphens with en dashes .
Results
English ?
Polish
The model for the English ?
Polish direction was derived from the multilingual quadro-huge model - similarly to the other models for directions with Polish , Czech or English .
The successive steps and respective BLEU scores are reported in Table 6 .
We started with fine-tuning the quadrodirectional model on the parallel data for the specific direction .
Next , we used an ensemble of our best models to back - translate Newscrawl 2018 and 2019 , we filtered it ( 3.5 M sentences ) and merged with the parallel corpus ( 8.6M ) .
The fine-tuning gave us + 1.5 BLEU improvement .
We were able to achieve an additional + 0.9 BLEU with the rulebased post-processing ( see above ) .
In the next step , we used the MADL procedure to generate additional data .
To further increase the amount of data and its variability , we picked the top 2 best translations , according to the sentence - level BLEU in the distillation process - instead of choosing just one .
Again , we up-sampled the original parallel corpus twice .
This procedure gave additional 52 M sentences ( a 6 - fold increase ) .
We back - translated all the monolingual indomain data ( i.e. 89 M after filtration ) and used both corpora to fine - tune the next generation model .
We augmented the data by randomly masking up to 10 % of the input tokens with a random punctuation mark , and observed yet another performance boost .
Using all these corpora , we trained another model from scratch , hoping to get a less correlated model .
Although the fresh model performance was poorer than the previous best ( 30.2 BLEU vs. 31.6 BLEU ) , the grid-search ensemble optimization included it in the best ensemble .
As a final step , we used Moore - Lewis filtering to choose 1M Newscrawl sentences that were closest to the concatenated newsdev2020 and newstest2020 .
We translated them with the best ensemble , and used it to finetune our best-performing model .
Again , we ran ensemble optimization including this model into the models reservoir .
The optimal ensemble was the one we submitted as the primary system .
Polish ?
English
For the Polish to English direction , we proceeded similarly to our solution for English to Polish .
We started with the quadro-huge model .
We backtranslated Newscrawl 2018 , filtered it ( 12 M sentences ) and merged with the parallel corpus ( 8.6M ) .
We kept on training the fine-tuned quadro-huge model , increasing the performance by 0.9 BLEU .
We used the same MADL procedure as before , distilling 2 best translations for each source sentence .
We also back - translated Newscrawl 2007 - 2017 ( 144 M ) and merged it with the MADL corpus .
With this corpus , single model performance increased by + 0.7 BLEU .
We used the same corpus to train a fresh model .
Similarly to the English to Polish direction , the fresh model performed poorer ( 32.9 BLEU ) than the fine-tuned one ( 34.4 BLEU ) , but - again - in ensemble it gave additional improvement .
Finally , we fine-tuned on the 1M corpora filtered out from Newscrawl in the domain of the concatenated newsdev2020 and newstest2020 , and ensembled for the final submission .
English ?
Czech
We started with fine-tuning the quadro-huge model with only English to Czech parallel data and ensembling several models into one .
This model specialization gave us + 1.4 BLEU .
Next , we backtranslated Newscrawl 2018 and 2019 in two flavors : normally , and with adding Gumbel noise ( -- output-sampling in Marian ) .
Then , we filtered the result ( see section 2.1 ) , obtaining 35 M sentences .
With this additional corpus , the single model performance improved by 0.9 BLEU .
In contrast to the Cs ?
En direction , using backtranslations from CzEng 2.0 seemed to hurt the model performance .
In the next iteration , we produced the MADL corpus ( 120M ) and merged it with back - translated Newscrawl 2009 - 2017 ( 102 M , with and without noise ) and used this data to train yet another model .
Finally , we ensembled this model with models trained from scratch and fine-tuned on the 1M from Newscrawl common with the concatenated news - dev2020 and newstest 2020 .
Before the last stepdocument level re-ranking - we used the sentencesplitter from NLTK ( Bird et al. , 2009 ) to preprocess the testset .
It was required because of our systems being trained with sentence - level data and in newstest2020 some of the segments contain multiple sentences .
We translated the pre-processed testset with the best ensemble , re-ranked on the document level and finally glued back the translations together .
The document level re-ranking gave us - 0.1 BLEU on the dev-set but + 0.1 on the test-set .
Czech ?
English Again , the specialization of the quadro-huge model with only Czech to English data gave us almost 1 BLEU gain in performance .
Next , we backtranslated Newscrawl 2018 and 2019 and filtered it with our pipeline , obtaining 49 M sentences .
We added the Newscrawl translations from CzEng 2.0 ( 79M ) and the original filtered parallel corpus ( 43 M ) , ending up with 171 M parallel sentences as our training set .
Using this data , we improved the single model performance by 3.6 BLEU .
Fine - tuning on the MADL corpus ( 120M ) and the backtranslated Newscrawl 2017 ( 25 M ) gave additional + 1.2 BLEU on the single model .
Finally , we ensembled them with a model trained from scratch and fine-tuned on the 1M sentences from Newscrawl that were similar to the concatenated newsdev2020 and newstest 2020 w.r.t. the Moore - Lewis score .
For sentence splitting , we used the same splitter as for En ? Cs. Results on the previous test-sets of the final systems for the En ?
Cs directions , without document level re-ranking , in the Appendix ( Table 10 ) .
English ?
Inuktitut
In contrast to all the other directions , for Inuktitut we had much less monolingual data ( 10 k after cleaning ) than bitext ( 1.1M ) .
In the first step , we back - translated the monolingual data with beam 10 , and kept all the possible variants ( 0.1 M sentences ) .
We also back - translated the English Europarl v10 corpus ( 2 M ) , because we believed it to help with the Hansard ( Joanis et al. , 2020 ) part of the dev-and test-sets .
We merged it with the two -directional parallel data ( 2.2 M ) and trained a bi-directional model , from scratch .
We used it for the general first iteration of the MADL corpus ( 6.6 M ) , and used all of the data to once more train a model from scratch .
Here we examined the effect of pre-training .
We used the sample ( 10M ) from the English Newscrawl 2019 and the Inuktitut part of the parallel data , up -sampled 5 times ( 5.5M ) .
With the pipeline approach , fine - tuning on bitext was giving us similar results as training on bitext from scratch .
Nevertheless , we were however able to achieve some improvement , when training a fresh model on the merged parallel and noised monolingual data .
We were able to achieve further improvement with increased model size - 1024 embedding dimension , 4096 forward dimension and 16 heads .
Next , we started fine-tuning on each direction independently , using the parallel data for En ?
Iu , and 20 - times up -sampled the parallel data ( 22 M ) together with the back - translated Newscrawl 2018 and 2019 ( 48 M ) for Iu ?
En .
Then , we used an ensemble of models to once again generate the MADL corpus , use it to fine - tune the unidirectional models and the ensemble once again .
We used the Noisy Channel Reranking method and saw some improvement on both the dev-set and the test-set .
Conclusions
In this paper , we have described the submission to the WMT20 shared news translation task by Samsung R&D Institute Poland .
All submitted systems were constrained and utilized only the permitted data .
With our approach , we were able to leverage two important techniques that improve the translation quality .
One method was deepening the model , while still being able to train it effectively .
The other one was filtering and improving the quality of the training data and producing high quality synthetic data .
Our iterative approach of improving the training data and improving the translation model proved to be successful , showing gradual increase in the BLEU scores .
A Appendix source " To tys vymyslel mihotav ?
reflektory ? " < SEP > " ... Ne. < SEP >
V nanouffu nejsem moc dobr ?. < SEP > P?i?el na n? m? j p?tel z Lond?na. target " A vymyslel jste taky ty reflektorky ? "
< SEP > " Ne. < SEP >
V nanotechnologii tak dobr ?
nejsem .
< SEP >
S t?m p?i?el jeden m? j zn?m? z Lond?na. source A na ?est sv?ho domu , prohla? uji , ?e m?j milovan ?
Robert .. . . - Ur?it ?? < SEP > Rad ? i se podepi ?
s Jaimem Lannisterem , Kr?lokatem .
< SEP >
To m?sto je nudn ?.
< SEP > Pros?m , Andrewe .
target " A j? prohla?uji na ?est sv?ho rodu , ?e m?j milovan ?
bratr Robert ... " < SEP >
Dej tam serem Jaimem Lannisterem , Kr?lokatem .
< SEP >
Tohle m?sto p?chne ... < SEP > Pros ?m! source ?ekla jsem : " ? byl n?pad ? " < SEP >
Jejich modr ?
o? i byly jasn ?
jako plaveck ?
baz ?n. < SEP >
" To p?i?el to ?en? Ernesto . " vzd?len ?
Dezert ?r nebo lh ?. < SEP >
Byli jste dost dob ? p?tel ? ? " target " ? to byl n?pad ? " zeptala jsem se .
< SEP >
Pod?val se na m?
zp? ma a jeho modr ?
o? i byly pr?zra?n?
jak stud?nky . < SEP >
" Earnesto s t?m p?i?el . " < SEP >
" Byli jste dob ? kamar ?di ? "
Input : { trn b ( s j ) }
- n- best list ( b = 1..N ) with translations of sentence s j Input : L repair ( { sa j } j=1..4 , { sb j } j=1..4 ) ?
likelihood computed with repair model for two 4 sentence sequences Output : Re-ranked translations rep 1 : for all paragraph in test- set do rep i?3 , . . . , rep i?1 , trn 1 ( s i ) 8 : seq b = rep i?3 , . . . , rep i?1 , trn b ( s i )
Table 1 : 1 Number of sentences in the parallel corpus originally , after simple rule- based cleaning - up , and after filtering out sentence pairs based on alignment probability .
Table 5 : 5 Number of sentences in monolingual datasets after clean- up and domain- based filtering .
Pl En Cs Newscrawl 3.7 M 230M 80.5 M + Moore-Lewis 96.5 M - - demonstrated that , with a fixed number of layers , it was more efficient to have a deeper encoder than decoder .
It also makes de -
Table 6 : 6 Successive improvements in the BLEU scores on the English ? Polish and Polish ?
English directions , computed with SacreBLEU .
newsdev2020
Table 7 : 7 Successive improvements in the BLEU scores on the English ? Czech and Czech ?
English directions , computed with SacreBLEU .
newsdev2020
Table 8 : 8 Successive improvements in the BLEU scores on the English ? Inuktitut and Inuktitut ?
English directions , computed with SacreBLEU .
newsdev2020 System En ? Iu En ? Iu Baseline 15.3 28.3 + BT 15.5 29.9 + MADL 15.5 30.4 + masked mono 15.8 30.5 + transformer - big 15.8 32.2 + fine-tune 15.8 32.5 + ensemble 16.2 32.7 + MADL2 15.8 32.7 + ensemble 16.3 32.9 + noisy channel 16.4 33.2 newstest2020 WMT '20 SUBMISSION 11.0 25.6
Table 9 : 9 Example of the training data used to train the document- level re-rank model .
Target is a quadruple of consecutive sentences extracted from the CzEng 2.0 parallel corpus .
Source is a translation of the matching English sequence , produced on the sentence level .
En ? Cs Cs ?
En newstest2019 WMT '19 best SRPOL '20 29.9 31.3 ( + 1.4 ) -- newstest2018 WMT '18 best SRPOL '20 26.0 27 . 4 ( + 1.4 ) 35.3 ( + 1.4 ) 33.9 newstest2017 WMT '17 best SRPOL '20 26.1 27.7 ( + 1.6 ) 35.1 ( + 4.2 ) 30.9
Table 10 : 10 SacreBLEU scores of the final systems for the En ?
Cs directions , without document level re-ranking , on test-sets from previous years .
github.com/clab/fast_align 2 github.com/moses-smt/mosesdecoder / scripts/ tokenizer / normalize-punctuation .
perl
github.com/google/sentencepiece 4 github.com/marian-nmt/marian
