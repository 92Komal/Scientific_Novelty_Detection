title
Bi-Directional Differentiable Input Reconstruction for Low-Resource Neural Machine Translation
abstract
We aim to better exploit the limited amounts of parallel text available in low-resource settings by introducing a differentiable reconstruction loss for neural machine translation ( NMT ) .
This loss compares original inputs to reconstructed inputs , obtained by backtranslating translation hypotheses into the input language .
We leverage differentiable sampling and bi-directional NMT to train models end-to-end , without introducing additional parameters .
This approach achieves small but consistent BLEU improvements on four language pairs in both translation directions , and outperforms an alternative differentiable reconstruction strategy based on hidden states .
Introduction Neural Machine Translation ( NMT ) performance degrades sharply when parallel training data is limited ( Koehn and Knowles , 2017 ) .
Past work has addressed this problem by leveraging monolingual data ( Sennrich et al. , 2016a ; Ramachandran et al. , 2017 ) or multilingual parallel data ( Zoph et al. , 2016 ; Johnson et al. , 2017 ; Gu et al. , 2018a ) .
We hypothesize that the traditional training can be complemented by better leveraging limited training data .
To this end , we propose a new training objective for this model by augmenting the standard translation cross-entropy loss with a differentiable input reconstruction loss to further exploit the source side of parallel samples .
Input reconstruction is motivated by the idea of round-trip translation .
Suppose sentence f is translated forward to e using model ?
f e and then translated back to f using model ?
ef , then e is more likely to be a good translation if the distance between f and f is small ( Brislin , 1970 ) .
Prior work applied round-trip translation to monolingual examples and sampled the intermediate translation e from a K-best list generated by model ?
f e using beam search ( Cheng et al. , 2016 ; .
However , beam search is not differentiable which prevents back - propagating reconstruction errors to ? f e .
As a result , reinforcement learning algorithms , or independent updates to ?
f e and ?
ef were required .
In this paper , we focus on the problem of making input reconstruction differentiable to simplify training .
In past work , Tu et al . ( 2017 ) addressed this issue by reconstructing source sentences from the decoder 's hidden states .
However , this reconstruction task can be artificially easy if hidden states over-memorize the input .
This approach also requires a separate auxiliary reconstructor , which introduces additional parameters .
We propose instead to combine benefits from differentiable sampling and bi-directional NMT to obtain a compact model that can be trained endto-end with back - propagation .
Specifically , ? Translations are sampled using the Straight - Through Gumbel Softmax ( STGS ) estimator ( Jang et al. , 2017 ; Bengio et al. , 2013 ) , which allows back - propagating reconstruction errors . ?
Our approach builds on the bi-directional NMT model ( Niu et al. , 2018 ; Johnson et al. , 2017 ) , which improves low-resource translation by jointly modeling translation in both directions ( e.g. , Swahili ? English ) .
A single bi-directional model is used as a translator and a reconstructor ( i.e. ? ef = ? f e ) without introducing more parameters .
Experiments show that our approach outperforms reconstruction from hidden states .
It achieves consistent improvements across various low-resource language pairs and directions , showing its effectiveness in making better use of limited parallel data .
Background Using round-trip translations ( f ? e ? f ) as a training signal for NMT usually requires auxiliary models to perform back -translation and cannot be trained end-to - end without reinforcement learning .
For instance , Cheng et al . ( 2016 ) added a reconstruction loss for monolingual examples to the training objective .
evaluated the quality of e by a language model and f by a reconstruction likelihood .
Both approaches have symmetric forward and backward translation models which are updated alternatively .
This require policy gradient algorithms for training , which are not always stable .
Back- translation ( Sennrich et al. , 2016a ) performs half of the reconstruction process , by generating a synthetic source side for monolingual target language examples : e ? f .
It uses an auxiliary backward model to generate the synthetic data but only updates the parameters of the primary forward model .
Iteratively updating forward and backward models ( Zhang et al. , 2018 ; Niu et al. , 2018 ) is an expensive solution as back - translations are regenerated at each iteration .
Prior work has sought to simplify the optimization of reconstruction losses by side-stepping beam search .
Tu et al . ( 2017 ) first proposed to reconstruct NMT input from the decoder 's hidden states while Wang et al . ( 2018 a , b ) suggested to use both encoder and decoder hidden states to improve translation of dropped pronouns .
However , these models might achieve low reconstruction errors by learning to copy the input to hidden states .
To avoid copying the input , Artetxe et al . ( 2018 ) and Lample et al . ( 2018 ) used denoising autoencoders ( Vincent et al. , 2008 ) in unsupervised NMT .
Our approach is based instead on the Gumbel Softmax ( Jang et al. , 2017 ; Maddison et al. , 2017 ) , which facilitates differentiable sampling of sequences of discrete tokens .
It has been successfully applied in many sequence generation tasks , including artificial language emergence for multiagent communication ( Havrylov and Titov , 2017 ) , composing tree structures from text ( Choi et al. , 2018 ) , and tasks under the umbrella of generative adversarial networks ( Goodfellow et al. , 2014 ) such as generating the context-free grammar ( Kusner and Hern?ndez-Lobato , 2016 ) , machine comprehension ( Wang et al. , 2017 ) and machine translation ( Gu et al. , 2018 b ) .
Approach NMT is framed as a conditional language model , where the probability of predicting target token e t at step t is conditioned on the previously generated sequence of tokens e <t and the source sequence f given the model parameter ?.
Suppose each token is indexed and represented as a one- hot vector , its probability is realized as a softmax function over a linear transformation a( h t ) where h t is the decoder 's hidden state at step t : P ( e t |e <t , f ; ? ) = softmax ( a( h t ) ) e t . ( 1 )
The hidden state is calculated by a neural network g given the embeddings of the previous target tokens e <t in the embedding matrix E(e < t ) and the context c t coming from the source : h t = g ( E( e < t ) , c t ) .
( 2 ) In our bi-directional model , the source sentence can be either f or e and is respectively translated to e or f .
The language is marked by a tag ( e.g. , < en > ) at the beginning of each source sentence ( Johnson et al. , 2017 ; Niu et al. , 2018 ) .
To facilitate symmetric reconstruction , we also add language tags to target sentences .
The training data corpus is then built by swapping the source and target sentences of a parallel corpus and appending the swapped version to the original .
Bi-Directional Reconstruction
Our bi-directional model performs both forward translation and backward reconstruction .
By contrast , uni-directional models require an auxiliary reconstruction module , which introduces additional parameters .
This module can be either a decoder- based reconstructor ( Tu et al. , 2017 ; Wang et al. , 2018 a , b) or a reversed dual NMT model ( Cheng et al. , 2016 ; Wang et al. , 2018c ; Zhang et al. , 2018 ) .
Here the reconstructor , which shares the same parameter with the translator T ( ? ) , can also be trained end-to -end by maximizing the loglikelihood of reconstructing f : L R = f log P ( f | T ( f ; ? ) ; ? ) , ( 3 ) Combining with the forward translation likelihood L T = ( f e ) log P ( e | f ; ? ) , ( 4 ) we use L = L T +L R as the final training objective for f ? e. The dual e ? f model is trained simultaneously by swapping the language direction in bi-directional NMT .
Reconstruction is reliable only with a model that produces reasonable base translations .
Following prior work ( Tu et al. , 2017 ; Cheng et al. , 2016 ) , we pre-train a base model with L T and fine-tune it with L T + L R .
Differentiable Sampling
We use differentiable sampling to side-step beam search and back - propagate error signals .
We use the Gumbel - Max reparameterization trick ( Maddison et al. , 2014 ) to sample a translation token at each time step from the softmax distribution in Equation 1 : e t = one- hot arg max k a( h t ) k + G k ( 5 ) where G k is i.i.d. and drawn from Gumbel ( 0 , 1 ) 1 .
We use scaled Gumbel with parameter ? , i.e. Gumbel ( 0 , ? ) , to control the randomness .
The sampling becomes deterministic ( which is equivalent to greedy search ) as ? approaches 0 .
Since arg max is not a differentiable operation , we approximate its gradient with the Straight - Through Gumbel Softmax ( STGS ) ( Jang et al. , 2017 ; Bengio et al. , 2013 ) : ? ? e t ? ? ? ?t , where ?t = softmax ( a( h t ) + G ) / ? ( 6 ) As ? approaches 0 , softmax is closer to arg max but training might be more unstable .
While the STGS estimator is biased when ? is large , it performs well in practice ( Gu et al. , 2018 b ; Choi et al. , 2018 ) and is sometimes faster and more effective than reinforcement learning ( Havrylov and Titov , 2017 ) .
To generate coherent intermediate translations , the decoder used for sampling only consumes its previously predicted ?<t .
This contrasts with the usual teacher forcing strategy ( Williams and Zipser , 1989 ) , which always feeds in the groundtruth previous tokens e <t when predicting the current token ?t .
With teacher forcing , the sequence concatenation [ e <t ; ?t ] is probably coherent at each time step , but the actual predicted sequence [ ?
<t ; ?t ] would break the continuity .
2 1 i.e. G k = ? log (? log ( u k ) ) and u k ? Uniform ( 0 , 1 ) .
Experiments
Tasks and Data
We evaluate our approach on four low-resource language pairs .
Parallel data for Swahili?
English ( SW?EN ) , Tagalog ?
English ( TL?EN ) and Somali? English ( SO?EN ) contains a mixture of domains such as news and weblogs and is collected from the IARPA MATERIAL program 3 , the Global Voices parallel corpus 4 , Common Crawl ( Smith et al. , 2013 ) , and the LORELEI Somali representative language pack ( LDC2018T11 ) .
The test samples are extracted from the held- out ANALYSIS set of MATERIAL .
Parallel Turkish ?
English ( TR?EN ) data is provided by the WMT news translation task ( Bojar et al. , 2018 ) .
We use pre-processed " corpus " , " newsdev2016 " , " newstest2017 " as training , development and test sets .
5
We apply normalization , tokenization , truecasing , joint source- target BPE with 32,000 operations ( Sennrich et al. , 2016 b ) and sentencefiltering ( length 80 cutoff ) to parallel data .
Itemized data statistics after preprocessing can be found in Table 1 .
We report case- insensitive BLEU with the WMT standard ' 13 a ' tokenization using SacreBLEU ( Post , 2018 ) .
Model Configuration and Baseline
We build NMT models upon the attentional RNN encoder-decoder architecture ( Bahdanau et al. , 2015 ) implemented in the Sockeye toolkit ( Hieber et al. , 2017 )
Table 2 : BLEU scores on eight translation directions .
The numbers before and after ' ? ' are the mean and standard deviation over five randomly seeded models .
Our proposed methods ( ? = 0/0.5 ) achieve small but consistent improvements .
?BLEU scores are in bold if mean ?
std is above zero while in red if the mean is below zero . 2016 ) and add dropout to embeddings and RNNs ( Gal and Ghahramani , 2016 ) with probability 0.2 .
We train using the Adam optimizer ( Kingma and Ba , 2015 ) with a batch size of 48 sentences and we checkpoint the model every 1000 updates .
The learning rate for baseline models is initialized to 0.001 and reduced by 30 % after 4 checkpoints without improvement of perplexity on the development set .
Training stops after 10 checkpoints without improvement .
The bi-directional NMT model ties source and target embeddings to yield a bilingual vector space .
It also ties the output layer 's weights and embeddings to achieve better performance in lowresource scenarios ( Press and Wolf , 2017 ; Nguyen and Chiang , 2018 ) .
We train five randomly seeded bi-directional baseline models by optimizing the forward translation objective L T and report the mean and standard deviation of test BLEU .
We fine-tune baseline models with objective L T + L R , inheriting all settings except the learning rate which is re-initialized to 0.0001 .
Each randomly seeded model is fine-tuned independently , so we are able to report the standard deviation of ?BLEU .
Contrastive Reconstruction Model
We compare our approach with reconstruction from hidden states ( HIDDEN ) .
Following the best practice of Wang et al . ( 2018a ) , two reconstructors are used to take hidden states from both the encoder and the decoder .
The corresponding two reconstruction losses and the canonical translation loss were originally uniformly weighted ( i.e. 1 , 1 , 1 ) , but we found that balancing the reconstruction and translation losses yields better results ( i.e. 0.5 , 0.5 , 1 ) in preliminary experiments .
6
We use the reconstructor exclusively to compute the reconstruction training loss .
It has also been 6 We observed around 0.2 BLEU gains for TR ?EN tasks .
used to re-rank translation hypotheses in prior work , but Tu et al . ( 2017 ) showed in ablation studies that the gains from re-ranking are small compared to those from training .
Results
Table 2 shows that our reconstruction approach achieves small but consistent BLEU improvements over the baseline on all eight tasks .
7
We evaluate the impact of the Gumbel Softmax hyperparameters on the development set .
We select ? = 2 and ? = 0/0.5 based on training stability and BLEU .
Greedy search ( i.e. ? = 0 ) performs similarly as sampling with increased Gumbel noise ( i.e. more random translation selection when ? = 0.5 ) : increased randomness in sampling does not have a strong impact on BLEU , even though random sampling may approximate the data distribution better .
We hypothesize that more random translation selection introduces lower quality samples and therefore noisier training signals .
This is consistent with the observation that random sampling is less effective for back - translation in low-resource settings ( Edunov et al. , 2018 ) .
Sampling - based reconstruction is effective even if there is moderate domain mismatch between the training and the test data , such as in the case that the word type out-of-vocabulary ( OOV ) rate of TR ?EN is larger than 20 % .
Larger improvements can be achieved when the test data is closer to training examples .
For example , the OOV rate of SW ?EN is much smaller than the OOV rate of TR ?EN and the former obtains higher ?BLEU .
Our approach yields more consistent results than reconstructing from hidden states .
The latter fails to improve BLEU in more difficult cases , such as TR ?EN with high OOV rates .
We observe extremely low training perplexity for HID - DEN compared with our proposed approach ( Figure 1a ) .
This suggests that HIDDEN yields representations that memorize the input rather than improve output representations .
Another advantage of our approach is that all parameters were jointly pre-trained , which results in more stable training behavior .
By contrast , reconstructing from hidden states requires to initialize the reconstructors independently and suffers from unstable early training behavior ( Figure 1 ) .
Conclusion
We studied reconstructing the input of NMT from its intermediate translations to better exploit training samples in low-resource settings .
We used a bi-directional NMT model and the Straight - Through Gumbel Softmax to build a fully differentiable reconstruction model that does not require any additional parameters .
We empirically demonstrated that our approach is effective in low-resource scenarios .
In future work , we will investigate the use of differentiable reconstruction from sampled sequences in unsupervised and semi-supervised sequence generation tasks .
In particular , we will exploit monolingual corpora in addition to parallel corpora for NMT .
Figure 1 : 1 Figure 1 : Training curves of perplexity on the training and the development sets for TR?EN .
Reconstructing from hidden states ( HIDDEN ) and reconstructing from sampled translations ( ? = 0 ) are compared .
HIDDEN achieves extremely low training perplexity and suffers from unstable training during the early stage .
