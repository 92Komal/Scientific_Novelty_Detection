title
Tencent AI Lab Machine Translation Systems for the WMT20 Biomedical Translation Task
abstract
This paper describes the Tencent AI Lab submission of the WMT2020 shared task on biomedical translation in four language directions : German ?
English , English ?
German , Chinese ?
English and English ?
Chinese .
We implement our system with model ensemble technique on different transformer architectures ( DEEP , HYBRID , BIG , LARGE Transformers ) .
To enlarge the in-domain bilingual corpus , we use back -translation of monolingual in-domain data in the English language as additional in-domain training data .
Our systems in German ?
English and English ?
German are ranked 1st and 3rd respectively according to the official evaluation results in terms of BLEU scores .
1
Introduction Neural machine translation ( Bahdanau et al. , 2015 ; Vaswani et al. , 2017 , NMT ) has achieved great progress in recent years .
However , as Koehn and Knowles ( 2017 ) pointed out , NMT systems suffer from poor translation performance in out-ofdomain scenarios , which poses a great challenge for the biomedical translation task .
In this paper , we present our submission to the WMT20 shared task on biomedical translation task .
We participated in two language directions : German-English and Chinese-English .
To address the domain problem , on one hand , we adopt model ensemble technique ( Liu et al. , 2018 ) with different transformer architectures to build a more robust model .
On the other hand , we enlarge the in-domain bilingual corpus with back - translation approach ( Sennrich et al. , 2016 a ) .
Our contributions are as follows : ?
We adopt the model ensemble technique and the back-translation approach to achieve 1 Details of our systems are introduced in https : / / github.com/hsing-wang /WMT2020
_BioMedical the state- of- the - art performance on WMT19 biomedical translation task test sets . ?
To promote further studies , we release some pre-trained models and the in-domain synthetic Chinese - English bilingual data for the community .
The rest of this paper is organized as follows .
Section 2 presents our system with four different transformer architectures : DEEP , HYBRID , BIG , LARGE Transformers .
Section 3 describes the training data used in our system , including bilingual data , monolingual data and synthetic bilingual data .
Section 4 reports experimental results in two language directions .
Finally , we conclude our work in Section 5 .
System
In our systems , we adopt four different model architectures with TRANSFORMER ( Vaswani et al. , 2017 ) : ? DEEP TRANSFORMER
( Dou et al. , 2018 ; Dou et al. , 2019 ) is the TRANSFORMER - BASE model with the 40layer encoder .
? HYBRID TRANSFORMER
( Hao et al. , 2019 b ) is the TRANSFORMER - BASE model with 40 layer hybrid encoder .
The 40 - layer hybrid encoder stacks 35 - layer self -attention - based encoder on top of 5 - layer bi-directional ON - LSTM ( Shen et al. , 2019 ) encoder .
? BIG TRANSFORMER is the TRANSFORMER - BIG model as used by Vaswani et al . ( 2017 ) .
The main differences between these models are presented in Table 1 . Pre-Norm ) is adopted in above four models .
All models are implemented on top of the open-source toolkit Fairseq 2 . Model ensemble is used through ensemble decoding with different model architectures . ?
Data
The data used to train our system consists of three parts : bilingual data , monolingual data and synthetic bilingual data .
Bilingual Data In-domain bilingual data
The in- domain bilingual data is provided by WMT20 biomedical translation shared task .
For German-English , we choose Biomedical Translation 3 and UFAL Medical Corpus 4 to use as the in-domain training data .
For Chinese -English out -of- domain ( OOD ) data , we adopt data selection ( Axelrod et al. , 2011 ; Liu et al. , 2014 ) to select the in-house data ( 8.5 M sentence pairs ) as the in-domain training data .
General-domain bilingual data
To alleviate the data scarce problem , we collect generaldomain bilingual data from WMT20 news translation shared task 5 . For German-English , we use Europarl - v10 6 , ParaCrawl - v5.1 7 , News Commentary - v15 8 and Wiki Titles - v2 9 . For Chinese-English , we use CCMT Corpus 10 , UN Parallel Corpus v1.0 11 , News Commentary - v15 12 .
Monolingual Data
As WMT20 biomedical translation shared task provides in- domain bilingual data in other language pairs , we gather in-domain monolingual data from bilingual data in other language pair .
Specifically , we collect the English side of the bilingual sentence pairs from Biomedical Translation and UFAL Medical Corpus .
The statistics of the in-domain bilingual and monolingual data is listed in Table 2 .
Synthetic Bilingual Data
To enlarge the in-domain bilingual corpus , we adopt back - translation method ( Sennrich et al. , 2016a ) to generate synthetic bilingual sentence pairs .
For Chinese - English , as we lack of sufficient in-domain bilingual data , we use an online translation system TranSmart 13 to translate the in-domain monolingual English back to Chinese .
For German-English , we train a English - German LARGE model on the combination of in-domain and general - domain bilingual data , and use the model to generate synthetic bilingual data .
Experiment
We report experimental results in four language pairs : German-English ( de/en ) , English - German ( en / de ) , Chinese-English ( zh/en ) and English - Chinese ( en/zh ) .
Experimental Setup Data Pre-Processing
We follow previous work ( Saunders et al. , 2019 ; Peng et al. , 2019 ) use Moses scripts 14 to preprocess 15 the data and filter the bilingual data with following heuristics rules : ?
Filter out duplicate sentence pairs ( Khayrallah and Koehn , 2018 ; Ott et al. , 2018 ) . ?
Filter out sentence pairs with wrong language ( Khayrallah and Koehn , 2018 ) . ?
Filter out sentences pairs containing more than 120 tokens or fewer than 3 . ?
Filter out sentence pairs with source / target length ratio exceeding 1.5 ( Ott et al. , 2018 ) .
Evaluation For German-English , we use the Khresmoi development data as the development set , and use the sentence pairs with the correct alignment in WMT19 biomedical translation task test set as our test set .
For Chinese - English , we use the in-house bilingual test set ( 1,000 sentence pairs ) and the sentence pairs with the correct alignment in WMT19 biomedical translation task test set as development set and test set , respectively .
14 https://github.com/moses-smt/ mosesdecoder/tree/master/scripts 15 normalize-punctuation.perl , tokenizer.perl , remove-nonprinting -char.perl Follow Bawden et al. ( 2019 ) , we use multibleu .
perl from Moses 16 to compute BLEU scores and report case-sensitive BLEU scores on development and test sets .
Data Pre-processing
For each language pair , we perform byte-pair encoding 17 ( BPE ) ( Sennrich et al. , 2016 b ) processing on the combination of in-domain bilingual data and general - domain bilingual data , and set the number of BPE merge operations to 50,000 for source and target sides , respectively .
Model Training
The learning rate is set to 0.0007 .
All models are trained for 600K steps on 8 Tesla V100 GPUs where each is allocated with a batch size of 8192 tokens .
German-English Results For German-English task , we first train the models on the general- domain data .
Then we combine the general- domain data and the in-domain data and train the models from scratch .
Finally , we introduce the synthetic bilingual data to the combination data and use all data to train the models .
with best validation loss throughout the training process is selected as the final model for the testing .
For model inference , the length penalty is set to 0.6 and the beam size is set to 4 .
The German-English results are listed in Table 3 .
Our observations are : ?
Due to the largest model capacity , LARGE model obtains the best translation performance among the four model variants .
?
Ensemble decoding with different transformer architectures ( ENSEMBLE in Table 3 ) achieves best translation performance .
?
Leveraging in- domain bilingual data ( " + Indomain " ) and synthetic bilingual data ( " + BT In-domain " ) achieves significant translation improvement .
Data rejuvenation 18 ( Jiao et al. , 2020 ) is an approach which exploits the inactive training examples for neural machine translation on large-scale datasets .
We adopt the data rejuvenation approach to German ?
English translation task .
Experimental results are presented in Tale 7 and the data rejuvenation approach achieves significant improvement over the baseline LARGE model .
Chinese-English Results For Chinese -English task , we gradually add the general- domain data , the synthetic bilingual data and OOD in-house data to the training data and 18 https://github.com/wxjiao/ Data-Rejuvenation train the models from scratch .
Since the development set and test set have different data distribution , we save checkpoints every epoch and average the last 5 checkpoints rather than choose the model with best validation loss .
For model inference , the length penalty is set to 2.0 and the beam size is set to 8 .
Similar phenomena are observed in Chinese -English translation task .
Table 4 shows Chinese-English translation results .
Finally , our systems obtain 32.24 BLEU points and 33.23 BLEU points on the development and test sets , respectively .
Main Results Main results are reported in Table 5 .
Our submissions ( Tencent AI Lab Machine Translation , TMT ) with model ensemble technique achieve strong performances in WMT19 German ?
English and Chinese ?
English biomedical test sets .
Official Results
The official automatic evaluation results of our submissions for WMT 2020 are presented in Table 6 .
Our final systems rank the 1st and 3rd places on German-English and English - German , respectively , in terms of BLEU score .
Conclusion
In this paper , we present Tencent AI Lab machine translation systems for the WMT20 biomedical translation shared task and release the pre-trained models as well as the in-domain synthetic Chinese - English bilingual data for the research commu- nity .
Our systems in German-English and English - German are ranked 1st and the 3rd respectively according to the official evaluation results in terms of BLEU scores .
We also participate in the news translation ( Wu et al. , 2020 ) and the chat translation tasks .
In the future , we plan to explore domain adaptation ( Peng et al. , 2019 ; Saunders et al. , 2019 ; Chu and Wang , 2018 ; Wang et al. , 2017a ) , phrase modeling ( Wang et al. , 2017 b , c ; Hao et al. , 2019a ) , structural modeling ( Hao et al. , 2019c ; strategies to improve the system performance .
Table 1 : 1 Hyper-parameters of different Transformer models used in our system .
LARGE TRANSFORMER is similar to TRANSFORMER - BIG model except that it uses a 20 - layer encoder .
