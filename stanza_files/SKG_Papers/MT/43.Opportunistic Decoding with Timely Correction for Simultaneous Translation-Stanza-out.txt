title
Opportunistic Decoding with Timely Correction for Simultaneous Translation
abstract
Simultaneous translation has many important application scenarios and attracts much attention from both academia and industry recently .
Most existing frameworks , however , have difficulties in balancing between the translation quality and latency , i.e. , the decoding policy is usually either too aggressive or too conservative .
We propose an opportunistic decoding technique with timely correction ability , which always ( over - ) generates a certain mount of extra words at each step to keep the audience on track with the latest information .
At the same time , it also corrects , in a timely fashion , the mistakes in the former overgenerated words when observing more source context to ensure high translation quality .
Experiments show our technique achieves substantial reduction in latency and up to + 3.1 increase in BLEU , with revision rate under 8 % in Chinese- to - English and English - to - Chinese translation .
Introduction Simultaneous translation , which starts translation before the speaker finishes , is extremely useful in many scenarios , such as international conferences , travels , and so on .
In order to achieve low latency , it is often inevitable to generate target words with insufficient source information , which makes this task extremely challenging .
Recently , there are many efforts towards balancing the translation latency and quality with mainly two types of approaches .
On one hand , propose very simple frameworks that decode following a fixed - latency policy such as waitk .
On the other hand , there are many attempts to learn an adaptive policy which enables the model to decide READ or WRITE action on the fly using various techniques such as reinforcement learning ( Gu et al. , 2017 ; Alinejad et al. , 2018 ; Grissom II * These authors contributed equally .
y t < l a t e x i t s h a 1 _ b a s e 6 4 = " S e G B d m F 0 K 2 6 9 B z W 9 N T I 6 y l i l w t w = " > A A A B 6 n i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e z 6 Q I 9 B L x 4 j m g c k S 5 i d z C Z D Z m e X m V 4 h L P k E L x 4 U 8 e o X e f N v n C R 7 0 M S C h q K q m + 6 u I J H C o O t + O 4 W V 1 b X 1 j e J m a W t 7 Z 3 e v v H / Q N H G q G W + w W M a 6 H V D D p V C 8 g Q I l b y e a 0 y i Q v B W M b q d + 6 4 l r I 2 L 1 i O O E + x E d K B E K R t F K D + M e 9 s o V t + r O Q J a J l 5 M K 5 K j 3 y l / d f s z S i C t k k h r T 8 d w E / Y x q F E z y S a m b G p 5 Q N q I D 3 r F U 0 Y g b P 5 u d O i E n V u m T M N a 2 F J K Z + n s i o 5 E x 4 y i w n R H F o V n 0 p u J / X i f F 8 N r P h E p S 5 I r N F 4 W p J B i T 6 d + k L z R n K M e W U K a F v Z W w I d W U o U 2 n Z E P w F l 9 e J s 2 z q n d e v b y / q N R u 8 j i K c A T H c A o e X E E N 7 q A O D W A w g G d 4 h T d H O i / O u / M x b y 0 4 + c w h / I H z + Q N 1 B o 3 r < / l a t e x i t > ?6 w
t < l a t e x i t s h a 1 _ b a s e 6 4 = " K B 9 f + I Q o M d o V 0 w 7 A q 9 F 4 Q d 4 E a 5 M = " > A A A C C X i c b V C 5 T s N A E F 2 H K 4 Q r Q E m z I k K i i m w O Q R l B Q x k k c k i x s d a b d b L K + m B 3 D L I s t z T 8 C g 0 F C N H y B 3 T 8 D Z v E B S Q 8 a a S n 9 2 Y 0 M 8 + L B V d g m t 9 G a W F x a X m l v F p Z W 9 / Y 3 K p u 7 7 R V l E j K W j Q S k e x 6 R D H B Q 9 Y C D o J 1 Y 8 l I 4 A n W 8 U a X Y 7 9 z z 6 T i U X g D a c y c g A x C 7 n N K Q E t u F d t D A p k d E B h 6 f p b m u Q u 3 m S 3 Y n R I k B P y Q u 9 W a W T c n w P P E K k g N F W i 6 1 S + 7 H 9 E k Y C F Q Q Z T q W W Y M T k Y k c C p Y X r E T x W J C R 2 T A e p q G J G D K y S a f 5 P h A K 3 3 s R 1 K X X j 9 R f 0 9 k J F A q D T z d O T 5 Z z X p j 8 T + v l 4 B / 7 m Q 8 j B N g I Z 0 u 8 h O B I c L j W H C f S 0 Z B p J o Q K r m + F d M h k Y S C D q + i Q 7 B m X 5 4 n 7 a O 6 d V w / v T 6 p N S 6 K O M p o D + 2 j Q 2 S h M 9 R A V 6 i J W o i i R / S M X t G b 8 W S 8 G O / G x 7 S 1 Z B Q z u + g P j M 8 f J a O b Q g = = < / l a t e x i t > revision window decoding time t < l a t e x i t s h a 1 _ b a s e 6 4 = " t 6 X a y t d I s H w d U 4 A e C N D S j P N P 5 s M = " > A A A B 6 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 f 6 D H o x W M C 5 g H J E m Y n s 8 m Y 2 d l l p l c I S 7 7 A i w d F v P p J 3 v w b J 8 k e N L G g o a j q p r s r S K Q w 6 L r f z s r q 2 v r G Z m G r u L 2 z u 7 d f O j h s m j j V j D d Y L G P d D q j h U i j e Q I G S t x P N a R R I 3 g p G d 1 O / 9 c S 1 E b F 6 w H H C / Y g O l A g F o 2 i l O v Z K Z b f i z k C W i Z e T M u S o 9 U p f 3 X 7 M 0 o g r Z J I a 0 / H c B P 2 M a h R M 8 k m x m x q e U D a i A 9 6 x V N G I G z + b H T o h p 1 b p k z D W t h S S m f p 7 I q O R M e M o s J 0 R x a F Z 9 K b i f 1 4 n x f D G z 4 R K U u S K z R e F q S Q Y k + n X p C 8 0 Z y j H l l C m h b 2 V s C H V l K H N p m h D 8 B Z f X i b N 8 4 p 3 U b m q X 5 a r t 3 k c B T i G E z g D D 6 6 h C v d Q g w Y w 4 P A M r / D m P D o v z r v z M W 9 d c f K Z I / g D 5 / M H 4 m 2 M / w = = < / l a t e x i t > t + 1 < l a t e x i t s h a 1 _ b a s e 6 4 = " t G Q 1 G T w h m 5 7 M / c u P N J R I L / V D c N U = " > A A A B 6 n i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A E I e z 6 Q I 9 B L x 4 j m g c k S 5 i d z C Z D Z m e X m V 4 h L P k E L x 4 U 8 e o X e f N v n C R 7 0 M S C h q K q m + 6 u I J H C o O t + O 0 v L K 6 t r 6 4 W N 4 u b W 9 s 5 u a W + / Y e J U M 1 5 n s Y x 1 K 6 C G S 6 F 4 H Q V K 3 k o 0 p 1 E g e T M Y 3 k 7 8 5 h P X R s T q E U c J 9 y P a V y I U j K K V H v D U 6 5 b K b s W d g i w S L y d l y F H r l r 4 6 v Z i l E V f I J D W m 7 b k J + h n V K J j k 4 2 I n N T y h b E j 7 v G 2 p o h E 3 f j Y 9 d U y O r d I j Y a x t K S R T 9 f d E R i N j R l F g O y O K A z P v T c T / v H a K 4 b W f C Z W k y B W b L Q p T S T A m k 7 9 J T 2 j O U I 4 s o U w L e y t h A 6 o p Q 5 t O 0 Y b g z b + 8 S B p n F e + 8 c n l / U a 7 e 5 H E U 4 B C O 4 A Q 8 u I I q 3 E E N 6 s C g D 8 / w C m + O d F 6 c d + d j 1 r r k 5 D M H 8 A f O 5 w + 4 y Y 1 v < / l a t e x i t > c o rr e c ti o n 8 > > < > > : < l a t e x i t s h a 1 _ b a s e 6 4 = " v O X x V X r m u i v 7 9 A W 8 s e b 2 T K J 6 M C A = " > A A A C N 3 i c f V D L S g M x F M 3 4 d n x V X b o J F s V V m f G B L o t u X I m C V a E p J Z P e a Y O Z z J D c E c v Q v 3 L j b 7 j T j Q t F 3 P o H p r W C L 7 w k c D j n H u 6 9 J 8 q U t B g E 9 9 7 I 6 N j 4 x O T U t D 8 z O z e / U F p c O r N p b g T U R K p S c x F x C 0 p q q K F E B R e Z A Z 5 E C s 6 j y 4 O + f n 4 F x s p U n 2 I 3 g 0 b C 2 1 r G U n B 0 V L N 0 x B T E y A o W Q V v q I u F o 5 H X P p + v U P c b o P 8 h n o F u f D m Z k u 4 O V Z q k c V I J B 0 d 8 g H I I y G d Z x s 3 T H W q n I E 9 A o F L e 2 H g Y Z N g p u U A o F P Z / l F j I u L n k b 6 g 5 q n o B t F I O 7 e 3 T N M S 0 a p 8 Z 9 j X T A f n U U P L G 2 m 0 S u 0 6 3 Z s T + 1 P v m X V s 8 x 3 m s U U m c 5 g h Y f g + J c U U x p P 0 T a k g Y E q q 4 D X B j p d q W i w w 0 X 6 K L 2 X Q j h z 5 N / g 7 P N S r h V 2 T n Z L l f 3 h 3 F M k R W y S j Z I S H Z J l R y S Y 1 I j g t y Q B / J E n r 1 b 7 9 F 7 8 V 4 / W k e 8 o W e Z f C v v 7 R 1 t L 6 Y 8 < / l a t e x i t > ? ? ? 8 > > < > > : < l a t e x i t s h a 1 _ b a s e 6 4 = " v O X x V X r m u i v 7 9 A W 8 s e b 2 T K J 6 M C A = " > A A A C N 3 i c f V D L S g M x F M 3 4 d n x V X b o J F s V V m f G B L o t u X I m C V a E p J Z P e a Y O Z z J D c E c v Q v 3 L j b 7 j T j Q t F 3 P o H p r W C L 7 w k c D j n H u 6 9 J 8 q U t B g E 9 9 7 I 6 N j 4 x O T U t D 8 z O z e / U F p c O r N p b g T U R K p S c x F x C 0 p q q K F E B R e Z A Z 5 E C s 6 j y 4 O + f n 4 F x s p U n 2 I 3 g 0 b C 2 1 r G U n B 0 V L N 0 x B T E y A o W Q V v q I u F o 5 H X P p + v U P c b o P 8 h n o F u f D m Z k u 4 O V Z q k c V I J B 0 d 8 g H I I y G d Z x s 3 T H W q n I E 9 A o F L e 2 H g Y Z N g p u U A o F P Z / l F j I u L n k b 6 g 5 q n o B t F I O 7 e 3 T N M S 0 a p 8 Z 9 j X T A f n U U P L G 2 m 0 S u 0 6 3 Z s T + 1 P v m X V s 8 x 3 m s U U m c 5 g h Y f g + J c U U x p P 0 T a k g Y E q q 4 D X B j p d q W i w w 0 X 6 K L 2 X Q j h z 5 N / g 7 P N S r h V 2 T n Z L l f 3 h 3 F M k R W y S j Z I S H Z J l R y S Y 1 I j g t y Q B / J E n r 1 b 7 9 F 7 8 V 4 / W k e 8 o W e Z f C v v 7 R 1 t L 6 Y 8 < / l a t e x i t > t + 2 < l a t e x i t s h a 1 _ b a s e 6 4 = " 9 + 8 c o 5 X + 9 c u a D I / 4 v v U 0 v 8 C j H t c = " > A A A B 6 n i c b V D L S g N B E O y N r x h f U Y 9 e B o M g C G E 3 K n o M e v E Y 0 T w g W c L s Z J I M m Z 1 d Z n q F s O Q T v H h Q x K t f 5 M 2 / c Z L s Q R M L G o q q b r q 7 g l g K g 6 7 7 7 e R W V t f W N / K b h a 3 t n d 2 9 4 v 5 B w 0 S J Z r z O I h n p V k A N l 0 L x O g q U v B V r T s N A 8 m Y w u p 3 6 z S e u j Y j U I 4 5 j 7 o d 0 o E R f M I p W e s C z S r d Y c s v u D G S Z e B k p Q Y Z a t / j V 6 U U s C b l C J q k x b c + N 0 U + p R s E k n x Q 6 i e E x Z S M 6 4 G 1 L F Q 2 5 8 d P Z q R N y Y p U e 6 U f a l k I y U 3 9 P p D Q 0 Z h w G t j O k O D S L 3 l T 8 z 2 s n 2 L / 2 U 6 H i B L l i 8 0 X 9 R B K M y P R v 0 h O a M 5 R j S y j T w t 5 K 2 J B q y t C m U 7 A h e I s v L 5 N G p e y d l y / v L 0 r V m y y O P B z B M Z y C B 1 d Q h T u o Q R 0 Y D O A Z X u H N k c 6 L 8 + 5 8 z F t z T j Z z C H / g f P 4 A u k 2 N c A = = < / l a t e x i t > correction y <t < l a t e x i t s h a 1 _ b a s e 6 4 = " 5 w m Z K B h k P + V l N y 8 p x W Z S + / T m o i A = " > A A A B + n i c b V D L S s N A F L 2 p r 1 p f q S 7 d D B b B V U l 8 o A s X R T c u K 9 g H t C F M p p N 2 6 O T B z E Q J M Z / i x o U i b v 0 S d / 6 N k z Y L r R 4 Y O J x z L / f M 8 W L O p L K s L 6 O y t L y y u l Z d r 2 1 s b m 3 v m P X d r o w S Q W i H R D w S f Q 9 L y l l I O 4 o p T v u x o D j w O O 1 5 0 + v C 7 9 1 T I V k U 3 q k 0 p k 6 A x y H z G c F K S 6 5 Z z 4 Y B V h P P z 9 I 8 d 7 N L l b t m w 2 p a M 6 C / x C 5 J A 0 q 0 X f N z O I p I E t B Q E Y 6 l H N h W r J w M C 8 U I p 3 l t m E g a Y z L F Y z r Q N M Q B l U 4 2 i 5 6 j Q 6 2 M k B 8 J / U K F Z u r P j Q w H U q a B p y e L n H L R K 8 T / v E G i / A s n Y 2 G c K B q S + S E / 4 U h F q O g B j Z i g R P F U E 0 w E 0 1 k R m W C B i d J t 1 X Q J 9 u K X / 5 L u c d M + a Z 7 d n j Z a V 2 U d V d i H A z g C G 8 6 h B T f Q h g 4 Q e I A n e I F X 4 9 F 4 N t 6 M 9 / l o x S h 3 9 u A X j I 9 v L j K U m g = = < / l a t e x i t > irreversible Figure 1 : Besides y t , opportunistic decoding continues to generate additional w words which are represented as ?
w t .
The timely correction only revises this part in future steps .
Different shapes denote different words .
In this example , from step t to t + 1 , all previously opportunistically decoded words are revised , and an extra triangle word is generated in opportunistic window .
From step t + 1 to t + 2 , two words from previous opportunistic window are kept and only the triangle word is revised .
et al. , 2014 ) , supervised learning over pseudooracles , imitation learning , model ensemble ( Zheng et al. , 2020 ) or monotonic attention ( Ma et al. , 2019d ; Arivazhagan et al. , 2019 ) .
Though the existing efforts improve the performance in both translation latency and quality with more powerful frameworks , it is still difficult to choose an appropriate policy to explore the optimal balance between latency and quality in practice , especially when the policy is trained and applied in different domains .
Furthermore , all existing approaches are incapable of correcting the mistakes from previous steps .
When the former steps commit errors , they will be propagated to the later steps , inducing more mistakes to the future .
Inspired by our previous work on speculative beam search , we propose an opportunistic decoding technique with timely correction mechanism to address the above problems .
As shown in Fig. 1 , our proposed method always decodes more words than the original policy at each step to catch up with the speaker and reduce the latency .
At the same time , it also employs a timely correction mechanism to review the extra outputs from previous steps with more source context , and revises these outputs with current preference when there is a disagreement .
Our algorithm can be used in both speech - to - text and speech - to-speech simultaneous translation ( Oda et al. , 2014 ; Bangalore et al. , 2012 ; Yarmohammadi et al. , 2013 ) .
In the former case , the audience will not be overwhelmed by the modifications since we only review and modify the last few output words with a relatively low revision rate .
In the later case , the revisable extra words can be used in look - ahead window in incremental TTS .
By contrast , the alternative retranslation strategy ( Arivazhagan et al. , 2020 ) will cause non-local revisions which makes it impossible to be used in incremental TTS .
We also define , for the first time , two metrics for revision -enabled simultaneous translation : a more general latency metric Revision - aware Average Lagging ( RAL ) as well as the revision rate .
We demonstrate the effectiveness of our proposed technique using fixed and adaptive policies in both Chineseto-English and English - to - Chinese translation .
Preliminaries Full-sentence NMT .
The conventional fullsentence NMT processes the source sentence x = ( x 1 , ... , x n ) with an encoder , where x i represents an input token .
The decoder on the target side ( greedily ) selects the highest - scoring word y t given source representation h and previously generated target tokens , y <t = ( y 1 , ... , y t?1 ) , and the final hypothesis y = ( y 1 , ... , y t ) with y t = < eos > has the highest probability : p(y | x ) = | y | t=1 p(y t | x , y <t ) ( 1 ) Simultaneous Translation .
Without loss of generality , regardless the actual design of policy , simultaneous translation is represented as : p g ( y | x ) = | y | t=1 p(y t | x g ( t ) , y <t ) ( 2 ) where g ( t ) can be used to represent any arbitrary fixed or adaptive policy .
For simplicity , we assume the policy is given and does not distinguish the difference between two types of policies .
Opportunistic Decoding with Timely Correction and Beam Search Opportunistic Decoding .
For simplicity , we first apply this method to fixed policies .
We de-fine the original decoded word sequence at time step t with y t , which represents the word that is decoded in time step t with original model .
We denote the additional decoded words at time step t as ?
w t = ( y 1 t , ... , y w t ) , where w denote the number of extra decoded words .
In our setting , the decoding process is as follows : p g ( y t ? ? w t | x g ( t ) ) = p g ( y t | x g ( t ) ) w i=1 p g ( ?
i t | x g ( t ) , y t ? ?< i t ) ( 3 ) where ? is the string concatenation operator .
We treat the procedure for generating the extra decoded sequence as opportunistic decoding , which prefers to generate more tokens based on current context .
When we have enough information , this opportunistic decoding eliminates unnecessary latency and keep the audience on track .
With a certain chance , when the opportunistic decoding tends to aggressive and generates inappropriate tokens , we need to fix the inaccurate token immediately .
Timely Correction .
In order to deliver the correct information to the audience promptly and fix previous mistakes as soon as possible , we also need to review and modify the previous outputs .
At step t + 1 , when encoder obtains more information from x g ( t ) to x g ( t + 1 ) , the decoder is capable to generate more appropriate candidates and may revise and replace the previous outputs from opportunistic decoding .
More precisely , ? w t and y t+1 ? ? w?1 t+ 1 are two different hypothesis over the same time chunk .
When there is a disagreement , our model always uses the hypothesis from later step to replace the previous commits .
Note our model does not change any word in y t from previous step and it only revise the words in ?
w t . Modification for Adaptive Policy .
For adaptive policies , the only difference is , instead of committing a single word , the model is capable of generating multiple irreversible words .
Thus our proposed methods can be easily applied to adaptive policies .
Correction with Beam Search .
When the model is committing more than one word at a time , we can use beam search to further improve the translation quality and reduce revision rate ( Murray and Chiang , 2018 ; .
The decoder maintains a beam B k t of size b at step t , which is ordered list of pairs
The decoder generates target word y 4 = " his " and two extra words " welcome to " at step t = 4 when input x 9 = " z?nt?ng " ( " agreement " ) is not available yet .
When the model receives x 9 at step t = 5 , the decoder immediately corrects the previously made mistake " welcome " with " agreement " and emits two additional target words ( " to President " ) .
The decoder not only is capable to fix the previous mistake , but also has enough information to perform more correct generations .
Our framework benefits from opportunistic decoding with reduced latency here .
Note though the word " to " is generated in step t = 4 , it only becomes irreversible at step t = 6 . hypothesis , probability , where k denotes the k th step in beam search .
At each step , there is an initial beam B 0 t = [ y t?1 , 1 ] .
We denote one-step transition from the previous beam to the next as B k+1 t = next b 1 ( B k t ) = b top { y ? v , u?p( v|x g( t ) , y ) | y , u ?
B k t } where top b ( ? ) returns the top-scoring b pairs .
Note we do not distinguish the revisable and nonrevisable output in y for simplicity .
We also define the multi-step advance beam search function with recursive fashion as follows : next b i ( B k t ) = next b 1 ( next b i?1 ( B k t ) )
When the opportunistic decoding window is w at decoding step t , we define the beam search over w + 1 ( include the original output ) as follows : y t , u t = top 1 next b n+w ( B 0 t ) ( 4 ) where next b n+w ( ? ) performs a beam search with n + w steps , and generate y t as the outputs which include both original and opportunistic decoded words .
n represents the length of y t
Revision - aware AL and Revision Rate
We define , for the first time , two metrics for revision -enabled simultaneous translation .
Revision - aware AL AL is introduced in to measure the average delay for simultaneous translation .
Besides the limitations that are mentioned in ( Cherry and Foster , 2019 ) , AL is also not sensitive to the modifications to the committed words .
Furthermore , in the case of re-translation , AL is incapable to measure the meaningful latency anymore .
A A B A D B C A C F A D F E F A D F B E A D F B E target source final outputs s = 0 < l a t e x i t s h a 1 _ b a s e 6 4 = " D 1 y S 6 R z r A 1 l V O B t v B t n R j R F u 1 H 8 = " > A A A B 6 n i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e z 6 Q C 9 C 0 I v H i O Y B y R J m J 7 P J k N n Z Z a Z X C E s + w Y s H R b z 6 R d 7 8 G y f J H j R a 0 F B U d d P d F S R S G H T d L 6 e w t L y y u l Z c L 2 1 s b m 3 v l H f 3 m i Z O N e M N F s t Y t w N q u B S K N 1 C g 5 O 1 E c x o F k r e C 0 c 3 U b z 1 y b U S s H n C c c D + i A y V C w S h a 6 d 5 c u b 1 y x a 2 6 M 5 C / x M t J B X L U e + X P b j 9 m a c Q V M k m N 6 X h u g n 5 G N Q o m + a T U T Q 1 P K B v R A e 9 Y q m j E j Z / N T p 2 Q I 6 v 0 S R h r W w r J T P 0 5 k d H I m H E U 2 M 6 I 4 t A s e l P x P 6 + T Y n j p Z 0 I l K X L F 5 o v C V B K M y f R v 0 h e a M 5 R j S y j T w t 5 K 2 J B q y t C m U 7 I h e I s v / y X N k 6 p 3 W j 2 / O 6 v U r v M 4 i n A A h 3 A M H l x A D W 6 h D g 1 g M I A n e I F X R z r P z p v z P m 8 t O P n M P v y C 8 / E N 0 R m N f w = = < / l a t e x i t > s = 1 < l a t e x i t s h a 1 _ b a s e 6 4 = " t v 1 g l 2 x E n O T i h O n r Q 3 x w G 0 I y w L M = " > A A A B 6 n i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e z 6 Q C 9 C 0 I v H i O Y B y R J m J 7 P J k N n Z Z a Z X C E s + w Y s H R b z 6 R d 7 8 G y f J H j R a 0 F B U d d P d F S R S G H T d L 6 e w t L y y u l Z c L 2 1 s b m 3 v l H f 3 m i Z O N e M N F s t Y t w N q u B S K N 1 C g 5 O 1 E c x o F k r e C 0 c 3 U b z 1 y b U S s H n C c c D + i A y V C w S h a 6 d 5 c e b 1 y x a 2 6 M 5 C / x M t J B X L U e + X P b j 9 m a c Q V M k m N 6 X h u g n 5 G N Q o m + a T U T Q 1 P K B v R A e 9 Y q m j E j Z / N T p 2 Q I 6 v 0 S R h r W w r J T P 0 5 k d H I m H E U 2 M 6 I 4 t A s e l P x P 6 + T Y n j p Z 0 I l K X L F 5 o v C V B K M y f R v 0 h e a M 5 R j S y j T w t 5 K 2 J B q y t C m U 7 I h e I s v / y X N k 6 p 3 W j 2 / O 6 v U r v M 4 i n A A h 3 A M H l x A D W 6 h D g 1 g M I A n e I F X R z r P z p v z P m 8 t O P n M P v y C 8 / E N 0 p 2 N g A = = < / l a t e x i t > s = 2 < l a t e x i t s h a 1 _ b a s e 6 4 = " O y O D L H v y A V g c g V e W 1 v 0 a n u u 3 N T 0 = " > A A A B 6 n i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e x G R S 9 C 0 I v H i O Y B y R J m J 5 1 k y O z s M j M r h C W f 4 M W D I l 7 9 I m / + j Z N k D 5 p Y 0 F B U d d P d F c S C a + O 6 3 0 5 u Z X V t f S O / W d j a 3 t n d K + 4 f N H S U K I Z 1 F o l I t Q K q U X C J d c O N w F a s k I a B w G Y w u p 3 6 z S d U m k f y 0 Y x j 9 E M 6 k L z P G T V W e t D X l W 6 x 5 J b d G c g y 8 T J S g g y 1 b v G r 0 4 t Y E q I 0 T F C t 2 5 4 b G z + l y n A m c F L o J B p j y k Z 0 g G 1 L J Q 1 R + + n s 1 A k 5 s U q P 9 C N l S x o y U 3 9 P p D T U e h w G t j O k Z q g X v a n 4 n 9 d O T P / K T 7 m M E 4 O S z R f 1 E 0 F M R K Z / k x 5 X y I w Y W 0 K Z 4 v Z W w o Z U U W Z s O g U b g r f 4 8 j J p V M r e W f n i / r x U v c n i y M M R H M M p e H A J V b i D G t S B w Q C e 4 R X e H O G 8 O O / O x 7 w 1 5 2 Q z h / A H z u c P 1 C G N g Q = = < / l a t e x i t > s = 3 < l a t e x i t s h a 1 _ b a s e 6 4 = " 2 c q 5 h R I 5 o O f i 0 + K T b g x p w 8 V i P N I = " > A A A B 6 n i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e w a R S 9 C 0 I v H i O Y B y R J m J 5 1 k y O z s M j M r h C W f 4 M W D I l 7 9 I m / + j Z N k D 5 p Y 0 F B U d d P d F c S C a + O 6 3 0 5 u Z X V t f S O / W d j a 3 t n d K + 4 f N H S U K I Z 1 F o l I t Q K q U X C J d c O N w F a s k I a B w G Y w u p 3 6 z S d U m k f y 0 Y x j 9 E M 6 k L z P G T V W e t D X l W 6 x 5 J b d G c g y 8 T J S g g y 1 b v G r 0 4 t Y E q I 0 T F C t 2 5 4 b G z + l y n A m c F L o J B p j y k Z 0 g G 1 L J Q 1 R + + n s 1 A k 5 s U q P 9 C N l S x o y U 3 9 P p D T U e h w G t j O k Z q g X v a n 4 n 9 d O T P / K T 7 m M E 4 O S z R f 1 E 0 F M R K Z / k x 5 X y I w Y W 0 K Z 4 v Z W w o Z U U W Z s O g U b g r f 4 8 j J p n J W 9 S v n i / r x U v c n i y M M R H M M p e H A J V b i D G t S B w Q C e 4 R X e H O G 8 O O / O x 7 w 1 5 2 Q z h / A H z u c P 1 a W N g g = = < / l a t e x i t > s = 4 < l a t e x i t s h a 1 _ b a s e 6 4 = " X P A T y e S D T 1 r 3 Y A s 7 G 2 0 W 9 v i Z 8 V M = " > A A A B 6 n i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e x q R C 9 C 0 I v H i O Y B y R J m J 5 N k y O z s M t M r h C W f 4 M W D I l 7 9 I m / + j Z N k D 5 p Y 0 F B U d d P d F c R S G H T d b y e 3 s r q 2 v p H f L G x t 7 + z u F f c P G i Z K N O N 1 F s l I t w J q u B S K 1 1 G g 5 K 1 Y c x o G k j e D 0 e 3 U b z 5 x b U S k H n E c c z + k A y X 6 g l G 0 0 o O 5 r n S L J b f s z k C W i Z e R E m S o d Y t f n V 7 E k p A r Z J I a 0 / b c G P 2 U a h R M 8 k m h k x g e U z a i A 9 6 2 V N G Q G z + d n T o h J 1 b p k X 6 k b S k k M / X 3 R E p D Y 8 Z h Y D t D i k O z 6 E 3 F / 7 x 2 g v 0 r P x U q T p A r N l / U T y T B i E z / J j 2 h O U M 5 t o Q y L e y t h A 2 p p g x t O g U b g r f 4 8 j J p n J W 9 8 / L F f a V U v c n i y M M R H M M p e H A J V b i D G t S B w Q C e 4 R X e H O m 8 O O / O x 7 w 1 5 2 Q z h / A H z u c P 1 y m N g w = = < / l a t e x i t > s = 5 < l a t e x i t s h a 1 _ b a s e 6 4 = " n u j Z y 2 6 r K b M t S s G O N O D K K 1 q + R 0 0 = " > A A A B 6 n i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y q Q S 9 C 0 I v H i O Y B y R J m J 5 1 k y O z s M j M r h C W f 4 M W D I l 7 9 I m / + j Z N k D 5 p Y 0 F B U d d P d F c S C a + O 6 3 0 5 u Z X V t f S O / W d j a 3 t n d K + 4 f N H S U K I Z 1 F o l I t Q K q U X C J d c O N w F a s k I a B w G Y w u p 3 6 z S d U m k f y 0 Y x j 9 E M 6 k L z P G T V W e t D X l W 6 x 5 J b d G c g y 8 T J S g g y 1 b v G r 0 4 t Y E q I 0 T F C t 2 5 4 b G z + l y n A m c F L o J B p j y k Z 0 g G 1 L J Q 1 R + + n s 1 A k 5 s U q P 9 C N l S x o y U 3 9 P p D T U e h w G t j O k Z q g X v a n 4 n 9 d O T P / K T 7 m M E 4 O S z R f 1 E 0 F M R K Z / k x 5 X y I w Y W 0 K Z 4 v Z W w o Z U U W Z s O g U b g r f 4 8 j J p n J W 9 8 3 L l / q J U v c n i y M M R H M M p e H A J V b i D G t S B w Q C e 4 R X e H O G 8 O O / O x 7 w 1 5 2 Q z h / A H z u c P 2 K 2 N h A = = < / l a t e x i t > s = 6 < l a t e x i t s h a 1 _ b a s e 6 4 = " c L 0 D G l O Q B A r H B g Y Y u F O l 5 u 4 o v j s = " > A A A B 6 n i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e z 6 v g h B L x 4 j m g c k S 5 i d d J I h s 7 P L z K w Q l n y C F w + K e P W L v P k 3 T p I 9 a G J B Q 1 H V T X d X E A u u j e t + O 7 m l 5 Z X V t f x 6 Y W N z a 3 u n u L t X 1 1 G i G N Z Y J C L V D K h G w S X W D D c C m 7 F C G g Y C G 8 H w d u I 3 n l B p H s l H M 4 r R D 2 l f 8 h 5 n 1 F j p Q V 9 f d I o l t + x O Q R a J l 5 E S Z K h 2 i l / t b s S S E K V h g m r d 8 t z Y + C l V h j O B 4 0 I 7 0 R h T N q R 9 b F k q a Y j a T 6 e n j s m R V b q k F y l b 0 p C p + n s i p a H W o z C w n S E 1 A z 3 v T c T / v F Z i e l d + y m W c G J R s t q i X C G I i M v m b d L l C Z s T I E s o U t 7 c S N q C K M m P T K d g Q v P m X F 0 n 9 p O y d l s / v z 0 q V m y y O P B z A I R y D B 5 d Q g T u o Q g 0 Y 9 O E Z X u H N E c 6 L 8 + 5 8 z F p z T j a z D 3 / g f P 4 A 2 j G N h Q = = < / l a t e x i t > t = 1 < l a t e x i t s h a 1 _ b a s e 6 4 = " E R 7 j z w o V g j s U g X L Z Q Q c A 8 B L C H M 0 = " > A A A B 6 n i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e z 6 Q C 9 C 0 I v H i O Y B y R J m J 7 P J k N n Z Z a Z X C E s + w Y s H R b z 6 R d 7 8 G y f J H j R a 0 F B U d d P d F S R S G H T d L 6 e w t L y y u l Z c L 2 1 s b m 3 v l H f 3 m i Z O N e M N F s t Y t w N q u B S K N 1 C g 5 O 1 E c x o F k r e C 0 c 3 U b z 1 y b U S s H n C c c D + i A y V C w S h a 6 R 6 v v F 6 5 4 l b d G c h f 4 u W k A j n q v f J n t x + z N O I K m a T G d D w 3 Q T + j G g W T f F L q p o Y n l I 3 o g H c s V T T i x s 9 m p 0 7 I k V X 6 J I y 1 L Y V k p v 6 c y G h k z D g K b G d E c W g W v a n 4 n 9 d J M b z 0 M 6 G S F L l i 8 0 V h K g n G Z P o 3 6 Q v N G c q x J Z R p Y W 8 l b E g 1 Z W j T K d k Q v M W X / 5 L m S d U 7 r Z 7 f n V V q 1 3 k c R T i A Q z g G D y 6 g B r d Q h w Y w G M A T v M C r I 5 1 n 5 8 1 5 n 7 c W n H x m H 3 7 B + f g G 1 C O N g Q = = < / l a t e x i t > t = 2 < l a t e x i t s h a 1 _ b a s e 6 4 = " J T T A G u 8 m L C M q v Y J 9 W V / X C a G a n J g = " > A A A B 6 n i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e x G R S 9 C 0 I v H i O Y B y R J m J 5 N k y O z s M t M r h C W f 4 M W D I l 7 9 I m / + j Z N k D 5 p Y 0 F B U d d P d F c R S G H T d b y e 3 s r q 2 v p H f L G x t 7 + z u F f c P G i Z K N O N 1 F s l I t w J q u B S K 1 1 G g 5 K 1 Y c x o G k j e D 0 e 3 U b z 5 x b U S k H n E c c z + k A y X 6 g l G 0 0 g N e V 7 r F k l t 2 Z y D L x M t I C T L U u s W v T i 9 i S c g V M k m N a X t u j H 5 K N Q o m + a T Q S Q y P K R v R A W 9 b q m j I j Z / O T p 2 Q E 6 v 0 S D / S t h S S m f p 7 I q W h M e M w s J 0 h x a F Z 9 K b i f 1 4 7 w f 6 V n w o V J 8 g V m y / q J 5 J g R K Z / k 5 7 Q n K E c W 0 K Z F v Z W w o Z U U 4 Y 2 n Y I N w V t 8 e Z k 0 K m X v r H x x f 1 6 q 3 m R x 5 O E I j u E U P L i E K t x B D e r A Y A D P 8 A p v j n R e n H f n Y 9 6 a c 7 K Z Q / g D 5 / M H 1 a e N g g = = < / l a t e x i t > t = 3 < l a t e x i t s h a 1 _ b a s e 6 4 = " z t m F + N W K 7 Y H J F l S K 1 U J I t k k J V K c = " > A A A B 6 n i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e w a R S 9 C 0 I v H i O Y B y R J m J 5 N k y O z s M t M r h C W f 4 M W D I l 7 9 I m / + j Z N k D 5 p Y 0 F B U d d P d F c R S G H T d b y e 3 s r q 2 v p H f L G x t 7 + z u F f c P G i Z K N O N 1 F s l I t w J q u B S K 1 1 G g 5 K 1 Y c x o G k j e D 0 e 3 U b z 5 x b U S k H n E c c z + k A y X 6 g l G 0 0 g N e V 7 r F k l t 2 Z y D L x M t I C T L U u s W v T i 9 i S c g V M k m N a X t u j H 5 K N Q o m + a T Q S Q y P K R v R A W 9 b q m j I j Z / O T p 2 Q E 6 v 0 S D / S t h S S m f p 7 I q W h M e M w s J 0 h x a F Z 9 K b i f 1 4 7 w f 6 V n w o V J 8 g V m y / q J 5 J g R K Z / k 5 7 Q n K E c W 0 K Z F v Z W w o Z U U 4 Y 2 n Y I N w V t 8 e Z k 0 z s p e p X x x f 1 6 q 3 m R x 5 O E I j u E U P L i E K t x B D e r A Y A D P 8 A p v j n R e n H f n Y 9 6 a c 7 K Z Q / g D 5 / M H 1 y u N g w = = < / l a t e x i t > t = 4 < l a t e x i t s h a 1 _ b a s e 6 4 = " 5 A 3 3 j C S J Y n P / z s 9
We hereby propose a new latency , Revisionaware AL ( RAL ) , which can be applied to any kind of translation scenarios , i.e. , full-sentence translation , use re-translation as simultaneous translation , fixed and adaptive policy simultaneous translation .
Note that for latency and revision rate calculation , we count the target side difference respect to the growth of source side .
As it is shown in Fig. 3 , there might be multiple changes for each output words during the translation , and we only start to calculate the latency for this word once it agrees with the final results .
Therefore , it is necessary to locate the last change for each word .
For a given source side time s , we denote the t th outputs on target side as f ( x s ) t .
Then we are able to find the Last Revision ( LR ) for the t th word on target side as follows : 1 N H 2 Z C 1 x V M 4 0 = " > A A A B 6 n i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e x q R C 9 C 0 I v H i O Y B y R J m J 5 N k y O z s M t M r h C W f 4 M W D I l 7 9 I m / + j Z N k D 5 p Y 0 F B U d d P d F c R S G H T d b y e 3 s r q 2 v p H f L G x t 7 + z u F f c P G i Z K N O N 1 F s l I t w J q u B S K 1 1 G g 5 K 1 Y c x o G k j e D 0 e 3 U b z 5 x b U S k H n E c c z + k A y X 6 g l G 0 0 g N e V 7 r F k l t 2 Z y D L x M t I C T L U u s W v T i 9 i S c g V M k m N a X t u j H 5 K N Q o m + a T Q S Q y P K R v R A W 9 b q m j I j Z / O T p 2 Q E 6 v 0 S D / S t h S S m f p 7 I q W h M e M w s J 0 h x a F Z 9 K b i f 1 4 7 w f 6 V n w o V J 8 g V m y / q J 5 J g R K Z / k 5 7 Q n K E c W 0 K Z F v Z W w o Z U U 4 Y 2 n Y I N w V t 8 e Z k 0 z s r e e f n i v l K q 3 m R x 5 O E I j u E U P L i E K t x B D e r A Y A D P 8 A p v j n R e n H f n Y 9 6 a c 7 K Z Q / g D 5 / M H 2 K + N h A = = < / l a t e x i t > t = 5 < l a t e x i t s h a 1 _ b a s e 6 4 = "
V B n B 3 N x f k o z r s c J f P w x v I b 0 q 3 i U = " > A A A B 6 n i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y q Q S 9 C 0 I v H i O Y B y R J m J 5 N k y O z s M t M r h C W f 4 M W D I l 7 9 I m / + j Z N k D 5 p Y 0 F B U d d P d F c R S G H T d b y e 3 s r q 2 v p H f L G x t 7 + z u F f c P G i Z K N O N 1 F s l I t w J q u B S K 1 1 G g 5 K 1 Y c x o G k j e D 0 e 3 U b z 5 x b U S k H n E c c z + k A y X 6 g l G 0 0 g N e V 7 r F k l t 2 Z y D L x M t I C T L U u s W v T i 9 i S c g V M k m N a X t u j H 5 K N Q o m + a T Q S Q y P K R v R A W 9 b q m j I j Z / O T p 2 Q E 6 v 0 S D / S t h S S m f p 7 I q W h M e M w s J 0 h x a F Z 9 K b i f 1 4 7 w f 6 V n w o V J 8 g V m y / q J 5 J g R K Z / k 5 7 Q n K E c W 0 K Z F v Z W w o Z U U 4 Y 2 n Y I N w V t 8 e Z k 0 z s r e e b l y f 1 G q 3 m R x 5 O E I j u E U P L i E K t x B D e r A Y A D P 8 A p v j n R e n H f n Y 9 6 a c 7 K Z Q / g D 5 / M H 2 j O N h Q = = < / l a t e x i t > LR ( t ) = argmax s<| x | f ( x ( s?1 ) ) t = f ( x s ) t From the audience point of view , once the former words are changed , the audience also needs to take the efforts to read the following as well .
Then we also penalize the later words even there are no changes , which is shown with blue arrow in Fig.
3 . We then re-formulate the LR ( t ) as follows ( assume LR ( 0 ) = 0 ) : LR ( t ) = max { LR ( t ? 1 ) , LR ( t ) } ( 5 ) The above definition can be visualized as the thick black line in Fig.
3 . Similar with original AL , our proposed RAL is defined as follows : RAL ( x , y ) = 1 ? ( | x | ) ? ( | x | ) t=1 LR ( t ) ? t ? 1 r ( 6 ) where ? ( | x | ) denotes the cut-off step , and r = | y | / |x | is the target- to- source length ratio .
Revision Rate Since each modification on the target side would cost extra effort for the audience to read , we penalize all the revisions during the translation .
We define the revision rate as follows : | x|?1 s=1 dist f ( x s ) , f ( x s +1 ) | x| s=1 |f ( x s ) | where dist can be arbitrary distance measurement between two sequences .
For simplicity , we design where pad is a padding symbol in case b is shorter than a .
Experiments Datasets and Implementation
We evaluate our work on Chinese-to-English and English - to - Chinese simultaneous translation tasks .
We use the NIST corpus ( 2 M sentence pairs ) as the training data .
We first apply BPE ( Sennrich et al. , 2015 ) on all texts to reduce the vocabulary sizes .
For evaluation , we use NIST 2006 and NIST 2008 as our dev and test sets with 4 English references .
We re-implement wait -k model and adaptive policy .
We use Transformer ( Vaswani et al. , 2017 ) based waitk model and pre-trained full-sentence model for learning adaptive policy .
Performance on Wait-k Policy
We perform experiments using opportunistic decoding on waitk policies with k ? { 1 , 3 , 5 , 7 , 9 } , opportunistic window w ? { 1 , 3 , 5 } and beam size b ? { 1 , 3 , 5 , 7 , 10 , 15 } .
We select the best beam size for each policy and window pair on dev-set .
We compare our proposed method with a baseline called re-translation which uses a fullsentence NMT model to re-decode the whole target sentence once a new source word is observed .
The final output sentences of this method are identical to the full sentence translation output with the same model but the latency is reduced .
Fig. 4 ( left ) shows the Chinese-to - English results of our proposed algorithm .
Since our greedy opportunistic decoding does n't change the final output , there is no difference in BLEU compared with normal decoding , but the latency is reduced .
However , by applying beam search , we can achieve 3.1 BLEU improvement and 2.4 latency reduction on wait - 7 policy .
Fig. 4 ( right ) shows the English - to - Chinese results .
Compare to the Chinese-to - English translation results in previous section , there is comparatively less latency reduction by using beam search because the output translations are slightly longer which hurts the latency .
As shown in Fig. 5 ( right ) , the revision rate is still controlled under 8 % .
Fig. 5 shows the revision rate with different window size on wait -k policies .
In general , with opportunity window w ?
5 , the revision rate of our proposed approach is under 8 % , which is much lower than re-translation .
Performance on Adaptive Policy Fig. 6 shows the performance of the proposed algorithm on adaptive policies .
We use threshold ? ? { 0.55 , 0.53 , 0.5 , 0.47 , 0.45 } .
We vary beam size b ? { 1 , 3 , 5 , 7 , 10 } and select the best one on devset .
Comparing with conventional beam search on consecutive writes , our decoding algorithm achieves even much higher BLEU and less latency .
We further investigate the revision rate with different beam sizes on wait -k policies .
Fig. 7 shows that the revision rate is higher with lower wait -k policies .
This makes sense because the low k policies are always more aggressive and easy to make mistakes .
Moreover , we can find that the revision rate is not very sensitive to beam size .
Conclusions
We have proposed an opportunistic decoding timely correction technique which improves the latency and quality for simultaneous translation .
We also defined two metrics for revision -enabled simultaneous translation for the first time .
Figure2 : The decoder generates target word y 4 = " his " and two extra words " welcome to " at step t = 4 when input x 9 = " z?nt?ng " ( " agreement " ) is not available yet .
When the model receives x 9 at step t = 5 , the decoder immediately corrects the previously made mistake " welcome " with " agreement " and emits two additional target words ( " to President " ) .
The decoder not only is capable to fix the previous mistake , but also has enough information to perform more correct generations .
Our framework benefits from opportunistic decoding with reduced latency here .
Note though the word " to " is generated in step t = 4 , it only becomes irreversible at step t = 6 .
Figure 3 : 3 Figure3 : The red arrows represent the changes between two different commits , and the last changes for each output word is highlighted with yellow .
Figure 4 : Figure 5 : 45 Figure4 : BLEU against RAL using wait -k polocies . : wait - 1 policies , : wait - 3 policies , : wait - 5 policies , : wait - 7 policies , : wait - 9 policies , ( ) : re-translation with pre-trained NMT model with greedy ( beam search ) decoding , ( ) : full-sentence translation with pre-trained NMT model with greedy ( beam search ) decoding .
The baseline for wait -k policies is decoding with w = 0 , b = 1 .
a modified Hamming Distance to measure the difference : dist ( a , b ) = hamming a , b ?| a| ? pad max ( | a| ? | b| ,0 )
Figure 6 : 6 Figure 6 : BLEU against RAL using adaptive policies .
Baseline is decoded with w = 0 , b = 1 and w = 0 , b > 1 .
Figure 7 : 7 Figure 7 : Revision rate against beam size with window size of 3 and different wait -k policies .
Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 437-442 July 5 - 10 , 2020 .
c 2020 Association for Computational Linguistics
