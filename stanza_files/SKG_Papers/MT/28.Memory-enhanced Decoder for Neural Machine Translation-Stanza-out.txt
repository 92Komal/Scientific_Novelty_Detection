title
Memory-enhanced Decoder for Neural Machine Translation
abstract
We propose to enhance the RNN decoder in a neural machine translator ( NMT ) with external memory , as a natural but powerful extension to the state in the decoding RNN .
This memory - enhanced RNN decoder is called MEMDEC .
At each time during decoding , MEMDEC will read from this memory and write to this memory once , both with content - based addressing .
Unlike the unbounded memory in previous work ( Bahdanau et al. , 2014 ) to store the representation of source sentence , the memory in MEMDEC is a matrix with predetermined size designed to better capture the information important for the decoding process at each time step .
Our empirical study on Chinese-English translation shows that it can improve by 4.8 BLEU upon Groundhog and 5.3 BLEU upon on Moses , yielding the best performance achieved with the same training set .
Introduction
The introduction of external memory has greatly expanded the representational capability of neural network - based model on modeling sequences ( Graves et al. , 2014 ) , by providing flexible ways of storing and accessing information .
More specifically , in neural machine translation , one great improvement came from using an array of vectors to represent the source in a sentencelevel memory and dynamically accessing relevant segments of them ( alignment ) ( Bahdanau et al. , 2014 ) through content - based addressing ( Graves et al. , 2014 ) .
The success of RNNsearch demonstrated the advantage of saving the entire sentence of arbitrary length in an unbounded memory for operations of next stage ( e.g. , decoding ) .
In this paper , we show that an external memory can be used to facilitate the decoding / generation process thorough a memory - enhanced RNN decoder , called MEMDEC .
The memory in MEMDEC is a direct extension to the state in the decoding , therefore functionally closer to the memory cell in LSTM ( Hochreiter and Schmidhuber , 1997 ) .
It takes the form of a matrix with pre-determined size , each column ( " a memory cell " ) can be accessed by the decoding RNN with content - based addressing for both reading and writing during the decoding process .
This memory is designed to provide a more flexible way to select , represent and synthesize the information of source sentence and previously generated words of target relevant to the decoding .
This is in contrast to the set of hidden states of the entire source sentence ( which can viewed as another form of memory ) in ( Bahdanau et al. , 2014 ) for attentive read , but can be combined with it to greatly improve the performance of neural machine translator .
We apply our model on English - Chinese translation tasks , achieving performance superior to any published results , SMT or NMT , on the same training data ( Xie et al. , 2011 ; Meng et al. , 2015 ; Tu et al. , 2016 ; Hu et al. , 2015 )
Our contributions are mainly two -folds ?
we propose a memory - enhanced decoder for neural machine translator which naturally extends the RNN with vector state .
? our empirical study on Chinese-English translation tasks show the efficacy of the proposed model .
Roadmap
In the remainder of this paper , we will first give a brief introduction to attentionbased neural machine translation in Section 2 , presented from the view of encoder-decoder , which treats the hidden states of source as an unbounded memory and the attention model as a content - based reading .
In Section 3 , we will elaborate on the memory - enhanced decoder MEMDEC .
In Section 4 , we will apply NMT with MEMDEC to a Chinese -English task .
Then in Section 5 and 6 , we will give related work and conclude the paper .
Neural machine translation with attention
Our work is built on attention - based NMT ( Bahdanau et al. , 2014 ) , which represents the source sentence as a sequence of vectors after being processed by RNN or bidirectional RNNs , and then conducts dynamic alignment and generation of the target sentence with another RNN simultaneously .
Attention - based NMT , with RNNsearch as its most popular representative , generalizes the conventional notion of encoder-decoder in using a unbounded memory for the intermediate representation of source sentence and content - based addressing read in decoding , as illustrated in Figure 1 .
More specifically , at time step t , RNNsearch first get context vector c t after reading from the source representation M S , which is then used to update the state , and generate the word y t ( along with the current hidden state s t , and the previously generated word y i ?1 ) .
Formally , given an input sequence x = [ x 1 , x 2 , . . . , x Tx ] and the previously generated sequence y <t = [ y 1 , y 2 , . . . , y t?1 ] , the probability of next word y t is p(y t |y <t ; x ) = f ( c t , y t?1 , s t ) , ( 1 ) where s t is state of decoder RNN at time step t calculated as s t = g(s t?1 , y t?1 , c t ) . ( 2 ) where g( ? ) can be an be any activation function , here we adopt a more sophisticated dynamic operator as in Gated Recurrent Unit ( GRU , ( Cho et al. , 2014 ) ) .
In the remainder of the paper , we will also use GRU to stand for the operator .
The reading c t is calculated as c t = j=Tx j=1 ? t , j h j , ( 3 ) where h j is the j th cell in memory M S .
More formally , h j = [ ? ? h j , ? ? h j ] is the annotations of x j and contains information about the whole input sequence with a strong focus on the parts surrounding x j , which is computed by a bidirectional RNN .
The weight ?
t , j is computed by ?
t , j = exp( e t , j ) k=Tx k=1 exp( e t , k ) . where e i , j = v T a tanh ( W a s t?1 + U a h j ) scores how well s t?1 and the memory cell h j match .
This is called automatic alignment ( Bahdanau et al. , 2014 ) or attention model ( Luong et al. , 2015 ) , but it is essentially reading with content - based addressing defined in ( Graves et al. , 2014 ) .
With this addressing strategy the decoder can attend to the source representation that is most relevant to the stage of decoding .
Improved Attention Model
The alignment model ?
t , j scores how well the output at position t matches the inputs around position j based on s t?1 and h j .
It is intuitively beneficial to exploit the information of y t?1 when reading from M S , which is missing from the implementation of attention - based NMT in ( Bahdanau et al. , 2014 ) .
In this work , we build a more effective alignment path by feeding both previous hidden state s t?1 and the context word y t?1 to the attention model , inspired by the recent implementation of attention - based NMT 1 .
Formally , the calculation of e t , j becomes e t , j = v T a tanh ( W a st ?1 + U a h j ) , where ? st ?1 = H( s t?1 , e y t?1 ) is an intermediate state tailored for reading from M S with the information of y t?1 ( its word embedding being e y t?1 ) added ; ?
H is a nonlinear function , which can be as simple as tanh or as complex as GRU .
In our preliminary experiments , we found GRU works slightly better than tanh function , but we chose the latter for simplicity .
1 github.com/nyu-dl/dl4mt-tutorial / tree/master/session2
Decoder with External Memory
In this section we will elaborate on the proposed memory - enhanced decoder MEMDEC .
In addition to the source memory M S , MEMDEC is equipped with a buffer memory M B as an extension to the conventional state vector .
In the remainder of the paper , we will refer to the conventional state as vector-state ( denoted s t ) and its memory extension as memory -state ( denoted as M B t ) .
Both states are updated at each time step in a interweaving fashion , while the output symbol y t is predicted based solely on vectorstate s t ( along with c t and y t?1 ) .
The diagram of this memory - enhanced decoder is given in Figure 2 . Vector-State Update
At time t , the vector-state s t is first used to read M B r t?1 = read B ( s t?1 , M B t?1 ) ( 4 ) which then meets the previous prediction y t?1 to form an " intermediate " state - vector s t = tanh ( W r r t?1 + W y e y t?1 ) . ( 5 ) where e y t?1 is the word-embedding associated with the previous prediction y t?1 .
This pre-state s t is used to read the source memory M S c t = read S ( s t , M S ) .
( 6 ) Both readings in Eq. ( 4 ) & ( 6 ) follow contentbased addressing ( Graves et al. , 2014 ) ( details later in Section 3.1 ) .
After that , r t?1 is combined with output symbol y t?1 and c t to update the new vector-state s t = GRU ( r t?1 , y t?1 , c t ) ( 7 )
The update of vector-state is illustrated in Figure 4 . Memory - State Update
As illustrated in Figure 5 , the update for memory -state is simple after the update of vector-state : with the vector-state s t+1 the updated memory - state will be M B t = write ( s t , M B t?1 ) ( 8 )
The writing to the memory -state is also contentbased , with same forgetting mechanism suggested in ( Graves et al. , 2014 ) , which we will elaborate with more details later in this section .
Prediction
As illustrated in Figure 6 , the prediction model is same as in ( Bahdanau et al. , 2014 ) , where the score for word y is given by score ( y ) = DNN ( [ s t , c t , e y t?1 ] ) ? y ( 9 ) where ?
y is the parameters associated with the word y .
The probability of generating word y at time t is then given by a softmax over the scores p( y|s t , c t , y t?1 ) = exp(score ( y ) ) y exp(score(y ) ) .
Reading Memory - State Formally M B t ?
R n?m is the memory -state at time t after the memory -state update , where n is the number of memory cells and m is the dimension of vector in each cell .
Before the vector-state update at time t , the output of reading r t is given by r t = j=n j=1 w R t ( j ) M B t?1 ( j ) where w R t ?
R n specifies the normalized weights assigned to the cells in M B t .
Similar with the reading from M S ( a.k.a. attention model ) , we use content - based addressing in determining w R t .
More specifically , w R t is also updated from the one from previous time w R t?1 as w R t = g R t w R t?1 + ( 1 ? g R t ) w R t , ( 10 ) where ?
g R t = ?( w
R g s t ) is the gate function , with parameters w R g ? R m ; ?
w t gives the contribution based on the current vector-state s t w R t = softmax ( a R t ) ( 11 ) a R t ( i ) = v ( W R a M B t?1 ( i ) + U R a s t?1 ) , ( 12 ) with parameters W R a , U R a ?
R m?m and v ?
R m .
Writing to Memory - State
There are two types of operation on writing to memory -state : ERASE and ADD .
Erasion is similar to the forget gate in LSTM or GRU , which determines the content to be remove from memory cells .
More specifically , the vector ?
ERS t ?
R m specifies the values to be removed on each dimension in memory cells , which is than assigned to each cell through normalized weights w W t .
Formally , the memory -state after ERASE is given by M B t ( i ) = M B t?1 ( i ) ( 1 ? w W t ( i ) ? ? ERS t ) ( 13 ) i = 1 , ? ? ? , n where ? ? ERS t = ?( W ERS s t ) is parametrized with W ERS ? R m?m ; ? w W t ( i ) specifies the weight associated with the i th cell in the same parametric form as in Eq. ( 10 ) -( 12 ) with generally different parameters .
ADD operation is similar with the update gate in LSTM or GRU , deciding how much current information should be written to the memory .
M B t ( i ) = M B t ( i ) + w W t ( i ) ?
ADD t ? ADD t = ?( W ADD s t ) where ?
ADD t ?
R m and W ADD ? R m?m .
In our experiments , we have a peculiar but interesting observation : it is often beneficial to use the same weights for both reading ( i.e. , w R t in Section 3.1 ) and writing ( i.e. , w W t in Section 3.2 ) for the same vector-state s t .
We conjecture that this acts like a regularization mechanism to encourage the content of reading and writing to be similar to each other .
Some Analysis
The writing operation in Eq. ( 13 ) at time t can be viewed as an nonlinear way to combine the previous memory -state M B t?1 and the newly updated vector-state s t , where the nonlinearity comes from both the content - based addressing and the gating .
This is in a way similar to the update of states in regular RNN , while we conjecture that the addressing strategy in MEMDEC makes it easier to selectively change some content updated ( e.g. , the relatively short- term content ) while keeping other content less modified ( e.g. , the relatively long- term content ) .
The reading operation in Eq. ( 10 ) can " extract " the content from M B t relevant to the alignment ( reading from M S ) and prediction task at time t.
This is in contrast with the regular RNN decoder including its gated variants , which takes the entire state vector to for this purpose .
As one advantage , although only part of the information in M B t is used at t , the entire memory -state , which may store other information useful for later , will be carry over to time t + 1 for memory -state update ( writing ) .
Experiments on Chinese-English Translation
We test the memory - enhanced decoder to task of Chinese- to - English translation , where MEMDEC is put on the top of encoder same as in ( Bahdanau et al. , 2014 ) .
Datasets and Evaluation metrics
Our training data for the translation task consists of 1.25 M sentence pairs extracted from LDC corpora 2 , with 27.9M Chinese words and 34.5M
Training details
We initialize the recurrent weight matrices as random orthogonal matrices .
All the bias vectors were initialize to zero .
For other parameters , we initialize them by sampling each element from the Gaussian distribution of mean 0 and variance 0.01 2 . Parameter optimization is performed using stochastic gradient descent .
Adadelta ( Zeiler , 2012 ) is used to automatically adapt the learning rate of each parameter ( = 10 ?6 and ? = 0.95 ) .
To avoid gradients explosion , the gradients of the cost function which had 2 norm larger than a predefined threshold 1.0 was normalized to the threshold ( Pascanu et al. , 2013 ) .
Each SGD is of a minibatch of 80 sentences .
We train our NMT model with the sentences of length up to 50 words in training data , while for moses system we use the full training data .
Memory Initialization
Each memory cell is initialized with the source sentence hidden state computed as M B ( i ) = m + ? i ( 14 ) m = ?( W INI i=Tx i=0 h i ) /T x ( 15 ) LDC2004T08 and LDC2005T06 .
where W INI ? R m?2 ? m ; ? is tanh function .
m makes a nonlinear transformation of the source sentence information . ?
i is a random vector sampled from N ( 0 , 0.1 ) .
Dropout we also use dropout for our NMT baseline model and MEMDEC to avoid overfitting ( Hinton et al. , 2012 ) .
The key idea is to randomly drop units ( along with their connections ) from the neural network during training .
This prevents units from co-adapting too much .
In the simplest case , each unit is omitted with a fixed probability p , namely dropout rate .
In our experiments , dropout was applied only on the output layer and the dropout rate is set to 0.5 .
We also try other strategy such as dropout at word embeddings or RNN hidden states but fail to get further improvements .
Pre-training For MEMDEC , the objective function is a highly non-convex function of the parameters with more complicated landscape than that for decoder without external memory , rendering direct optimization over all the parameters rather difficult .
Inspired by the effort on easing the training of very deep architectures ( Hinton and Salakhutdinov , 2006 ) , we propose a simple pre-training strategyFirst we train a regular attention - based NMT model without external memory .
Then we use the trained NMT model to initialize the parameters of encoder and parameters of MEMDEC , except those related to memory -state ( i.e. , { W R a , U R a , v , w R g , W ERS , W ADD } ) .
After that , we fine-tune all the parameters of NMT with MEMDEC decoder , including the parameters initialized with pre-training and those associated with accessing memory -state .
Comparison systems
We compare our method with three state - of- theart systems : ? RNNSearch : an attention - based NMT model with default settings .
We use the open source system GroundHog as our NMT baseline 4 . ?
Coverage model : a state - of- the - art variant of attention - based NMT model ( Tu et al. , 2016 ) which improves the attention mechanism through modelling a soft coverage on the source representation .
Results
The main results of different models are given in Table 1
This further verifies our conjecture the the external memory is mostly used to store part of the source and history of target sentence .
Case study
We show in
Related Work
There is a long thread of work aiming to improve the ability of RNN in remembering long sequences , with the long short- term memory RNN ( LSTM ) ( Hochreiter and Schmidhuber , 1997 ) being the most salient examples and GRU ( Cho et al. , 2014 ) being the most recent one .
Those works focus on designing the dynamics of the RNN through new dynamic operators and appropriate gating , while still keeping vector form RNN states .
MEMDEC , on top of the gated RNN , explicitly adds matrix -form memory equipped with content - based addressing to the system , hence greatly improving the power of the decoder RNN in representing the information important for the translation task .
MEMDEC is obviously related to the recent effort on attaching an external memory to neural networks , with two most salient examples being Neural Turing Machine ( NTM ) ( Graves et al. , 2014 ) and Memory Network ( Weston et al. , 2014 ) .
In fact MEMDEC can be viewed as a special case of NTM , with specifically designed reading ( from two different types of memory ) and writing mechanism for the translation task .
Quite remarkably MEMDEC is among the rare instances of NTM which significantly improves upon state - of - the -arts on a real-world NLP task with large training corpus .
Our work is also related to the recent work on machine reading ( Cheng et al. , 2016 ) , in which the machine reader is equipped with a memory tape , enabling the model to directly read all the previous hidden state with an attention mechanism .
Different from their work , we use an external bounded memory and make an abstraction of previous information .
In ( Meng et al. , 2015 ) , Meng et. al. also proposed a deep architecture for sequence - to-sequence learning with stacked layers of memory to store the intermediate representations , while our external memory was applied within a sequence .
Conclusion
We propose to enhance the RNN decoder in a neural machine translator ( NMT ) with external memory .
Our empirical study on Chinese-English translation shows that it can significantly improve the performance of NMT .
Figure 1 : 1 Figure 1 : RNNsearch in the encoder-decoder view .
