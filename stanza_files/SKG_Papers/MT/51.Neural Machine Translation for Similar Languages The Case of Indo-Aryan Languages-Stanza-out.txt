title
Neural Machine Translation for Similar Languages :
The Case of Indo-Aryan Languages
abstract
In this paper we present the WIPRO - RIT systems submitted to the Similar Language Translation shared task at WMT 2020 .
The second edition of this shared task featured parallel data from pairs / groups of similar languages from three different language families : Indo-Aryan languages ( Hindi and Marathi ) , Romance languages ( Catalan , Portuguese , and Spanish ) , and South Slavic Languages ( Croatian , Serbian , and Slovene ) .
We report the results obtained by our systems in translating from Hindi to Marathi and from Marathi to Hindi .
WIPRO - RIT achieved competitive performance ranking 1 st in Marathi to Hindi and 2 nd in Hindi to Marathi translation among 22 systems .
Introduction WMT 2020 is the fifth edition of WMT as a conference following a series of well - attended workshops that date back to 2006 .
WMT became a well - established conference due to its blend of research papers and popular shared tasks on different topics such as translation in various domains ( e.g. biomedical , news ) , translation quality estimation , and automatic post-editing .
The competitions co-organized with WMT provide important datasets and benchmarks widely used in the MT community .
The vast majority of these tasks so far , however , involved training systems to translate to and from English ( Bojar et al. , 2016 ( Bojar et al. , , 2017 while only a few of them addressed the problem of translating between pairs of languages with less resources .
To address this issue , in 2019 , the Similar Language Translation ( SLT ) shared task was introduced at WMT .
SLT 's purpose was to evaluate the performance of state - of - the - art MT systems on translating between pairs of similar languages without English as a pivot language ( Barrault et al. , 2019 ) .
The organizers provided participants with training , development , and testing parallel data from three pairs of languages from three different language families : Spanish - Portuguese ( Romance languages ) , Czech - Polish ( Slavic languages ) , and Hindi - Nepali ( Indo - Aryan languages ) .
Systems were evaluated using automatic metrics , namely BLEU ( Papineni et al. , 2002 ) and TER ( Snover et al. , 2006 ) .
In SLT 2020 , the task organizes once again included an Indo- Aryan language track with Hindi and Marathi .
Indo -Aryan languages are a sub-family of the Indo-European language family which includes Bengali , Bohjpuri , Hindi , Marathi , and Nepali .
These languages are mainly spoken in North and Central India , and some neighbouring countries such as Nepal , Bangladesh , and Pakistan etc .
The script used in most of these languages are derived from the ancient Brahmi script and enriched with high grapheme to phoneme correspondence leading to many orthographic similarities across these languages .
In addition to Hindi and Marathi , SLT 2020 features two other tracks with similar languages from the following language families : Romance languages ( Catalan , Portuguese , and Spanish ) and South Slavic Languages ( Croatian , Serbian , and Slovene ) .
In this paper we describe the WIPRO - RIT submission to the SLT 2020 Indo - Aryan track .
Our WIPRO - RIT system is based on the model described in Johnson et al . ( 2017 ) . WIPRO - RIT achieved competitive performance ranking 1 st in Marathi to Hindi and 2 nd in Hindi to Marathi translation among 22 systems .
Related Work
With the substantial performance improvements brought to MT by neural approaches , a growing interest in translating between pairs of similar languages , language varieties , and dialects has been observed .
Recent studies have addressed MT between Arabic dialects ( Harrat et al. , 2019 ; Shapiro and Duh , 2019 ) Catalan and Spanish , Croatian and Serbian ( Popovi ?
et al. , 2020 ) , ( Costa-juss ? , 2017 ) , Brazilian and European Portuguese ( Costajuss ?
et al. , 2018 ) , and several pairs of languages and language varieties such as Brazilian and European Portuguese , Canadian and European French , and similar languages such as Croatian and Serbian , and Indonesian and Malay ( Lakew et al. , 2018 ) .
The interest on diatopic language variation is evidenced by the recent iterations of the Var-Dial workshop in which papers on MT applied to similar languages varieties , and dialects ( Shapiro and Duh , 2019 ; Myint Oo et al. , 2019 ; Popovi ?
et al. , 2020 ) have been presented along with evaluation campaigns featuring multiple shared tasks on a number of related topics such as cross-lingual morphological analysis , cross-lingual parsing , dialect identification , and morphosyntactic tagging ( Zampieri et al. , 2018 ( Zampieri et al. , , 2019 G?man et al. , 2020 ) .
Data
For our experiments , we use the Hindi-Marathi and Marathi-Hindi WMT 2020 SLT data .
The released parallel dataset was collected from news ( Siripragada et al. , 2020 ) , PMIndia ( Haddow and Kirefu , 2020 ) and Indic Wordnet ( Bhattacharyya , 2010 ; Kunchukuttan , 2020a ) datasets .
To augment our dataset , we use English -Hindi parallel data released in WMT 2014 ( Bojar et al. , 2014 ) , consisting of more than 2 million parallel sentences , which is available as an additional resource .
We use a subset of 5 million segments of Hindi monolingual news crawled from ca. 32 million data .
We also use a subset 5 million Marathi monolingual data .
We performed similar cleaning and pre-processing methods as we described in case of parallel data .
The five million Hindi monolingual sentences were first back - translated to English using a Hindi-English NMT system .
The Hindi-English NMT system was trained on English -Hindi parallel data released in WMT 2014 ( Bojar et al. , 2014 ) , IITB parallel corpus ( Kunchukuttan et al. , 2018 ) , the parallel dataset was collected from news ( Siripragada et al. , 2020 ) and the PMIndia ( Haddow and Kirefu , 2020 ) parallel corpus ( see Table 1 ) .
We also back - translated 5 million Marathi monolingual sehments using our WIPRO - RIT CONTRASTIVE 1 system described in more detail Section 6 .
For Marathi-Hindi we did not use any back translation data in our CON -TRASTIVE 2 and PRIMARY submissions .
In the both cases 5 million English -Hindi backtranslation data provide significant ( p < 0.01 ) improvements over CONTRASTIVE 1 ( detailed in Section 6 ) .
The released WMT 2014 EN - HI data and the WMT SLT 2020 data were noisy for our purposes , so we apply methods for cleaning ( see data statistics in We performed the following two steps : ( i ) we use the cleaning process described in , and ( ii ) we execute the Moses ( Koehn et al. , 2007 ) corpus cleaning scripts with minimum and maximum number of tokens set to 1 and 100 , respectively .
After cleaning and re- moving duplicates , we have 1M EN - HI parallel sentences .
Next , we perform punctuation normalization , and then we use the Moses tokenizer to tokenize the English side of the parallel corpus with ' no- escape ' option .
Finally , we apply true-casing .
For the case of Hindi and Marathi , we use Indic NLP Library 1 ( Kunchukuttan , 2020 b ) for tokenizaton .
Model Architecture
Our model is based on a transformer architecture ( Vaswani et al. , 2017 ) built solely upon such attention mechanisms completely replacing recurrence and convolutions .
The transformer uses positional encoding to encode the input and output sequences , and computes both self - and cross-attention through so-called multi-head attentions , which are facilitated by parallelization .
We use multi-head attention to jointly attend to information at different positions from different representation subspaces .
We present a single multilingual NMT system based on the transformer architecture that can translate between multiple languages .
To make use of multilingual data within a single NMT model , we perform one simple modification to the source side of the multilingual data , we use an additional token at the beginning of the each source sentence to indicate the target language by the NMT model would be translated as shown in Table 3 .
We train the model with all the processed multilingual data consisting of sen - 1 https://github.com/anoopkunchukuttan/ indic_nlp_library / tence aligned multiple language pairs at once , During inference , we also need to add the aforementioned additional token to each input source sentence of the source data to specify the desired target language .
Experiments
In the next sub-sections we describe the experiments we carried out for translating from Hindi to Marathi and from Marathi to Hindi for WIPRO - RIT 's WMT 2020 SLT shared task submission .
Experiment Setup
To handle out - of- vocabulary words and to reduce the vocabulary size , instead of considering words , we consider subword units ( Sennrich et al. , 2016 ) by using byte-pair encoding ( BPE ) .
In the preprocessing step , instead of learning an explicit mapping between BPEs in the English ( EN ) , Hindi ( HI ) and Marathi ( MR ) , we define BPE tokens by jointly processing all parallel data .
Thus , all derive a single BPE vocabulary .
Since HI and MR belong to the similar languages , they naturally share a good fraction of BPE tokens , which reduces the vocabulary size .
We report evaluation results ( evaluated by the shared task organizers ) of our approach with the released Test data .
BLEU ( Papineni et al. , 2002 ) , RIBES ( Isozaki et al. , 2010 ) and TER ( Snover et al. , 2006 )
Hyper-parameter Setup
We follow a similar hyper-parameter setup for all reported systems .
All encoders , and the decoder , are composed of a stack of N X = 6 identical layers followed by layer normalization .
Each layer again consists of two sublayers and a residual connection ( He et al. , 2016 ) around each of the two sub-layers .
We apply dropout ( Srivastava et al. , 2014 ) to the output of each sub-layer , before it is added to the sub-layer input and normalized .
Furthermore , dropout is applied to the sums of the word embeddings and the corresponding positional encodings in both encoders as well as the decoder stacks .
We set all dropout values in the network to 0.1 .
During training , we employ label smoothing with value ? ls = 0.1 .
The output dimension produced by all sub-layers and embedding layers is d model = 512 .
Each encoder and decoder layer contains a fully connected feedforward network ( F F N ) having dimensionality of d model = 512 for the input and output and dimensionality of d f f = 2048 for the inner layers .
For the scaled dot-product attention , the input consists of queries and keys of dimension d k , and values of dimension d v .
As multi-head attention parameters , we employ h = 8 for parallel attention layers , or heads .
For each of these we use a dimensionality of d k = d v = d model / h = 64 .
For optimization , we use the Adam optimizer ( Kingma and Ba , 2015 ) with ?
1 = 0.9 , ? 2 = 0.98 and ? = 10 ?9 .
The learning rate is varied throughout the training process , and increasing for the first training steps warmup steps = 16000 and afterwards decreasing as described in ( Vaswani et al. , 2017 ) .
All remaining hyper-parameters are set analogously to those of the transformer 's base model .
At training time , the batch size is set to 25 K tokens , with a maximum sentence length of 256 subwords , and a vocabulary size of 32K .
After each epoch , the training data is shuffled .
During decoding , we perform beam search with a beam size of 4 .
We use 32 K BPE operations to train our BPE models .
We use shared embeddings in all our experiments .
Results
We present the results obtained by our systems for Hindi-Marathi in Table 5 and for Marathi-Hindi in Table 6 in terms of BLEU , RIBES , and TER .
We apply our proposed method to train multilingual models in three different configurations .
CONTRASTIVE 1 ( C1 ) Our CON -TRASTIVE 1 submission is a multilingual single system and does not use any monolingual back translation data .
The system is trained on the released HI - MR and MR - HI parallel data .
In addition to we also use EN - HI parallel data .
CONTRASTIVE 2 ( C2 )
This submission is similar to CONTRASTIVE 1 , however in this case we used 5 M back - translated Marathi-Hindi and 5 M back - translated Hindi-Marathi corpus .
Source back - translated sentences begin with an additional token indicating the target language .
PRIMARY ( P ) Our primary submission is trained using the same setting as we described in CONTRASTIVE 2 system .
The difference is our primary system is an ensemble of three different CONTRASTIVE 2 systems initiated with three different random seeds .
Conclusion and Future Work
This paper presented the WIPRO - RIT system submitted to the Similar Language Translation shared task at WMT 2020 .
We presented the results obtained by our system in translating from Hindi to Marathi and Marathi to Hindi .
Our primary system achieved competitive performance ranking first in Marathi to Hindi and second in Hindi to Marathi among 22 teams in terms of BLEU score .
In future work , we would like to further explore the similarity between these two languages in translating to other Indo-Aryan languages ( e.g. Bengali , Bhojpuri , and Nepali ) .
We expect the models presented in this paper to perform well for other Indo - Aryan language provided that suitable training data is available .
Furthermore , we would like to apply and evaluate our method on the two other groups of languages in the WMT SLT 2020 shared task , Romance languages : Catalan , Portuguese , and Spanish , and South Slavic languages : Croatian , Serbian , and Slovene .
Finally , we will be incorporating the translation models presented in this paper to CATa - Log , an open-source online CAT tool that provides users with both MT and TM outputs ( Nayek et al. , 2015 ; Pal et al. , 2016 ) . are used to evaluate the performance of all participating systems in the shared task .
The training criteria data statistics of our submitted systems ( C1 = Contrastive 1 , C2 = Contrastive 2 , P = Primary , and BT = Back- translated data ) .
