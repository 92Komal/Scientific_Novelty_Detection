title
Generating Diverse Translation from Model Distribution with Dropout
abstract
Despite the improvement of translation quality , neural machine translation ( NMT ) often suffers from the lack of diversity in its generation .
In this paper , we propose to generate diverse translations by deriving a large number of possible models with Bayesian modelling and sampling models from them for inference .
The possible models are obtained by applying concrete dropout to the NMT model and each of them has specific confidence for its prediction , which corresponds to a posterior model distribution under specific training data in the principle of Bayesian modeling .
With variational inference , the posterior model distribution can be approximated with a variational distribution , from which the final models for inference are sampled .
We conducted experiments on Chinese-English and English - German translation tasks and the results shows that our method makes a better trade - off between diversity and accuracy .
Introduction
In the past several years , neural machine translation ( NMT ) ( Kalchbrenner and Blunsom , 2013 ; Gehring et al. , 2017 ; Vaswani et al. , 2017 ; Zhang et al. , 2019 ) based on the end-to - end model has achieved impressive progress in improving the accuracy of translation .
Despite its remarkable success , NMT still faces problems in diversity .
In natural language , due to lexical , syntactic and synonymous factors , there are usually multiple proper translations for a sentence .
However , existing NMT models mostly implement one - to - one mapping between natural languages , that is , one source language sentence corresponds to one target language sentence .
Although beam search , a widely used decoding algorithm , can generate a group of translations , its search space is too narrow to extract diverse translations .
There are some researches working at enhancing translation diversity in recent years .
Li et al. ( 2016 ) and Vijayakumar et al . ( 2016 ) proposed to add regularization terms to the beam search algorithm so that it can possess greater diversity .
He et al. ( 2018 ) and Shen et al . ( 2019 ) introduced latent variables into the NMT model , thus the model can generate diverse outputs using different latent variables .
Moreover , Sun et al. ( 2019 ) proposed to combine the structural characteristics of Transformer and use the different weights between each head in the multi-head attention mechanism to obtain diverse results .
In spite of improvement in balancing accuracy and diversity , these methods do not represent the diversity in the NMT model directly .
In this paper , we take a different approach to generate diverse translation by explicitly maintaining different models based on the principle of Bayesian Neural Networks ( BNNs ) .
These models are derived by applying concrete dropout ( Gal et al. , 2017 ) to the original NMT model and each of them is given a probability to show its confidence in generation .
According to Bayesian theorem , the probabilities over all the possible models under a specific training dataset forms a posterior model distribution which should be involved at inference .
To make the posterior model distribution obtainable at inference , we further employ variational inference ( Hinton and Van Camp , 1993 ; Neal , 1995 ; Graves , 2011 ; Blundell et al. , 2015 ) to infer a variational distribution to approximate it , then at inference we can sample a specific model based on the variational distribution for generation .
We conducted experiments on the NIST Chinese-English and the WMT '14 English - German translation tasks and compared our method with different strong baseline approaches .
The experiment results show that our method can get a good trade - off in translation diversity and accuracy with little train-ing cost .
Our contributions in this paper are as follows : ?
We introduce Bayesian neural networks with variational inference to NMT tasks to explicitly maintain different models for diverse generation .
?
We apply concrete dropout to the NMT model to derive the possible models which only demands a small cost in computation .
Background Assume a source sentence with n words x = x 1 , x 2 , ... , x n , and its corresponding target sentence with m words y = y 1 , y 2 , ... , y m , NMT models the probability of generating y with x as the input .
Based on the encoder-decoder framework , NMT model ?
encodes source sentence into hidden states by its encoder , and uses its decoder to find the probability of t-th word y t , which depends on the hidden states and the first t ?
1 words of target sentence y.
The translation probability from sentence x and y can be expressed as : P ( y|x ) = m t=1 P ( y t |y <t , x ; ? ) ( 1 ) Given a training dataset with source - target sentence pairs D = {( x 1 , y * 1 ) , ... , ( x D , y * D ) } , the loss function we want to minimize in the training is the sum of negative log-likelihood of Equation 1 : L = ? ( x i ,y * i ) ?
D log P ( y = y * i |x = x i ; ? ) ( 2 ) In practice , by properly designing neural network structures and training strategies , we can get the specific model parameters that minimize Equation 2 and obtain translation results through the model with beam search .
One of the most popular model in NMT is Transformer , which was proposed by Vaswani et al . ( 2017 ) .
Without recurrent and convolutional networks , Transformer constructs its encoder and decoder by stacking self-attention and fullyconnected network layers .
Self attention is operated with three inputs : query ( Q ) , key ( K ) and value ( V ) as : Attention ( Q , K , V ) = softmax ( QK T ? d k V ) ( 3 ) where the dimension of key is d k .
Note that Transformer implements the multihead attention mechanism , projecting inputs into h group inputs to generate h different outputs in Equation 3 , and these outputs are concatenated and projected into final outputs : head i = Attention ( QW Q i , KW K i , V W V i ) ( 4 ) Output = Concat ( head 1 , ... , head h ) W O ( 5 ) where W Q i , W K i , W V i and W O are the projection matrices .
The output of Equation 5 is then fed into the fully - connected layer named feed -forward network .
The feed -forward network uses two linear networks and a ReLU activation function : FFN ( x ) = ReLU ( xW 1 + b 1 ) W 2 + b 2 ( 6 ) We only give a brief description of Transformer above .
Please refer to Vaswani et al . ( 2017 ) Following Gal et al. ( 2017 ) , we employ Bayesian neural networks ( BNNs ) to represent the P ( ? |X , Y ) , which is the posterior distribution of the models under ( X , Y ) .
BNNs offer a probabilistic interpretation of deep learning models by inferring distributions over the models ' parameters which are trained using Bayesian inference .
The posterior distribution can be got by invoking Bayes ' theorem as : P ( ? |X , Y ) = P ( Y |X , ?) P ( ? ) P ( Y |X ) = P ( Y |X , ?) P ( ? ) E ? [ P ( Y |X , ? ) ] . ( 7 ) Then according to BNNs , given a new test data x , the predictive distribution of the output y is : P ( y |x , X , Y ) = E ?P ( ?|X , Y ) [ P ( y |x , ? ) ] ( 8 )
The expectations in Equation 7 and 8 are integrated over model distribution ? , and the huge space of ? makes it intractable to obtain the results .
Therefore , inspired by Hinton and Van Camp ( 1993 ) , Graves ( 2011 ) proposes a variational approximation method , using a variational distribution Q ( ?|? ) with the parameters ? to approximate the posterior distribution P ( ? |X , Y ) .
To this end , the training objective is to minimize the Kullback - Leibler ( KL ) divergence between the model distribution and the posterior distribution KL ( Q ( ?|? ) ||P ( ?|X , Y ) ) .
With variational inference , the objective is equivalent to maximizing its evidence lower bound ( ELBO ) , so we get ? * = arg max ? E Q (?|? ) log P ( Y |X , ? ) ? KL ( ( Q ( ?|? ) ||P ( ? ) ) ( 9 ) As we can see in Equation 9 , the first term on the right side is the expectation of the predicted probability over model distribution on the training set , which can be unbiased estimated with the Monte - Carlo method .
And the second term is the KL divergence between the approximate model distribution and the prior distribution .
From the perspective of Hinton and Van Camp ( 1993 ) and Graves ( 2011 ) , with the above objective , we can express model uncertainty under the training data and meanwhile regularize model parameters and avoid over-fitting .
Therefore , at inference , we can use the distribution Q ( ?|? ) instead of P ( ? |X , Y ) to evaluate model confidence ( i.e. , model uncertainty ) .
Model distribution with Dropout
To derive the BNN , we need to first decide how to explore for the possible models and then decide the prior distribution and the variational distribution for the models .
As in Gal et al. , 2017 , we can define a simple model with parameters ?
W ( W ? R m?n ) and then drop out some column of ?
W to get the possible models .
We use matrix Gaussian distribution as the prior model distribution and Bernoulli distribution as the posterior model distribution .
Using W .j to denote the j-th column of W , we draw a matrix Gaussian distribution as the probability distribution of dropping out the j-th column as P (? W .j ) ? MN (? W .j ; 0 , I/l , I /l ) ( 10 ) where l is the hyper-parameter .
The above matrix Gaussian distribution is used as the prior distribution of the models got by dropping out the j-th column .
Then we introduce p ( p ? R 1?n ) as the probability vector of dropping out the columns of ?
W , which means dropping out the j-th column with the probability of p j , and keeping the j-th column unchanged with the probability of 1 ? p j .
Therefore the posterior model distribution of dropping out the j-th column is defined as Q (?
W .j |? ) = 1 ? p j ? W .j = W .j p j ? W .j = 0 ( 11 ) where W ? ? and p ? ? are trainable parameters .
With Equation 10 as prior , the KL divergence for the j-th column of the matrix can be represented as : KL ( Q ( ? W .j |? ) ||P (? W .j ) ) =R ( p W .j , W .j , l ) ? H( p W .j ) ( 12 ) where R(p W .j , W .j , l ) = ( 1 ? p j ) l 2 2 m i=1 W 2 ij ( 13 ) and H(p W .j ) = ?[ p j log( p j ) + ( 1 ? p j ) log ( 1 ? p j ) ] ( 14 )
Since the probability distribution among different neural networks and different columns of neural network are independent .
For a complex multilayer neural network ? , the KL divergence between model distribution Q ( ?|? ) and prior distribution P ( ? ) is KL ( Q (?|? ) ||P ( ? ) ) = W m?n , p? n j=1 R ( p W .j , W .j , l ) ? H( p W .j ) ( 15 )
Application to Transformer Previous sections show how to use concrete dropout to realize variational approximation of the posterior model distribution .
In this section we will introduce the implementation in representing model distribution for Transformer with aforementioned methods .
Dropout in Transformer Stated in detail in Vaswani et al. ( 2017 ) , in Transformer , dropout is commonly used to the output of modules , including the output of embedding , attention layer and feed -forward layer .
Also , from Equation 13 , we find it 's important to find the network W corresponding to the dropout module .
The correspondences in Transformer are as follows :
Embedding module Embedding module works for mapping the words into embedding vectors .
The embedding module contains a matrix W E ?
R l d ?d , where l d is the length of dictionary and d is the dimension of embedding vector .
For the i-th word in the dictionary , its embedding vector is the i-th column of W E .
Since dropout the jth dimension of word embedding is equivalent to dropping out the j-th row of W E , we utilize W T E and its corresponding dropout in Equation 13 . Attention module For attention modules , as we can see in 5 , their outputs are generated by concatenating the output of different heads and projecting by matrix W O .
Since dropout is used in the output of attention module , we take W O and its corresponding dropout in calculating Equation 13 . Feed-forward module
As shown in Equation 6 , the output is generated through W 2 with bias b 2 .
As we can see , for network y = xW + b , we can find that y = Concat ( x , 1 ) ? Concat ( W T , b T ) T ( 16 ) as we can see , dropout to the output of the feed -forward module can be regraded as dropping out W 2 and b 2 .
So , during training , we use Concat ( W T , b T ) T to calculate Equation 13 .
Training and Inference Although dropout is frequently utilized in Transformer , there are some networks in Transformer like 4 , and their output is not masked by dropout .
So , in our implementation , we obtain the model distribution by fine-tuning the pre-trained model , freezing their parameters and only updating dropout probabilities .
Moreover , we choose different trained modules to train their output dropout probability , and in calculating Equation 15 , we only take those trained dropout probabilities into consideration .
By allowing dropout probabilities to change , our method can better represent model distributions under the training dataset than the fixed dropout probabilities .
W Q i , W K i , W V i in Equation 2 : Split D into ( X 1 , Y 1 ) , ... , ( X n , Y n ) with size M 1 , ... , M n 3 : i = 0 4 : while i < E do 5 : for j = 1 to n do 6 : sample ? from Q ( ?|? ) 7 : L = ( x, y ) ?
( X j , Y j ) k log P ( y k |y < k , x ; ? ) 8 : L = L + M j N [ W m?n ? n j=1 ?H( p W .j ) + R( p W .j , W .j , l ) ]
9 : ? ? ? + ? ? ? L 10 : end for 11 : end while During updating the dropout probability , due to the discrete characteristics of the Bernoulli distribution , we cannot directly calculate the gradient of the first term in Equation 9 to the dropout probability .
So , we adopt concrete dropout , which is used in Gal et al . ( 2017 ) .
As a continuous relaxation of dropout , for its input y , the output can be expressed as y = y z , and vector z satisfies : z = sigmoid ( 1 t ( log ( p ) ? log ( 1 ? p ) + log ( u ) ? log ( 1 ? u ) ) ) ( 17 ) where u ? U ( 0 , 1 ) , p is dropout probability .
In the inference stage , we just randomly mask model parameters with trained dropout probabil-ities , with different random seeds , NMT models with different parameters are sampled .
Since diverse translations are demanded , we performed several forward passes through different sampled NMT models , and different translations are generated with different model outputs and beam search .
Experiment Setup Dataset
In our experiment , we select datasets in the following translation tasks : ? NIST Chinese-to-English ( NIST Zh-En ) .
Its dataset is based on LDC news corpus and contains about 1.34 million sentence pairs .
It also includes 6 relatively small datasets , MT02 , MT03 , MT04 , MT05 , MT06 , and MT08 .
In our experiments , we use MT02 as the development set , and the rest work as the test sets .
Without special explanation , we use average result of test sets as final results .
? WMT '14 English-to- German ( WMT ' 14 En -De ) .
Its dataset comes from the WMT '14 news translation task , which contains about 4.5 million sentence pairs .
In our experiment , we use newstest2013 as the development set and newstest2014 as the test set .
For above two datasets , We adopt Moses tokenizer ( Koehn et al. , 2007 ) in English and German corpus .
We also use the byte pair encoding ( BPE ) algorithm ( Sennrich et al. , 2015 ) , and limit the size of the vocabulary K = 32000 .
And we train a joint dictionary for WMT '14 En- De .
For NIST , we use THULAC toolkit to segment Chinese sentence into words .
In addition , we remove the examples in datasets from the above two tasks where length of the source language sentence or target language sentence exceed 100 words .
Model Architecture
In our experiments , we all adopt the Transformer Base model in Vaswani et al . ( 2017 ) .
Transformer base model has 6 layers in encoder and decoder , and it has hidden units with 512 dimension , except for the feed -forward network , where the inner-layer output dimension is 2048 .
The number of heads in Transformer base model is 8 and the default dropout probability is 0.1 .
And our model is implemented in python3 with the Fairseq - py toolkit .
Experimental Setting During training , in order to improve the accuracy , we use the label smoothing ( Szegedy et al. , 2016 ) with = 0.1 .
In terms of optimizer , we adopt the Adam optimizer ( Kingma and Ba , 2014 ) , the main parameters of the opti-mizer is ?
1 = 0.9 , ? 2 = 0.98 , and = 10 ?9 .
As for the learning rate , we adopt the dynamic learning rate method in Vaswani et al . ( 2017 ) with warmup steps = 4000 .
Also , we use mini-batch training with max token = 4096 .
Metrics
In terms of evaluation metrics , referring to Shen et al . ( 2019 ) , we adopt the BLEU and Pairwise - BLEU to evaluate translation quality and diversity .
Both two metrics are calculated with case-insensitive BLEU algorithm in Papineni et al . ( 2002 ) .
In our experiments , the BLEU is to measure the average similarity between the output translations and the standard translation .
The higher the BLEU value , the better the accuracy of translation .
And the Pairwise - BLEU reflects the average similarity between the output translations of different groups .
The lower the Pairwise - BLEU value , the lower the similarities , and the more diverse the translations .
In our experiment , we use the NLTK toolkit to calculate the two metrics .
6 Experiment Results
Analysis of Training Modules and Hyper-parameter
In this experiment , we train models with different training modules and hyper-parameter l with NIST dataset to evaluate their effects , and some results are shown in Table 1 .
As we can see in Table 1 , for those training modules , when l is small , with the same hyperparameter l , choosing smaller part of training modules will lead to lower BLEU and Pairwise - BLEU , showing that accuracy of the generate translations increases while diversity decreases .
We also find that in the same training modules , with the increase of l , the Pairwise - BLEU decreases steadliy , and then increases when the BLEU is close to zero ; and the BLEU has similar trends with Pairwise - BLEU , however , when l is relatively low , the BLEU tends to stablize .
For the above-mentioned experimental results , we can interpret as follows : in training modules , since Equation 15 is the sum of the training modules ' KL divergence , with the training modules increase , the KL divergence accordingly increases , pushing the dropout probability higher and making translations diverse .
In terms of hyper-parameter l , as we can see in Equation 10 , when l increases , the prior distribution is squeezed to zero matrix ; thus during training , the dropout probabilities will get higher to make the model distribution close to prior distribution .
However , when the l is too high to make most of dropout probabilities close to 1 , uncertainty of model parameters decreases , making Pairwise - BLEU increases .
Results in Diverse Translation
From the previous section , we can see that by selecting different training modules and hyperparameter , translations with different accuracy and diversity can be obtained .
Then we conduct experiments to generate 5 groups of different translations on NIST Zh- En dataset and WMT '14 En - De dataset , and compare the diversity and accuracy of the translations generated by our method and the following baseline approaches : ?
Beam Search : we choose the optimal 5 results directly generated by beam search in this paper .
? Diverse Beam Search ( DBS ) ( Vijayakumar et al. , 2016 ) : it works by grouping the outputs and adding regularization terms in beam search to encourage diversity .
In our experiment , the number of output translations of groups are all 5 . ? HardMoE
( Shen et al. , 2019 ) : it trains model with different hidden states and obtains different translations by controlling hidden state .
In our experiment , we set the number of hidden states is 5 . ? Head Sampling ( Sun et al. , 2019 ) : it generate different translations by sampling different encoder-decoder attention heads according to their attention weight , and copying the samples to other heads in some conditions .
Here , we set the parameter K = 3 .
The results are shown in Figure 1 .
In Figure 1 we plot the BLEU versus Pairwise - BLEU , the scattered points show the results of baseline approaches , and the points on the curves are results in the same training modules with different hyper - Source ?
Reference
One of the important topics for discussion in this meeting is the cross atlantic relation .
One of the top agendas of the meeting is to discuss the cross-atlantic relations .
An important item on the agenda of the meeting is the trans-atlantic relations .
One of the major topics for the conference this time is transatlantic relations .
Beam search
An important item on the agenda of this meeting is transatlantic relations .
An important topic of this conference is transatlantic relations .
An important topic of this meeting is transatlantic relations .
An important topic of this conference is transatlantic ties .
An important topic of the meeting is transatlantic relations .
Our Work
One of the important topics of this conference is transatlantic relations .
An important item on the agenda of this meeting is the transatlantic relationship .
An important item on the agenda of this conference is the transatlantic relationship .
An important topic for discussion at this conference is cross-atlantic relations .
One of the important topics of this conference is the transatlantic relationship .
Also , we suggest that in NIST Zh- En task , by adjusting training modules and hyper-parameter l , our results which has higher BLEU and lower Pairwise - BLEU values than baselines without HardMoE , even for HardMoE itself , our method is comparable with proper l while training the whole model .
In WMT '14 En- De , we also find that our method exceeds the baseline approach except HardMoE .
For the gap in performace with HardMoE , we interpreted that since our models are randomly sampled from the model distribution , it could be hard for our models to represent such distinguishable characteristics like HardMoE , which trains multiple different latent variables .
Also , to intuitively display the improvement of diversity in our translations , we choose a case from NIST Zh- En task , the results are shown in Table 2 .
The case shows that compared with beam search , which only varies in few words , our method can obtain more diverse translations while ensuring the accuracy of translation , and diversities are not only shown in words , but also reflected in lexical characteristic .
Analyzing Module Importance with Dropout Probability Some researches ( Voita et al. , 2019 ; Michel et al. , 2019 ; found that a well - trained Transformer model is over-parameterized .
Useful information gathers in some parameters and some modules and layers can be pruned to improve the efficiency during test time .
Since dropout can play the role of regularization and there are differences in the trained dropout probabilities of different neuron , we conjecture that the trained dropout probability and the importance of each module are correlated .
To investigate this , we choose the model in which dropout probabilities of the full model is trained with l 2 = 400 in NIST Zh- En task , and separately calculate the average dropout probability pdropout of different attention modules .
Also , we manually pruned the corresponding modules of the model , obtained translations and calculated its BLEU .
the more the BLEU drops , the more important the module is to translation .
To quantify their relevance , we calculate the Pearson correlation coefficient ( PCC ) ? in different kinds of training modules , and highlights the highest and lowest results .
Results of our experiment are shown in Table 3 pdropout and BLEU , we can find that the dropout probabilities pdropout and the BLEU of translations show some similar information in module importance .
Also , we quantify the correlation between the pdropout and BLEU , finding that it is highly correlated in self-attention module in encoder and in E-D attention in decoder , since its correlation coefficient ? > 0.8 , and the pdropout and BLEU is also correlated in self-attention in decoder .
2012 ) , dropout , which is easy to implement , works as a stochastic regularization to avoid over-fitting .
And there are several theoretical explainations such as getting sufficient model combinations ( Hinton et al. , 2012 ; Srivastava et al. , 2014 ) to train and augumenting training data ( Bouthillier et al. , 2015 ) . Gal and Ghahramani ( 2016 ) proposes that dropout can be understood as a bayesian inferences algorithm , and Gal et al . ( 2017 ) uses concrete dropout in updating dropout probabilities .
Also , the author implements the dropout methods to represent uncertainty in different kinds of deep learning tasks in Gal ( 2016 ) .
Related Work
In neural machine translation task , lack of diversity is a widely acknowledged problem , some researches like Ott et al . ( 2018 ) investigate the cause of uncertainty in NMT , and some provide metrics to evaluate the translation uncertainty like Galley et al . ( 2015 ) ; Dreyer and Marcu ( 2012 ) .
There are also other researches that put forward methods to obtain diverse translation .
There are also a few papers in interpreting Transformer model , Voita et al . ( 2019 ) suggests that some heads play a consistent role in machine translation , and their roles can be interpreted linguistically ; also , they implement L 0 penalty to prune heads .
Michel et al. ( 2019 ) shows that huge amounts of heads in Transformer can be pruned , and the importance of head is cross-domain .
Also , shows that the layers in Transformer are also able to be pruned : similar to our work , during training , they drop the whole layer with dropout and trained their probability ; however , variational inference strategy is not used in their paper , and they take different kinds of inference strategies to balance performance and efficiency rather than sampling .
Conclusion
In this paper , we propose to utilize variational inference in diverse machine translation tasks .
We represent the Transformer model distribution with dropout , and train the model distributions to minimize its distance to the posterior distribution under specific training dataset .
Then we generate diverse translations with the models sampled from the trained model distribution .
We further analyze the correlations between module importance and trained dropout probabilities .
Experiment results in Chinese-English and English - German translation tasks suggest that by properly adjusting trained modules and prior parameters , we can generate translations which balance accuracy and diversity well .
In future work , firstly , since our model is randomly sampled from model distribution to generate diverse translation , it is meaningful to explore better algorithms and training strategies to represent model distribution and search for the most distinguishable results in model distribution .
Also , we 'll try to extend our methods in a wider range of NLP tasks .
The mini-batch training algorithm is expressed in Equation 1 .
It is worth to mention that since we train the model distribution with batches of data , we scale the KL divergence with the proposition of the batch in the entire training dataset .
Algorithm 1 Mini-batch Training of Bayesian NN using Variational Inference with Dropout in NMT Input : Training dataset D = ( X , Y ) with size N , pre-trained model parameter ? , learning rate ? , learning epoch E Output : model parameter ?
1 : Initial ?
