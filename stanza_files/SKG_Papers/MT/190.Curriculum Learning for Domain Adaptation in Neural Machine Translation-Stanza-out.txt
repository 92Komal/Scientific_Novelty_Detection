title
Curriculum Learning for Domain Adaptation in Neural Machine Translation
abstract
We introduce a curriculum learning approach to adapt generic neural machine translation models to a specific domain .
Samples are grouped by their similarities to the domain of interest and each group is fed to the training algorithm with a particular schedule .
This approach is simple to implement on top of any neural framework or architecture , and consistently outperforms both unadapted and adapted baselines in experiments with two distinct domains and two language pairs .
Introduction Neural machine translation ( NMT ) performance often drops when training and test domains do not match and when in- domain training data is scarce ( Koehn and Knowles , 2017 ) .
Tailoring the NMT system to each domain could improve performance , but unfortunately high-quality parallel data does not exist for all domains .
Domain adaptation techniques address this problem by exploiting diverse data sources to improve indomain translation , including general domain data that does not match the domain of interest , and unlabeled domain data whose domain is unknown ( e.g. webcrawl like Paracrawl ) .
One approach to exploit unlabeled - domain bitext is to apply data selection techniques ( Moore and Lewis , 2010 ; Axelrod et al. , 2011 ; Duh et al. , 2013 ) to find bitext that are similar to in- domain data .
This selected data can additionally be combined with in- domain bitext and trained in a con- framework , the selected samples are concatenated with in - domain data , then used for continued training .
This effectively increases the in- domain training size with " pseudo " in - domain samples , and is helpful in continued training .
A challenge with employing data selection in continued training is that there exists no clear-cut way to define whether a sample is sufficiently similar to in-domain data to be included .
In practice , one has to define a threshold based on similarity scores , and even so the continued training algorithm may be faced with samples of diverse similarities .
We introduce a new domain adaptation technique that addresses this challenge .
Inspired by curriculum learning ( Bengio et al. , 2009 ) , we use the similarity scores given by data selection to rearrange the order of training samples , such that more similar examples are seen earlier and more frequently during training .
To the best of our knowledge , this is the first work applying curriculum learning to domain adaptation .
We demonstrate the effectiveness of our approach on TED Talks and patent abstracts for German-English and Russian - English pairs , using two distinct data selection methods , Moore -Lewis method ( Moore and Lewis , 2010 ) and cynical data selection ( Axelrod , 2017 ) .
Results show that our approach consistently outperforms standard continued training , with up to 3.22 BLEU improvement .
Our S 4 error analysis ( Irvine et al. , 2013 ) reveal that this approach reduces a reasonable number of SENSE and SCORE errors .
2 Curriculum Learning for Adaptation Weinshall and Cohen ( 2018 ) provide guidelines for curriculum learning : " A practical curriculum learning method should address two main questions : how to rank the training examples , and how to modify the sampling procedure based on this ranking . "
For domain adaptation we choose to estimate the difficulty of a training sample based on its distance to the in-domain data , which can be quantified by existing data selection methods ( Section 2.1 ) .
For the sampling procedure , we adopt a probabilistic curriculum training ( CL ) strategy that takes advantage of the spirit of curriculum learning in a nondeterministic fashion without discarding the good practice of original standard training policy , like bucketing and mini-batching .
Domain Similarity Scoring
We adopt similarity metrics from prior work on data selection to score examples for curriculum learning .
Let I be an in- domain corpus , and N be a unlabeled - domain data set .
Data selection models rank sentences in N according to a domain similarity measure with respect to I , and choose top n samples from N by a cut-off threshold for further training purpose .
We examine two data selection methods , Moore -Lewis method ( Moore and Lewis , 2010 ) and cynical data selection ( Axelrod , 2017 ) . Moore-Lewis Method
Each sentence s in N is assigned a cross-entropy difference score , H I ( s ) ? H N ( s ) , ( 1 ) where H I ( s ) is the per-word cross-entropy of s according to a language model trained on I , and H N ( s ) is the per-word cross-entropy of s according to a language model trained on a random sample of N with roughly the same size as I .
A lower cross-entropy difference indicates that s is more like the in-domain data and less like the unlabeled - domain data .
Cynical Data Selection
Iteratively select sentence s from N to construct a training corpus that would approximately model I .
At each iteration , each sentence is scored by the expected cross-entropy change from adding it to the already selected subset of N .
The selected sentence is the one which most decreases H n , the cross-entropy between previously selected n-sentence corpus and I .
Curriculum Learning Training Strategy
We identify two general types of curriculum learning strategy .
The deterministic curriculum ( c.f. Kocmi and Bojar ( 2017 ) ) trains on a fixed order of samples based on their scores ( e.g. " easyto- hard " or " more similar to less " ) .
While simple to motivate , this may not always perform well because neural methods benefit from randomization in the minibatches and multiple epochs .
In contrast , the probabilistic curriculum ( Bengio et al. , 2009 ) works by dividing the training procedure into distinct phases .
Each phase creates a random sample from the entire pool of data , but earlier phases sample the " easier " or " more similar " sentence with higher probability ..
Since each phase can be viewed as creating a new training dataset , all the well -tested tricks of the trade for neural network optimization can be employed .
In this paper , we use the same probabilistic curriculum strategy and code base 1 as Zhang et al . ( 2018 ) .
The main difference here is the application to domain adaptation .
The proposed strategy is summarized as follows : ?
Sentences are first ranked by similarity scores and then distributed evenly into shards , such that each shard contains samples with similar similarity criteria values .
Experiments and Results
We evaluate on four domain adaptation tasks .
The code base is provided to ensure reproducibility .
2
Data and Setup General Domain Data
We have two general domain datasets , Russian - English ( ru ) and German-English ( de ) .
Both are a concatenation of OpenSubtitles2018 ( Lison and Tiedemann , 2016 ) and WMT 2017 In-domain Data
We evaluate our proposed methods on two distinct domains per language pair : ? TED talks : data-split from Duh ( 2018 ) . ? Patents : from the World International Property Organization COPPA - V2 dataset ( Junczys - Dowmunt et al. , 2016 ) .
We randomly sample 15 k parallel sentences from the original corpora as our in-domain bitext .
3
We also have around 2 k sentences of development and test data for TED and 3 k for patent .
Unlabeled - domain Data
For additional unlabeled - domain data , we use web-crawled bitext from the Paracrawl project .
4
We filter the data using the Zipporah cleaning tool ( Xu and Koehn , 2017 ) , with a threshold score of 1 .
After filtering , we have around 13.6 million Paracrawl sentences available for German-English and 3.7 million Paracrawl sentences available for Russian - English .
Using different data selection methods , we include up to the 4096 k and 2048 k sentence - pairs for our German and Russian experiments , respectively .
Data Preprocessing
All datasets are tokenized using the Moses ( Koehn et al. , 2007 ) tokenizer .
We learn byte pair encoding ( BPE ) segmentation models ( Sennrich et al. , 2016 ) from general domain data .
The BPE models are trained separately for each language , and the number of BPE symbols is set to 30k .
We then apply the BPE models to in-domain and Paracrawl data , so that the parameters of the generic model can be applied as an initialization for continued training .
Once we have a converged generic NMT model , which is very expensive to train , we can adapt it to different domains , without building up a new vocabulary and retraining the model .
NMT Setup Our NMT models are developed in Sockeye 5 ( Hieber et al. , 2017 ) .
The generic model and continued training model are trained with the same hyperparameters .
We use the seq2seq attention architecture ( Bahdanau et al. , 2015 ) with 2 LSTM layers for both encoder and decoder , and 512 hidden nodes in each layer .
The word embedding size is also set to 512 .
Our models apply Adam ( Kingma and Ba , 2014 ) as the optimizer , with an initial learning rate 0.0003 .
The learning rate is multiplied by 0.7 whenever validation perplexity does not surpass the previous best in 8 checkpoints .
6
We use minibatches of 4096 words .
Training stops when the perplexity on the development set has not improved for 20 checkpoints ( 1000 updates / batches per checkpoint ) .
Domain Similarity Scoring Setup
To get similarity scores , we build 5 - gram language models on the source side 7 with modified Kneser - Ney smoothing using KenLM ( Heafield , 2011 ) .
Curriculum Learning Setup
The number of batches in each curriculum phase is set to 1000 .
We split the training data into 40 shards 8 , with all the 15 k in- domain data in the first shard , and Paracrawl data split into the remaining 39 shards .
Experimental Comparison
Our goal is to empirically test whether the proposed curriculum learning method improves translation quality in the continued training setup of 5 github.com/awslabs/sockeye
6
The Adam optimizer for continued training model is initialized without reloading from the trained generic model .
7 Appendix
D also shows the effect of using language models built from target side and both sides .
8 After experimenting with various values from 5 to 100 ( Appendix B ) , we found best performance can be achieved at 40 shards .
Results
Table 1 summarizes the key results , where we continue train on 15 k in- domain samples and 4096 k Paracrawl samples ( for de ) or 2048 k Paracrawl samples ( for ru ) : ?
The baseline BLEU scores confirm the need 2 shows how the curriculum uses increasing amounts of Paracrawl better than standard continued training .
Standard continued training model hurts BLEU when too much Paracrawl data is added : for TED ( de ) , there 's a 1.94 BLEU drop when increasing CDS data from 64 k to 4096 k , and for patent ( de ) , the decrease is 2.43 BLEU .
By contrast , the curriculum learning models achieve a BLEU score that is as good or better as the initial model , even after being trained on the most dissimilar examples .
This trend is clearest on the patent ( ru ) CL ML model , where the BLEU score consistently rises from 32.41 to 34.18 .
The method used to score domain relevance has a different impact on the TED domain ( top plots ) and on the patent domain ( bottom plots ) .
On the patent domain , which is more distant from Paracrawl , CDS significantly outperforms ML .
Replacing ML with CDS improve BLEU from 2.18 to 4.05 BLEU points for standard models and 2.20 to 4.25 BLEU points for curriculum learning models .
Interestingly , for patents , the Moore - Lewis method does not beat the random selection , even when curriculum learning is applied .
For example , at 64 k selected sentences for patent ( de ) , std rand gets 4.26 higher BLEU scores than CL ML .
By contrast on the TED domain , which is closer to Paracrawl , the Moore - Lewis method slightly outperforms cynical data selection .
Due to these differences , we suggest trying different data selection methods with curriculum learning on new tasks ; a potential direction for future work may be a curriculum that considers multiple similarity scores jointly .
Analysis
Comparison of Curriculum Strategies
We compare our approach to other curriculum strategies .
CL reverse reverses the presenting order of the shards , so that shards containing less similar examples will be visited first , CL scrambled is a model that adopts the same training schedule as CL , but no data selection method and ranking is involved here - Paracrawl data are evenly split and randomly assigned to shards ; CL noshuffle is another curriculum learning model that does not shuffle shards in each curriculum phase .
Results from Figure 3 show that CL outperforms CL reverse and CL noshuffle for 5 out of 7 cases and outperforms CL scrambled in 6 out of 7 cases .
This suggests that it is beneficial to train on examples that are closest to in- domain first and to use a probabilistic curriculum .
Analyzing the detailed difference between CL and CL reverse would be interesting future work .
One potential hypothesis why CL might help is that it first trains on a low-entropy subset of the data before moving on to the whole training set , which may have regularization effects .
Learning Curves Learning curves ( Figure 4
Impact of Curriculum Learning on Lexical Choice : S 4 Analysis
How do translations improve when using curriculum learning ?
We characterize the impact of curriculum learning on lexical translation errors using the S 4 taxonomy of domain change errors introduced by Irvine et al . ( 2013 ) for phrase - based machine translation : ( 1 ) SEEN : incorrect translation for a source word that has never been seen in the training corpus ; ( 2 ) SENSE : incorrect translation for a previously seen source word , whose correct translation ( sense ) has never been seen in the training corpus ; ( 3 ) SCORE : a score error is made when the source word and its correct translation are both observed in training data , but the incorrect translation is scored higher than the correct alternative ; and ( 4 ) SEARCH : an error caused by pruning in beam search 11 .
We extend this taxonomy to neural machine translation .
As the unit of S 4 analysis is word alignment between a source word and a reference target word , we first run fast-align ( Dyer et al. , 2013 ) to get the source-target word alignments .
After this , we follow the algorithm shown in Appendix C to give a summary of S 4 errors on the model 's translation of test set .
Figure 5 shows the word translation results for the test set of German-English TED .
Most of the errors are SCORE errors , while SEEN and SENSE errors are relatively rare .
Curriculum learning significantly improves the adapted NMT systems at the word level - with 4096 k Paracrawl data selected by CDS , curriculum continued training model can translate 554 more words correctly than the standard continued training model .
This improvement mainly happens in SCORE errors : 1.75 % of SCORE errors are corrected .
SEEN and SENSE errors are also reduced by 0.02 % and 0.026 % , respectively .
But overall , CL does not help much on SEEN errors .
Characteristics of Selected Data
We characterize the sentences chosen by different data selection methods , to understand their effect on adaptation as observed in Section 3.3 .
Selected Sentences Overlap
For each domain in German-English , we compute the overlap between the top n ML and CDS Paracrawl sentences .
The overlap is as low as 3.69 % for the top 64 k sentences in the TED domain , and 8.43 % for the patent domain .
Even in the top 4096 k sentences , there are still 46.25 % and 65.40 % different ones in TED and patent domain respectively .
See Table 2 for examples of selected sentences .
Average Sentence Length
The ML score prefers longer sentences and is more correlated with sentence length ( See Figure 6 ) - the curve TED ML is near linear , which might be a side-effect of sentence - length normalization .
CDS produces sentences that better match the average sentence length in the in-domain corpus , which was also observed in Santamar ? a and Axelrod ( 2017 ) .
Out-of-Vocabulary Words
We count out -ofvocabulary ( OOV ) tokens in in- domain corpus based on the vocabulary of selected unlabeleddomain data ( Figure 7 ) .
The CDS subsets cover in- domain vocabulary better than ML subsets as expected , since CDS is based on vocabulary coverage .
Unigram Distribution Distance
TED ML
It changes the way we think ; it changes the way we walk in the world ; it changes our responses ; it changes our attitudes towards our current situations ; it changes the way we dress ; it changes the way we do things ; it changes the way we interact with people .
TED CDS
But , on the other hand , this signifies that the right of self-determination , as a part of the proletarian peace program , possesses not a " Utopian " but a revolutionary character .
patent ML
The sites x , y and z can accommodate a large variety of cations with x=na + , k+ , ca2 + , vacancy ; y=m g2 + , fe2 + , al3 + , fe3 + , li + , mn2 + and z=al3 + , m g2 + , fe3 + , v3 + , cr3 + ; while the t site is predominantly occupied by si4 +.
patent CDS
To select alternative viewing methods , such as for 3d - tv .
We measure the difference of unigram distributions from two corpora by Hellinger distance , which is defined as Equation 2 when the probability distribution is discrete , where P and Q are the unigram distributions for the source side of indomain and Paracrawl .
V is the vocabulary size .
12 H HD ( P , Q ) = 1 ? 2 V i=1 ( ? p i ? ? q i ) 2 . ( 2 ) From Figure 8 , we can see ML can better match the in-domain vocabulary distribution than CDS .
With respect to the OOV rate and unigram distribution , patent is more distant from the Paracrawl data than TED is .
Figure 2 suggests that CDS dominates ML for distant domains such as Patent , while ML can do slightly better than CDS for domains that are not that distant such as TED .
Related Work Curriculum learning has shown its potential to improve sample efficiency for neural models ( Graves et al. , 2017 ; Weinshall and Cohen , 2018 ) by guiding the order of presented samples , usually from easier - to - learn samples to difficult samples .
Although there is no single criterion to measure difficulty for general neural machine translation tasks ( Kocmi and Bojar , 2017 ; Zhang et al. , 2018 ; Kumar et al. , 2019 ; Platanios et al. , 2019 ) , for the domain adaptation scenario , we measure difficulty based on the distance from indomain data .
Compared to previous work , our application of curriculum learning mainly focuses on improvements on translation quality without consideration of convergence speed .
Chu and Wang ( 2018 ) surveyed recent domain adaptation methods for NMT .
In their taxonomy , our workflow in Figure 1 can be considered a hybrid that uses both data-centric and model-centric techniques due to the use of additional unlabeleddomain data , with a modified training procedure based for continued training .
For data-centric domain adaptation methods , our curriculum learning approach has connections to instance weighting .
In our work , the presentation of certain examples at specific training phases is equivalent to up-weighting those examples and down - weight the others at that time .
Weights of similar samples and less similar ones are adjusted dynamically during the training of NMT models based on the curriculum training strategy .
In NMT , instance weighting is usually implemented by modifying the objective function ( Chen and Huang , 2016 ; Wang et al. , 2017 ; .
In statistical machine translation , Matsoukas et al . ( 2009 ) extract features from sentences to capture their domains and then use a classifier to map features to sentence weights .
Foster et al.
extend this method by weighting at the level of phrase pairs .
Shah et al. ( 2010 ) use resampling to weight corpora and alignments .
Mansour and Ney ( 2012 ) focus on sentence - level weighting for phrase extraction .
Zhou et al . ( 2015 ) weight examples based on their word distributions .
For model-centric domain adaptation methods , our work is related to van der Wees et al . ( 2017 ) .
They adopt gradual fine-tuning , which does the opposite of our method : training starts from the whole dataset , and the training set gradually decreases by removing less similar sentences .
use a similar approach , where the NMT model is trained on progressively noisereduced data batches .
However , such schedules have the risk of wasting computation on non-relevant data , especially when most of the Paracrawl data is not similar to the target domain .
Conclusion
We introduced a curriculum learning approach to adapt neural machine translation models to new domains .
Our approach first ranks unlabeleddomain training samples based on their similarity to in- domain data , and then adopts a probabilistic curriculum learning strategy so that more similar samples are used earlier and more frequently during training .
We show the effectiveness of our method on four tasks .
Results show that curriculum learning models can improve over the standard continued training model by up to 3.22 BLEU points and can take better advantage of distant and noisy data .
According to our S 4 analysis of lexical choice errors , this improvement is mainly due to better scoring of words that acquire a new SENSE or have a different SCORE distribution in the new domain .
Our extensive empirical analysis suggests that this approach is effective for several reasons : ( 1 ) It provides a robust way to augment the training data with samples that have different levels of similarity to the in-domain data .
Unlabeled - domain data such as webcrawls naturally have a diverse set of sentences , and the probabilistic curriculum allows us to exploit as much diversity as possible . ( 2 ) It implements the intuition that samples more similar to in- domain data are seen earlier and more frequently ; when adding a new shard into the training set , the previously visited shards are still used , so the model will not forget what it just learned .
( 3 ) It builds on a strong continued training baseline , which continues on in- domain data .
( 4 ) The method implements best practices that have shown to be helpful in NMT , e.g. bucketing , mini-batching , and data shuffling .
For future work , it would be interesting to measure how curriculum learning models perform on the general domain test set ( rather than the indomain test set we focus on in this work ) ; do they suffer more or less from catastrophic forgetting ( Goodfellow et al. , 2014 ; Kirkpatrick et al. , 2017 ; Thompson et al. , 2019 )
The total amount of the in-domain data in each domain is summarized in Table 3 .
In this paper , we uniformly sample 15 k in- domain data from each dataset .
We choose the amount of 15 k , which makes up a relatively small percentage of the original corpora , in order to evaluate the extreme case of low-resource domain adaptation settings .
Under this setting , the positive effect of adding more selected unlabeled - domain data into training corpus is more obvious in terms of the performance improvement of NMT models .
Our pilot experiments show that curriculum learning can scale with more in- domain data - it consistently outperforms the standard training policy , but with less improvement .
This is not surprising , as when there is enough in - domain data , continued training on only the in-domain data can already achieve a pretty good performance , and we do not need to use extra unlabeled - domain data to augment it any more , neither does curriculum learning .
