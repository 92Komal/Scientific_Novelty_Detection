title
Personalized Machine Translation : Preserving Original Author Traits
abstract
The language that we produce reflects our personality , and various personal and demographic characteristics can be detected in natural language texts .
We focus on one particular personal trait of the author , gender , and study how it is manifested in original texts and in translations .
We show that author 's gender has a powerful , clear signal in originals texts , but this signal is obfuscated in human and machine translation .
We then propose simple domainadaptation techniques that help retain the original gender traits in the translation , without harming the quality of the translation , thereby creating more personalized machine translation systems .
Introduction
Among many factors that mold the makeup of a text , gender and other authorial traits play a major role in our perception of the content we face .
Many studies have shown that these traits can be identified by means of automatic classification methods .
Classical examples include gender identification ( Koppel et al. , 2002 ) , and authorship attribution and profiling ( Seroussi et al. , 2014 ) .
Most research , however , addressed texts in a single language , typically English .
We investigate a related but different question : we are interested in understanding what happens to personality and demographic textual markers during the translation process .
It is generally agreed that good translation goes beyond transformation of the original content , by preserving more subtle and implicit characteristics inferred by author 's personality , as well as era , geography , and various cultural and sociological aspects .
In this work we explore whether translations preserve the stylistic characteristic of the author and , furthermore , whether the prominent signals of the source are retained in the target language .
As a first step , we focus on gender as a demographic trait ( partially due to the absence of parallel data annotated for other traits ) .
We evaluate the accuracy of automatic gender classification on original texts , on their manual translations and on their automatic translations generated through statistical machine translation ( SMT ) .
We show that while gender has a strong signal in originals , this signal is obfuscated in human and machine translation .
Surprisingly , determining gender over manual translation is even harder than over SMT ; this may be an artifact of the translation process itself or the human translators involved in it .
were the first to show that authorial gender signals tend to vanish through both manual and automatic translation , using a small TED talks dataset .
We use their data and extend it with a version of Europarl that we annotated with age and gender ( ?3 ) .
Furthermore , we conduct experiments with two language pairs , in both directions ( ?4 ) .
We also adopt a different classification methodology based on the finding that the translation process itself has a stronger signal than the author 's gender ( ?4.1 ) .
We then move on to assessing gender traits in SMT ( ?5 ) .
Since SMT systems typically do not take personality or demographic information into account , we hypothesize that the author 's style , affected by their personality , will fade .
Furthermore , we propose simple domain-adaptation techniques that do consider gender information and can therefore better retain the original traits .
We build " gender- aware " SMT systems , and show ( ?6 ) that they retain gender markers while preserving general translation quality .
Our findings therefore suggest that SMT can be made much more personalized , leading to translations that are more faith -ful to the style of the original texts .
Finally , we analyze the prominent features that reflect gender in originals and translations ( ?7 ) .
Our experiments reveal that gender markers differ greatly by language , and the specific source language has a significant impact on the features and classification accuracy of the translated text .
In particular , gender traits of the original language overshadow those of the target language in both manual and automatic translation products .
The main contributions of this paper are thus : ( i ) a new parallel corpus annotated with gender and age information , ( ii ) an in- depth assessment of the projection of gender traits in manual and automatic translation , and ( iii ) experiments showing that gender - personalized SMT systems better project gender traits while maintaining translation quality .
Related work
While modeling of demographic traits has been proven beneficial in some NLP tasks such as sentiment analysis ( Volkova et al. , 2013 ) or topic classification ( Hovy , 2015 ) , very little attention has been paid to translation .
We provide here a brief summary of research relevant to our work .
Machine translation ( MT ) Virtually no previous work in MT takes into account personal traits .
State - of- the- art MT systems are built from examples of translations , where the general assumption is that the more data available to train models , the better , and a single model is usually produced .
Exceptions to this assumption revolve around work on domain adaption , where systems are customized by using data that comes from a particular text domain ( Hasler et al. , 2014 ; Cuong and Sima'an , 2015 ) ; and work on data cleaning , where spurious data is removed from the training set to ensure the quality of the final models ( Cui et al. , 2013 ; Simard , 2014 ) .
Personal traits , sometimes well marked in the translation examples , are therefore not explicitly addressed .
Learning from different , sometimes conflicting writing styles can hinder model performance and lead to translations that are unfaithful to the source text .
Focusing on reader preferences , Mirkin and Meunier ( 2015 ) used a collaborative filtering approach from recommender systems , where a user 's preferred translation is predicted based on the preferences of similar users .
However , the user preferences in this case refer to the overall choice between MT systems of a specific reader , rather than a choice based on traits of the writer .
motivated the need for personalization of MT models by showing that automatic translation does not preserve demographic and psychometric traits .
They suggested treating the problem as a domain adaptation one , but did not provide experimental results of personalized MT models .
Gender classification
A large body of research has been devoted to isolating distinguishing traits of male and female linguistic variations , both theoretically and empirically .
Apart from content , male and female speech has been shown to exhibit stylistic and syntactic differences .
Several studies demonstrated that literary texts and blog posts produced by male and female writers can be distinguished by means of automatic classification , using ( content - independent ) function words and ngrams of POS tags ( Koppel et al. , 2002 ; Schler et al. , 2006 ; Burger et al. , 2011 ) .
Although the tendencies of individual word usage are a subject of controversy , distributions of word categories across male and female English speech is nearly consensual : pronouns and verbs are more frequent in female texts , while nouns and numerals are more typical to male productions .
Newman et al. ( 2008 ) carried out a comprehensive empirical study corroborating these findings with large and diverse datasets .
However , little effort has been dedicated to investigating the variation of individual markers of demographic traits across different languages .
Johannsen et al. ( 2015 ) conducted a large-scale study on linguistic variation over age and gender across multiple languages in a social media domain .
They showed that gender differences captured by shallow syntactic features were preserved across languages , when examined by linguistic categories .
However , they did not study the distribution of individual gender markers across domains and languages .
Our work demonstrates that while marker categories are potentially preserved , individual words typical to male and female language vary across languages and , more prominently , across different domains .
Authorial traits in translationese
A large body of previous research has established that translations constitute an autonomic language variety : a special dialect of the target language , often re-ferred to as translationese ( Gellerstam , 1986 ) .
Recent corpus-based investigations of translationese demonstrated that originals and translations are distinguishable by means of supervised and unsupervised classification ( Baroni and Bernardini , 2006 ; Volansky et al. , 2015 ; .
The identification of machinetranslated text has also been proven an easy task ( Arase and Zhou , 2013 ; Aharoni et al. , 2014 ) .
Previous work has investigated how gender artifacts are carried over into human translation in the context of social and gender studies , as well as cultural transfer ( Simon , 2003 ; Von Flotow , 2010 ) .
Shlesinger et al. ( 2009 ) conducted a computational study exploring the implications of the translator 's gender on the final product .
They conclude that " the computer could not be trained to accurately predict the gender of the translator " .
Preservation of authorial style in literary translations was studied by Lynch ( 2014 ) , identifying Russian authors of translated English literature , by using ( shallow ) stylistic and syntactic features .
Forsyth and Lam ( 2014 ) investigated authorial discriminability in translations of French originals into English , inspecting two distinct human translations , as well as automatic translation of the same sources .
Our work , to the best of our knowledge , is the first to automatically identify speaker gender in manual , and more prominently , automatic translations over multiple domains and languagepairs , examining distribution of gender markers in source and target languages .
Europarl with demographic info
We created a resource 1 based on the parallel corpus of the European Parliament ( Europarl ) Proceedings ( Koehn , 2005 ) .
More specifically , we utilize the extension of its en-fr and en-de parallel versions , where each sentence - pair is annotated with speaker name , the original language the sentence was uttered in , and the date of the corresponding session protocol .
To extend speaker information with demographic properties , we used the Europarl website 's MEP information pages 2 and applied a procedure of gender and age identification , as further detailed in ?3.1 .
The final resource comprises en-fr and en-de parallel bilingual corpora where metadata of mem- 1 Available at http://cl.haifa.ac.il/projects/pmt 2 http://www.europarl.europa.eu/meps/en/ bers of the European Parliament ( MEPs ) is enriched with their gender and age at the time of the corresponding session .
The data is restricted to sentence - pairs originally produced in English , French , or German .
Table 1 provides statistics on the two datasets .
We also release the full list of 3 , 586 MEPs with their meta information .
en- fr fr-en en-de de-en male 100K 67 K 101K 88 K female 44 K 40 K 61 K 43 K total 144 K 107 K 162 K 131 K
Table 1 : Europarl corpora ( EP ) statistics ( # of sentence - pairs ) ; gender refers to an author of the source utterance .
Identification of MEP gender Gender annotation was conducted using three different resources : Wikidata , Genderize and Alche-myVision , which we briefly describe below .
Wikidata ( Vrande ?i? and Kr?tzsch , 2014 ) is a human-curated knowledge repository of structured data from Wikipedia and other Wikimedia projects .
Wikidata provides an API 3 through which one can retrieve details about people in the repository , including place and date of birth , occupation , and gender .
For MEPs found in the Wikidata , we first verified that the person holds ( or held ) a position of Member of the European Parliament and if so , retrieved the gender .
Wikidata information is not complete : not all MEP names , positions or gender data is included .
In total we obtained gender information for 2 , 618 MEPs ( 73 % of the total 3 , 586 ) , of which 1 , 882 ( 72 % ) are male and 736 female ( 28 % ) .
Genderize 4 is an open resource containing over 2 million distinct names grouped by countries .
It determines people 's gender based on their first name and the country of origin .
Provided with the first name and the country a MEP represents .
5 Genderize was able to predict the gender of 2 , 785 MEPs , the vast majority of them with a probability of 0.9 or higher .
We filtered out the 55 lowerconfidence entries , keeping 2 , 730 MEPs ( 76 % of total ) , of which 2001 ( 73 % ) are male and 729 ( 27 % ) female .
AlchemyVision
The European Parliament website maintains a page for every MEP , including personal photos .
We classified MEP personal images using AlchemyVision , 6 a publicly available image recognition service .
In total , we retrieved the gender of 2 , 236 MEPs using AlchemyVision .
Similarly to Genderize , we filtered out all predictions with a confidence score below 0.9 , thus obtaining the gender of 2 , 138 MEPs ( 60 % of total ) , of which 1 , 528 are male and 610 female ( 71 % and 29 % , respectively ) .
Resource evaluation and statistics Even though Wikidata was created manually , to verify its correctness , we manually annotated the gender of 100 randomly selected MEPs with available Wikidata gender information ; we found the metadata perfectly accurate .
We therefore rely on Wikidata as a gold-standard against which we can assess the accuracy of the two other resources .
Table 2 Given information obtained from the three resources , we assign each MEP with a single gender prediction in the following way : whenever it is found in Wikidata ( 2 , 618 MEPs ) , the gender is determined by this resource .
Otherwise , if both Genderize and AlchemyVision produced agreed - upon gender information ( 336 out of 338 cases ) , we set gender according to this prediction ; the same applies to the case where only one of Genderize or AlchemyVision provided a prediction ( 346 and 178 , respectively ) .
We ended up with gender annotation for a total of 3 , 478 out of 3 , 586 members .
The remaining 108 MEPs ( 92 male , 16 female ) were annotated manually , a rather laborintensive annotation in this case .
In total , the resource includes 947 ( 26 % ) female and 2 , 639 ( 74 % ) male MEPs .
Based on the above accuracy estimations , and assuming that manual annotation is correct , the overall accuracy of gender information in this resource is 99.88 % .
Utilizing the information on session dates and 6 https://www.ibm.com/smarterplanet/us/en/ ibmwatson/developercloud/alchemy-vision.html
MEPs dates of birth available in the metadata , we also annotated each sentence - pair with the age of the MEP at the time the sentence was uttered .
To summarize , we release the following resources : ( i ) meta information for 3 , 586 MEPs , as described above , ( ii ) bilingual parallel en-fr and en-de corpora , where each sentence - pair metadata is enriched with speaker MEPID , gender and age .
Experimental setup
We evaluate the extent to which gender traits are preserved in translation by evaluating the accuracy of gender classification of original and translated texts .
The rationale is that the more prominent gender markers are in the text , the easier it is to classify the gender of its author .
Translationese vs. gender traits
Since we use the accuracy of gender identification as our evaluation metric , we isolate the dimension of gender in our data : the classification experiments are carried out separately on original , human translated text , as well as on each one of the MT products .
Human , and more prominently , machine translations constitute distinct and distinguishable language variation , characterized by unique feature distributions ( ?2 ) .
We posit that in both human and machine translation products , the differences between original texts and translations overshadow the differences in gender .
We corroborate this assumption by analysing a sample data distribution by two dimensions : ( i ) translation status and ( ii ) gender .
Figure 1 presents the results for the English Europarl corpus .
Both charts display data distributions of the same four classes : original ( O ) and translated ( T ) English 7 by male ( M ) and female ( F ) speakers ( OM , OF , TM , TF ) .
For the sake of visualization , the dimension of function words feature vectors was reduced to 2 , using principal component analysis ( Jolliffe , 2002 ) .
The left graph depicts color-separation by gender ( male vs. female ) , while the right one by translation status ( original vs. translated ) .
Evidently , the linguistic variable of translationese stands out against the weaker signal of gender .
Datasets
In addition to the Europarl corpus annotated for gender ( ?3 ) , we experimented with a corpus of TED talks ( transcripts and translations ) : a collection of texts from a completely different genre , where demographic traits may manifest differently .
Testing the potential benefits of personalized SMT models on these two very diverse datasets allows us to examine the robustness of our approach .
We used the TED gender-annotated data from .
8
This corpus contains annotation of the speaker 's gender included in the English - French corpus of the IWSLT 2014 Evaluation Campaign 's MT track ( Cettolo et al. , 2012 ) .
We annotated 68 additional talks from the development and test sets of IWSLT 2014 , 2015 and 2016 .
Using the full set , we split the TED parallel corpora by gender to obtain sub-corpora of 140 K and 43 K sentence pairs for male and female speakers , respectively .
The sizes of the datasets used for training , tuning and testing of SMT are shown in Table 3 .
Relatively large test sets are used for evaluation of the MT results for the sake of reliable per-outcome gender classification ( ?4.1 ) .
Although the size of the training / tuning / test sets in either direction for any language - pair is the same , their content is different .
We use data in both translation directions ( i.e. , en-fr and fr-en , or en-de and de-en ) for both SMT experiments .
Out of these data , 2 K and 15 K sentence - pairs ( for each gender ) are held out for tuning and test , respectively , where they comply with the translation direction .
That is , for en-fr experiments , tuning and test sets are sampled from the en-fr direction only and vice-versa .
The additional bilingual data ( ADD ) for training the models comes from the gender-unannotated portion of Europarl ( all but the gender - annotated sub-corpus detailed in ?3 ) for the EP experiments , and from combining TED 's male and female data for the experiments with TED .
Classification setting All datasets were split by sentence , filtering out sentence alignments other than one-to-one .
For POS tagging , we employed the Stanford implementation 9 with its models for English , French and German .
We divided all datasets into chunks of approximately 1,000 tokens , respecting sentence boundaries , and normalized the values of lexical features by the actual number of tokens in each chunk .
For classification , we used Platt 's sequential minimal optimization algorithm ( Keerthi et al. , 2001 ) to train support vector machine classifiers with the default linear kernel ( Hall et al. , 2009 ) .
In all experiments we used ( the maximal ) equal amount of data from each category ( M and F ) , specifically , 370 chunks for each gender .
Aiming to abstract away from content and capture instead stylistic and syntactic characteristics , we used as our feature set the combination of function words ( FW ) 10 and ( the top - 1,000 most frequent ) POS - trigrams .
We employ 10 - fold crossvalidation for evaluation of classification accuracy .
SMT setting
We trained phrase - based SMT models with Moses ( Koehn et al. , 2007 ) , an open source SMT system .
KenLM ( Heafield , 2011 ) was used for language modeling .
We trained 5 - gram language models with Kneser - Ney smoothing ( Chen and Goodman , 1996 ) .
The models were tuned using Minimum Error Rate Tuning ( MERT ) ( Och , 2003 ) .
Our preprocessing included cleaning ( removal of empty , long and misaligned sentences ) , tokenization and punctuation normalization .
The Stanford tokenizer ( Manning et al. , 2014 ) was used for tokenization and standard Moses scripts were used for other preprocessing tasks .
We used BLEU ( Papineni et al. , 2002 ) to evaluate MT quality against one reference translation .
Personalized SMT models
In order to investigate and improve gender traits transfer in MT , we devise and experiment with gender - aware SMT models .
We demonstrate that despite their simplicity , these models lead to better preservation of gender traits , while not harming the general quality of the translations .
training tuning test dataset language- pair M F ADD M F M F EP en-fr & fr-en 144 K 65 K 1.71 M 2 K 2 K 15 K 15 K en-de & de-en 170K 86 K 1.50 M 2 K 2 K 15 K 15 K TED en-fr 117 K 21 K 138K 2K 2 K 20 K 20K Table 3 : MT datasets split for train , tuning and test , after cleaning .
We treat the task of personalizing SMT models as a domain adaptation task , where the domain is the gender .
We applied two common techniques : ( i ) gender-specific model components ( phrase table and language model ( LM ) ) and ( ii ) genderspecific tuning sets .
These personalized configurations are further compared to a baseline model where gender information is disregarded , as described below .
In all cases , we use a single reordering table built from the entire training set .
Baseline
The baseline ( MT - B ) system was trained using the complete parallel corpus available for a language - pair .
The training set contained both gender-specific and unannotated data , but no distinction was made between them .
A single translation model and a single LM were built , and the model was tuned using a random sample of 2K sentence - pairs from the mixed data dedicated for tuning , preserving , therefore , the gender distribution of the underlying dataset .
Personalized models
These models use three datasets : male , female , and additional in-domain bilingual data .
Two configurations were devised : MT - P1 , a model with three phrase tables and three LMs trained on the three datasets ; and MT - P2 , where for each gender a phrase table and a language model were built using only the genderspecific data , as well as a general phrase table and LM .
In both configurations , each of the two genderized model variants was tuned using the gender-specific tuning set .
In order to evaluate the translation quality of a personalized model , we separately translated the male and female source segments , merged the outputs and evaluated the merged result .
Results
Recall that we use the accuracy of gender classification as a measure of the strength of gender markers in texts .
We assessed this accuracy below on originals and ( human and machine ) translations .
First , however , we establish that the quality of SMT is not harmed with our personalized models .
MT evaluation
We trained a baseline ( MT - B ) and two personalized models ( MT - P1 and MT - P2 ) for each language pair as detailed in ?5 .
The BLEU scores of en-fr and fr-en personalized models were 38.42 , 38.34 and 37.16 , 37.16 , with the baseline models scoring 38.65 and 37.35 , respectively .
Similarly , for experiments with en-de and de-en and the TED data , the baseline scores ( 21.95 , 26.37 and 33.25 ) were only marginally higher than those of the personalized models ( 21.65 , 21.80 ; 26.35 , 26.21 ; and 33.19 , 33.16 ) , with differences ranging from 0.02 to 0.3 .
Neither MT - P1 nor MT - P2 was consistently better than the other .
We conclude , therefore , that all MT systems are comparable in terms of general quality .
4 and 5 present the results of gender classification accuracy in original ( O ) , human -( HT ) and machine - translated texts in the EP corpus .
Female texts are distinguishable from their male counterparts with 77.3 % and 77.1 % accuracy for English originals , in line with accuracies reported in the literature ( Koppel et al. , 2002 ) . Classification of original French and German texts reach 81.4 % ( Table 4 ) and 76.1 % ( Table 5 chine translation .
The relatively low accuracy for human translation can be ( partially ) explained by the extensive editing procedure applied on Europarl proceedings prior to publishing ( Cucchi , 2012 ) , as well as the potential " fingerprints " of ( male or female ) human translators left on the final product .
Classification accuracy Tables Both MT - P1 and MT - P2 models yield translations that better preserve gender traits , compared to their manual and gender-agnostic automatic counterparts : accuracy improvements vary between 3.8 for fr-en translations to 7.0 percent points for de-en 11 ( MT - P1 vs MT - B in both cases ) .
Per-class precision and recall scores do not exhibit significant differences , despite the unbalanced amount of per-gender data used for training the MT models .
Gender classification results in the TED dataset are presented in Table 6 .
The classification accuracy of English originals is 80.4 % .
While , similarly to Europarl , the gender signal is generally weakened in human translations 12 and baseline MT , overall accuracies are in most cases higher than in Europarl across all models .
We attribute this difference to the more emotional and personal nature of TED speeches , compared with the formal language of the EP proceedings .
Both personalized SMT models significantly outperform their baseline counterpart , as well as the manual translation , yielding 77.2 % and 77.7 % accuracy for MT - P1 and MT - P2 , respectively .
Analysis Analysis of gender markers
To analyze the extent to which personal traits are preserved in translations , we extract the set of most discriminative FWs in various texts by employing the InfoGain feature selection procedure ( Gray , 1990 ) .
Gender markers vary across original languages ( with few exceptions ) ; in EP , the most discriminating English features are also , very , perhaps , as , its , others , you .
The French list includes on , vous , dire , afin , doivent , doit , aussi , avait , voil ? , je , while the German list consists of wir , man , wirklich , sollten , von , f?r , dass , allen , ob .
The list of discriminative markers in the TED English dataset contains mainly personal pronouns : she , her , I , you , my , our , me , and , who , it .
Figure 2 ( top ) presents weights assigned to various gender markers by the InfoGain attribute evaluator in originals and translations .
Gender markers are carried over to ( both manual and machine ) translations to an extent that overshadows the original markers of the target language .
In particular , the markers observed in translated English mirror their original French counterparts , in the same marker role : I ( M ) in English translations reflecting the original French je ( M ) , say ( M ) reflecting dire ( M ) , must ( F ) translated from doit ( F ) and doivent ( F ) ; the latter contradicting the original English must which characterizes M speech .
The original English prominent gender markers ( e.g. , also , very ) almost completely lose their discriminative power in translations .
A similar phenomenon is exhibited by English translations from German , as depicted in Figure 2 ( bottom ) : the German wir ( we ) , f?r ( for ) and ob ( whether ) are preserved in ( both manual and machine ) English translations , in the same marker role .
We conclude that ( i ) gender traits in translation are weakened , compared to their originals .
Furthermore , ( ii ) translations tend to embrace gender tendencies of the original language , thus resulting in a hybrid outcome , where male and female traits are affected both by markers of the source and ( to a much lesser extent ) the target language .
Capturing the " personalization " effect Both manual - and all machine - translations of Europarl are tested on a strictly identical set of sentences ; therefore , the performance gap introduced by personalized SMT models can be captured by a subset of sentences misclassified by the baseline model , but classified correctly when applying a more personalized approach .
The inspection of differences in these translations can shed some light on the underlying nature of our personalized models .
Table 7 ( top ) shows manual , baseline , and personalized machine translations of examples of French and German sentences .
The translation of the French word " vraiment " ( in a male utterance ) varies in English as " really " or " exactly " , where the former is more frequent in female English texts , and the latter is a male marker .
The choice of a male English marker over its female equivalent by the gender - aware SMT model demonstrates the effect of personalization as proposed in this paper .
The translations of the German female sentence into English , as presented in Table 7 ( bottom ) , further highlight this phenomenon by choosing the English female marker think in its personalized translation over the more neutral consider and believe in the manual and baseline versions , respectively .
Conclusions
We presented preliminary results of employing personalized SMT models for better preservation of gender traits in automatic translation .
This work leaves much room for further research and practical activities .
Authors ' personal traits are utilized by recommendation systems , conversational agents and other personalized applications .
While resources annotated for personality traits mainly exist for English ( and recently , for a small set of additional languages ) , they are scarce or missing from most other languages .
Employing MT models that are sensitive to authors ' personal traits can fr O ... on a corrig ?
la traduction du mot qui a ?t? traduit en franc ? ais par " propri?t ? " qui n'est pas vraiment la m?me chose qu ' " appropriation " .
fr-en HT ... it had been translated into French using the word for " property " , which is not really the same thing as " ownership " .
fr-en MT -B ... it was corrected the translation of the word which has been translated into French as " ownership " , which is not really the same as " ownership " .
fr-en MT - P1 ... it has corrected the translation of the word which has been translated into French as " ownership " , which is not exactly the same as " ownership " .
de O Entsprechend halte ich es auch f ?r notwendig , da ?
die Kennzeichnung m?glichst schnell und verpflichtend eingef ?
hrt wird , und zwar f?r Rinder und f?r Rindfleisch . de-en HT
Accordingly , I consider it essential that both the identification of cattle and the labelling of beef be introduced as quickly as possible on a compulsory basis .
de-en MT -B Similarly , I believe that it is necessary , as quickly as possible and that compulsory labelling will be introduced , and for bovine animals and for beef and veal .
de-en MT -P1
Accordingly , I also think it is essential that the labelling and become mandatory as quickly as possible , and for bovine animals and for beef .
Table 7 : Translation of fr ( M ) and de ( F ) sentences into English manually , and by different MT models .
facilitate user modeling in other languages as well as augment English data with translated content .
Our future plans include experimenting with more sophisticated MT models , and with additional demographic traits , domains and languagepairs .
Figure 1 : 1 Figure 1 : English EP data distributions across two dimensions : gender ( left ) and trans .
status ( right ) .
Figure 2 : 2 Figure 2 : Persistence of en and fr markers in fr-en translations ( top ) ; en and de markers in de-en translations ( bottom ) .
The transparent bars refer to ( weak ) F / M markers , assigned weight < 0.01 by InfoGain .
Table 2 : 2 presents the accuracy and coverage of each resource based on this methodology .
Gender prediction performance ( % ) .
resource Wikidata Genderize Alchemy coverage 73.0 76.1 59.6 accuracy 100.0 99.6 99.1
Table 4 : 4 EP en-fr , fr-en classification scores ( % ) . ) , respectively .
precision recall acc. dataset M F M F en O 77.7 76.9 76.5 78.1 77.3 fr O 80.9 81.9 82.2 80.5 81.4 fr-en HT 75.6 74.4 73.8 76.2 75.0 fr-en MT -B 77.0 78.2 78.6 76.5 77.6 fr-en MT -P1 82.0 80.7 80.3 82.4 81.4 fr-en MT -P2 79.1 81.0 81.6 78.4 80.0 en-fr HT 56.6 56.4 55.7 57.3 56.5 en-fr MT -B 60.2 60.1 60.0 60.3 60.1 en-fr MT - P1 62.7 63.0 63.5 62.2 62.8 en-fr MT - P2 65.2 65.3 65.4 65.1 65.3
Evidently , gender traits are significantly obfus - cated by both manual and non-personalized ma -
Table 5 : 5 EP en-de , de-en classification scores ( % ) .
Table 6 : 6 TED en- fr classification scores ( % ) .
precision recall acc. dataset M F M F en O 81.2 79.7 79.2 81.6 80.4 en-fr HT 74.0 73.5 73.2 74.3 73.8 en-fr MT -B 71.3 70.1 69.2 72.2 70.7 en-fr MT - P1 77.5 76.8 76.5 77.8 77.2 en-fr MT -P2 78.2 77.2 76.8 78.6 77.7
https://www.mediawiki.org/wiki/Wikibase/API 4 https://genderize.io/ 5
We assume that the country MEPs represent is highly correlated , if not strictly identical , to their country of origin .
This experiment refers to English translated from French ; other language - pairs exhibited similar trends .
Downloaded from http://cm.xrce.xerox.com/.
http://nlp.stanford.edu/software/tagger.shtml
10
We used the lists of function words available at https://code.google.com/archive/p/stop-words.
All differences between MT - P1 and MT - P2 and baseline models are statistically significant .
12 TED talks are subtitled , rather than transcribed , undergoing some editing and rephrasing .
