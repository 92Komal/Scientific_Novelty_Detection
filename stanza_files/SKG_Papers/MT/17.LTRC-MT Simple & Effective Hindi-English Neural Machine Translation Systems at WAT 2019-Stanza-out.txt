title
LTRC - MT Simple & Effective Hindi-English Neural Machine Translation Systems at WAT 2019
abstract
This paper describes the Neural Machine Translation systems of IIIT -Hyderabad ( LTRC - MT ) for WAT 2019 Hindi-English shared task .
We experimented with both Recurrent Neural Networks & Transformer architectures .
We also show the results of our experiments of training NMT models using additional data via backtranslation .
Introduction Neural Machine Translation ( Luong et al. , 2015 ; Bahdanau et al. , 2014 ; Johnson et al. , 2017 ; Vaswani et al. , 2017 ) has been receiving considerable attention in the recent years , given its superior performance without the demand of heavily hand crafted engineering efforts .
NMT often outperforms Statistical Machine Translation ( SMT ) techniques but it still struggles if the parallel data is insufficient like in the case of Indian languages .
Hindi being one of the most common spoken Indian languages , continue to remain as a low resource language demanding further attention from the research community .
The Hindi-English pair has limited availability of sentence level aligned bitext as parallel corpora .
This paper describes an overview of the submission of IIIT Hyderabad ( LTRC ) in WAT 2019 ( Nakazawa et al. , 2019 ) Hindi-English Machine Translation shared task .
We experimented with both attention - based LSTM encoder-decoder architecture & the recently proposed Transformer architecture .
We used Byte Pair Encoding ( BPE ) to enable open vocabulary translation .
We then leveraged synthetic data generated by our own models to improve the translation performance .
Neural MT Architecture
In this section , we briefly discuss the attentionbased LSTM encoder-decoder architecture & the Transformer model .
Attention - based encoder-decoder
In this architecture , the NMT model consists of an encoder and a decoder , each of which is a Recurrent Neural Network ( RNN ) as described in ( Luong et al. , 2015 ) .
The model directly estimates the posterior distribution P ? ( y|x ) of translating a source sentence x = ( x 1 , .. , x n ) to a target sentence y = ( y 1 , .. , y m ) as : P ? ( y|x ) = m t=1 P ? ( y t |y 1 , y 2 , .. , y t?1 , x ) ( 1 ) Each of the local posterior distribution P ( y t |y 1 , 2 , .. , y t?1 , x ) is modeled as a multinomial distribution over the target language vocabulary which is represented as a linear transformation followed by a softmax function on the decoder 's output vector hdec t : c t = AttentionF unction ( h enc 1:n ; h dec t ) ( 2 ) hdec t = tanh ( W o [ h dec t ; c t ] ) ( 3 ) P ( y|y 1 , y 2 , .. , y t?1 , x ) = sof tmax ( W s hdec t ; ? ) ( 4 ) where c t is the context vector , h enc and h dec are the hidden vectors generated by the encoder and decoder respectively , AttentionFunction ( . , .) is the attention mechanism as shown in ( Luong et al. , 2015 ) and [ . ; .] is the concatenation of two vectors .
An RNN encoder first encodes x to a continuous vector , which serves as the initial hidden vector for the decoder and then the decoder performs recursive updates to produce a sequence of hidden vectors by applying the transition function f as : h dec t = f ( h dec t?1 , [ hdec t?1 ; e( y t ) ] ) ( 5 ) where e(. ) is the word embedding operation .
Popular choices for mapping f are Long- Short - Term Memory ( LSTM ) units and Gated Recurrent Units ( GRU ) , the former of which we use in our models .
An NMT model is typically trained under the maximum log-likelihood objective : max ? J ( ? ) = max ? E ( x,y ) ? D [ log P ? ( y| x ) ] ( 6 ) where D is the training set .
Our NMT model uses a bi-directional LSTM as an encoder and a unidirectional LSTM as a decoder with global attention ( Luong et al. , 2015 ) .
The Transformer ( Vaswani et al. , 2017 ) model is the first NMT model relying completely on selfattention mechanism to compute representations of its input and output without using recurrent neural networks ( RNN ) or convolutional neural networks ( CNN ) .
RNNs read one word at a time , having to perform multiple steps before generating an output that depends on words that are far away .
But it has been shown that the more steps required , the harder it is for the network to learn to make these decisions ( Bahdanau et al. , 2014 ) .
RNNs being sequential in nature , do not effectively exploit the modern computing devices such as GPUs which rely on parallel processing .
The Transformer is also an encoder-decoder model that was conceived to solve these problems .
The encoder is composed of three stages .
In the first stage input words are projected into an embedded vector space .
In order to capture the notion of token position within the sequence , a positional encoding is added to the embedded input vectors .
The second stage is a multi-headed selfattention .
Instead of computing a single attention , this stage computes multiple attention blocks over the source , concatenates them and projects them linearly back onto a space with the initial dimensionality .
The individual attention blocks compute the scaled dot-product attention with different linear projections .
Finally a position - wise fully connected feed -forward network is used , which consists of two linear transformations with a ReLU activation ( Nair and Hinton , 2010 ) in between .
Transformer Model
The decoder operates similarly , but generates one word at a time , from left to right .
It is composed of five stages .
The first two are similar to the encoder : embedding and positional encoding and a masked multi-head self-attention , which unlike in the encoder , forces to attend only to past words .
The third stage is a multi-head attention that not only attends to these past words , but also to the final representations generated by the encoder .
The fourth stage is another position - wise feed -forward network .
Finally , a softmax layer allows to map target word scores into target word probabilities .
For more specific details about the architecture , refer to the original paper ( Vaswani et al. , 2017 ) .
Subword Segmentation for NMT Neural Machine Translation relies on first mapping each word into the vector space , and traditionally we have a word vector corresponding to each word in a fixed vocabulary .
Addressing the problem of data scarcity and the hardness of the system to learn high quality representations for rare words , ( Sennrich et al. , 2015 b ) proposed to learn subword units and perform translation at a subword level .
With the goal of open vocabulary NMT , we incorporate this approach in our system as a preprocessing step .
In our early experiments , we note that Byte Pair Encoding ( BPE ) works better than UNK replacement techniques & also aids in better translation performance .
For all of our systems , we learn separate vocabularies for Hindi and English each with 32 k merge operations .
With the help of BPE , the vocabulary size is reduced drastically and we no longer need to prune the vocabularies .
After the translation , we do an extra post processing step to convert the target language subword units back to normal words .
We found this approach to be very helpful in handling rare word representations .
Synthetic Training Data
To utilize monolingual data along with IITB corpus , we employ back translation .
Backtranslation ( Sennrich et al. , 2015a ) is a widely used data augmentation technique for aiding Neural Machine Translation for languages low on parallel data .
The method works by generating synthetic data on the source side from target side monolingual data using a target- to - source NMT model .
The synthetic parallel data thus formed is combined with the actual parallel data to train a new NMT model .
We used around 10M English sentences and backtranslated them into Hindi using a English - Hindi NMT model .
Experimental Setup
Dataset
In our experiments , we used IIT -Bombay ( Kunchukuttan et al. , 2017 ) Hindi-English parallel data provided by the organizers .
The training corpus provided by the organizers , consists of data from mixed domains .
There are roughly 1.5 M samples in training data from diverse sources , while the development and test sets are from news domains .
In addition to this , around 10M English monolingual data from WMT14 newscrawl articles is used in our backtranslation enabled attempts at training an NMT system .
Data Processing
We used Moses ( Koehn et al. , 2007 ) toolkit for tokenization and cleaning the English side of the data .
Hindi side of the data is first normalized with Indic NLP library 1 followed by tokenization with the same library .
As our preprocessing step , we removed all the sentences of length greater than 80 from our training corpus .
We used BPE segmentation with 32 k merge operations .
During training , we lowercased all of our training data & used truecase 2 as a truecaser during testing .
Training Details
For all of our experiments , we used OpenNMTpy ( Klein et al. , 2018 ) toolkit .
We used both attention - based LSTM models and Transformer models in our submissions .
We used an LSTM based Bi-directional encoder and a unidirectional decoder along with global attention mechanism .
We kept 4 layers in both the encoder & decoder with embedding size set to 512 .
The batch size was set to 64 and a dropout rate of 0.3 .
We used Adam optimizer ( Kingma and Ba , 2014 ) for all our experiments .
For our transformer model , we used 6 layers in both encoder and decoder with 512 hidden units in each layer .
The word embedding size was set to 512 with 8 heads .
The training is run in batches of maximum 4096 tokens at a time with dropout set to 0.3 .
The model parameters are optimized using Adam optimizer .
Results
In table 2 , we report Bilingual Evaluation Understudy ( BLEU ) ( Papineni et al. , 2002 ) score , Rank - based Intuitive Bilingual Evaluation Score ( RIBES ) ( Isozaki et al. , 2010 ) , Adequacy - fluency metrics ( AM - FM ) ( Banchs et al. , 2015 ) and the Human Evaluation results provided by WAT 2019 for all our attempts .
The results show that our NMT system based on Transformer & backtranslation is ranked 2nd among all the constraint submissions made in WAT 2019 Hindi-English shared task & is ranked 3rd overall .
Conclusion Future Work
We believe that NMT is indeed a promising approach for Machine Translation of low resource languages .
In this paper , we showed the effectiveness of Transformer models on a low resource languages pair Hindi-English .
Additionally we show , how synthetic data can help improving the NMT systems for Hindi-English .
Figure 1 : 1 Figure 1 : Transformer model architecture from Vaswani et al . ( 2017 )
