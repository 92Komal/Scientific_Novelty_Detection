title
Dependency Analysis of Scrambled References for Better Evaluation of Japanese Translations
abstract
In English-to - Japanese translation , BLEU ( Papineni et al. , 2002 ) , the de facto standard evaluation metric for machine translation ( MT ) , has very weak correlation with human judgments ( Goto et al. , 2011 ; Goto et al. , 2013 ) .
Therefore , RIBES ( Isozaki et al. , 2010 ; was proposed .
RIBES measures similarity of the word order of a machine - translated sentence and that of a corresponding human-translated reference sentence .
RIBES has much stronger correlation than BLEU but most Japanese sentences have alternative word orders ( scrambling ) , and one reference sentence is not sufficient for fair evaluation .
proposed a solution to this problem .
This solution generates semantically equivalent word orders of reference sentences .
Automatically generated word orders are sometimes incomprehensible or misleading , and they introduced a heuristic rule that filters out such bad sentences .
However , their rule is too conservative and generated alternative word orders for only 30 % of reference sentences .
In this paper , we present a rule-free method that uses a dependency parser to check scrambled sentences and generated alternatives for 80 % of sentences .
The experimental results show that our method improves sentence - level correlation with human judgments .
In addition , strong system-level correlation of single reference RIBES is not damaged very much .
We expect this method can be applied to other languages such as German , Korean , * This work was done while the second author was a graduate student of Okayama Prefectural University .
Spearman's ? with adequacy NTCIR-7 JE RIBES JE BLEU NTCIR-9 JE RIBES JE BLEU EJ RIBES EJ BLEU NTCIR-10 JE RIBES JE BLEU EJ RIBES EJ BLEU 0.0 0.2 0.4 0.6 0.8 1.0 Figure 1 : RIBES has better correlation with adequacy than BLEU ( system- level correlation ) Turkish , Hindi , etc .
Introduction
For translation among European languages , BLEU ( Papineni et al. , 2002 ) has strong correlation with human judgments and almost all MT papers use BLEU for evaluation of translation quality .
However , BLEU has very weak correlation with human judgments in English-to - Japanese / Japanese - to - English translation , and a new metric RIBES ( Isozaki et al. , 2010 ; has strong correlation with human judgments .
RIBES measures similarity of the word order of a machine translated sentence and that of a human-translated reference sentence .
Figure 1 compares RIBES and BLEU in terms of Spearman 's ? with human judgments of adequacy based on NTCIR -7/9/10 data ( Isozaki et al. , 2010 ; Goto et al. , 2011 ; Goto et al. , 2013 ) .
Japanese and English have completely different word order , and phrase - based SMT systems tend to output bad word orders .
RIBES correctly points out their word order problems .
In this paper , we propose a method to improve " sentence - level correlation " , which is useful for MT developers to find problems of their MT systems .
If the sentence - level correlation is strong , low RIBES scores indicate bad translations , and we will find typical failure patterns from them .
However , improvement of sentence - level correlation is more difficult than system-level correlation and current automatic evaluation metrics do not have strong correlation .
( Leusch et al. , 2003 ; Stanojevi ? and Sima'an , 2014 ; Echizen-ya and Araki , 2010 ; Callison - Burch et al. , 2012 )
Scrambling
As for Japanese translation , however , we should consider " scrambling " or acceptable reordering of phrases .
For example , " John ga Tokyo de PC wo katta " ( John bought a PC in Tokyo ) consists of the main verb " katta " ( bought ) and its modifiers .
" Ga " , " de " , and " wo " are case markers . ?
" Ga " is a nominative case marker . ?
" De " is a locative case marker . ?
" Wo " is an accusative case marker .
This sentence can be reordered as follows .
1 . John ga Tokyo de PC wo katta . ( 1.00 ) 2 . John ga PC wo Tokyo de katta . ( 0.86 ) 3 . Tokyo de John ga PC wo katta . ( 0.86 ) 4 . Tokyo de PC wo John ga katta . ( 0.71 ) 5 . PC wo John ga Tokyo de katta . ( 0.71 ) 6 . PC wo Tokyo de John ga katta . ( 0.57 )
All of the above sentences are acceptable and have the same meaning , and this is called " scrambling " .
However , RIBES outputs different scores for these sentences .
When we use the first one as the reference sentence , RIBES output scores in the parentheses .
Human judges will give almost equal scores to all of them , and we should improve these RIBES scores for better evaluation .
Scrambling is also observed in other languages such as German ( Maier et al. , 2014 ) , Korean ( Chun , 2013 ) , Turkish ( ld? z et al. , 2014 , Hindi ( Sharma and Paul , 2014 ) , etc. Figure 2 ( a ) shows the dependency tree of " John ga Tokyo de PC wo katta " .
Each box indicates a bunsetsu ( chunk ) .
Arrows indicate modification relations .
The source node of an arrow modifies the target node of the arrow .
The root " katta " has three modifiers ( children ) , " John ga " , " Tokyo de " , and " PC wo " .
We can generate 3 ! = 6 word orders by post-order traversal of this tree because the order of siblings does not matter .
Figure 2 ( b ) shows a permutation and its dependency tree .
In this case , all permutations are acceptable .
However , more complex dependency trees tend to generate misleading / incomprehensible sentences .
Figure 2 ( c ) shows such a sentence : " John ga PC wo katta ato ni Alice kara denwa ga atta " .
( After John bought a PC , there was a phone call from Alice ) .
" X ato ni Y " means " After X , Y " .
" Denwa " means " a phone call " .
" Atta " means " there was " .
This tree has 2 ! ? 3 ! = 12 post-order permutations .
Some of them are misleading .
For example , " Alice kara John ga PC wo katta ato ni denwa ga atta " sounds like " After John bought a PC from Alice , there was a phone call " because " Alice kara " ( from Alice ) precedes " katta " ( bought ) .
This sentence will have a dependency tree in Figure 2 ( d ) . 1.2 Rule- based filtering of bad sentences tried to solve the above problem by automatic generation of reordered sentences and use of a heuristic rule ( constraint ) to filter out bad sentences .
?
Use a Japanese dependency parser to get dependency trees of reference sentences .
?
Check the dependency trees and manually correct wrong ones because sentence - level accuracy of dependency analyzers are still low . ?
In order to get Japanese - like head final sentences , output words in the corrected dependency tree in post-order .
That is , recursively output all child nodes before a mother node .
They called this method " postOrder " .
?
The above " postOrder " generates misleading / incomprehensible sentences .
In order to inhibit them , they introduced the following rule called " Simple Case Marker Constraint " :
If a reordered sentence has a case marker phrase of a verb that precedes another verb before the verb , the sentence is rejected .
" wo " case markers can precede adjectives before the verb .
Here , we call this " rule2014 " .
This " rule2014 " improved sentence - level correlation of NTCIR -7 EJ data .
However , rule2014 is so conservative that only 30 % of reference sentences obtained alternative word orders .
In the next section , we present a method that covers more reference sentences .
Methodology
Our idea
We do not want to introduce more rules to cover more sentences .
Instead we present a rule-free method .
Our idea is simple : if a reordered sentence is misleading or incomprehensible , a dependency parser will output a dependency tree different from the original dependency tree .
That is , use a dependency parser for detecting misleading sentences .
We apply a dependency parser to the reordered reference sentences .
If the dependency parser outputs the same dependency tree with the original reference sentence except sibling orders , accept the word order as a new reference .
Otherwise , it is a misleading word order and reject it .
( We do not parse MT output because it is often broken and dependency analysis will fail . )
For example , " PC wo Tokyo de John ga katta " has the dependency tree in Figure 2 ( b ) .
This tree is the same as ( a ) except the order of three siblings .
We do n't care about the order of siblings , and accept this as a new reference sentence .
On the other hand , the parser shows that " Alice kara John ga PC wo katta ato ni denwa ga atta " has the dependency tree in ( d ) , which is different from ( c ) and we reject this sentence .
We call this method " com - pDep " because it compares dependency trees of reordered reference sentences with the original dependency tree .
Each MT output sentence is evaluated by the best of RIBES scores for remaining reordered reference sentences .
This is a sentence - level score .
A system 's score ( system- level score ) is the average of sentence - level scores of all test sentences .
Data and tools
We use NTCIR -7 PatentMT EJ data ( Fujii et al. , 2008 ) and NTCIR-9 PatentMT EJ data ( Goto et al. , 2011 ) . 1 NTCIR -7 EJ human judgment data consists of 100 sentences ? five MT systems .
NTCIR -9 EJ human judgment data consists of 300 sentences ? 17 MT systems .
NTCIR provided only one reference sentence for each sentence .
When we use only the provided reference sentences , we call it " single ref " .
We apply a popular Japanese dependency parser CaboCha 2 to the reference sentences , and manually corrected its output just like . 40 % of NTCIR - 7 dependency trees and 50 % of NTCIR - 9 dependency trees were corrected .
Based on the corrected dependency trees , we generate all post-order permutations .
Then we apply CaboCha to these reordered sentences .
We compare the dependency tree of the original reference sentence with that of a reordered reference sentence .
We accept a reordered reference sentence only when its tree is the same as that of the original reference sentence except the sibling order .
This tree comparison is implemented by removing word IDs and chunk IDs from the trees keeping their dependency structures and sorting children of each node by their surface strings .
These sorted dependency trees are compared recursively from their roots .
Experimental Results
Table 1 shows that our compDep method succeeded in generating more reordered sentences ( permutations ) than rule2014 .
The column with # perms = 1 indicates failure of generation of reordered sentences .
As for NTCIR -7 , rule2014 failed for 70 % From the viewpoint of the number of such failures , postOrder ( ?1.2 ) is the best method , but pos-tOrder does not filter out bad sentences , and it leads to the loss of system-level correlation with adequacy ( See ?3.2 ) .
Sentence-level correlation
Here , we focus on adequacy because it is easy to generate fluent sentences if we disregard adequacy .
Figure 3 shows NTCIR -7 EJ results .
our compDep succeeded in improving sentence - level correlation with adequacy for four MT systems among five .
The average of ? was improved from single ref's 0.558 to 0.606 .
Figure 4 shows NTCIR -9 EJ results .
our com - pDep succeeded in improving sentence - level correlation of all 17 MT systems .
The average of ? was improved from single ref's 0.385 and rule2014 's 0.396 to compDep 's 0.420 .
The improvement from single ref to compDep is statistically significant with p = 0.000015 ( two - sided sign test ) for NTCIR -9 data .
The improvement from rule2014 to compDep is also statistically significant with p = 0.01273 .
2014 ) pointed out that postOrder loses system-level correlation with adequacy because it also generates bad word orders .
Figure 5 shows that system-level correlation of compDep is comparable to that of single ref and rule 2014 .
Spearman's ? of compDep in NTCIR -7 ( 0.90 ) looks slightly worse than single ref and rule 2014 ( 1.00 ) .
However , this is not a big problem because the NTCIR - 7 correlation is based on only five systems as described in ?2.2 , and the NTCIR -9 correlation based on 17 systems did not degrade very much ( compDep : 0.690 , single ref : 0.695 , rule2014 : 0.668 ) .
Table 2 shows details of system-level correlation of NTCIR -7 EJ .
Single reference RIBES and rule2014 completely follows the order of adequacy .
On the other hand , compDep slightly violates this order at the bottom of the table .
NICT - ATR and kuro is swapped .
The " single ref " and " rule2014 " scores of this table are slightly different from that of Table 5 of .
This difference is caused by the difference of normalization of punctuation symbols and full- width / half - width alphanumeric letters .
Figure 6 shows that the effects of manual correction of dependency trees .
The average of sin- gle ref , compDep , and compDep without correction are 0.388 , 0.422 , and 0.420 , respectively .
Thus , the difference between compDep ( with correction ) and compDep without correction is very small and we can skip the manual correction step .
We used dependency analysis twice in the above method .
First , we used it for generation of reordered reference sentences .
Second , we used it for detecting misleading word orders .
In the first usage , we manually corrected dependency trees of the given reference sentences .
In the second usage , however , we did not correct dependency trees of reordered reference sentences because some sentences have thousands of permutations ( Table 1 ) and it is time - consuming to correct all of them manually .
Moreover , some reordered sentences are meaningless or incomprehensible , and we cannot make their correct dependency trees .
Therefore , we did not correct them .
Our experimental results have shown that we can omit correction in the first step .
Related Work
Our method uses syntactic information .
Use of syntactic information in MT evaluation is not a We designed our method not to parse MT outputs because some MT outputs are broken and it is difficult to parse them .
Our method does not parse MT outputs and we expect our method is more robust than these methods .
Recently ,
Yu et al. ( 2014 ) proposed RED , an evaluation metric based on reference dependency trees .
They also avoided parsing of " results of noisy machine translations " and used only dependency trees of reference sentences .
However , their research motivation is completely different from ours .
They did not mention scrambling at all , and they did not try to generate reordered reference sentences , but it is closely related to our method .
It might be possible to make a better evaluation method by combining our method and their method .
Some readers might think that adequacy is not very reliable .
WMT - 2008 ( Callison - Burch et al. , 2008 gave up using adequacy as a human judgement score because of unreliability .
NTCIR organizers used relative comparison to improve reliability of adequacy .
The details are described in Appendix A of Goto et al . ( 2011 ) .
Conclusions RIBES ( Isozaki et al. , 2010 ) is a new evaluation metric of translation quality for distant language pairs .
It compares the word order of an MT output sentence with that of a corresponding reference sentence .
However , most Japanese sentences can be reordered and a single reference sentence is not sufficient for fair evaluation .
proposed a rule-based method for this problem but it succeeded in generating alternative word orders for only 11 - 30 % of reference sentences .
In this paper , we proposed a method that uses a dependency parser to detect misleading reordered sentences .
Only when a reordered sentence has the same dependency tree with its original reference sentence except the order of siblings , we accept the reordered sentence as a new reference sentence .
This method succeeded in generating alternative word orders for 80 - 89 % and improved sentence - level correlation of RIBES with adequacy and its system-level correlation is comparable to the single reference RIBES .
In conventional MT evaluations , we have to prepare multiple references for better evaluation .
This paper showed that we can automatically generate multiple references without much effort .
Future work includes use of the generated reference sentences in other metrics such as BLUE .
We expect that this method is applicable to other languages such as German , Korean , Turkish , Hindi , etc. because they have scrambling .
Figure 2 : Dependency trees
Table 2 : 2 Details of system- level RIBES scores ( NTCIR -7 EJ ) Pearson with adequacy 0.0 0.2 0.4 0.6 0.8 1.0 NTCIR - 7 single ref compDep rule2014 postOrder NTCIR -9 single ref compDep rule2014 postOrder Spearman's ? with adequacy 0.0 0.2 0.4 0.6 0.8 1.0 NTCIR - 7 single ref rule2014 compDep postOrder NTCIR -9 single ref rule2014 compDep postOrder Figure 5 : System-level correlation with adequacy Adequacy Averaged RIBES single ref rule2014 compDep tsbmt 3.527 0.722 0.726 0.750 Moses 2.897 0.707 0.720 0.745 NTT 2.740 0.670 0.682 0.722 NICT -ATR 2.587 0.658 0.667 0.706 kuro 2.420 0.633 0.643 0.711
NTCIR - 8 did not provide human judgments .
NTCIR -10 submission data was not publicly available yet at the time of writing this paper . 2
http://code.google.com/p/cabocha/
