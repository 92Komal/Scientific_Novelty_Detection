title
Meta Ensemble for Japanese - Chinese Neural Machine Translation : Kyoto-U+ECNU Participation to WAT 2020
abstract
This paper describes the Japanese - Chinese Neural Machine Translation ( NMT ) system submitted by the joint team of Kyoto University and East China Normal University ( Kyoto - U+ ECNU ) to WAT 2020 ( Nakazawa et al. , 2020 .
We participate in APSEC Japanese - Chinese translation task .
We revisit several techniques for NMT including various architectures , different data selection and augmentation methods , denoising pre-training , and also some specific tricks for Japanese - Chinese translation .
We eventually perform a meta ensemble to combine all of the models into a single model .
BLEU results of this meta ensembled model rank the first both on 2 directions of ASPEC Japanese - Chinese translation .
Introduction Neural Machine Translation ( NMT ) ( Sutskever et al. , 2014 ; Bahdanau et al. , 2015 ) has led to large improvements in machine translation quality when large parallel corpora are available for training .
In this work , we revisit several existing NMT based techniques on ASPEC Japanese - Chinese translation task .
Furthermore , we conduct a meta ensemble to fuse various NMT based systems .
The aspects this work feature can be summarized as : ?
We revisit various NMT based systems including LSTM ( Sutskever et al. , 2014 ) , Transformer ( Vaswani et al. , 2017 ) , ConvS2S ( Gehring et al. , 2017 ) and Lightconv with different data augmentation and filtering techniques . ?
We implement a meta ensemble on various NMT systems and obtain the state - of- the - art results on ASPEC Japanese - Chinese translation task . ?
We empirically compare fully ( semi - ) supervised training with the recent popular language model pre-training based methods ( Conneau and Lample , 2019 ; Song et al. , 2019 ; . ?
We revisit and explore the trick of character mapping Chen et al. , 2020 ) between Chinese and Japanese on AS - PEC translation task .
Although only our team participated in the ASPEC Japanese - Chinese translation task this year , BLEU results we report on the WAT official leader - board rank 1st both on ja?zh and zh? ja compared with all the previous submitted systems .
ASPEC Japanese - Chinese Translation Task Number ASPEC ( Asian Scientific Paper Excerpt Corpus ) was constructed in the Japanese - Chinese machine translation project from 2006 to 2010 by Japan Science and Technology Agency .
ASPEC Japanese - Chinese ( ASPEC - JC , shown in Table 1 ) and ASPEC Japanese -English ( ASPEC - JE ) respectively consists approximately 0.68 M and 3 M parallel sentences for training .
In this work , we focus on NMT system training for ASPEC - JC while parts of ASPEC - JE is leveraged for data augmentation .
Our System
Sequence-to-Sequence Framework Sequence-to-sequence framework ( S2S ) is the basic technique being used to learn a mapping from a source sentence to a target sentence in an end-toend manner .
In this section , we revisit four different architectures for sequence - to-sequence learning including LSTM , ConvS2S , Transformer , and Lightconv .
In our system , most experimental settings are based on Transformer while we also implement other 3 architectures to compare the performance on ASPEC task .
LSTM ( Sutskever et al. , 2014 ) . Long- Short - Term Memory ( LSTM ) is a special recurrent neural network ( RNN ) , which solves the problem of gradient vanishing / exploding on the long sequence by integrating three gates ( one input gate , one forget gate , and one output gate ) and memory cells .
Therefore , LSTM is capable of storing and forgetting information for longer periods of time on the sequence .
ConvS2S ( Gehring et al. , 2017 ) .
Compared with RNNs that maintain a hidden state of the entire past , convolution operations can be fully parallelized during training .
ConvS2S integrates the convolution operations into the sequence - to-sequence framework , which not only improves computation efficiency but also captures the long-range dependencies over the input sequence through multi-layer hierarchical structure .
Transformer ( Vaswani et al. , 2017 ) .
Transformer eschew recurrence and convolutions entirely , relying on the attention mechanism to capture global dependencies between input and output sequence .
Due to its high parallelism and the high quality in the translation task , it has become the most popular model among researchers .
Lightconv .
Lightconv is a lightweight convolution model that utilizes lightweight convolutions and dynamic convolutions instead of the self-attention mechanism .
The attention weights of self-attention are computed by comparing the current time -step to all elements on the sequence , which brings a great challenge for longer sequences due to the quadratic complexity .
Lightconv builds dynamic convolutions to predict a different kernel at each time -step rather than the entire sequence , which drastically reduces the number of parameters .
Data Augmentation by Filtering Out-of-domain Parallel Data NMT quality depends highly on the size of the training data .
Thus , high quality augmented training dataset help ameliorate NMT .
There exist just around 0.68 M parallel sentences in ASPEC - JC , so we expect that extra parallel data can significantly improve the translation quality .
We use Japanese - Chinese parallel datasets on other domains collected by IWSLT 2020 ( Ansari et al. , 2020 ) .
1
All the out-of- domain datasets used are shown in Table 2 including Ubuntu corpora from OPUS ( Tiedemann , 2012 ) , Global Voices , and News Commentary ; OpenSubtitles ( Lison and Tiedemann , 2016 ) ; TED talks ; Wikipedia ( Chu et al. , 2014 ( Chu et al. , , 2016 ; Wikitionary.org ; 2 Tatoeba.org under CC -BY License ; 34 and WikiMatrix .
In total , over 1.9 M out - of- domain parallel data are publicly available which can be leveraged to enhance the performance on the ASPEC task .
However , we observe some sentence alignments are of low accuracy .
Thus , we also conduct a filtering by using LASER ( Artetxe and Schwenk , 2019 ) .
Specifically , we remove sentence pairs with the cossimilarity score less than a fixed threshold where similarity scores are calculated by LASER embeddings of two sentences .
We observe that alignment quality tends to be high if the similarity score is over 0.6 , so we use this value as the filtering thresh - old for most experiments .
This results in 1.5 M filtered out - of- domain parallel sentences which we leverage to train the NMT system jointly with ASPEC - JC dataset .
We also revisit the domain adaption method ( Chu et al. , 2017 ) by adding tags of 2indomain and 2outof domain during the training phase .
Data Augmentation by Back Translation
Besides using out - of- domain parallel data , we also implement back translation ( Sennrich et al. , 2016a ; Edunov et al. , 2018 ) , another effective data augmentation technique for NMT .
For monolingual corpora , we use the Japanese sentences in ASPEC - JE as in- domain Japanese monolingual data , where 3M Japanese sentences are used to augment the ASPEC - JC corpus .
We do not perform the back translation by using Chinese monolingual data because no in-domain Chinese sentences are available .
We neither do not consider using other outof-domain monolingual data .
For the translation direction of ja?zh , we name it forward translation because of the absence of the target - side monolingual data , which means we use the pre-trained system of ja?zh to forward translate monolingual Japanese sentences into Chinese to augment the original dataset .
Character Mapping Character mapping is another essential trick frequently being used in Japanese - Chinese translation tasks Chen et al. , 2020 ) because there are a large number of shared Chinese characters in Chinese and Japanese .
Usually they not only share the character but also share the semantic function within a sentence , so pre-mapping Chinese characters to the target - side can help amplify the cross-lingual supervision .
More precisely , for Kana characters in Japanese , we remain them with the same tokenization granularity whereas for Chinese characters , we first tokenize them into by characters , then use the character mapping table ( Chu et al. , 2012 ) to pre-map them to target-side .
mBART : Multilingual Denoising Pre-training
After the apperance of BERT ( Devlin et al. , 2019 ) , several pre-training methods are proposed for enhancing NMT ( Conneau and Lample , 2019 ; Song et al. , 2019 ; Ren et al. , 2019 ; . Recently , mBART is a multilingual sequence - to-sequence language model pre-trained by denoising tasks on 25 languages including Japanese and Chinese .
Specifically , we fine- tune mBART25 5 by Japanese - Chinese parallel sentences and compare this kind of multilingual pre-training with other fully ( semi - ) supervised baselines .
Meta Ensemble Systems ( Kasai et al. , 2020 ) .
Finally , we conduct an self-ensemble to combine all the above introduced systems .
We take the checkpoint average for each system , thus the final ensemble is an ensemble of self-ensemble .
Consequently , we call the final ensemble phase by meta ensemble .
Preprocessing and Training Details
We conduct tokenization by using Juman 6 ( Kurohashi et al. , 1994 ; Morita et al. , 2015 ) for Japanese and stanford parser pku 7 for Chinese .
Sentences over 175 tokens are removed for training .
We build a joint vocabulary with 30 k merge operations by Byte-Pair Encoding ( Sennrich et al. , 2016 b ) .
This results in a joint vocabulary with approximately 40 k tokens .
Note that we build a single vocabulary for all the settings except fine-tuning mBART25 .
Table 3 : BLEU results on test sets of ASPEC ja-zh and zh-ja translation ) .
Bold denotes top - 5 BLEU scores of each column .
" Best " and " Avg .10 " respectively means the best checkpoint and the average of last 10 checkpoints .
" out - of- domain " means training with additional out - of- domain parallel data .
" BT / FT ( i ) " denotes i-th turns of the back translation or forward translation .
" big " means transformer - big setting and " big ( 12/1 ) " means transformerbig with 12 encoder layers and 1 decoder layer .
" ls " , " dp " and " ffhd " represents label smoothing , dropout and feed -forward hidden dimension , respectively .
Model settings without declarations of " ls " , " dp " and " ffhd " are set to " ls ( 0.1 ) " , " dp ( 0.3 ) " and " ffhd ( 4,096 ) " for transformer - big and default for other S2S frameworks .
" Threshold " means the filtering threshold value for LASER embedding .
for fine-tuning mBART25 .
For parts of the outof-domain Chinese sentences that are in traditional Chinese , we transfer Traditional Chinese characters to Simplified Chinese ones .
8
We conduct all the experiments by using Fairseq 9 ( Ott et al. , 2019 ) , an open source sequenceto-sequence framework implementation .
Most systems are set to the Transformer - big setting except those built up by other architectures or different model capacities .
In particular , our model has a 6layer encoder and decoder , a hidden size of 1,024 , a feed-forward hidden layer size of 4,096 , batch - size of 2,048 , dropout rate of 0.3 and 16 attention heads .
For LSTM , we also use a 6 - layer encoder and decoder architecture .
For ConvS2S and Lightconv , we use the default settings in Fairseq .
All the systems are early stopped if BLEU does not improve for continuous 50,000 steps .
Experiments are run on 8 TITAN X ( Pascal ) GPUs except that single V100 GPU is used for mBART25 fine-tuning .
We use BLEU ( Papineni et al. , 2002 ) for automatic evaluation .
Results
Single Model Results Results of 16 single models with different settings are shown in Table 3 . First , we report the results of vanilla transformerbig model ( # 1 ) .
By averaging last 10 checkpoints , BLEU results of 35.82 and 49.03 on ja-zh and zh- ja are obtained , which is slightly higher than results on the best checkpoints .
Second , we train NMT systems with out-ofdomain parallel corpora ( # 2 & # 3 ) .
We observe almost no BLEU improvements from additional training data , which can be attributed to extremely high alignment quality of the ASPEC - JC training data .
BLEUs of averaged models increase by adding the domain tag ( # 3 ) that has once been demonstrated effective in LSTM architecture on the same task by Chu et al . ( 2017 ) .
Third , by conducting back translation and forward translation ( # 4 ? # 7 ) with Japanese indomain monolingual data from ASPEC - JE , we obtain significant improved BLEUs .
However , iterative back ( or forward ) translation have marginal contributions .
Furthermore , we also mix out -ofdomain parallel data with back ( or forward ) translated synthetic parallel data to train the NMT system with the threshold of 0.9 .
Although no BLEU improvements observed , different generations will contribute to model ensembling .
Fourth , we report the results of systems trained by different architectures .
For the implementation of LSTM , ConvS2S , and Lightconv , we utilize the same training set as that of back ( or forward ) translation with 0.6 filtering threshold .
As shown by # 8 , 9 , 10 in Table 3 , Lightconv yields better results compared with transformer architecture ( # 5 ) .
LSTM and ConvS2S underperform the other 2 architectures but are capable to yield generations with discrepancy ( see 5.2 ) .
Fifth , we report results of deep encoder shallow decoder system ( # 11 ) , larger transformer ( # 12 ) and deeper transformer ( # 13 ) .
These 3 systems yield comparable results as the standard transformer - big system ( # 5 ) .
Whether they give contributions to model ensembling will be demonstrated in the next section .
Sixth , we revisit the trick of character mapping on Japanese - Chinese translation task .
The results of # 14 show that only averaged result on jazh marginally outperforms the vanilla transformer ( # 1 ) .
The failure of this trick on APPEC task can be ascribed to the high quality of ASPEC - JC , which indicates that this trick harms the Chinese character embedding learning on a well - constructed parallel corpus .
Last , we explore recent popular pre-training methods on this task .
We observe improvements by fine-tuning mBART25 on zh-ja whereas BLEU decreases on ja-zh ( # 15 ) .
Moreover , by fine- tuning mBART25 with the back ( or forward ) translated training set , no significant BLEU improvements are observed , which indicates the conflict nature between denoising pre-training and back translation in the semi-supervised scenario .
Ensembled Model Results
In this section , we report ensembled model results which are shown in Table 4 .
We have trained 16 NMT systems and 12 of them ( # 1 , # 2 , # 4 ? # 13 ) can be directly ensembled because they share an identical vocabulary and the same format of the source sentence ( without adding tags or premapping ) .
However , it is difficult to discover the best ensemble combinations .
Thus , we conduct model ensembling in a greedy manner by the model order ( start from # 1 ) .
In Table 4 , we only list out positive ensemble combinations .
We observe that BLEU improves significantly for the first 2 ensemble ( # 17 ? # 19 ) while BLEU improvements slow down after # 19 .
We also observe that most ensemble combinations give BLEU improvements ( 9 models give positive contribution on zh-ja while 8 on zh-ja ) .
This demonstrates that ensembled systems can yield better results by changing the training set , utilizing different architectures , and modifying model capacities even though their single models do not provide performance improvements .
As shown in Table 5 , we observe generated results are of extremely high quality .
Last , systems we submitted for official evaluation are shown in Table 6 .
It is worth mentioning that the result of # 25 on zh-ja and the result of # 26 on ja-zh rank first on ASPEC - JC leaderboard .
Ensembled results without using other resources ( # 28 & # 29 ) respectively rank second on zh-ja and third on ja-zh .
Conclusion
In this work , we participated in ASPEC - JC translation task .
We revisited several strong NMT base-lines , tricks for handling training set , and also pre-training + fine -tuning methods for Japanese - Chinese NMT .
Furthermore , we conducted a deliberate search of better ensembled models and obtained state - of - the - art translation results on this task .
Because of the high BLEU results on this task , in the future , adapting systems trained on ASPEC to other domains should be explored and unsupervised machine translation on this task can be focused on .
Table 1 : 1 ASPEC - JC overview .
We merge " dev " and " devtest " as development set in our experiments .
of parallel sentences train 672,315 dev 2,090 devtest 2,148 test 2,107
