title
Hierarchical Chunk-to-String Translation *
abstract
We present a hierarchical chunk- to-string translation model , which can be seen as a compromise between the hierarchical phrasebased model and the tree- to-string model , to combine the merits of the two models .
With the help of shallow parsing , our model learns rules consisting of words and chunks and meanwhile introduce syntax cohesion .
Under the weighed synchronous context-free grammar defined by these rules , our model searches for the best translation derivation and yields target translation simultaneously .
Our experiments show that our model significantly outperforms the hierarchical phrasebased model and the tree-to-string model on English - Chinese Translation tasks .
Introduction
The hierarchical phrase - based model ( Chiang , 2007 ) makes an advance of statistical machine translation by employing hierarchical phrases , which not only uses phrases to learn local translations but also uses hierarchical phrases to capture reorderings of words and subphrases which can cover a large scope .
Besides , this model is formal syntax - based and does not need to specify the syntactic constituents of subphrases , so it can directly learn synchronous context- free grammars ( SCFG ) from a parallel text without relying on any linguistic annotations or assumptions , which makes it used conveniently and widely .
However , it is often desirable to consider syntactic constituents of subphrases , e.g. the hierarchical phrase X ?
X 1 for X 2 , X 2 de X 1 can be applied to both of the following strings in Figure 1 " A request for a purchase of shares " " filed for bankruptcy " , and get the following translation , respectively " goumai gufen de shenqing " " pochan de shenqing " .
In the former , " A request " is a NP and this rule acts correctly while in the latter " filed " is a VP and this rule gives a wrong reordering .
If we specify the first X on the right - hand side to NP , this kind of errors can be avoided .
The tree-to-string model ( Liu et al. , 2006 ; Huang et al. , 2006 ) introduces linguistic syntax via source parse to direct word reordering , especially longdistance reordering .
Furthermore , this model is formalised as Tree Substitution Grammars , so it observes syntactic cohesion .
Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous .
Fox ( 2002 ) shows that translation between English and French satisfies cohesion in the majority cases .
Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs ( Wu , 1997 ; Yamada and Knight , 2001 ; Eisner , 2003 ; Graehl and Knight , 2004 ; Quirk et al. , 2005 ; Cherry , 2008 ; Feng et al. , 2010 ) .
But unfortunately , the tree-to-string model requires each node must be strictly matched during rule matching , which makes it strongly dependent on the relationship of tree nodes and their roles in the whole sentence .
This will lead to data sparseness and being vulnerable to parse errors .
In this paper , we present a hierarchical chunk - tostring translation model to combine the merits of the two models .
Instead of parse trees , our model introduces linguistic information in the form of chunks , so it does not need to care the internal structures and the roles in the main sentence of chunks .
Based on shallow parsing results , it learns rules consisting of either words ( terminals ) or chunks ( nonterminals ) , where adjacent chunks are packed into one nonterminal .
It searches for the best derivation through the SCFG - motivated space defined by these rules and get target translation simultaneously .
In some sense , our model can be seen as a compromise between the hierarchical phrase - based model and the tree-tostring model , specifically ?
Compared with the hierarchical phrase - based model , it integrates linguistic syntax and satisfies syntactic cohesion .
?
Compared with the tree-to-string model , it only needs to perform shallow parsing which introduces less parsing errors .
Besides , our model allows a nonterminal in a rule to cover several chunks , which can alleviate data sparseness and the influence of parsing errors .
? we refine our hierarchical chunk - to-string model into two models : a loose model ( Section 2.1 ) which is more similar to the hierarchical phrase - based model and a tight model ( Section 2.2 ) which is more similar to the tree-to-string model .
The experiments show that on the 2008 NIST English - Chinese MT translation test set , both the loose model and the tight model outperform the hierarchical phrase - based model and the tree- to-string model , where the loose model has a better performance .
While in terms of speed , the tight model runs faster and its speed ranking is between the treeto-string model and the hierarchical phrase - based model .
Modeling Shallow parsing ( also chunking ) is an analysis of a sentence which identifies the constituents ( noun groups , verbs , verb groups , etc ) , but neither specifies their internal structures , nor their roles in the main sentence .
In Figure 1 , we give the chunk sequence in the first row for each sentence .
We treat shallow parsing as a sequence label task , and a sentence f can have many possible different chunk label sequences .
Therefore , in theory , the conditional probability of a target translation e conditioned on the source sentence f is given by taking the chunk label sequences as a latent variable c : p( e | f ) = c p( c | f ) p( e | f , c ) ( 1 ) In practice , we only take the best chunk label sequence ? got by ? = argmax c p( c | f ) ( 2 ) Then we can ignore the conditional probability p(? |f ) as it holds the same value for each translation , and get : p( e |f ) = p(?|f ) p( e| f , ? ) = p( e|f , ? ) ( 3 ) We formalize our model as a weighted SCFG .
In a SCFG , each rule ( usually called production in SCFGs ) has an aligned pair of right - hand sidesthe source side and the target side , just as follows : X ? ? , ? , ? where X is a nonterminal , ? and ? are both strings of terminals and nonterminals , and ? denotes one- toone links between nonterminal occurrences in ? and nonterminal occurrences in ?.
A SCFG produces a derivation by starting with a pair of start symbols and recursively rewrites every two coindexed nonterminals with the corresponding components of a matched rule .
A derivation yields a pair of strings on the right - hand side which are translation of each other .
In a weighted SCFG , each rule has a weight and the total weight of a derivation is the production of the weights of the rules used by the derivation .
A translation may be produced by many different derivations and we only use the best derivation to evaluate its probability .
With d denoting a derivation and r denoting a rule , we have p( e |f ) = max d p(d , e|f , ? ) = max d r?d p( r , e|f , ? ) ( 4 ) Following Och and Ney ( 2002 ) , we frame our model as a log-linear model : p( e|f ) = exp k ? k H k ( d , e , ? , f ) exp d ?
,e ? , k ? k H k ( d ? , e ? , ? , f ) ( 5 ) where H k ( d , e , ? , f ) = r h k ( f , ? , r)
So the best translation is given by ?
= argmax e k ? k H k ( d , e , ? , f ) ( 6 )
We employ the same set of features for the loglinear model as the hierarchical phrase - based model does ( Chiang , 2005 ) .
We further refine our hierarchical chunk - to-string model into two models : a loose model which is more similar to the hierarchical phrase - based model and a tight model which is more similar to the tree-tostring model .
The two models differ in the form of rules and the way of estimating rule probabilities .
While for decoding , we employ the same decoding algorithm for the two models : given a test sentence , the decoders first perform shallow parsing to get the best chunk sequence , then apply a CYK parsing algorithm with beam search .
A Loose Model
In our model , we employ rules containing nonterminals to handle long-distance reordering where boundary words play an important role .
So for the subphrases which cover more than one chunk , we just maintain boundary chunks : we bundle adjacent chunks into one nonterminal and denote it as the first chunk tag immediately followed by " - " and next followed by the last chunk tag .
Then , for the string pair < filed for bankruptcy , shenqing pochan > , we can get the rule r 1 : X ? VBN 1 for NP 2 , VBN 1 NP 2 while for the string pair < A request for a purchase of shares , goumai gufen de shenqing > , we can get r 2 : X ? NP 1 for NP -NP 2 , NP -NP 2 de NP 1 . The rule matching " A request for a purchase of shares was " will be r 3 : X ? NP -NP 1 VBD 2 , NP -NP 1 VBD 2 . We can see that in contrast to the method of representing each chunk separately , this representation form can alleviate data sparseness and the influence of parsing errors .
S 1 , S 1 ? S 2 X 3 , S 2 X 3 ? X 4 X 3 , X 4 X 3 ? NP -NP 5 VBD 6 X 3 , NP -NP 5 VBD 6 X 3 ? NP 7 for NP -NP 8 VBD 6 X 3 , NP -NP 8 de NP 7 VBD 6 X 3 ? A request for NP -NP 8 VBD 6 X 3 , NP -NP 8 de shenqing VBD 6 X 3 ?
A request for a purchase of shares VBD 6 X 3 , goumai gufen de shenqing VBD 6 X 3 ?
A request for a purchase of shares was X 3 , goumai gufen de shenqing bei X 3 ?
A request for a purchase of shares was made , goumai gufen de shenqing bei dijiao
In these rules , the left - hand nonterminal symbol X can not match any nonterminal symbol on the righthand side .
So we need a set of rules such as NP ? X 1 , X 1 NP -NP ?
X 1 , X 1 and so on , and set the probabilities of these rules to 1 .
To simplify the derivation , we discard this kind of rules and assume that X can match any nonterminal on the right - hand side .
Only with r 2 and r 3 , we cannot produce any derivation of the whole sentence in Figure 1 ( a ) .
In this case we need two special glue rules : r 4 : S ? S 1 X 2 , S 1 X 2 r 5 : S ? X 1 , X 1 Together with the following four lexical rules , r 6 : X ? a request , shenqing r 7 : X ?
a purchase of shares , goumai gufen r 8 : X ? was , bei r 9 : X ? made , dijiao Figure 3 ( a ) shows the derivation of the sentence in Figure 1 ( a ) .
A Tight Model
In the tight model , the right - hand side of each rule remains the same as the loose model , but the lefthand side nonterminal is not X but the corresponding chunk labels .
If a rule covers more than one chunk , we just use the first and the last chunk labels to denote the left - hand side nonterminal .
The rule set used in the tight model for the example in Figure 1 ( a ) corresponding to that in the loose model becomes : r 2 : NP -NP ? NP 1 for NP -NP 2 , NP -NP 2 de NP 1 r 3 : NP-VBD ? NP -NP 1 VBD 2 , NP -NP 1 VBD 2 . r 6 : NP ? a request , shenqing r 7 : NP - NP ?
a purchase of shares , goumai gufen r 8 : VBD ? was , bei r 9 : VP ? made , dijiao During decoding , we first collect rules for each span .
For a span which does not have any matching rule , if we do not construct default rules for it , there will be no derivation for the whole sentence , then we need to construct default rules for this kind of span by enumerating all possible binary segmentation of the chunks in this span .
For the example in Figure 1 ( a ) , there is no rule matching the whole sentence , so we need to construct default rules for it , which should be NP -VP ? NP -VBD 1 VP 2 , NP-VBD 1 VP 2 . NP-VP ? NP -NP 1 VBD-VP 2 , NP -NP 1 VBD -VP 2 . and so on .
Figure 3 ( b ) shows the derivation of the sentence in Figure 1 ( a ) .
Shallow Parsing
In a parse tree , a chunk is defined by a leaf node or an inner node whose children are all leaf nodes ( See Figure 2 ( a ) ) .
In our model , we identify chunks by traversing a parse tree in a breadth - first order .
Once a node is recognized as a chunk , we skip its children .
In this way , we can get a sole chunk sequence given a parse tree .
Then we label each word with a label indicating whether the word starts a chunk ( B - ) or continues a chunk ( I - ) .
Figure 2 ( a ) gives an example .
In this method , we get the training data for shallow parsing from Penn Tree Bank .
We take shallow Parsing ( chunking ) as a sequence label task and employ Conditional Random Field ( CRF ) 1 to train a chunker .
CRF is a good choice for label tasks as it can avoid label bias and use more statistical correlated features .
We employ the features described in Sha and Pereira ( 2003 ) for CRF .
We do not introduce CRF - based chunkier in this paper and more details can be got from Hammersley and Clifford ( 1971 ) , Lafferty et al. ( 2001 ) , Taskar et al. ( 2002 ) , Sha and Pereira ( 2003 ) .
Rule Extraction
In what follows , we introduce how to get the rule set .
We learn rules from a corpus that first is bidirectionally word-aligned by the GIZA ++ toolkit ( Och and Ney , 2000 ) and then is refined using a " final - and " strategy .
We generate the rule set in two steps : first , we extract two sets of phrases , basic phrases and chunk - based phrases .
Basic phrases are defined using the same heuristic as previous systems ( Koehn et al. , 2003 ; Och and Ney , 2004 ; Chiang , 2005 ) .
A chunk - based phrase is such a basic phrase that covers one or more chunks on the source side .
1 We use the open source toolkit CRF ++ got in http://code.google.com/p/crfpp/ .
We identity chunk - based phrases c j 2 j 1 , f j 2 j 1 , e i 2 i 1 as follows : 1 . A chunk - based phrase is a basic phrase ; 2 . c j 1 begins with " B - " ; 3 . f j 2 is the end word on the source side or c j 2 + 1 does not begins with " I - " .
Given a sentence pair f , e , ? , we extract rules for the loose model as follows 1 . If f j 2 j 1 , e i 2 i 1 is a basic phrase , then we can have a rule X ? f j 2 j 1 , e i 2 i 1 2 . Assume X ? ? , ? is a rule with ? = ?
1 f j 2 j 1 ? 2 and ? = ?
1 e i 2 i 1 ? 2 , and f j 2 j 1 , e i 2 i 1 is a chunk - based phrase with a chunk sequence Y u ? ? ?
Y v , then we have the following rule X ? ?
1 Y u - Y v k ? 2 , ? 1 Y u - Y v k ? 2 . We evaluate the distribution of these rules in the same way as Chiang ( 2007 ) .
We extract rules for the tight model as follows 1 . If f j 2 j 1 , e i 2 i 1 is a chunk - based phrase with a chunk sequence Y s ? ? ?
Y t , then we can have a rule Y s - Y t ? f j 2 j 1 , e i 2 i 1 2 . Assume Y s - Y t ? ? , ? is a rule with ? = ?
1 f j 2 j 1 ? 2 and ? = ?
1 e i 2 i 1 ? 2 , and f j 2 j 1 , e i 2 i 1 is a chunk - based phrase with a chunk sequence Y u ? ? ?
Y v , then we have the following rule Y s - Y t ? ?
1 Y u - Y v k ? 2 , ? 1 Y u - Y v k ? 2 . We evaluate the distribution of rules in the same way as Liu et al . ( 2006 ) .
For the loose model , the nonterminals must be cohesive , while the whole rule can be noncohesive : if both ends of a rule are nonterminals , the whole rule is cohesive , otherwise , it may be noncohesive .
In contrast , for the tight model , both the whole rule and the nonterminal are cohesive .
Even with the cohesion constraints , our model still generates a large number of rules , but not all of the rules are useful for translation .
So we follow the method described in Chiang ( 2007 ) to filter the rule set except that we allow two nonterminals to be adjacent .
2008 ) presented a tree-to-string phrase - based method which is based on SCFGs .
This method generates SCFGs through syntactic transformation including a word- to- phrase tree transformation model and a phrase reordering model while our model learns SCFG - based rules from word-aligned bilingual corpus directly
There are also some works aiming to introduce linguistic knowledge into the hierarchical phrasebased model .
Marton and Resnik ( 2008 ) took the source parse tree into account and added soft constraints to hierarchical phrase - based model .
Cherry ( 2008 ) used dependency tree to add syntactic cohesion .
These methods work with the original SCFG defined by hierarchical phrase - based model and use linguistic knowledge to assist translation .
Instead , our model works under the new defined SCFG with chunks .
Related Works Besides , some other researchers make efforts on the tree-to-string model by employing exponentially alternative parses to alleviate the drawback of 1 - best parse .
Mi et al. ( 2008 ) presented forest - based translation where the decoder translates a packed forest of exponentially many parses instead of i-best parse .
proposed to parse and to translate jointly by taking tree - based translation as parsing .
Given a source sentence , this decoder produces a parse tree on the source side and a translation on the target side simultaneously .
Both the models perform in the unit of tree nodes rather than chunks .
Experiments
Data Preparation Data for shallow parsing
We got training data and test data for shallow parsing from the standard Penn Tree Bank ( PTB ) English parsing task by splitting the sections 02 - 21 on the Wall Street Journal Portion ( Marcus et al. , 1993 ) into two sets : the last 1000 sentences as the test set and the rest as the training set .
We filtered the features whose frequency was lower than 3 and substituted ' ' and ' ' with ? to keep consistent with translation data .
We used L2 algorithm to train CRF .
Data for Translation
We used the NIST training set for Chinese-English translation tasks excluding the Hong Kong Law and Hong Kong Hansard 2 as the training data , which contains 470 K sentence pairs .
For the training data set , we first performed word alignment in both directions using GIZA ++ toolkit ( Och and Ney , 2000 ) then refined the alignments using " final- and " .
We trained a 5 - gram language model with modified Kneser - Ney smoothing on the Xinhua portion of LDC Chinese Gigaword corpus .
For the tree-to-string model , we parsed English sentences using Stanford parser and extracted rules using the GHKM algorithm ( Galley et al. , 2004 ) .
We used our in- house English - Chinese data set as the development set and used the 2008 NIST English - Chinese MT test set ( 1859 sentences ) as the test set .
Our evaluation metric was BLEU -4 ( Papineni et al. , 2002 ) based on characters ( as the target language is Chinese ) , which performed caseinsensitive matching of n-grams up to n = 4 and used the shortest reference for the brevity penalty .
We used the standard minimum error-rate training to tune the feature weights to maximize the BLEU score on the development set .
Shallow Parsing
The standard evaluation metrics for shallow parsing are precision P , recall R , and their harmonic mean F1 score , given by :
Table 1 summaries the results of shallow parsing .
For ' ' and ' ' were substituted with ? , the performance was slightly influenced .
P = number of The F1 score of all chunks is 91.25 % and the F1 score of One and NP , which in number account for about 90 % of chunks , is 90.65 % and 94.22 % respectively .
F score of NP chunking approaches 94.38 % given in Sha and Pereira ( 2003 ) .
Performance Comparison
We compared our loose decoder and tight decoder with our in-house hierarchical phrase - based decoder ( Chiang , 2007 ) and the tree-to-string decoder ( Liu et al. , 2006 ) .
We set the same configuration for all the decoders as follows : stack size = 30 , nbest size = 30 .
For the hierarchical chunk - based and phrase - based decoders , we set max rule length to 5 .
For the treeto-string decoder , we set the configuration of rule extraction as : the height up to 3 and the number of leaf nodes up to 5 .
We give the results in Table 2 .
From the results , we can see that both the loose and tight decoders outperform the baseline decoders and the improvement is significant using the sign-test of Collins et al . ( 2005 ) ( p < 0.01 ) .
Specifically , the loose model has a better performance while the tight model has a faster speed .
Compared with the hierarchical phrase - based model , the loose model only imposes syntactic cohesion cohesion to nonterminals while the tight model imposes syntax cohesion to both rules and nonterminals which reduces search space , so it decoders faster .
We can conclude that linguistic syntax can indeed improve the translation performance ; syntactic cohesion for nonterminals can explain linguistic phenomena well ; noncohesive rules are useful , too .
The extra time consumption against hierarchical phrase - based system comes from shallow parsing .
By investigating the translation result , we find that our decoder does well in rule selection .
For example , in the hierarchical phrase - based model , this kind of rules , such as X ?
X of X , * , X ?
X for X , * and so on , where * stands for the target component , are used with a loose restriction as long as the terminals are matched , while our models employ more stringent constraints on these rules by specifying the syntactic constituent of " X " .
With chunk labels , our models can make different treatment for different situations .
System
Dev NIST08 Speed cohesive 0.2936 0.4023 1.429 noncohesive 0.2937 0.3964 1.734
Compared with the tree-to-string model , the result indicates that the change of the source-side linguistic syntax from parses to chunks can improve translation performance .
The reasons should be our model can reduce parse errors and it is enough to use chunks as the basic unit for machine translation .
Although our decoders and tree-to-string decoder all run in linear - time with beam search , tree- to-string model runs faster for it searches through a smaller SCFG - motivated space .
Influence of Cohesion
We verify the influence of syntax cohesion via the loose model .
The cohesive model imposes syntax cohesion on nonterminals to ensure the chunk is reordered as a whole .
In this experiment , we introduce a noncohesive model by allowing a nonterminal to match part of a chunk .
For example , in the noncohesive model , it is legal for a rule with the source side " NP for NP - NP " to match " request for a purchase of shares " in Figure 1 ( a ) , where " request " is part of NP .
As well , the rule with the source side " NP for a NP - NP " can match " request for a purchase of shares " .
In this way , we can ensure all the rules used in the cohesive system can be used in the noncohesive system .
Besides cohesive rules , the noncohesive system can use noncohesive rules , too .
We give the results in Table 3 .
From the results , we can see that cohesion helps to reduce search space , so the cohesive system decodes faster .
more rules , but this does not bring any improvement of translation performance .
As other researches said in their papers , syntax cohesion can explain linguistic phenomena well .
Influence of the number of nonterminals
We also tried to allow a rule to hold three nonterminals at most .
We give the result in Table 4 .
The result shows that using three nonterminals does not bring a significant improvement of translation performance but quite more time consumption .
So we only retain two nonterminals at most in a rule .
Conclusion
In this paper , we present a hierarchical chunkto-string model for statistical machine translation which can be seen as a compromise of the hierarchical phrase - based model and the tree-to-string model .
With the help of shallow parsing , our model learns rules consisting of either words or chunks and compresses adjacent chunks in a rule to a nonterminal , then it searches for the best derivation under the SCFG defined by these rules .
Our model can combine the merits of both the models : employing linguistic syntax to direct decoding , being syntax cohesive and robust to parsing errors .
We refine the hierarchical chunk - to-string model into two models : a loose model ( more similar to the hierarchical phrasebased model ) and a tight model ( more similar to the tree-to-string model ) .
Our experiments show that our decoder can improve translation performance significantly over the hierarchical phrase - based decoder and the tree-tostring decoder .
Besides , the loose model gives a better performance while the tight model gives a faster speed .
Figure 1 : Figure 2 : 12 Figure 1 : A running example of two sentences .
For each sentence , the first row gives the chunk sequence .
