title
Translating Phrases in Neural Machine Translation
abstract
Phrases play an important role in natural language understanding and machine translation ( Sag et al. , 2002 ; Villavicencio et al. , 2005 ) .
However , it is difficult to integrate them into current neural machine translation ( NMT ) which reads and generates sentences word by word .
In this work , we propose a method to translate phrases in NMT by integrating a phrase memory storing target phrases from a phrase - based statistical machine translation ( SMT ) system into the encoder-decoder architecture of NMT .
At each decoding step , the phrase memory is first re-written by the SMT model , which dynamically generates relevant target phrases with contextual information provided by the NMT model .
Then the proposed model reads the phrase memory to make probability estimations for all phrases in the phrase memory .
If phrase generation is carried on , the NMT decoder selects an appropriate phrase from the memory to perform phrase translation and updates its decoding state by consuming the words in the selected phrase .
Otherwise , the NMT decoder generates a word from the vocabulary as the general NMT decoder does .
Experiment results on the Chinese ?
English translation show that the proposed model achieves significant improvements over the baseline on various test sets .
Introduction Neural machine translation ( NMT ) has been receiving increasing attention due to its impressive * Corresponding author translation performance ( Kalchbrenner and Blunsom , 2013 ; Cho et al. , 2014 ; Sutskever et al. , 2014 ; Bahdanau et al. , 2015 ; . Significantly different from conventional statistical machine translation ( SMT ) ( Brown et al. , 1993 ; Koehn et al. , 2003 ; Chiang , 2005 ) , NMT adopts a big neural network to perform the entire translation process in one shot , for which an encoderdecoder architecture is widely used .
Specifically , the encoder encodes a source sentence into a continuous vector representation , then the decoder uses the continuous vector representation to generate the corresponding target translation word by word .
The word- by- word generation philosophy in NMT makes it difficult to translate multi-word phrases .
Phrases , especially multi-word expressions , are crucial for natural language understanding and machine translation ( Sag et al. , 2002 ; Villavicencio et al. , 2005 ) as the meaning of a phrase cannot be always deducible from the meanings of its individual words or parts .
Unfortunately current NMT is essentially a word- based or character - based ( Chung et al. , 2016 ; Costa-juss ?
and Fonollosa , 2016 ; Luong and Manning , 2016 ) translation system where phrases are not considered as translation units .
In contrast , phrases are much better than words as translation units in SMT and have made a significant advance in translation quality .
Therefore , a natural question arises : Can we translate phrases in NMT ?
Recently , there have been some attempts on multi-word phrase generation in NMT ( Stahlberg et al. , 2016 b ; Zhang and Zong , 2016 ) .
However these efforts constrain NMT to generate either syntactic phrases or domain phrases in the wordby - word generation framework .
To explore the phrase generation in NMT beyond the word- byword generation framework , we propose a novel architecture that integrates a phrase - based SMT model into NMT .
Specifically , we add an auxiliary phrase memory to store target phrases in symbolic form .
At each decoding step , guided by the decoding information from the NMT decoder , the SMT model dynamically generates relevant target phrase translations and writes them to the memory .
Then the NMT decoder scores phrases in the phrase memory and selects a proper phrase or word with the highest probability .
If the phrase generation is carried out , the NMT decoder generates a multi-word phrase and updates its decoding state by consuming the words in the selected phrase .
Furthermore , in order to enhance the ability of the NMT decoder to effectively select appropriate target phrases , we modify the encoder of NMT to make it fit for exploring structural information of source sentences .
Particularly , we integrate syntactic chunk information into the NMT encoder , to enrich the source-side representation .
We validate our proposed model on the Chinese ?
English translation task .
Experiment results show that the proposed model significantly outperforms the conventional attention - based NMT by 1.07 BLEU points on multiple NIST test sets .
The rest of this paper is organized as follows .
Section 2 briefly introduces the attentionbased NMT as background knowledge .
Section 3 presents our proposed model which incorporates the phrase memory into the NMT encoder-decoder architecture , as well as the reading and writing procedures of the phrase memory .
Section 4 presents our experiments on the Chinese ?
English translation task and reports the experiment results .
Finally we discuss related work in Section 5 and conclude the paper in Section 6 .
Background Neural machine translation often adopts the encoder-decoder architecture with recurrent neural networks ( RNN ) to model the translation process .
The bidirectional RNN encoder which consists of a forward RNN and a backward RNN reads a source sentence x = x 1 , x 2 , ... , x
Tx and transforms it into word annotations of the entire source sentence h = h 1 , h 2 , ... , h Tx .
The decoder uses the annotations to emit a target sentence y = y 1 , y 2 , ... , y
Ty in a word- by - word manner .
In the training phase , given a parallel sentence ( x , y ) , NMT models the conditional probability as follows , P ( y|x ) = Ty i=1 P ( y i |y < i , x ) ( 1 ) where y i is the target word emitted by the decoder at step i and y < i = y 1 , y 2 , ... , y i?1 .
The conditional probability P ( y i |y < i , x ) is computed as P ( y i |y < i , x ) = sof tmax ( f ( s i , y i?1 , c i ) ) ( 2 ) where f ( ? ) is a non-linear function and s i is the hidden state of the decoder at step i : s i = g( s i?1 , y i?1 , c i ) ( 3 ) where g( ? ) is a non-linear function .
Here we adopt Gated Recurrent Unit ( Cho et al. , 2014 ) as the recurrent unit for the encoder and decoder .
c i is the context vector , computed as a weighted sum of the annotations h : c i = Tx j=1 ? t , j h j ( 4 ) where h j is the annotation of source word x j and its weight ?
t , j is computed by the attention model .
We train the attention - based NMT model by maximizing the log-likelihood : C ( ? ) = N n=1 Ty i=1 log P ( y n i |y n < i , x n ) ( 5 ) given the training data with N bilingual sentences ( Cho , 2015 ) .
In the testing phase , given a source sentence x , we use beam search strategy to search a target sentence ? that approximately maximizes the conditional probability P ( y|x ) ? = argmax y P ( y|x ) ( 6 ) 3 Approach
In this section , we introduce the proposed model which incorporates a phrase memory into the encoder-decoder architecture of NMT .
Inspired by the recent work on attaching an external structure to the encoder-decoder architecture ( Gulcehre et al. , 2016 ; Gu et al. , 2016 ; Tang et al. , 2016 ; Wang et al. , 2017 ) , we adopt a similar approach to incorporate the phrase memory into NMT .
Figure 1 : Architecture of the NMT decoder with the phrase memory .
The NMT decoder performs phrase generation using the balancer and the phrase memory .
Framework Figure 1 shows an example .
Given the generated words " President Bush emphasized that " , the model generates the next fragment either from a word generation mode or a phrase generation mode .
If the model selects the word generation mode , it generates a word by the NMT decoder as in the standard NMT framework .
Otherwise , it generates a multi-word phrase by enquiring a phrase memory , which is written by an SMT decoder based on the dynamic decoding information from the NMT model for each step .
The trade- off between word generation mode and phrase generation mode is balanced by a weight ? , which is produced by a neural network based balancer .
Formally , a generated translation y = {y 1 , y 2 , . . . , y
Ty } consists of two sets of fragments : words generated by NMT decoder w = {w 1 , w 2 , . . . , w K } and phrases generated from the phrase memory p = {p 1 , p 2 , . . . , p L } .
The probability of generating y is calculated by P ( y|x ) = w k ?w ( 1 ? ? t( w k ) ) P word ( w k ) ? p l ?p ? t( p l ) P phrase ( p l ) ( 7 ) where P word ( w k ) is the probability of generating the word w k ( see Equation 2 ) , P phrase ( p l ) is that of generating the phrase p l which will be described in Section 3.2 , and t ( ? ) is the decoding step to generate the corresponding fragment .
The balancing weight ? is produced by the balancer - a multi-layer network .
The balancer network takes as input the decoding information , including the context vector c i , the previous decoding state s i?1 and the previous generated word y i?1 : ? i = ?( f b ( s i , y i?1 , c i ) ) ( 8 ) where ?(? ) is a sigmoid function and f b ( ? ) is the activation function .
Intuitively , the weight ? can be treated as the estimated importance of the phrase to be generated .
We expect ? to be high if the phrase is appropriate at the current decoding step .
Well - Formed Phrases
We employ a sourceside chunker to chunk the source sentence , and only phrases that corresponds to a source chunk are used in our model .
We restrict ourselves to the well - formed chunk phrases based on the following considerations : ( 1 ) In order to take advantage of dynamic programming , we restrict ourselves to non-overlap phrases .
1 ( 2 ) We explicitly utilize the boundary information of the source-side chunk phrases , to better guide the proposed model to adopt a target phrase at an appropriate decoding step .
( 3 ) We enable the model to exploit the syntactic categories of chunk phrases to enhance the proposed model with its selection preference for special target phrases .
With these information , we enrich the context vector c i to enable the proposed model to make better decisions , as described below .
Following the commonly - used strategy in sequence tagging tasks ( Xue and Shen , 2003 ) , we allow the words in a phrase to share the same chunk tag and introduce a special tag for the beginning word .
For example , the phrase " &E S ( information security ) " is tagged as a noun phrase " NP " , and the tag sequence should be " NP B NP " .
Partially motivated by the work on integrating linguistic features into NMT , we represent the encoder input as the combination of word embeddings and chunking tag embeddings , instead of word embeddings alone in the conventional NMT .
The new input is formulated as follows : [ E w x i , E t t i ] ( 9 ) where E w ?
R dw ?|V N M T | is a word embedding matrix and dw is the word embedding dimensionality , E t ?
R dt ?|V T AG | is a tag embedding matrix and dt is the tag embedding dimensionality . [ ? ] is the vector concatenation operation .
Phrase Memory
The phrase memory stores relevant target phrases provided by an SMT model , which is trained on the same bilingual corpora .
At each decoding step , the memory is firstly erased and re-written by the SMT model , the decoding of which is based on the translation information provided by the NMT model .
Then , the proposed model enquires phrases along with their probabilities P phrase from the memory .
Writing to Phrase Memory Given a partial translation y < i = {y 1 , y 2 , . . . , y t?1 } generated from NMT , the SMT model picks potential phrases extracted from the translation table .
The phrases are scored with multiple SMT features , including the language model score , the translation probabilities , the reordering score , and so on .
Specially , the reordering score depends on alignment information between source and target words , which is derived from attention distribution produced by the NMT model ( Wang et al. , 2017 ) . SMT coverage vector in ( Wang et al. , 2017 ) is also introduced to avoid repeat phrasal recommendations .
In our work , the potential phrase is phrase with high SMT score which is defined as following : SM T score ( p l |y <t , x ) = M m=1 w m h m ( p l , x( p l ) ) ( 10 ) where p l is a target phrase and x( p l ) is its corresponding source span .
h m ( p l , x( p l ) ) is a SMT feature function and w m is its weight .
The feature weights can be tuned by the minimum error rate training ( MERT ) algorithm ( Och , 2003 ) .
This leads to a better interaction between SMT and NMT models .
It should be emphasized that our memory is dynamically updated at each decoding step based on the decoding history from both SMT and NMT models .
The proposed model is very flexible , where the phrase memory can be either fully dynamically generated by an SMT model or directly extracted from a bilingual dictionary , or any other bilingual resources storing idiomatic translations or bilin-gual multi-word expressions , which may lead to a further improvement .
2 Reading Phrase Memory
When phrases are read from the memory , they are rescored by a neural network based score function .
The score function takes as input the phrase itself and decoding information from NMT ( i = t( p l ) denotes the current decoding step ) : score phrase ( p l ) = g s e( p l ) , s i , y i?1 , c i ( 11 ) where g s ( ? ) is either an identity or a non-linear function .
e( p l ) is the representation of phrase p l , which is modeled by a recurrent neural networks .
Again , s i is the decoder state , y i?1 is the lastly generated word , and c i is the context vector .
The scores are normalized for all phrases in the phrase memory , and the probability for phrase p l is calculated as P phrase ( p l ) = sof tmax ( score phrase ( p l ) ) ( 12 )
The probability calculation is controlled with parameters , which are trained together with the parameters from the NMT model .
Training Formally , we train both the default parameters of standard NMT and the new parameters associated with phrase generation on a set of training examples { [ x n , y n ] } N n=1 : C ( ? ) = N n=1 log P ( y n |x n ) ( 13 ) where P ( y n |x n ) is defined in Equation 7 . Ideally , the trained model is expected to produce a higher balance weight ? and phrase probability P phrase when a phrase is selected from the memory , and lower scores in other cases .
Decoding During testing , the NMT decoder generates a target sentence which consists of a mixture of words and phrases .
Due to the different granularities of words and phrases , we design a variant of beam search strategy :
At decoding step i , we first compute P phrase for all phrases in the phrase memory and P word for all words in NMT vocabulary .
Then the balancer outputs a balancing weight ?
i , which is used to scale the phrase and word probabilities : ? i ?
P phrase and ( 1 ? ? i ) ?
P word .
Now outputs are normalized probabilities on the concatenation of phrase memory and the general NMT vocabulary .
At last , the NMT decoder generates a proper phrase or word of the highest probability .
If a target phrase in the phrase memory has the highest probability , the decoder generates the target phrase to complete the multi-word phrase generation process , and updates its decoding state by consuming the words in the selected phrase as described in Equation 3 .
All translation hypotheses are placed in the corresponding beams according to the number of generated target words .
Experiments
In this section , we evaluated the effectiveness of our model on the Chinese ?
English machine translation task .
The training corpora consisted of about 1.25 million sentence pairs 3 with 27.9 million Chinese words and 34.5 million English words respectively .
We used NIST 2006 ( NIST06 ) dataset as development set , and NIST 2004 ( NIST04 ) , 2005 ( NIST05 ) and 2008 ( NIST08 ) datasets as test sets .
We report experiment results with case-insensitive BLEU score 4 .
We compared our proposed model with two state - of- the - art systems : * Moses : a state - of- the - art phrase - based SMT system ( Koehn et al. , 2007 ) with its default settings , where feature function weights are tuned by the minimum error rate training ( MERT ) algorithm ( Och , 2003 ) . * RNNSearch : an in-house implementation of the attention - based NMT system ( Bahdanau et al. , 2015 ) with its default settings .
For Moses , we used the full bilingual training data to train the phrase - based SMT model and the target portion of the bilingual training data to train a 4 - gram language model using KenLM 5 . We ran Giza ++ on the training data in both Chinese-to- English and English - to - Chinese directions and applied the " grow-diag-final " refinement rule ( Koehn et al. , 2003 ) to obtain word alignments .
The maximum phrase length is set to 7 .
For RNNSearch , we generally followed settings in the previous work ( Bahdanau et al. , 2015 ; Tu et al. , 2017 a , b) .
We only kept a shortlist of the most frequent 30,000 words in Chinese and English , covering approximately 97.7 % and 99.3 % of the data in the two languages respectively .
We constrained our source and target sequences to have a maximum length of 50 words in the training data .
The size of embedding layer of both sides was set to 620 and the size of hidden layer was set to 1000 .
We used a minibatch stochastic gradient descent ( SGD ) algorithm of size 80 together with Adadelta ( Zeiler , 2012 ) to train the NMT models .
The decay rates ? and were set as 0.95 and 10 ?6 .
We clipped the gradient norm to 1.0 ( Pascanu et al. , 2013 ) .
We also adopted the dropout technique .
Dropout was applied only on the output layer and the dropout rate was set to 0.5 .
We used a simple beam search decoder with beam size 10 to find the most likely translation .
For the proposed model , we used a Chinese chunker 6 ( Zhu et al. , 2015 ) to chunk the sourceside Chinese sentences .
13 chunking tags appeared in our chunked sentences and the size of chunking tag embedding was set to 10 .
We used the trained phrase - based SMT to translate the source-side chunks .
The top 5 translations according to their translation scores ( Equation 10 ) were kept and among them multi-word phrases were used as phrasal recommendations for each source chunk phrase .
For a source-side chunk phrase , if there exists phrasal recommendations from SMT , the output chunk tag was used as its chunking tag feature as described in Section 3.1 .
Otherwise , the words in the chunk were treated as general words by being tagged with the default tag .
In the phrase memory , we only keep the top 7 target translations with highest SMT scores at each decoding step .
We used a forward neural network with two hidden layers for both the balancer ( Equation 8 ) and the scoring function ( Equation 11 ) .
The numbers of units in the hidden layers were set to 2000 and 500 respectively .
We used a backward RNN encoder to learn the phrase representations of target phrases in the phrase memory .
Number of Sentences Affected by Generated Phrases
We also check the number of translations that contain phrases generated by the proposed model , as shown in Table 2 .
As seen , a large portion of translations take the recommended phrases , and the number increases when the chunking tag feature is used .
7 Considering BLEU scores reported in Table 1 , we believe that the chunking tag feature benefits the proposed model on its phrase generation .
Analysis on Generated Phrases Syntactic Categories of Generated Phrases
We first investigate which category of phrases is more likely to be selected by the proposed approach .
There are some phrases , such as Table 3 : Percentages of phrase categories to the total number of generated ones .
" All " denotes all generated phrases , and " New " means new phrases that cannot be found in translations generated by the baseline system .
" Total " is the total number of generated phrases and " Correct " denotes the fully correct ones .
noun phrases ( NPs , e.g. , " national laboratory " and " vietnam airlines " ) and quantifier phrases ( QPs , e.g. , " 15 seconds " and " two weeks " ) , that we expect to be favored by our approach .
Statistics shown in Table 3 confirm our hypothesis .
Let 's first concern all generated phrases ( i.e. , column " All " ) : most selected phrases are noun phrases ( 81.0 % ) and quantifier phrases ( 10.8 % ) .
Among them , 44.5 % percent of them are fully correct 8 . Specifically , NPs have relative higher generation accuracy ( i.e. , 47.8 % = 38.7%/81.0 % ) while VPs have lower accuracy ( i.e. , 21.2 % = 1.7% /8.0 % ) .
By looking into the wrong cases , we found most errors are related to verb tense , which is the drawback of SMT models .
Concerning the newly introduced phrases that cannot be found in baseline translations ( i.e. , column " New " ) , 13.2 % of generated phrases are both new and fully correct , which contribute most to the performance improvement .
We can also find that most newly introduced verb phrases and quantifier phrases are not correct , the patterns of which can be well learned by word - based NMT models .
5 : Additional experiment results on the translation task to directly measure the improvement obtained by the phrase generation .
" + NULL " denotes that we replace the generated target phrases with a special symbol / NULL0 in test sets .
BLEU scores in the table are case insensitive .
Effect of Generated Phrases on Translation Performance
Note that the proposed model benefits not only from fully matched phrases , but also from partially matched phrases .
For example , the baseline system translates " I [ ? ?o? " in a word- by - word manner and outputs " state aviation and space department " .
The generated phrase provided by SMT is " national aviation and space administration " , but the only correct reference is " national aeronautics and space administration " .
The generated phrase is not fully correct but still useful .
To directly measure the improvement obtained by the phrase generation , we replace the generated target phrases with a special symbol " NULL " in test sets .
As shown in Table 5 , when deleting the generated target phrases , ( " + memory + chunking tag " ) and ( " + memory " ) translation performances decrease by 2.74 BLEU points and 1.32 BLEU points respectively .
Moreover , translation performances on NIST08 decrease less than those on NIST04 and NIST05 in both settings .
The reason is that NIST08 which contains sentences from web data has little influence on generating target phrases which are provided from a different domain 9 .
The overall results demonstrate that neural machine translation benefits from phrase translation .
Effect of Balancer Weight Test Dynamic 33.55 Constant ( ? = 0.1 ) 31.35 Table 6 : Translation performance with a variety of balancing weight strategies .
" Dynamic " is the proposed approach and " Constant ( ? = 0.1 ) " denotes fixing the balancing weight to 0.1 .
BLEU scores in the table are case insensitive .
The balancer which is used to coordinate the phrase generation and word generation is very crucial for the proposed model .
We conducted an additional experiment to validate the effectiveness of the neural network based balancer .
We use the setting " + memory + chunking tag " as baseline system to conduct the experiments .
In this experiment , we fixed the balancing weight ?
( Equation 8 ) to 0.1 during training and testing and report the results .
As shown in Table 6 , we find that using the fixed value for the balancing weight ( Constant ( ? = 0.1 ) ) decreases the translation performance sharply .
This demonstrates that the neural network based balancer is an essential component for the proposed model .
Comparison to Word-Level Recommendations and Discussions
Our approach is related to our previous work ( Wang et al. , 2017 ) which integrates the SMT word- level knowledge into NMT .
To make a comparison , we conducted experiments followed settings in ( Wang et al. , 2017 ) .
The comparison results are reported in Table 7 .
We find that our approach is marginally better than the word- level SYSTEM Test + word level recommendation 33.27 + memory + chunking tag 33.55 Table 7 : Experiment results on the translation task .
" + word level recommendation " is the proposed model in ( Wang et al. , 2017 ) .
BLEU scores in the table are case insensitive .
model proposed in ( Wang et al. , 2017 ) by 0.28 BLEU points .
In our approach , the SMT model translates source-side chunk phrases using the NMT decoding information .
Although we use high-quality target phrases as phrasal recommendations , our approach still suffers from the errors in segmentation and chunking .
For example , the target phrase " laptop computers " cannot be recommended by the SMT model if the Chinese phrase " ?
J > M " is not chunked as a phrase unit .
This is the reason why some sentences do not have corresponding phrasal recommendations ( Table 2 ) .
Therefore , our approach can be further enhanced if we can reduce the error propagations from the segmenter or chunker , for example , by using n-best chunk sequences instead of the single best chunk sequence .
Additionally , we also observe that some target phrasal recommendations have been also generated by the baseline system in a word- by - word manner .
These phrases , even taken as parts of final translations by the proposed model , do not lead to improvements in terms of BLEU as they have already occurred in translations from the baseline system .
For example , the proposed model successfully carries out the phrase generation mode to generate a target phrase " guangdong province " ( the translation of Chinese phrase " 2 ? ? " ) which has appeared in the baseline system .
As external resources , e.g. , bilingual dictionary , which are complementary to the SMT phrasal recommendations , are compatible with the proposed model , we believe that the proposed model will get further improvement by using external resources .
Related work
Our work is related to the following research topics on NMT : Generating phrases for NMT
In these studies , the generated NMT multi-word phrases are either from an SMT model or a bilingual dictio-nary .
In syntactically guided neural machine translation ( SGNMT ) , the NMT decoder uses phrase translations produced by the hierarchical phrasebased SMT system Hiero , as hard decoding constraints .
In this way , syntactic phrases are generated by the NMT decoder ( Stahlberg et al. , 2016 b ) .
Zhang and Zong ( 2016 ) use an SMT translation system , which is integrated an additional bilingual dictionary , to synthesize pseudo-parallel sentences and feed the sentences into the training of NMT in order to translate low-frequency words or phrases .
Tang et al. ( 2016 ) propose an external phrase memory that stores phrase pairs in symbolic forms for NMT .
During decoding , the NMT decoder enquires the phrase memory and properly generates phrase translations .
The significant differences between these efforts and ours are 1 ) that we dynamically generate phrase translations via an SMT model , and 2 ) that at the same time we modify the encoder to incorporate structural information to enhance the capability of NMT in phrase translation .
Incorporating linguistic information into NMT NMT is essentially a sequence to sequence mapping network that treats the input / output units , eg. , words , subwords , characters ( Chung et al. , 2016 ; Costa-juss ? and Fonollosa , 2016 ) , as non-linguistic symbols .
However , linguistic information can be viewed as the taskspecific knowledge , which may be a useful supplementary to the sequence to sequence mapping network .
To this end , various kinds of linguistic annotations have been introduced into NMT to improve its translation performance .
enrich the input units of NMT with various linguistic features , including lemmas , part- of-speech tags , syntactic dependency labels and morphological features .
Garc?a- Mart?nez et al. ( 2016 ) propose factored NMT using the morphological and grammatical decomposition of the words ( factors ) in output units .
Eriguchi et al. ( 2016 ) explore the phrase structures of input sentences and propose a tree-to-sequence attention model for the vanilla NMT model .
propose to linearize source-side parse trees to obtain structural label sequences and explicitly incorporated the structural sequences into NMT , while Aharoni and Goldberg ( 2017 ) propose to incorporate target -side syntactic information into NMT by serializing the target sequences into linearized , lexicalized constituency trees .
integrate topic knowledge into NMT for domain / topic adaptation .
Combining NMT and SMT
A variety of approaches have been explored for leveraging the advantages of both NMT and conventional SMT .
He et al. ( 2016 ) integrate SMT features with the NMT model under the log-linear framework in order to help NMT alleviate the limited vocabulary problem ( Luong et al. , 2015 ; Jean et al. , 2015 ) and coverage problem ( Tu et al. , 2016 ) .
Arthur et al. ( 2016 ) observe that NMT is prone to making mistakes in translating low-frequency content words and therefore attempt at incorporating discrete translation lexicons into the NMT model , to alliterate the imprecise translation problem ( Wang et al. , 2017 ) .
Motivated by the complementary strengths of syntactical SMT and NMT , different combination schemes of Hiero and NMT have been exploited to form SGNMT ( Stahlberg et al. , 2016 a , b) . Wang et al. ( 2017 ) propose an approach to incorporate the SMT model into attention - based NMT .
They combine NMT posteriors with SMT word recommendations through linear interpolation implemented by a gating function which dynamically assigns the weights .
Niehues et al. ( 2016 ) propose to use SMT to pre-translate the inputs into target translations and employ the target pre-translations as input sequences in NMT .
propose a neural system combination framework to directly combine NMT and SMT outputs .
The combination of NMT and SMT has been also introduced in interactive machine translation to improve the system 's suggestion quality ( Wuebker et al. , 2016 ) .
In addition , word alignments from the traditional SMT pipeline are also used to improve the attention mechanism in NMT ( Cohn et al. , 2016 ; Mi et al .
, 2016 ; .
Conclusion
In this paper , we have presented a novel model to translate source phrases and generate target phrase translations in NMT by integrating the phrase memory into the encoder-decoder architecture .
At decoding , the SMT model dynamically generates relevant target phrases with contextual information provided by the NMT model and writes them to the phrase memory .
Then the proposed model reads the phrase memory and uses the balancer to make probability estimations for the phrases in the phrase memory .
Finally the NMT decoder selects a phrase from the phrase memory or a word from the vocabulary of the highest probability to generate .
Experiment results on Chinese ?
English translation have demonstrated that the proposed model can significantly improve the translation performance .
Table 1 : 1 Main experiment results on the NIST Chinese -English translation task .
BLEU scores in the table are case insensitive .
Moses and RNNSearch are SMT and NMT baseline system respectively . " ? " : significantly better than RNNSearch ( p < 0.05 ) ; " ? " : significantly better than RNNSearch ( p < 0.01 ) .
SYSTEM NIST04 NIST05 NIST08 Avg Moses 34.74 31.99 23.69 30.14 RNNSearch 37.80 34.70 24.93 32.48 + memory 38.21 35.15 25.48 ? 32.95 + memory + chunking tag 38.83 ? 35.72 ? 26.09 ? 33.55 NIST04 NIST05 NIST08 + memory 34.3 % 29.4 % 22.2 % + chunking tag 66.4 % 63.1 % 58.4 % Table 2 : Percentages of sentences that contain phrases generated by the proposed model .
4.1 Main Results
Table 1 reports main results of different models measured in terms of BLEU score .
We observe that our implementation of RNNSearch outper - forms Moses by 2.34 BLEU points .
( + memory ) which is the proposed model with the phrase mem- ory obtains an improvement of 0.47 BLEU points over the baseline RNNSearch .
With the source - side chunking tag feature , ( + memory + chunking tag ) outperforms the baseline RNNSearch by 1.07 BLEU points , showing the effectiveness of chunk - ing syntactic categories on the selection of ap- propriate target phrases .
From here on , we use " + memory + chunking tag " as the default setting in the following experiments if not otherwise stated .
