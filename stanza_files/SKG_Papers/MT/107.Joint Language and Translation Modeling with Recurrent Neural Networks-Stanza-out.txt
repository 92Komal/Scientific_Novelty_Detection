title
Joint Language and Translation Modeling with Recurrent Neural Networks
abstract
We present a joint language and translation model based on a recurrent neural network which predicts target words based on an unbounded history of both source and target words .
The weaker independence assumptions of this model result in a vastly larger search space compared to related feedforward - based language or translation models .
We tackle this issue with a new lattice rescoring algorithm and demonstrate its effectiveness empirically .
Our joint model builds on a well known recurrent neural network language model ( Mikolov , 2012 ) augmented by a layer of additional inputs from the source language .
We show competitive accuracy compared to the traditional channel model features .
Our best results improve the output of a system trained on WMT 2012 French - English data by up to 1.5 BLEU , and by 1.1 BLEU on average across several test sets .
Introduction Recently , several feed -forward neural networkbased language and translation models have achieved impressive accuracy improvements on statistical machine translation tasks ( Allauzen et al. , 2011 ; Le et al. , 2012 b ; Schwenk et al. , 2012 ) .
In this paper we focus on recurrent neural network architectures , which have recently advanced the state of the art in language modeling ( Mikolov et al. , 2010 ; Mikolov , 2012 ) , outperforming multi-layer feed -forward based networks in both perplexity and word error rate in speech recognition ( Arisoy et al. , 2012 ; Sundermeyer et al. , 2013 ) .
The major attraction of recurrent architectures is their potential to capture long-span dependencies since predictions are based on an unbounded history of previous words .
This is in contrast to feed -forward networks as well as conventional n-gram models , both of which are limited to fixed - length contexts .
Building on the success of recurrent architectures , we base our joint language and translation model on an extension of the recurrent neural network language model ( Mikolov and Zweig , 2012 ) that introduces a layer of additional inputs ( ?2 ) .
Most previous work on neural networks for speech recognition or machine translation used a rescoring setup based on n-best lists ( Arisoy et al. , 2012 ; Mikolov , 2012 ) for evaluation , thereby side stepping the algorithmic and engineering challenges of direct decoder-integration .
1 Instead , we exploit lattices , which offer a much richer representation of the decoder output , since they compactly encode an exponential number of translation hypotheses in polynomial space .
In contrast , n-best lists are typically very redundant , representing only a few combinations of top scoring arcs in the lattice .
A major challenge in lattice rescoring with a recurrent neural network model is the effect of the unbounded history on search since the usual dynamic programming assumptions which are exploited for efficiency do not hold up anymore .
We apply a novel algorithm to the task of rescoring with an unbounded language model and empirically demonstrate its effectiveness ( ?3 ) .
The algorithm proves robust , leading to significant improvements with the recurrent neural network language model over a competitive n-gram baseline across several language pairs .
We even observe consistent gains when pairing the model with a large n-gram model trained on up to 575 times more data , demonstrating that the model provides complementary information ( ?4 ) .
Our joint modeling approach is based on adding a continuous space representation of the foreign sentence as an additional input to the recurrent neural network language model .
With this extension , the language model can measure the consistency between the source and target words in a contextsensitive way .
The model effectively combines the functionality of both the traditional channel and language model features .
We test the power of this new model by using it as the only source of traditional channel information .
Overall , we find that the model achieves accuracy competitive with the older channel model features and that it can improve over the gains observed with the recurrent neural network language model ( ?5 ) .
Model Structure
We base our model on the recurrent neural network language model of Mikolov et al . ( 2010 ) which is factored into an input layer , a hidden layer with recurrent connections , and an output layer ( Figure 1 ) .
The input layer encodes the target language word at time t as a 1 - of - N vector e t , where | V | is the size of the vocabulary , and the output layer y t represents a probability distribution over target words ; both of size | V |.
The hidden layer state h t encodes the history of all words observed in the sequence up to time step t.
This model is extended by an auxiliary input layer f t which provides complementary information to the input layer ( Mikolov and Zweig , 2012 ) .
While the auxiliary input layer can be used to feed in arbitrary additional information , we focus on encodings of the foreign sentence ( ?5 ) .
The state of the hidden layer is determined by the input layer , the auxiliary input layer and the hidden layer configuration of the previous time step h t?1 .
The weights of the connections between the layers are summarized in a number of matrices : U , F and W , represent weights from the input layer to the hidden layer , from the auxiliary input layer to the hidden layer , and from the previous hidden layer to the current hidden layer , respectively .
Matrix
V represents connections between the current hidden layer and the output layer ; G represents direct weights between the auxiliary input and output layers .
The hidden and output layers are computed via a series of matrix-vector products and non-linearities : h t = s( Ue t + Wh t?1 + Ff t ) y t = g( Vh t + Gf t ) where s( z ) = 1 1 + exp {?z} , g( z m ) = exp {z m } k exp {z k } are sigmoid and softmax functions , respectively .
Additionally , the network is interpolated with a maximum entropy model of sparse n-gram features over input words .
2
The maximum entropy weights are added to the output activations before computing the softmax .
The model is optimized via a maximum likelihood objective function using stochastic gradient descent .
Training is based on the back propagation through time algorithm , which unrolls the network and then computes error gradients over multiple time steps ( Rumelhart et al. , 1986 ) .
After training , the output layer represents posteriors p( e t+1 |e t t? n+ 1 , h t , f t ) ; the probabilities of words in the output vocabulary given the n previous input words e t t? n+ 1 , the hidden layer configuration h t as well as the auxiliary input layer configuration f t .
Na?ve computation of the probability distribution over the next word is very expensive for large vocabularies .
A well established efficiency trick uses word- classing to create a more efficient two -step process ( Goodman , 2001 ; Emami and Jelinek , 2005 ; where each word is assigned a unique class .
To compute the probability of a word , we first compute the probability of its class , and then multiply it by the probability of the word conditioned on the class : p( e t+1 |e t t? n+ 1 , h t , f t ) = p( c i |e t t? n+ 1 , h t , f t ) ? p(
Lattice Rescoring with an Unbounded Language Model
We evaluate our joint language and translation model in a lattice rescoring setup , allowing us to search over a much larger space of translations than would be possible with n-best lists .
While very space efficient , lattices also impose restrictions on the context available to features , a particularly challenging setting for our model which depends on the entire prefix of a translation .
In the ensuing description we introduce a new algorithm to efficiently tackle this issue .
Phrase - based decoders operate by maintaining a set of states representing competing translations , either partial or complete .
Each state is scored by a number of features including the n-gram language model .
The independence assumptions of the features determine the amount of context each state needs to maintain in order for it to be possible to assign a score to it .
For example , a trigram language model is indifferent to any context other than the two immediately preceding words .
Assuming the trigram model dominates the Markov assumptions of all other features , which is typically the case , then we have to maintain at least two words at each state , also known as the n-gram context .
However , a recurrent neural network language model makes much weaker independence assumptions .
In fact , the predictions of such a model depend on all previous words in the sentence , which would imply a potentially very large context .
But storing all words is an inefficient solution from a dynamic programming point of view .
Fortunately , we do not need to maintain entire translations as context in the states : the recurrent model compactly encodes the entire history of previous words in the hidden layer configuration h i .
It is therefore sufficient to add h i as context , instead of the entire translation .
The language model can then simply score any new words based on h i from the previous state when a new state is created .
1 : function RESCORELATTICE (k , V , E , s , T ) 2 : Q ? TOPOLOGICALLY -SORT ( V ) 3 : for all v in V A much larger problem is that items , that were previously equivalent from a dynamic programming perspective , may now be different .
Standard phrasebased decoders ( Koehn et al. , 2007 ) recombine decoder states with the same context into a single state because they are equivalent to the model features ; usually recombination retains only the highest scoring candidate .
3
However , if the context is large , then the amount of recombination will decrease significantly , leading to less variety in the decoder beam .
This was confirmed in preliminary experiments where we simulated context sizes of up to 100 words but found that accuracy dropped by between 0.5- 1.0 BLEU .
Integrating a long-span language model na?vely requires to keep context equivalent to the entire left prefix of the translation , a setting which would permit very little recombination .
Instead of using inefficient long-span contexts , we propose to maintain the usual n-gram context and to keep a fixed number of hidden layer configurations k at each decoder state .
This leads to a new split-state dynamic program which splits each decoder state into at most k new items , each with a separate hidden layer configuration representing an unbounded history ( Figure 2 ) .
This maintains diversity in the explored translation hypothesis space and preserves high-scoring hidden layer configurations .
What is the effect of this strategy ?
To answer this question we measured translation accuracy for various settings of k on our lattice rescoring setup ( see ?4 for details ) .
In the same experiment , we compare lattices to n-best lists in terms of accuracy , model score and wall time impact .
4
The results ( Table 1 and Figure 3 ) show that reranking accuracy on lattices is not significantly better , however , rescoring lattices with k = 1 is much faster than n-best lists .
Similar observations have been made in previous work on minimum error-rate training ( Macherey 3 Assuming a max-translation decision rule .
In a minimum-risk setting , we may assign the sum of the scores of all candidates to the retained item .
4
We measured running times on an HP z800 workstation equipped with 24 GB main memory and two Xeon E5640 CPUs with four cores each , clocked at 2.66 GHz .
All experiments were run single-threaded . , 2008 ) .
The recurrent language model adds an overhead of about 54 % at k = 1 on top of the time to produce the baseline 1 - best output , a considerable but not necessarily prohibitive overhead .
Larger values of k return higher probability solutions , but there is little impact on accuracy : the BLEU score is nearly identical when retaining up to 100 histories compared to keeping only the highest scoring .
While surprising at first , we believe that this effect is due to the high similarity of the translations represented by the histories in the beam .
Each history represents a different translation but all translation hypothesis share the same n-gram context , and , more importantly , they are translations of the same foreign words , since they have exactly the same coverage vector .
These commonalities are likely to result in similar recurrent histories , which in turn reduces the effect of aggressive pruning .
Language Model Experiments
Recurrent neural network language models have previously only been used in n-best rescoring settings and on small -scale tasks with baseline language models trained on only 17.5 m words ( Mikolov , 2012 ) .
We extend this work by experimenting on lattices using strong baselines with ngram models trained on over one billion words and by evaluating on a number of language pairs .
Experimental Setup Baseline .
We experiment with an in-house phrasebased system similar to Moses ( Koehn et al. , 2003 ) , scoring translations by a set of common features including maximum likelihood estimates of source given target mappings p M LE ( e|f ) and vice versa p M LE ( f |e ) , as well as lexical weighting estimates p LW ( e|f ) and p LW ( f |e ) , word and phrasepenalties , a linear distortion feature and a lexicalized reordering feature .
Log-linear weights are estimated with minimum error rate training ( Och , 2003 ) . Evaluation .
We use training and test data from the WMT 2012 campaign and report results on French -English , German-English and English - German .
Translation models are estimated on 102 m words of parallel data for French - English , 91 m words for German-English and English - German ; between 3.5 - 5 m words are newswire , depending on the language pair , and the remainder are parliamentary proceedings .
The baseline systems use two 5 - gram modified Kneser - Ney language models ; the first is estimated on the target-side of the parallel data , while the second is based on a large newswire corpus released as part of the WMT campaign .
For French -English and German- English we use a language model based on 1.15 bn words , and for English - German we train a model on 327 m words .
We evaluate on the newswire test sets from 2010 - 2011 containing between 2034 - 3003 sentences .
Log-linear weights are estimated on the 2009 data set comprising 2525 sentences .
We rescore the lattices produced by the baseline systems with an aggressive but effective context beam of k = 1 that did not harm accuracy in preliminary experiments ( ?3 ) .
Neural Network Language Model .
The vocabularies of the language models are comprised of the words in the training set after removing singletons .
We obtain word-classes using a version of Brown - Clustering with an additional regularization term to optimize the runtime of the language model ( Brown et al. , 1992 ; Zweig and Makarychev , 2013 ) .
Direct connections use maximum entropy features over unigrams , bigrams and trigrams .
We use the standard settings for the model with the default learning rate ? = 0.1 that decays exponentially if the validation set entropy does not increase after each epoch .
Back propagation through time computes error gradients over the past twenty time steps .
Training is stopped after 20 epochs or when the validation entropy does not decrease over two epochs .
We experiment with varying training data sizes and randomly draw the data from the same corpora used for the baseline systems .
Throughout , we use a hidden layer size of 100 which provided a good trade - off between time and accuracy in initial experiments .
Results
Training times for neural networks can be a major bottleneck .
Recurrent architectures are particularly hard to parallelize due to their inherent dependence on the previous hidden layer configuration .
One straightforward way to influence training time is to change the size of the training corpus .
Our results ( Table 2 , Table 3 and Table 4 ) show that even small models trained on only two million words significantly improve over the 1 - best decoder output ( Baseline ) ; this represents only 0.6 percent of the data available to the n-gram model used by the baseline .
Models of this size can be trained in only about 3.5 hours .
A model trained on 50 m words took 63 hours to train .
When paired with an n-gram model trained on 25 times more data , accuracy improved by up to 0.7 BLEU on French - English .
Joint Model Experiments
In the next set of experiments , we turn to the joint language and translation model , an extension of the recurrent neural network language model with additional inputs for the foreign sentence .
We first introduce two continuous space representations of the foreign sentence ( ?5.1 ) .
Using these representations we evaluate the accuracy of the joint model in the lattice rescoring setup and compare against the traditional translation channel model features ( ?5.2 ) .
Next , we establish an upper bound on accuracy for the joint model via an oracle experiment ( ?5.3 ) .
Inspired by the results of the oracle experiment we train a transform between the source words and the reference representations .
This leads to the best results improving 1.5 BLEU over the 1 - best decoder output and adding 0.2 BLEU on average to the gains achieved by the recurrent language model ( ?5.4 ) .
Setup .
Conventional language models can be trained on monolingual or bilingual data ; however , the joint model can only be trained on the latter .
In order to control for data size effects , we restrict training of all models , including the baseline n-gram model , to the target side of the parallel corpus , about 102 m words for French - English .
Furthermore we train recurrent models only on the newswire portion ( about 3.5 m words for training and 250 k words for validation ) since initial experiments showed comparable results to using the full parallel corpus , available to the baseline .
This is reasonable since the test data is newswire .
Also , it allows for more rapid experimentation .
Foreign Sentence Representations
We represent foreign sentences either by latent semantic analysis ( LSA ; Deerwester et al. 1990 ) or by word encodings produced as a by-product of training the recurrent neural network language model on the source words .
LSA is widely used for representing words and documents in low-dimensional vector space .
The method applies reduced singular value decomposition ( SVD ) to a matrix M of word counts ; in our setting , rows represent sentences and columns represent foreign words .
SVD reduces the number of columns while preserving similarity among the rows , effectively mapping from a high-dimensional representation of a sentence , as a set of words , to a low-dimensional set of concepts .
The output of SVD is an approximation of M by three matrices :
T contains single word representations , R represents full sentences , and S is a diagonal scaling matrix : M ? T SR T Given vocabulary V and n sentences , we construct M as a matrix of size | V ? n| .
The ij - th entry is the number of times word i occurs in sentence j , also known as the term frequency value ; the entry is also weighted by the inverse document frequency , the relative importance of word i among all sentences , expressed as the negative logarithm of the fraction of sentences in which word i occurs .
As a second representation we use single word embeddings implicitly learned by the input layer weights U of the recurrent neural network language model ( ? 2 ) , denoted as RNN .
Each word is represented by a vector of size |h i | , the number of neurons in the hidden layer ; in our experiments , we consider concatenations of individual word vectors to represent foreign word contexts .
These encodings have previously been found to capture syntactic and semantic regularities and are readily available in our experimental framework via training a recurrent neural network language model on the source-side of the parallel corpus .
Results
We first experiment with the two previously introduced representations of the source-side sentence .
Table 5 shows the results compared to the 1 - best decoder output and an RNN language model ( targetonly ) .
We first try LSA encodings of the entire foreign sentence as 80 or 240 dimensional vectors ( sent-lsa - dim80 , sent-lsa - dim240 ) .
Next , we experiment with single - word RNN representations of sliding word -windows in the hope of representing relevant context more precisely .
Word - windows are constructed relative to the source words aligned to the current target word , and individual word vectors are concatenated into a single vector .
We first try contexts which do not include the aligned source words , in the hope of capturing information not already modeled by the channel models , starting with the next five words ( ww-rnn -dim50.n5 ) , the five previous and the next five words ( ww-rnn -dim50. p5n5 ) as well as the previous three words ( ww-rnn -dim50.p3 ) .
Next , we experiment with word - windows of up to five aligned source words ( ww-rnn -dim50.c5 ) .
Finally , we try contexts based on LSA word vectors ( ww -lsa - dim50.n5 , ww-lsa - dim50.p3 ) .
5
While all models improve over the baseline , none significantly outperforms the recurrent neural network language model in terms of BLEU .
However , the perplexity results suggest that the models utilize the foreign representations since all joint models improve vastly over the target-only language ?p( f |e ) ?p( e|f ) ?p( e|f ) Baseline without CM 24.0 22.5 + target-only 24.5 22.6 + sent-lsa-dim240 24.9 23.3 + ww-rnn-dim50.n5 24.9 24.0 + ww-rnn-dim50.p5n5 24.6 23.7 + ww-rnn-dim50.p3 24.6 22.3 + ww-rnn-dim50.c5 24.9 24.0 + ww-lsa-dim50.n5 24.8 23.9 + ww-lsa-dim50.p3 23.8 23.2 model .
The lowest perplexity is achieved by the context covering the aligned source words ( ww-rnn -dim50.c5 ) since the source words are a better predictor of the target words than outside context .
The experiments so far measured if the joint model can improve in addition to the four channel model features used by the baseline , that is , the maximum likelihood and lexical translation features in both translation directions .
The joint model clearly overlaps with these features , but how well does the recurrent model perform compared against the channel model features ?
To answer this question , we removed channel model features corresponding to the same translation direction as the joint model , specifically p M LE ( e|f ) and p LW ( e|f ) , from the lattices and measured the effect of adding the joint models .
The results ( Table 6 , column ?p( e | f ) ) clearly show that our joint models are competitive with the channel model features by outperforming the original baseline with all channel model features ( 24.7 BLEU ) by 0.2 BLEU ( ww-rnn -dim50.n5 , ww-rnn-dim50.c5 ) .
As a second experiment , we removed all channel model features ( column ?p( e | f ) , p( f | e ) ) , diminishing baseline accuracy to 22.5 BLEU .
In this setting , the best joint model is able to make up 1.5 of the 2.2 BLEU lost due to removal of the channel model features , while modeling only a single translation direction .
This setup also shows the negligible effect of the target-only language model in the absence of translation scores , whereas the joint models are much more effective since they do model translation .
Overall , the best joint models prove very competitive to the traditional channel features .
Oracle Experiment
The previous section examined the effect of a set of basic foreign sentence representations .
Although we find some benefit from these representations , the differences are not large .
One might naturally ask whether there is greater potential upside from this channel model .
Therefore we turn to measuring the upper bound on accuracy for the joint approach as a whole .
Specifically , we would like to find a bound on accuracy given an ideal representation of the source sentence .
To answer this question , we conducted an experiment where the joint model has access to an LSA representation of the reference translation .
Table 7 shows that the joint approach has an oracle accuracy of up to 4.3 BLEU above the baseline .
This clearly confirms that the joint approach can exploit the additional information to improve BLEU , given a good enough representation of the foreign sentence .
In terms of perplexity , we see an improvement of up to 65 % over the target-only model .
It should be noted that since LSA representations are computed on reference words , perplexity no longer has its standard meaning .
Target Language Projections
Our experiments so far showed that joint models based on direct representations of the source words are very competitive to the traditional channel models ( ?5.2 ) .
However , these experiments have not shown any improvements over the normal recurrent neural network language model .
The previous section demonstrated that good representations can lead to substantial gains ( ?5.3 ) .
In order to bridge the gap , we propose to learn a separate transform from the foreign words to an encoding of the reference target words , thus making the source -side representations look more like the target - side encodings used in the oracle experiment .
Specifically , we learn a linear transform d ? : x ?
r mapping directly from a vector encoding of the foreign sentence x to an l-dimensional LSA representation r of the reference sentence .
source-side representation .
The transform models all foreign words in the parallel corpus except singletons , which are collapsed into a unique class , similar to the recurrent neural network language model .
We train the transform to minimize the squared error with respect to the reference LSA vector using an SGD online learner : ? * = arg min ?
n i=1 r i ? d ? ( x i ) 2 ( 1 )
We found a simple constant learning rate , tuned on the validation data , to be as effective as schedules based on constant decay , or reducing the learning rate when the validation error increased .
Our feature -set includes unigram and bigram word features .
The value of unigram features is simply the unigram count in that sentence ; bigram features receive a weight of the bigram count divided by two to help prevent overfitting .
Then the vector for each sentence was divided by its L2 norm .
Both weighting and normalization led to substantial improvements in test set error .
More complex features such as skip-bigrams , trigrams and character n-grams did not yield any significant improvements .
Even this representation of sentences is composed of a large number of instances , and so we resorted to feature hashing by computing feature ids as the least significant 20 bits of each feature name .
Our best transform achieved a cosine similarity of 0.816 on the training data , 0.757 on the validation data , and 0.749 on news2011 .
The results ( Table 8 ) show that the transform improves over the recurrent neural network language model on all test sets and by 0.2 BLEU on average .
We verified significance over the target-only model using paired bootstrap resampling ( Koehn , 2004 ) over all test sets ( 7526 sentences ) at the p < 0.001 level .
Overall , we improve accuracy by up to 1.5 BLEU and by 1.1 BLEU on average across all test sets over the decoder 1 - best with our joint language and translation model .
Related Work
Our approach of combining language and translation modeling is very much in line with recent work on n-gram- based translation models ( Crego and Yvon , 2010 ) , and more recently continuous space - based translation models ( Le et al. , 2012a ; Gao et al. , 2013 ) .
The joint model presented in this paper differs in a number of key aspects : we use a recurrent architecture representing an unbounded history of both source and target words , rather than a feedforward style network .
Feed-forward networks and n-gram models have a finite history which makes predictions independent of anything but a small history of words .
Furthermore , we only model the target - side which is different to previous work modeling both sides .
We introduced a new algorithm to tackle lattice rescoring with an unbounded model .
The automatic speech recognition community has previously addressed this issue by either approximating longspan language models via simpler but more tractable models ( Deoras et al. , 2011 b ) , or by identifying confusable subsets of the lattice from which n-best lists are constructed and rescored .
We extend their work by directly mapping a recurrent neural network model onto the structure of the lattice , rescoring all states instead of focusing only on subsets .
Conclusion and Future Work Joint language and translation modeling with recurrent neural networks leads to substantial gains over the 1 - best decoder output , raising accuracy by up to 1.5 BLEU and by 1.1 BLEU on average across several test sets .
The joint approach also improves over the gains of the recurrent neural network language model , adding 0.2 BLEU on average across several test sets .
Our models are competitive to the traditional channel models , outperforming them in a head - to - head comparison .
Furthermore , we tackled the issue of lattice rescoring with an unbounded recurrent model by means of a novel algorithm that keeps a beam of recurrent histories .
Finally , we have shown that the recurrent neural network language model can significantly improve over n-gram baselines across a range of language - pairs , even when the baselines were trained on 575 times more data .
In future work we plan to directly learn representations of the source-side during training of the joint model .
Thus , the model itself can decide which encoding is best for the task .
We also plan to change the cross entropy objective to a BLEU - inspired objective in a discriminative training regime , which we hope to be more effective .
We would also like to apply recent advances in tackling the vanishing gradient problem ( Pascanu et al. , 2013 ) using a regularization term to maintain the magnitude of the gradients during back propagation through time .
Finally , we would like to integrate the recurrent model directly into first - pass decoding , a straightforward extension of lattice rescoring using the algorithm we developed .
Figure 1 : 1 Figure 1 : Structure of the recurrent neural network model , including the auxiliary input layer f t .
