title
On the Evaluation of Semantic Phenomena in Neural Machine Translation Using Natural Language Inference
abstract
We propose a process for investigating the extent to which sentence representations arising from neural machine translation ( NMT ) systems encode distinct semantic phenomena .
We use these representations as features to train a natural language inference ( NLI ) classifier based on datasets recast from existing semantic annotations .
In applying this process to a representative NMT system , we find its encoder appears most suited to supporting inferences at the syntax -semantics interface , as compared to anaphora resolution requiring worldknowledge .
We conclude with a discussion on the merits and potential deficiencies of the existing process , and how it may be improved and extended as a broader framework for evaluating semantic coverage .
1
Introduction
What do neural machine translation ( NMT ) models learn about semantics ?
Many researchers suggest that state- of- the - art NMT models learn representations that capture the meaning of sentences ( Gu et al. , 2016 ; Johnson et al. , 2017 ; Zhou et al. , 2017 ; Andreas and Klein , 2017 ; Neubig , 2017 ; Koehn , 2017 ) .
However , there is limited understanding of how specific semantic phenomena are captured in NMT representations beyond this broad notion .
For instance , how well do these representations capture Dowty ( 1991 ) 's thematic proto-roles ?
Are these representations sufficient for understanding paraphrastic inference ?
Do the sentence representations encompass complex anaphora resolution ?
We argue that existing semantic annotations recast as Natural Language Inference ( NLI ) can be leveraged to investigate whether sentence representations encoded by NMT models capture these semantic phenomena .
1 Code developed and data used are available at https : //github.com/boknilev/nmt-repr-analysis .
Sara adopted Jill , she wanted a child DPR Sara adopted Jill , Jill wanted a child
We use sentence representations from pretrained NMT encoders as features to train classifiers for NLI , the task of determining if one sentence ( a hypothesis ) is supported by another ( a context ) .
2
If the sentence representations learned by NMT models capture distinct semantic phenomena , we hypothesize that those representations should be sufficient to perform well on NLI datasets that test a model 's ability to capture these phenomena .
Figure 1 shows example NLI sentence pairs with their respective labels and semantic phenomena .
We evaluate NMT sentence representations of 4 NMT models from 2 domains on 4 different NLI datasets to investigate how well they capture different semantic phenomena .
We use White et al . ( 2017 ) 's Unified Semantic Evaluation Framework ( USEF ) that recasts three semantic phenomena NLI : 1 ) semantic proto-roles , 2 ) paraphrastic inference , 3 ) and complex anaphora resolution .
Additionally , we evaluate the NMT sentence representations on 4 ) Multi- NLI , a recent extension of the Stanford Natural Language Inference dataset ( SNLI ) ( Bowman et al. , 2015 ) that includes multiple genres and domains .
We contextualize our results with a standard neural encoder described in Bowman et al . ( 2015 ) and used in White et al . ( 2017 ) .
Based on the recast NLI datasets , our investigation suggests that NMT encoders might learn more about semantic proto-roles than anaphora resolution or paraphrastic inference .
We note that the target- side language affects how an NMT sourceside encoder captures these semantic phenomena .
Motivation
Why use recast NLI ?
We focus on NLI , as opposed to a wide range of NLP taks , as a unified framework that can capture a variety of semantic phenomena based on arguments by White et al . ( 2017 ) .
Their recast dataset enables us to study whether NMT encoders capture " distinct types of semantic reasoning " under just one task .
We choose these specific semantic phenomena for two reasons .
First , a long term goal is to understand how combinations of different corpora and neural architectures can contribute to a system 's ability to perform general language understanding .
As humans can understand ( annotate consistently ) the sentence pairs used in our experiments , we would similarly like our final system to have this same capability .
We posit that it is necessary but not necessarily sufficient for a language understanding system to be able to capture the semantic phenomena considered here .
Second , we believe these semantic phenomena might be relevant for translation .
We demonstrate this with a few examples .
Anaphora Anaphora resolution connects tokens , typically pronouns , to their referents .
Anaphora resolution should occur when translating from morphologically poor languages into some morphologically rich languages .
For example , when translating " The parent fed the child because she was hungry , " a Spanish translation should describe the child as la ni ?a ( fem . ) and not el ni ?o ( masc. ) since she refers to the child .
Because world knowledge is often required to perform anaphora resolution ( Rahman and Ng , 2012 ; Javadpour , 2013 ) , this may enable evaluating whether an NMT encoder learns world knowledge .
In this example , she refers to the child and not the parent since world knowledge dictates that parents often feed children when children are hungry .
Proto-roles Dowty ( 1991 ) 's proto-roles may be expressed differently in different languages , and so correctly identifying them can be important for translation .
For example , English does not usually explicitly mark volition , a proto-role , except by using adverbs like intentionally or accidentally .
Other languages mark volitionality by using special affixes ( e.g. , Tibetan and Sesotho , a Bantu language ) , case marking ( Hindi , Sinhalese ) , or auxiliaries ( Japanese ) .
3 Correctly generating these markers may require the MT system to encode volitionality on the source side .
Paraphrases Callison-Burch ( 2007 ) discusses how paraphrases help statistical MT ( SMT ) when alignments from source words to target - language words are unknown .
If the alignment model can map a paraphrase of the source word to a word in the target language , then the SMT model can translate the original word based on its paraphrase .
4 Paraphrases are also used by professional translators to deal with non-equivalence of words in the source and target languages ( Baker , 2018 ) .
Methodology
We use NMT models based on bidirectional long short-term memory ( Bi - LSTM ) encoderdecoders with attention ( Sutskever et al. , 2014 ; Bahdanau et al. , 2015 ) , trained on a parallel corpus .
Given an NLI context- hypothesis pair , we pass each sentence independently through a trained NMT encoder to extract their respective vector representations .
We represent each sentence by concatenating the last hidden state from the forward and backward encoders , resulting in v and u ( in R 2d ) for the context and hypothesis .
5
We follow the common practice of feeding the concatenation ( v , u ) ?
R 4d to a classifier ( Rockt ?
schel et al. , 2016 ; Bowman et al. , 2015 ; Mou et al. , 2016 ; Liu et al. , 2016 ; Cheng et al. , 2016 ; Munkhdalai and Yu , 2017 ) .
Sentence pair representations are fed into a classifier with a softmax layer that maps onto the number of labels .
Experiments with both linear and non-linear classifiers have not shown major differences , so we report results with the linear classifier unless noted otherwise .
We report implementation details in Appendix B .
Data MT data
We train NMT models on four language pairs : English ? { Arabic ( ar ) , Spanish ( es ) , Chinese ( zh ) , and German ( de ) } .
See Appendix B for training details .
The first three pairs use the United Nations parallel corpus ( Ziemski et al. , 2016 ) and for English - German , we use the WMT dataset ( Bojar et al. , 2014 ) .
Although the entailment classifier only uses representations extracted from the English-side encoders as features , using multiple language pairs allows us to explore whether different target languages affect what semantic phenomena are captured by an NMT encoder .
Natural Language Inference data
We use four distinct datasets to train classifiers : Multi-NLI , a recent expansion of SNLI containing a broad array of domains that was used in the 2017 RepEval shared task , and three recast NLI datasets from The JHU Decompositional Semantics Initiative ( Decomp ) 6 released by White et al . ( 2017 ) .
Sentence -pairs and labels were recast , i.e. automatically converted , from existing semantic annotations : FrameNet Plus ( FN + ) ( Pavlick et al. , 2015 ) , Definite Pronoun Resolution ( DPR ) ( Rahman and Ng , 2012 ) , and Semantic Proto-Roles ( SPR ) ( Reisinger et al. , 2015 ) .
The FN + portion contains sentence pairs based on paraphrastic inference , DPR 's sentence pairs focus on identifying the correct antecedent for a definite pronoun , and SPR 's sentence pairs test whether the semantic proto-roles from Reisinger et al . ( 2015 ) apply based on a given sentence .
7 Recasting makes it easy to determine how well an NLI method captures the fine- grained semantics inspired by Dowty ( 1991 ) 's thematic proto-roles , paraphrastic inference , and complex anaphora resolutions .
Results
Table 1 shows results of NLI classifiers trained on representations from different NMT encoders .
We also report the majority baseline and the results of Bowman et al . 's 3 - layer deep 200 dimensional neural network used by White et al . ( " USEF " ) .
Paraphrastic entailment ( FN + )
Our classifiers predict FN + entailment worse than the majority baseline , and drastically worse than USEF when trained on FN + 's training set .
Since FN + tests paraphrastic inference and NMT models have been shown to be useful to generate sentential paraphrase pairs ( Wieting and Gimpel , 2017 ; , it is surprising that our classifiers using the representations from the NMT encoder perform poorly .
Although the sentences in FN + are much longer than in the other datasets , sentence length does not seem to be responsible for the poor FN + results .
The classifiers do not noticeably perform better on shorter sentences than longer ones , as noted in Appendix C.
Upon manual inspection , we noticed that in many not-entailed examples , swapped paraphrases had different part-of-speech ( POS ) tags .
This begs the question of whether different POS tags for swapped paraphrases affects the accuracies .
Using Stanford CoreNLP ( Manning et al. , 2014 ) , we partition our validation set based on whether the paraphrases share the same POS tag .
Table 3 reports dev set accuracies using classifiers trained on FN + .
Classifiers using features from NMT encoders trained on the three languages from the UN corpus noticeably perform better on cases where paraphrases have different POS tags compared to paraphrases with the same POS tags .
These dif- ferences might suggest that the recast FN + might not be an ideal dataset to test how well NMT encoders capture paraphrastic inference .
The sentence representations may be impacted more by ungrammaticality caused by different POS tags as opposed to poor paraphrases .
Anaphora entailment ( DPR )
The low accuracies for predicting NLI targeting anaphora resolution are similar to White et al . ( 2017 ) 's findings .
They suggest that the model has difficulty in capturing complex anaphora resolution .
By using contrastive evaluation pairs , Bawden et al . ( 2017 ) recently suggested as well that NMT models are poorly suited for co-reference resolution .
Our results are not surprising given that DPR tests whether a model contains common sense knowledge ( Rahman and Ng , 2012 ) .
In DPR , syntactic cues for co-reference are purposefully balanced out as each pair of pro-nouns appears in at least two context- hypothesis pairs ( Table 9 ) .
This forces the model 's decision to be informed by semantics and world knowledge - a model cannot use syntactic cues to help perform anaphora resolution .
8
Although the poor performance of NMT representations may be explained by a variety of reasons , e.g. training data , architectures , etc. , we would still like ideal MT systems to capture the semantics of co-reference , as evidenced in the example in ?2 .
Even though the classifiers perform poorly when predicting paraphrastic entailment , they surprisingly outperform USEF by a large margin ( around 25 - 30 % ) when using a model trained on DPR .
9
This might suggest that an NMT encoder can pick up on how pronouns may be used as a type of lexical paraphrase ( Bhagat and Hovy , 2013 ) .
Proto-role entailment ( SPR )
When predicting SPR entailments using a classifier trained on SPR data , we noticeably outperform the majority baseline but are below USEF .
Both ours and USEF 's accuracies are lower than Teichert et al . ( 2017 )
Whenever one of the classifiers outperforms the baseline for a proto-role , all the other classifiers do as well .
The classifiers outperform the majority baseline for 6 of the reported 16 proto-roles .
We observe these 6 properties are more associated with proto-agents than proto-patients .
The larger improvements over the majority baseline for SPR compared to FN + and DPR is not surprising .
Dowty ( 1991 ) posited that proto-agent , and - patient should correlate with English syntactic subject , and object , respectively , and empirically the necessity of [ syntactic ] parsing for predicate argument recognition has been observed in practice ( Gildea and Palmer , 2002 ; Punyakanok et al. , 2008 ) .
Further , recent work is suggestive that LSTM - based frameworks implicitly may encode syntax based on certain learning objectives ( Linzen et al. , 2016 ; Shi et al. , 2016 ; Belinkov et al. , 2017 b ) .
It is unclear whether NMT encoders capture semantic proto-roles specifically or just underlying syntax that affects the proto-roles .
NMT target language
Our experiments show differences based on which target language was used to train the NMT encoder , in capturing semantic proto-roles and paraphrastic inference .
In Table 1 , we notice a large improvement using sentence representations from an NMT encoder that was trained on en-es parallel text .
The improvements are most profound when a classifier trained on DPR data predicts entailment focused on se- mantic proto-roles or paraphrastic inference .
We also note that using the NMT encoder trained on en-es parallel text results in the highest results in 5 of the 6 proto-roles in the top portion of Table 4 .
When using other sentence representations ( Appendix A ) , we notice that using representations from English - German encoders consistently outperforms using the other encoders ( Tables 6 and 7 ) .
This prevents us from making generalizations regarding specific target side languages .
NLI across multiple domains
Though our main focus is exploring what NMT encoders learn about distinct semantic phenomena , we would like to know how useful NMT models are for general NLI across multiple domains .
Therefore , we also evaluate the sentence representations with Multi-NLI .
As indicated by Table 5 , the representations perform noticeably better than a majority baseline .
However , our results are not competitive with state - of - the - art systems trained specifically for Multi- NLI .
Related Work
In concurrent work , Poliak et al . ( 2018 ) explore whether NLI datasets contain statistical irregularities by training a model with access to only hypotheses .
Their model significantly outperforms the majority baseline and our results on Multi- NLI , SPR , and FN + .
They suggest that these , among other NLI datasets , contain statistical irregularities .
Their findings illuminate issues with the recast datasets we consider , but do not invalidate our approach of using recast NLI to determine whether NMT encoders capture distinct semantic phenomena .
Instead , they force us to re-evaluate the majority baseline as an indicator of whether encoders learn distinct semantics and to what extent we can make conclusions based on these recast datasets .
Prior work has focused on the relationship between semantics and machine translation .
MEANT and its extension XMEANT evaluate MT systems based on semantics ( Lo and Wu , 2011 ; Lo et al. , 2014 ) .
Others have focused on incorporating semantics directly in MT .
Chan et al. ( 2007 ) use word sense disambiguation to help statistical MT , Gao and Vogel ( 2011 ) add semantic-roles to improve phrase - based MT , and Carpuat et al . ( 2017 ) demonstrate how filtering parallel sentences that are not parallel in meaning improves translation .
Recent work explores how representations learned by NMT systems can improve semantic tasks .
Mc - Cann et al. ( 2017 ) show improvements in many tasks by using contextualized word vectors extracted from a LSTM encoder trained for MT .
Their goal is to use NMT to improve other tasks while we focus on using NLI to determine what NMT models learn about different semantic phenomena .
Researchers have explored what NMT models learn about other linguistic phenomena , such as morphology ( Dalvi et al. , 2017 ; Belinkov et al. , 2017a ) , syntax ( Shi et al. , 2016 ) , and lexical semantics ( Belinkov et al. , 2017 b ) , including word senses ( Marvin and Koehn , 2018 ; Liu et al. , 2018 )
Conclusion and Future Work Researchers suggest that NMT models learn sentence representations that capture meaning .
We inspected whether distinct types of semantics are captured by NMT encoders .
Our experiments suggest that NMT encoders might learn the most about semantic proto-roles , do not focus on anaphora resolution , and may poorly capture paraphrastic inference .
We conclude by suggesting that target - side language affects how well an NMT encoder captures these semantic phenomena .
In future work , we would like to study how well NMT encoders capture other semantic phenomena , possibly by recasting other datasets .
Comparing how semantic phenomena are represented in different NMT architectures , e.g. purely convolutional ( Gehring et al. , 2017 ) or attentionbased ( Vaswani et al. , 2017 ) , may shed light on whether different architectures may better capture semantic phenomena .
Finally , investigating how multilingual systems learn semantics can bring a new perspective to questions of universality of representation ( Schwenk and Douze , 2017 ) .
