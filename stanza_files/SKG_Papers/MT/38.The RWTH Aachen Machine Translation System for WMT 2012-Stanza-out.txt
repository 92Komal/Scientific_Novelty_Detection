title
The RWTH Aachen Machine Translation System for WMT 2012
abstract
This paper describes the statistical machine translation ( SMT ) systems developed at RWTH Aachen University for the translation task of the NAACL 2012 Seventh Workshop on Statistical Machine Translation ( WMT 2012 ) .
We participated in the evaluation campaign for the French -English and German- English language pairs in both translation directions .
Both hierarchical and phrase - based SMT systems are applied .
A number of different techniques are evaluated , including an insertion model , different lexical smoothing methods , a discriminative reordering extension for the hierarchical system , reverse translation , and system combination .
By application of these methods we achieve considerable improvements over the respective baseline systems .
Introduction
For the WMT 2012 shared translation task 1 RWTH utilized state - of- the - art phrase - based and hierarchical translation systems as well as an in-house system combination framework .
We give a survey of these systems and the basic methods they implement in Section 2 .
For both the French-English ( Section 3 ) and the German-English ( Section 4 ) language pair , we investigate several different advanced techniques .
We concentrate on specific research directions for each of the translation tasks and present the respective techniques along with the empirical results they yield :
For the French ?
English task ( Section 3.1 ) , we apply a standard phrase - based system .
1 http://www.statmt.org/wmt12/ translation-task.html
For the English ?
French task ( Section 3.2 ) , we augment a hierarchical phrase - based setup with a number of enhancements like an insertion model , different lexical smoothing methods , and a discriminative reordering extension .
For the German ?
English ( Section 4.3 ) and English ?
German ( Section 4.4 ) tasks , we utilize morpho-syntactic analysis to preprocess the data ( Section 4.1 ) and employ system combination to produce a consensus hypothesis from normal and reverse translations ( Section 4.2 ) of phrase - based and hierarchical phrase - based setups .
Translation Systems
Phrase - Based System
The phrase - based translation ( PBT ) system used in this work is an in-house implementation of the state - of - the - art decoder described in ( Zens and Ney , 2008 ) .
We use the standard set of models with phrase translation probabilities and lexical smoothing in both directions , word and phrase penalty , distance - based distortion model , an n-gram target language model and three binary count features .
The parameter weights are optimized with minimum error rate training ( MERT ) ( Och , 2003 ) .
Hierarchical Phrase - Based System
For our hierarchical phrase - based translation ( HPBT ) setups , we employ the open source translation toolkit Jane Vilar et al. , 2012 ) , which has been developed at RWTH and is freely available for non-commercial use .
In hierarchical phrase - based translation ( Chiang , 2007 ) , a weighted synchronous context-free grammar is induced from parallel text .
In addition to contiguous lexical phrases , hierarchical phrases with up to two gaps are extracted .
The search is carried out with a parsing - based procedure .
The standard models integrated into our Jane systems are : phrase translation probabilities and lexical smoothing probabilities in both translation directions , word and phrase penalty , binary features marking hierarchical phrases , glue rule , and rules with non-terminals at the boundaries , four binary count features , and an n-gram language model .
Optional additional models comprise IBM model 1 ( Brown et al. , 1993 ) , discriminative word lexicon ( DWL ) models and triplet lexicon models ( Mauser et al. , 2009 ) , discriminative reordering extensions ( Huck et al. , 2011a ) , insertion and deletion models , and several syntactic enhancements like preference grammars and string - to- dependency features ( Peter et al. , 2011 ) .
We utilize the cube pruning algorithm ( Huang and Chiang , 2007 ) for decoding and optimize the model weights with MERT .
System Combination System combination is used to produce consensus translations from multiple hypotheses generated with different translation engines .
The basic concept of RWTH 's approach to machine translation system combination is described in ( Matusov et al. , 2006 ; Matusov et al. , 2008 ) .
This approach includes an enhanced alignment and reordering framework .
A lattice is built from the input hypotheses .
The translation with the best score within the lattice according to a couple of statistical models is selected as consensus translation .
Other Tools and Techniques We employ GIZA ++ ( Och and Ney , 2003 ) to train word alignments .
The two trained alignments are heuristically merged to obtain a symmetrized word alignment for phrase extraction .
All language models ( LMs ) are created with the SRILM toolkit ( Stolcke , 2002 ) and are standard 4 - gram LMs with interpolated modified Kneser - Ney smoothing ( Kneser and Ney , 1995 ; Chen and Goodman , 1998 ) .
We evaluate in truecase , using the BLEU ( Papineni et al. , 2002 ) and TER ( Snover et al. , 2006 ) 3 French -English Setups
We trained phrase - based translation systems for French ?
English and hierarchical phrase - based translation systems for English ?
French .
Corpus statistics for the French - English parallel data are given in Table 1 .
The LMs are 4 - grams trained on the provided resources for the respective language ( Europarl , News Commentary , UN , 10 9 , and monolingual News Crawl language model training data ) .
2 For French ?
English we also investigate a smaller English LM on Europarl and News Commentary data only .
For English ?
French we experiment with additional target-side data from the LDC French Gigaword Second Edition ( LDC2009T28 ) , which is an archive of newswire text data that has been acquired over several years by the LDC .
3 The LDC French Gigaword v2 is permitted for constrained submissions in the WMT shared translation task .
As a development set for MERT , we use newstest2009 in all setups .
Experimental Results French ?
English
For the French ?
English task , the phrase - based SMT system ( PBT ) is set up using the standard models listed in Section 2.1 .
We vary the training data we use to train the system and compare the results .
It should be noted that these setups do not use any English LDC Gigaword data for LM training at all .
Our baseline system uses the Europarl and News Commentary data for training LM and phrase table .
Corpus statistics are shown in the " EP +NC " section of Table 1 .
This results in a performance of 24.7 points BLEU on newstest 2011 .
Then we add the 10 9 as well as UN data and more monolingual English data from the News Crawl corpus to the data used for training the language model .
This system obtains a score of 27.7 points BLEU on newstest2011 .
Our final system uses Europarl , News Commentary , 10 9 and UN data and News Crawl monolingual data for LM training and the Europarl , News Commentary and 10 9 data ( Table 1 ) for phrase table training .
Using these data sets the system reaches 29.1 points BLEU .
The experimental results are summarized in Table 2 .
Experimental Results English ?
French
For the English ?
French task , the baseline system is a hierarchical phrase - based setup including the standard models as listed in Section 2.2 , apart from the binary count features .
We limit the recursion depth for hierarchical rules with a shallow - 1 grammar ( de Gispert et al. , 2010 ) .
In a shallow - 1 grammar , the generic non-terminal X of the standard hierarchical approach is replaced by two distinct non-terminals XH and XP .
By changing the left - hand sides of the rules , lexical phrases are allowed to be derived from XP only , hierarchical phrases from XH only .
On all right - hand sides of hierarchical rules , the X is replaced by XP .
Gaps within hierarchical phrases can thus solely be filled with purely lexicalized phrases , but not a second time with hierarchical phrases .
The initial rule is substituted with S ? XP ?0 , XP ?0 S ? XH ?0 , XH ?0 , ( 1 ) and the glue rule is substituted with S ? S ?0 XP ?1 , S ?0 XP ?1 S ? S ?0 XH ?1 , S ?0 XH ?1 . ( 2 )
The main benefit of a restriction of the recursion depth is a gain in decoding efficiency , thus allowing us to set up systems more rapidly and to explore more model combinations and more system configurations .
The experimental results for English ?
French are given in Table 3 . Starting from the shallow hierarchical baseline setup on Europarl and News Commentary parallel data only ( but Europarl , News Commentary , 10 9 , UN , and News Crawl data for LM training ) , we are able to improve translation quality considerably by first adopting more parallel ( 10 9 and UN ) and monolingual ( French LDC Gigaword v2 ) training resources and then employing several different models that are not included in the baseline already .
We proceed with individual descriptions of the methods we use and report their respective effect in BLEU on the test sets .
10 9 and UN ( up to + 2.5 points BLEU )
While the amount of provided parallel data from Europarl and News Commentary sources is rather limited ( around 2 M sentence pairs in total ) , the UN and the 10 9 corpus each provide a substantial collection of further training material .
By appending both corpora , we end up at roughly 35 M parallel sentences ( cf. Table 1 ) .
We utilize this full amount of data in our system , but extract a phrase table with only lexical ( i.e. nonhierarchical ) phrases from the full parallel data .
We add it as a second phrase table to the baseline system , with a binary feature that enables the system to reward or penalize the application of phrases from this table .
LDC Gigaword v2 ( up to + 0.5 points BLEU )
The LDC French Gigaword Second Edition ( LDC2009T28 ) provides some more monolingual French resources .
We include a total of 28.2 M sentences from both the AFP and APW collections in our LM training data .
insertion model ( up to + 0.4 points BLEU )
We add an insertion model to the log-linear model combination .
This model is designed as a means to avoid the omission of content words in the hypotheses .
It is implemented as a phrase -level feature function which counts the number of inserted words .
We apply the model in source-totarget and target- to -source direction .
A targetside word is considered inserted based on lexical probabilities with the words on the foreign language side of the phrase , and vice versa for a source-side word .
As thresholds , we compute individual arithmetic averages for each word from the vocabulary . noisy - or lexical scores ( up to + 0.4 points BLEU )
In our baseline system , the t Norm ( ? ) lexical scoring variant as described in ( Huck et al. , 2011a ) is employed with a relative frequency ( RF ) lexicon model for phrase table smoothing .
The single- word based translation probabilities of the RF lexicon model are extracted from wordaligned parallel training data , in the fashion of .
We exchange the baseline lexical scoring with a noisy - or ( Zens and Ney , 2004 ) lexical scoring variant t NoisyOr ( ? ) .
DWL ( up to + 0.3 points BLEU )
We augment our system with phrase - level lexical scores from discriminative word lexicon ( DWL ) models ( Mauser et al. , 2009 ; Huck et al. , 2011a ) in both source- to- target and target - to -source direction .
The DWLs are trained on News Commentary data only .
IBM -1 ( up to + 0.1 points BLEU ) On News Commentary and Europarl data , we train IBM model - 1 ( Brown et al. , 1993 ) lexicons in both translation directions and also use them to compute phrase - level scores .
discrim .
RO ( up to + 0.4 points BLEU )
The modification of the grammar to a shallow - 1 version restricts the search space of the decoder and is convenient to prevent overgeneration .
In order not to be too restrictive , we reintroduce more flexibility into the search process by extending the grammar with specific reordering rules XP ? XP ?0 XP ?1 , XP ?1 XP ?0 XP ? XP ?0 XP ?1 , XP ?0 XP ?1 . ( 3 )
The upper rule in Equation ( 3 ) is a swap rule that allows adjacent lexical phrases to be transposed , the lower rule is added for symmetry reasons , in particular because sequences assembled with these rules are allowed to fill gaps within hierarchical phrases .
Note that we apply a length constraint of 10 to the number of terminals spanned by an XP .
We introduce two binary indicator features , one for each of the two rules in Equation ( 3 ) .
In addition to adding these rules , a discriminatively trained lexicalized reordering model is applied .
German-English Setups
We trained phrase- based and hierarchical translation systems for both translation directions of the German- English language pair .
Corpus statistics for German-English can be found in Table 4 .
The language models are 4 - grams trained on the respective target side of the bilingual data as well as on the provided News Crawl corpus .
For the English language model the 10 9 French - English , UN and LDC Gigaword Fourth Edition corpora are used additionally .
For the 10 9 French - English , UN and LDC Gigaword corpora we apply the data selection technique described in ( Moore and Lewis , 2010 ) .
We examine two different language models , one with LDC data and one without .
All German ?
English systems are optimized on newstest 2010 .
For English ?
German , we use newstest2009 as development set .
The news - test2011 set is used as test set and the scores for new-stest2008 are included for completeness .
Morpho-Syntactic Analysis
In order to reduce the source vocabulary size for the German ?
English translation , the German text is preprocessed by splitting German compound words with the frequency - based method described in ( Koehn and Knight , 2003 ) .
To further reduce translation complexity of PBT , we employ the long- range part- of-speech based reordering rules proposed by Popovi ? and Ney ( 2006 ) .
Reverse Translation
For reverse translations we need to change the word order of the bilingual corpus .
For example , if we re-verse both source and target language , the original training example " der Hund mag die Katze . ? the dog likes the cat . " is converted into a new training example " .
Katze die mag Hund der ? . cat the likes dog the " .
We call this type of modification of source or target language reversion .
A system trained of this data is called reverse .
This modification changes the corpora and hence the language model and alignment training produce different results .
Experimental Results German?
English
Our results for the German ?
English task are shown in Table 5 .
For this task , we apply the idea of reverse translation for both the phrase - based and the hierarchical approach .
It seems that the reversed systems perform slightly worse .
However , when we employ system combination using both reverse translation setups ( PBT reverse and HPBT reverse ) and both baseline setups ( PBT baseline and HPBT baseline ) , the translation quality is improved by up to 0.4 points in BLEU and 1.0 points TER compared to the best single system .
The addition of LDC Gigaword corpora ( + GW ) to the language model training data of the baseline setups shows improvements in both BLEU and TER .
Furthermore , with the system combination including these setups , we are able to report an improvement of up to 0.7 points BLEU and 1.0 points TER over the best single setup .
Compared to the system combination based on systems which are not using the LDC Gigaword corpora , we gain 0.3 points in BLEU and 0.4 points in TER .
Experimental Results English ?
German
Our results for the English ?
German task are shown in Table 6 . For this task , we first compare systems using one , two or three language models of different parts of the data .
The language model for systems with only one language model is created with all monolingual and parallel data .
A language model with all monolingual data and a language model with all parallel data is created for the systems with two language models .
For the systems with three language models , we also split the parallel data in two parts consisting of either only Europarl data or only News Commentary data .
For PBT the system with two language models performs best for all test sets .
Further , we apply the idea of reverse translation for both the phrase - based and the hierarchical approach .
The PBT reverse 2 LM systems perform slightly worse compared to PBT baseline 2 LM .
The HPBT reverse 2 LM performs better compared to HPBT baseline 2 LM .
When we employ system combination using both reverse translation setups ( PBT reverse 2 LM and HPBT reverse 2 LM ) and both baseline setups ( PBT baseline 2 LM and HPBT baseline 2 LM ) , the translation quality is improved by up to 0.2 points in BLEU and 2.1 points in TER compared to the best single system .
Conclusion
For the participation in the WMT 2012 shared translation task , RWTH experimented with both phrasebased and hierarchical translation systems .
Several different techniques were evaluated and yielded considerable improvements over the respective base - line systems as well as over our last year 's setups ( Huck et al. , 2011 b ) .
Among these techniques are an insertion model , the noisy - or lexical scoring variant , additional phrase - level lexical scores from IBM model 1 and discriminative word lexicon models , a discriminative reordering extension for hierarchical translation , reverse translation , and system combination .
measures .
French English EP + NC Sentences 2.1 M Running Words 63.3 M 57.6 M Vocabulary 147.8 K 128.5 K Singletons 5.4 K 5.1 K + 10 9 Sentences 22.9 M Running Words 728.6 M 624.0M Vocabulary 1.7M 1.7 M Singletons 0.8M 0.8M + UN Sentences 35.4 M Running Words 1 113.5M 956.4 M Vocabulary 1.9M 2.0M Singletons 0.9M 1.0M
Table 1 : Corpus statistics of the preprocessed French - English parallel training data .
EP denotes Europarl , NC denotes News Commentary .
In the data , numerical quan - tities have been replaced by a single category symbol .
