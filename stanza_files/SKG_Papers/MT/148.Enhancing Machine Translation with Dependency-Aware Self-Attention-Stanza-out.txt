title
Enhancing Machine Translation with Dependency - Aware Self-Attention
abstract
Most neural machine translation models only rely on pairs of parallel sentences , assuming syntactic information is automatically learned by an attention mechanism .
In this work , we investigate different approaches to incorporate syntactic knowledge in the Transformer model and also propose a novel , parameter -free , dependency - aware self-attention mechanism that improves its translation quality , especially for long sentences and in low-resource scenarios .
We show the efficacy of each approach on WMT English ?
German and English ?
Turkish , and WAT English ?
Japanese translation tasks .
Introduction Research in neural machine translation ( NMT ) has mostly exploited corpora consisting of pairs of parallel sentences , with the assumption that a model can automatically learn prior linguistic knowledge via an attention mechanism ( Luong et al. , 2015 ) .
However , Shi et al. ( 2006 ) found that these models still fail to capture deep structural details , and several studies ( Sennrich and Haddow , 2016 ; Eriguchi et al. , 2017 ; Chen et al. , 2017
Chen et al. , , 2018 have shown that syntactic information has the potential to improve these models .
Nevertheless , the majority of syntax - aware NMT models are based on recurrent neural networks ( RNNs ; Elman 1990 ) , with only a few recent studies that have investigated methods for the Transformer model ( Vaswani et al. , 2017 ) .
Wu et al. ( 2018 ) evaluated an approach to incorporate syntax in NMT with a Transformer model , which not only required three encoders and two decoders , but also target - side dependency relations ( precluding its use to low-resource target languages ) .
Zhang et al. ( 2019 ) integrate source-side syntax by concatenating the intermediate representations of a dependency parser to word embeddings .
*
Work done while at Tokyo Institute of Technology .
In contrast to ours , this approach does not allow to learn sub-word units at the source side , requiring a larger vocabulary to minimize out - of- vocabulary words .
Saunders et al. ( 2018 ) interleave words with syntax representations which results in longer sequences - requiring gradient accumulation for effective training - while only leading to + 0.5 BLEU on WAT Ja - En when using ensembles of Transformers .
Finally , Currey and Heafield ( 2019 ) propose two simple data augmentation techniques to incorporate source - side syntax : one that works well on low-resource data , and one that achieves a high score on a large-scale task .
Our approach , on the other hand , performs equally well in both settings .
While these studies improve the translation quality of the Transformer , they do not exploit its properties .
In response , we propose to explicitly enhance the its self-attention mechanism ( a core component of this architecture ) to include syntactic information without compromising its flexibility .
Recent studies have , in fact , shown that self-attention networks benefit from modeling local contexts by reducing the dispersion of the attention distribution ( Shaw et al. , 2018 ; Yang et al. , , 2019 , and that they might not capture the inherent syntactic structure of languages as well as recurrent models , especially in low-resource settings Tang et al. , 2018 ) .
Here , we present parentscaled self-attention ( PASCAL ) : a novel , parameterfree local attention mechanism that lets the model focus on the dependency parent of each token when encoding the source sentence .
Our method is simple yet effective , improving translation quality with no additional parameter or computational overhead .
Our main contributions are : ? introducing PASCAL : an effective parameterfree local self-attention mechanism to incorporate source-side syntax into Transformers ; ? adapting LISA ( Strubell et al. , 2018 ) to subword representations and applying it to NMT ; ? similar to concurrent work ( Pham et al. , 2019 ) , we find that modeling linguistic knowledge into the self-attention mechanism leads to better translations than other approaches .
Our extensive experiments on standard En?De , En?Tr and En? Ja translation tasks also show that ( a ) approaches to embed syntax in RNNs do not always transfer to the Transformer , and ( b ) PASCAL consistently exhibits significant improvements in translation quality , especially for long sentences .
D? d model W V ? W Q ? W K ? d d
Model
In order to design a neural network that is efficient to train and that exploits syntactic information while producing high-quality translations , we base our model on the Transformer architecture ( Vaswani et al. , 2017 ) and upgrade its encoder with parent-scaled self-attention ( PASCAL ) heads at layer l s .
PASCAL heads enforce contextualization from the syntactic dependencies of each source token , and , in practice , we replace standard selfattention heads with PASCAL ones in the first layer as its inputs are word embeddings that lack any contextual information .
Our PASCAL sub-layer has the same number H of attention heads as other layers .
Source syntax Similar to previous work , instead of just providing sequences of tokens , we supply the encoder with dependency relations given by an external parser .
Our approach explicitly exploits sub-word units , which enable open-vocabulary translation : after generating sub-word units , we compute the middle position of each word in terms of number of tokens .
For instance , if a word in position 4 is split into three tokens , now in positions 6 , 7 and 8 , its middle position is 7 .
We then map each sub-word of a given word to the middle position of its parent .
For the root word , we define its parent to be itself , resulting in a parse that is a directed graph .
The input to our encoder is a sequence of T tokens and the absolute positions of their parents .
Parent-Scaled Self-Attention
Figure 1 shows our parent-scaled self-attention sublayer .
Here , for a sequence of length T , the input to each head is a matrix X ? R T ?d model of token embeddings and a vector p ?
R T whose t-th entry p t is the middle position of the t-th token 's dependency parent .
Following Vaswani et al. ( 2017 ) , in each attention head h , we compute three vectors ( called query , key and value ) for each token , resulting in the three matrices K h ? R T ?d , Q h ? R T ?d , and V h ?
R T ?d for the whole sequence , where d = d model / H .
We then compute dot products between each query and all the keys , giving scores of how much focus to place on other parts of the input when encoding a token at a given position .
The scores are divided by ?
d to alleviate the vanishing gradient problem arising if dot products are large : S h = Q h K h / ? d. ( 1 ) Our main contribution is in weighing the scores of the token at position t , s t , by the distance of each token from the position of t's dependency parent : n h tj = s h tj d p tj , for j = 1 , ... , T , ( 2 ) where n h t is the t-th row of the matrix N h ?
R T ?T representing scores normalized by the proximity to t's parent .
d p tj = dist ( p t , j ) is the ( t , j ) th entry of the matrix D p ?
R T ?T containing , for each row d t , the distances of every token j from the middle position of token t's dependency parent p t .
In this paper , we compute this distance as the value of the probability density of a normal distribution centered at p t and with variance ?
2 , N p t , ? 2 : dist ( p t , j) = f N j p t , ?
2 = 1 ? 2 ? 2 e ? ( j? p t ) 2 2 ?
2 . ( 3 ) Finally , we apply a softmax function to yield a distribution of weights for each token over all the tokens in the sentence , and multiply the resulting matrix with the value matrix V h , obtaining the final representations M h for PASCAL head h .
One of the major strengths of our proposal is being parameter - free : no additional parameter is required to train our PASCAL sub-layer as D p is obtained by computing a distance function that only depends on the vector of tokens ' parent positions and can be evaluated using fast matrix operations .
Parent ignoring Due to the lack of parallel corpora with gold -standard parses , we rely on noisy annotations from an external parser .
However , the performance of syntactic parsers drops abruptly when evaluated on out-of- domain data ( Dredze et al. , 2007 ) .
To prevent our model from overfitting to noisy dependencies , we introduce a regularization technique for the PASCAL sub-layer : parent ignoring .
In a similar vein as dropout ( Srivastava et al. , 2014 ) , we disregard information during the training phase .
Here , we ignore the position of the parent of a given token by randomly setting each row of D p to 1 ? R T with some probability q. Gaussian weighing function
The choice of weighing each score by a Gaussian probability density is motivated by two of its properties .
First , its bell - shaped curve :
It allows us to focus most of the probability density at the mean of the distribution , which we set to the middle position of the sub-word units of the dependency parent of each token .
In our experiments , we find that most words in the vocabularies are not split into sub-words , hence allowing PASCAL to mostly focus on the actual parent .
In addition , non-negligible weights are placed on the neighbors of the parent token , allowing the attention mechanism to also attend to them .
This could be useful , for instance , to learn idiomatic expressions such as prepositional verbs in English .
The second property of Gaussian - like distributions that we exploit is their support :
While most of the weight is placed in a small window of tokens around the mean of the distribution , all the values in the sequence are actually multiplied by non-zero factors ; allowing a token j farther away from the parent of token t , p t , to still play a role in the representation of t if its score s h tj is high .
PASCAL can be seen as an extension of the local attention mechanism of Luong et al . ( 2015 ) , with the alignment now guided by syntactic information .
proposed a method to learn a Gaussian bias that is added to , instead of multiplied by , the original attention distribution .
As we will see next , our model significantly outperforms this .
Training
We implement our models in PyTorch on top of the Fairseq toolkit .
2 Hyperparameters , including the number of PASCAL heads , that achieved the highest validation BLEU ( Papineni et al. , 2002 ) score were selected via a small grid search .
Experiments
We report previous results in syntax - aware NMT for completeness , and train a Transformer model as a strong , standard baseline .
We also investigate the following syntax - aware Transformer approaches : 1 ? + PASCAL : The model presented in ?2 .
The variance of the normal distribution was set to 1 ( i.e. , an effective window size of 3 ) as 99.99 % of the source words in our training sets are at most split into 7 sub-words units .
? + LISA : We adapt LISA ( Strubell et al. , 2018 ) to NMT and sub-word units by defining the parent of a given token as its first sub-word ( which represents the root of the parent word ) .
? + MULTI - TASK : Our implementation of the multi-task approach by Currey and Heafield ( 2019 ) where a standard Transformer learns to both parse and translate source sentences .
Table 1 : Test BLEU ( and RIBES for En-Ja ) scores on small-scale ( left ) and large-scale ( right ) data sets .
Models that also require target -side syntax information are marked with ? , while ? indicates statistical significance ( p < 0.01 ) against the Transformer baseline via bootstrap re-sampling ( Koehn , 2004 ) .
Results
Table 1 presents the main results of our experiments .
Clearly , the base Transformer outperforms previous syntax - aware RNN - based approaches , proving it to be a strong baseline in our experiments .
The table shows that the simple approach of Sennrich and Haddow ( 2016 ) does not lead to notable advantages when applied to the embeddings of the Transformer model .
We also see that the multi-task approach benefits from better parameterization , but it only attains comparable performance with the baseline on most tasks .
On the other hand , LISA , which embeds syntax in a self-attention head , leads to modest but consistent gains across all tasks , proving that it is also useful for NMT .
Finally , PASCAL outperforms all other methods , with consistent gains over the Transformer baseline independently of the source language and corpus size :
It gains up to + 0.9 BLEU points on most tasks and a substantial + 1.75 in RIBES ( Isozaki et al. , 2010 ) , a metric with stronger correlation with hu-man judgments than BLEU in En? Ja translations .
On WMT17 , our slim model compares favorably to other methods , achieving the highest BLEU score across all source -side syntax - aware approaches .
3 Overall , our model achieves substantial gains given the grammatically rigorous structure of English and German .
Not only do we expect performance gains to further increase on less rigorous sources and with better parses ( Zhang et al. , 2019 ) , but also higher robustness to noisier syntax trees obtained from back - translated with parent ignoring .
Performance by sentence length
As shown in Figure 2 , our model is particularly useful when translating long sentences , obtaining more than + 2 BLEU points when translating long sentences in all low-resource experiments , and + 3.5 BLEU points on the distant En - Ja pair .
However , only a few sentences ( 1 % ) in the evaluation datasets are long .
SRC
In a cooling experiment , only a tendency agreed Qualitative performance Table 2 presents examples where our model correctly translated the source sentence while the Transformer baseline made a syntactic error .
For instance , in the first example , the Transformer misinterprets the adverb " only " as an adjective of " tendency : " the word " only " is an adverb modifying the verb " agreed . "
In the second example , " do n't " is incorrectly translated to the past tense instead of present .
BASE ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? OURS ? ? ? ? ? ? ? ? ? ? ? ? ? SRC
Of course I do n't PASCAL layer
When we introduced our model , we motivated our design choice of placing PASCAL heads in the first layer in order to enrich the representations of words from their isolated embeddings by introducing contextualization from their parents .
We ran an ablation study on the NC11 data in order to verify our hypothesis .
As shown in Table 3a , the performance of our model on the validation sets is lower when placing Pascal heads in upper layers ; a trend that we also observed with the LISA mechanism .
These results corroborate the findings of Raganato and Tiedemann ( 2018 ) who noticed that , in the first layer , more attention heads solely focus on the word to be translated itself rather than its context .
We can then deduce that enforcing syntactic dependencies in the first layer effectively leads to better word representations , which further enhance the translation accuracy of the Transformer model .
Investigating the performance of multiple syntax - aware layers is left as future work .
Gaussian variance
Another design choice we made was the variance of the Gaussian weighing function .
We set it to 1 in our experiments motivated by the statistics of our datasets , where the vast majority of words is at most split into a few tokens after applying BPE .
Table 3 b corroborates our choice , showing higher BLEU scores on the NC11 validation sets when the variance equals 1 .
Here , " parent-only " is the case where weights are only placed to the middle token ( i.e. the parent ) .
Sensitivity to hyperparameters
Due to the large computational cost required to train Transformer models , we only searched hyperparameters in a small grid .
In order to estimate the sensitivity of the proposed approach to hyperparameters , we trained the NC11 De - En model with the hyperparameters of the En- De one .
In fact , despite being trained on the same data set , we find that more PASCAL heads help when German ( which has a higher syntactic complexity than English ) is used as the source language .
In this test , we only find ?0.2 BLEU points with respect to the score listed in Table 1 , showing that our general approach is effective regardless of extensive fine-tuning .
Additional analyses are reported in Appendix B.
Conclusion
This study provides a thorough investigation of approaches to induce syntactic knowledge into self-attention networks .
Through extensive evaluations on various translation tasks , we find that approaches effective for RNNs do not necessarily transfer to Transformers ( e.g. + S&H ) .
Conversely , dependency - aware self-attention mechanisms ( LISA and PASCAL ) best embed syntax , for all corpus sizes , with PASCAL consistently outperforming other all approaches .
Our results show that exploiting core components of the Transformer to embed linguistic knowledge leads to higher and consistent gains than previous approaches .
