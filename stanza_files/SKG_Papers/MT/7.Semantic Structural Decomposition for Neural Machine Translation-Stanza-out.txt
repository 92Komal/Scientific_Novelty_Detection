title
Semantic Structural Decomposition for Neural Machine Translation
abstract
Building on recent advances in semantic parsing and text simplification , we investigate the use of semantic splitting of the source sentence as preprocessing for machine translation .
We experiment with a Transformer model and evaluate using large-scale crowd-sourcing experiments .
Results show a significant increase in fluency on long sentences on an English - to - French setting with a training corpus of 5 M sentence pairs , while retaining comparable adequacy .
We also perform a manual analysis which explores the tradeoff between adequacy and fluency in the case where all sentence lengths are considered .
1 * This work was done when being affiliated to the Hebrew University of Jerusalem 1
The code and the evaluation data are available at https://github.com/eliorsulem/ Semantic-Structural-Decomposition-for-NMT
This work is licensed under a Creative Commons Attribution 4.0 International License .
License details : http : //creativecommons.org/licenses/by/4.0/.
Introduction
In this paper , we apply a semantic decomposition approach for Neural Machine Translation ( NMT ) and demonstrate that it can tackle two of the main limitations of state - of- the- art NMT .
The first is the translation of long sentences , which is a recurrent issue arising in NMT evaluation ( Sutskever et al. , 2014 ; Pouget - Abadie et al. , 2014 ; Su et al. , 2018 ; Currey and Heafield , 2018 ) .
The second limitation is that current research in NMT mostly focuses on translating single sentences to single sentences , and is evaluated accordingly .
However , Li and Nenkova ( 2015 ) showed that using several sentences to translate a source sentence is sometimes the preferable option .
Therefore , the simplicity of the output could be an important quality marker for translation .
In our model , each source sentence is split ( or decomposed ) into semantic units , namely scenes , building on the Direct Semantic Splitting algorithm ( DSS ; Sulem et al. , 2018 b ) that uses the Universal Conceptual Cognitive Annotation ( UCCA ; Abend and Rappoport , 2013 ) scheme for semantic representation .
Scenes are then translated separately and concatenated for generating the final translation output , which may consist of several sentences .
Our main experiments use the state- of- the- art Transformer model ( Vaswani et al. , 2017 ) in English - to - French settings .
We also include experiments with other MT architectures and training set sizes , and evaluate our results using the crowdsourcing protocol of Graham et al . ( 2016 ) ( ?4 ) .
We obtain a significant increase in fluency on sentences longer than 30 words on the newstest2014 test corpus for English - to - French translation , with a training corpus of 5 M sentence pairs , without degrading adequacy .
Considering all sentence lengths , we observe a tradeoff between fluency and adequacy .
We explore it using a manual analysis , suggesting that the decrease in adequacy is partly due to the loss of cohesion resulting from the splitting ( ?6 ) .
We then proceed to investigate the case of simulated low-resource settings as well as the effect of other sentence splitting methods , including Splitand - Rephrase models ( Aharoni and Goldberg , 2018 ; Botha et al. , 2018 ) ( ?7 ) .
The latter yield considerably lower scores than the use of simple semantic rules , supporting the case for corpusindependent simplification rules .
Related Work Sentence segmentation for MT .
Segmenting sentences into sub-units , based on punctuation and syntactic structures , and recombining their output has been explored by a number of statistical MT works ( Xiong et al. , 2009 ; Goh and Sumita , 2011 ; Sudoh et al. , 2010 ) .
In NMT , Pouget - Abadie et al. ( 2014 ) segmented the source using ILP , tackling English - to - French neural translation .
They con-cluded that segmentation improves overall translation quality but quality may decrease if the segmented fragments are not well -formed .
The concatenation may sometimes degrade fluency and result in errors in punctuation and capitalization .
Kuang and Xiong ( 2016 ) attempted to find split positions such that no reordering will be necessary in the target side for Chinese - English .
We differ from these approaches in using a separate text simplification module that can be applied to different kinds of MT systems , and using a semanticallymotivated segmentation .
Moreover , we allow the final output to be composed of several sentences , taking into account the structural simplicity aspect of translation quality ( Li and Nenkova , 2015 ) .
Text Simplification for MT .
Sentence splitting , which goes beyond segmentation and denotes the conversion of one sentence into one or several sentences , is the main structural operation studied in Text Simplification ( TS ) .
While MT preprocessing was one of the main motivations for the first automatic simplification system ( Chandrasekar et al. , 1996 ) , only few works empirically explored the usefulness of simplification techniques for MT .
Mishra et al. ( 2014 ) used sentence splitting as a preprocessing step for Hindi-to - English translation with a dependency parser and additional modules for gerunds and shared arguments .
? tajner and Popovi ? ( 2016 ) performed structural and lexical simplification as part of a preprocessing step for English -to -Serbian MT .
Manual correction is carried out before translation .
? tajner and Popovi ? ( 2018 ) investigated the use of TS as a processing step for NMT , focusing on syntax - based rules that address relative clauses ( Siddhathan , 2011 ) for English - to - German and English -to -Serbian translation .
Investigating the translation of 106 out of 1000 sentences that have been modified by simplification , they find that the automatic simplification of English relative clauses can improve translation only if simplifications are quality - controlled or corrected in post-processing .
We differ from this work in using semantic rules and by translating independently each of the obtained sentences .
Semantic Decomposition UCCA ( Universal Cognitive Conceptual Annotation ; Abend and Rappoport , 2013 ) is a semantic annotation scheme rooted in typological and cognitive linguistic theory ( Dixon , 2010 b , a ; Langacker , 2008 ) .
It aims to represent the main semantic phe - nomena in the text , abstracting away from syntax .
Formally , UCCA structures are directed acyclic graphs whose nodes ( or units ) correspond either to the leaves of the graph or to several elements viewed as a single entity according to some semantic or cognitive consideration .
A scene is UCCA 's notion of an event or a frame , and is a unit that corresponds to a movement , an action or a state which persists in time .
Every scene contains one main relation , which can be either a Process or a State .
Scenes may contain one or more Participants , interpreted in a broad sense to include locations and destinations .
For example , the sentence " John went home " has a single scene whose Process is " went " .
The two Participants are " John " and " home " .
Scenes can provide additional information about an established entity ( Elaborator scenes ) , commonly participles or relative clauses .
For example , " ( child ) who went home " is an Elaborator scene in " The child who went home is John " .
A scene may also be a Participant in another scene .
For example , " John went home " in the sentence : " He said John went home " .
In other cases , scenes are annotated as parallel scenes ( H ) , which are flat structures and may include a Linker ( L ) , as in : " When L [ he arrives ] H , [ he will call them ] H " .
For UCCA parsing , we use TUPA , a transitionbased parser ( Hershcovich et al. , 2017 ) ( specifically , the TUPA BiLST M model ) .
We build on the DSS rule- based semantic splitting method ( Sulem et al. , 2018 b ) , and use Rule # 1 which targets parallel scenes .
We further explore the use of the additional kinds of scenes in Section 7 for less conservative sentence splitting .
In Rule # 1 , parallel scenes of a given sentence are extracted , split into different sentences and concatenated according to the order of appearance .
More formally , given a decomposition of a sentence S into parallel scenes Sc 1 , Sc 2 , ? ? ?
Sc n ( indexed by the order of the first token ) , we obtain the following rule , where " | " is the sentence delimiter : S ? Sc1 | Sc2 | ? ? ? | Scn
As UCCA allows argument sharing between scenes , the rule may duplicate the same sub-span of S across sentences .
For example , the rule will convert " He came back home and played the piano " into " He came back home " | " He played the piano . " .
Using UCCA - based sentence splitting in our model is motivated by the corpus-based analysis presented in Sulem et al . ( 2015 ) where it is shown that a scene in English is generally translated to a scene in French .
Experimental Setup Corpora
We experiment on the full English - French training data provided in the WMT setting ( Bojar et al. , 2014 ) , which corresponds to about 39 M sentence pairs after cleaning .
2
We refer to this setting as the FullTrain Setting .
We also experiment on the LessTrain Setting where less training data is involved by removing the large UN Corpus and the 10 9 French - English Corpus from the training data , obtaining a new training corpus of about 5 M sentence pairs .
The development set is Newstest 2013 , that consists of 3000 sentences .
The test set is Newstest2014 , consisting of 3003 sentences .
Systems
To investigate the use of semantic structural decomposition for NMT , we propose a twostep method .
First , the original sentence is split into several sentences the DSS rule ( see ? 3 ) , implemented with the UCCA software .
3 . Then , each of the obtained sentences is translated separately by the OpenNMT - py implementation of the Transformer ( Vaswani et al. , 2017 ) . 4
The translated sentences are concatenated to form the final output .
We name the combined system Transformer Sem-Split and compare it to the Transformer Baseline , where no splitting is performed .
The pipeline architecture is summarized in Figure 1 .
The Transformer is trained for 200K training steps , both in the FullTrain and the LessTrain settings .
The development data was used for selecting the model with the highest accuracy ( where perplexity was used in cases of ties ) .
The system was evaluated on the development data every 10 K steps .
For comparison , we also implement our system in the case where the Transformer is replaced by another NMT system , namely a two -layers LSTM model and the Moses phrase - based machine translation system ( Koehn et al. , 2007 ) .
The neural model , also implemented with OpenNMT -py , is trained and validated in the same way as the Transformer .
For Moses , the default model is used in a single setting ( LessTrain ) with MGIZA word alignment , 5 and KenLM language model ( Heafield , 2011 ) using the monolingual data provided in WMT 2014 , and MERT tuning on the development set .
Here too we compare the combined systems to baseline systems which do not perform decomposition .
Evaluation Using Crowdsourcing
In addition to the limitations of BLEU evaluation ( Papineni et al. , 2002 ) in the context of MT ( Callison - Burch et al. , 2006 , and much subsequent work ) , BLEU may correlate negatively with output quality in cases that involve sentence splitting ( Sulem et al. , 2018a ) .
We therefore evaluate using crowdsourcing , and follow the protocol proposed by Graham et al . ( 2016 ) . Evaluation was carried out using Amazon Mechanical Turk .
6 See Appendix
A for a detailed description .
Results
The results in both FullTrain and LessTrain settings are presented in Table 1 .
In terms of fluency , LessTrain Transformer Sem-Split ranks first in this setting and significantly outperforms the corresponding baseline system ( 52.5 vs. 42.5 , p < 10 ?4 ) .
7 For Moses too , the use of semantic sentence splitting increases fluency ( 40.2 vs. 38.1 ) , but not significantly .
On the other hand , where splitting is used as preprocessing , adequacy scores decrease .
In particular , LessTrain Transformer Baseline significantly outperforms the SemSplit counterpart ( 47.5 vs. 39.8 , p < 10 ?4 ) .
For sentences longer than 30 , SemSplit Transformer in the LessTrain setting significantly outperforms the baseline in terms of fluency ( 52.1 vs. 39.6 , p = 0.02 ) , with only a non-significant ( small ) degradation in adequacy ( 41.7 vs. 40.1 , p = 0.46 ) .
Manual Analysis
To further zoom in on the obtained adequacy scores , we decompose adequacy into two dimensions : preservation of semantic content in the level of scenes and the cohesion of the text ( i.e. , whether the different scenes are cohesively linked together ) .
To do so , we manually annotate a sample of 150 sentences from the original test set with a similar proportion of sentences in different length categories as the original corpus , and assess the semantic preservation at the scene-level for each of the extracted scenes , as well as the sentence - level cohesion ( see Appendix B for the protocol ) .
For LessTrain , we find that 66.2 % of the scenes are deemed equally preserved by the SemSplit and Baseline systems .
On the other hand , 20.9 % of the scenes are better preserved by the baseline and 10.7 % of the scenes are better preserved by the SemSplit system .
Averaging over scenes that belong to the same sentence , we find that 68 % of the sentences are either better preserved by SemSplit or equally preserved .
Regarding cohesion , Sem-Split and the Baseline have a comparable cohesion for 59 % of the sentences .
The Baseline has a better cohesion for 36 % of the sentences , while it is improved by SemSplit in 5 % of the cases .
The analysis suggests that cohesion has a central role in the decrease ( and the non-increase for long sentences ) of the adequacy scores .
Therefore the tradeoff between adequacy and fluency observed when all sentence lengths are considered can be explained by a tradeoff between the cohesion and structural simplicity aspects of translation quality .
The different aspects of the translation quality are further illustrated in Table 3 , where two input and output examples are presented , focusing on the LessTrain setting .
In example ( 1 ) , the SemSplit output is similar to the Baseline one at the lexical level but differs in its structure , the SemSplit system behaving as a cross-lingual simplifier at the structural level .
On the other hand , linkers such as " so " are not translated in the case of SemSplit .
In example ( 2 ) , the word " interference " is correctly translated by SemSplit , while it is translated into " ing?rence " ( " intervention " ) in French , which is wrong in this context .
Additional Experiments
We first explore the performance of the proposed system in low-resource machine translation , by following the approach of Hoang et al . ( 2018 ) and randomly select 1 M and 100K sentence pairs from the entire English - French training set , defining the 1 MTrain and 100 KTrain settings respectively .
Tuning and testing remain as before .
The resulted raw scores for the 1 MTrain and 100 KTrain settings are presented in Appendix D , Table 4 .
We observe that while in 1 MTrain , the SemSplit models obtain low results compared to the respective baselines , the SemSplit models obtain higher fluency in 100 KTrain , though not significantly .
Second , to further explore the sentence splitting component , we replicate our model , separating both parallel and embedded scenes before the ( 1 ) Input : Hamas has defended its use of tunnels in the fight against Israel , stating that the aim was to capture Israeli soldiers so they could be exchanged for Palestinian prisoners .
Baseline ( LessTrain )
Output : Le Hamas a d?fendu son utilisation de tunnels dans la lutte contre Isra?l , affirmant que l'objectif ? tait de capturer des soldats isra?liens afin qu'ils puissent ? tre ?chang ?s contre des prisonniers palestiniens .
Literal translation : Hamas has defended its use of tunnels in the fight against Israel , stating that the aim was to capture Israeli soldiers so they could be exchanged for Palestinian prisoners .
SemSplit ( LessTrain )
Output : Le Hamas a d?fendu son utilisation de tunnels dans la lutte contre Isra?l .
Le Hamas a d?clar ?
que l'objectif ? tait de capturer des soldats isra?liens .
Ils pourraient ? tre ?chang ?s contre des prisonniers palestiniens .
Literal translation : Hamas has defended its use of tunnels in the fight against Israel .
Hamas stated that the aim was to capture Israeli soldiers .
They could be exchanged for Palestinian prisoners .
( 2 ) Input : Douglas Kidd of the National Association of Airline Passengers said he believes interference from the devices is genuine even if the risk is minimal .
Baseline ( LessTrain ) Output : Douglas. , de l'Association nationale des compagnies a?riennes , a d?clar ?
qu'il consid ?
rait que l'ing ?
rence des appareils ? tait r?elle , m?me si le risque ? tait minimal .
Literal translation : Douglas. , from the Association National of the companies airline , claimed that he believed that the intervention of the devices was genuine , even if the risk is minimal .
SemSplit ( LessTrain ) Output : Douglas. , de l'Association nationale des compagnies a?riennes , a d?clar ?
qu ' il estimait que l'interf ?
rence avec les appareils r?elle .
Le risque est minimal .
Literal translation : Douglas. , from the Association national of the companies airline , claimed that he believed the interference with the devices was genuine .
The risk is minimal .
translation .
We use Rule # 2 from the DSS system ( Sulem et al. , 2018 b ) addressing Elaborator scenes ( See Appendix C ) , which we further extend to also include Participant scenes .
We denote the resulting system with Transformer SemSplit 1 + 2 .
We also compare the model with two additional sentence splitting systems , where DSS is replaced with the Seq2Seq Copy 512 model for Split-and - Rephrase ( Aharoni and Goldberg , 2018 ) trained on the WEB - SPLIT corpus ( Narayan et al. , 2017 ) ( version 1.0 ) , and the same model trained on the WikiSplit corpus ( Botha et al. , 2018 ) .
Each of the obtained new sentences is translated by the FullTrain Transformer system .
Finally the translated sentences are directly concatenated .
The resulting systems are denoted with Transformer NeuralWEB -SPLIT and Tranformer NeuralWiki-Split .
The results for the FullTrain and LessTrain settings are presented in Table 2 .
As in the case where only the first rule is used , adequacy scores decrease following splitting .
On the other hand , in this case the SemSplit models do not have higher fluency scores than their corresponding baselines , probably because of the more aggressive splitting compared to # Rule 1 alone .
For both adequacy and fluency , the Split-and - Rephrase models obtain very low scores .
Observing their outputs , we find many wrong splits and word repetitions at the splitting phase , which affects the final output .
As this trend is not observed on the standard WEB - SPLIT test corpus , these results may suggest a domain adap-tation effect , which supports the case for corpusindependent sentence splitting .
Conclusion
This work investigates the application of semantic structural decomposition for NMT , proposing an intermediate way between sentence segmentation used in MT and TS preprocessing , where each of the semantic components is separately translated .
Using the Transformer and large-scale crowd- sourcing evaluation , we obtain an increase in fluency on long sentences on an English - to - French setting without significantly lowering adequacy .
We further observe increased fluency when evaluating on all the sentences , albeit at the cost of adequacy .
Future work concerns the recombination of the output sentences , inserting the linkage between them , so as not to lose semantic content .
