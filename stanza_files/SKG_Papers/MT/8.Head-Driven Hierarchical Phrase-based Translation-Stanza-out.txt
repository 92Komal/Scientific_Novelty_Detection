title
Head -Driven Hierarchical Phrase- based Translation
abstract
This paper presents an extension of Chiang 's hierarchical phrase - based ( HPB ) model , called Head - Driven HPB ( HD - HPB ) , which incorporates head information in translation rules to better capture syntax - driven information , as well as improved reordering between any two neighboring non-terminals at any stage of a derivation to explore a larger reordering search space .
Experiments on Chinese-English translation on four NIST MT test sets show that the HD - HPB model significantly outperforms Chiang 's model with average gains of 1.91 points absolute in BLEU .
Introduction Chiang 's hierarchical phrase- based ( HPB ) translation model utilizes synchronous context free grammar ( SCFG ) for translation derivation ( Chiang , 2005 ; Chiang , 2007 ) and has been widely adopted in statistical machine translation ( SMT ) .
Typically , such models define two types of translation rules : hierarchical ( translation ) rules which consist of both terminals and non-terminals , and glue ( grammar ) rules which combine translated phrases in a monotone fashion .
Due to lack of linguistic knowledge , Chiang 's HPB model contains only one type of nonterminal symbol X , often making it difficult to select the most appropriate translation rules .
1
What is more , Chiang 's HPB model suffers from limited phrase reordering combining translated phrases in a monotonic way with glue rules .
In addition , once a 1 Another non-terminal symbol S is used in glue rules .
glue rule is adopted , it requires all rules above it to be glue rules .
One important research question is therefore how to refine the non-terminal category X using linguistically motivated information : Zollmann and Venugopal ( 2006 ) ( SAMT ) e.g. use ( partial ) syntactic categories derived from CFG trees while Zollmann and Vogel ( 2011 ) use word tags , generated by either POS analysis or unsupervised word class induction .
Almaghout et al . ( 2011 ) employ CCGbased supertags .
Mylonakis and Sima'an ( 2011 ) use linguistic information of various granularities such as Phrase - Pair , Constituent , Concatenation of Constituents , and Partial Constituents , where applicable .
Inspired by previous work in parsing ( Charniak , 2000 ; Collins , 2003 ) , our Head-Driven HPB ( HD - HPB ) model is based on the intuition that linguistic heads provide important information about a constituent or distributionally defined fragment , as in HPB .
We identify heads using linguistically motivated dependency parsing , and use their POS to refine X .
In addition HD - HPB provides flexible reordering rules freely mixing translation and reordering ( including swap ) at any stage in a derivation .
Different from the soft constraint modeling adopted in ( Chan et al. , 2007 ; Marton and Resnik , 2008 ; Shen et al. , 2009 ; He et al. , 2010 ; Huang et al. , 2010 ; Gao et al. , 2011 ) , our approach encodes syntactic information in translation rules .
However , the two approaches are not mutually exclusive , as we could also include a set of syntax - driven features into our translation model .
Our approach maintains the advantages of Chiang 's HPB model while at the same time incorporating head information and flex - ible reordering in a derivation in a natural way .
Experiments on Chinese-English translation using four NIST MT test sets show that our HD - HPB model significantly outperforms Chiang 's HPB as well as a SAMT - style refined version of HPB .
Head -Driven HPB Translation Model Like Chiang ( 2005 ) and Chiang ( 2007 ) , our HD - HPB translation model adopts a synchronous context free grammar , a rewriting system which generates source and target side string pairs simultaneously using a context-free grammar .
Instead of collapsing all non-terminals in the source language into a single symbol X as in Chiang ( 2007 ) , given a word sequence f i j from position i to position j , we first find heads and then concatenate the POS tags of these heads as f i j 's non-terminal symbol .
Specifically , we adopt unlabeled dependency structure to derive heads , which are defined as : Definition 1 .
For word sequence f i j , word f k ( i ? k ? j ) is regarded as a head if it is domi- nated by a word outside of this sequence .
Note that this definition ( i ) allows for a word sequence to have one or more heads ( largely due to the fact that a word sequence is not necessarily linguistically constrained ) and ( ii ) ensures that heads are always the highest heads in the sequence from a dependency structure perspective .
For example , the word sequence ouzhou baguo lianming in Figure 1 has two heads ( i.e. , baguo and lianming , ouzhou is not a head of this sequence since its headword baguo falls within this sequence ) and the non-terminal corresponding to the sequence is thus labeled as NN - AD .
It is worth noting that in this paper we only refine non-terminal X on the source side to headinformed ones , while still using X on the target side .
According to the occurrence of terminals in translation rules , we group rules in the HD - HPB model into two categories : head - driven hierarchical rules ( HD - HRs ) and non-terminal reordering rules ( NRRs ) , where the former have at least one terminal on both source and target sides and the later have no terminals .
For rule extraction , we first identify initial phrase pairs on word-aligned sentence pairs by using the same criterion as most phrase - based translation models ( Och and Ney , 2004 ) and Chiang 's HPB model ( Chiang , 2005 ; Chiang , 2007 ) .
We extract HD - HRs and NRRs based on initial phrase pairs , respectively .
HD - HRs : Head-Driven Hierarchical Rules
As mentioned , a HD - HR has at least one terminal on both source and target sides .
This is the same as the hierarchical rules defined in Chiang 's HPB model ( Chiang , 2007 ) , except that we use head POSinformed non-terminal symbols in the source language .
We look for initial phrase pairs that contain other phrases and then replace sub-phrases with POS tags corresponding to their heads .
Given the word alignment in Figure 1 , Table 1 demonstrates the difference between hierarchical rules in Chiang ( 2007 ) and HD - HRs defined here .
Similar to Chiang 's HPB model , our HD - HPB model will result in a large number of rules causing problems in decoding .
To alleviate these problems , we filter our HD - HRs according to the same constraints as described in Chiang ( 2007 ) .
Moreover , we discard rules that have non-terminals with more than four heads .
NRRs : Non-terminal Reordering Rules NRRs are translation rules without terminals .
Given an initial phrase pair on the source side , there are four possible positional relationships for their target side translations ( we use Y as a variable for nonterminals on the source side while all non-terminals on the target side are labeled as X ) : Chiang ( 2007 ) and HD - HRs .
Indexed underlines indicate sub-phrases and corresponding non-terminal symbols .
The non-terminals in HD - HRs ( e.g. , NN , VV , VV - NR ) capture the head ( s ) POS tags of the corresponding word sequence in the source language .
? Monotone Y ? Y 1 Y 2 , X ? X 1 X 2 ; ? Discontinuous monotone Y ? Y 1 Y 2 , X ? X 1 . . . X 2 ; ? Swap Y ? Y 1 Y 2 , X ? X 2 X 1 ; ? Discontinuous swap Y ? Y 1 Y 2 , X ? X 2 . . . X Merging two neighboring non-terminals into a single non-terminal , NRRs enable the translation model to explore a wider search space .
During training , we extract four types of NRRs and calculate probabilities for each type .
To speed up decoding , we currently ( i ) only use monotone and swap NRRs and ( ii ) limit the number of non-terminals in a NRR to 2 .
Features and Decoding Given e for the translation output in the target language , s and t for strings of terminals and nonterminals on the source and target side , respectively , we use a feature set analogous to the default feature set of Chiang ( 2007 ) , including : ? P hd-hr ( t|s ) and P hd-hr ( s|t ) , translation probabilities for HD - HRs ; ? P lex ( t|s ) and P lex ( s|t ) , lexical translation probabilities for HD - HRs ; ?
P ty hd-hr = exp ( ? 1 ) , rule penalty for HD - HRs ; ? P nrr ( t | s ) , translation probability for NRRs ; ?
P ty nrr = exp ( ? 1 ) , rule penalty for NRRs ; ? P lm ( e ) , language model ; ?
P ty word ( e ) = exp ( ?|e| ) , word penalty .
Our decoder is based on CKY - style chart parsing with beam search and searches for the best derivation bottom - up .
For a source span [ i , j ] , it applies both types of HD - HRs and NRRs .
However , HD - HRs are only applied to generate derivations spanning no more than K words - the initial phrase length limit used in training to extract HD - HRswhile NRRs are applied to derivations spanning any length .
Unlike in Chiang 's HPB model , it is possible for a non-terminal generated by a NRR to be included afterwards by a HD - HR or another NRR .
Experiments
We evaluate the performance of our HD - HPB model and compare it with our implementation of Chiang 's HPB model ( Chiang , 2007 ) , a source- side SAMTstyle refined version of HPB ( SAMT - HPB ) , and the Moses implementation of HPB .
For fair comparison , we adopt the same parameter settings for our HD - HPB and HPB systems , including initial phrase length ( as 10 ) in training , the maximum number of non-terminals ( as 2 ) in translation rules , maximum number of non-terminals plus terminals ( as 5 ) on the source , beam threshold ? ( as 10 ?5 ) ( to discard derivations with a score worse than ? times the best score in the same chart cell ) , beam size b ( as 200 ) ( i.e. each chart cell contains at most b derivations ) .
For Moses HPB , we use " grow-diag-final - and " to obtain symmetric word alignments , 10 for the maximum phrase length , and the recommended default values for all other parameters .
We train our model on a dataset with ?1.5 M sentence pairs from the LDC dataset .
2
We use the 2002 NIST MT evaluation test data ( 878 sentence pairs ) as the development data , and the , 2004 , 2005 , 2006 - news NIST MT evaluation test data ( 919 , 1788 , 1082 , and 616 sentence pairs , respectively ) as the test data .
To find heads , we parse the source sentences with the Berkeley Parser 3 ( Petrov and Klein , 2007 ) trained on Chinese TreeBank 6.0 and use the Penn2 Malt toolkit 4 to obtain ( unlabeled ) dependency structures .
We obtain the word alignments by running GIZA ++ ( Och and Ney , 2000 ) on the corpus in both directions and applying " grow-diag-final - and " refinement ( Koehn et al. , 2003 ) .
We use the SRI language modeling toolkit to train a 5 - gram language model on the Xinhua portion of the Gigaword corpus and standard MERT ( Och , 2003 ) to tune the feature weights on the development data .
For evaluation , the NIST BLEU script ( version 12 ) with the default settings is used to calculate the BLEU scores .
To test whether a performance difference is statistically significant , we conduct significance tests following the paired bootstrap approach ( Koehn , 2004 ) .
In this paper , '* * ' and '* ' denote p-values less than 0.01 and in- between [ 0.01 , 0.05 ) , respectively .
Table 2 lists the rule table sizes .
The full rule table size ( including HD - HRs and NRRs ) of our HD - HPB model is ?1.5 times that of Chiang's , largely due to refining the non-terminal symbol X in Chiang 's model into head - informed ones in our model .
It is also unsurprising , that the test set-filtered rule table size of our model is only ?0.7 times that of Chiang's : this is due to the fact that some of the refined translation rule patterns required by the test set are unattested in the training data .
Furthermore , the rule table size of NRRs is much smaller than that of HD - HRs since a NRR contains only two non-terminals .
Table 3 lists the translation performance with BLEU scores .
Note that our re-implementation of Chiang 's original HPB model performs on a par with Moses HPB .
Table 3 shows that our HD - HPB model significantly outperforms Chiang 's HPB model with an average improvement of 1.91 in BLEU ( and similar improvements over Moses HPB ) .
Table 3 shows that the head- driven scheme outperforms a SAMT - style approach ( for each test set p < 0.01 ) , indicating that head information is more effective than ( partial ) CFG categories .
Taking lianming zhichi in Figure 1 as an example , HD - HPB labels the span VV , as lianming is dominated by zhichi , effecively ignoring lianming in the translation rule , while the SAMT label is ADVP : AD +VV 5 which is more susceptible to data sparsity .
In addition , SAMT resorts to X if a text span fails to satisify pre-defined categories .
extracted from the SAMT training data shows that 28 % of them are labeled as X .
In order to separate out the individual contributions of the novel HD - HRs and NRRs , we carry out an additional experiment ( HD - HR + Glue ) using HD - HRs with monotonic glue rules only ( adjusted to refined rule labels , but effectively switching off the extra reordering power of full NRRs ) .
Table 3 shows that on average more than half of the improvement over HPB ( Chiang and Moses ) comes from the refined HD - HRs , the rest from NRRs .
Examining translation rules extracted from the training data shows that there are 72,366 types of non-terminals with respect to 33 types of POS tags .
On average each sentence employs 16.6/5.2 HD - HRs / NRRs in our HD - HPB model , compared to 15.9/3.6 hierarchical rules / glue rules in Chiang 's model , providing further indication of the importance of NRRs in translation .
Conclusion
We present a head-driven hierarchical phrase - based ( HD - HPB ) translation model , which adopts head information ( derived through unlabeled dependency analysis ) in the definition of non-terminals to better differentiate among translation rules .
In ad-dition , improved and better integrated reordering rules allow better reordering between consecutive non-terminals through exploration of a larger search space in the derivation .
Experimental results on Chinese-English translation across four test sets demonstrate significant improvements of the HD - HPB model over both Chiang 's HPB and a sourceside SAMT - style refined version of HPB .
Figure 1 : 1 Figure 1 : An example word alignment for a Chinese - English sentence pair with the dependency parse tree for the Chinese sentence .
Here , each Chinese word is attached with its POS tag and Pinyin .
