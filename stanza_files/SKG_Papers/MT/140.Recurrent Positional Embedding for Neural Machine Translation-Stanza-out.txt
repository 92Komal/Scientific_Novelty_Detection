title
Recurrent Positional Embedding for Neural Machine Translation
abstract
In the Transformer network architecture , positional embeddings are used to encode order dependencies into the input representation .
However , this input representation only involves static order dependencies based on discrete numerical information , that is , are independent of word content .
To address this issue , this work proposes a recurrent positional embedding approach based on word vector .
In this approach , these recurrent positional embeddings are learned by a recurrent neural network , encoding word content - based order dependencies into the input representation .
They are then integrated into the existing multi-head selfattention model as independent heads or part of each head .
The experimental results revealed that the proposed approach improved translation performance over that of the stateof - the- art Transformer baseline in WMT '14 English - to - German and NIST Chinese - to - English translation tasks .
Introduction Transformer translation systems ( Vaswani et al. , 2017 ) , without recurrent and convolutional neural networks , rely on a positional embedding ( PE ) approach to encode order information into the input representation .
PE is typically learned based on the position index of each word and is added to corresponding word embedding .
This allows the Transformer to encode order dependencies between words in addition to the words themselves .
Finally , the Transformer uses these combined vectors as the input to selfattention networks ( SANs ) , achieving state - of - theart translation performance with several language pairs ( Vaswani et al. , 2017 ; Dou et al. , 2018 ; Marie et al. , 2018 Marie et al. , , 2019 . * Corresponding author
In spite of their success , the input representation only involves static order dependencies based on discrete numerical information .
That is , any word in the entire vocabulary has the same PE on the same position index .
As a result , the dependencies encoded by the original PEs are independent of word content , which may further hinder the improvement of translation capacity .
Recently , Chen et al. ( 2018 ) and Hao et al . ( 2019 ) introduced the additional source representation learned by an RNN - based encoder into Transformer to alleviate this issue , and reported improvements on the WMT '14 English - to - German translation task .
Inspired by their works ( Chen et al. , 2018 ; Hao et al. , 2019 ) , we propose an simple and efficient recurrent positional embedding approach to capture order dependencies based on word content in a sentence , thus learning a more effective sentence representation for the Transformer .
In addition , we designed two simple multi-head self-attentions to introduce these learned RPEs and original input representation into the existing Transformer model for enhancing the sentence representation of Transformer .
Experimental results on WMT '14 English - to - German and NIST Chinese- to - English translation tasks show that our models significantly improved translation performance over a strong Transformer baseline .
Background
Input Representation
In the Transformer network architecture ( Vaswani et al. , 2017 ) , given a sentence of length J , the positional embedding of each word is firstly computed based on its position : pe ( j,2i ) = sin( j/10000 2i / d model ) , pe ( j,2 i + 1 ) = cos( j/10000 2i / d model ) , ( 1 ) where j is the word 's numerical position index in the sentence and i is the dimension of position index .
The word embedding x j is then added with pe j to get a combined embedding v j : v j = x j + pe j . ( 2 ) As a result , there is a sequence of vectors { v 1 , ? ? ? , v J } as the input to the encoder or decoder of the Transformer to learn the source or target sentence representations .
Self-Attention Mechanism
Following the input layer , the self-attention layer is used to learn the sentence representation for the Transformer ( Vaswani et al. , 2017 ) .
These combined vectors { v 1 , ? ? ? , v J } are then packed into a query matrix Q , their keys and values matrices K and V .
The output of the self-attention layer can be computed by Eq. ( 3 ) :
Att ( Q , K , V ) = softmax ( QK T ? d model ) V. ( 3 ) O h = Att ( QW Q h , KW K h , VW V h ) , ( 4 ) where the parameter matrices are W Q h ?R d model ?d k , W K h ?R d model ?d k , and W V h ?R d model ? dv .
For example , if there are H=8 heads and d model is 512 , d k =d v = 512/8=64 .
Finally , the outputs of H heads are concatenated to serve as the sentence representation S : S = Concat ( O 1 , ? ? ? , O H?1 , O H )W O , ( 5 ) where W O ?
R Hdv?d model is a parameter matrix .
Recurrent Positional Embedding
We propose a recurrent positional embedding approach based on part of word embedding instead of numerical indices of words to capture order dependencies between words in a sentence .
Specifically , the embedding of each word x j is divided into two parts x p j and x r j , and their dimensions are d p and d r ( d model =d p +d r ) , respectively .
As a result , the sequence of word vectors {x 1 , ? ? ? , x J } are spited into { x p 1 , ? ? ? , x p J } and { x r 1 , ? ? ? , x r J }.
An RNN with an nonlinear projection layer is then designed to learn its recurrent state r j for each word over { x r 1 , ? ? ? , x r J } : r j = Tanh ( f RNN ( x r j , r j?1 ) W r + b r ) , ( 6 ) where W r ?
R dr?dr is a parameter matrix and b r ?
R dr is a bias item .
1 Note that the x r j is derived from part of the word embedding x j .
Finally , there is a sequence R ={r 1 , ? ? ? , r J } , called as recurrent positional embeddings ( RPEs ) .
In this work , a bidirectional RNN and a forward RNN ( Bahdanau et al. , 2015 ) are used to learn source RPEs and target RPEs , respectively .
Noth that the RNN is also replaced by other neural networks for learning order dependency information , such as GRU ( Cho et al. , 2014 ) , and SRU ( Li et al. , 2018a ) .
In addition , other sub-sequence { x p 1 , ? ? ? , x p J } is used to gain the reduced dimension input representation P={p 1 , ? ? ? , p J } according to the Section 2.1 .
Both of R and P will be together as the input to the encoder ( or decoder ) to learn a more effective source ( or target ) representation for the Transformer .
Neural Machine Translation with RPE
To make use of these learned RPEs , we propose two simple methods : RPE head ( RPEHead ) selfattention and mixed positional representation head ( MPRHead ) self-attention .
Both of RPEHead and MPRHead can utilize RPEs to learn sentence representation for the Transformer .
Second , the T is mapped to a new query matrix Q T , and their keys and values matrices K T and V T . According to Eq. ( 4 ) , the output of each head is computed by Eq. ( 7 ) :
RPEHead Self-Attention O T h = Att( Q T W Q h , K T W K h , V T W V h ) .
( 7 ) Therefore , the final sentence representation S T is formally represented as : S T = Concat( O T 1 , ? ? ? , O T H?1 , O T H )W O . ( 8 )
MPRHead Self-Attention Compared with the RPEHead model , the MPRHead model applies the multi-head mechanism to the RPEs .
In other words , RPEs are encoded into the sentence representation from different vector sub-spaces , as shown in Fig 1 ( c ) .
To this end , each vector of P is divided into the H heads {p 1 j , ? ? ? , p H j }.
Similarly , each vector of R is divided into the H heads {r 1 j , ? ? ? , r H j }.
The corresponding heads for p j and r j are then concatenated as a combined sequence pr j , in turn : pr j = {p 1 j : r 1 j , ? ? ? , p H j : r H j } , ( 9 ) where " : " is the concatenation operation .
All heads in pr j are further concatenated as a mixed embedding m j ?
R d model in turn .
As a result , there is a new sequence of mixed embeddings : M = {m 1 , ? ? ? , m J }. ( 10 ) The M is mapped to a query matrix Q M , and their keys and values matrices K M and V M . According to Eq. ( 4 ) and Eq. ( 5 ) , the final sentence representation S M is represented as : O M h = Att( Q M W Q h , K M W K h , V M W V h ) , S M = Concat ( O M 1 , ? ? ? , O M H?1 , O M H )W O . ( 11 ) Note that for both RPEHead and MPRHead models , the RPEs will be jointly learned with the existing Transformer architecture .
Experiments
Experimental Setup
The proposed methods were evaluated on the WMT '14 English to German ( EN - DE ) and NIST Chinese-to- English ( ZH - EN ) translation tasks .
The ZH-EN training set includes 1.28 million bilingual sentence pairs from the LDC corpora , where the NIST06 and the NIST02/NIST03 / NIST04 data sets were used as the development and test sets , respectively .
The EN - DE training set includes 4.43 million bilingual sentence pairs of the WMT '14 corpora , where the newstest2013 and newstest2014 data sets were used as the development and test sets , respectively .
The BPE ( Sennrich et al. , 2016 ) was adopted and the vocabulary size was set as 32K .
The dimension of all input and output layers was set to 512 , and that of the inner feedforward neural network layer was set to 2048 .
The total heads of all multi-head modules were set to 8 in both encoder and decoder layers .
In each training batch , there was a set of sentence pairs containing approximately 4096 * 4 source tokens and 4096 * 4 target tokens .
For the other setting not mentioned , we followed the setting in Vaswani et al . ( 2017 ) .
Baseline systems included a vanilla Transformer ( Vaswani et al. , 2017 ) , Relative PEs ( Shaw et al. , 2018 ) et al. , 2018 ) .
Besides , we reported results of the existing works ( Vaswani et al. , 2017 ; Chen et al. , 2018 ; Shaw et al. , 2018 ; Hao et al. , 2019 ; Liu et al. , 2019 ; . We reimplemented the baseline Transformer , Relative PEs , and DiSAN models on the OpenNMT toolkit ( Klein et al. , 2017 ) .
All the models were trained for 200k batches and evaluated on a single V100 GPU .
The multi-bleu.perl was used as the evaluation metric to obtain the case-sensitive 4 gram BLEU score of EN - DE and ZH -EN tasks .
Effect of RPEs
In this work , we extracted d r dimensions of each word vector to learn recurrent embeddings .
To explore the relation between d r and translation performance , Figure 2 shows the translation performance on the different d r .
For + RPEHead ( or + MPRHead ) , with the increasing in the dimension of d r , BLEU scores are gradually increasing , but BLEU scores begin to decrease when d r is more than 320 ( or 256 ) .
In particular , + RPEHead and + MPRHead achieve the highest BLEU scores at d r = 320 and d r = 256 , respectively .
This means that the original partial input representation and our RPE can complement each other to improve translation performance .
Main Results
According to the results of Fig 2 , d r of RPEHead is set to 320 and d r of MPRHead is set to 256 .
The main translation results are shown in Table 1 . 1 ) For the proposed methods , both RPEHead ( base ) and MPRHead ( base ) outperformed Transformer ( base ) , especially are better than + RPE and + DiSAN .
This indicates that the learned RPEs are beneficial for the Transformer system .
2 ) Moreover , + MPRHead ( base ) performed better than RPEHead / RPE ( base ) .
The reason may be that adding RPEs into variant heads can encode order dependencies based word vectors from variant vector sub-spaces , which is one of the advantages of the multi-head mechanism .
In particular , + MPRHead ( base / big ) is slightly better than Transformer ( base / big ) + BiARN .
This denotes that the RPEs can more effective improve the performance of Transformer .
3 ) MPRHead ( big ) was superior to Transformer ( big ) significantly .
MPRHead ( base ) achieved a comparable performance compared to Transformer ( big ) which contains approximately three times parameters , indicating that the proposed RPE is efficient .
In addition , Table 2 shows that the proposed models also gave similar improvements over the baseline system and the compared methods on the NIST ZH -EN task .
approach is a universal method for improving the translation of other language pairs .
Ablation Experiments
To further explore the effect of position information , PEs and RPEs were applied on the encoder ( Enc ) and decoder ( Dec ) side , respectively .
1 ) For Enc / Dec / Enc&Dec , the proposed RPE - Head / MPRHead outperformed the Transformer in corresponding ablation settings , indicating that our methods worked well on both the encoder and decoder .
2 ) For PE / RPEs , performance became slightly lower when PE was removed from the decoder .
However , when PE was removed from the encoder , the translation performance drastically decreased .
We think that the source sentence representation is more sensitive to position information than the target sentence representation .
It is possible that each hidden state in the decoder takes the previous hidden state into consideration , which would be somewhat similar to the proposed RPEs .
In contrast , the encoder would not be expected to contain this mechanism .
Therefore , the position information would be more important in the encoder than the decoder .
3 ) The training speeds of the proposed RPEHead ( base ) and MPRHead ( base ) were just slightly slower than those of the vanilla Transformer ( base ) , because we nearly did not introduce additional ( less than 2 % ) parameters .
Conclusion and Future Work
In this paper , we presented a recurrent embedding to capture the order dependencies in a sentence .
Empirical results show that this method can improve the performance of NMT efficiently .
In future work , we plan to extend this method to unsupervised NMT and other natural language processing tasks , such as dependency parsing ( Li et al. , 2018 b ) and reading comprehension ( Zhang et al. , 2018 b ) . Figure 1 : 1 Figure 1 : ( a) Multi-Head Self-Attention ; ( b) RPEHead Self-Attention ; ( c ) MPRHead Self-Attention .
