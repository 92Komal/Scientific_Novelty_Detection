title
PARANMT -50M : Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations
abstract
We describe PARANMT -50M , a dataset of more than 50 million English - English sentential paraphrase pairs .
We generated the pairs automatically by using neural machine translation to translate the non-English side of a large parallel corpus , following .
Our hope is that PARANMT - 50M can be a valuable resource for paraphrase generation and can provide a rich source of semantic knowledge to improve downstream natural language understanding tasks .
To show its utility , we use PARANMT - 50 M to train paraphrastic sentence embeddings that outperform all supervised systems on every SemEval semantic textual similarity competition , in addition to showing how it can be used for paraphrase generation .
1
Introduction
While many approaches have been developed for generating or finding paraphrases ( Barzilay and McKeown , 2001 ; Lin and Pantel , 2001 ; , there do not exist any freelyavailable datasets with millions of sentential paraphrase pairs .
The closest such resource is the Paraphrase Database ( PPDB ; Ganitkevitch et al. , 2013 ) , which was created automatically from bilingual text by pivoting over the non-English language ( Bannard and Callison - Burch , 2005 ) .
PPDB has been used to improve word embeddings ( Faruqui et al. , 2015 ; Mrk?i? et al. , 2016 ) .
However , PPDB is less useful for learning sentence embeddings .
In this paper , we describe the creation of a dataset containing more than 50 million sentential paraphrase pairs .
We create it automatically by scaling up the approach of .
We use neural machine translation ( NMT ) to translate the Czech side of a large Czech - English parallel corpus .
We pair the English translations with the English references to form paraphrase pairs .
We call this dataset PARANMT -50M .
It contains examples illustrating a broad range of paraphrase phenomena ; we show examples in Section 3 .
PARANMT -50M has the potential to be useful for many tasks , from linguistically controlled paraphrase generation , style transfer , and sentence simplification to core NLP problems like machine translation .
We show the utility of PARANMT - 50 M by using it to train paraphrastic sentence embeddings using the learning framework of Wieting et al . ( 2016 b ) .
We primarily evaluate our sentence embeddings on the SemEval semantic textual similarity ( STS ) competitions from 2012- 2016 .
Since so many domains are covered in these datasets , they form a demanding evaluation for a general purpose sentence embedding model .
Our sentence embeddings learned from PARANMT -50M outperform all systems in every STS competition from 2012 to 2016 .
These tasks have drawn substantial participation ; in 2016 , for example , the competition attracted 43 teams and had 119 submissions .
Most STS systems use curated lexical resources , the provided supervised training data with manually - annotated similarities , and joint modeling of the sentence pair .
We use none of these , simply encoding each sentence independently using our models and computing cosine similarity between their embeddings .
We experiment with several compositional architectures and find them all to work well .
We find benefit from making a simple change to learning ( " mega- batching " ) to better leverage the large training set , namely , increasing the search space of negative examples .
In the supplementary , we evaluate on general - purpose sentence embedding tasks used in past work ( Kiros et al. , 2015 ; Conneau et al. , 2017 ) , finding our embeddings to perform competitively .
Finally , in Section 6 , we briefly report results showing how PARANMT - 50 M can be used for paraphrase generation .
A standard encoderdecoder model trained on PARANMT -50M can generate paraphrases that show effects of " canonicalizing " the input sentence .
In other work , fully described by Iyyer et al . ( 2018 ) , we used PARANMT - 50 M to generate paraphrases that have a specific syntactic structure ( represented as the top two levels of a linearized parse tree ) .
We release the PARANMT -50M dataset , our trained sentence embeddings , and our code .
PARANMT -50 M is the largest collection of sentential paraphrases released to date .
We hope it can motivate new research directions and be used to create powerful NLP models , while adding a robustness to existing ones by incorporating paraphrase knowledge .
Our paraphrastic sentence embeddings are state - of - the - art by a significant margin , and we hope they can be useful for many applications both as a sentence representation function and as a general similarity metric .
Related Work
We discuss work in automatically building paraphrase corpora , learning general - purpose sentence embeddings , and using parallel text for learning embeddings and similarity functions .
Paraphrase discovery and generation .
Many methods have been developed for generating or finding paraphrases , including using multiple translations of the same source material ( Barzilay and McKeown , 2001 ) , using distributional similarity to find similar dependency paths ( Lin and Pantel , 2001 ) , using comparable articles from multiple news sources Dolan and Brockett , 2005 ; , aligning sentences between standard and Simple English Wikipedia ( Coster and Kauchak , 2011 ) , crowdsourcing ( Xu et al. , 2014 ( Xu et al. , , 2015 Jiang et al. , 2017 ) , using diverse MT systems to translate a single source sentence ( Suzuki et al. , 2017 ) , and using tweets with matching URLs ( Lan et al. , 2017 ) .
The most relevant prior work uses bilingual corpora .
Bannard and Callison- Burch ( 2005 ) used methods from statistical machine translation to find lexical and phrasal paraphrases in parallel text .
Ganitkevitch et al. ( 2013 ) scaled up these techniques to produce the Paraphrase Database ( PPDB ) .
Our goals are similar to those of PPDB , which has likewise been generated for many languages ( Ganitkevitch and Callison - Burch , 2014 ) since it only needs parallel text .
In particular , we follow the approach of , who used NMT to translate the non-English side of parallel text to get English - English paraphrase pairs .
We scale up the method to a larger dataset , produce state - of - the - art paraphrastic sentence embeddings , and release all of our resources .
Sentence embeddings .
Our learning and evaluation setting is the same as that of our recent work that seeks to learn paraphrastic sentence embeddings that can be used for downstream tasks ( Wieting et al. , 2016 b , a ; .
We trained models on noisy paraphrase pairs and evaluated them primarily on semantic textual similarity ( STS ) tasks .
Prior work in learning general sentence embeddings has used autoencoders ( Socher et al. , 2011 ; Hill et al. , 2016 ) , encoder-decoder architectures ( Kiros et al. , 2015 ; Gan et al. , 2017 ) , and other sources of supervision and learning frameworks ( Le and Mikolov , 2014 ; Pham et al. , 2015 ; Arora et al. , 2017 ; Pagliardini et al. , 2017 ; Conneau et al. , 2017 ) .
Parallel text for learning embeddings .
Prior work has shown that parallel text , and resources built from parallel text like NMT systems and PPDB , can be used for learning embeddings for words and sentences .
Several have used PPDB as a knowledge resource for training or improving embeddings ( Faruqui et al. , 2015 ; Wieting et al. , 2015 ; Mrk?i? et al. , 2016 ) . NMT architectures and training settings have been used to obtain better embeddings for words ( Hill et al. , 2014 a , b) and words- in- context ( McCann et al. , 2017 ) .
Hill et al. ( 2016 ) evaluated the encoders of Englishto -X NMT systems as sentence representations .
adapted trained NMT models to produce sentence similarity scores in semantic evaluations .
The PARANMT -50M Dataset
To create our dataset , we used back - translation of bitext .
We used a Czech-English NMT system to translate Czech sentences from the training data into English .
We paired the translations with the English references to form English - English paraphrase pairs .
We used the pretrained Czech-English model from the NMT system of .
Its training data includes four sources : Common Crawl , CzEng 1.6 ( Bojar et al. , 2016 ) , Europarl , and News Commentary .
We did not choose Czech due to any particular linguistic properties .
found little difference among Czech , German , and French as source languages for backtranslation .
There were much larger differences due to data domain , so we focus on the question of domain in this section .
We leave the question of investigating properties of back -translation of different languages to future work .
Choosing a Data Source
To assess characteristics that yield useful data , we randomly sampled 100K English reference translations from each data source and computed statistics .
Table 1 shows the average sentence length , the average inverse document frequency ( IDF ) where IDFs are computed using Wikipedia sentences , and the average paraphrase score for the two sentences .
The paraphrase score is calculated by averaging PARAGRAM - PHRASE embeddings ( Wieting et al. , 2016 b ) for the two sentences in each pair and then computing their cosine similarity .
The table also shows the entropies of the vocabularies and constituent parses obtained using the Stanford Parser .
2 Europarl exhibits the least diversity in terms of rare word usage , vocabulary entropy , and parse entropy .
This is unsurprising given its formulaic and repetitive nature .
CzEng has shorter sentences than the other corpora and more diverse sentence structures , as shown by its high parse entropy .
In terms of vocabulary use , CzEng is not particularly more diverse than Common Crawl and News Commentary , though this could be due to the prevalence of named entities in the latter two .
In Section 5.3 , we empirically compare these data sources as training data for sentence embeddings .
The CzEng corpus yields the strongest performance when controlling for training data size .
Since its sentences are short , we suspect this helps ensure high-quality back - translations .
A large portion of it is movie subtitles which tend to use a wide vocabulary and have a diversity of sentence structures ; however , other domains are included as well .
It is also the largest corpus , containing over 51 million sentence pairs .
In addition to providing a large number of training examples for downstream tasks , this means that the NMT system should be able to produce quality translations for this subset of its training data .
For all of these reasons , we chose the CzEng corpus to create PARANMT - 50M .
When doing so , we used beam search with a beam size of 12 and selected the highest scoring translation from the beam .
It took over 10,000 GPU hours to backtranslate the CzEng corpus .
We show illustrative examples in Table 2 .
Manual Evaluation
We conducted a manual analysis of our dataset in order to quantify its noise level and assess how the Para .
Score # Avg. Tri. Paraphrase Fluency Range ( M ) Overlap 1 2 3 1 2 3 ( -0.1 , 0.2 ] 4.0 0.00?0.0 92 6 2 1 5 94 ( 0.2 , 0.4 ]
3.8 0.02?0.1 53 32 15 1 12 87 ( 0.4 , 0.6 ] 6.9 0.07?0.1 22 45 33 2 9 89 ( 0.6 , 0.8 ] 14.4 0.17?0.2 1 43 56 11 0 89 ( 0.8 , 1.0 ]
18.0 0.35?0.2 1 13 86 3 0 97 Table 3 : Manual evaluation of PARANMT -50M. 100 - pair samples were drawn from five ranges of the automatic paraphrase score ( first column ) .
Paraphrase strength and fluency were judged on a 1 - 3 scale and counts of each rating are shown .
noise can be ameliorated with filtering .
Two native English speakers annotated a sample of 100 examples from each of five ranges of the Paraphrase Score .
3
We obtained annotations for both the strength of the paraphrase relationship and the fluency of the translations .
To annotate paraphrase strength , we adopted the annotation guidelines used by Agirre et al . ( 2012 ) .
The original guidelines specify six classes , which we reduced to three for simplicity .
We combined the top two into one category , left the next , and combined the bottom three into the lowest category .
Therefore , for a sentence pair to have a rating of 3 , the sentences must have the same meaning , but some unimportant details can differ .
To have a rating of 2 , the sentences are roughly equivalent , with some important information missing or that differs slightly .
For a rating of 1 , the sentences are not equivalent , even if they share minor details .
For fluency of the back - translation , we use the following :
A rating of 3 means it has no grammatical errors , 2 means it has one to two errors , and 1 means it has more than two grammatical errors or is not a natural English sentence .
Table 3 summarizes the annotations .
For each score range , we report the number of pairs , the mean trigram overlap score , and the number of times each paraphrase / fluency label was present in the sample of 100 pairs .
There is noise but it is largely confined to the bottom two ranges which together comprise only 16 % of the entire dataset .
In the highest paraphrase score range , 86 % of the pairs possess a strong paraphrase relationship .
The annotations suggest that PARANMT -50M contains approximately 30 million strong paraphrase pairs , and that the paraphrase score is a good indi-cator of quality .
At the low ranges , we inspected the data and found there to be many errors in the sentence alignment in the original bitext .
With regards to fluency , approximately 90 % of the backtranslations are fluent , even at the low end of the paraphrase score range .
We do see an outlier at the second- highest range of the paraphrase score , but this may be due to the small number of annotated examples .
Learning Sentence Embeddings
To show the usefulness of the PARANMT - 50M dataset , we will use it to train sentence embeddings .
We adopt the learning framework from Wieting et al . ( 2016 b ) , which was developed to train sentence embeddings from pairs in PPDB .
We first describe the compositional sentence embedding models we will experiment with , then discuss training and our modification ( " megabatching " ) .
Models .
We want to embed a word sequence s into a fixed - length vector .
We denote the tth word in s as s t , and we denote its word embedding by x t .
We focus on three model families , though we also experiment with combining them in various ways .
The first , which we call WORD , simply averages the embeddings x t of all words in s .
This model was found by Wieting et al . ( 2016 b ) to perform strongly for semantic similarity tasks .
The second is similar to WORD , but instead of word embeddings , we average character trigram embeddings ( Huang et al. , 2013 ) .
We call this TRIGRAM .
Wieting et al. ( 2016a ) found this to work well for sentence embeddings compared to other n-gram orders and to word averaging .
The third family includes long short-term memory ( LSTM ) networks ( Hochreiter and Schmidhuber , 1997 ) .
We average the hidden states to produce the final sentence embedding .
For regularization during training , we scramble words with a small probability .
We also experiment with bidirectional LSTMs ( BLSTM ) , averaging the forward and backward hidden states with no concatenation .
4 Training .
The training data is a set S of paraphrase pairs s , s and we minimize a margin- based loss ( s , s ) = max ( 0 , ? ? cos ( g ( s ) , g( s ) ) + cos ( g ( s ) , g ( t ) ) ) where g is the model ( WORD , TRIGRAM , etc. ) , ? is the margin , and t is a " negative example " taken from a mini-batch during optimization .
The intuition is that we want the two texts to be more similar to each other than to their negative examples .
To select t we choose the most similar sentence in some set .
For simplicity we use the mini- batch for this set , i.e. , t = argmax t : t , ?
?S b \{ s , s } cos ( g ( s ) , g( t ) ) where S b ?
S is the current mini-batch .
Modification : mega-batching .
By using the mini-batch to select negative examples , we may be limiting the learning procedure .
That is , if all potential negative examples in the mini-batch are highly dissimilar from s , the loss will be too easy to minimize .
Stronger negative examples can be obtained by using larger mini-batches , but large mini-batches are sub-optimal for optimization .
Therefore , we propose a procedure we call " mega- batching . "
We aggregate M mini-batches to create one mega-batch and select negative examples from the mega-batch .
Once each pair in the mega-batch has a negative example , the megabatch is split back up into M mini-batches and training proceeds .
We found that this provides more challenging negative examples during learning as shown in Section 5.5 .
Table 6 shows results for different values of M , showing consistently higher correlations with larger M values .
Experiments
We now investigate how best to use our generated paraphrase data for training paraphrastic sentence embeddings .
Evaluation
We evaluate sentence embeddings using the Sem-Eval semantic textual similarity ( STS ) tasks from 2012 to 2016 ( Agirre et al. , 2012 ( Agirre et al. , , 2013 ( Agirre et al. , , 2014 ( Agirre et al. , , 2015 ( Agirre et al. , , 2016 and the STS Benchmark ( Cer et al. , 2017 ) .
Given two sentences , the aim of the STS tasks is to predict their similarity on a 0 - 5 scale , where 0 indicates the sentences are on different topics and 5 means they are completely equivalent .
over each year of the STS tasks from 2012- 2016 .
We use the small ( 250 - example ) English dataset from SemEval 2017 ( Cer et al. , 2017 ) as a development set , which we call STS2017 below .
The supplementary material contains a description of a method to obtain a paraphrase lexicon from PARANMT - 50 M that is on par with that provided by PPDB 2.0 .
We also evaluate our sentence embeddings on a range of additional tasks that have previously been used for evaluating sentence representations ( Kiros et al. , 2015 ) .
Experimental Setup
For training sentence embeddings on PARANMT - 50M , we follow the experimental procedure of Wieting et al . ( 2016 b ) .
We use PARAGRAM -SL999 embeddings ( Wieting et al. , 2015 ) to initialize the word embedding matrix for all models that use word embeddings .
We fix the mini-batch size to 100 and the margin ? to 0.4 .
We train all models for 5 epochs .
For optimization we use Adam ( Kingma and Ba , 2014 ) with a learning rate of 0.001 .
For the LSTM and BLSTM , we fixed the scrambling rate to 0.3 . 5
Dataset Comparison
We first compare parallel data sources .
We evaluate the quality of a data source by using its backtranslations paired with its English references as training data for paraphrastic sentence embeddings .
We compare the four data sources described in Section 3 .
We use 100K samples from each corpus and trained 3 different models on each : WORD , TRIGRAM , and LSTM .
CzEng is diverse in terms of both vocabulary and sentence structure .
It has significantly shorter sentences than the other corpora , and has much more training data , so its translations are expected to be better than those in the other corpora .
found that sentence length was the most important factor in filtering quality training data , presumably due to how NMT quality deteriorates with longer sentences .
We suspect that better translations yield better data for training sentence embeddings .
Data Filtering Since the PARANMT -50M dataset is so large , it is computationally demanding to train sentence embeddings on it in its entirety .
So , we filter the data to create a training set for sentence embeddings .
We experiment with three simple methods : ( 1 ) the length - normalized translation score from decoding , ( 2 ) trigram overlap , and ( 3 ) the paraphrase score from Section 3 .
Trigram overlap is calculated by counting trigrams in the reference and translation , then dividing the number of shared trigrams by the total number in the reference or translation , whichever has fewer .
We filtered the back- translated CzEng data using these three strategies .
We ranked all 51 M + paraphrase pairs in the dataset by the filtering measure under consideration and then split the data into tenths ( so the first tenth contains the bottom 10 % under the filtering criterion , the second contains those in the bottom 10 - 20 % , etc. ) .
We trained WORD , TRIGRAM , and LSTM models for a single epoch on 1M examples sampled from each of the ten folds for each filtering criterion .
We averaged the correlation on the STS2017 data across models for each fold .
Table 5 shows the results of the filtering methods .
Filtering based on the paraphrase score produces the best data for training sentence embeddings .
We randomly selected 5 M examples from the top two scoring folds using paraphrase score fil -
Effect of Mega-Batching Table 6 shows the impact of varying the megabatch size M when training for 5 epochs on our 5 M - example training set .
For all models , larger mega-batches improve performance .
There is a smaller gain when moving from 20 to 40 , but all models show clear gains over M = 1 .
Table 7 shows negative examples with different mega-batch sizes M .
We use the BLSTM model and show the negative examples ( nearest neighbors from the mega- batch excluding the current training example ) for three sentences .
Using larger mega-batches improves performance , presumably by producing more compelling negative examples for the learning procedure .
This is likely more important when training on sentences than ( Conneau et al. , 2017 ) 4096 70.6 C-PHRASE ( Pham et al. , 2015 63.9 GloVe ( Pennington et al. , 2014 ) 300 40.6 word2vec ( Mikolov et al. , 2013 ) 300 56.5 sent2vec ( Pagliardini et al. , 2017 ) 700 75.5 Related Work ( Supervised ) Dep. Tree LSTM ( Tai et al. , 2015 ) 71.2 Const .
Tree LSTM ( Tai et al. , 2015 ) 71.9 CNN ( Shao , 2017 ) 78.4 Table 9 : Results on STS Benchmark test set .
prior work on learning from text snippets ( Wieting et al. , 2015 ( Wieting et al. , , 2016 b
Pham et al. , 2015 ) .
Model Comparison
Table 8 shows results on the 2012 - 2016 STS tasks and Table 9 shows results on the STS Benchmark .
Table 10 : The means ( over all 25 STS competition datasets ) of the absolute differences in Pearson 's r between each pair of models .
aware on the 2012 - 2016 STS datasets .
Note that the large improvement over BLEU and METEOR suggests that our embeddings could be useful for evaluating machine translation output .
Overall , our individual models ( WORD , TRI - GRAM , LSTM ) perform similarly .
Using 300 dimensions appears to be sufficient ; increasing dimensionality does not necessarily improve correlation .
When examining particular STS tasks , we found that our individual models showed marked differences on certain tasks .
Table 10 shows the mean absolute difference in Pearson 's r over all 25 datasets .
The TRIGRAM model shows the largest differences from the other two , both of which use word embeddings .
This suggests that TRIGRAM may be able to complement the other two by providing information about words that are unknown to models that rely on word embeddings .
We experiment with two ways of combining models .
The first is to define additive architectures Target Syntax Paraphrase original with the help of captain picard , the borg will be prepared for everything . ( SBARQ ( ADVP ) ( , ) ( S ) ( , ) ( SQ ) ) now , the borg will be prepared by picard , will it ? ( S( NP ) ( ADVP ) ( VP ) ) the borg here will be prepared for everything .
original you seem to be an excellent burglar when the time comes . ( S( SBAR ) ( , ) ( NP ) ( VP ) ) when the time comes , you 'll be a great thief . ( S ( ' ' ) ( UCP ) ( ' ' ) ( NP ) ( VP ) ) " you seem to be a great burglar , when the time comes . " you said .
Table 11 : Syntactically controlled paraphrases generated by the SCPN trained on PARANMT -50M .
that form the embedding for a sentence by adding the embeddings computed by two ( or more ) individual models .
All parameters are trained jointly just like when we train individual models ; that is , we do not first train two simple models and add their embeddings .
The second way is to define concatenative architectures that form a sentence embedding by concatenating the embeddings computed by individual models , and again to train all parameters jointly .
In Table 8 and Table 9 , these combinations show consistent improvement over the individual models as well as the larger LSTM and BLSTM .
Concatenating WORD and TRIGRAM results in the best performance on average across STS tasks , outperforming the best supervised systems from each year .
We have released the pretrained model for these " WORD , TRIGRAM " embeddings .
In addition to providing a strong baseline for future STS tasks , these embeddings offer the advantages of being extremely efficient to compute and being robust to unknown words .
We show the usefulness of PARANMT by also reporting the results of training the " WORD , TRI - GRAM " model on SimpWiki , a dataset of aligned sentences from Simple English and standard English Wikipedia ( Coster and Kauchak , 2011 ) .
It has been shown useful for training sentence embeddings in past work .
However , Table 8 shows that training on PARANMT leads to gains in correlation of 3 to 6 points compared to SimpWiki .
Paraphrase Generation
In addition to powering state - of- the - art paraphrastic sentence embeddings , our dataset is useful for paraphrase generation .
We briefly describe two efforts in paraphrase generation here .
We have found that training an encoder-decoder model on PARANMT - 50 M can produce a paraphrase generation model that canonicalizes text .
For this experiment , we used a bidirectional LSTM encoder and a two -layer decoder original overall , i that it 's a decent buy , and am happy that i own it .
paraphrase it 's a good buy , and i'm happy to own it .
original oh , that 's a handsome women , that is .
paraphrase that 's a beautiful woman .
with soft attention over the encoded states ( Bahdanau et al. , 2015 ) .
The attention computation consists of a bilinear product with a learned parameter matrix .
Table 12 shows examples of output generated by this model , showing how the model is able to standardize the text and correct grammatical errors .
This model would be interesting to evaluate for automatic grammar correction as it does so without any direct supervision .
Future work could also use this canonicalization to improve performance of models by standardizing inputs and removing noise from data .
PARANMT -50M has also been used for syntactically - controlled paraphrase generation ; this work is described in detail by Iyyer et al . ( 2018 ) .
A syntactically controlled paraphrase network ( SCPN ) is trained to generate a paraphrase of a sentence whose constituent structure follows a provided parse template .
A parse template contains the top two levels of a linearized parse tree .
Table 11 shows example outputs using the SCPN .
The paraphrases mostly preserve the semantics of the input sentences while changing their syntax to fit the target syntactic templates .
The SCPN was used for augmenting training data and finding adversarial examples .
We believe that PARANMT -50M and future datasets like it can be used to generate rich paraphrases that improve the performance and robustness of models on a multitude of NLP tasks .
Discussion
One way to view PARANMT -50 M is as a way to represent the learned translation model in a mono-lingual generated dataset .
This raises the question of whether we could learn an effective sentence embedding model from the original parallel text used to train the NMT system , rather than requiring the intermediate step of generating a paraphrase training set .
However , while Hill et al . ( 2016 ) and used trained NMT models to produce sentence similarity scores , their correlations are considerably lower than ours ( by 10 % to 35 % absolute in terms of Pearson ) .
It appears that NMT encoders form representations that do not necessarily encode the semantics of the sentence in a way conducive to STS evaluations .
They must instead create representations suitable for a decoder to generate a translation .
These two goals of representing sentential semantics and producing a translation , while likely correlated , evidently have some significant differences .
Our use of an intermediate dataset leads to the best results , but this may be due to our efforts in optimizing learning for this setting ( Wieting et al. , 2016 b ; . Future work will be needed to develop learning frameworks that can leverage parallel text directly to reach the same or improved correlations on STS tasks .
Conclusion
We described the creation of PARANMT - 50M , a dataset of more than 50M English sentential paraphrase pairs .
We showed how to use PARANMT - 50 M to train paraphrastic sentence embeddings that outperform supervised systems on STS tasks , as well as how it can be used for generating paraphrases for purposes of data augmentation , robustness , and even grammar correction .
The key advantage of our approach is that it only requires parallel text .
There are hundreds of millions of parallel sentence pairs , and more are being generated continually .
Our procedure is immediately applicable to the wide range of languages for which we have parallel text .
We release PARANMT - 50M , our code , and pretrained sentence embeddings , which also exhibit strong performance as general - purpose representations for a multitude of tasks .
We hope that PARANMT - 50M , along with our embeddings , can impart a notion of meaning equivalence to improve NLP systems for a variety of tasks .
We are actively investigating ways to apply these two new resources to downstream applications , including machine translation , question answering , and additional paraphrase generation tasks .
Table 1 : 1 Statistics of 100K - samples of Czech-English parallel corpora ; standard deviations are shown for averages .
Dataset Avg. Length Avg. IDF Avg. Para .
Score Vocab .
Entropy Parse Entropy Total Size Common Crawl 24.0?34.7 7.7?1.1 0.83?0.16 7.2 3.5 0.16 M CzEng 1.6 13.3?19.3 7.4?1.2 0.84?0.16 6.8 4.1 51.4 M Europarl 26.1?15.4 7.1?0.6 0.95?0.05 6.4 3.0 0.65 M News Commentary 25.2?13.9 7.5?1.1 0.92?0.12 7.0 3.4 0.19 M
Reference Translation Machine Translation so , what 's half an hour ?
half an hour wo n't kill you .
well , do n't worry .
i've taken out tons and tons of guys .
lots of guys .
do n't worry , i've done it to dozens of men .
it 's gonna be ...... classic .
yeah , sure .
it 's gonna be great .
greetings , all !
hello everyone !
but she does n't have much of a case .
but as far as the case goes , she does n't have much .
it was good in spite of the taste .
despite the flavor , it felt good .
