title
Document-level Neural Machine Translation Using BERT as Context Encoder
abstract
Large-scale pre-trained representations such as BERT have been widely used in many natural language understanding tasks .
The methods of incorporating BERT into documentlevel machine translation are still being explored .
BERT is able to understand sentence relationships ( Devlin et al. , 2019 ) since BERT is pre-trained using the next sentence prediction task .
In our work , we leverage this property to improve document - level machine translation .
In our proposed model , BERT performs as a context encoder to achieve documentlevel contextual information , which is then integrated into both the encoder and decoder .
Experiment results show that our proposed method can significantly outperform strong document - level machine translation baselines on BLEU score .
Moreover , the ablation study shows our method can capture document - level context information to boost translation performance .
Introduction Recent years have witnessed the great success of neural machine translation ( NMT ) Bahdanau et al. , 2014 ; Vaswani et al. , 2017 ) . NMT systems have even achieved human parity on resource - rich language pairs ( Hassan et al. , 2018 ) .
However , standard NMT systems perform translation only at the sentence level , which ignores the dependencies among sentences when translating entire documents .
To address the above challenges , various document- level NMT models , have been proposed to extract contextual information from surrounding sentences and have achieved substantial improvements in generating consistent translations ( Voita et al. , 2018 ; Zhang et al. , 2018 ; Werlen et al. , 2018 ; Maruf et al. , 2019 ; Ma et al. , 2020 ) .
Large-scale pre-trained text representations like GPT - 2 ( Radford et al. , 2018 ( Radford et al. , , 2019 , BERT ( Devlin et al. , 2019 ) , RoBERTa , ALBERT ( Lan et al. , 2019 ) , have been widely used in many natural language understanding tasks .
Among them , BERT is one of the most effective representations that has inspired many other representations such as RoBERTa , ALBERT .
It significantly boosts the performance of many natural language understanding tasks , including text classification , question answering , etc ( Devlin et al. , 2019 ) .
There have been few recent attempts to incorporate BERT into NMT models ( Xiong et al. , 2019 ; Weng et al. , 2019 ; Chen et al. , 2020 ) .
Intuitively , as one of BERT 's pre-training tasks is the binarized next sentence prediction ( NSP ) task , a natural assumption is that the NSP task has enabled the model to understand the relationship between two sentences , the relationship information is helpful to model the context information for documentlevel machine translation .
In this work , we propose to extend the Transformer model to take advantage of BERT document - level contextual representation .
We use the pre-trained BERT as a context encoder to achieve document - level representation , which is then integrated into both the encoder and the decoder of Transformer NMT model .
We use a multihead attention mechanism and context gate to control how each layer interacts with BERT context representations adaptively .
We conducted experiments on two documentlevel machine translation datasets .
Experimental results show that our proposed model can outperform Transformer baselines and previous state - of- theart document - level NMT models on BLEU score .
Also , we perform an ablation study showing that the BERT context encoder can capture documentlevel context representation to improve translation performance .
Formally , denote X = x 1 , x 2 , . . . , x N as a sourcelanguage document with N source sentences .
The corresponding target - language document is denoted by Y = y 1 , y 2 , . . . , y N .
We assume that ( x i , y i ) is a parallel sentence pair .
Following ( Zhang et al. , 2018 ) , we omit the target - side document - level context y < i because of the translation error propagation problem , and source side document - level context x < i conveys the same information with y < i .
Therefore , the probability can be approximated as : P ( Y | X ; ? ) ? N i=1 P ( y i | x i ; x < i ; x >i ; ? ) ( 1 ) where x i is the source sentence aligned to y i , x < i and x >i are the document - level context sentences used to translate y i .
BERT Context Encoder
The context encoder is a BERT model .
The input x ctx of BERT is the concatenation of current sentence x i and document- level context sentences ( x < i , x >i ) as follows : Where [ CLS ] and [ SEP ] are special tokens for BERT .
The context input x ctx is encoded by BERT into document - level context representation C B = BERT ( x ) .
C B is the output from the last layer of BERT .
BERT Context Representation Integration Inspired by ( Zhang et al. , 2018 ; , we use multi-head attention to integrate BERT context representation C B into both the encoder and the decoder of Transformer NMT model .
Integration into the Encoder
As shown in Figure 1 , we follow Vaswani et al . ( 2017 ) using a stack of L identical layers to encode x i .
Every layer consists of two attention models with different parameters .
The first attention model is a multi-head self-attention : B ( l ) = M ultiHead S ( l?1 ) , S ( l?1 ) , S ( l?1 ) ( 2 ) where S ( 0 ) denotes the word embedding of sequence x i .
The second attention model is context 103 attention that integrate BERT document- level context into the encoder : D ( l ) = M ultiHead S ( l?1 ) , C B , C B ( 3 ) If we directly combine the outputs of the two attention mechanisms , the influence of document - level context will be enhanced in an uncontrolled way as the context information will be added to every layer .
Also , different source sentences require different amount of context information for translation .
Inspired by context gate in Werlen et al . ( 2018 ) ; Zhang et al. ( 2018 ) , we propose to use context gate to combine the output of the two attention mechanisms .
g l = ?
W l g B ( l ) , D ( l ) + b l g A ( l ) = g l B ( l ) + 1 ? g l D ( l ) ( 4 ) Where ? is a sigmoid function .
Then the combination is further processed by a position - wise fully connected feed-forward neural network F F N ( . ) : S ( l ) = F F N ( A ( l ) ) ( 5 ) S ( l ) is the representation for the source sentence x i and its context at the l-th layer .
Integration into the decoder Similar to the encoder layer , we use context gate and attention mechanism to integrate the BERT document - level context representation into standard Transformer decoder .
In the l-th layer , E ( l ) = M ultiHead T ( l?1 ) , T ( l?1 ) , T ( l?1 ) F ( l ) = M ultiHead E ( l ) , C B , C B G ( l ) = M ultiHead E ( l ) , S ( L ) , S ( L ) d l = ?
W l d F ( l ) , G ( l ) + b l d H ( l ) = d l F ( l ) + 1 ? d l G ( l ) T ( l ) = F F N ( H ( l ) ) ( 6 ) After achieving the final representations of the last decoder layer T ( L ) , the output probability of the current target sentence y i are computed as : 3 Experiments p ( y i | x i , x < i , x >i ) = t p ( y i , t | y i , ?t , x i , x < i , x >i ) = t sof tmax E [ y i , t ] T L i , t (
Dataset
We use two English - German datasets as the benchmark datasets , which are TED and News .
The corpora statistics are shown in Table 1 . ? TED : This corpus is from the IWSLT 2017 MT track ( Cettolo et al. , 2012 ) aligned at the sentence level .
Every TED talk is treated as a document .
?
News Commentary :
This corpus is from document- delimited News Commentary v11 1 aligned at the sentence level .
We obtain the processed datasets from Maruf et al . ( 2019 ) 2 . We use the same train / valid / test datasets with Maruf et al . ( 2019 ) , so that our results can be compared with previous work .
We use the script of Moses toolkit 3 to tokenize the sentence .
We use byte pair encoding ( Sennrich et al. , 2016 ) to segment all sentences with 30 K merge operations .
The evaluation metrics is BLEU ( Papineni et al. , 2002 ) .
Implementation Details Firstly ( Maruf et al. , 2019 ) 24.62 24.84 QCN ( Yang et al. , 2019 b ) 25.19 22.37 Doc-Transformer ( Zhang et al. , 2018 ) ( Vaswani et al. , 2017 ) with 4000 warmup steps .
The batch size is limited to 4000 tokens .
We also apply label smoothing to the cross-entropy loss , and the smoothing rate is 0.1 .
Our Transformer implementation is based on Fairseq .
Experimental results
We list the results of our experiments in Table 2 , comparing six context - aware NMT models .
For Document - aware Transformer ( Zhang et al. , 2018 ) , Hierarchical Attention NMT ( Werlen et al. , 2018 ) , Selective Attention NMT ( Maruf et al. , 2019 ) and Query - guided Capsule Network ( Yang et al. , 2019 b ) , Flat-Transformer ( Ma et al. , 2020 , using BERT to initialize the encoder of Flat-Transformer ( + BERT ) .
Most of the previous work 's results are from Ma et al . ( 2020 ) , except BERTfused .
The result of BERTfused is my re-implementation using the current sentence and one previous sentence as BERT input .
The reproduced Transformer uses the 4 - layers setting , which is the same as our proposed model .
As shown in Our model achieved significant improvement on the News dataset , but relatively smaller gains on the TED dataset and have n't achieved state - of - theart performance .
Since BERT is pre-trained on BooksCorpus and Wikipedia , and the document in News dataset is more similar to the pre-training corpus , BERT can better encode context information on News dataset .
Ablation study Effect of Context Integration Table 3 shows the effect of integrating BERT context representation into the encoder and the decoder .
We can find that integrating BERT context representation into the encoder brings more improvements , it is also beneficial to integrate representation into the decoder .
The results indicate that the BERT context representation should be integrated into both encoder and decoder to achieve better performance .
Does the BERT encoder really capture the contextual information ?
Yes . ? Context : Concatenation of the previous sentence and the current sentence .
?
Random : Concatenation of a sentence consisting of words randomly selected from the source vocabulary and the current sentence .
?
Fixed : Concatenation of a fixed sentence and the current sentence .
As shown in Table 4 , the performance of Random and Fixed decrease due to the incorrect context , which is different from the result in .
This indicates that our proposed model can really capture the contextual information .
Although the performance of Random and Fixed decreases , they can still outperform the standard Transformer model significantly .
This is because current sentence usually plays a more important role in target sentence generation , and our proposed model can leverage the representation of current sentence obtained by BERT as extra representation .
Related Work Document- level NMT
Document- level NMT models incorporate the document-level contextual information to generate more consistent and coherent translations compared with sentence - level NMT models .
Most of the existing documentlevel NMT models can be divided into two categories : Uni-encoder models ( Tiedemann and Scherrer , 2017 ; Li et al. , 2019 ; Ma et al. , 2020 ) and dual-encoder models ( Voita et al. , 2018 ; Zhang et al. , 2018 ; Werlen et al. , 2018 ; Maruf et al. , 2019 ; Yang et al. , 2019 b ) . Uni-encoder models ( Tiedemann and Scherrer , 2017 ; Li et al. , 2019 ; Ma et al. , 2020 ) take the concatenation of contexts and source sentences as the input .
Dual-encoder ( Voita et al. , 2018 ; Zhang et al. , 2018 ; Werlen et al. , 2018 ; Maruf et al. , 2019 ; Yang et al. , 2019 b ) exploits the representation from BERT by integrating it into all layers of Transformer model .
BERT - fused model can also be extended to document- level NMT , but our work is different in the modeling and experimental part .
While are mainly focusing on improving sentence - level machine translation performance , they proposed a drop-net trick to combine the output of BERT encoder and the standard Transformer encoder , our proposed context gate combination can better leverage document - level context information since it is more correspond to the fact that different source sentences require a different amount of context information for translation .
Conclusion
We have presented a method for leveraging BERT to capture contextual information for documentlevel neural machine translation .
Experiments on two document- level machine translation tasks demonstrate the effectiveness of our approach .
Besides , we have shown that our approach can really capture the context information to improve the translation performance .
For future work , we plan to compress our model into a light version to leverage more context sen -106 tences .
Also , we plan to do experiments on largescale datasets and some other language pairs like Chinese - English .
Figure 1 : 1 Figure 1 : Illustration of using BERT as context encoder for document- level NMT model .
C B denote the output of BERT context encoder , S ( L ) denote the last layer output of Transformer encoder
