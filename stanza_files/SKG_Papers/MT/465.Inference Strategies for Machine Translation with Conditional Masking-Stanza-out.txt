title
Inference Strategies for Machine Translation with Conditional Masking
abstract
Conditional masked language model ( CMLM ) training has proven successful for nonautoregressive and semi-autoregressive sequence generation tasks , such as machine translation .
Given a trained CMLM , however , it is not clear what the best inference strategy is .
We formulate masked inference as a factorization of conditional probabilities of partial sequences , show that this does not harm performance , and investigate a number of simple heuristics motivated by this perspective .
We identify a thresholding strategy that has advantages over the standard " mask - predict " algorithm , and provide analyses of its behavior on machine translation tasks .
Introduction
The widely successful masked language modeling paradigm popularized by BERT ( Devlin et al. , 2019 ) has recently been adapted to conditional masked language model ( CMLM ) training for semiautoregressive sequence generation ( Ghazvininejad et al. , 2019 ) , where model predictions are conditioned on the complete input sequence and the observed ( non- masked ) portion of the output sequence .
The CMLM 's simplicity and its clear links to the very active field of linguistic representation learning are advantages over its semiautoregressive competitors , such as iterative refinement of token sequences ( Lee et al. , 2018 ) , refinement of non-linguistic intermediate representations ( Kaiser et al. , 2018 ; Shu et al. , 2020 ) and learning to predict parallel edit operations ( Stern et al. , 2019 ; Gu et al. , 2019 ) .
It is not obvious how to best perform inference with the CMLM .
Starting from a partially - observed output sequence , the optimal choice to complete it within a single step would be to generate the most likely token at each unobserved ( masked ) position independently .
However , it is less clear how to progress from an initial , completely masked sequence to a final hypothesis semi-autoregressively over a number of steps , with each successive step unmasking new context for the next .
This requires not only ordering the tokens for generation , but also making decisions about how many tokens to simultaneously predict in each step .
Ghazvininejad et al. ( 2019 ) propose the maskpredict algorithm , which iteratively generates fresh model predictions for all masked positions , and then unmasks a predefined number of the most likely predictions .
Given a fixed number of iterations , a decaying schedule determines how many predictions to unmask in each iteration .
Each successive iteration provides mode-breaking ( Gu et al. , 2018 ) context for the next .
By fixing the number of iterations , this approach allows for constant - time semi-autoregressive decoding .
The fixed - iteration strategy is very practical and has yielded empirical success in a range of machine translation experiments , but there is no guarantee that it is optimal .
The tokens to be unmasked on a given iteration are all predicted independently , and therefore might contain repeated words , or words with low model confidence .
These issues can be mitigated by later re-masking a token to repair it ( Ghazvininejad et al. , 2019 ) or by adapting the model to incorrect contexts ( Ghazvininejad et al. , 2020 ) .
We instead adopt a fully probabilistic view of the masked prediction sequence , which we enable by simply disallowing the re-masking of previously unmasked tokens .
This view guides us to a heuristic inference schedule that selects sets of unmasked tokens according to a threshold on the product of their conditionally independent model probabilities .
This heuristic naturally slows down in the situations mentioned above , and speeds up in the presence of high confidence , which allows us to achieve favorable quality - to-speed trade- offs .
We focus on strengthening the CMLM inference ( Section 3 ) while leaving its training algorithm unchanged ( Section 2 ) , and maintaining much of the structure of the original inference strategy .
For our experiments on machine translation ( Section 4 ) , we compare inference heuristics in terms of their quality - speed trade-offs .
We analyze the development of quality over iterations , and the influence of sentence length .
With examples of unmasking schedules we furthermore illustrate the role of mode breaking through choosing the right contexts .
CMLM Model and Training
The CMLM is a model for p( Y mask | Y obs , X ) , the probability of masked tokens Y mask given a partially observed output sequence Y obs and an input sequence X . Y mask and Y obs are sets of tokens at specified positions that together form a complete output sequence Y : Y mask = Y \ Y obs .
The model is implicitly conditioned on output sequence length N = | Y | , and the tokens in Y mask are conditionally independent : p( Y mask | Y obs , X ) = y i ?Y mask p(y i | Y obs , X , N ) .
During training , masks are placed randomly :
First , the mask size S ? { 1 , . . . , N } is sampled from a uniform distribution , then S positions are randomly chosen to define the subsets Y obs and Y mask .
Cross-entropy loss is incurred via p(y i | Y obs , X ) for each y i ?
Y mask .
An additional classifier on top of encoder representations is trained to predict the output length N .
CMLM Inference Inference starts with a context of only MASK tokens .
Until a stop condition is met , decoder predictions iteratively replace a subset of these in selected positions ( " unmasking " ) .
With a single iteration , inference is non-autoregressive ; when the number of iterations T is less than the sentence length N it is semi-autoregressive ; and when N = T it is fully autoregressive .
Due to the use of a uniform distribution over reference contexts , training is agnostic to these different regimes .
In general , we seek to minimize T without trading off too much quality .
The challenge in doing so is to identify the subset of predictions that are most likely to provide suitable conditioning context for future iterations ( Mansimov et al. , 2019 ) .
Structural or linguistic dependencies in the output may also play an important role for resolving linguistic ambiguities ( Martins and Kreutzer , 2017 ) .
For example , in German it might be harder to first generate the determiner before knowing the grammatical gender of the head word ( see examples in Figure 6 ) .
The length predictor first predicts b different lengths , then one hypothesis is decoded for each length independently using the iterative process just outlined .
The hypothesis with the highest lengthnormalized model score is selected as the output .
We refer to b as the length beam in the following .
t M ( t ) Y ( t ) p( Y ( t ) | Y ( < t ) , M ( ?t ) , X ) 0 { 1,2,3 } {} - 1 { } { a , b , c} p( a | X ) p( b|X ) p( c|X ) 0 { 1,2,3 } {} - 1 { 2,3 } { a} p( a | X ) 2 { 3 } { b} p( b| a , X ) 3 { } {c } p( c | a , b, X ) 0 { 1,2,3 } {} - 1 { 2 } { a , c} p( a | X ) p( c|X ) 2 {} { b} p( b | a , c , X )
Update Strategies
The CMLM can make predictions at all positions , whether they correspond to masked input or not .
This lends itself to various strategies for choosing how to update current predictions and masks : 1 ? update-all : update tokens and scores at all positions , no constraint on new mask 2 ? update-masked : update tokens at masked positions only , no constraint on new mask 3 ? update-masked -sub : update tokens at masked positions only , new mask must be a subset of the current one
In this paper we focus on the update-masked - sub strategy .
It is empirically competitive ( Section 4.1 ) , and interesting because it corresponds to a valid probabilistic factorization of the target sequence , governed by a latent variable M = M ( 0 ) . . . M ( T ) which represents the sequence of masking decisions : p( Y , M | X ) = T t=1 p( Y ( t ) | Y ( < t ) , M ( ?t ) , X ) ? p( M ( t ) | Y ( < t ) , M ( < t ) , X ) , ( 1 ) where 1 ) , and Y ( t ) is the set of tokens unmasked on the tth iteration .
Figure 1 illustrates this computation for various choices of M . 4
The class of inference strategies we explore can thus be seen as greedy search for the mostly likely factorization , subject to a constraint on the number of iterations : at each iteration , we choose a subset of tokens to add to the current hypothesis , balancing high model probabilities with the risk of making an error and degrading future predictions .
Because tokens are predicted independently , the risk of an error grows with the size of the subset .
M ( 0 ) = { 1 , . . . , N } , M ( T ) = { } , M ( t ) ? M ( t?
Unmasking Heuristics
Under the update-masked - sub constraint , the role of greedy inference heuristics is to choose which positions to unmask , given a full set of predictions for all currently - masked positions .
The maskpredict strategy of Ghazvininejad et al . ( 2019 ) chooses the N/T highest - probability tokens , in order to finish in a constant T iterations , regardless of N .
This generates more tokens per iteration for long sentences , which may not be ideal for sentences with complex structure .
To measure its effect , we propose a variant that unmasks a constant K tokens per iteration , in order to achieve approximately K-fold speedup over autoregressive performance , independent of hypothesis length .
Unmasking highest - ranked tokens according to probability is reasonable , but it ignores the magnitude of the probabilities , creating the potential for selecting tokens in which the model has low confidence , and vice versa .
To address this , we design several simple thresholding strategies that vary the number of tokens per iteration , ideally generating more when the conditioning context licences many confident predictions , and fewer otherwise .
1 . The most straightforward strategy , thresh , unmasks all tokens with probabilities greater than a given threshold ? . 2 . The comb-thresh strategy unmasks the largest set of highest - ranked tokens Y whose joint probability p( Y ) > ? .
3 . Finally , in order to account for lower - ranked predictions , the fcomb-thresh strategy unmasks the largest set Y for which p( Y ) * ( 1 ? p( ? ) ) > ? , where Y consists of the highestranked tokens , and ? is its complement .
All threshold strategies unmask the single highestranked token in contexts where the threshold criterion is not met .
Experiments
Our CMLM is implemented with a base Transformer ( Vaswani et al. , 2017 ) built on a Tensor - Flow implementation of ( Ghazvininejad et al. , 2019 ) .
The input to the decoder is Y obs , with MASK tokens at masked positions , and the output is Y mask , predictions for all masked positions without future attention masking .
We use data from WMT14 en?de ( Bojar et al. , 2014 ) and WMT17 zh?en ( Bojar et al. , 2017 ) with a sentence piece vocabulary of 32 k , focusing mainly on en?de , and providing results for all pairs in appendix A .
The CMLM is trained on distilled training data from an autoregressive Transformer and initialized with its parameters .
Figure 2 shows the performance of the update strategies described in section 3.1 versus length beam b.
All strategies use the mask -predict heuristic with a fixed 10 - iteration limit .
As beam size increases past 2 , the update-masked strategies increasingly dominate , indicating that their scores are more reliable for choosing among length hypotheses .
There is no significant difference between the two variants of update-masked .
This suggests that our probabilistic factorization constraint ( updatemasked - sub ) does not hurt in practice .
Update Strategies
Heuristics
To compare the speed-quality trade-off of different heuristics on an equal footing , we vary the values of the hyper-parameter that controls speedup : T for fixed - iteration mask - predict , K for variable - iteration mask - predict , and ? for thresholding strategies .
In each case , we measure the resulting speedup as the total number of tokens in the test set divided by the total number of iterations required for all sentences , 5 and corpus BLEU on the output of the last iteration .
The generation speed is expressed in average number of tokens per iteration , e.g. comb-thresh : 3.5 stands for the comb-thresh heuristic with a threshold value set so that 3.5 tokens are generated per iteration on average .
Figure 3 compares heuristics using 5 length candidates .
First of all , fixed - K mask - predict beats fixed - T by a substantial margin ( especially at higher speeds ) , indicating that it is worth allocating more iterations for longer sentences .
Second , the comb-thresh strategy has a small but consistent advantage over fixed - K mask - predict across all speeds .
This strategy exhibits a roughly 4x gain while sacrificing less than 0.3 BLEU relative to the equivalent autoregressive Transformer ( 27.6 BLEU ) .
Both thresh and fcomb-thresh underperform .
Despite their superficial similarity to comb-thresh , they perform much worse ; this holds for other language pairs as well ( Figure 7 in Appendix A ) .
For thresh , the poor performance as speedup increases reflects many relatively low-probability tokens exceeding lower thresholds , a condition that is penalized by all other heuristics , which take rank into account .
For fcomb- thresh the effect is more subtle ; we believe that it is due to the probabilities of lower - ranked tokens having worse calibration , leading to less reliable unmasking decisions .
A practical impediment to a thresholding strategy is that it does not provide direct control over desired speedup : this must be identified by tuning ?
appropriately on a development set .
However , we found that dev and test speedups were well correlated across speedups ranging from 1 to 11 , with the largest absolute error being 0.8 ( 11.1 speedup on dev versus 10.3 on test ) , and the average error being 0.3 .
Analysis
Having freed the heuristics from a globally imposed iteration limit for constant - time decoding as in the original mask - predict inference heuristic , we observed better quality - speed trade - offs in the above discussed results .
Intuitively , we would expect the heuristics to allocate more iterations for longer sentences and save iterations on shorter sentences .
Figure 4 shows how many iterations the models spend on sentences in relation to their length .
For a fair comparison , the generation is constrained to oracle output lengths , and we set the hyperparameters such that they result in the same generation speed ( 5 tokens per iteration on average ) .
We see that flexible -iteration strategies spend fewer iterations on sentences up to a length of around 30 when compared to a fixed -iteration strategy .
comb-thresh spends on average the largest number of iterations on longer sentences ( which pays off in terms of quality , see Figure 3 ) , while thresh spends even fewer iterations on longer sentences than the mask - predict model .
The development of BLEU over iterations for comparable generation speeds across heuristics is shown in Figure 5 . 6 We can see that speedier generation gives a faster initial increase in translation quality over iterations in exchange for slightly lower final quality ( dashed vs solid lines ) .
Maskpredict levels off early after reaching its fixed number of iterations , but climbs quickly before that point due to an averaging effect over short sentences .
Fixed - K mask - predict and comb-thresh both extract useful work out of each iteration , with comb-thresh maintaining a slight edge over all iterations , especially at higher generations speeds .
Figure 6 shows an example for generation strate- gies under mask - predict and comb-thresh ( see appendix B ) .
They illustrate the workings of iterative decoding and main differences between strategies :
Iterative decoding is crucially needed to resolve subject - verb agreement ( e.g. " man [ . . . ] erreichen " ( generic " you " ) vs. " Sie [. . . ] erreichen " ( formal " you " ) in ex. 2 ) and rough sentence structure ( e.g. placement of the comma ) , and offers room for less literal translations ( " von heute auf morgen " ( literally " from today to tomorrow " ) rather than " ? ber Nacht " ( literally " over night " ) in ex. 1 ) .
The two tokens " Ger " and " ster " ( a name ) show how the correct conditioning changes model scores in both cases :
After the former token is predicted , the probability for the latter increases drastically , since its only valid position in the sentence is there .
While both strategies use the same number of iterations to generate this translation , one can see that it pays off for comb-thresh to unmask certain tokens earlier ( " ab " , " Lob " ) , which allows a valid resolution of neighboring tokens ( " bschluss " , " zum " ) .
7
Conclusion
We investigated inference strategies for machine translation based on CMLM with a focus on the trade - off between generation speed and quality .
We introduce a perspective which views generation sequences as probabilistic factorizations of the final output sequence , and use it to analyze and extend previous heuristics .
Our new heuristics achieve better speed / quality balance by flexibly adjusting the number of total iterations , and by taking the probabilities of sets of tokens into account .
For future work we would like to explore if their success transfers to other generation tasks with MLMs where inference efficiency is a concern .
