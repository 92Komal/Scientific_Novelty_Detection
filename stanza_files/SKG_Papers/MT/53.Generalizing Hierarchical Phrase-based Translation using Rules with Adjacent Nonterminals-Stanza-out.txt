title
Generalizing Hierarchical Phrase- based Translation using Rules with Adjacent Nonterminals
abstract
Hierarchical phrase - based translation ( Hiero , ( Chiang , 2005 ) ) provides an attractive framework within which both short - and longdistance reorderings can be addressed consistently and ef ciently .
However , Hiero is generally implemented with a constraint preventing the creation of rules with adjacent nonterminals , because such rules introduce computational and modeling challenges .
We introduce methods to address these challenges , and demonstrate that rules with adjacent nonterminals can improve Hiero 's generalization power and lead to signi cant performance gains in Chinese - English translation .
Introduction Hierarchical phrase - based translation ( Hiero , ( Chiang , 2005 ) ) has proven to be a very useful compromise between syntactically informed and purely corpus-driven translation .
By automatically learning synchronous grammar rules from parallel text , Hiero captures short - and long-distance reorderings consistently and ef ciently .
However , implementations of Hiero generally forbid adjacent nonterminal symbols on the source side of hierarchical rules , a practice we will refer to as the non-adjacent nonterminals constraint .
The main argument against such rules is that they cause the system to produce multiple derivations that all lead to the same translation a form of redundancy known as spurious ambiguity .
Spurious ambiguity can lead to drastic reductions in decoding ef ciency , and the obvious solutions , such as reducing beam width , erode translation quality .
In Section 2 , we argue that the non-adjacent nonterminals constraints severely limits Hiero 's generalization power , limiting its coverage of important reordering phenomena .
In Section 3 , we discuss the challenges that arise in relaxing this constraint .
In Section 4 we introduce new methods to address those challenges , and Section 5 validates the approach empirically .
Improving Hiero via variations on rule pruning and ltering is well explored , e.g. , ( Chiang , 2005 ; Chiang et al. , 2008 ; Zollmann and Venugopal , 2006 ) , to name just a few .
Hiero can correctly translate the example if it learns any of the following rules from training data : X ? X 1 , rank 10th at X 1 ( 1 ) X ? X 1 , X 1 at Eastern div . ( 2 ) X ? X 1 X 2 , X 2 X 1 Eastern div .
( 3 )
However , in practice , data sparsity makes the chance of learning these rules rather slim .
For instance , learning Rule 1 depends on training data containing instances of the shift with identical wording for the VP -A , which belongs to an open word class .
If Hiero fails to learn any of the above rules , it will apply the glue rules S ? S X 1 , S X 1 and S ? X , X .
But these glue rules clearly cannot model the VP - A's movement .
In failing to learn Rules 1 - 3 , Hiero has no choice but to translate VP - A in a monotone order .
On the other hand , consider the following rules with adjacent nonterminals on the source side ( or XX rules , for brevity ) : X? X 1 X 2 , X 2 at X 1 ( 4 ) X? X 1 X 2 , rank 10th X 1 X 2 ( 5 ) X? X 1 X 2 , X 2 X 1 ( 6 ) Note that although XX rules 4 - 6 can potentially increase the chance of modeling the pre-verbal to postverbal shift , not all of them are bene cial to learn .
For instance , Rule 5 models the word order shift but introduces spurious ambiguity , since the nonterminals are translated in monotone order .
Rule 6 , which resembles the inverted rule of the Inversion Transduction Grammar ( Wu , 1997 ) , is highly ambiguous because its application has no lexical grounding .
Rule 4 avoids both problems , and is also easier to learn , since it is lexically anchored by a preposition , ( at ) , which we can expect to appear frequently in training .
These observations will motivate us to focus on rules that model non-monotone reordering of phrases surrounding a lexical item on the target side .
More formally , we de ne P ori t ( ori t ( Y , X ) |Y ) , where ori t ( Y , X ) ? { MA , RA , MG , RG } is the orientation of a target phrase X with a source function word Y as the reference point .
Experiments
We evaluated the generalization of Hiero to include XX rules on a Chinese-to - English translation task .
We treat the N = 128 most frequent words in the corpus as function words , an approximation that has worked well in the past and minimized dependence on language - speci c resources ( Setiawan et al. , 2007 ) .
We report BLEU r4n4 and assess signicance using the standard bootstrapping approach .
We trained on the NIST MT06 Eval corpus excluding the UN data ( approximately 900 K sentence pairs ) , segmenting Chinese using the Harbin segmenter ( Zhao et al. , 2001 ) .
To our knowledge , the work reported here is the rst to relax the non-adjacent nonterminals constraint in hierarchical phrase - based models .
The results con rm that judiciously adding rules to a Hiero grammar , adjusting the modeling accordingly , can achieve signi cant gains .
Although we found that XX -nonmono rules performed better than general XX rules , we believe the latter may nonetheless prove useful .
Manually inspecting our system 's output , we nd that the output is often shorter than the references , and the missing words often correspond to function words that are modeled by those rules .
Using XX rules to model legitimate word insertions is a topic for future work .
( 2007 ) Figure 1 : 20071 Figure 1 : A Chinese-English verb phrase translation
3 Addressing XX Rule Challenges
The rst challenge created by introducing XX rules is computational : relaxing the constraint signicantly increases the grammar size .
Motivated by our earlier discussion , we address this by permitting only rules that model non-monotone reordering , i.e. those rules whose nonterminals are projected into the target language in a different word order , leaving monotone mappings to be handled by the glue rules as previously .
This choice helps keep the search space more manageable , and also avoids spurious ambiguity .
In addition , we disallow rules in which nonterminals are adjacent on both the source and target sides , by imposing the non adjacent nonterminal constraint on the target side whenever the constraint is relaxed on the source side .
This forces any nonmonotone reorderings to always be grounded in lexical evidence .
We refer to the permitted subset of XX rules as XX -nonmono rules .
The second challenge involves modeling : introducing XX rules places them in competition with the existing glue rules .
In particular , these two kinds of rules try to model the same phenomena , namely the translations of phrases that appear next to each other .
However , they differ in terms of the features associated with the rules .
XX rules will be associated with the same features as any other hierarchical rules , since they are all learned via an identical training method .
In contrast , glue rules are introduced into the grammar in an ad hoc manner , and the only feature associated with them is a glue penalty .
These distinct feature sets makes direct comparison of scores unreliable .
As a result the decoder may simply prefer to always select glue rules because they are associated with fewer features resulting in adjacent phrases always being translated in a monotone order .
To address this issue , we introduce a new model , which we call the target -side function words orientation - based model , or simply P orit , which evaluates the application of the two kinds of rules on the same context , i.e. for our ex- The P orit model is motivated by the function words reordering hypothesis ( Setiawan et al. , 2007 ) , which suggests that function words encode essential information about the ( re ) ordering of their neighboring phrases .
In contrast to Setiawan et al . ( 2007 ) , who looked at neighboring contexts for function words on the source side , we focus here on modeling the in uence of function words on neighboring phrases on the target side .
We argue that this focus better ts our purpose , since the phrases that we want to model are the function words ' neighbors on the target side , as illustrated in Fig.1 .
To develop this idea , we rst de ne an ori t function that takes a source function word as a reference point , along with its neighboring phrase on the target side .
The ori t function outputs one of the following orientation values ( Nagata et al. , 2006 ) : Monotone-Adjacent ( MA ) ; Reverse-Adjacent ( RA ) ; Monotone - Gap ( MG ) ; and Reverse-Gap ( RG ) .
The Monotone / Reverse distinction indicates whether the source order follows the target order .
The Adjacent / Gap distinction indicates whether the two phrases are adjacent or separated by an intervening phrase on the source side .
For example , in Fig. 1 , the value of ori t for right neighbor Eastern division with respect to function word ( at ) is MA , since its corresponding source phrase is adjacent to ( at ) and their order is preserved on the English side .
The value for left neighbor rank 10th with respect to ( at ) is RG , since is separated from ( at ) and their order is reversed on the English side .
Our 5 - gram language model with modi ed Kneser - Ney smoothing was trained on the English side of our training data plus portions of the Gigaword v2 English corpus .
We optimized the feature weights using minimum error rate training , using the NIST MT03 test set as the development set .
We report the results on the NIST 2006 evaluation test ( MT06 ) and the NIST 2008 evaluation test ( MT08 ) .
Table 1 1 orit model ( + XX -nonmono + ori t ) .
The combination produces a signi cant , consistent gain across all test sets .
This result suggests that the orientation model contributes more strongly in unseen cases when Hiero also considers non-monotone reordering .
We interpret this result as a validation of our hypothesis that carefully relaxing the non- reports experiments in an incremental fashion , starting from the baseline model ( the orig- inal Hiero ) , then adding different sets of rules , and nally adding the orientation - based model .
In our rst experiments , we investigated the introduction of three different sets of XX rules .
First ( + itg ) , we simply add the ITG 's inverted rule ( Rule 6 ) to the baseline system in an ad-hoc manner , similar to the glue rules .
This hurts performance consistently across MT06 and MT08 sets , which we suspect is a result of ITG rule applications often aggravating search error .
Second ( + XX ) , we permitted general XX rules .
This results in a grammar size increase of 25 - 26 % , ltering out rules irrelevant for the test set ,
In fact , separate models are developed for left and right neighbors , although for clarity we suppress this distinction throughout .
