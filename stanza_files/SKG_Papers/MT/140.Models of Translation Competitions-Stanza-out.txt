title
Models of Translation Competitions
abstract
What do we want to learn from a translation competition and how do we learn it with confidence ?
We argue that a disproportionate focus on ranking competition participants has led to lots of different rankings , but little insight about which rankings we should trust .
In response , we provide the first framework that allows an empirical comparison of different analyses of competition results .
We then use this framework to compare several analytical models on data from the Workshop on Machine Translation ( WMT ) .
The WMT Translation Competition Every year , the Workshop on Machine Translation ( WMT ) conducts a competition between machine translation systems .
The WMT organizers invite research groups to submit translation systems in eight different tracks : Czech to / from English , French to / from English , German to / from English , and Spanish to / from English .
For each track , the organizers also assemble a panel of judges , typically machine translation specialists .
1
The role of a judge is to repeatedly rank five different translations of the same source text .
Ties are permitted .
In Table 1 , we show an example 2 where a judge ( we 'll call him " jdoe " ) has ranked five translations of the French sentence " Il ne va pas . "
Each such elicitation encodes ten pairwise comparisons , as shown in Table 2 .
For each competition track , WMT typically elicits between 5000 and 20000 comparisons .
Once the elicitation process is complete , WMT faces a large database of comparisons and a question that must be answered : whose system is the best ?
Table 2 : Pairwise comparisons encoded by Table 1 . A preference of 0 means neither translation was preferred .
Otherwise the preference specifies the preferred system .
A Ranking Problem
For several years , WMT used the following heuristic for ranking the translation systems : ORIGWMT ( s ) = win( s ) + tie ( s ) win( s ) + tie ( s ) + loss ( s )
For system s , win( s ) is the number of pairwise comparisons in which s was preferred , loss ( s ) is the number of comparisons in which s was dispreferred , and tie (s ) is the number of comparisons in which s participated but neither system was preferred .
Recently , ( Bojar et al. , 2011 ) questioned the adequacy of this heuristic through the following ar-gument .
Consider a competition with systems A and B. Suppose that the systems are different but equally good , such that one third of the time A is judged better than B , one third of the time B is judged better than A , and one third of the time they are judged to be equal .
The expected values of ORIGWMT ( A ) and ORIGWMT ( B ) are both 2/3 , so the heuristic accurately judges the systems to be equivalently good .
Suppose however that we had duplicated B and had submitted it to the competition a second time as system C. Since B and C produce identical translations , they should always tie with one another .
The expected value of ORIGWMT ( A ) would not change , but the expected value of ORIGWMT ( B ) would increase to 5 /6 , buoyed by its ties with system C .
This vulnerability prompted ( Bojar et al. , 2011 ) to offer the following revision : BOJAR ( s ) = win( s ) win( s ) + loss ( s )
The following year , it was BOJAR 's turn to be criticized , this time by ( Lopez , 2012 ) : Superficially , this appears to be an improvement .... could n't a system still be penalized simply by being compared to [ good systems ] more frequently than its competitors ?
On the other hand , could n't a system be rewarded simply by being compared against a bad system more frequently than its competitors ?
Lopez 's concern , while reasonable , is less obviously damning than ( Bojar et al. , 2011 ) 's criticism of ORIGWMT .
It depends on whether the collected set of comparisons is small enough or biased enough to make the variance in competition significant .
While this hypothesis is plausible , Lopez makes no attempt to verify it .
Instead , he offers a ranking heuristic of his own , based on a Minimum Feedback Arc solver .
The proliferation of ranking heuristics continued from there .
The WMT 2012 organizers ( Callison - Burch et al. , 2012 ) took Lopez 's ranking scheme and provided a variant called Most Probable Ranking .
Then , noting some potential pitfalls with that , they created two more , called Monte Carlo Playoffs and Expected Wins .
While one could raise philosophical objections about each of these , where would it end ?
Ultimately , the WMT 2012 findings presented five different rankings for the English - German competition track , with no guidance about which ranking we should pay attention to .
How can we know whether one ranking is better than other ?
Or is this even the right question to ask ?
A Problem with Rankings Suppose four systems participate in a translation competition .
Three of these systems are extremely close in quality .
We 'll call these close1 , close2 , and close3 .
Nevertheless , close1 is very slightly better 3 than close2 , and close2 is very slightly better than close3 .
The fourth system , called terrific , is a really terrific system that far exceeds the other three .
Now which is the better ranking ?
terrific , close3 , close1 , close2 ( 1 ) close1 , terrific , close2 , close3 ( 2 ) Spearman 's rho 4 would favor the second ranking , since it is a less disruptive permutation of the gold ranking .
But intuition favors the first .
While its mistakes are minor , the second ranking makes the hard - to - forgive mistake of placing close1 ahead of the terrific system .
The problem is not with Spearman 's rho .
The problem is the disconnnect between the knowledge that we want a ranking to reflect and the knowledge that a ranking actually contains .
Without this additional knowledge , we cannot determine whether one ranking is better than another , even if we know the gold ranking .
We need to determine what information they lack , and define more rigorously what we hope to learn from a translation competition .
From Rankings to Relative Ability
Ostensibly the purpose of a translation competition is to determine the relative ability of a set of translation systems .
Let S be the space of all translation systems .
Hereafter , we will refer to S as the space of students .
We choose this term to evoke the metaphor of a translation competition as a standardized test , which shares the same goal : to assess the relative abilities of a set of participants .
But what exactly do we mean by " ability " ?
Before formally defining this term , first recognize that it means little without context , namely : 1 . What kind of source text do we want the systems to translate well ?
Say system A is great at translating travel - related documents , but terrible at translating newswire .
Meanwhile , system B is pretty good at both .
The question " which system is better ? " requires us to state how much we care about travel versus newswire documents - otherwise the question is underspecified .
2 . Who are we trying to impress ?
While it 's tempting to think that translation quality is a universal notion , the 50 - 60 % interannotator agreement in WMT evaluations ( Callison - Burch et al. , 2012 )
With this in mind , let 's define some additional elements of a translation competition .
Let X be the space of all possible segments of source text , J be the space of all possible judges , and ? = { 0 , 1 , 2 } be the space of pairwise preferences .
5
We assume all spaces are countable .
Unless stated otherwise , variables s 1 and s 2 represent students from S , variable x represents a segment from X , variable j represents a judge from J , and variable ?
represents a preference from ?.
Moreover , define the negation ? of preference ? such that ?
= 2 ( if ? = 1 ) , ? = 1 ( if ? = 2 ) , and ? = 0 ( if ? = 0 ) .
Now assume a joint distribution P ( s 1 , s 2 , x , j , ? ) specifying the probability that we ask judge j to evaluate students s 1 and s 2 's respective translations of source text x , and that judge j's preference is ?.
We will further assume that the choice of student pair , source text , and judge are marginally independent of one another .
In other words : P ( s 1 , s 2 , x , j , ? ) = P (?|s 1 , s 2 , x , j ) ? P ( x|s 1 , s 2 , j ) ?P ( j|s 1 , s 2 ) ? P ( s 1 , s 2 ) = P ( ?|s 1 , s 2 , x , j ) ? P ( x ) ? P ( j ) ? P ( s 1 , s 2 ) = P X ( x ) ? P J ( j ) ? P ( s 1 , s 2 ) ? P (?|s 1 , s 2 , x , j) 5
As a reminder , 0 indicates no preference .
It will be useful to reserve notation P X and P J for the marginal distributions over source text and judges .
We can marginalize over the source segments and judges to obtain a useful quantity : P ( ?|s 1 , s 2 ) = x?X j?J P X ( x ) ? P J ( j ) ? P (?|s 1 , s 2 , x , j)
We refer to this as the P X , P J - relative ability of students s 1 and s 2 .
By using different marginal distributions P X , we can specify what kinds of source text interest us ( for instance , P X could focus most of its probability mass on German tweets ) .
Similarly , by using different marginal distributions P J , we can specify what judges we want to impress ( for instance , P J could focus all of its mass on one important corporate customer or evenly among all fluent bilingual speakers of a language pair ) .
With this machinery , we can express the purpose of a translation competition more clearly : to estimate the P X , P J - relative ability of a set of students .
In the case of WMT , P J presumably 6 defines a space of competent source-totarget bilingual speakers , while P X defines a space of newswire documents .
We 'll refer to an estimate of P ( ?|s 1 , s 2 ) as a preference model .
In other words , a preference model is a distribution Q (?|s 1 , s 2 ) .
Given a set of pairwise comparisons ( e.g. , Table 2 ) , the challenge is to estimate a preference model Q (?|s 1 , s 2 ) such that Q is " close " to P .
For measuring distributional proximity , a natural choice is KL - divergence ( Kullback and Leibler , 1951 ) , but we cannot use it here because P is unknown .
Fortunately , if we have i.i.d. data drawn from P , then we can do the next best thing and compute the perplexity of preference model Q on this heldout test data .
Let D be a sequence of triples s 1 , s 2 , ? where the preferences ? are i.i.d. samples from P ( ?|s 1 , s 2 ) .
The perplexity of preference model Q on test data D is : perplexity ( Q| D ) = 2 ? s 1 , s 2 , ? ?D 1 |D| log 2 Q ( ?|s 1 , s 2 ) How do we obtain such a test set from competition data ?
Recall that a WMT competition produces pairwise comparisons like those in Table 2 . Let C be the set of comparisons s 1 , s 2 , x , j , ? obtained from a translation competition .
Competition data C is not necessarily 7 sampled i.i.d. from P ( s 1 , s 2 , x , j , ? ) because we may intentionally 8 bias data collection towards certain students , judges or source text .
Also , because WMT elicits its data in batches ( see Table 1 ) , every segment x of source text appears in at least ten comparisons .
To create an appropriately - sized test set that closely resembles i.i.d. data , we isolate the subset C of comparisons whose source text appears in at most k comparisons , where k is the smallest positive integer such that | C | >= 2000 .
We then create the test set D from C : D = { s 1 , s 2 , ? | s 1 , s 2 , x , j , ? ? C } We reserve the remaining comparisons for training preference models .
Table 3 shows the resulting dataset sizes for each competition track .
Unlike with raw rankings , the claim that one preference model is better than another has testable implications .
Given two competing models , we can train them on the same comparisons , and compare their perplexities on the test set .
This gives us a quantitative 9 answer to the question of which is the better model .
We can then publish a system ranking based on the most trustworthy preference model .
Baselines
Let 's begin then , and create some simple preference models to serve as baselines .
Uniform
The simplest preference model is a uniform distribution over preferences , for any choice of students s 1 , s 2 : Q (?|s 1 , s 2 ) = 1 3 ? ? ?
This will be our only model that does not require training data , and its perplexity on any test set will be 3 ( i.e. equal to number of possible preferences ) .
Adjusted Uniform
Now suppose we have a set C of comparisons available for training .
Let C ? ?
C denote the subset of comparisons with preference ? , and let C(s 1 , s 2 ) denote the subset comparing students s 1 and s 2 .
Perhaps the simplest thing we can do with the training data is to estimate the probability of ties ( i.e. preference 0 ) .
We can then distribute the remaining probability mass uniformly among the other two preferences : Q (?|s 1 , s 2 ) = ? ? ? ? ? ? ? ? ? | C 0 | |C | if ? = 0 1 ? |C 0 | | C| 2 otherwise 6 Simple Bayesian Models
Independent Pairs
Another simple model is the direct estimation of each relative ability P ( ?|s 1 , s 2 ) independently .
In other words , for each pair of students s 1 and s 2 , we estimate a separate preference distribution .
The maximum likelihood estimate of each distribution would be : Q (?|s 1 , s 2 ) = | C ? ( s 1 , s 2 ) | + | C ?( s 2 , s 1 ) | | C( s 1 , s 2 ) | + | C(s 2 , s 1 ) |
However the maximum likelihood estimate would test poorly , since any zero probability estimates for test set preferences would result in infinite perplexity .
To make this model practical , we assume a symmetric Dirichlet prior with strength ? for each preference distribution .
This gives us the following Bayesian estimate : Q (?|s 1 , s 2 ) = ? + | C ? ( s 1 , s 2 ) | + | C ?( s 2 , s 1 ) | 3 ? + | C( s 1 , s 2 ) | + | C( s 2 , s 1 ) |
We call this the Independent Pairs preference model .
Independent Students
The Independent Pairs model makes a strong independence assumption .
It assumes that even if we know that student A is much better than student B , and that student B is much better than student C , we can infer nothing about how student A will fare versus student C. Instead of directly estimating the relative ability P ( ?|s 1 , s 2 ) of students s 1 and s 2 , we could instead try to estimate the universal ability P ( ?|s 1 ) = s 2 ?S P (?|s 1 , s 2 ) ? P ( s 2 |s 1 ) of each individual student s 1 and then try to reconstruct the relative abilities from these estimates .
For the same reasons as before , we assume a symmetric Dirichlet prior with strength ? for each preference distribution , which gives us the following Bayesian estimate : Q ( ?|s 1 ) = ? + s 2 ?S |C ? ( s 1 , s 2 ) | + | C ?( s 2 , s 1 ) | 3 ? + s 2 ?S | C(s 1 , s 2 ) | + | C(s 2 , s 1 ) |
The estimates Q (?|s 1 ) do not yet constitute a preference model .
A downside of this approach is that there is no principled way to reconstruct a preference model from the universal ability estimates .
We experiment with three ad-hoc reconstructions .
The asymmetric reconstruction simply ignores any information we have about student s 2 : Q (?|s 1 , s 2 ) = Q (?|s 1 ) The arithmetic and geometric reconstructions compute an arithmetic / geometric average of the two universal abilities : Q (?|s 1 , s 2 ) = Q (?|s 1 ) + Q (?|s 2 ) 2 Q (?|s 1 , s 2 ) = [ Q ( ?|s 1 ) * Q ( ?| s 2 ) ]
1 2 We respectively call these the ( Asymmetric / Arithmetic / Geometric ) Independent Students preference models .
Notice the similarities between the universal ability estimates Q (?|s 1 ) and the BOJAR ranking heuristic .
These three models are our attempt to render the BOJAR heuristic as preference models .
Item - Response Theoretic ( IRT ) Models Let 's revisit ( Lopez , 2012 ) 's objection to the BO - JAR ranking heuristic : " ... could n't a system still be penalized simply by being compared to [ good systems ] more frequently than its competitors ? "
The official WMT 2012 findings ( Callison - Burch et al. , 2012 ) echoes this concern in justifying the exclusion of reference translations from the 2012 competition : [ W] orkers have a very clear preference for reference translations , so including them unduly penalized systems that , through ( un ) luck of the draw , were pitted against the references more often .
Presuming the students are paired uniformly at random , this issue diminishes as more comparisons are elicited .
But preference elicitation is expensive , so it makes sense to assess the relative ability of the students with as few elicitations as possible .
Still , WMT 2012 's decision to eliminate references entirely is a bit of a draconian measure , a treatment of the symptom rather than the ( perceived ) disease .
If our models cannot function in the presence of training data variation , then we should change the models , not the data .
A model that only works when the students are all about the same level is not one we should rely on .
We experiment with a simple model that relaxes some independence assumptions made by previous models , in order to allow training data variation ( e.g. who a student has been paired with ) to influence the estimation of the student abilities .
Figure 1 ( left ) shows plate notation ( Koller and Friedman , 2009 ) for the model 's independence structure .
First , each student 's ability distribution is drawn from a common prior distribution .
Then a number of translation items are generated .
Each item is authored by a student and has a quality drawn from the student 's ability distribution .
Then a number of pairwise comparisons are generated .
Each comparison has two options , each a translation item .
The quality of each item is observed by a judge ( possibly noisily ) and then the judge states a preference by comparing the two observations .
We investigate two parameterizations of this model : Gaussian and categorical .
Figure 1 ( right ) shows an example of the Gaussian parameterization .
The student ability distributions are Gaussians with a known standard deviation ?
a , drawn from a zero-mean Gaussian prior with known standard deviation ?
0 . In the example , we show the ability distributions for students 6 ( an aboveaverage student , whose mean is 0.4 ) and 14 ( a poor student , whose mean is - 0.6 ) .
We also show an item authored by each student .
Item 43 has a somewhat low quality of - 0.3 ( drawn from student 14 's ability distribution ) , while item 205 is not student 6 's best work ( he produces a mean quality of 0.4 ) , but still has a decent quality at 0.2 .
Comparison 1 pits these items against one another .
A judge draws noise from a zero-mean Gaussian with known standard deviation ? obs , then adds this to the item 's actual quality to get an observed quality .
For the first option ( item 43 ) , the judge draws a noise of - 0.12 to observe a quality of - 0.42 ( worse than it actually is ) .
For the second option ( item 205 ) , the judge draws a noise of 0.15 to observe a quality of 0.35 ( better than it actually is ) .
Finally , the judge compares the two observed qualities .
If the absolute difference is lower than his decision radius ( which here is 0.5 ) , then he states no preference ( i.e. a preference of 0 ) .
Otherwise he prefers the item with the higher observed quality .
The categorical parameterization is similar to the Gaussian parameterization , with the following differences .
Item quality is not continuous , but rather a member of the discrete set { 1 , 2 , ... , ?}.
The student ability distributions are categorical distributions over { 1 , 2 , ... , ?} , and the student ability prior is a symmetric Dirichlet with strength ?
a .
Finally , the observed quality is the item quality ? plus an integer-valued noise ? ? { 1 ? ? , ... , ? ? ?}. Noise ? is drawn from a discretized zero-mean Gaussian with standard deviation ? obs . Specifically , P r( ? ) is proportional to the value of the probability density function of the zero-mean Gaussian N ( 0 , ? obs ) .
We estimated the model parameters with Gibbs sampling ( Geman and Geman , 1984 ) .
We found that Gibbs sampling converged quickly and consistently 10 for both parameterizations .
Given the parameter estimates , we obtain a preference model Q (?|s 1 , s 2 ) through the inference query : P r(comp.c .pref = ? | item .i . author = s 1 , item .i . author = s 2 , comp.c .opt1 = i , comp.c .opt2 = i ) 10
We ran 200 iterations with a burn- in of 50 .
where c , i , i are new comparison and item ids that do not appear in the training data .
We call these models Item-Response Theoretic ( IRT ) models , to acknowledge their roots in the psychometrics ( Thurstone , 1927 ; Bradley and Terry , 1952 ; Luce , 1959 ) and item-response theory ( Hambleton , 1991 ; van der Linden and Hambleton , 1996 ; Baker , 2001 ) literature .
Itemresponse theory is the basis of modern testing theory and drives adaptive standardized tests like the Graduate Record Exam ( GRE ) .
In particular , the Gaussian parameterization of our IRT models strongly resembles 11 the Thurstone ( Thurstone , 1927 ) and Bradley - Terry - Luce ( Bradley and Terry , 1952 ; Luce , 1959 ) models of paired comparison and the 1 PL normal-ogive and Rasch ( Rasch , 1960 ) models of student testing .
From the testing perspective , we can view each comparison as two students simultaneously posing a test question to the other : " Give me a translation of the source text which is better than mine . "
The students can answer the question correctly , incorrectly , or they can provide a translation of analogous quality .
An extra dimension of our models is judge noise , not a factor when modeling multiple - choice tests , for which the right answer is not subject to opinion .
Experiments
We organized the competition data as described at the end of Section 4 .
To compare the preference models , we did the following : ?
Randomly chose a subset of k comparisons from the training set , for k ? { 100 , 200 , 400 , 800 , 1600 , 3200 }. 12 ? Trained the preference model on these comparisons .
?
Evaluated the perplexity of the trained model on the test preferences , as described in Section 4 .
For each model and training size , we averaged the perplexities from 5 trials of each competition track .
We then plotted average perplexity as a function of training size .
These graphs are shown For WMT10 and WMT11 , the best models were the IRT models , with the Gaussian parameterization converging the most rapidly and reaching the lowest perplexity .
For WMT12 , in which reference translations were excluded from the competition , four models were nearly indistinguishable : the two IRT models and the two averaged Independent Student models .
This somewhat validates the organizers ' decision to exclude the references , particularly given WMT 's use of the BOJAR ranking heuristic ( the nucleus of the Independent Student models ) for its official rankings .
13 Results for WMT10 exclude the German-English and English - German tracks , since we used these to tune our model hyperparameters .
These were set as follows .
The Dirichlet strength for each baseline was 1 .
For IRT - Gaussian : ?0 = 1.0 , ? obs = 1.0 , ?a = 0.5 , and the decision radius was 0.4 .
For IRT - Categorical : ? = 8 , ? obs = 1.0 , ?a = 0.5 , and the decision radius was 0 .
The IRT models proved the most robust at handling judge noise .
We repeated the WMT10 experiment using the same test sets , but using the unfiltered crowdsourced comparisons ( rather than " expert " 14 comparisons ) for training .
Figure 5 shows the results .
Whereas the crowdsourced noise considerably degraded the Geometric Independent Students model , the IRT models were remarkably robust .
IRT - Gaussian in particular came close to replicating the performance of Geometric Independent Students trained on the much cleaner expert data .
This is rather impressive , since the crowdsourced judges agree only 46.6 % of the time , compared to a 65.8 % agreement rate among 14 I.e. , machine translation specialists .
expert judges ( Callison - Burch et al. , 2010 ) .
Another nice property of the IRT models is that they explicitly model student ability , so they yield a natural ranking .
For training size 1600 of the WMT11 English - Czech track , Figure 6 ( left ) shows the mean student abilities learned by the IRT - Gaussian model .
The error bars show one standard deviation of the ability means ( recall that we performed 5 trials , each with a random training subset of size 1600 ) .
These results provide further insight into a case analyzed by ( Lopez , 2012 ) , which raised concern about the relative ordering of online - B , cu-bojar , and cu-marecek .
According to IRT - Gaussian 's analysis of the data , these three students are so close in ability that any ordering is essentially arbitrary .
Short of a full ranking , the analysis does suggest four strata .
Viewing one of IRT - Gaussian 's induced preference models as a heatmap 15 ( Figure 6 , right ) , four bands are discernable .
First , the reference sentences are clearly the darkest ( best ) .
Next come students 2 - 7 , followed by the slightly lighter ( weaker ) students 8- 10 , followed by the lightest ( weakest ) student 11 .
Conclusion WMT has faced a crisis of confidence lately , with researchers raising ( real and conjectured ) issues with its analytical methodology .
In this paper , we showed how WMT can restore confidence in its conclusions - by shifting the focus from rankings to relative ability .
Estimates of relative ability ( the expected head - to - head performance of system pairs over a probability space of judges and source text ) can be empirically compared , granting substance to previously nebulous questions like : 1 . Is my analysis better than your analysis ?
Rather than the current anecdotal approach to comparing competition analyses ( e.g. presenting example rankings that seem somehow wrong ) , we can empirically compare the predictive power of the models on test data .
2 . How much of an impact does judge noise have on my conclusions ?
We showed that judge noise can have a significant impact on the quality of our conclusions , if we use the wrong models .
However , the IRT - Gaussian appears to be quite noise-tolerant , giving similar-quality conclusions on both expert and crowdsourced comparisons .
