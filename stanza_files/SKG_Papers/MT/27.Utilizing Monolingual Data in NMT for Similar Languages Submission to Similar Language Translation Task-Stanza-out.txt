title
Utilizing Monolingual Data in NMT for Similar Languages : Submission to Similar Language Translation
Task
abstract
This paper describes our submission to Shared Task on Similar Language Translation in Fourth Conference on Machine Translation ( WMT 2019 ) .
We submitted three systems for Hindi ?
Nepali direction in which we have examined the performance of a Recursive Neural Network ( RNN ) based Neural Machine Translation ( NMT ) system , a semi-supervised NMT system where monolingual data of both languages is utilized using the architecture by ( Artetxe et al. , 2017 ) and a system trained with extra synthetic sentences generated using copy of source and target sentences without using any additional monolingual data .
Introduction
In this paper , we present the submission for Similar Language Translation Task in WMT 2019 .
The task focuses on improving machine translation results for three language pairs Czech -Polish ( Slavic languages ) , Hindi-Nepali ( Indo - Aryan languages ) and Spanish - Portuguese ( Romance languages ) .
The main focus of the task is to utilize monolingual data in addition to parallel data because the provided parallel data is very small in amount .
The detail of task is provided in ( Barrault et al. , 2019 ) .
We participated for Hindi-Nepali language pair and submitted three systems based on NMT for Hindi ? Nepali direction .
We have utilized monolingual data of both languages and also trained an NMT system with copy data from both sides with no additional monolingual data .
The rest of the paper is organized as follows :
We start with introduction to NMT , followed by a list of some of the existing methods for how to utilize monolingual data in NMT .
A brief introduction to unsupervised and semi-supervised NMT is provided .
We also describe in brief about two existing popular methods of training cross-lingual word embeddings in an unsupervised way .
In Section 4.3 we describe our three submitted systems for the task .
Neural Machine Translation
Many architectures have been proposed for neural machine translation .
Most famous one is RNN based encoder-decoder proposed in ( Cho et al. ) , where encoder and decoder are both recursive neural networks , encoder can be bi-directional .
After this attention based sequence to sequence models where attention is utilized to improve performance in NMT are proposed in ( Bahdanau et al. , 2014 ) , ( Luong et al. , 2015 ) .
Attention basically instructs the system about which words to focus more , while generating a particular target word .
Transformer based encoder-decoder architecture for NMT is proposed in ( Vaswani et al. , 2017 ) , which is completely based on self-attention and positional encoding .
This does not follow recurrent architecture .
Positional encoding provides the system with information of order of words .
NMT needs lots of parallel data to train a system .
This task basically focuses on how to improve performance for languages which are similar but resource scarce .
There are many language pairs for which parallel data does not exist or exist in a very small amount .
In past , to improve the performance of NMT systems various techniques like Back -Translation ( Sennrich et al. , 2016a ) , utilizing other similar language pairs through pivoting ( Cheng et al. , 2017 ) or transfer learning ( Zoph et al. , 2016 ) , complete unsupervised architectures ( Artetxe et al. , 2017 ) ( Lample et al. , 2018 ) and many others have been proposed .
Utilizing monolingual data in NMT
There has been good amount of work done on how we can utilize monolingual data to improve performance of an NMT system .
Back - Translation was introduced by ( Sennrich et al. , 2016 b ) , to utilize monolingual data of target language .
This requires a translation system in opposite direction .
In ( Sennrich et al. , 2016 b ) , a method where empty sentences are provided in the input for target side monolingual data is also evaluated , backtranslation performs better than this .
In iterative Back - Translation , systems in both directions improve each other ( Hoang et al. , 2018 ) , it is done in an incremental fashion .
To generate backtranslated data , current system in opposite direction is utilized .
In ( Currey et al. , 2017 ) , target side monolingual data is copied to generate source synthetic translations and the system is trained by combining this synthetic data with parallel data .
In ( Zhang and Zong , 2016 ) , source side monolingual data is utilized to iteratively generate synthetic sentences from the same model .
In ( Domhan and Hieber , 2017 ) , there is a separate layer for target side language model in training , decoder utilize both source dependent and source independent representations to generate a particular target word .
In ( Burlot and Yvon , 2018 ) , it is claimed that quality of back - translated sentences is important .
Recently many systems have been proposed for Unsupervised NMT , where only monolingual data is utilized .
The Unsupervised NMT approach proposed in ( Artetxe et al. , 2017 ) follows an architecture where encoder is shared and decoder is separate for each language .
Encoder tries to map sentences from both languages in the same space , which is supported by cross-lingual word embeddings .
They fix cross-lingual word embeddings in the encoder while training , which helps in generating cross-lingual sentence representations in the same space .
The system with one shared encoder and two separate decoders with no parallel data is trained by iterating between Denoising and Back - Translation .
Denoising tries to generate the correct sentence from noisy sentences , in that way the decoder is learning how to generate sentences in that particular language .
These noisy sentences are created by shuffling words within a window .
If the system is only trained with denoising then it may turn out to be a denoising auto-encoder .
So they have also introduced back -translation in the training process to introduce translation task .
Training is done by alternating between denoising and back -translation for mini-batches if parallel data is not available .
In a semi-supervised setting if some amount of parallel data is available , training alternates between denoising , back - translation and parallel sentences .
In ( Lample et al. , 2018 ) , encoder and decoder both are shared between the languages .
Training is done by alternating between denoising and back -translation .
Initialization is performed using a system trained on wordword translated sentences which is performed using cross-lingual word embeddings trained using MUSE ( Conneau et al. , 2017 ) .
They also utilize a discriminator which tries to identify the language from the encoder representations , this leads to adversarial training .
Cross-lingual word embeddings Cross-lingual word embeddings tries to map two word embedding spaces of different languages in the same space .
The basic assumption for generating the cross-lingual word embeddings in most papers is that both the embedding spaces must be isometric .
Cross-lingual word embeddings is generated by learning a linear transformation which minimizes the distances between words given in a dictionary .
There are many methods proposed for training cross-lingual word embeddings in an unsupervised way .
While training cross-lingual word embeddings in an unsupervised manner there is no dictionary available , only the monolingual embeddings are available .
In ( Artetxe et al. , 2018 ) , cross lingual word embeddings are generated following a series of steps which involves : normalization of the embeddings so they can be used together to utilize for a distance metric , unsupervised initialization using normalized embeddings , self - learning framework using adversarial training where it iterates between creating the dictionary and finding the optimal mapping , and some weighting refinement over this .
Through these steps a transformation of these spaces to a common space is learnt .
In ( Conneau et al. , 2017 ) an adversarial training process is followed where discriminator tries to correctly identify the language using its representation and the mapping matrix W tries to confuse the discriminator .
System Overview
This section describes the specification of the systems submitted in detail .
We have submitted sys-tems for Hindi-Nepali language pair in Hindi ? Nepali direction .
Hindi and Nepali both are Indo-Aryan languages and are very similar to each other .
They share a significant portion of the vocabulary and similar word orders .
The three submitted systems are : ?
A pure RNN based NMT system ?
Semi-supervised RNN based NMT system ?
Utilization of copied data in RNN based NMT First system is pure RNN based NMT system .
To train this we have utilized only parallel corpora .
Second system is trained using a semi-supervised NMT system where monolingual data from both languages is utilized .
We have utilized architecture proposed in ( Artetxe et al. , 2017 ) where encoder is shared and decoders are separate for each language and model is trained by alternating between denoising and back -translation .
This architecture can also be utilized for completely unsupervised setting .
Third system is also a pure RNN based NMT system where additional parallel data ( synthetic data ) is created by copying source side sentences to target side and target side sentences to source side , but we do this only for the available parallel sentences , no additional monolingual data is utilized .
In this way the amount of available data becomes three times of the original data .
All the data is combined together , shuffled and then provided to the NMT system , there is no identification provided to distinguish between parallel data and copy data .
To train all three systems we have utilized the implementation of ( Artetxe et al. , 2017 ) .
4 Experimental Details
Dataset
We have utilized monolingual corpora of both languages in our primary system .
The dataset details are given in Table 1 . Hindi-Nepali parallel data is provided in the task , which contains 65505 sentences .
Hindi monolingual corpora is IITB Hindi monolingual corpora ( Kunchukuttan et al. , 2018 ) . Nepali monolingual sentences are created using the monolingual data of Wikipedia and Common-Crawl provided for Parallel corpus filtering task 1 1 http://www.statmt.org/wmt19/ parallel-corpus-filtering.html by separating each sentence using | and keeping sentences of length 500 and less .
Dataset
Preprocessing Sentences are preprocessed using tokenization and Byte Pair Encoding ( BPE ) .
Sentences are tokenized for both hindi and nepali using IndicNLP 2 library .
This tokenized data is preprocessed using BPE .
Number of merge operations for BPE is set to 20000 for both languages and learnt separately for each language .
The results may improve if we learn BPE jointly because both languages are similar .
Byte pair Encoding is learnt using the implementation by ( Sennrich et al. , 2016 b ) .
Monolingual embeddings are trained using Fast- Text 3 ( Bojanowski et al. , 2017 ) using bpe applied monolingual data for both languages .
The dimension of embeddings is set to 100 .
Cross-lingual embeddings are created using VecMap ( Artetxe et al. , 2018 ) .
System detail Table 2 reports BLEU score for the test and dev data for all three systems .
We have not utilized dev data while training .
We have used encoder and decoder with 2 layers , 600 hidden units each , GRU cells , batch size of 50 and maximum sentence length of 50 .
Adam optimizer is used with learning rate 0.0002 .
We have trained all three systems with fixed 300000 iterations .
The number of sentences in test and dev data is 1567 and 3000 respectively .
The BLEU score for test data is provided by task organizers and for dev data BLEU score is calculated using multi-bleu.pl from moses toolkit ( Koehn et al. , 2007 ) .
System Test
Results
As it is clear from the results in Table 2 that the system with only parallel data is performing better than when we are utilizing monolingual data .
To answer why this is happening , a study of size and quality of monolingual data , the study of ratio of monolingual and parallel data provided to the system is required .
The intuition behind using copied data with parallel data is , that both the languages are similar and this may provide more data to the system .
But the results show the system is getting confused as we are providing all the data together without any distinguishing mark between parallel and copied sentences .
For the same sentence both original translation and its copy is given in the output which may be causing confusion .
Summary
In this paper we have explained about systems submitted for Similar Language Translation task in WMT 2019 .
We have reported results for a semi-supervised technique which utilizes denoising and back -translation .
We have utilized lots of monolingual data together with available parallel data for training a neural machine translation system which share encoder and have separate decoders for each language , in a semi-supervised setting .
A study of size and quality of monolingual data is required to analyze the performance which is left as future work .
We have also explained results for utilizing copied data with parallel data and compared both the above mentioned techniques with a pure RNN based NMT system .
Table 1 : 1 Dataset details
Table 2 : 2 Experimental results ( BLEU scores ) Dev Basic 3.5 4.6 With Monolingual Data 2.8 3.27
With copy data 2.7 4.38
