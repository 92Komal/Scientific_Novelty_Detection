title
Fast Generation of Translation Forest for Large- Scale SMT Discriminative Training
abstract
Although discriminative training guarantees to improve statistical machine translation by incorporating a large amount of overlapping features , it is hard to scale up to large data due to decoding complexity .
We propose a new algorithm to generate translation forest of training data in linear time with the help of word alignment .
Our algorithm also alleviates the oracle selection problem by ensuring that a forest always contains derivations that exactly yield the reference translation .
With millions of features trained on 519K sentences in 0.03 second per sentence , our system achieves significant improvement by 0.84 BLEU over the baseline system on the NIST Chinese - English test sets .
Introduction Discriminative model ( Och and Ney , 2002 ) can easily incorporate non-independent and overlapping features , and has been dominating the research field of statistical machine translation ( SMT ) in the last decade .
Recent work have shown that SMT benefits a lot from exploiting large amount of features ( Liang et al. , 2006 ; Tillmann and Zhang , 2006 ; Watanabe et al. , 2007 ; Blunsom et al. , 2008 ; Chiang et al. , 2009 ) .
However , the training of the large number of features was always restricted in fairly small data sets .
Some systems limit the number of training examples , while others use short sentences to maintain efficiency .
Overfitting problem often comes when training many features on a small data ( Watanabe et al. , 2007 ; Chiang et al. , 2009 ) .
Obviously , using much more data can alleviate such problem .
Furthermore , large data also enables us to globally train millions of sparse lexical features which offer accurate clues for SMT .
Despite these advantages , to the best of our knowledge , no previous discriminative training paradigms scale up to use a large amount of training data .
The main obstacle comes from the complexity of packed forests or n-best lists generation which requires to search through all possible translations of each training example , which is computationally prohibitive in practice for SMT .
To make normalization efficient , contrastive estimation ( Smith and Eisner , 2005 ; Poon et al. , 2009 ) introduce neighborhood for unsupervised log-linear model , and has presented positive results in various tasks .
Motivated by these work , we use a translation forest ( Section 3 ) which contains both " reference " derivations that potentially yield the reference translation and also neighboring " non- reference " derivations that fail to produce the reference translation .
1 However , the complexity of generating this translation forest is up to O(n 6 ) , because we still need biparsing to create the reference derivations .
Consequently , we propose a method to fast generate a subset of the forest .
The key idea ( Section 4 ) is to initialize a reference derivation tree with maximum score by the help of word alignment , and then traverse the tree to generate the subset forest in linear time .
Besides the efficiency improvement , such a forest allows us to train the model without resort - r 1 X ? ?X 1 bei X 2 , X 1 was X 2 ? e 2 r 2 X ? ?qiangshou bei X 1 , the gunman was X 1 ? e 3 r 3 X ? ? jingfang X 1 , X 1 by the police ?
e 4 r 4 X ? ? jingfang X 1 , police X 1 ? e 5 r 5 X ? ?qiangshou , the gunman ?
e 6 r 6 X ? ? jibi , shot dead ?
Figure 1 : A translation forest which is the running example throughout this paper .
The reference translation is " the gunman was killed by the police " .
( 1 ) Solid hyperedges denote a " reference " derivation tree t 1 which exactly yields the reference translation .
( 2 ) Replacing e 3 in t 1 with e 4 results a competing non-reference derivation t 2 , which fails to swap the order of X 3,4 . ( 3 ) Removing e 1 and e 5 in t 1 and adding e 2 leads to another reference derivation t 3 .
Generally , this is done by deleting a node X 0,1 . ing to constructing the oracle reference ( Liang et al. , 2006 ; Watanabe et al. , 2007 ; Chiang et al. , 2009 ) , which is non-trivial for SMT and needs to be determined experimentally .
Given such forests , we globally learn a log-linear model using stochastic gradient descend ( Section 5 ) .
Overall , both the generation of forests and the training algorithm are scalable , enabling us to train millions of features on large-scale data .
To show the effect of our framework , we globally train millions of word level context features motivated by word sense disambiguation ( Chan et al. , 2007 ) together with the features used in traditional SMT system ( Section 6 ) .
Training on 519K sentence pairs in 0.03 seconds per sentence , we achieve significantly improvement over the traditional pipeline by 0.84 BLEU .
Synchronous Context Free Grammar
We work on synchronous context free grammar ( SCFG ) ( Chiang , 2007 ) based translation .
The elementary structures in an SCFG are rewrite rules of the form : X ? ? , ? where ? and ? are strings of terminals and nonterminals .
We call ? and ? as the source side and the target side of rule respectively .
Here a rule means a phrase translation ( Koehn et al. , 2003 ) or a translation pair that contains nonterminals .
We call a sequence of translation steps as a derivation .
In context of SCFG , a derivation is a se-quence of SCFG rules {r i }.
Translation forest ( Mi et al. , 2008 ; ) is a compact representation of all the derivations for a given sentence under an SCFG ( see Figure 1 ) .
A tree t in the forest corresponds to a derivation .
In our paper , tree means the same as derivation .
More formally , a forest is a pair ?V , E ? , where V is the set of nodes , E is the set of hyperedge .
For a given source sentence f = f n 1 , Each node v ?
V is in the form X i , j , which denotes the recognition of nonterminal X spanning the substring from the i through j ( that is f i + 1 ...f j ) .
Each hyperedge e ?
E connects a set of antecedent to a single consequent node and corresponds to an SCFG rule r( e ) .
Our Translation Forest
We use a translation forest that contains both " reference " derivations that potentially yield the reference translation and also some neighboring " nonreference " derivations that fail to produce the reference translation .
Therefore , our forest only represents some of the derivations for a sentence given an SCFG rule table .
The motivation of using such a forest is efficiency .
However , since this space contains both " good " and " bad " translations , it still provides evidences for discriminative training .
First see the example in Figure 1 .
The derivation tree t 1 represented by solid hyperedges is a reference derivation .
We can construct a non-reference derivation by making small change to t 1 .
By replacing the e 3 of t 1 with e 4 , we obtain a non-reference deriva-tion tree t 2 .
Considering the rules in each derivation , the difference between t 1 and t 2 lies in r 3 and r 4 .
Although r 3 has a same source side with r 4 , it produces a different translation .
While r 3 provides a swapping translation , r 4 generates a monotone translation .
Thus , the derivation t 2 fails to move the subject " police " to the behind of verb " shot dead " , resulting a wrong translation " the gunman was police shot dead " .
Given such derivations , we hope that the discriminative model is capable to explain why should use a reordering rule in this context .
Generally , our forest contains all the reference derivations RT for a sentence given a rule table , and some neighboring non-reference derivations N T , which can be defined from RT .
More formally , we call two hyperedges e 1 and e 2 are competing hyperedges , if their corresponding rules r( e 1 ) = ?
1 , ? 1 ? and r( e 2 ) = ?
2 , ? 2 ? : ? 1 = ? 2 ? ? 1 ? = ? 2 ( 1 ) This means they give different translations for a same source side .
We use C( e ) to represent the set of competing hyperedges of e .
Two derivations t 1 = ?V 1 , E 1 ? and t 2 = ?V 2 , E 2 ? are competing derivations if there exists e 1 ? E 1 and e 2 ? E 2 : 2 V 1 = V 2 ? E 1 ? e 1 = E 2 ? e 2 ? e 2 ? C( e 1 ) ( 2 )
In other words , derivations t 1 and t 2 only differ in e 1 and e 2 , and these two hyperedges are competing hyperedges .
We use C( t ) to represent the set of competing derivations of tree t , and C( t , e ) to represent the set of competing derivations of t if the competition occurs in hyperedge e in t. Given a rule table , the set of reference derivations RT for a sentence is determined .
Then , the set of non-reference derivations N T can be defined from RT : ? t?RT C( t ) ( 3 ) Overall , our forest is the compact representation of RT and N T .
Fast Generation
It is still slow to calculate the entire forest defined in Section 3 , therefore we use a greedy decoding for fast generating a subset of the forest .
Starting form a reference derivation , we try to slightly change the derivation into a new reference derivation .
During this process , we collect the competing derivations of reference derivations .
We describe the details of local operators for changing a derivation in section 4.1 , and then introduce the creation of initial reference derivation with max score in Section 4.2 .
For example , given derivation t 1 , we delete the node X 0,1 and the related hyperedge e 1 and e 5 .
Fixing the other nodes and edges , we try to add a new edge e 2 to create a new reference translation .
In this case , if rule r 2 really exists in our rule table , we get a new reference derivation t 3 .
After constructing t 3 , we first collect the new tree and C( t 3 , e 2 ) .
Then , we will move to t 3 , if the score of t 3 is higher than t 2 .
Notably , if r 2 does not exist in the rule table , we fail to create a new reference derivation .
In such case , we keep the origin derivation unchanged .
Algorithm 1 shows the process of generation .
3
The input is a reference derivation t , and the output is a new derivation and the generated derivations .
The list used for storing forest is initialized with the input tree ( line 2 ) .
We visit the nodes in t in postorder ( line 3 ) .
For each node v , we first append the competing derivations C( t , e ) to list , where e is incoming edge of v ( lines 4 - 5 ) .
Then , we apply operators on the child nodes v from left to right ( lines 6 - 13 ) .
The operators returns a reference derivation t n ( line 7 ) .
If it is new ( line 8 ) , we collect both the t n ( line 9 ) , and also the competing derivations C( t n , e ? ) of the new derivation on those edges e ? which only occur in the new derivation ( lines 10 - 11 ) .
Finally , if the new derivation has a larger score , we will replace the origin derivation with new one ( lines 12 - 13 ) .
Although there is a two -level loop for visiting nodes ( line 3 and 6 ) , each node is visited only one time in the inner loops .
Thus , the complexity is linear with the number of nodes # node .
Considering that the number of source word ( also leaf node here ) is less than the total number of nodes and is more than ?(# node + 1 ) / 2 ? , the time complexity of the process is also linear with the number of source word .
Lexicalize and Generalize
The function OPERATE in Algorithm 1 uses two operators to change a node : lexicalize and generalize .
Figure 2 shows the effects of the two operators .
The lexicalize operator works on nonterminal nodes .
It moves away a nonterminal node and attaches the children of current node to its parent .
In Figure 2 ( b ) , the node X 0,1 is deleted , requiring a more lexicalized rule to be applied to the parent node X 0,4 ( one more terminal in the source side ) .
We constrain the lexicalize operator to apply on pre-terminal nodes whose children are all terminal nodes .
In contrast , the generalize operator works on terminal nodes and inserts a nonterminal node between current node and its parent node .
This operator generalizes over the continuous terminal sibling nodes left to the current node ( including the current node ) .
Generalizing the node bei in Figure 2 ( b ) results Figure 2 ( c ) .
A new node X 0,2 is inserted as the parent of node qiangshou and node bei .
Notably , there are two steps when apply an operator .
Suppose we want to lexicalize the node X 0,1 in t 1 of Figure 1 , we first delete the node X 0,1 and related edge e 1 and e 5 , then we try to add the new edge e 2 .
Since rule table is fixed , the second step is a process of decoding .
Therefore , sometimes we may fail to create a new reference derivation ( like r 2 may not exist in the rule table ) .
In such case , we keep the origin derivation unchanged .
The changes made by the two operators are local .
Considering the change of rules , the lexicalize operator deletes two rules and adds one new rule , while the generalize operator deletes one rule and adds two new rules .
Such local changes provide us with a way to incrementally calculate the scores of new derivations .
We use this method motivated by Gibbs Sampler ( Blunsom et al. , 2009 ) which has been used for efficiently learning rules .
The different lies in that we use the operator for decoding where the rule table is fixing .
Initialize a Reference Derivation
The generation starts from an initial reference derivation with max score .
This requires bi-parsing ( Dyer , 2010 ) over the source sentence f and the reference translation e.
In practice , we may face three problems .
First is efficiency problem .
Exhaustive search over the space under SCFG requires O ( | f | 3 | e| 3 ) .
To parse quickly , we only visit the tight consistent ( Zhang et al. , 2008 ) bi-spans with the help of word alignment a .
Only visiting tight consistent spans greatly speeds up bi-parsing .
Besides efficiency , adoption of this constraint receives support from the fact that heuristic SCFG rule extraction only extracts tight consistent initial phrases ( Chiang , 2007 ) .
Second is degenerate problem .
If we only use the features as traditional SCFG systems , the biparsing may end with a derivation consists of some giant rules or rules with rare source / target sides , which is called degenerate solution ( DeNero et al. , 2006 ) .
That is because the translation rules with rare source / target sides always receive a very high translation probability .
We add a prior score log (# rule ) for each rule , where # rule is the number of occurrence of a rule , to reward frequent reusable rules and derivations with more rules .
Finally , we may fail to create reference derivations due to the limitation in rule extraction .
We create minimum trees for ( f , e , a ) using shift- reduce ( Zhang et al. , 2008 ) .
Some minimum rules in the trees may be illegal according to the definition of Chiang ( 2007 ) .
We also add these rules to the rule table , so as to make sure every sentence is reachable given the rule table .
A source sentence is reachable given a rule table if reference derivations exists .
We refer these rules as added rules .
However , this may introduce rules with more than two variables and increase the complexity of bi-parsing .
To tackle this problem , we initialize the chart with minimum parallel tree from the Zhang et al . ( 2008 ) algorithm , ensuring that the bi-parsing has at least one path to create a reference derivation .
Then we only need to consider the traditional rules during bi-parsing .
Training
We use the forest to train a log-linear model with a latent variable as describe in Blunsom et al . ( 2008 ) .
The probability p( e|f ) is the sum over all possible derivations : p( e|f ) = ? t?( e , f ) p( t , e|f ) ( 4 ) where ?( e , f ) is the set of all possible derivations that translate f into e and t is one such derivation .
4 for n = 1 to N do 5 : t n ? INITIAL ( f n , e n , a n ) 6 : i ? 0 7 : for m = 0 to M do 8 : for n = 0 to N do 9 : ? ? LEARNRATE ( i ) 10 : ( ?L( w i , t n ) , t n ) ?GENERATE ( t n ) 11 : w i ? w i + ? ? ?L( w i , t n ) 12 : i ? i + 1 13 : return ? M N i=1 w i M N
This model defines the conditional probability of a derivation t and the corresponding translation e given a source sentence f as : p( t , e|f ) = exp ? i ? i h i ( t , e , f ) Z ( f ) ( 5 ) where the partition function is Z( f ) = ? e ? t?( e , f ) exp ? i ?
i h i ( t , e , f ) ( 6 )
The partition function is approximated by our forest , which is labeled as Z(f ) , and the derivations that produce reference translation is approximated by reference derivations in Z ( f ) .
We estimate the parameters in log-linear model using maximum a posteriori ( MAP ) estimator .
It maximizes the likelihood of the bilingual corpus S = {f n , e n } N n=1 , penalized using a gaussian prior ( L2 norm ) with the probability density function p 0 ( ? i ) ? exp (? 2 i /2 ? 2 ) .
We set ?
2 to 1.0 in our experiments .
This results in the following gradient : ?L ? i = E p( t | e , f ) [ h i ] ? E p( e|f ) [ h i ] ? ? i ? 2 ( 7 ) We use an online learning algorithm to train the parameters .
We implement stochastic gradient descent ( SGD ) recommended by Bottou .
5
The dynamic learning rate we use is N ( i+ i 0 ) , where N is the number of training example , i is the training iteration , and i 0 is a constant number used to get a initial learning rate , which is determined by calibration .
Algorithm 2 shows the entire process .
We first create an initial reference derivation for every training examples using bi-parsing ( lines 4 - 5 ) , and then online learn the parameters using SGD ( lines 6 - 12 ) .
We use the GENERATE function to calculate the gradient .
In practice , instead of storing all the derivations in a list , we traverse the tree twice .
The first time is calculating the partition function , and the second time calculates the gradient normalized by partition function .
During training , we also change the derivations ( line 10 ) .
When training is finished after M epochs , the algorithm returns an averaged weight vector ( Collins , 2002 ) to avoid overfitting ( line 13 ) .
We use a development set to select total epoch m , which is set as M = 5 in our experiments .
Experiments
Our method is able to train a large number of features on large data .
We use a set of word context features motivated by word sense disambiguation ( Chan et al. , 2007 ) to test scalability .
A word level context feature is a triple ( f , e , f + 1 ) , which counts the number of time that f is aligned to e and f + 1 occurs to the right of f .
Triple ( f , e , f ?1 ) is similar except that f ?1 locates to the left of f .
We retain word alignment information in the extracted rules to exploit such features .
To demonstrate the importance of scaling up the size of training data and the effect of our method , we compare three types of training configurations which differ in the size of features and data .
MERT .
We use MERT ( Och , 2003 ) to training 8 features on a small data .
The 8 features is the same as Chiang ( 2007 ) including 4 rule scores ( direct and reverse translation scores ; direct and reverse lexical translation scores ) ; 1 target side language model score ; 3 penalties for word counts , extracted rules and glue rule .
Actually , traditional pipeline often uses such configuration .
Perceptron .
We also learn thousands of context word features together with the 8 traditional features on a small data using perceptron .
Following ( Chiang et al. , 2009 ) coder to generate n-best lists for training .
The complexity of CKY decoding limits the training data into a small size .
We fix the 8 traditional feature weights as MERT to get a comparable results as MERT .
Our Method .
Finally , we use our method to train millions of features on large data .
The use of large data promises us to use full vocabulary of training data for the context word features , which results millions of fully lexicalized context features .
During decoding , when a context feature does not exit , we simply ignore it .
The weights of 8 traditional features are fixed the same as MERT also .
We fix these weights because the translation feature weights fluctuate intensely during online learning .
The main reason may come from the degeneration solution mentioned in Section 4.2 , where rare rules with very high translation probability are selected as the reference derivations .
Another reason could be the fact that translation features are dense intensify the fluctuation .
We leave learning without fixing the 8 feature weights to future work .
Data
We focus on the Chinese-to - English translation task in this paper .
The bilingual corpus we use contains 519 , 359 sentence pairs , with an average length of 16.5 in source side and 20.3 in target side , where 186 , 810 sentence pairs ( 36 % ) are reachable ( without added rules in Section 4.2 ) .
The monolingual data includes the Xinhua portion of the GIGAWORD corpus , which contains 238M English words .
We use the NIST evaluation sets of 2002 ( MT02 ) as our development set , and sets of MT03 / MT04 / MT05 as test sets .
Table 2 shows the statistics of all bilingual corpus .
We use GIZA ++ ( Och and Ney , 2003 ) Table 2 : Effect of our method comparing with MERT and perceptron in terms of BLEU .
We also compare our fast generation method with different data ( only reachable or full data ) .
# Data is the size of data for training the feature weights .
* means significantly ( Koehn , 2004 ) better than MERT ( p < 0.01 ) .
word alignment in both directions , and grow-diagfinal - and ( Koehn et al. , 2003 ) to generate symmetric word alignment .
We extract SCFG rules as described in Chiang ( 2007 ) and also added rules ( Section 4.2 ) .
Our algorithm runs on the entire training data , which requires to load all the rules into the memory .
To fit within memory , we cut off those composed rules which only happen once in the training data .
Here a composed rule is a rule that can be produced by any other extracted rules .
A 4 - grams language model is trained by the SRILM toolkit ( Stolcke , 2002 ) .
Case-insensitive NIST BLEU4 ( Papineni et al. , 2002 ) is used to measure translation performance .
The training data comes from a subset of the LDC data including LDC2002E18 , LDC2003E07 , LDC2003E14 , Hansards portion of LDC2004T07 , LDC2004T08 and LDC2005T06 .
Since the rule table of the entire data is too large to be loaded to the memory ( even drop one- count rules ) , we remove many sentence pairs to create a much smaller data yet having a comparable performance with the entire data .
The intuition lies in that if most of the source words of a sentence need to be translated by the added rules , then the word alignment may be highly crossed and the sentence may be useless .
We create minimum rules from a sentence pair , and count the number of source words in those minimum rules that are added rules .
For example , suppose the result minimum rules of a sentence contain r 3 which is an added rule , then we count 1 time for the sentence .
If the number of such source word is more than 10 % of the total number , we will drop the sentence pair .
We compare the performances of MERT setting on three bilingual data : the entire data that contains 42.3 M Chinese and 48.2 M English words ; 519 K data that contains 8.6 M Chinese and 10.6 M English words ; FBIS ( LDC2003E14 ) parts that contains 6.9M Chinese and 9.1M English words .
They produce 33.11/32.32/30.47 BLEU tested on MT05 respectively .
The performance of 519K data is comparable with that of entire data , and much higher than that of FBIS data .
Result Table 3 shows the performance of the three different training configurations .
The training of MERT and perceptron run on MT02 .
For our method , we compare two different training sets : one is trained on all 519K sentence pairs , the other only uses 186 K reachable sentences .
Although the perceptron system exploits 2.4 K features , it fails to produce stable improvements over MERT .
The reason may come from overfitting , since the training data for perceptron contains only 878 sentences .
However , when use our method to learn the word context feature on the 519K data , we significantly improve the performance by 0.84 points on the entire test sets ( ALL ) .
The improvements range from 0.60 to 1.16 points on MT03- 05 .
Because we use the full vocabulary , the number of features increased into 13.9 millions , which is impractical to be trained on the small development set .
These results confirm the necessity of exploiting more features and learning the parameters on large data .
Meanwhile , such results also demonstrate that we can benefits from the forest generated by our fast method instead of traditional CKY algorithm .
Not surprisingly , the improvements are smaller when only use 186K reachable sentences .
Sometimes we even fail to gain significant improvement .
This verifies our motivation to guarantee all sentence 886 are reachable , so as to use all training data .
Speed
How about the speed of our framework ?
Our method learns in 32 mlliseconds / sentence .
Figure 3 shows training times ( including forest generation and SGD training ) versus sentence length .
The plot confirms that our training algorithm scales linearly .
If we use n-best lists which generated by CKY decoder as MERT , it takes about 3105 milliseconds / sentence for producing 100 - best lists .
Our method accelerates the speed about 97 times ( even though we search twice to calculate the gradient ) .
This shows the efficiency of our method .
The procedure of training includes two steps .
( 1 ) Bi-parsing to initialize a reference derivation with max score .
( 2 ) Training procedure which generates a set of derivations to calculate the gradient and update parameters .
Step ( 1 ) only runs once .
The average time of processing a sentence for each step is about 9.5 milliseconds and 30.2 milliseconds respectively .
For simplicity we do not compress the generated derivations into forests , therefore the size of resulting derivations is fairly small , which is about 265.8 for each sentence on average , where 6.1 of them are reference derivations .
Furthermore , we use lexicalize operator more often than generalize operator ( the ration between them is 1.5 to 1 ) .
Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable ( thus small ) rules .
Related Work Minimum error rate training ( Och , 2003 ) is perhaps the most popular discriminative training for SMT .
However , it fails to scale to large number of features .
Researchers have propose many learning algorithms to train many features : perceptron ( Shen et al. , 2004 ; Liang et al. , 2006 ) , minimum risk ( Smith and Eisner , 2006 ; , MIRA ( Watanabe et al. , 2007 ; Chiang et al. , 2009 ) , gradient descent ( Blunsom et al. , 2008 Blunsom and Osborne , 2008 ) .
The complexity of n-best lists or packed forests generation hamper these algorithms to scale to a large amount of data .
For efficiency , we only use neighboring derivations for training .
Such motivation is same as contrastive estimation ( Smith and Eisner , 2005 ; Poon et al. , 2009 ) .
The difference lies in that the previous work actually care about their latent variables ( pos tags , segmentation , dependency trees , etc ) , while we are only interested in their marginal distribution .
Furthermore , we focus on how to fast generate translation forest for training .
The local operators lexicalize / generalize are use for greedy decoding .
The idea is related to " pegging " algorithm ( Brown et al. , 1993 ) and greedy decoding ( Germann et al. , 2001 ) .
Such types of local operators are also used in Gibbs sampler for synchronous grammar induction ( Blunsom et al. , 2009 ; Cohn and Blunsom , 2009 ) .
Conclusion and Future Work
We have presented a fast generation algorithm for translation forest which contains both reference derivations and neighboring non-reference derivations for large-scale SMT discriminative training .
We have achieved significantly improvement of 0.84 BLEU by incorporate 13.9 M feature trained on 519 K data in 0.03 second per sentence .
In this paper , we define the forest based on competing derivations which only differ in one rule .
There may be better classes of forest that can produce a better performance .
It 's interesting to modify the definition of forest , and use more local operators to increase the size of forest .
Furthermore , since the generation of forests is quite general , it 's straight to apply our forest on other learning algorithms .
Finally , we hope to exploit more features such as reordering features and syntactic features so as to further improve the performance .
Figure 2 : 2 Figure 2 : Lexicalize and generalize operators over t 1 ( part ) in Figure 1 .
Although here only shows the nodes , we also need to change relative edges actually .
( 1 ) Applying lexicalize operator on the non-terminal node X 0,1 in ( a ) results a new derivation shown in ( b ) .
( 2 ) When visiting bei in ( b ) , the generalize operator changes the derivation into ( c ) .
