title
Lexical Micro-adaptation for Neural Machine Translation
abstract
This work is inspired by a typical machine translation industry scenario in which translators make use of in-domain data for facilitating translation of similar or repeating sentences .
We introduce a generic framework applied at inference in which a subset of segment pairs are first extracted from training data according to their similarity to the input sentences .
These segments are then used to dynamically update the parameters of a generic NMT network , thus performing a lexical micro-adaptation .
Our approach demonstrates strong adaptation performance to new and existing datasets including pseudo in-domain data .
We evaluate our approach on a heterogeneous English - French training dataset showing accuracy gains on all evaluated domains when compared to strong adaptation baselines .
Introduction
High-quality domain-specific translation is crucial for nowadays machine translation industry , which has adopted neural machine translation ( NMT ) as its dominating paradigm [ 1 , 2 , 3 , 4 , 5 ] .
The data-driven nature of NMT conditions the quality of translations to the availability of large volumes of adequate training resources for a given domain .
Despite that an ever increasing amount of data is becoming available , domain-specific resources are usually scarce and the most common approach when dealing with domain specific data is either to train multi-domain models [ 6 ] and dynamically provide a token to select the domain or to fine - tune a generic translation model by running additional learning iterations ( fine-tuning ) over in - domain data [ 7 ] .
However , these solutions involving prior training of models are a ) expensive on industrial production workflows , given the large number of specialised models usually required , resulting from the combination of genres , domains , styles , customer products , etc , b ) impracticable and not flexible - enough to account for further fine-tuning to specific translators , covering very specific datasets [ 8 , 9 ] .
This work covers a typical industry scenario where highquality translations are needed of repetitive material for which similar examples are available in the training dataset ( e.g. translation of product manuals , drafts of legislation , technical documentation , etc. ) .
Texts to be translated are relatively large and built from semantically related sentences .
In this context translations are usually human post-edited , consequently , making available a valuable knowledge that could be incorporated by MT engines to further boost translation of upcoming documents .
We suggest a simple but yet powerful micro-adaptation framework applied on - the -fly at inferencetime in two steps : first , a reduced set of training instances similar to input sentences are retrieved from the training dataset ; second , such retrieved sentences together with their corresponding human translations are used to update the parameters of a generic NMT network .
These two steps being repeated for each new translation batch .
A main drawback of our micro-adaptation framework is the additional workload when translating new documents .
However , little overhead is introduced by sentence retrieval , which can be performed very efficiently , while adaptation can be speed up by reducing the number of training examples it considers .
The contributions of this paper are the following : we analyse the adaptation ability of our framework under reduced training data conditions and compare this ability to traditional adaption process ; an extensive analysis of the different parameters of our framework is conducted ; we evaluate the performance of different similarity measures to collect training examples related to input sentences ; and we assess the ability of the presented framework to further improve performance using out - of- domain datasets .
We briefly introduce neural machine translation in Section 2 .
Section 3 presents the similarity measures considered in this work .
Section 4 reports on the experiments conducted to evaluate the proposed framework .
Section 5 outlines related work and finally , Section 6 draws conclusions and details further work .
Neural Machine Translation
Our micro-adaptation framework is built on top of the stateof - the- art Transformer model introduced by [ 5 ] .
A neural network following the encoder-decoder architecture , where each word x j in the input sentence x J 1 is encoded in a continuous space .
Fixed positional embeddings are also added to the word vectors to represent a word embedding xj .
The encoder is a self-attentive module that maps an input sequence of words xJ 1 into a sequence of continuous representations h J 1 = H enc ( x J 1 ; ? enc ) where ? enc are encoder parameters .
The decoder is also a self-attentive module that at each time step outputs a single hidden state s i = H dec ( h J 1 , ?< i ; ? dec ) , conditioned on the sequence of previously seen embedded target words ?< i and the encoder outputs h J 1 , where ? dec are decoder parameters .
The hidden state s i is projected to the output vocabulary and normalised with a softmax operation resulting in a probability distribution over target words : p(y i |y < i , x J 1 ) = sof tmax ( W ? s i + b ) .
Parameters ( ? enc , ? dec , . . . ) of the model need to be learned in order to generate high-quality translations .
The standard way of training neural MT networks is by minimising cross entropy of the training data .
Distance ( error ) between hypotheses and reference translations are measured and parameters are moved one step towards the direction that reduces the error .
The length of the step is moderated by the learning rate , that may vary according to the optimisation algorithm .
As in the original research work of Transformer , in this work we use the Adam [ 10 ] optimiser .
During training , the optimisation algorithm goes through multiple iterations or several so-called epochs .
Figure 1 ( left ) illustrates the training process with a model built after I iterations .
In the case of micro-adaptation , the network is updated with a reduced set of examples considered similar to input sentences .
Figure 1 ( right ) illustrates the micro-adaptation process where K additional iterations are performed .
Since micro-adaptation is performed using few training samples , it results extremely important to control overfitting and catastrophic forgetting risk .
In particular for training samples with low similarity to test sentences .
Selection of Similar Sentences
As illustrated by Figure 1 ( right ) , our framework collects onthe-fly training sentence pairs similar to test sentences .
Such sentences are then used to update the network parameters ( micro-adaptation ) .
Thus , the quality of the collected sentences is crucial to improve translation performance .
There exists many methods to calculate the similarity between two sentences .
We employ and compare two standard methodologies .
The first is fully lexicalised , based on n-gram overlaps .
The second employs distributed word representations .
Lexical Overlap
Our initial method relies on fuzzy matches .
This is , for each input sentence to translate ( s t ) we identify the N most similar training sentences ( s T ) as measured by a fuzzy match score ( F M ) .
We define F M ( s t , s T ) as : F M ( s t , s T ) = 1 ? ED ( s t , s T ) max ( | s t | , |s T | ) ( 1 ) where ED(s t , s T ) is the edit distance between s t and s T , and | s | is the length of s .
Table 1 shows examples of a test sentence ( s t ) and training sentences ( s T1 and s T2 ) with scores F M ( s t , s T1 ) = 1 ? 1 6 = 0.8 ? 3 ( one substitution ) and F M ( s t , s T2 ) = 1 ? 4 10 = 0.6 ( four insertions ) .
Note that good micro-adaptation candidates ( like s T2 in our example ) may receive a low score when training and test sentences have different sizes ( needing for multiple insertion / deletion operations ) .
Thus , we introduce a second similarity score that focus on the number of n-gram matchings ( N M ) between s t and s T .
We define N M as : N M ? , ? ( s t , s T ) = ? n=? Cn?st [ C n ? s T ] ( 2 ) where C n is an n-gram ( consecutive sequence of n words ) , ? and ? are the lower and higher n-gram lengths considered and [ P ] is the Iverson bracket that returns one if P is true and zero otherwise .
Following with the examples of Table 1 , n-gram matching scores are N M 2,4 ( s t , s T1 ) = 6 and N M 2,4 ( s t , s T2 ) = 12 .
In order to implement fast retrieval we use Suffix Arrays [ 11 ] .
Training sentences containing any n-gram ( n ? [ 2 , 4 ] ) present in the input sentence are initially retrieved .
Then , F M and N M scores are computed leading to collect the most similar train sentences of a given input sentence .
Sentence Distributed Representations
As mentioned in Section 2 , the encoder module of Transformer outputs a sequence of hidden representations h J 1 corresponding to words x J 1 of the input sentence .
Inspired from the work of [ 12 ] , we combine the recurrent h J 1 with mean pooling and max pooling to obtain a fixed - size vector representation h ( h mean or h max ) .
Thus , similarity between sentences s 1 and s 2 can be easily computed via cosine similarity of their distributed representations h 1 and h 2 : sim(s 1 , s 2 ) = h 1 ? h 2 ||h 1 || ? ||h 2 || ( 3 )
In order to implement fast similarity retrieval between test and training sentences we use faiss 1 toolkit [ 13 ] .
Training sentences identified as similar to each test input sentence are used for micro-adaptation .
Experimental Setup
Corpora
We perform English ?
French translation experiments .
Data used in our experiments is a combination of heterogeneous datasets publicly available 2 : documentation from the European Central Bank ( ECB ) ; documents from the European Medicines Agency ( EMEA ) ; news commentaries ( NEWS ) ; European Parliament proceedings ( EPPS ) ; crawled parallel data ( COMM ) .
Table 2 shows some statistics computed after a light tokenisation using the OpenNMT tokenizer 3 ( conservative mode ) which basically splits -off punctation .
We pay special attention to our in- domain datasets ( ECB and EMEA ) for which we measure the accuracy of different NMT engines over their respective validation and test sets .
We use three additional out -of- domain datasets ( NEWS , EPPS and COMM ) summing up to near ?7 M sentence pairs .
Validation and test sets for ECB and EMEA are randomly selected from the original corpus discarding sentences that also appear in the training set .
We expect our framework to perform differently for different overlapping levels between test and in-domain train sets .
Figure 2 summarises the number of n-gram overlaps between test sets and their corresponding train sets .
Very similar trends are found for both domains ( ECB and EMEA ) .
The fact that ?40 % of the test 10 - grams are present in their corresponding in-domain train sets indicates the important overlapping between both sets .
Similarly , Figure 3 illustrates the number of validation sentences ( left axis ) for which the similarity score of the closest example found in training belongs to the given similarity range .
For both domains , half of the validation sets are assigned a similarity score higher than 0.9 .
It is worth to note that low overlapping levels between test and train data is expected to result on poor adaptation performance .
Network Configuration
Our model follows the state- of- the- art Transformer architecture [ 5 ] implemented by the OpenNMT - tf 4 toolkit [ 14 ] learned with the hyper-parameters : size of word embedding : 512 ; size of hidden layers : 512 ; size of inner feed forward layer : 2,048 ; Multi-head : 8 ; number of layers : 6 ; batch size : 3,072 tokens .
We use the lazy Adam optimiser .
To vary the learning rate over training we use the original formula with parameters ?
1 = 0.9 , ? 2 = 0.98 and = 10 ?9 and 4 , 000 warmup steps .
Learning rate is updated every 8 iterations .
Fine-tuning is performed continuing Adam with the same learning rate decay schedule .
Micro-adaptation is performed with a fixed learning rate value .
We train a 32 K joint byte-pair encoding ( BPE ) to jointly preprocess the French and English data [ 15 ] .
We limit the sentence length to 80 based on BPE preprocessing in both source and target sides .
In inference we use a beam size of 5 .
Then , we remove the BPE joiners and evaluate translation hypotheses with multi- bleu.perl [ 16 ] .
Results
Table 3 summarises BLEU scores over in- domain validation and test sets for different network configurations 5 . Results are computed as average of three different optimisations ( micro-adaptations ) .
Our base network ( Mixed ) is trained with a combination of the training sets detailed in Table 2 , summing up to near ?7 M sentence pairs .
Training is performed over 300,000 iterations .
Following [ 7 ] , we fine- tune the Mixed network with each of the in-domain data sets , thus obtaining two networks specialised on each of the domains ( FT ) .
Further training the network is performed over 5 epochs .
This is , ? 8000 iterations for ECB and ?
26000 iterations for EMEA .
Finally , we perform micro-adaptation over our base Mixed network as described above : ?A hmean and ?A NM .
The two similarity measures described in section 3 are considered : in ?A NM , sentences are collected based on n-gram matchings ( Equation 2 ) , while ?A hmean retrieves sentences based on distributed representations ( Equation 3 ) .
We use optimal micro-adaptation parameters : N = 35 train sentences are collected for each input sentence .
Microadaptation is performed during 9 epochs with learning rate set to 0.0002 .
As we can see in table 3 , micro-adaptation results clearly outperform those obtained by fine-tuning ( FT ) for both test sets .
Very similar results are obtained when using lexicalised ( ? A NM ) and distributed representations ( ?
A hmean ) for similarity computation .
Figure 3 illustrates the number of validation sentences ( left axis ) for which the similarity score of the closest example found in training belongs to the given range .
In addition , it also shows the BLEU gain ( right axis ) of each range when comparing ?A hmean to Mixed .
6
As expected , highest improvements are measured for ranges with large similarity scores .
In the next section we use ?A hmean as default model and analyse the impact on accuracy of different micro-adaptation parameters .
Micro-adaptation Analysis
We first evaluate the performance of the methods to compute sentence similarity ( Similarity ) .
For distributed representations , mean-pooling outperforms max-pooling for both validation sets .
For lexicalised methods , n-gram matching ( N M ) obtains better results for the EMEA set while equivalent results for ECB .
Results are inline with findings of [ 19 ] where improvements are reported with local n-gram matchings rather than global sentence similarity .
Similar accuracies are obtained by h mean and NM .
We also assess the impact on accuracy of the number of similar in-domain training samples collected for each input sentence ( N- best ) .
Note that micro-adaptation is performed using the union of samples collected for all validation sentences .
In this case , best performance is found when micro-adaptation considers the 35 most similar training sentences ( according to h mean ) of each input sentence .
Thus , using 35 x 1,000 in- domain training samples .
Note that microadaptation requires additional workload in inference .
We provide efficiency results ( Time ) indicated by seconds spent per epoch and per input sentence according to the number of similar training sentences retrieved .
7 Results are inline with the efficiency results showed in [ 20 ] .
Overfitting and catastrophic forgetting are important issues when learning over a reduced set of samples .
Thus , we now finely tune the number of additional iterations and learning rate applied for micro-adaptation work .
Concerning the number of iterations ( # Epochs ) , best results are obtained when adaptation is run between 9 and 11 epochs .
Since ECB and EMEA contain ?35,000 training samples of different lengths , 9 epochs imply around 3,000 iterations ( depending on the number and length of sentences in a batch ) .
Larger values result in overfitting .
Thus , limiting the generalisation ability of the network .
Lower values results in under-fitting .
Pseudo In-domain Data
In previous experiments we showed that in- domain training sentences similar to a given test set are suitable for microadaptation .
We now show that our framework can also take advantage of similar sentences present in an out-of- domain training set .
Thus , we employ h mean to identify the M most similar out - of- domain training sentences to each input sentence .
Notice that micro-adaptation is now performed over the union of the M most similar out - of- domain train sentences and the 35 - best most similar in-domain train sentences .
Table 5 shows BLEU scores over development sets when varying the number M of sentences retrieved .
Scores for the best configuration over test sets are also given .
Results indicate a light improvement when adding 5 - best outof-domain sentences over the ECB validation set ( + 0.16 ) while no gain is observed for the EMEA validation set .
Results over test set confirm this improvement ( + 0.37 ) .
Table 5 : BLEU scores using similar sentences retrieved from both in - and out - of- domain train data .
and out-of- domain ( T OU T ) training sentences found using h mean similarity measure .
The first example shows that none of the most similar sentences identified carry the exact same meaning than the test one .
Both are fairly close and may be useful for adaptation broadening the diversity of training data .
In the second example both , in - domain ( T ECB ) and out-of- domain ( T OU T ) similar sentences are equally extremely close to the input sentence .
It is therefore difficult , to determine to what an extent the use of both sentences in micro-adaptation can contribute to the resulting model .
Hypotheses produced by both micro-adapted models are extremely similar ( outlined using bold letters ) .
A single word replaced by a synonym .
In both cases , the synonym used in Hyp 2 appears in the reference .
Related Work Domain adaptation has been deeply studied from a number of perspectives , ranging from theoretical analysis to more applied work , and for which many solutions have been proposed .
In the case of Machine Translation , literature of domain adaptation typically distinguishes data- based approaches from model - based approaches [ 21 , 22 ] .
One of the most common adaptation scenarios uses out -of- domain ( or heterogeneous ) data sources for training , while testing on in- domain texts .
In this setting , data- based approaches aim to bias the distribution of the training data towards matching that of the target domain , using data selection techniques [ 23 , 24 , 25 ] , or producing synthetic parallel data following the in-domain distribution [ 26 , 27 , 28 , 29 ] .
In contrast , model - based approaches build domain- adapted models by biasing the training objective towards the desired domain using in - domain data [ 7 , 30 , 31 ] , or building networks with domain-specialised layers [ 32 , 6 , 33 , 17 ] .
Thus , effectively enabling multi-domain networks , a practical scenario in the industry which allows to be both data efficient ( all data is used to train all domains ) and computationally efficient ( a single model is built for all domains ) .
Our work follows a framework where a unique generic system is built off-line , and adaptation is dynamically applied t ECB :
The SEPA project represents the next major step towards closer European integration .
Ref : Le projet SEPA constitue une nouvelle ? tape majeure vers une plus grande int?gration europ ?enne .
T ECB : The draft Constitution is an important step in preparing the Union for the future .
Hyp 1 : Le projet SEPA repr?sente la prochaine ? tape majeure vers une int?gration europ ?
enne plus ? troite .
T OU T : This was a significant new step along the road towards European integration .
Hyp 2 : Le projet SEPA constitue la prochaine ? tape majeure vers une int?gration europ ?
enne plus ? troite .
t ECB : The external environment remains favourable , providing support for euro area exports .
Ref : L ' environnement ext?rieur reste bien orient ?
et soutient les exportations de la zone euro .
T ECB : The external environment is favourable , providing support for euro area exports .
Hyp 1 : L'environnement ext?rieur demeure favorable , soutenant ainsi les exportations de la zone euro .
T OU T : External conditions thus continue to provide support for euro area exports .
Hyp 2 : L'environnement ext?rieur reste favorable , soutenant ainsi les exportations de la zone euro .
Table 6 : Input test sentences ( t ECB ) together with their most similar in- domain ( T ECB ) and out-of- domain ( T OU T ) training sentences as found by h mean similarity measure .
Reference ( Ref ) and translation hypotheses ( Hyp 1 and Hyp 2 ) are also shown , produced by ?A hmean micro-adapted to in- domain ( Hyp 1 , best system in Table 4 ( N- best ) ) or in- domain + out- of- domain ( Hyp 2 , best system in Table 5 ) train sentences .
using a small amount of training data considered similar to input sentences .
Closely related , in the context of Statistical MT , [ 34 ] proposes to dynamically extend the translation model with translation memory entries obtaining high levels ( above 70 % ) of fuzzy matches with input sentences .
These examples are encoded as additional translation rules in the translation model .
In [ 35 ] is presented a generic framework that employs an additional language model to guide the decoder .
The language model is built over lexical hypotheses produced by auxiliary translation engines .
Concerning Neural MT , in [ 36 ] is presented a dynamic data selection method to fine tune NMT models .
The authors show that increasingly reducing the amount of data used for fine tuning and regularly computing sentence similarities outperforms static data selection . [ 37 ] follows a data selection strategy to train only on semantically related sentences .
Thus , building from scratch a model adapted to a given test set .
Further adaptation is applied at inference time following the work in [ 7 ] .
Our work mainly differs from the previous as we apply adaptation at inference time .
In [ 18 ] and [ 20 ] a generic translation model is dynamically adapted to each input sentence making use of the most similar sentences found in training datasets .
To compute similarity , the first work uses a fully lexicalised n-gram matching score and adapts a model built from heterogeneous corpora , while the second includes dense vector representations and applies adaptation to a model built from parliamentary documents ( UN ) .
In our work we consider a different scenario where test sentences are semantically related while in previous works input sentences come from different domains with no order .
In addition , our NMT model follows the Transformer architecture of [ 5 ] while the architecture described in [ 4 ] is used in previous works .
A non-parametric adaptation solution is proposed by [ 19 ] .
Similar to our work the authors compute sentence similarities using dense vector representations .
However , they focus on retrieving n-grams rather than sentences , and propose a network that is able to incorporate relevant information present in such n-grams .
A similar solution is proposed by [ 38 ] .
But they modify a standard NMT model to apply a bonus to hypotheses that contain the collected ngram translations .
Similarly , [ 40 ] employs similar training translations but modifies the NMT network architecture to more accurately distinguish and take into account the useful information of the retrieved similar translations .
Finally , [ 39 ] uses also n-gram fuzzy matching to collect similar sentences collected on translation memories but makes use of an NMT engine trained to translate an input stream composed of the input sentence concatenated with related translations , requiring existence of translation memory before training .
Conclusions
In this paper we propose a generic framework to adapt a neural MT network built from heterogeneous corpora to a homogeneous test set , a very common industry scenario .
At inference-time , a reduced amount of training instances similar to input sentences are dynamically retrieved , and used to update the network parameters .
Experimental results demonstrate both efficiency and performance of the framework : it outperforms the state - of - the - art fine tuning , typically performed as a pre-training task incompatible to online adaptation to new data .
Both experienced methodologies for similarity computation ( ?
A hmean and ?A NM ) demonstrated similar performance .
We also showed that out - of- domain corpora may also be used for micro-adaptation to further boost accuracy .
In the future , we plan to study the similarity required to apply micro-adaptation as well as dynamically vary the size of the adaptation set .
Similar to [ 41 ] we also plan to apply micro-adaptation over semantically highly related sentences rather than entire test sets .
We also would like to improve similarity retrieval methods , which currently focus on high overlapping rates .
However , some sentences may also be useful for adaptation if they include relevant n-grams .
Figure 1 : 1 Figure 1 : Generic training ( left ) and micro-adaptation ( right ) workflows .
Training is performed using random samples of the entire data set .
Micro-adaptation updates the network using a reduced amount of samples found similar to a given test set .
s t : t The 2nd paragraph of the Article s T1 : The 3rd paragraph of the Article s T2 : The 2nd paragraph of the Article 1 is deleted .
Figure 2 : 2 Figure 2 : Lexical overlap ( percentage of n-grams ) between test sets and corresponding in-domain train sets .
Figure 3 : 3 Figure 3 : Number of validation sentences ( bars , left axis ) and BLEU gain ( lines , right axis ) by similarity range ( h mean ) .
Table 1 : 1 Test ( s t ) and train ( s T1 and s T2 ) sentences .
Table 2 : 2 ECB En Fr 193 K 6.1M 7.1M 37,149 51,211 -- EMEA En Fr 1,090K 15.2M 17.9M 49,903 60,246 -- NEWS En Fr 258 K 6.7M 8.4M 77,656 81,119 -- EPPS En Fr 2,007 K 56.4 M 65.3M 125,556 96,878 -- COMM En Fr 3,244 K 84.2M
754,861 96.2M 839,012 -- Validation ECB En Fr 1,000 31,917 36,953 4,483 5,252 127 137 EMEA En Fr 1,000 21,174 24,583 3,821 4,266 44 51 Test ECB En Fr 1,000 33,073 39,072 4,515 5,526 63 114 EMEA En Fr 1,000 20,187 23,839 4,277 4,747 77 103
Statistics for training , validation and test sets for the corpora used in this work .
K stands for thousands and M for millions .
Statistics were computed over the original data after splitting -off punctuation .
Table 3 : 3 BLEU scores of different network configurations .
Network Validation ECB EMEA ECB EMEA Test Mixed 42.95 48.82 43.65 42.04 FT 45.98 52.38 48.41 43.48 ?A NM 47.15 54.15 49.72 44.12 ?A hmean 47.19 54.02 49.37 43.98
Thus , limiting the ability to effectively learn from examples .
Regarding learning rate ( LRate ) , best results are ob-tained when set to 2 ? 10 ?4 .
Lower values produce very small updates of the model , leading to similar results than those obtained by the original Mixed network .
Higher values imply large updates , resulting in unstable training .
Similarity ECB EMEA . h max 46.50 53.65 h mean 47.19 54.02 N M 47.15 54.15 F M 47.16 53.69 N-best ECB EMEA Time 15 45.67 52.60 0.052 20 46.54 53.98 0.073 25 46.40 53.88 0.092 30 47.02 53.64 0.106 35 47.19 54.02 0.124 40 46.83 53.46 0.142 # Epochs ECB EMEA 5 46.13 53.06 6 46.47 52.78 7 46.86 53.15 8 46.81 53.53 9 47.19 54.02 10 47.30 53.81 11 47.37 53.55 12 47.28 53.72 LRate ECB EMEA 1 ? 10 ?3 45.83 53.34 2 ? 10 ?4 47.19 54.02 1 ? 10 ?4 46.43 53.05 1 ? 10 ?5 44.32 50.36 1 ? 10 ?6 43.30 49.27
Table 4 : 4 BLEU scores computed on validation sets and time overhead by sentence ( Time ) for different values of several micro-adaptation parameters .
Table 6 6 illustrates two examples of ECB test sentences ( t ECB ) together with the most similar in- domain ( T ECB ) M - best ECB EMEA Validation 0 47.19 54.02 1 47.33 53.61 3 47.10 53.44 5 47.35 54.00 7 46.86 53.35 9 46.88 53.37 Test 0 49.37 43.98 5 49.74 43.99
https://github.com/facebookresearch/faiss 2 http://opus.nlpl.eu 3 https://pypi.org/project/pyonmttok/ Corpus Lang .
Lines Words Vocab .
OOV Training
https://github.com/OpenNMT/OpenNMT-tf
Our BLEU scores are lower than those published on earlier studies [ 17 , 18 ] .
This is because EMEA and ECB datasets as distributed by OPUS contain many duplicates and we decided to make validation / test sets entirely disjoints from their respective train sets .
6
No results are given for lower similarity range sets as they contain very few sentences .
