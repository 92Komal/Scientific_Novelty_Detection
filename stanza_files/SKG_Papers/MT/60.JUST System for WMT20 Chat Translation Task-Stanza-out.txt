title
JUST System for WMT20 Chat Translation Task
abstract
Machine Translation ( MT ) is a sub-field of Artificial Intelligence and Natural Language Processing that investigates and studies the ways of automatically translating a text from one language to another .
In this paper , we present the details of our submission to the WMT20 Chat Translation Task , which consists of two language directions , English ?
German and German ?
English .
The major feature of our system is applying a pre-trained BERT embedding with a bidirectional recurrent neural network .
Our system ensembles three models , each with different hyperparameters .
Despite being trained on a very small corpus , our model produces surprisingly good results .
Introduction
The language of chat texts is considered a common language where people are rarely paying attention to correct spelling .
Therefore , using the traditional methods of Machine Translation ( MT ) , like dictionaries , is insufficient ( Hern?ndez , 2009 ) .
As deep learning ( DL ) models are becoming more evolved and complex , this motivates the natural language processing ( NLP ) community researchers to employ them for challenging tasks such as MT of informal language , such as what is used in chat .
Techniques like contextual word embeddings and pre-trained DL models are becoming very common in natural language generation ( NLG ) tasks such as MT ( Kusner et al. , 2015 ; Zou et al. , 2013 ; Abdullah and Shaikh , 2018 ; Al - Bdour et al. , 2019 ) .
The Chat Translation
Task is a new task in the Fifth Conference on Machine Translation ( WMT20 ) .
1 Translating chat text , specifically the chats of customer support , is a main and exciting task in the field of MT .
This kind of tasks has not been widely considered in previous MT studies , 1 http://www.statmt.org/wmt20/chat-task.html mostly because of the absence of openly existing datasets .
The target of this new Chat Translation Task is to translate the customer support chat text from English to German and vice versa .
The essential goal of this task is to develop models that can translate conversational text and study the use of multilingual models .
We take part in the WMT20 shared chat translation task in two language directions : English ?
German and German ?
English .
In this paper , we discuss our submission for this task , which is based on the bidirectional recurrent neural networks ( bi - RNN ) ( Schuster and Paliwal , 1997 ) and using the pre-trained BERT embedding , known as bert- base- multilingual - cased ( Devlin et al. , 2018 ) .
This paper is constructed as follows .
In Section 2 , the task and data descriptions are provided .
Section 3 discusses our proposed model .
Section 4 shows the experiments we conduct and their results .
Finally , the Conclusion is in Section 5 .
Task and Data Description
The Chat Translation shared task of WMT20 offers participants the opportunity to address a challenging problem faced by many companies today as they expand their customer support units to multiple different languages .
The shared task provides a dataset consisting of a set of conversations between agents and customers .
The organizers supplied a corpus for the English - German language pair .
Specifically , the task involves translating the chat text of an agent speaking English and a customer speaking German .
We are asked to translate the agent 's chat text from English to German , and the customer 's from German to English .
The dataset used for this shared task depends on the corpus of Taskmaster - 1 ( Byrne et al. , 2019 ) , which has the English language , and it consists of dialogues in six fields .
A small part of this dataset was chosen and translated to German .
The shared task has been provided with train , development , and test sets in JSON format .
Each chat in the data file has a specific structure .
Table 1 Each conversation contains a speaker ( who is either an agent or a customer ) , a source chat text , and a target chat text .
For the test set file , we are asked to translate the source chat text to target depending on the speaker .
If it is an agent , the translation is from English to German .
Otherwise , the translation is from German to English .
For evaluating the participating models , the task organizers employ both automatic metrics ( BLEU ( Papineni et al. , 2002 ) and TER ( Snover et al. , 2006 ) ) as well as human evaluation .
JUST System Our System follows the sequence of steps shown in Figure 1 .
In the following subsections , we discuss each step in details .
Preprocessing Data
For the dataset preprocessing , we first converted the files from JSON file , as given in the shared task , to text files , so we can work with them easily .
The training , dev , and test sets are divided into two groups : one that contains the agent as the speaker ( English ?
German ) and one that contains the customer as the speaker ( German ?
English ) .
Extracting Features
After preparing the dataset and preprocessing it , we use the pre-trained BERT model to get the word em-beddings of the dataset .
Specifically , we use Bertbase- multilingual - cased 2 to extract feature vectors of the dataset to be used in the training of our models .
For each word in the sentence of the encoder side , we get a file containing the word 's embedding .
The same is done for the decoder side .
The System Architecture
Our system is an adaptation of OpenNMT 3 , an open-source toolkit for neural machine translation ( NMT ) ( Klein et al. , 2017 ) .
It is created on the PyTorch framework ( Paszke et al. , 2017 ) .
After ensuring that the dataset is ready to be trained in our system , we feed our dataset to the bi-RNN with long short - term memory ( LSTM ) cells ( Hochreiter and Schmidhuber , 1997 ) and an attention mechanism ( Luong et al. , 2015 ) along with the word embeddings we extract from the dataset and trained everything jointly .
For each different set of hyperparameters , we train the model separately .
We save the best three models .
Table 3 shows the different hyperparameters used for the three models as well some of the experiments that have been done using GloVe embedding ( Pennington et al. , 2014 ) + byte pair encoding ( BPE ) ( Sennrich et al. , 2015 ) with a vocabulary of 10 K sub-word units ( Experiment - 1 ) , GloVe + without BPE ( Experiment - 2 ) , and the default model .
The rest of the hyperparameters are left at their default value .
We also experiment with the celebrated Transformer mode ( Vaswani et al. , 2017 ) .
However , this model results in very low BLEU scores when evaluated on the dev set .
Moreover , it takes about four days to finish training in one experiment .
So , we decide to exclude it from further consideration .
Model Ensembling
Before the test set is released , we train different models using the training set and evaluate them using the dev set .
After training our system , we choose the best three models and ensemble them to get the final output .
Results
The results based on the dev set are show in For evaluation on the test set , we combine the train and dev dataset of each group into one file .
We train each group separately and then we ensemble the three models into one .
This model is used to get the target of each sentence in the test set of each group .
It is worth mentioning that we only use the small dataset provided with the shared task .
Table 6 shows the results for the human evaluation between the human , best score and our model for the English ?
German scores .
Table 7 shows the results we get in the shared task compared to the baseline and the best results .
We can see that the agent BLEU score of our model is higher than the baseline , which is translating from English to German .
On the other hand , the customer BLEU score for the baseline beat our model , which is translating from German to English .
Conclusion
This work describes JUST 's submission to the WMT20 chat translation task .
For all two translation directions , English ?
German and German ?
English , we used the pre-trained BERT embedding with the bi-RNN .
We trained one model with different hyperparameters and then ensembled to one final system to translate the test set provided by the shared task .
At the end of this work , we find out that a simple NMT model with BERT embedding can achieve surprisingly good results even if it is trained on a very small corpus .
Figure 1 : 1 Figure 1 : Flowchart of our system .
