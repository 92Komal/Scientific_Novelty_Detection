title
Bilingual Correspondence Recursive Autoencoders for Statistical Machine Translation
abstract
Learning semantic representations and tree structures of bilingual phrases is beneficial for statistical machine translation .
In this paper , we propose a new neural network model called Bilingual Correspondence Recursive Autoencoder ( BCor- rRAE ) to model bilingual phrases in translation .
We incorporate word alignments into BCorrRAE to allow it freely access bilingual constraints at different levels .
BCorrRAE minimizes a joint objective on the combination of a recursive autoencoder reconstruction error , a structural alignment consistency error and a crosslingual reconstruction error so as to not only generate alignment - consistent phrase structures , but also capture different levels of semantic relations within bilingual phrases .
In order to examine the effectiveness of BCorrRAE , we incorporate both semantic and structural similarity features built on bilingual phrase representations and tree structures learned by BCorrRAE into a state - of - the - art SMT system .
Experiments on NIST Chinese - English test sets show that our model achieves a substantial improvement of up to 1.55 BLEU points over the baseline .
Introduction
Recently a variety of " deep architecture " approaches , including autoencoders , have been successfully used in statistical machine translation ( SMT ) Zou et al. , 2013 ; Devlin et al. , 2014 ; Tamura et al. , 2014 ; Sundermeyer et al. , 2014 ; Ko?isk ?
et al. , 2014 ) .
Typically , these approaches represent words as dense , low-dimensional and * Corresponding author .
real-valued vectors , i.e. , word embeddings .
However , translation units in machine translation have long since shifted from words to phrases ( sequence of words ) , of which syntactic and semantic information cannot be adequately captured and represented by word embeddings .
Therefore , learning compact vector representations for phrases or even longer expressions is more crucial for successful " deep " SMT .
To address this issue , many efforts have been initiated on learning representations for bilingual phrases in the context of SMT , inspired by the success of work on monolingual phrase embeddings ( Socher et al. , 2010 ; Socher et al. , 2011a ; Socher et al. , 2013 b ; Chen and Manning , 2014 ; Kalchbrenner et al. , 2014 ; Kim , 2014 ) .
The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT , e.g. , with reordering models , translation models ( Cui et al. , 2014 ; Gao et al. , 2014 ) , or both language and translation models .
In spite of their success , these approaches center around capturing relations between entire source and target phrases .
They do not take into account internal phrase structures and bilingual correspondences of sub-phrases within source and target phrases .
The neglect of these important clues may be due to the big challenge imposed by the integration of them into the learning process of bilingual phrase representations .
However , we believe such internal structures and correspondences can help us learn better phrase representations since they provide multi-level syntactic and semantic constraints .
In this paper , we propose a Bilingual Correspondence Recursive Autoencoder ( BCorrRAE ) to learn bilingual phrase embeddings .
BCorrRAE substantially extends the Bilingually - constrained Recursive Auto-encoder ( BRAE ) to exploit both inner structures and corre - Figure 1 : BRAE vs BCorrRAE models for generating of a bilingual phrase ( " ? ? ? ? " , " resolution adopted today " ) with word alignments ( " 0 - 2 2 - 1 3 - 0 " ) .
The subscript number of each word indicates its position within phrase .
Solid lines depict the generation procedure of phrase structures , while dash lines illustrate the reconstruction procedure from one language to the other .
In this paper , the dimensionality of vector d in all figures is set to 3 for better illustration .
spondences within bilingual phrases .
The intuitions behind BCorrRAE are twofold : 1 ) bilingual phrase structure generation should satisfy word alignment constraints as much as possible ; and 2 ) corresponding sub-phrases on the source and target side of bilingual phrases should be able to reconstruct each other as they are semantic equivalents .
In order to model the first intuition , BCorrRAE punishes bilingual structures that violate word alignment constraints and rewards those in consistent with word alignments .
This enables BCorrRAE to produce desirable bilingual phrase structures from the perspective of word alignments .
With regard to the second intuition , BCorrRAE reconstructs structures of sub-phrases of one language according to aligned nodes in the other language and minimizes the gap between original and reconstructed structures .
In doing so , BCorrRAE is capable of capturing semantic relations at different levels .
To better illustrate our model , let us consider the example in Figure 1 . Similar to the conventional recursive antoencoder ( RAE ) , BRAE neglects bilingual correspondences of sub-phrases .
Thus , it may combine " adopted " and " today " together to generate an undesirable target tree structure which violates word alignments .
In contrast , BCorrRAE aligns source-side nodes ( e.g. ( " ? ? " , " ? ? " ) ) to their corresponding target - side nodes ( accordingly ( " resolution " , " adopted " ) ) according to word alignments .
Furthermore , in BCorrRAE , each subtree on the target side can be reconstructed from the corresponding source node that aligns to the target - side node dominating the subtree and vice versa .
These advantages allow us to obtain improved bilingual phrase embeddings with better inner correspondences of sub-phrases and word alignment consistency .
We conduct experiments with a state - of- the - art SMT system on large-scale data to evaluate the effectiveness of BCorrRAE model .
Results on the NIST 2006 and 2008 datasets show that our system achieves significant improvements over baseline methods .
The main contributions of our work lie in the following three aspects : ?
We learn both embeddings and tree structures for bilingual phrases using cross-lingual RAE reconstruction that minimizes semantic distances between original and reconstructed subtrees .
To the best of our knowledge , this has not been investigated before .
?
We incorporate word alignment information to guide phrase structure generation and establish internal semantic associations of subphrases within bilingual phrases .
RAE and BRAE
In this section , we briefly introduce the RAE and its bilingual variation BRAE .
This will provide background knowledge on our proposed BCor-rRAE .
RAE
The component in the dash box of Figure 2 illustrates an instance of an RAE applied to a threeword phrase .
The input to the RAE is x = ( x 1 , x 2 , x 3 ) , which are the d-dimensional vector representations of the ordered words in a phrase .
For two children c 1 = x 1 and c 2 = x 2 , the parent vector y 1 can be computed in the following way : p = f ( W ( 1 ) [ c 1 ; c 2 ] + b ( 1 ) ) ( 1 ) where [ c 1 ; c 2 ] ?
R 2d?1 is the concatenation of c 1 and c 2 , W ( 1 ) ?
R d?2d is a parameter matrix , b ( 1 ) ?
R d?1 is a bias term , and f is an element - y 2 x 1 x 2 x 3 y 1 W ( 1 ) W ( 1 ) W ( 2 ) W ( 2 ) W ( 3 ) Reconstruction Error
Reconstruction Error Max-Semantic-Margin Error wise activation function such as tanh ( ? ) , which is used for all activation functions in BRAE and our model .
The learned parent vector p is also a ddimensional vector .
In order to measure how well p represents its children , we reconstruct the original children nodes in a reconstruction layer : [ c 1 ; c 2 ] = f ( W ( 2 ) p + b ( 2 ) ) ( 2 ) where c 1 and c 2 are reconstructed children vectors , W ( 2 ) ? R 2d?d and b ( 2 ) ? R 2d?1 .
We can set y 1 = p and then further use Eq. ( 1 ) again to compute y 2 by setting [ c 1 ; c 2 ] = [ y 1 ; x 3 ] .
This combination and reconstruction process of auto-encoder repeats at each node until the vector of the entire phrase is generated .
To obtain the optimal binary tree and phrase representation for x , we employ a greedy algorithm ( Socher et al. , 2011 c ) to minimize the sum of reconstruction error at each node in the binary tree T ( x ) : E rec ( x ; ? ) = n?T ( x ) 1 2 [ c 1 ; c 2 ] n ? [ c 1 ; c 2 ] n 2 ( 3 ) where ? denotes model parameters and n represents a node in T ( x ) .
BRAE BRAE jointly learns two RAEs for source and target phrase embeddings as shown in Figure 1 ( a ) .
The core idea behind BRAE is that a source phrase and its target correct translation should share the same semantic representations , while non-equivalent pairs should have different semantic representations .
use this intuition to constrain semantic pharse embedding learning .
As shown in Figure 2 , in addition to the abovementioned reconstruction error , BRAE introduces a max-semantic -margin error to minimize the semantic distance between translation equivalents and maximize the semantic distance between non-equivalent pairs simultaneously .
Formally , the max-semantic -margin error of a bilingual phrase ( f , e ) is defined as E sem ( f , e ; ? ) = E * sem ( f |e , ? ) +E * sem ( e|f , ? ) ( 4 ) where E * sem ( f |e , ? ) is used to ensure that the semantic error for an equivalent pair is much smaller than that for a non-equivalent pair ( the source phrase f and a bad translation e ) : E * sem ( f |e , ? ) = max{0 , E sem ( f |e , ? ) ?
E sem ( f |e , ? ) + 1 } ( 5 ) where E sem ( f |e , ? ) is defined as the semantic distance between the learned vector representations of f and e , denoted by p f and p e , respectively .
Since phrase embeddings for the source and target language are learned separately in different vector spaces , a transformation matrix W ( 3 ) f ?
R d?d is introduced to capture this semantic transformation in the source - to- target direction .
Thus , E sem ( f |e , ? ) is calculated as E sem ( f |e , ? ) = 1 2 p e ? f ( W ( 3 ) f p f + b ( 3 ) f ) 2 ( 6 ) where b ( 3 ) f ?
R d?1 is a bias term .
E * sem ( e|f , ? ) and E sem ( e|f , ? ) can be computed in a similar way .
The joint error of ( f , e ) is therefore defined as follows : E(f , e ; ? ) = ?( E rec ( f , ? ) + E rec ( e , ? ) ) +( 1 ? ? ) ( E * sem ( f |e , ? ) + E * sem ( e|f , ? ) ) ( 7 )
The final BRAE objective function over the training instance set D becomes : J BRAE = ( f,e ) ?
D E(f , e ; ? ) + ? 2 ? 2 ( 8 ) Model parameters can be optimized over the total errors on training bilingual phrases in a co-training style algorithm .
The BCorrRAE Model
As depicted above , the learned embeddings using BRAE may be unreasonable due to the neglect of bilingual constraints at different levels .
To address this drawback , we propose the BCor-rRAE for bilingual phrase embeddings , which incorporates bilingual correspondence information into the learning process of structures and embeddings via word alignments .
In our model , we explore word alignments in two ways : ( 1 ) ensuring that a learned bilingual phrase structure is consistent with word alignments as much as possi-ble ; ( 2 ) identifying corresponding sub-phrases in the source language for reconstructing sub-phrases in the target language , and vice versa .
More specifically , the former is to encourage alignmentconsistent generation of sub-structures , while the latter is to minimize semantic distances between bilingual sub-phrases .
In this section , we first formally introduce a concept of structural alignment consistency encoded in bilingual phrase structure learning , which is the basis of our model .
Then , we describe the objective function which is composed of three types of errors .
Finally , we provide details on the training of our model .
Structural Alignment Consistency
We adapt word alignment to structural alignment and introduce some related concepts .
Given a bilingual phrase ( f , e ) with its binary tree structures ( T f , T e ) , if the source node n f ?
T f covers a source- side sub-phrase f , and there exists a target - side sub-phrase ? such that ( f , ? ) are consistent with word alignments ( Och and Ney , 2003 ) , we say n f satisfies the structural alignment consistency , and it is referred to as a structuralalignment -consistent ( SAC ) node .
Further , if ? is covered by a target node n ? ?
T e , we say n ? is the aligned node of n f .
In this way , several different target nodes may be all aligned to the same source node because of null alignments .
For this , we choose the target node with the smallest span as the aligned one for the considered source node .
This is because a smaller span reflects a stronger semantic relevance in most situations .
Likewise , we have similar definitions for target nodes .
Note that alignment relations between source - and target - side nodes may not be symmetric .
For example , in Figure 1 ( b ) , node n ? is the aligned node of node n f1 , while node n f2 rather than n f1 is the aligned node of n ?.
The Objective Function
We elaborate the three types of errors defined for a bilingual phrase ( f , e ) with its binary tree structures ( T f , T e ) on both sides below .
Reconstruction Error Similar to RAE , the first error function is used to estimate how well learned phrase embeddings represent corresponding phrases .
The reconstruction error E rec ( f , e ; ? ) of ( f , e ) is defined as follows : E rec ( f , e ; ? ) = E rec ( f ; ? ) + E rec ( e ; ? ) ( 9 ) where both E rec ( f ; ? ) and E rec ( e ; ? ) can be calculated according to Eq. ( 3 ) .
Consistency Error
This metric corresponds to the first way in which we exploit word alignments mentioned before , which enables our model to generate as many SAC nodes as possible to respect word alignments .
Formally , the consistency error E con ( f , e ; ? ) of ( f , e ) is defined in the following way : E con ( f , e ; ? ) = E con ( T f ; ? ) + E con ( T e ; ? ) ( 10 ) where E con ( T f ; ? ) and E con ( T e ; ? ) denote the consistency error score for T f and T e , given word alignments .
Here we only describe the calculation of the former while the latter can be calculated in exactly the same way .
To calculate E con ( T f ; ? ) , we first judge whether a source node n f is an SAC node according to word alignments .
Let p n f be the vector representation of n f . Following Socher et al. ( 2010 ) , who use a simple inner product to measure how well the two words are combined into a phrase , we use inner product to calculate the consistency / inconsistency score for n f : s( n f ) = W score p n f ( 11 ) where W score ?
R 1?d is the score parameter .
We calculate W score by distinguishing SAC from non -SAC nodes defined as follows : W score = W score cns if n f is an SAC node W score inc otherwise where the subscript cns and inc represent consistency and inconsistency , respectively .
For example , in Figure 3 , as n f3 is a non -SAC node , we calculate the inconsistency score using W score inc for it .
We expect T f to satisfy structural alignment consistency as much as possible .
Therefore we encourage the consistency score for T f to be larger than its inconsistency score using a max-margin consistency error function : E con ( T f ; ? ) =max{0 , 1 ? s( T f ) cns + s( T f ) ins } ( 12 ) where s( T f ) cns denotes the sum of consistency scores over all SAC nodes and s( T f ) ins the sum of inconsistency scores over all non -SAC nodes in T f .
Minimizing this error function will maximize the sum of consistency scores of SAC nodes and minimize ( up to a margin ) the sum of inconsis- resolution 0 adopted 1 ? 1 ? 2 ? 3 n ? W u e W ( 3 ) f n f1 n f2 n f3 Figure 3 : The structure generation procedure of the source sub-phrase " ? ? ? " and the structure reconstruction procedure of the target sub-phrase " resolution adopted " .
According to word alignments ( " 2 - 1 3 - 0 " ) , the node n f1 and n f2 are SAC ones while the node n f3 is a non-SAC node .
tency scores of non-SAC nodes .
Cross-Lingual Reconstruction Error
This metric corresponds to the second way in which we exploit word alignments .
The assumption behind this is that a source / target node should be able to reconstruct the entire subtree rooted at its target / source aligned node as they are semantically equivalent .
Based on this , for the considered node , we calculate the cross-lingual reconstruction error along the entire subtree rooted at its aligned node in the other language and use the error to measure how well the learned vector represents this node .
Similarly , the cross-lingual reconstruction error E clrec ( f , e ; ? ) of ( f , e ) can be decomposed into two parts as follows : E clrec ( f , e ; ? ) = E f 2e?rec ( T f , T e ; ? ) + E e2 f ?rec ( T f , T e ; ? ) ( 13 ) where E f 2e?rec ( T f , T e ; ? ) denotes the error score using T f to reconstruct T e .
Note that in this process , the structure and the original node vector representations of T e have been already generated .
E e2 f ?rec ( T f , T e ; ? ) denotes the reconstruction error score using T e to reconstruct T f .
Here we still only describe the method of computing the former , which also applies to the latter .
To calculate E f 2e?rec ( T f , T e ; ? ) , we first collect all source nodes ( n f ) in T f and their aligned nodes ( n ? ) in T e to form a set of aligned node pairs S = { n f , n ?
} according to word alignments .
We then calculate E f 2e?rec ( T f , T e ; ? ) as the sum of error scores over all node pairs in S. Given a source node n f with its aligned node n ? on the target side , we use n f to reconstruct the sub-tree structure T ? rooted at n ? and compute the error score based on the semantic distance between the original and reconstructed vector representations of nodes in T ?.
As source and target phrase em-beddings are separately learned , we first introduce a transformation matrix W p n? = f ( W ( 3 ) f p n f + b ( 3 ) f ) ( 14 ) here p n? denotes the reconstructed vector representation of n ? , which is transformed from the vector representation p n f of n f .
Then , we repeat the reconstruction procedure in a top-down manner along the corresponding target tree structure until leaf nodes are reached , following Socher et al . ( 2011a ) .
Specifically , given the vector representation p n? , we reconstruct vector representations of its two children nodes : [ c u e1 ; c u e2 ] = f ( W u e p n? + b u e ) ( 15 ) where c u e1 and c u e2 are the reconstructed vector representations of the children nodes , W u e ? R 2d?d , and b u e ?R 2d?1 .
Eventually , given the original and reconstructed target phrase representations , we calculate E f 2e?rec ( T f , T e ; ? ) as follows : E f 2e?rec ( T f , T e ; ? ) = 1 2 n f ,n ?
?S n?T? p n ?p n 2 ( 16 ) where p n and p n are the original and reconstructed vector representations of node n in the sub-tree structure T ? rooted at n ?.
This error function will be minimized so that semantic differences between original and reconstructed structures are minimal .
Figure 3 demonstrates the structure reconstruction from a generated source sub-tree to its target counterpart .
In this way , BCorrRAE propagates semantic information along dash lines sequentially until leaf nodes in the generated structure of the target phrase .
The Final Objective Similar to Eq. ( 8 ) , we define the final objective function of our model based on the three types of errors described above J BCorrRAE = ( f,e ) ?
D {? ( E rec ( f ; ? ) + E rec ( e ; ? ) ) + ?
( E con ( T f ; ? ) + E con ( T e ; ? ) ) + ? ( E f 2e?rec ( T f , T e ; ? ) + E e2 f ?rec ( T f , T e ; ? ) ) } + R ( ? ) ( 17 ) where weights ? , ? , ? ( s.t. ?+? +? = 1 ) are used to balance the preference among the three errors , and R ( ? ) is the regularization term .
Parameters ? are divided into four sets 1 : 1 . ? L : the word embedding matrix ; 2 . ? rec : the RAE parameter matrices W ( 1 ) , W ( 2 ) and bias terms b ( 1 ) , b ( 2 ) ( Section 2.1 ) ; 3 . ? con : the consistency / inconsistency score parameter matrices W score cns , W score inc ( Section 3.2.2 ) ; 4 . ? clrec : the cross-lingual RAE semantic transformation parameter matrices W ( 3 ) , W u and bias terms b ( 3 ) , b u ( Section 3.2.3 ) .
For regularization , we assign each parameter set a unique weight : R ( ? ) = ? L 2 ? L 2 + ? rec 2 ? rec 2 + ? con 2 ? con 2 + ? lcrec 2 ? lcrec 2 ( 18 ) Additionally , in order to prevent the hidden layer from being very small , we normalize all output vectors of the hidden layer to have length 1 , p = p p , following Socher et al . ( 2011 c ) .
Model Training Similar to , we adopt a cotraining style algorithm to train model parameters in the following two steps :
First , we use a normal distribution ( ? = 0 , ? = 0.01 ) to randomly initialize all model parameters , and adopt the standard RAE to pre-train sourceand target - side phrase embeddings and tree structures ( Section 2.1 ) .
Second , for each bilingual phrase , we update its source-side parameters to obtain the fine-tuned vector representation and binary tree of the source phrase , given the target - side phrase structure and node representations , and vice versa .
In this process , we apply L-BFGS to tune parameters based on gradients over the joint error , as implemented in ( Socher et al. , 2011 c ) .
We repeat the procedure of the second step until either the joint error ( shown in Eq. ( 17 ) ) reaches a local minima or the number of iterations is larger than a pre-defined number ( 25 is used in experiments ) .
Decoding with BCorrRAE
Once the model training is completed , we incorporate two different phrasal similarity features built on the trained BCorrRAE into the standard log-linear framework of SMT .
Given a bilingual phrase ( f , e ) , we first obtain their semantic phrase representations ( p f , p e ) .
Then we transform p f into p e in the target semantic space and p e into p f in the source semantic space via transformation matrixes .
Finally , we reconstruct sub-trees of p f along the source structure T f learned by BCor- rRAE , sub-trees of p e along the target structure T e .
We exploit two kinds of phrasal similarity features based on the learned phrase representations and their tree structures as follows : ? Semantic Similarity measures the similarity between original and transformed phrase representations of ( f , e ) : Sim SM ( p f , p f ) = 1 2 p f ? p f 2 Sim SM ( p e , p e ) = 1 2 p e ? p e 2 ( 19 ) ? Structural Similarity calculates the similarity between original and reconstructed tree structures learned by BCorrRAE for ( f , e ) : Sim ST ( p f , p f ) = 1 2 C f n?T f p n ? p n 2 Sim ST ( p e , p e ) = 1 2C e n?Te p n ? p n 2 ( 20 ) where p n and p n represent vector representations of original and reconstructed node n , and C f and C e count the number of nodes in the source and target tree structure respectively .
Note that if we only compute the similarity for root nodes in the bilingual tree of ( f , e ) , the structural similarity equals to the semantic similarity in Eq. ( 19 ) .
Experiments
We conducted experiments on NIST Chinese-English translation task to validate the effectiveness of BCorrRAE .
System Overview
Our baseline decoder is a state - of - the - art phrasebased translation system equipped with a maximum entropy based reordering model ( MEBTG ) .
It adopts three bracketing transduction grammar rules ( Wu , 1997 ; Xiong et al. , 2006 ) :
The whole translation model is organized in a log-linear framework ( Och and Ney , 2002 ) .
The adopted sub-models mainly include : ( 1 ) rule translation probabilities in two directions , ( 2 ) lexical weights in two directions , ( 3 ) targets - side word number , ( 4 ) phrase number , ( 5 ) language model score , and ( 6 ) the score of maximal entropy based reordering model .
We perform minimum error rate training ( Och , 2003 ) to tune various feature weights .
During decoding , we set ttable - limit=20 for translation candidates kept for each source phrase , stack - size = 100 for hypotheses in each span , and swap-span =15 for the length of the maximal reordering span .
merging rules A ? [ A 1 , A 2 ] | A 1 ,
Setup
Our bilingual data is the combination of the FBIS corpus and Hansards part of LDC2004T07 corpus , which contains 1.0M parallel sentences ( 25.2 M Chinese words and 29M English words ) .
Following , we collected 1.44 M bilingual phrases using forced decoding ( Wuebker et al. , 2010 ) to train BCorrRAE from the training data .
We used a 5 - gram language model trained on the Xinhua portion of Gigaword corpus using SRILM Toolkits 4 . Translation quality is evaluated by case-insensitive BLEU - 4 metric ( Papineni et al. , 2002 ) .
We performed paired bootstrap sampling ( Koehn , 2004 ) to test the significance in BLEU score differences .
In our experiments , we used NIST MT05 and MT06 / MT08 data set as the 2 A block is a bilingual phrase without maximum length limitation .
3 http://homepages.inf.
development and test set , respectively .
In addition to the baseline described below , we also compare our method against the BRAE model , which focuses on modeling relations of source and target phrases as a whole unit .
Word embeddings in BRAE are pre-trained with toolkit Word2 Vec 5 ( Mikolov et al. , 2013 ) on large-scale monolingual data that contains 0.83B words for Chinese and 0.11B words for English .
Hyper-parameters in all neural models are optimized by random search ( Bergstra and Bengio , 2012 ) based on related joint errors .
We randomly extracted 250 , 000 bilingual phrases from the above-mentioned training data as training set , 5 , 000 as development set and another 5 , 000 as test set .
We drew ? , ? , ? uniformly from 0.10 to 0.50 , and ?
L , ? rec , ? con and ?
lcrec exponentially from 10 ?8 to 10 ?2 . Final parameters are shown in Table 1 for both BRAE and BCorrRAE .
Dimensionality of Embeddings
To investigate the impact of embedding dimensionality on our BCorrRAE , we tried four different dimensions from 25 to 100 with an increment of 25 each time .
The results are displayed in Table 2 .
We can observe that the performance of our model is not consistently improved with the increment of dimensionality .
This may be because a larger dimension brings in much more parameters , and therefore makes parameter tuning more difficult .
In practice , setting the dimension d to 50 , we can get satisfactory results without much computation effort , which has also been found by .
Structural Similarity vs. Semantic Similarity
Table 2 also shows that the performance of BCorrRAE ST , the system with the structural similarity feature in Eq. ( 20 ) , is always superior to that of BCorrRAE SM with the semantic similarity feature in Eq. ( 19 ) .
BCorrRAE ST is better than BCorrRAE SM by 0.483 BLEU points on average .
In most cases , differences between BCorrRAE ST and BCorrRAE SM with the same dimensionality are statistically significant .
This suggests that digging into structures of bilingual phrases ( BCorrRAE ST ) can obtain further improvements over only modeling bilingual phrases as whole units ( BCorrRAE SM ) .
Overall Performance
Analysis
We compute a ratio of aligned nodes ( Section 3.1 ) over all nodes to estimate how well tree structures of bilingual phrases generated by BRAE and BCorrRAE are consistent with word alignments .
We consider two factors when computing the ratio : the length of the source side of a bilingual phrase l s and the length of a span covered by an aligned node l a .
The result is illustrated in Table 4 . 6 We find that BCorrRAE significantly outper - forms BRAE model by 7.22 % on average in terms of the aligned node ratio .
This strongly demonstrates that the proposed BCorrRAE is able to generate tree structures that are more consistent with word alignments than those generated by BRAE .
We further show example source phrases in Table 5 with their most semantically similar translations learned by BRAE and BCorrRAE in the training corpus .
Both models can select correct translations for content words .
However , they are different in dealing with function words .
Compared to our model , the BRAE model prefers longer target phrases surrounded with function words .
Take the source phrase " ? ? " as an example , the BRAE model learns both " a serious challenge to " and " a serious challenge from " as its semantically similar target phrases .
Although the content words " ? " and " ? " are translated correctly into " serious " and " challenge " , the function words " to " and " from " express exactly the opposite meanings .
In contrast , our model , especially the BCorrRAE ST model , tends to choose shorter translations that are consistent with word alignments .
Related Work
A variety of efforts have been devoted to learning vector representations for words / phrases with deep neural networks .
According to the difference of learning contexts , previous work mainly include the following two strands .
( 1 ) Monolingual Word / Phrase Embeddings .
The straightforward approach to represent word / phrases is to learn their hidden representations with traditional feature vectors , which requires manual and task - dependent feature engineering ( Cui et al. , 2014 ; Chen and Manning , 2014 ) .
To avoid exploiting manually input features , Bengio et al . ( 2003 ) convert words to dense , real-valued vectors by learning probability distributions of n-grams .
Mikolov et al. ( 2013 ) generate word vectors by predicting their limited context words .
Instead of exploiting outside context information , recursive auto-encoder is usually adopted to learn the composition of internal words ( Socher et al. , 2010 ; Socher et al. , 2011 b ; Socher et al. , 2013 b ; Socher et al. , 2013a ) .
Recently , convolution architecture has drawn more and more attention due to its ability to explicitly capture short and long-range relations ( Collobert et al. , 2011 ; Kalchbrenner and Blunsom , 2013 ; Kalchbrenner et al. , 2014 ; Kim , 2014 ) . ( 2 ) Bilingual Word / Phrase Embeddings .
In the field of machine translation and cross-lingual information processing , bilingual embedding learning has become an increasingly important study .
The bilingual embedding research origins in the word embedding learning , upon which Zou et al . ( 2013 ) utilize word alignments to constrain translational equivalence .
Ko?isk ?
et al. ( 2014 ) propose a probability model to capture more semantic information by marginalizing over word alignments .
More specifically to SMT , its main components have been exploited to learn better bilingual phrase embeddings in different aspects : language models Garmash and Monz , 2014 ) , reordering models and translation models ( Tran et al. , 2014 ; . Instead of exploiting a single model , combine the recursive and recurrent neural network to incorporate the language and translation model .
Different from the methods mentioned above , our model considers both the cross-language consistency of phrase structures and internal correspondence relations inside bilingual phrases .
The most related works include and Socher et al . ( 2011a ) .
Compared with these works , our model exploits different levels of correspondence relations inside bilingual phrases instead of only the top level of entire phrases , and reconstructs tree structures of sub-phrases in one language according to aligned nodes in the other language , which , to the best of our knowledge , has never been investigated before .
Conclusions and Future Work
In this paper , we have presented the BCorrRAE to learn phrase embeddings and tree structures of bilingual phrases for SMT .
Punishing structuralalignment -inconsistent sub-structures and minimizing the gap between original and reconstructed structures , our approach is able to not only generate alignment - consistent phrase structures , but also capture different levels of semantic relations within bilingual phrases .
Experiment results demonstrate the effectiveness of our model .
In the future , we would like to derive more features from BCorrRAE , e.g. , consistency / inconsistency scores of bilingual phrases , to further enhance SMT .
Additionally , we also want to apply our model to other bilingual tasks , e.g. , learning bilingual terminology or paraphrases .
Figure 2 : 2 Figure 2 : An illustration of the BRAE architecture .
phrase embeddings into the target-side semantic space , following and Hermann and Blunsom ( 2014 ) :
resolution 0 adopted 1 today 2 ? 0 ? 1 ? 2 ? 3 ( a) BRAE n f1 n f2 n ? ? 0 ? 1 ? 2 ? 3 resolution 0 adopted 1 today 2 ( b )
BCorrRAE reconstructing target sub-trees according to corresponding source nodes reconstructing source sub-trees according to corresponding target nodes n f1 n ?
n f2
A 2 which are used to merge two neighboring blocks 2 A 1 and A 2 in a straight | inverted order , and lexical rule A ? f /e used to translate a source phrase f into a target phrase e.The MEBTG system features a maximal entropy classifier based reordering model that predicts orientations of neighboring blocks .
During training , we extract bilingual phrases containing up to 7 words on the source side from the training corpus .
With the collected reordering examples , we adopt the maximal entropy toolkit 3 developed by Zhang to train the reordering model with the following parameters : iteration number iter=200 and gaussian prior g=1.0 .
Following Xiong et al. ( 2006 ) , we use only boundary words of blocks to trigger the reordering model .
Table 1 : 1 Hyper-parameters for BCorrRAE and BRAE model .
Parameter BRAE BCorrRAE ? 0.119 0.121 ? - 0.6331 ? - 0.2459 ?L 4.95 ?10 ?5 3.13 ?10 ?5 ?rec 2.64 ?10 ?7 2.05 ?10 ?5 ?con - 7.32 ?10 ?6 ? lcrec 9.31 ?10 ?5 5.25 ?10 ?6 Method d MT06 MT08 AVG 25 30.81 22.68 ? 26.75 BCorrRAESM 50 75 30.58 ? 30.50 22.72 ? 26.65 22.53 ? 26.52 100 30.34 ? 22.61 ? 26.48 25 30.56 23.28 26.92 BCorrRAEST 50 75 30.94 30.73 23.33 23.40 27.14 27.07 100 30.90 23.50 27.20 ed.ac.uk/lzhang10/maxent toolkit.html 4 http://www.speech.sri.com/projects/srilm/download.html
Table 2 : 2 Experiment results for different dimensions ( d ) .
BCorrRAESM and BCorrRAEST are our systems that are enhanced with the semantic and structural similarity features learned by BCorrRAE , respectively . ?/? : significantly worse than the BCorrRAEST with the same dimensionality ( p < 0.05 / p < 0.01 ) .
Table 3 3 summarizes the comparison results of dif- ferent models on the test sets .
The BCorrRAE SM outperforms the baseline and BRAE by 1.06 and 0.25 BLEU points on average respectively , while BCorrRAE ST gains 1.55 and 0.74 BLEU points on average over the baseline and BRAE .
The im - provements of BCorrRAE ST over the baseline , BRAE and BCorrRAE SM are statistically signif- icant at different levels .
This demonstrates the advantage of our BCorrRAE over BRAE in that BCorrRAE is able to explore sub-structures of bilingual phrases .
Table 3 : 3 Experiment results on the test sets .
AVG = average BLEU scores for test sets .
For both BRAE and BCorrRAE , we set d=50 . ?/? : significantly worse than the BCorrRAEST with d=50 ( p < 0.05 / p < 0.01 , respectively ) .
Method MT06 MT08 AVG Baseline 29.66 ? 21.52 ? 25.59 BRAE 30.27 ? 22.53 ? 26.40 BCorrRAESM 30.58 ? 22.72 ? 26.65 BCorrRAEST 30.94 23.33 27.14 [ ls , la ] [ 3,2 ] [ 4,2 ] [ 4,3 ] BRAE 52.70 % 39.88 % 46.58 % BCorrRAE 60.08 % 46.32 % 54.43 %
Table 4 : 4 Aligned node ratio for source phrases of different lengths .
Table 5 : 5 2014 ; Semantically similar target phrases in the training set for example source phrases .
Source Phrase BRAE BCorrRAESM BCorrRAEST to advocate the out to advocate encouraging ? ? ? ? ? in preaching the been encouraging claimed ( advocate ) the promotion of an advocate advocate as well as severe challenges of rigorous challenges rigorous challenge ? ? ? ? ? ? ? ? ? ? a serious challenge to as well as severe challenges enormous challenge ( serious challenge ) a serious challenge from of severe challenges severe challenge by the figures published by the to the estimates announced published data ? ? ? ? ? ? ? ? ? ? ? ? ? the statistics released by at the figures published released figures ( data released ) data published by the the statistics released by the estimates announced
Note that the source and target languages have different four sets of parameters .
https://code.google.com/p/word2vec/
We only give ratios for bilingual phrases with sourceside length from 3 to 4 words because 1 ) ratios of BRAE and BCorrRAE in the case of la < 3 are very close and 2 ) phrases with length > 4 are rarely used during decoding ( accounting for < 0.5 % ) .
