title
Improving Evaluation of Document-level Machine Translation Quality Estimation
abstract
Meaningful conclusions about the relative performance of NLP systems are only possible if the gold standard employed in a given evaluation is both valid and reliable .
In this paper , we explore the validity of human annotations currently employed in the evaluation of document - level quality estimation for machine translation ( MT ) .
We demonstrate the degree to which MT system rankings are dependent on weights employed in the construction of the gold standard , before proposing direct human assessment as a valid alternative .
Experiments show direct assessment ( DA ) scores for documents to be highly reliable , achieving a correlation of above 0.9 in a self-replication experiment , in addition to a substantial estimated cost reduction through quality controlled crowdsourcing .
The original gold standard based on post-edits incurs a 10 - 20 times greater cost than DA .
Introduction Evaluation of NLP systems commonly takes the form of comparison of system- generated outputs with a corresponding human-sourced gold standard .
The suitability of the employed gold standard representation greatly impacts the reliability and validity of conclusions drawn in any such evaluation .
With respect to reliability , measures such as inter-annotator agreement ( IAA ) enable the likelihood of replicability to be taken into account , were an evaluation to be repeated with a distinct set of human annotators .
One approach to achieving high IAA is through the development of a strict set of annotation guidelines , while for machine translation ( MT ) , human assessment is more subjective , making high IAA difficult to achieve .
For example , in past large-scale human evaluations of MT , low IAA levels have been highlighted as a cause of concern ( Callison - Burch et al. , 2007 ; Bojar et al. , 2016 ) .
Such problems cause challenges not only for evaluation of MT systems , but also for MT quality estimation ( QE ) , where the ideal gold standard comprises human assessment .
Although concern surrounding the reliability of human annotations is by far the most common complaint with respect to human evaluation of MT , the validity of the particular gold standard representation used in a given evaluation is also highly important .
When it comes to validity , conventionally speaking , the very fact that human annotators manually generate the gold standard provides reassurance of its validity , as results at least reflect the judgment of one or more members of the target audience , i.e. human users .
In the case of there being some " interpretation " of the human annotations , tuned to the particulars of a given task , validity becomes a concern .
In recent documentlevel QE shared tasks , for example , the gold standard is generated through a linear combination of two separate human evaluation components , with weights tuned to optimize mean absolute error ( MAE ) and variance with respect to gold label distributions .
In this paper , we explore the validity of the gold standard , and investigate to what degree tuning the gold standard impacts the validity of the resultant system performance estimates .
Our contribution shows the method used to generate the gold standard has a substantial impact on the resultant system ranking , and propose an alternate gold standard representation for document - level quality estimation that is both more reliable and more valid as a gold standard .
Background Document - level QE ( Soricut and Echihabi , 2010 ) is a relatively new area , with only two shared tasks taking place to date ( Bojar et al. , 2015 ; Bojar et al. , 2016 ) .
In WMT - 15 , gold standard labels took the form of automatic metric scores for documents ( specifically Meteor scores ( Denkowski and Lavie , 2011 ) ) , and system predictions were compared to gold labels via MAE .
A conclusion that emerged from the initial shared task was that automatic metric scores were not adequate , based on the following observation : if the average of the training set scores is used as a prediction value for all data points in the test set , this results in a system as good as the baseline system when evaluated with MAE .
The fact that average scores are good predictors is more likely a consequence of the applied evaluation measure , MAE , however , as outlined in Graham ( 2015 ) .
When evaluated with the Pearson correlation , such a set of predictions would not be a reasonable entry to the shared task since the prediction distribution would effectively be a constant and its correlation with anything is therefore undefined .
Regardless of the predictability of automatic metric scores when evaluated with MAE , they unfortunately do not provide a suitable gold standard , simply because they are known to provide an insufficient substitute for human assessment , often unfairly penalizing translations that happen to be superficially dissimilar to reference translations ( Callison - Burch et al. , 2006 ) .
Consequently , for WMT - 16 , the gold standard was modified to take the form of a linear combination of two human-targeted translation edit rate ( HTER ) ( Snover et al. , 2006 ) scores assigned to a given document .
Scores were produced via two human post-editing steps : firstly , sentences within a given MT - output document were post-edited independent of other sentences in that document , producing post-edition 1 ( P E 1 ) .
Secondly , P E 1 sentences were concatenated to form a documentlevel translation , and post-edited a second time by the same annotator , with the aim of isolating errors only identifiable when more context is available , to produce post-edition 2 ( P E 2 ) .
Next , two translation edit rate ( TER ) scores were computed by : ( 1 ) comparing the document- level MT output with P E 1 , TER ( P E 1 , M T ) ; and TER between P E 2 and P E 1 , TER ( P E 2 , P E 1 ) .
Finally , these two scores were combined into a single gold standard label , G , as follows : G = W 1 TER ( P E 1 , M T ) + W 2 TER ( P E 2 , P E 1 ) where weights , W 1 and W 2 , are decided by the outcome of the following tuning process : W 1 is held static at 1 ; W 2 is increased by 1 from a starting value of 1 until either of the following stopping criteria is reached : ( i ) the ratio between the standard deviation and the mean is 0.5 for the official baseline QE system predictions , or ( ii ) a baseline prediction distribution is constructed by assigning to all prediction labels the expected value of the training set labels .
This second case is designed to deal with the degenerate behaviour described above of assigning to each test item the average over the training data , with the stopping criteria being such that the difference between the MAE achieved by such a system and the official baseline MAE is at least 0.1 .
The final values used to produce official results were W 1 = 1 and W 2 = 13 .
The way in which the gold standard is constructed deviates to quite a degree from conventional gold standards , therefore , which raises some important questions .
Firstly , it appears that the optimization process is carried out with direct reference to the test set .
If so , does such a process overly blur the lines with respect to what is considered true unseen test data ?
Secondly , neither of the two TER scores corresponds to a straightforward human assessment , putting into doubt the conventional validity attributed to human- generated gold standards .
For example , the component assigned most weight in the final evaluation is TER ( P E 2 , P E 1 ) , and this unfortunately corresponds more closely to a measure of the dependence of the meaning of the sentences within a given document on other sentences in that document , as opposed to the overall quality of the MT output document .
Finally , and most importantly , assigning weights to components of the human evaluation through a somewhat arbitrary optimization process deviates from the expected interpretation of each reported correlation , i.e. the correlation between system predictions of translation quality and the actual quality of translated documents .
Including such weights in the construction of a gold standard potentially invalidates the human evaluation , and is unfortunately very likely to exaggerate the apparent performance of some systems while under-rewarding others .
To demonstrate to what degree this could be the case , since post-editions employed in the creation of the actual gold standard used to produce results in the shared task are unavailable , we simulate a possible set of TER ( P E 1 , M T ) and TER ( P E 2 , P E 1 ) labels for test documents in the following way : A possible set of TER ( P E 1 , M T ) labels are simulated by relocation of the TER score distribution ( of the MT output document with reference translations as opposed to postedits ) to more closely resemble scores of our later human evaluation , before rescaling that score distribution according to the mean and standard deviations ( provided in the QE task findings paper ) of TER ( P E 1 , M T ) .
TER ( P E 2 , P E 1 ) scores were then reverse-engineered from the correspondence between TER ( P E 1 , M T ) and gold labels .
1 Final gold labels arrived at through our simulation of TER ( P E 1 , M T ) and TER ( P E 2 , P E 1 ) are identical to the original evaluation for W 1 = 1 and W 2 = 13 .
Figure 1 shows correlations achieved by all systems participating in the shared task when the weight of our simulated TER ( P E 2 , P E 1 ) component is varied from 1 up towards the origi-1 All data employed in this work is available at http : //github.com/ygraham/eacl2017 nal weight of 13 and beyond .
The correlation achieved by all systems varies dramatically with W 2 , demonstrating how correlations achieved by QE systems are highly dependent on the chosen weights .
Alternate Human Gold Standard
A recent development in human evaluation of MT is direct assessment ( " DA " ) , a human assessment shown to yield highly replicable segment - level scores , by combination of a minimum of 15 repeat human assessments per translation into mean scores .
Human adequacy assessments are collected via a 0 - 100 rating scale that facilitates reliable quality control of crowd-sourcing .
Document - level DA scores are computed by repeat assessment of the individual segments within a given document , computation of the mean score for each segment ( micro-average ) , and finally , combination of the mean segment scores into an overall mean document score ( macro-average ) .
2 DA assessments are carried out by comparison of a given MT output segment ( rendered in black ) with a human-generated reference translation ( in gray ) , and human annotators rate the degree to which they agree with the statement :
The black text adequately expresses the meaning of the gray text in Spanish .
3 Reference translations employed in DA are manually translated by an expert with reference to the entire source document , thus ensuring individual reference segments retain any elements needed to stay faithful to the meaning of the source document as a whole .
Since in creation of a test set in general in MT , the professional human translator will have access to and make use of the entire source document , reference translations found in standard MT test sets can directly be employed .
Self-replication Experiment
Although DA has been shown to produce highly reliable human scores for translations on the segment level , achieving a correlation of above 0.9 between scores for segments collected in separate data collection runs , the reliability of DA on the document level has yet to be tested .
Similar to Graham et Quality control is carried out by inclusion of pairs of genuine MT outputs and automatically degraded versions of them ( bad references ) within 100- translation HITs , before a difference of means significance test is applied to the ratings belonging to a given worker .
The resulting p-value is employed as an estimate of the reliability of a given human assessor to accurately distinguish between the quality of translations ( Graham et al. , 2013 ; .
Table 1 shows numbers of judgments collected in total for each data collection run on Mechanical Turk , including numbers of assessments before and after quality control filtering , where only data belonging to workers with a p-value below 0.05 were retained .
Figure 2 shows the correlation between document- level DA scores collected in Run A with scores produced in Run B , where , for Run B , repeat assessments are down-sampled to show the increasing correspondence between scores as ever-increasing numbers of repeat assessments are collected for a given document .
Correlation between scores collected in the two separate data collection runs reaches r = 0.901 by a minimum of 27 repeat assessments of the sentences of a given document , or by an average 107 sentence assessments per document .
4 Since DA scores achieve a correlation of r > 0.9 in our self-replication experiment , we now know that DA provides reliable human evaluation 4 Variance in numbers of repeat assessments per document is due to sentences of all documents being sampled without preference for documents made up of larger numbers of sentences .
scores for not only segments but also documents .
The validity of DA is superior to the existing gold standard employed for document - level QE as it avoids arbitrary weighting or tuning of component scores to reach final gold standard labels .
It is therefore highly unlikely to ever unfairly exaggerate ( or under-reward ) the performance of any QE system in a given evaluation .
With regard to resources required to construct each gold standard , a single DA data collection run cost USD $ 109 on average , while the cost estimate provided to us by a professional post-editor for the same test set came between USD $ 1,422 and USD $ 2,728 .
In other words , the cost of producing the gold standard is 10 - 20 times greater for postediting than DA .
5
Re-evaluating Doc-level QE WMT - 16
In order to demonstrate DA 's potential as a gold standard , Table 2 shows correlations for WMT - 16 document - level QE shared task systems when evaluated with DA and the original gold standard .
Results show system rankings that diverge from the original , as the original gold standard exaggerated the performance of three participating sys - tems , while under-rewarding two other systems .
Notably , system GRAPH - DISC , which includes discourse features learned from document - level features , achieves a higher correlation when evaluated with DA compared to the original gold standard .
Differences in correlations are small , however , and ca n't be interpreted as differences in performance without significance testing .
Differences in dependent correlations showed no significant difference for all pairs of competing systems according to Williams test ( Williams , 1959 ; .
Discussion of DA Fluency Omission
In development of the newly proposed variant of DA for document- level QE , the question arose if the assessment should also include an assessment of the fluency of documents ( in addition to adequacy ) , as in Graham et al . ( 2016 b ) .
Besides the several other design criteria in DA aimed at avoiding possible sources of bias in general , the motivation for including a separate fluency assessment was originally to counter any bias resulting from comparison of the MT output with a reference translation in the adequacy assessment , similar to the reference bias encountered in automatic metrics scores .
Although genuine human assessors of MT are unlikely to be biased by the reference by anything close to the degree to which automatic metrics will be , there still exists the possibility that reference bias could impact the accuracy of DA scores to some degree .
Inclusion of fluency does of course have a trade-off , however , requiring additional resources , resources that could otherwise be employed to increase the number of translations in the test set , for example .
It is important to investigate the degree to which reference bias may or may not be a problem for DA before including it in document- level QE evaluation therefore .
Graham et al. ( 2016a ) provide an investigation into reference bias in monolingual evaluation of MT and despite the risk of reference bias that DA adequacy could potentially encounter , experiment results show no evidence of reference bias .
Human assessors of MT appear to genuinely read and compare the meaning of the reference translation and the MT output , as requested with DA , applying their human intelligence to the task in a reliable way , and are not overly influenced by the generic reference .
Although DA fluency could still have its own applications , for the purpose of evaluating MT or MT QE , this additional insight into the lack of reference bias encountered by DA adequacy means that there is no longer any real motivation for including DA fluency when resources are constrained .
Given the choice of inclusion of DA fluency in evaluation of document- level QE or expanding the test set ( with respect to adequacy ) , there is no question that the latter is now the more sensible choice .
Conclusion Methodological concerns were raised with respect to optimization of weights employed in construction of document- level QE gold standards in WMT - 16 .
We demonstrated the degree to which MT system rankings are dependent on weights employed in the construction of the gold standard .
Experiments showed with respect to the alternate gold standard we propose , direct assessment ( DA ) , scores for documents are highly reliable , achieving a correlation of above 0.9 in a self-replication experiment .
Finally , DA resulted in a substantial estimated cost reduction , with the original post-editing gold standard incurring a 10 - 20 times greater cost than that of DA .
Figure 1 : 1 Figure1 : System performance as the weight of the TER ( P E 2 , P E 1 ) human evaluation component is increased to 13 , as in official evaluation , and beyond ( WMT - 16 document - level QE English to Spanish shared task systems ) .
