title
SAO WMT19 Test Suite : Machine Translation of Audit Reports
abstract
This paper describes a machine translation test set of documents from the auditing domain and its use as one of the " test suites " in the WMT19 News Translation Task for translation directions involving Czech , English and German .
Our evaluation suggests that current MT systems optimized for the general news domain can perform quite well even in the particular domain of audit reports .
The detailed manual evaluation however indicates that deep factual knowledge of the domain is necessary .
For the naked eye of a non-expert , translations by many systems seem almost perfect and automatic MT evaluation with one reference is practically useless for considering these details .
Furthermore , we show on a sample document from the domain of agreements that even the best systems completely fail in preserving the semantics of the agreement , namely the identity of the parties .
BLEU chrF3 nCDER nCharacTER nPER nTER nWER
Introduction Domain mismatch is often the main sources of machine translation errors .
At the same time , it has been suggested in the speech recognition area that models trained on extremely large data can perform well across domains , i.e. without any particular domain adaptation ( Narayanan et al. , 2018 ) .
We believe that for some of the language pairs annually tested in the WMT shared translation task , the best machine translation systems may have grown to sizes where the domain dependence may be less critical .
At the same time , we know that most of current MT systems still operate at the level of individual sentences and therefore have no control over document- level coherence e.g. in terms of lexical choice .
To investigate the two questions , domain independence and document- level coherence , we cleaned and prepared a dedicated set of documents from the auditing domain and submitted it as one of the " test suites " to this year 's WMT News Translation Task .
The collection is called " SAO WMT19 Test Suite " after the Supreme Audit Office of the Czech Republic ( SAO ) who provided the original audit reports created in cooperation with other national supreme audit institutions ( SAIs ) .
1
This paper is organized as follows :
In Section 2 we describe the source and our processing of the test documents .
Section 3 provides automatic scores of WMT19 MT systems on the test suite and Section 4 presents the manual evaluation .
One more document type , namely a sublease agreement , was evaluated separately , see Section 5 .
We release the test suite for public use , see Section 6 , and we conclude in Section 7 .
Composition of SAO Test Suite The SAO Test Suite consists of 10 multi-language audit reports issued by the SAO .
The reports describe investigations carried out jointly by SAO and one or more other national auditing institutions between the years 2004 and 2015 .
The reports were published in multiple language versions or as multilingual documents .
They were created jointly by the co-operating SAIs in English and later on , they were translated by translation agencies and finally corrected by the authorized auditors from the respective countries .
The end effect of this careful procedure is that from time to time , the different language versions slightly depart in the exact wording , including minor shifts of the conveyed meanings .
All the reports come in 3 different languages .
All of them include Czech and English , the third used language differs .
See Table 1 for a summary .
Creation of the SAO Test Suite
The audit reports were collected primarily from the website of SAO .
It is important to note that while being publicly available , these documents did not make it to any of WMT19 constrained training data , probably because the texts appear on the web only in the form of PDFs .
We doublechecked that there is no overlap by searching the data for exact and near sentence matches .
Very short segments like generic titles or section numbers were naturally present in the training data but we did not find any longer sentences , let alone more sentences from a test document .
First , we converted the documents from the PDF format to plain text .
We note that some of the documents were bitmap PDFs ( scans ) and we had to use OCR to obtain the text .
This was particularly tedious for multi-language documents with texts side by side in two or three columns .
The rest of the processing was applied only to Czech , English and German versions of the documents , because other languages were not considered in WMT19 News Translation Task .
The plain text versions were automatically segmented into sentences using the trainable tokenizer TrTok by Mar? k and Bojar ( 2012 ) .
We then automatically aligned sentences in English and Czech versions using hunalign ( Varga et al. , 2005 ) and manually revised this alignment .
During the manual revision of sentence alignments , we removed footnotes , tables and graph captions , as well as occasional paragraphs not present in one of the languages .
Sometimes , sentence segmentation had to be fixed as well .
In the final stage , we added the German side to the already sentence - aligned English - Czech files , creating a tri-parallel test set .
In some cases , the segmentation into sentences was not exactly parallel and we had to break primarily the German sentences into clauses , or introduce blank segments in some of the files to allow for a better match .
Once or twice even the order of the clauses in German was swapped compared to the aligned Czech and English .
SAO Test Suite in WMT19 Shared Task
We submitted our files as a " test suite " complementing the WMT19 News Translation Task .
This means that all primary MT systems participating in the News Translation Task also translated our files .
The English ?
Czech and German ?
English systems were supervised , i.e. trained on genuine parallel texts ( and target- side monolingual data ) .
The Czech ?
German research systems were unsupervised , i.e. trained only on monolingual source and target texts , optionally using a small parallel development set of a few thousand sentence pairs .
Our evaluation also includes several anonymized online systems ( " online -. . . " ) the internals of which are not known .
These online systems could in principle include our test suite as part of their training data .
The number of evaluated documents and MT systems for each examined language pair is in Table 2 .
Automatic Evaluation
For automatic evaluation , we use several of common MT evaluation metrics ( Papineni et al. , 2002 ; Popovi ? , 2015 ; Leusch and Ney , 2008 ; Wang et al. , 2016 ; Snover et al. , 2006 ) .
Metrics listed with the prefix " n " are reversed ( 1 ? score ) so that higher numbers indicate a better translation in all the figures we report .
We calculate the score for each of the documents in our test suite separately and report the average score and the standard deviation .
The scores are detailed in Tables 3 to 7 .
In the subsequent tables , we sometimes abbreviate system names for typesetting reasons .
The main observation across the tables is that all the scores heavily vary across individual documents .
The typical standard deviation is 3 - 5 for BLEU and similarly for other metrics .
The metrics do not always agree on the overall ranking of the systems , as indicated by " " in the tables , but these differences are much smaller that the variance due to the particular documents .
A big caveat should be taken when interpreting all automatic scores as an estimate of real translation quality , because they are all based on the single reference translation .
See also the discussion in Section 4.2 below .
Manual Evaluation
Due to the specific terminology in the documents and domain knowledge needed to verify translation quality , we asked the SAO 's employees serve as the annotators .
2
All of them were native Czech speakers with a high level of English and / or German proficiency .
We also attempted to find native German auditors but we were not successful so far .
English ?
German and German ?
English translation was thus evaluated by a single SAO employee , a native Czech speaker with a great command of both English and German , including the specific auditing domain .
Establishing Evaluation Criteria
Our manual evaluation criteria are based on the criteria used for the scoring of essays in the Czech GCSE counterpart ( " maturita " ) for the Czech language .
After a short test session with our prospective annotators , we realized how very narrow this specific field is and we simplified the original set of 7 criteria with 6 levels each to only 5 criteria and 4 levels each .
This simplification definitely saved some annotation time and we also believe that it increased the inter-annotator agreement , although we did not collect enough annotations to reliably measure it .
The final criteria to be used in the evaluation are as follows : 1 ) Language Resources - Spelling and Morphology ?
0 points : 10 or more spelling or morphology errors . ?
1 point : 9 - 6 spelling or morphology errors . ?
2 points : 5 - 3 spelling or morphology errors . ?
3 points : 2 - 0 spelling or morphology errors .
2 ) Vocabulary - Adequacy of Terms Used ?
0 points : Frequently , used terms are inappropriately chosen . ?
1 point : Sometimes , used terms are inappropriately chosen . ?
2 points : Rarely , used terms are inappropriately chosen . ?
3 points :
There are no terms , which would be inappropriately chosen .
3 ) Vocabulary - Clarity of the Text in Terms of Used Words ?
0 points :
The choice of words and phrases fundamentally impairs the understanding of the text . ?
1 point :
The choice of words and phrases sometimes impairs the understanding of the text . ?
2 points :
The choice of words and phrases rarely impairs the understanding of the text . ?
3 points :
The choice of words and phrases does not impair the understanding of the text .
4 ) Syntax and Word Order ?
0 points : Syntactic shortcomings are high in the text .
times incoherent and barely serves its communication purpose ( but the addressee believes that he or she understands the main content of the text more or less ) .
?
2 points :
The recipient navigates the text , though not entirely comfortably .
The text is coherent and more or less fulfils its communication purpose ( the addressee is sure he understands the text as a whole ) .
?
3 points :
The recipient is fully oriented in the text .
The text is completely coherent , it serves its communication purpose excellently ( the addressee fully and without difficulty understands the text as a whole ) .
Reference Effectively Useless
One observation that emerged from our consultation with the experts in the auditing field was that precise choice of terms is extremely important but that detailed knowledge of the respective legislation and practice is necessary to evaluate the translations .
We , highly proficient speakers of English , but lacking any substantial information on taxation and other topics discussed in the documents , often could not see any lexical errors , because at the general level , the choice of words seemed acceptable .
The experts discussed at length the various factual implications of using one of the nearsynonyms over another .
Anecdotally , voting among our three consultants would not always work either .
Without a chance to discuss a particular term , two of the consultants would label the choice of an MT system as wrong , but the third consultant , the most experienced expert in the very field actually approved it .
The reference translations proved effectively useless for these fine distinctions , because the particular term used in the single reference was often not the only possible one .
As already mentioned , the careful revision applied to the reference translations has sometimes slightly shifted the meaning , preferring a better match with the factual knowledge over the literality of the translation .
Execution of Evaluation
As was mentioned above , the annotators were the employees of the SAO .
We decided to score not the complete docu - ments but rather selected segments of about 15 consecutive sentences .
Each such segment takes something between a half and a full A4 page when printed .
For each evaluated page , the annotators were provided with another such page - the corresponding 15 sentences in the source language .
We deliberately avoided providing reference translations for two reasons : ( 1 ) we included the reference as if it was one of the competing MT systems , ( 2 ) we know that the source and the reference occasionally departed from each other ; judging MT systems based on the references would thus not be a fair comparison even if carried out by humans and not an automatic metric .
In a small probe , we estimated that the annotation of one such segment will take about 15 minutes .
Table 8 summarizes the number of annotated document segments and annotators providing the scores .
The actual evaluation of each segment was submitted by the annotators through a simple web interface , which recorded : ? the segment ID ; ? points assigned to the evaluated categories ; ? a free-form description of the most serious error ( s ) ; ?
a free-form field for further comments ; ? a check - box indicating whether the annotator is an expert in the given field of the segment ( e.g. in the field of value-added tax , VAT ) .
Results of Manual Evaluation
We did not have enough human capacity to calculate an full-fledged inter-annotator agreement .
To have at least some idea of how annotators agree , we let three of all segments be assessed by two different annotators .
Comparison of the scores reveals that annotators often differ in their assessment , even though the assigned points are almost always neighbouring .
Somewhat surprisingly , except for a single segment , the annotators did not consider themselves experts in the field of the documents presented to them , even though they all should be professionals in the auditing field .
English-to - Czech Translation
Altogether , the English ?
Czech translations were evaluated by 5 annotators .
They evaluated 48 segments randomly chosen from documents translated by 4 selected systems and the reference translation .
The translation systems were selected based on their automatic scores in WMT19 and their results in the past years .
TartuNLP -c was added as a representative of a system with an overall lower output quality , although it seemed to perform well in some of the observed phenomena .
Table 9 shows the mean scores and standard deviations collected on the translations according to the five criteria specified in Section 4.1 .
As our mini-comparison of annotator agreement suggests mismatches in score assignments , we provide also a statistic that abstracts from the absolute values of assigned scores .
Because the assigned scores are associated with a particular categorical description , we avoid the standard normalization of mean and variance .
Instead , we take all the assessments produced by a single annotator and sort the systems by the average of scores assigned by him or her in a given criterion .
Table 10 then shows the mean ordinal number of each of the systems across all the annotators .
Unlike the scores in Table 9 , the best ordinal number is 1 and it gets worse as it increases .
Even though some subtle differences occur in ordering of the systems in Tables 9 and 10 , the main observations remain the same .
Manual evaluation confirms the lower quality of TartuNLPc measured by automatic metrics .
On the other hand , online - B scored best and it appears on par with the human translation , whereas it was surpassed by CUNI systems in terms of the automatic metrics as well as in news translation ( see the main Findings of WMT19 paper ) .
Interestingly , apart from TartuNLP -c all the other MT systems seem to yield fewer spelling and morphology errors than the human translators , although the differences are within the standard deviation bounds .
CUNI - DocTransformer -T2T stands out by being better even beyond the reported standard devia-tion of the ordinal interpretation ( see 1.40 ?0.80 in " Spell .
& morpho. " in Table 10 ) .
Due to large values of standard deviations , the small sample size and the fact that the underlying set of evaluated document segments varied across the systems , it is difficult to draw reliable conclusions from these observations .
Some counterintuitive results can be thus attributed to pure randomness .
For example , CUNI - Transformer - T2T -2019 differs from CUNI - DocTransformer -T2T only in the fact that it operates on triples of consecutive sentences .
This should increase the adequacy of vocabulary chosen and should have no effect on spelling and morphology but we have seen the opposite .
The overall statement we can make is that for English - to - Czech , the specific domain of audit reports does not differ much from the general observations made in the main News Translation Task : the order of the systems generally matches and the better systems are very close to the human performance .
English ?
German Translation Manual evaluation of English ?
German translations was provided by a single annotator on 16 randomly selected segments , covering 3 systems and the human translation .
In the opposite translation direction , also 16 segments were evaluated by the same annotator , this time covering 2 systems and the human translation .
We chose the systems which are popular ( online - B ) , expected to score among the best based on their ( automatically assessed ) performance on the News Translation Task ( MSRA - MADL ) or are provided by the European Commission as a service for EU institutions ( eTranslation ) .
The mean scores in Tables 11 and 12 show that none of the systems outperforms human translation .
The ordering of the systems remains the same across most of the evaluation criteria .
Unlike in automatic evaluation , the human annotator considers the output of online - B in English ?
German translation of lower quality ( except spelling and morphology ) than the outputs of its competitors .
In German ?
English translation , the ordering of the systems according to the manual evaluation agrees with the automatic one .
All in all , comparison of manual and automatic evaluation suggests that the systems achieving high automatic scores may be judged differently by human annotators .
As the quality of translation decreases , it is sufficient to evaluate it automatically .
Most Common Mistakes
A part of the evaluation web interface was a freeform field for the description of the most serious error ( s ) encountered .
We collected these comments and manually organized them into several categories .
We found out that the most common mistakes were : ? fluency ; ? wrong translation of terms ; ? grammatical correctness ( such as a wrong gender chosen for pronouns ) ; ? non-translated abbreviations , or abbreviations which do not make sense in the Czech translation ; ? outputs completely missing a half of the sentence .
This was particularly likely after a punctuations such as the closing bracket in the middle of the sentence .
Table 13 summarizes the overall error counts by category .
( The reference is included in these counts . )
As mentioned above , we did not find any native German auditor who could annotate our SAO Test Suite , so the annotation was done by a single Czech auditor .
This could explain the relatively big differences between language pairs : with a single annotation , the annotator disagreements are not averaged out .
For instance , it is possible that this marked some of the errors as wrong grammatical constructions while en?cs annotators could score it in fluency criterion .
We also have to take into account the absolute number of annotated document segments ( 48 for Czech , 16 for English ?
German ) .
Considering the average number of errors per one annotated document segment , German ?
English translation seems the worst , see the last line of Table 13 .
Translation of Agreements Aside from the SAO audit documents , we added one moderately long document from a very specific domain related to auditing : agreements .
As the source document , we used the English version of a sublease agreement , which was in fact a ( non-professional ) translation from Czech .
The original Czech text was evaluated with all other WMT19 systems as if it was one of the systems .
Due to the different nature of the text , we decided to evaluate the translation of the sublease agreement differently from the evaluation of the main part of SAO Test Suite .
Manual Evaluation
The evaluation of this small set containing one source document , one human translation and 11 machine translated documents was done manually .
The evaluation was partially blind .
Technically , the candidate translations were not labelled with the system name , but the main annotator could guess some of the systems .
Only the systems online -X , Y and G are truly blind , we do not know their identity even from past evaluations .
We are confident that even the knowledge of the MT system did not affect our evaluation because we fully focused on the hard criteria such as named entity preservation or term consistence throughout the document .
The only soft criterion included was the " fluency " one .
We have also included the reference document in the evaluation .
Establishing Evaluation Criteria
By inspecting several of the MT outputs , we first defined the assessment criteria .
They generally fall into two categories : ( 1 ) target-only , and ( 2 ) source - based .
Whereas in the former category , we consider only quality of the target texts on their own , regardless the source , in the latter we validate if the selected bits of information were preserved or corrupted during the translation process .
In the target-only category , we focused on the following : ? fluency ; ? grammatical correctness ( this is very strict and well defined in Czech ; most errors were in morphological agreement and sometimes verb tense ) ; ? casing errors ( esp. in named entities ) ; ? incomprehensibility of the segment ; ? " spasm " , i.e. the situation when the MT system gets stuck in repeating some tokens ; ? superfluous words ; ? missing words or a whole sentence .
As for the source- based category , we have focused on the errors , which were formed either by wrong translation of a very domain-specific term or an inconsistence of used terms throughout the whole document .
?
Named Entities - here we checked mainly the preservation of the information : - Person ( e.g. name and surname ) ; - Address ( e.g. street name and number ) ; - Date ( esp. whether the format has been kept consistent ) ; - Numbers ( if the transcription of numerals was correct ) ; - Flat composition ( the Czech-specific way is to count rooms and kitchens / kitchinette and indicate it as a compact string , here " 1 + 1 " ) ; - Wrong abbreviation ; - Expanded abbreviation ( e.g. in Czech , the " ZIP CODE " should be translated as " PS ? " , which stands for " po?tovn ?
sm?rovac ? ?slo " , but this abbreviation is never spelled out in written text ) .
?
Document-specific terms : - Tenant ; - Lessee ; - Supplement ( of the agreement ) ; - Sublease agreement ; - Contracting parties ; - Apartment in question ; - Equipment ( e.g. the kitchen ) ; - Amenities ( e.g. a cellar or a segment of the garden ) ; - Housing cooperative ; - Team of owners ; - Term of the lease ; - The specification of the supplement ( " no. 1 " ) .
In the category of " Document-specific terms " , we focused on evaluation whether : ? the term is translated correctly , incorrectly ( incl. not translated at all ) , or missing altogether ; ? the target term is preserved in the document .
It should be noted that the MT system was often free to choose from several translation options of a term .
At the same time , a very important criterion was whether the translation of each of the terms was consistent throughout the document and also whether it did not clash with other choices .
For example , each of the terms " tenant " and " lessee " could be-depending on the particular situation - correctly translated as " pronaj?matelka " , " n?jemkyn ? " or " podn?jemkyn ? " ( all are feminine variants of the words , because incidentally , it was women who were entering this sample agreement ) .
If the two different parties however happened to have been referred to in any way that could lead to confusion , we marked this as a ( serious ) error .
In some cases , we had a strict expectation .
For instance the term " sublease " could be translated into Czech in principle either as " pron?jem " ( which corresponds to the relationship between a landlord and a tenant ) or as " podn?jem " ( which corresponds to the relationship between a tenant and a lessee ) .
Based on the text of the agreement , it was however clear that the correct term is " podn?jem " ( the tenant is not the actual owner of the property ) , so we demanded the this particular choice .
Execution of Evaluation Because of the relatively small amount of data , the evaluation was done on paper , see Figure 1 .
The annotations of " source - based " error types were done with respect to the source text using a fixed set of " markables " , i.e. the set of occurrences of words and expressions to annotate for correctness .
The set of markables was identical for all the candidate translations .
Each markable in each translation candidate received a label indicating if it was translated correctly , with an error , or if was fully missing .
The " target-only " error types were marked independently for each system , with no number of markable positions given apriori .
The question was how to deal with inconsistency in used terms .
At the beginning it was not clear whether we should assume that the first occurrence of term " defines " it for the rest of the document or whether we should take the most frequent one as the " intended one " by the MT system and treat other translations as errors .
After the first round of corrections , we chose the first option .
Some terms , e.g. " tenant " , " lessee " or " agreement " had always only one correct translation , but some , e.g. " sublease " could have had multiple possible translations .
In these latter cases , we always marked the first occurrence as correct .
Results of Manual Evaluation
The summary of manual evaluation is presented in Table 14 . Errors in the source - based categories are more frequent than in target-only .
This is mainly due to the incorrect translation of the term " lessee " ( see Section 5.4.2 below ) .
One thing worth mentioning is the 9 errors and 3 omissions in the reference translation .
This can be partly attributed to Czech being in fact the original and English ( i.e. the source for MT systems ) its translation .
What is a good Czech ?
English manual translation is not always literal enough when observed from the English side .
Three errors were for instance incurred from one single case where the Czech text referred to the agreement itself one time less than the English text , but this " missing reference " ( fully acceptable in the Czech ?
English direction ) counted as several missing expressions .
As for the true errors , there was one incorrect translation of term " lessee " and one mistake in the number of the Supplement .
The number of errors considerably varies across the systems .
The best system ( CUNI - Transformer - T2T - 2018 ) in our evaluation is also the winner on news in the evaluation last year .
As Bojar et al. ( 2018 ) report , this system significantly outperformed humans at the level of individual sentences in that evaluation .
In our setting , the number of errors by CUNI - Transformer - T2T - 2018 is twice the number of errors in the reference , but aside from term choice discussed in Section 5.4.2 , one could say that the translation is very good .
In the target-only category , we did not have any pre-defined items that could be correct or incorrect .
Therefore the number of errors varies greatly across the systems .
From the lowest number of errors in the CUNI - Transformer - T2T -2019 ( 5 errors ) and in CUNI - Transformer - T2T - 2018 ( 6 errors ) to the very high numbers in online - X and online - G ( 48 and 34 errors , respectively ) .
As for the " ( Miss ) " counts , there were two types of situations : ( 1 ) only a single word was missing in the output and ( 2 ) the whole sentence or a half of a paragraph was not there .
The second case often lead to a large increase in the " ( Miss ) " count because several markables from the source were supposed to appear in the lost part .
The systems uedin and online - X were most affected by this .
Another interesting fact worth mentioning is that even though the system online - Y had a relatively low number of mistakes , those errors made the readability and the comprehensibility of the message substantially more difficult than e.g. the translation by online - B with a higher error count .
is important but their type can be critical , too .
We already mentioned the missing sentences or " spasm " , which accounted for the 14 missing term translations in the output of uedin .
Another interesting case is a " misunderstanding " of the MT system .
For instance , uedin system misunderstood " I. " ( the Roman numeral ) for the pronoun " I " or mistranslated the " ZIP CODE " as " ob?anka " ( personal ID card ) .
It is exactly these types of errors , which are the most serious from the reader 's point of view .
Detailed Error Counts
Table 15 provides further details on error types observed in the outputs of individual MT systems .
The table is again sorted by the total number of errors as in Table 14 .
We see that the best system ( CUNI - Transformer - T2T - 2018 ) fully failed in the translation of the terms " lessee " , " amenities " and " term of the lease " .
This system was also the only one which dealt well with abbreviations .
In contrast to all other systems , CUNI - DocTransformer -Marian struggled to translate several named entities correctly .
This system used the same training data as CUNI - Transformer - T2T -2019 and both of these systems translate several consecutive sentences at once in order to improve cross-sentence consistency but they somewhat differ in the details of the handling of multi-sentence input , and they also differ in the underlying MT system : Tensor2Tensor vs. Marian , see Popel et al . ( 2019 ) for more details .
It is hard to explain why these sentences could adversely affect named entities , so the authors of the system should carefully look at this issue .
Referring to Contracting Parties
Our analysis so far does not sufficiently highlight the most severe flaw of all the MT systems .
The problem concerns a clear way of referring to the contracting parties , i.e. the translation of the terms " tenant " and " lessee " .
All the systems translated almost all occurrences of these terms using one word only , " n?jemce " , which causes a lot of confusion to any reader ( including native Czech speakers ) .
The problem which occurred here arose from the fact that there are actually three common roles and two types of agreements in apartment renting .
Commonly , the contracting parties are : ? landlord -tenant = pronaj?matel-n?jemce in the case when the landlord is the owner of the property ; ? tenant-lessee = n?jemce-podn?jemce for the sublease agreement , i.e. when the owner is not directly involved in the agreement .
The common translation in training corpora or dictionaries of the term " lessee " is apparently " n?jemce " which is possible , but only if the term " tenant " is not used in the document as well .
Should this happen , " lessee " needs to be translated as " podn?jemce " to avoid confusion .
Table 16 details the performance of the systems in this respect .
Each line sums up to 17 mentions of either of the two contracting parties .
We see that the reference translation made only one error by using the wrong term while all the other systems cause a term clash ( using the same term for both parties ) in half of the cases .
This , in fact , corresponds to all the mentions of the second party and all these translations by all the systems are thus completely wrong .
6 Test Suite Availability SAO Test Suite is available under CC-BY -SA at : https://github.com/ELITR/ wmt19-elitr-testsuite
Conclusion
We presented a test suite of Czech , English , German , Polish and Slovak documents from the au-diting domain and used its English - Czech - German tri-parallel part in the WMT19 Translation Shared Task .
We also added one more document type , namely a sublease agreement .
Despite the fact that the participating MT systems were trained for a rather general domain of news articles , many of them perform very well on general terms .
Our detailed manual evaluation used criteria similar to those used in the scoring of GCSE essays of the Czech language .
An important observation in our study was that a thorough domain knowledge is necessary to assess the correctness of the translation , esp. in terms of lexical choices , and that the reference translations are insufficient for the task .
Our impression is that automatic MT evaluation is effectively useless for assessing terminological subtleties , esp. with one reference translation only .
We find this observation particularly important for future research directions , because none of the MT systems are trained in a way which could directly address such subtle issues .
Terminology lists may be a good help for both MT and MT evaluation but we anticipate that the only practically possible ultimate solution for translation would be an interactive system supporting a domain expert in manual correction of terminological choices .
As for the translations of the Sublease Agreement , even though the dispersion in the number of errors is huge- varying from 21 errors ( CUNI - Transformer - T2T - 2018 ) to 125 errors online - Xthe number of errors alone is not as indicative of the practical usability of the translation .
The main problem was that all the systems made the same ( and from the readers ' perspective , the most severe ) translation error by translating the terms " tenant " and " lessee " using the same Czech word " n?jemce " , which made the whole text incomprehensible .
Other observed mistakes needed rather cosmetic adjustments , except for the occasions where the system forgot a whole sentence or the rest of a paragraph .
We released the texts of the test suite for future use and we are also happy to share our annotation protocols , but as of now , we cannot provide any novel automatic evaluation of MT on this test suite .
Figure 1 : 1 Figure 1 : Samples from our annotation with one of the best scoring systems ( CUNI - Transformer - T2T - 2018 ) on the left and one of the worst ones ( online - X ) on the right .
Crosses indicate errors in term translation , strange wordings are underlined , casing errors and other errors have their simple marks .
