title
Learning Probabilistic Synchronous CFGs for Phrase- based Translation
abstract
Probabilistic phrase - based synchronous grammars are now considered promising devices for statistical machine translation because they can express reordering phenomena between pairs of languages .
Learning these hierarchical , probabilistic devices from parallel corpora constitutes a major challenge , because of multiple latent model variables as well as the risk of data overfitting .
This paper presents an effective method for learning a family of particular interest to MT , binary Synchronous Context - Free Grammars with inverted / monotone orientation ( a.k.a. Binary ITG ) .
A second contribution concerns devising a lexicalized phrase reordering mechanism that has complimentary strengths to Chiang 's model .
The latter conditions reordering decisions on the surrounding lexical context of phrases , whereas our mechanism works with the lexical content of phrase pairs ( akin to standard phrase - based systems ) .
Surprisingly , our experiments on French - English data show that our learning method applied to far simpler models exhibits performance indistinguishable from the Hiero system .
Introduction
A fundamental problem in phrase - based machine translation concerns the learning of a probabilistic synchronous context-free grammar ( SCFG ) over phrase pairs from an input parallel corpus .
Chiang 's Hiero system ( Chiang , 2007 ) exemplifies the gains to be had by combining phrase - based translation ( Och and Ney , 2004 ) with the hierarchical reordering capabilities of SCFGs , particularly originating from Binary Inversion Transduc-tion Grammars ( BITG ) ( Wu , 1997 ) .
Yet , existing empirical work is largely based on successful heuristic techniques , and the learning of Hiero- like BITG / SCFG remains an unsolved problem ,
The difficulty of this problem stems from the need for simultaneously learning of two kinds of preferences ( see Fig. 1 ) ( 1 ) lexical translation probabilities ( P ( e , f | X ) ) of source ( f ) and target ( e ) phrase pairs , and ( 2 ) phrase reordering preferences of a target string relative to a source string , expressed in synchronous productions probabilities ( for monotone or switching productions ) .
Theoretically speaking , both kinds of preferences may involve latent structure relative to the parallel corpus .
The mapping between source-target sentence pairs can be expressed in terms of latent phrase segmentations and latent word / phrasealignments , and the hierarchical phrase reordering can be expressed in terms of latent binary synchronous hierarchical structures ( cf. Fig. 1 ) .
But each of these three kinds of latent structures may be made explicit using external resources : word- alignment could be considered solved using Giza ++ ( Och and Ney , 2003 ) ) , phrase pairs can be obtained from these word-alignments ( Och and Ney , 2004 ) , and the hierarchical synchronous structure can be grown over source / target linguistic syntactic trees output by an existing parser .
The Joint Phrase Translation Model ( Marcu and Wong , 2002 ) constitutes a specific case , albeit without the hierarchical , synchronous reordering Start S ? X 1 / X 1 ( 1 ) Monotone X ? X 1 X 2 /X 1 X 2 ( 2 ) Switching X ? X 1 X 2 /X 2 X 1 ( 3 ) Emission X ? e / f ( 4 ) Figure 1 : A phrase- pair SCFG ( BITG ) component .
Other existing work , e.g. ( Chiang , 2007 ) , assumes the word-alignments are given in the parallel corpus , but the problem of learning phrase translation probabilities is usually avoided by using surface counts of phrase pairs ( Koehn et al. , 2003 ) .
The problem of learning the hierarchical , synchronous grammar reordering rules is oftentimes addressed as a learning problem in its own right assuming all the rest is given ( Blunsom et al. , 2008 b ) .
A small number of efforts has been dedicated to the simultaneous learning of the probabilities of phrase translation pairs as well as hierarchical reordering , e.g. , ( DeNero et al. , 2008 ; Zhang et al. , 2008 ; Blunsom et al. , 2009 ) .
Of these , some concentrate on evaluating word-alignment , directly such as ( Zhang et al. , 2008 ) or indirectly by evaluating a heuristically trained hierarchical translation system from sampled phrasal alignments ( Blunsom et al. , 2009 ) .
However , very few evaluate on actual translation performance of induced synchronous grammars ( DeNero et al. , 2008 ) .
In the majority of cases , the Hiero system , which constitutes the yardstick by which hierarchical systems are measured , remains superior in translation performance , see e.g. ( DeNero et al. , 2008 ) .
This paper tackles the problem of learning generative BITG models as translation models assuming latent segmentation and latent reordering : this is the most similar setting to the training of Hiero .
Unlike all other work that heuristically selects a subset of phrase pairs , we start out from an SCFG that works with all phrase pairs in the training set and concentrate on the aspects of learning .
This learning problem is fraught with the risks of overfitting and can easily result in inadequate reordering preferences ( see e.g. ( DeNero et al. , 2006 ) ) .
Almost instantly , we find that the translation performance of all- phrase probabilistic SCFGs learned in this setting crucially depends on the interplay between two aspects of learning : ?
Defining a more constrained parameter space , where the reordering productions are phrase -lexicalised and made sensitive to neighbouring reorderings , and ?
Defining an objective function that effectively smoothes the maximum-likelihood criterion .
One contribution of this paper is in devis-ing an effective , data-driven smoothed Maximum - Likelihood that can cope with a model working with all phrase pair SCFGs .
This builds upon our previous work on estimating parameters of a " bag - of- phrases " model for Machine Translation ( Mylonakis and Sima'an , 2008 ) .
However , learning SCFGs poses significant novel challenges , the core of which lies on the hierarchical nature of a stochastic SCFG translation model and the relevant additional layer of latent structure .
We address these issues in this work .
Another important contribution is in defining a lexicalised reordering component within BITG that captures order divergences orthogonal to Chiang 's model ( Chiang , 2007 ) but somewhat akin to Phrase - Based Statistical Machine Translation reordering models ( Koehn et al. , 2003 ) .
Our analysis shows that the learning difficulties can be attributed to a rather weak generative model .
Yet , our best system exhibits Hiero-level performance on French - English Europarl data using an SCFG - based decoder ( Li et al. , 2009 ) .
Our findings should be insightful for others attempting to make the leap from shallow phrase - based systems to hierarchical SCFG - based translation models using learning methods , as opposed to heuristics .
The rest of the paper is structured as follows .
Section 2 briefly introduces the SCFG formalism and discusses its adoption in the context of Statistical Machine Translation ( SMT ) .
In section 3 , we consider some of the pitfalls of stochastic SCFG grammar learning and address them by introducing a novel learning objective and algorithm .
In the section that follows we browse through latent translation structure choices , while in section 5 we present our empirical experiments on evaluating the induced stochastic SCFGs on a translation task and compare their performance with a hierarchical translation baseline .
We close with a comparison of related work and a final discussion including future research directions .
Synchronous Grammars for Machine Translation Synchronous Context Free Grammars ( SCFGs ) provide an appealing formalism to describe the translation process , which explains the generation of parallel strings recursively and allows capturing long- range reordering phenomena .
Formally , an SCFG G is defined as the tuple ( N , E , F , R , S ) , where N is the finite set of non-terminals with S ?
N the start symbol , F and E are finite sets of words for the source and target language and R is a finite set of rewrite rules .
Every rule expands a left-hand side non-terminal to a right - hand side pair of strings , a source language string over the vocabulary F ?N and a target language string over E ? N .
The number of non-terminals in the two strings is equal and the rule is complemented with a mapping between them .
String pairs in the language of the SCFG are those with a valid derivation , consisting of a sequence of rule applications , starting from S and recursively expanding the linked non-terminals at the right - hand side of rules .
Stochastic SCFGs augment every rule in R with a probability , under the constraint that probabilities of rules with the same left - hand side sum up to one .
The probability of each derived string pair is then the product of the probabilities of rules used in the derivation .
Unless otherwise stated , for the rest of the paper when we refer to SCFGs we will be pointing to their stochastic extension .
The rank of an SCFG is defined as the maximum number of non-terminals in a grammar 's rule right - hand side .
Contrary to monolingual Context Free Grammars , there does not always exist a conversion of an SCFG of a higher rank to one of a lower rank with the same language of string pairs .
For this , most machine translation applications focus on SCFGs of rank two ( binary SCFGs ) , or binarisable ones witch can be converted to a binary SCFG , given that these seem to cover most of the translation phenomena encountered in language pairs ( Wu , 1997 ) and the related processing algorithms are less demanding computationally .
Although SCFGS were initially introduced for machine translation as a stochastic word - based translation process in the form of the Inversion - Transduction Grammar ( Wu , 1997 ) , they were actually able to offer state - of - the - art performance in their latter phrase - based implementation by Chiang ( Chiang , 2005 ) .
Chiang 's Hiero hierarchical translation system is based on a synchronous grammar with a single non-terminal X covering all learned phrase -pairs .
Beginning from the start symbol S , an initial phrase -span structure is constructed monotonically using a simple ' glue gram-mar ' : S ?S 1 X 2 / S 1 X 2 S ?X 1 / X 1
The true power of the system lies in expanding these initial phrase -spans with a set of hierarchical translation rules , which allow conditioning reordering decisions based on lexical context .
For the French to English language pair , some examples would be : S ? X 1 ?conomiques / f inancial X 1 S ? cette X 1 de X 2 / this X 1 X 2 S ? politique X 1 commune de X 2 / X 2 s common X 1 policy Further work builds on the Hiero grammar to expand it with constituency syntax motivated nonterminals ( Zollmann and Venugopal , 2006 ) .
Synchronous Grammar Learning
The learning of phrase- based stochastic SCFGs with a Maximum Likelihood objective is exposed to overfitting as other all-fragment models such as Phrase - Based SMT ( PBSMT ) ( Marcu and Wong , 2002 ; DeNero et al. , 2006 ) and Data-Oriented Parsing ( DOP ) ( Bod et al. , 2003 ; Zollmann and Sima'an , 2006 ) . Maximum Likelihood Estimation ( MLE ) returns degenerate grammar estimates that memorise well the parallel training corpus but generalise poorly to unseen data .
The bias-variance decomposition of the generalisation error Err sheds light on this learning problem .
For an estimator p with training data D , Err can be expressed as the expected Kullback - Leibler ( KL ) divergence between the target distribution q and that the estimate p .
This error decomposes into bias and variance terms ( Heskes , 1998 ) : Err = bias KL ( q , p ) + variance E D KL ( p , p ) ( 5 ) Bias is the KL - divergence between q and the mean estimate over all training data p = E D p ( D ) .
Variance is the expected divergence between the average estimate and the estimator 's actual choice .
MLE estimators for all- fragment models are zerobiased with zero divergence between the average estimate and the true data distribution .
In contrast , their variance is unboundedly large , leading to unbounded generalisation error on unseen cases .
Cross Validated MLE
A well -known method for estimating generalisation error is k-fold Cross-Validation ( CV ) ( Hastie et al. , 2001 ) .
By partitioning the training data D into k parts H k 1 , we estimate Err as the expected error over all 1 ? i ? k , when testing on H i with a model trained by MLE on the rest of the data D ?i = ? j =i H j .
Here we use CV to leverage the bias-variance trade - off for learning stochastic all - phrase SCFGs .
Given an input all - phrase SCFG grammar with phrase -pairs extracted from the training data , we maximise training data likelihood ( MLE ) subject to CV smoothing : for each data part H i ( 1 ? i ? k ) , we consider only derivations which employ grammar rules extracted from the rest of the data D ?i .
Other work ( Mylonakis and Sima'an , 2008 ) has also explored MLE under CV for a " bag - ofphrases model " that does not deal with reordering preferences , does not employ latent hierarchical structure and works with a non-hierarchical decoder , and partially considers the sparsity issues that arise within CV training .
The present paper deals with these issues .
Because of the latent segmentation and hierarchical variables , CV - smoothed MLE cannot be solved analytically and we devise a CV instance of the Expectation - Maximization ( EM ) algorithm , with an implementation based on a synchronous version of the Inside - Outside algorithm ( see Fig. 2 ) .
For each word-aligned sentence pair in a partition H i , the set of eligible derivations ( denoted D ?i ) are those that can be built using only phrase-pairs and productions found in D ?i .
An essential part of the learning process involves defining the grammar extractor G ( D ) , a function from data to an all- phrase SCFG .
We will discuss various extractors in section 4 . Our CV -EM algorithm is an EM instance , guaranteeing convergence and a non-decreasing CVsmoothed data likelihood after each iteration .
The running time remains O( n 6 ) , where n is input length , but by considering only derivation spans which do not cross word-alignment points , this runs in reasonable times for relatively large corpora .
Bayesian Aspects of CV-MLE
Beside being an estimator , the CV - MLE learning algorithm has the added value of being a grammar learner focusing on reducing generalisation error , For 1 ? i ?
k do Extract grammar rules set G i = G( H i ) Initialise G = ?
i G i , p0 uniform Let j = 0 Repeat Let j = j + 1 E-step : For 1 ? i ? k do Calculate expected counts given G , pj?1 , for derivations D ?i of H i using rules from ? k =i G ( k ) M-step : set pj to ML estimate given expected counts Until convergence Figure 2 : The CV Expectation Maximization algorithm in the sense that probabilities of grammar productions should reflect the frequency with which these productions are expected to be used for translating future data .
Additionally , since the CV criterion prohibits for every data point derivations that use rules only extracted from the same data part , such rules are assigned zero probabilities in the final estimate and are effectively excluded from the grammar .
In this way , the algorithm ' shapes ' the input grammar , concentrating probability mass on productions that are likely to be used with future data .
One view point of CV - MLE is that each partition D ?i and H i induces a prior probability P rior ( ? ; D ?i ) on every parameter assignment ? , obtained from D ?i .
This prior assigns zero probability to all ?
parameter sets with non-zero probabilities for rules not in G ( D ?i ) , and uniformly distributes probability to the rest of the parameter sets .
In light of this , the CV - MLE objective can be written as follows : arg max ?
i P rior ( ? ; D ?i ) ? P ( H i | ? ) ( 6 ) This data-driven prior aims to directly favour parameter sets which are expected to better generalise according to the CV criterion , without relying on arbitrary constraints such as limiting the length of phrase pairs in the right - hand side of grammar rules .
Furthermore , other frequently employed priors such as the Dirichlet distribution and the Dirichlet Process promote better generalising rule probability distributions based on externally set hyperparameter values , whose selection is frequently sensitive in terms of language pairs , or even the training corpus itself .
In contrast , the CV - MLE prior aims for a data-driven Bayesian model , focusing on getting information from the data , instead of imposing external human knowledge on them ( see also ( Mackay and Petoy , 1995 ) ) .
Smoothing the Model One remaining wrinkle in the CV - EM scheme is the treatment of boundary cases .
There will often be sentence - pairs in H i , that cannot be fully derived by the grammar extracted from the rest of the data D ?i either because of ( 1 ) ' unknown ' words ( i.e. not appearing in other parts of the CV partition ) or ( 2 ) complicated combinations of adjacent word-alignments .
We employ external smoothing of the grammar , prior to learning .
Our solution is to extend the SCFG extracted from D ?i with new emission productions deriving the ' unknown ' phrase -pairs ( i.e. , found in H i but not in D ?i ) .
Crucially , the probabilities of these productions are drawn from a fixed smoothing distribution , i.e. , they remain constant throughout estimation .
Our smoothing distribution of phrasepairs for all pre-terminals considers source-target phrase lengths drawn from a Poisson distribution with unit mean , drawing subsequently the words of each of the phrases uniformly from the vocabulary of each language , similar to ( Blunsom et al. , 2009 ) . p smooth ( f /e ) = p poisson ( | f | ; 1 ) p poisson ( | e| ; 1 ) V |f | f V |e| e Since the smoothing distribution puts stronger preference on shorter phrase - pairs and avoids competing with the ' known ' phrase - pairs , it leads the learner to prefer using as little as possible such smoothing rules , covering only the phrase -pairs required to complete full derivations .
Parameter Spaces and Grammar Extractors A Grammar Extractor ( GE ) plays a major role in our probabilistic SCFG learning pipeline .
A GE is a function from a word-aligned parallel corpus to a probabilistic SCFG model .
Together with the constraints that render a proper probabilistic SCFG 1 , this defines the parameter space .
The extractors used in this paper create SCFGs productions of two different kinds : ( a ) hierarchical synchronous productions that define the space of possible derivations up to the level of the SCFG pre-terminals , and ( 2 ) the phrase - pair emission rules that expand the pre-terminals to phrase - pairs of varying lengths .
Given the word-alignments , the set of phrase -pairs extracted is the set of all translational equivalents ( without length upperbound ) under the word-alignment as defined in ( Och and Ney , 2004 ; Koehn et al. , 2003 ) .
Below we focus on the two grammar extractors employed in our experiments .
We start out from the most generic , BITG - like formulation , and aim at incremental refinement of the hierarchical productions in order to capture relevant , content - based phrase - pair reordering preferences in the training data .
Single non-terminal SCFG
This is a phrasebased binary SCFG grammar employing a single non-terminal X covering each extracted phrasepair .
The other productions consist of monotone and switching expansions of phrase - pair spans covered by X. Finally , the whole sentence - pair is considered to be covered by X .
We will call this ' plain SCFG ' extractor .
See Fig. 1 . Lexicalised Reordering SCFG
One weakness of the plain SCFG is that the reordering decisions in the derivations are made without reference to lexical content of the phrases ; this is because all phrase - pairs are covered by the same nonterminal .
As a refinement , we propose a grammar extractor that aims at modelling the reordering behaviour of phrase - pairs by taking their content into account .
This time , the X non-terminal is reserved for phrase -pairs and spans which will take part in monotonic productions only .
Two fresh non-terminals , XSL and XSR , are used for covering phrase - pairs that participate in order switching with other , adjacent phrase-pairs .
The nonterminal XSL covers phrase - pairs which appear first in the source language order , and the latter those which follow them .
The grammar rules produced by this GE , dubbed ' switch grammar ' , are listed in Fig.
3 .
The reordering information captured by the switch grammar is in a sense orthogonal to that of Hiero- like systems utilising rules such as those listed in section 2 .
Hiero rules encode hierarchical reordering patterns based on surrounding context .
In contrast , the switch grammar models the reordering preferences of the phrasepairs themselves , similarly to the monotone - swapdiscontinuous reordering models of Phrase - based SMT models ( Koehn et al. , 2003 ) .
Furthermore , it strives to match pairs of such preferences , combining together phrase -pairs with compatible reordering preferences .
Start S ? X 1 /X 1 Monotone Expansion X ? X 1 X 2 /X 1 X 2 XSL ? X 1 X 2 / X 1 X 2 XSR ? X 1 X 2 /X 1 X 2 Switching Expansion X ? XSL 1 XSR 2 /XSR 2 XSL 1 XSL ? XSL 1 XSR 2 /XSR 2 XSL 1 XSR ? XSL 1 XSR 2 / XSR 2 XSL 1 Phrase - Pair Emission X ? e/f XSL ? e / f XSR ? e / f
Experiments
In this section we proceed to integrate our estimates within an SCFG - based decoder .
We subsequently evaluate our performance in relation to a state - of- the - art Hiero baseline on a French to English translation task .
Decoding
The joint model of bilingual string derivations provided by the learned SCFG grammar can be used for translation given a input source sentence , since arg max e p( e |f ) = arg max e p(e , f ) .
We use our learned stochastic SCFG grammar with the decoding component of the Joshua SCFG toolkit ( Li et al. , 2009 ) .
The full translation model interpolates log-linearly the probability of a grammar derivation together with the language model probability of the target string .
The model is further smoothed , similarly to phrase - based models and the Hiero system , with smoothing features ?
i such as the lexical translation scores of the phrase -pairs involved and rule usage penalties .
As usual with statistical translation , we aim for retrieving the target sentence e corresponding to the most probable derivation D * ? ( f , e ) with rules r , with : p( D ) ? p( e ) ? lm p scf g ( e , f ) ? scf g i r?D ? i ( r ) ? i
The interpolation weights are tuned using Minimum Error Rate Training ( Och , 2003 ) .
Results
We test empirically the learner 's output grammars for translating from French to English , using k = 5 for the Cross Validation data partitioning .
The training material is a GIZA ++ wordaligned corpus of 200K sentence - pairs from the Europarl corpus ( Koehn , 2005 ) , with our development and test parallel corpora of 2K sentencepairs stemming from the same source .
Training the grammar parameters until convergence demands around 6 hours on an 8 - core 2.26 GHz Intel Xeon system .
Decoding employs a 4 - gram language model , trained on English Europarl data of 19.5 M words , smoothed using modified Kneser - Ney discounting ( Chen and Goodman , 1998 ) , and lexical translation smoothing features based on the GIZA ++ alignments .
In a sense , the real baseline to which we might compare against should be a system employing the MLE estimate for the grammar extracted from the whole training corpus .
However , as we have already discussed , this assigns zero probability to all sentence - pairs outside of the training data and is subsequently bound to perform extremely poorly , as decoding would then completely rely on the smoothing features .
Instead , we opt to compare against a hierarchical translation baseline provided by the Joshua toolkit , trained and tuned on the same data as our learning algorithm .
The grammar used by the baseline is much richer than the ones learned by our algorithm , also employing rules which translate with context , as shown in section 2 .
Nevertheless , since it is not clear how the reordering rules probabilities of a grammar similar to the ones we use could be trained heuristically , we choose to relate the performance of our learned stochastic SCFG grammars to the particular , stateof - the - art in SCFG - based translation , system .
Furthermore , our results highlight the importance of the smoothing decoding features .
The unsmoothed baseline system itself scores considerably less when employing solely the heuristic translation score .
Our unsmoothed switch grammar decoding setup improves on the baseline by a considerable difference of 0.7 BLEU .
Subsequently , when adding the smoothing lexical translation features , both systems record a significant increase in performance , reaching comparable levels of performance .
The degenerate behaviour of MLE for SCFGs can be greatly limited by constraining ourselves to grammars employing minimal phrase -pairs ; phrase - pairs which cannot be further broken down into smaller ones according to the wordalignment .
One could argue that it is enough to perform plain MLE with such minimal phrase - pair SCFGs , instead of using our more elaborate learning algorithm with phrase - pairs of all lengths .
To investigate this , for our final experiment we used a plain MLE estimate of the switch grammar to translate , limiting the grammar 's phrase - pair emission rules to only those which involve minimal phrase-pairs .
The very low score of 17.82 BLEU ( without lexical smoothing ) not only highlights the performance gains of using longer phrase -pairs in hierarchical translation models , but most importantly provides a strong incentive to address the overfitting behaviour of MLE estimators for such models , instead of avoiding it .
Related work Most learning of phrase- based models , e.g. , ( Marcu and Wong , 2002 ; DeNero et al. , 2006 ; Mylonakis and Sima'an , 2008 ) , works without hierarchical components ( i.e. , not based on the explicit learning of an SCFG / BITG ) .
These learning problems pose other kinds of learning challenges than the ones posed by explicit learning of SCFGs .
Chiang 's original work ( Chiang , 2007 ) is also related .
Yet , the learning problem is not expressed in terms of an explicit objective function because surface heuristic counts are used .
It has been very difficult to match the performance of Chiang 's model without use of these heuristic counts .
A somewhat related work , ( Blunsom et al. , 2008 b ) , attempts learning new non-terminal labels for synchronous productions in order to improve translation .
This work differs substantially from our work because it employs a heuristic estimate for the phrase pair probabilities , thereby concentrating on a different learning problem : that of refining the grammar symbols .
Our approach might also benefit from such a refinement but we do not attempt this problem here .
In contrast , ( Blunsom et al. , 2008a ) works with the expanded phrase pair set of ( Chiang , 2005 ) , formulating an exponential model and concentrating on marginalising out the latent segmentation variables .
Again , the learning problem is rather different from ours .
Similarly , the work in ( Zhang et al. , 2008 ) reports on a multistage model , without a latent segmentation variable , but with a strong prior preferring sparse estimates embedded in a Variational Bayes ( VB ) estimator .
This work concentrates the efforts on pruning both the space of phrase pairs and the space of ( ITG ) analyses .
To the best of our knowledge , this work is the first to attempt learning probabilistic phrase - based BITGs as translation models in a setting where both a phrase segmentation component and a hierarchical reordering component are assumed latent variables .
Like this work , ( Mylonakis and Sima'an , 2008 ; DeNero et al. , 2008 ) also employ an all- phrases model .
Our paper shows that it is possible to train such huge grammars under iterative schemes like CV - EM , without need for sampling or pruning .
At the surface of it , our CV - EM estimator is also a kind of Bayesian learner , but in reality it is a more specific form of regularisation , similar to smoothing techniques used in language modelling ( Chen and Goodman , 1998 ; Mackay and Petoy , 1995 ) .
Discussion and Future Research Phrase- based stochastic SCFGs provide a rich formalism to express translation phenomena , which has been shown to offer competitive performance in practice .
Since learning SCFGs for machine translation has proven notoriously difficult , most successful SCFG models for SMT rely on rules extracted from word-alignment patterns and heuristically computed rule scores , with the impact and the limits imposed by these choices yet unknown .
Some of the reasons behind the challenges of SCFG learning can be traced back to the introduction of latent variables at different , competing levels : word and phrase - alignment as well as hierarchical reordering structure , with larger phrasepairs reducing the need for extensive reordering structure and vice versa .
While imposing priors such as the often used Dirichlet distribution or the Dirichlet Process provides a method to overcome these pitfalls , we believe that the data-driven regularisation employed in this work provides an effective alternative to them , focusing more on the data instead of importing generic external human knowledge .
We believe that this work makes a significant step towards learning synchronous grammars for SMT .
This is an objective not only worthy because of promises of increased performance , but , most importantly , also by increasing the depth of our understanding on SCFGs as vehicles of latent translation structures .
Our usage of the induced grammars directly for translation , instead of an intermediate task such as phrase -alignment , aims exactly at this .
While the latent structures that we explored in this paper were relatively simple in comparison with Hiero- like SCFGs , they take a different , content- driven approach on learning reordering preferences than the context-driven approach of Hiero .
We believe that these approaches are not merely orthogonal , but could also prove complementary .
Taking advantage of the possible synergies between content and context-driven reordering learning is an appealing direction of future research .
This is particularly promising for other language pairs , such as Chinese to English , where Hiero- like grammars have been shown to perform particularly well .
INPUT : Word-aligned parallel training data D Grammar extractor G
The number of parts k to partition D OUTPUT : SCFG
G with rule probabilities p Partition training data D into parts H 1 , . . . , H k .
Figure 3 : 3 Figure 3 : Lexicalised -Reordering SCFG
Table 1 presents the translation performance results of our systems and the baseline .
On first System Lexical Smoothing BLEU joshua- baseline
No 27.79 plain scfg No 28.04 switch scfg No 28.48 joshua- baseline Yes 29.96 plain scfg Yes 29.75 switch scfg Yes 29.88 Table 1 : Empirical results , with and without addi- tional lexical translation smoothing features dur - ing decoding observation , it is evident that our learning algo - rithm outputs stochastic SCFGs which manage to generalise , avoiding the degenerate behaviour of plain MLE training for these models .
Given the notoriety of the estimation process , this is note- worthy on its own .
Having a learning algorithm at hand which realises in a reasonable extent the potential of each stochastic grammar design ( as implemented in the relevant grammar extractors ) , we can now compare between the two grammar extractors used in our experiments .
The results table highlights the importance of conditioning the reordering process on lexical grounds .
The plain grammar with the single phrase - pair non- terminal cannot accomplish this and achieves a lower BLEU score .
On the other hand , the switch SCFG allows such conditioning .
The learner takes advantage of this feature to output a grammar which performs better in taking reordering deci - sions , something that is reflected in both the actual translations as well as the BLEU score achieved .
The sum of productions that have the same left - hand label must be one .
