title
Bilingual Word Embeddings for Phrase -Based Machine Translation
abstract
We introduce bilingual word embeddings : semantic embeddings associated across two languages in the context of neural language models .
We propose a method to learn bilingual embeddings from a large unlabeled corpus , while utilizing MT word alignments to constrain translational equivalence .
The new embeddings significantly out -perform baselines in word semantic similarity .
A single semantic similarity feature induced with bilingual embeddings adds near half a BLEU point to the results of NIST08 Chinese - English machine translation task .
Introduction
It is difficult to recognize and quantify semantic similarities across languages .
The Fr- En phrase - pair { ' un cas de force majeure ' , ' case of absolute necessity '} , Zh- En phrase pair {' ? ' , ' persist in a stubborn manner '} are similar in semantics .
If cooccurrences of exact word combinations are rare in the training parallel text , it can be difficult for classical statistical MT methods to identify this similarity , or produce a reasonable translation given the source phrase .
We introduce an unsupervised neural model to learn bilingual semantic embedding for words across two languages .
As an extension to their monolingual counter - part ( Turian et al. , 2010 ; Huang et al. , 2012 ; Bengio et al. , 2003 ) , bilingual embeddings capture not only semantic information of monolingual words , but also semantic relationships across different languages .
This prop-erty allows them to define semantic similarity metrics across phrase -pairs , making them perfect features for machine translation .
To learn bilingual embeddings , we use a new objective function which embodies both monolingual semantics and bilingual translation equivalence .
The latter utilizes word alignments , a natural sub-task in the machine translation pipeline .
Through largescale curriculum training ( Bengio et al. , 2009 ) , we obtain bilingual distributed representations which lie in the same feature space .
Embeddings of direct translations overlap , and semantic relationships across bilingual embeddings were further improved through unsupervised learning on a large unlabeled corpus .
Consequently , we produce for the research community a first set of Mandarin Chinese word embeddings with 100,000 words trained on the Chinese Gigaword corpus .
We evaluate these embedding on Chinese word semantic similarity from SemEval - 2012 ( Jin and Wu , 2012 ) .
The embeddings significantly out -perform prior work and pruned tf - idf base-lines .
In addition , the learned embeddings give rise to 0.11 F1 improvement in Named Entity Recognition on the OntoNotes dataset ( Hovy et al. , 2006 ) with a neural network model .
We apply the bilingual embeddings in an end-toend phrase - based MT system by computing semantic similarities between phrase pairs .
On NIST08 Chinese-English translation task , we obtain an improvement of 0.48 BLEU from a competitive baseline ( 30.01 BLEU to 30.49 BLEU ) with the Stanford Phrasal MT system .
Review of prior work Distributed word representations are useful in NLP applications such as information retrieval ( Pas ?ca et al. , 2006 ; , search query expansions ( Jones et al. , 2006 ) , or representing semantics of words ( Reisinger et al. , 2010 ) .
A number of methods have been explored to train and apply word embeddings using continuous models for language .
Collobert et al. ( 2008 ) learn embeddings in an unsupervised manner through a contrastive estimation technique .
Mnih and Hinton ( 2008 ) , Morin and Bengio ( 2005 ) proposed efficient hierarchical continuous -space models .
To systematically compare embeddings , Turian et al . ( 2010 ) evaluated improvements they bring to state - of- theart NLP benchmarks .
Huang et al. ( 2012 ) introduced global document context and multiple word prototypes .
Recently , morphology is explored to learn better word representations through Recursive Neural Networks ( Luong et al. , 2013 ) .
Bilingual word representations have been explored with hand -designed vector space models ( Peirsman and Pad ? , 2010 ; Sumita , 2000 ) , and with unsupervised algorithms such as LDA and LSA ( Boyd - Graber and Resnik , 2010 ; Tam et al. , 2007 ; Zhao and Xing , 2006 ) .
Only recently have continuous space models been applied to machine translation ( Le et al. , 2012 ) .
Despite growing interest in these models , little work has been done along the same lines to train bilingual distributioned word represenations to improve machine translation .
In this paper , we learn bilingual word embeddings which achieve competitive performance on semantic word similarity , and apply them in a practical phrase - based MT system .
Algorithm and methods
Unsupervised training with global context
Our method starts with embedding learning formulations in Collobert et al . ( 2008 ) .
Given a context window c in a document d , the optimization minimizes the following Context Objective for a word w in the vocabulary : J ( c , d ) CO = w r ?V R max ( 0 , 1 ? f ( c w , d ) + f ( c w r , d ) ) ( 1 )
Here f is a function defined by a neural network .
w r is a word chosen in a random subset V R of the vocabulary , and c w r is the context window containing word w r .
This unsupervised objective function contrasts the score between when the correct word is placed in context with when a random word is placed in the same context .
We incorporate the global context information as in Huang et al . ( 2012 ) , shown to improve performance of word embeddings .
Bilingual initialization and training
In the joint semantic space of words across two languages , the Chinese word ' ? ' is expected to be close to its English translation ' government ' .
At the same time , when two words are not direct translations , e.g. ' lake ' and the Chinese word ' ? ' ( deep pond ) , their semantic proximity could be correctly quantified .
We describe in the next sub-sections the methods to intialize and train bilingual embeddings .
These methods ensure that bilingual embeddings retain their translational equivalence while their distributional semantics are improved during online training with a monolingual corpus .
Initialization by MT alignments First , we use MT Alignment counts as weighting to initialize Chinese word embeddings .
In our experiments , we use MT word alignments extracted with the Berkeley Aligner ( Liang et al. , 2006 ) 1 . Specifically , we use the following equation to compute starting word embeddings : W t-init = S s=1 C ts + 1 C t + S W s ( 2 ) In this equation , S is the number of possible target language words that are aligned with the source word .
C ts denotes the number of times when word t in the target and word s in the source are aligned in the training parallel text ; C t denotes the total number of counts of word t that appeared in the target language .
Finally , Laplace smoothing is applied to this weighting function .
Single-prototype English embeddings by Huang et al. ( 2012 ) are used to initialize Chinese embeddings .
The initialization readily provides a set ( Align -Init ) of benchmark embeddings in experiments ( Section 4 ) , and ensures translation equivalence in the embeddings at start of training .
Bilingual training Using the alignment counts , we form alignment matrices A en?zh and A zh?en .
For A en?zh , each row corresponds to a Chinese word , and each column an English word .
An element a ij is first assigned the counts of when the ith Chinese word is aligned with the jth English word in parallel text .
After assignments , each row is normalized such that it sums to one .
The matrix A zh?en is defined similarly .
Denote the set of Chinese word embeddings as V zh , with each row a word embedding , and the set of English word embeddings as V en .
With the two alignment matrices , we define the Translation Equivalence Objective : J T EO-en?zh = V zh ?
A en?zh V en 2 ( 3 ) J T EO - zh ?en = V en ?
A zh?en V zh 2 ( 4 ) We optimize for a combined objective during training .
For the Chinese embeddings we optimize for : J CO-zh + ?J T EO-en?zh ( 5 ) For the English embeddings we optimize for : J CO-en + ?J T EO- zh?en ( 6 ) During bilingual training , we chose the value of ? such that convergence is achieved for both J CO and J T EO .
A small validation set of word similarities from ( Jin and Wu , 2012 ) is used to ensure the embeddings have reasonable semantics .
2
In the next sections , ' bilingual trained ' embeddings refer to those initialized with MT alignments and trained with the objective defined by Equation 5 .
' Monolingual trained ' embeddings refer to those intialized by alignment but trained without J T EO-en?zh . 2
In our experiments , ? = 50 .
Curriculum training
We train 100k - vocabulary word embeddings using curriculum training ( Turian et al. , 2010 ) with Equation 5 .
For each curriculum , we sort the vocabulary by frequency and segment the vocabulary by a band- size taken from { 5 k , 10k , 25 k , 50 k} .
Separate bands of the vocabulary are trained in parallel using minibatch L-BFGS on the Chinese Gigaword corpus 3 .
We train 100,000 iterations for each curriculum , and the entire 100k vocabulary is trained for 500,000 iterations .
The process takes approximately 19 days on a eight-core machine .
We show visualization of learned embeddings overlaid with English in Figure 1 .
The two -dimensional vectors for this visualization is obtained with t-SNE ( van der Maaten and .
To make the figure comprehensible , subsets of Chinese words are provided with reference translations in boxes with green borders .
Words across the two languages are positioned by the semantic relationships implied by their embeddings .
Experiments
Semantic Similarity
We evaluate the Mandarin Chinese embeddings with the semantic similarity test-set provided by the or - 1 . We show two evaluation metrics : Spearman Correlation and Kendall's Tau .
For both , bilingual embeddings trained with the combined objective defined by Equation 5 perform best .
For pruned tf -idf , we follow Reisinger et al .
( 2010 ; Huang et al. ( 2012 ) and count word co-occurrences in a 10 word window .
We use the best results from a range of pruning and feature thresholds to compare against our method .
The bilingual and monolingual trained embeddings 4 out-perform pruned tf -idf by 14.1 and 12.6 Spearman Correlation ( ? 100 ) , respectively .
Further , they out -perform embeddings initialized from alignment by 7.9 and 6.4 .
Both our tf -idf implementation and the word embeddings have significantly higher Kendall 's
Tau value compared to Prior work ( Jin and Wu , 2012 ) .
We verified Tau calculations with original submissions provided by the authors .
Named Entity Recognition perform NER experiments on OntoNotes ( v4.0 ) ( Hovy et al. , 2006 ) to validate the quality of the Chinese word embeddings .
Our experimental setup is the same as .
With embeddings , we build a naive feed -forward neural network ( Collobert et al. , 2008 ) with 2000 hidden neurons and a sliding window of five words .
This naive setting , without sequence modeling or sophisticated join optimization , is not competitive with state- ofthe - art .
Table 2 shows that the bilingual embeddings obtains 0.11 F1 improvement , lagging monolingual , but significantly better than Align -Init ( as in Section3.2.1 ) on the NER task .
Vector matching alignment Translation equivalence of the bilingual embeddings is evaluated by naive word alignment to match word embeddings by cosine distance .
5 The Alignment Error Rates ( AER ) reported in Table 3 suggest that bilingual training using Equation 5 produces embeddings with better translation equivalence compared to those produced by monolingual training .
Phrase - based machine translation
Our experiments are performed using the Stanford Phrasal phrase - based machine translation system .
In addition to NIST08 training data , we perform phrase extraction , filtering and phrase table learning with additional data from GALE MT evaluations in the past 5 years .
In turn , our baseline is established at 30.01 BLEU and reasonably competitive relative to NIST08 results .
We use Minimum Error Rate Training ( MERT ) ( Och , 2003 ) to tune the decoder .
In the phrase - based MT system , we add one feature to bilingual phrase-pairs .
For each phrase , the word embeddings are averaged to obtain a feature vector .
If a word is not found in the vocabulary , we disregard and assume it is not in the phrase ; if no word is found in a phrase , a zero vector is assigned to it .
We then compute the cosine distance between the feature vectors of a phrase pair to form a semantic similarity feature for the decoder .
Results on NIST08 Chinese-English translation task are reported in Table 4 6 .
An increase of 0.48 BLEU is obtained with semantic similarity with bilingual embeddings .
The increase is modest , just surpassing a reference standard deviation 0.29 BLEU Cer et al . ( 2010 ) 7 evaluated on a similar system .
We intend to publish further analysis on statistical significance of this result as an appendix .
From these suggestive evidence in the MT results , random initialized monolingual trained embeddings add little gains to the baseline .
Bilingual initialization and training seem to be offering relatively more consistent gains by introducing translational equivalence .
Conclusion
In this paper , we introduce bilingual word embeddings through initialization and optimization constraint using MT alignments
The embeddings are learned through curriculum training on the Chinese Gigaword corpus .
We show good performance on Chinese semantic similarity with bilingual trained embeddings .
When used to compute semantic similarity of phrase pairs , bilingual embeddings improve NIST08 end-to - end machine translation results by just below half a BLEU point .
This implies that semantic embeddings are useful features for improving MT systems .
Further , our results offer suggestive evidence that bilingual word embeddings act as high-quality semantic features and embody bilingual translation equivalence across languages .
Figure 1 : 1 Figure 1 : Overlaid bilingual embeddings : English words are plotted in yellow boxes , and Chinese words in green ; reference translations to English are provided in boxes with green borders directly below the original word .
