title
CUNI Systems for the Unsupervised and Very Low Resource Translation Task in WMT20
abstract
This paper presents a description of CUNI systems submitted to the WMT20 task on unsupervised and very low-resource supervised machine translation between German and Upper Sorbian .
We experimented with training on synthetic data and pre-training on a related language pair .
In the fully unsupervised scenario , we achieved 25.5 and 23.7 BLEU translating from and into Upper Sorbian , respectively .
Our low-resource systems relied on transfer learning from German - Czech parallel data and achieved 57.4 BLEU and 56.1 BLEU , which is an improvement of 10 BLEU points over the baseline trained only on the available small German - Upper Sorbian parallel corpus .
Introduction
An extensive area of the machine translation ( MT ) research focuses on training translation systems without large parallel data resources ( Artetxe et al. , 2018 b ( Artetxe et al. , , a , 2019 Lample et al. , 2018 a , b) .
The WMT20 translation competition presents a separate task on unsupervised and very low-resource supervised MT .
The organizers prepared a shared task to explore machine translation on a real-life example of a lowresource language pair of German ( de ) and Upper Sorbian ( hsb ) .
There are around 60 k authentic parallel sentences available for this language pair which is not sufficient to train a high-quality MT system in a standard supervised way , and calls for unsupervised pre-training ( Conneau and Lample , 2019 ) , data augmentation by synthetically produced sentences ( Sennrich et al. , 2016a ) or transfer learning from different language pairs ( Zoph et al. , 2016a ; Kocmi and Bojar , 2018 ) .
The WMT20 shared task is divided into two tracks .
In the unsupervised track , the participants are only allowed to use monolingual German and Upper Sorbian corpora to train their systems ; the low-resource track permits the usage of auxiliary parallel corpora in other languages as well as a small parallel corpus in German - Upper Sorbian .
We participate in both tracks in both translation directions .
Section 2 describes our participation in the unsupervised track and section 3 describes our systems from the low-resource track .
Section 4 introduces transfer learning via Czech ( cs ) into our low-resource system .
We conclude the paper in section 5 .
Unsupervised MT Unsupervised machine translation is the task of learning to translate without any parallel data resources at training time .
Both neural and phrasebased systems were proposed to solve the task ( Lample et al. , 2018 b ) .
In this work , we train several neural systems and compare the effects of different training approaches .
Methodology
The key concepts of unsupervised NMT include a shared encoder , shared vocabulary and model initialization ( pre-training ) .
The training relies only on monolingual corpora and switches between de-noising , where the model learns to reconstruct corrupted sentences , and online back - translation , where the model first translates a batch of sentences and immediately trains itself on the generated sentence pairs , using the standard cross-entropy MT objective ( Artetxe et al. , 2018 b ; Lample et al. , 2018a ) .
We use a 6 - layer Transformer architecture for our unsupervised NMT models following the approach by Conneau and Lample ( 2019 ) .
Both the encoder and the decoder are shared across languages .
We first pre-train the encoder and the decoder separately on the task of cross-lingual masked language modelling ( XLM ) using monolingual data only ( Conneau and Lample , 2019 ) .
Subsequently , the initialized MT system ( CUNI - Monolingual ) is trained using de-noising and online back - translation .
We then use this system to translate our entire monolingual corpus and train a new system ( CUNI - Synthetic - I ) from scratch on the two newly generated synthetic parallel corpora DE - HSB synth1 and HSB - DE synth1 .
Finally , we use the new system to generate DE - HSB synth2 and HSB - DE synth2 , and repeat the training to evaluate the effect of another back - translation round ( CUNI - Synthetic - II ) .
All unsupervised systems are trained using the same BPE subword vocabulary ( Sennrich et al. , 2016 b ) with 61 k items generated using fastBPE .
1
An overview of the systems and their training stages is given in fig .
1 .
Data
Our de training data comes from News Crawl ; the hsb data was provided for WMT20 by the Sorbian Institute and the Witaj Sprachzentrum .
2 Most of the hsb data was of high quality but we fed the web-scraped corpus ( web monolingual.hsb ) through a language identification tool fastText 3 to identify proper hsb sentences .
All de data was also cleaned using this tool .
The final monolingual training corpora have 22.5 M sentences ( DE mono ) and 0.6 M sentences ( HSB mono ) .
Synthetic parallel corpora are generated from the monolingual data sets by coupling the sentences with their translation counterparts as described in section 2.1 .
The parallel development ( dev ) and testing ( dev test ) data sets of 2 k sentence pairs provided by WMT20 organizers are used for parameter tuning and model selection .
The final evaluation is run on the blind test set newstest 2020 .
Results
The resulting scores measured on the blind new-stest2020 are listed in table 1 and table 2 .
The translation quality metrics BLEU ( Papineni et al. , 2002 ) , TER ( Snover et al. , 2006 ) , BEER ( Stanojevi ? and Sima'an , 2014 ) and CharacTER ( Wang et al. , 2016 ) provide consistent results .
The best quality is reached when using synthetic corpora from the second back - translation iteration , although the second round adds only a slight improvement .
A similar observation is made by Hoang et al . ( 2018 ) who show that the second round of back - translation does not enhance the system performance as much as the first round .
Additionally , the third round does not produce any significant gains .
When training on synthetic parallel corpora , it is still beneficial to perform back - translation on -thefly ( Artetxe et al. , 2018 b ) only on sentence pairs from the two synthetic corpora so we use it in all our unsupervised systems .
We used the XLM 4 toolkit for running the experiments .
Language model pre-training took 4 days on 4 GPUs 5 .
The translation models were trained on 1 GPU 6 with 8 - step gradient accumulation to reach an effective batch size of 8 ? 3400 tokens .
We used the Adam ( Kingma and Ba , 2015 ) optimizer with inverse square root decay ( ?
1 = 0.9 , ? 2 = 0.98 , lr = 0.0001 ) and greedy decoding .
3 Very Low-Resource Supervised MT
Methodology
Our systems introduced in this section have the same model architecture as described in section 2 , but now we allow the usage of authentic parallel data .
We pre-train a bilingual XLM model and finetune with either only authentic parallel data ( CUNI - Auth - w\o - BT ) or both parallel and monolingual data , using a combination of standard MT training and online back - translation ( CUNI - Auth -w\ - BT ) .
Finally , we utilize the trained model CUNI - Synthetic - II from section 2 and fine-tune it on the authentic parallel corpus , again using standard supervised training as well as online back- translation 4 https://github.com/facebookresearch/ XLM 5 GeForce GTX 1080 , 11 GB of RAM 6 Quadro P5000 , 16 GB of RAM ( CUNI - Synth + Authentic ) .
All systems are trained with the same BPE subword vocabulary of 61 k items .
Data
In addition to the data described in section 2.2 , we used the authentic parallel corpus of 60 k sentence pairs provided by Witaj Sprachzentrum mostly from the legal and general domain .
Results
The resulting scores are listed in the second part of table 1 and table 2 .
We compare system performance against a supervised baseline which is a vanilla NMT model trained only on the small parallel train set of 60 k sentences , without any pretraining or data augmentation .
Our best system gains 11.5 BLEU over this baseline , utilizing the larger monolingual corpora for XLM pre-training and online back - translation .
Fine-tuning one of the trained unsupervised systems on parallel data leads to a lower gain of ?10 BLEU points over the baseline .
The translation models were trained on 1 GPU 7 with 8 - step gradient accumulation to reach an effective batch size of 8 ? 1600 tokens .
Other training details are equivalent to section 2.1 .
4 Very Low-Resource Supervised MT with Transfer Learning
One of the main approaches to improving performance under low-resource conditions is transferring knowledge from different high- resource language pairs ( Zoph et al. , 2016 b ; Kocmi and Bojar , 2018 ) .
This section describes the unmodified strategy for transfer learning as presented by Kocmi and Bojar ( 2018 ) , using German - Czech as the parent language pair .
Since we do not modify the approach nor tune hyperparameters of the NMT model , we consider our system a transfer learning baseline for low-resource supervised machine translation .
Methodology Kocmi and Bojar ( 2018 ) proposed an approach to fine - tune a low-resource language pair ( called " child " ) from a pre-trained high- resource language pair ( called " parent " ) model .
The method has only one restriction and that is a shared subword vocabulary generated from the corpora of both the child and the parent .
The training procedure is as follows : first train an NMT model on the parent parallel corpus until it converges , then replace the training data with the child corpus .
We use the Tensor2Tensor framework ( Vaswani et al. , 2018 ) for our transfer learning baseline and model parameters " Transformer - big " as described in ( Vaswani et al. , 2018 ) .
Our shared vocabulary has 32 k wordpiece tokens .
We use the Adafactor ( Shazeer and Stern , 2018 ) optimizer and a reverse square root decay with 16 000 warm - up steps .
For the inference , we use beam search of size 8 and alpha 0.8 .
Data
In addition to the data described in section 3.2 , we used the cs-de parallel corpora available at the OPUS 8 website : OpenSubtitles , MultiParaCrawl , Europarl , EUBookshop , DGT , EMEA and JRC .
The cs-de corpus has 21.4 M sentence pairs after cleaning with the fastText language identification tool .
Results
We compare the results of our transfer learning baseline called CUNI - Transfer with three top performing systems of WMT20 .
These systems use state - of - the - art techniques such as BPE - dropout , ensembling of models , cross-lingual language modelling , filtering of training data and hyperparameter tuning .
Additionally , we also include results for a system we trained without any modifications solely on bilingual parallel data ( Bilingual only ) .
9
The results in table 4 show that training solely on German - Upper Sorbian parallel data leads to a performance of 47.8 BLEU for de?hsb and 46.7 BLEU for hsb?de .
When using transfer learning with a Czech - German parent , the performance increases by roughly 10 BLEU points to 57.4 and 56.1 BLEU .
As demonstrated by the winning system , the performance can be further boosted using additional techniques and approaches to 60.0 and 61.1 BLEU .
This shows that transfer learning plays an important role in the low-resource scenario .
Conclusion
We participated in the unsupervised and lowresource supervised translation task of WMT20 .
In the fully unsupervised scenario , the best scores of 25.5 ( hsb?de ) and 23.7 ( de?hsb ) were achieved using cross-lingual language model pre-training ( XLM ) and training on synthetic data produced by NMT models from earlier two iterations .
We submitted this system under the name CUNI - Synthetic -II .
In the low-resource supervised scenario , the best scores of 57.4 ( hsb?de ) and 56.1 ( de?hsb ) were achieved by pre-training on a large German - Czech parallel corpus and fine-tuning on the available German - Upper Sorbian parallel corpus .
We submitted this system under the name CUNI - Transfer .
We showed that transfer learning plays an important role in the low-resource scenario , bringing an improvement of ?10 BLEU points over a vanilla supervised MT model trained on the small parallel data only .
Additional techniques used by other competing teams yield further improvements of up to 4 BLEU over our transfer learning baseline .
Figure 1 : 1 Figure 1 : An overview of selected CUNI systems .
Corpora are illustrated in gray boxes , system names in black boxes .
Systems are trained with indicated training objectives : cross-lingual masked language modeling ( XLM ) , denoising ( DN ) , online back - translation ( BT ) , and standard machine translation objective ( MT ) .
Monolingual training sets DE mono and HSB mono were available for both WMT20 task tracks , the parallel training set HSB ?
DE auth was only allowed in the low-resource supervised track .
