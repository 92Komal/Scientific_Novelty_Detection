title
Towards Kurdish Text to Sign Translation
abstract
The resources and technologies for sign language processing of resourceful languages are emerging , while the low-resource languages are falling behind .
Kurdish is a multi-dialect language , and it is considered a low-resource language .
It is spoken by approximately 30 million people in several countries , which denotes that it has a large community with hearing - impairments as well .
This paper reports on a project which aims to develop the necessary data and tools to process the sign language for Sorani as one of the spoken Kurdish dialects .
We present the results of developing a dataset in HamNoSys and its corresponding SiGML form for the Kurdish Sign lexicon .
We use this dataset to implement a sign-supported Kurdish tool to check the accuracy of the sign lexicon .
We tested the tool by presenting it to hearing -impaired individuals .
The experiment showed that 100 % of the translated letters were understandable by a hearing -impaired person .
The percentages were 65 % for isolated words , and approximately 30 % for the words in sentences .
The data is publicly available at https://github.com/KurdishBLARK/KurdishSignLanguage for non-commercial use under the CC BY -NC -SA 4.0 licence .
Introduction
The studies on sign language processing have been emerging , but many areas are still unexplored ( Cormier et al. , 2019 ) .
As might be expected , this area of research has even not been initiated yet for many under-resourced languages .
Kurdish , a multi-dialect language which is spoken by approximately 30 million people in different countries , is considered an under-resourced language ( Hassani , 2018 ) .
It is also written in different scripts .
The usage of the scripts changes according to the geographical situation ( Hassani and Medjedovic , 2016 ) .
The current literature does not report on visible research on Kurdish Sign Language ( KuSL ) processing , nor are there any publicly available resources for this topic .
This research focuses on text to sign conversion for the Sorani dialect of Kurdish .
Sign language is the main communication method among the hearing -impaired community .
This language is based on visual interaction rather than using sound .
The interactions happen by manual and nonmanual signs and finger spelling ( Cooper et al. , 2011 ) .
Hand and body movement , shape , orientation and location are within manual signs ( Kelly et al. , 2009 ) , while facial expressions , eye gaze , and shoulder movement are called non-manual signs ( Halawani , 2008 ) .
Furthermore , the finger spelling is used to spell letters of certain words , for example , names and technical terms that do not have sign equivalents ( Liwicki and Everingham , 2009 ) .
Normally , the communication between two hearingimpaired persons is smooth and understandable .
The real challenge begins when a hearing person wants to interact with a hearing -impaired person ( Wazalwar and Shrawankar , 2017 ) .
Generally , if the target hearing - impaired person is educated , they try to communicate by exchanging written texts .
Otherwise , they turn to a human sign language interpreter as a recourse if available , or else perhaps they end up with serious miscommunication ( Wazalwar and Shrawankar , 2017 ) .
Although the spoken Kurdish dialects use different lexicons ( Ahmadi et al. , 2019 ) , the Kurdish Sign language , which is used in the Kurdistan Region of Iraq ( KRI ) , uses the same lexicon among the hearingimpaired community regardless of the spoken dialect .
While according to Jepsen et al . ( 2015 ) KuSL is not standardized , applying guidelines by Mohammed ( 2007 ) and using the Kurdish Sign dictionaries ( Nashat Salim et al. , 2013 ; Ghazi Dizayee , 2000 ) in the KRI education programs show some efforts towards KuSL standardization .
We develop a Kurdish Sign lexicon using the Kurdish Sign Language Dictionary ( KuSLD ) ( Ghazi Dizayee , 2000 ) , which is used in KRI .
Currently , no Kurdish Sign corpus is available , hence we aim at making Sorani texts sign-supported .
That is , in the text conversion process we follow the spoken language and not the sign language structure .
Sorani texts are mostly written in Persian - Arabic script ( Hassani , 2018 ) hence we use the developed Kurdish Sign lexicon to make this type of the Sorani texts signsupported .
The rest of this paper is organized as follows .
Section 2 . provides a brief background on sign language processing , Section 3 . reviews the related work , Section 4 . presents our approach , Section 5 . illustrates the developed dataset , Section 6 . discusses the results , finally , Section 7 . concludes the paper .
Sign Language Processing Sign languages are considered as genuine languages that place them among the minority languages ( Senghas and Monaghan , 2002 ) .
Since sign languages consist of visual gestures rather than voice as it is in spoken languages .
The analysis and feature extraction of the former significantly differ from latter languages .
However , for some languages , a variant of sign language also exists that follows the spoken / written language grammar , which is called sign-supported language ( Elliott et al. , 2008 ) .
The development of this variant is less challenging in the absence of required sign corpora and language models .
The outcome could be used in various experimental and real-life occasions .
Several approaches exist to process sign languages .
In the following sections , we discuss those approaches which are more related to our current stage of research .
Notation Systems
The sign visual gestures are normally denoted by special notations in order to be able to process them .
Different notation systems are used to capture these gestures .
The most popular ones are Stokoe , SignWriting , and HamNoSys .
Stokoe was one of the earliest attempts for a sign language notation system ( McCarty , 2004 ) .
However , it was only concerned with manual sign representation , and it lacked any consideration for non-manual signs , such as eye gaze and shoulders movements , which are an essential entity to convey meaning by facial expression .
SignWriting represents the signed gestures spatially in a 2D canvas ( Bouzid and Jemni , 2013 b ) .
It is designed to facilitate communication among the hearingimpaired community .
HamNoSys ( Hamburg Notation System ) is a phonetic translation system with iconicity , extensibility , and formal syntax characteristics used to denote sign languages ( Hanke , 2004 ) .
A comparative analysis by ( Dhanjal and Singh , 2019 ) concluded that HamNoSys is the most widely used notation system for a variety of sign languages .
Ham - NoSys symbols are available as a Unicode font ( Hanke , 2004 ) .
This Unicode font symbolizes manual sign gestures and allows the generation of the signs by dividing the description into the handshapes , orientations , locations , and actions .
Markup Languages
To provide computer encoding for sign languages and to make their processing more efficient , several adoptions of the Extensive Markup Language ( XML ) have been suggested based on various sign notation systems .
The Sign Writing Markup Language ( SWML ) is a markup language proposed by da Rocha Costa and Dimuro ( 2001 ) based on SignWriting .
HamNoSys uses Signing Gesture Markup Language ( SiGML ) , which gives a special XML tag to each Ham - NoSys symbol .
These markup languages are used in different applications , for instance , to be given to a 3D avatar to animate the signs .
Related Work
The only work on Kurdish Sign language processing that we were able to retrieve was by Hashim and Alizadeh ( 2018 ) wherein the researchers reported on their project on Kurdish Sign language recognition .
That project focused on the recognition of Kurdish manual alphabets .
Therefore , as literature does not report on active studies on Kurdish Sign language processing , we review the topic in the context of other languages .
Sugandhi and Kaur ( 2018 ) introduced an online multilingual dictionary for avatar- based Indian Sign language .
The system is designed to accept input from two languages English and Hindi .
The input is transliterated into Hindi and then goes through the parser to be translated into Indian Sign Language ( ISL ) .
After extracting the root words of the input script , the target Hamburg Notations are retrieved from the database and converted into its corresponding SiGML .
The generated SiGML is the input parameter for the Animation server , which uses Web Graphics Library ( We - bGL ) for the avatar representation .
Aouiti ( 2013 ) proposed an approach to convert Arabic text into Arabic Sign language .
The approach used an Arabic sentence / Sign language corpus as a core entity .
The corpus includes Arabic sentences that were aligned with their corresponding sign representation .
This helped to ensure that the represented sign refers to the real meaning of the input text .
Afterward , the target sentence was syntactically and semantically analyzed by applying techniques , such as Morphological , Syntactic , Semantic , and Pragmatic analysis , which led to the generation of the glosses .
The sign for each gloss was extracted from the corpus , which was sent to the avatar to be played .
Bouzid and Jemni ( 2013a ) developed an avatar-based system to enhance the usability and readability of notation systems for deaf people .
The system was developed using SignWriting ( SW ) notation and its markup language .
Their focus was to make the path easier for hearing - impaired people to understand and represent signs in a written format .
Since SW is presented in a 2D format and it is easy to guess the target gestures from the written notations , this helps hearing - impaired people to learn different sign languages depending on the SW notations .
SW is designed for daily communication purposes rather than linguistic and corpus development and processing .
An automated reading system for SignWriting representation of Brazilian Sign language was introduced by Stiehl et al . ( 2015 ) .
They focused on SignWriting of several Brazilian signs and classified the symbols into several categories .
Again , their purpose was to build a database of SignWriting representation for Brazilian Sign Language in order to involve hearing - impaired people into learning the notations and enable them to communicate with each other .
This approach can also be used to have books , newspapers , dictionaries and such that are written in notation symbols and can be understood by hearing - impaired people or sign learn-ers .
To summarize , we follow the approach of Sugandhi and Kaur ( 2018 ) because of two reasons .
First , because HamNoSys is a proper method for corpus development , and second because SignWiriting is majorly used for the communication among the hearing - impaired community and not between the hearing - impaired community and people with no hearing difficulties .
Method
We develop a dataset based on KuSLD .
We also prepare and adapt a tool to translate Sorani texts into Kurdish Sign language to be animated by an avatar .
We prepare the HamNoSys notations manually by analyzing the gestures available in KuSLD .
We use the Ham2HPSG and eSIGN ( Hanke and Popescu , 2003 ) to create the dataset and extract the SiGML codes .
We implement a sign-supported tool based on the architecture that is shown in Figure 1 .
In this architecture , the Language Model ( LM ) , in its current form , is the developed dataset , which could be considered as the Kurdish ( Sorani ) sign lexicon .
The input text goes through the tokenization process to extract the meaningful components from it .
Similar to the existing sign-supported tools for other languages , the translation is word - by - word for the words that are found in the LM .
Otherwise , the word will be replaced by a sequence of its letters in the sign language .
Then the whole text is compiled into SiGML files , which will be sent to an avatar to be animated .
We evaluate the tool by feeding it with input of four categories , namely alphabets , numbers , words , and sentences .
The tool then plays the translation to the human individuals who are either hearing -impaired or Kurdish Sign language educators .
As subjective understanding is not accurate ( Kipp et al. , 2011 ) , we ask the testers to write down their understanding .
We calculate the accuracy by the percentage of correctly understood cases for played alphabets , numbers , words , and sentences .
Developed Dataset
The KuSLD consists of 2315 different sign gestures from 38 different categories .
Our dataset , currently , consists of 20 % of each category .
However , we converted the alphabet and numbers completely .
This adds more entries to the dataset , which sums up to approximately 560 entries .
The KuSLD categories are listed in Table 1 . A sample for the prepared HamNoSys for Kurdish letters and words is shown in Figures 2 and 3 .
We extracted the generated SiGML for the corresponding HamNoSys dataset , which was sent to an avatar to be animated .
Two samples of extracted SiGML for the letter ? " ? " ?
( B ) and the word ? " ? " ?
( University ) in Kurdish are shown in Figures 4 and 5 .
Findings and Discussion
We played a sample of letters of the prepared dataset to the hearing - impaired individuals .
The test showed a 100 % understanding of the test data .
The results of playing words showed a 65 % correct understanding of the played words .
The accuracy of the tool for understanding sentences was approximately 30 % .
In the evaluation process , the person could recognize all shown letters successfully since they are clearly shown in the dictionary .
On the other hand , the signs for the words had a lower evaluation outcome .
The person could not understand some of the words .
One reason for this was the usage of two different sign dictionaries in KRI .
One of these dictionaries represents all signs based on the lexicon description , while the other ( Ghazi Dizayee , 2000 ) uses vocal description for some of its entries .
Our dataset was developed based on the latter .
Both dictionaries are used interchangeably , but they provide different representations for specific signs depending on the context where they appear .
This issue also affected sentence evaluation .
Also , since we used a word by word translation , the hearing - impaired person was unable to understand the meaning of a majority of the sentences as a whole .
Therefore , the sentence evaluation achieved low accuracy , which is typical for the sign-supported systems .
Conclusion
We used HamNoSys to develop a sign dataset and its equivalent SiGML for Kurdish .
We chose HamNoSys over SignWriting because of our plan to develop Kurdish Sign corpora in the future .
Our developed dataset includes approximately 560 entries consisting of the alphabet , numbers , and words .
We also implemented a tool to translate Sorani texts into the Kurdish Sign language , which could be animated by an avatar .
We evaluated the tool by showing the animated output to hearing - impaired persons on the three aspects of understanding the sign gestures , namely letters , words , and sentences .
The test showed a 100 % understanding for the letters , a 65 % for isolated words , and approxi-mately 30 % for sentences .
The main reasons for the low accuracy were the usage of more than one sign dictionary in the target community and the word - by - word translation of the input texts .
As future work , we are targeting the development of a language model based on the grammar of the Kurdish Sign language .
Additionally , we aim to add more entries to the developed dataset .
Furthermore , we would like to include other Kurdish dialects in the dataset .
