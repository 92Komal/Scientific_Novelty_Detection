title
Imitation Attacks and Defenses for Black - box Machine Translation Systems
abstract
Adversaries may look to steal or attack blackbox NLP systems , either for financial gain or to exploit model errors .
One setting of particular interest is machine translation ( MT ) , where models have high commercial value and errors can be costly .
We investigate possible exploitations of black - box MT systems and explore a preliminary defense against such threats .
We first show that MT systems can be stolen by querying them with monolingual sentences and training models to imitate their outputs .
Using simulated experiments , we demonstrate that MT model stealing is possible even when imitation models have different input data or architectures than their target models .
Applying these ideas , we train imitation models that reach within 0.6 BLEU of three production MT systems on both high- resource and low-resource language pairs .
We then leverage the similarity of our imitation models to transfer adversarial examples to the production systems .
We use gradient - based attacks that expose inputs which lead to semanticallyincorrect translations , dropped content , and vulgar model outputs .
To mitigate these vulnerabilities , we propose a defense that modifies translation outputs in order to misdirect the optimization of imitation models .
This defense degrades the adversary 's BLEU score and attack success rate at some cost in the defender 's BLEU and inference speed .
Transfer Solve Eq. ( 2 ) Save me it 's over 100 ?
F
Phase One : Model Imitation
Introduction NLP models deployed through APIs ( e.g. , Google Translate ) can be lucrative assets for an organization .
These models are typically the result of a considerable investment - up to millions of dollarsinto private data annotation and algorithmic improvements .
Consequently , such models are kept hidden behind black - box APIs to protect system integrity and intellectual property .
We consider an adversary looking to steal or attack a black - box NLP system .
Stealing a produc-tion model allows an adversary to avoid long-term API costs or launch a competitor service .
Moreover , attacking a system using adversarial examples ( Szegedy et al. , 2014 ) allows an adversary to cause targeted errors for a model , e.g. , bypassing fake news filters or causing systems to output malicious content that may offend users and reflect poorly on system providers .
In this work , we investigate these two exploits for black - box machine translation ( MT ) systems : we first steal ( we use " steal " following Tram ? r et al. 2016 ) production MT systems by training imitation models , and we then use these imitation models to generate adversarial examples for production MT systems .
We create imitation models by borrowing ideas from knowledge distillation ( Hinton et al. , 2014 ) : we query production MT systems with monolingual sentences and train imitation ( i.e. , student ) models to mimic the system outputs ( top of Figure 1 ) .
We first experiment with simulated studies which demonstrate that MT models are easy to imitate ( Section 3 ) .
For example , imitation models closely replicate the target model outputs even when they are trained using different architectures or on outof-domain queries .
Applying these ideas , we imitate production systems from Google , Bing , and Systran with high fidelity on English ?
German and Nepali ? English .
For example , Bing achieves 32.9 BLEU on WMT14 English ?
German and our imitation achieves 32.4 BLEU .
We then demonstrate that our imitation models aid adversarial attacks against production MT systems ( Section 4 ) .
In particular , the similarity of our imitation models to the production systems allows for direct transfer of adversarial examples obtained via gradient - based attacks .
We find small perturbations that cause targeted mistranslations ( e.g. , bottom of Figure 1 ) , nonsense inputs that produce malicious outputs , and universal phrases that cause mistranslations or dropped content .
The reason we identify vulnerabilities in NLP Figure 1 : Imitating and attacking an English ?
German MT system .
In phase one ( model imitation ) , we first select sentences from English corpora ( e.g. , Wikipedia ) , label them using the black - box API , and then train an imitation model on the resulting data .
In phase two ( adversarial attacks ) , we generate adversarial examples against our imitation model and transfer them to the production systems .
For example , we find an input perturbation that causes Google to produce a factually incorrect translation , see the link here ( all attacks work as of April 2020 ) .
systems is to robustly patch them .
To take steps towards this , we create a defense which finds alternate translations that cause the optimization of the imitation model to proceed in the wrong direction ( Section 5 ) .
These alternate translations degrade the imitation model 's BLEU score and the transfer rate of adversarial examples at some cost in the defender 's BLEU and inference speed .
How We Imitate MT Models
We have query access to the predictions ( but no probabilities or logits ) from a victim MT model .
This victim is a black box : we are unaware of its internals , e.g. , the model architecture , hyperparameters , or training data .
Our goal is to train an imitation model ( Orekondy et al. , 2019 ) that achieves comparable accuracy to this victim on held - out data .
Moreover , to enhance the transferability of adversarial examples , the imitation model should be functionally similar to the victim , i.e. , similar inputs translate to similar outputs .
Past Work on Distillation and Stealing
This problem setup is closely related to model distillation ( Hinton et al. , 2014 ) : training a student model to imitate the predictions of a teacher .
Distillation has widespread use in MT , including reducing architecture size ( Kim and Rush , 2016 ; Kim et al. , 2019 ) , creating multilingual models ( Tan et al. , 2019 ) , and improving non-autoregressive generation ( Ghazvininejad et al. , 2019 ; Stern et al. , 2019 ) .
Model stealing differs from distillation because the victim's ( i.e. , teacher 's ) training data is unknown .
This causes queries to typically be out-of- domain for the victim .
Moreover , because the victim 's output probabilities are unavailable for most APIs , imitation models cannot be trained using distribution matching losses such as KL divergence , as is common in distillation .
Despite these challenges , prior work shows that model stealing is possible for simple classification ( Lowd and Meek , 2005 ; Tram ? r et al. , 2016 ) , vision ( Orekondy et al. , 2019 ) , and language tasks ( Krishna et al. , 2020 ; Pal et al. , 2019 ) .
In particular , Pal et al . ( 2019 ) steal text classifiers and Krishna et al . ( 2020 ) steal reading comprehension and textual entailment models ; we extend these results to MT and investigate how model stealing works for production systems .
Our Approach
We assume access to a corpus of monolingual sentences .
We select sentences from this corpus , query the victim on each sentence , and obtain the associated translations .
We then train an imitation model on this " labeled " data .
Imitating Black- box MT Systems
We first study imitation models through simulated experiments : we train victim models , query them as if they are black boxes , and then train imitation 1 : Imitation models are highly similar to their victims .
We train imitation models that are different from their victims in input data and / or architecture .
We test the models on IWSLT ( Test ) and out - of- domain news data from WMT ( OOD ) .
We also measure functionality similarity by reporting the BLEU score between the outputs of the imitation and the victim models ( Inter ) .
models to mimic their outputs .
In Section 3.3 , we turn to imitating production systems .
Research Questions and Experiments
In practice , the adversary will not know the victim 's model architecture or source data .
We study the effect of this with the following experiments : ?
We use the same architecture , hyperparameters , and source data as the victim ( All Same ) .
Datasets
We consider German ?
English using the TED data from IWSLT 2014 ( Cettolo et al. , 2014 ) .
We follow common practice for IWSLT and report case-insensitive BLEU ( Papineni et 2002 ) .
For Data Different , we use English sentences from Europarl v7 .
The predictions from the victim are generated using greedy decoding .
We Closely Imitate Local Models Test BLEU Score
We first compare the imitation models to their victims using in - domain test BLEU .
For all settings , imitation models closely match their victims ( Test column in Table 1 ) .
We also evaluate the imitation models on OOD data to test how well they generalize compared to their victims .
We use the WMT14 test set ( newstest 2014 ) .
Imitation models perform similarly to their victims on OOD data , sometimes even outperforming them ( OOD column in Table 1 ) .
We suspect that imitation models can sometimes outperform their victims because distillation can act as a regularizer ( Furlanello et al. , 2018 ; Mobahi et al. , 2020 ) . Data Efficiency
When using OOD source data , model stealing is slowed but not prevented .
Figure 2 shows the learning curves of the original victim model , the All Same imitation model , and the Data Different imitation model .
Despite using OOD queries , the Data Different model can imitate the victim when given sufficient data .
On the other hand , when the source data is the same , the imitation model can learn faster than the victim .
In other words , stolen data is sometimes preferable to professionally - curated data .
This likely arises because model translations are simpler than human ones , which aids learning ( Zhou et al. , 2020 ) . Functional Similarity Finally , we measure the BLEU score between the outputs of the victim and the imitation models to measure their functional similarity ( henceforth inter-system BLEU ) .
As a reference for inter-system BLEU , two Transformer models trained with different random seeds achieve 62.1 inter-system BLEU .
The inter-system BLEU for the imitation models and their victims is as high as 70.5 ( Table 1 ) , i.e. , imitation models are more similar to their victims than two models which have been trained on the exact same dataset .
We Closely Imitate Production Models Given the effectiveness of our simulated experiments , we now imitate production systems from Google , Bing , and Systran .
Language Pairs and Data We consider two language pairs , English ?
German ( high- resource ) and the Nepali? English ( low- resource ) .
2
We collect training data for our imitation models by querying the production systems .
For English ?
German , we query the source side of the WMT14 training set ( ?
4.5 M sentences ) .
3 For Nepali ?
English , we query the Nepali Language Wikipedia ( ?
100,000 sentences ) and approximately two million sentences from Nepali common crawl .
We train Transformer Big ( Vaswani et al. , 2017 ) models on both datasets .
Test BLEU Scores
Our imitation models closely match the performance of the production systems .
For English ?
German , we evaluate models on the WMT14 test set ( newstest2014 ) and report standard tokenized case-sensitive BLEU scores .
Our imitation models are always within 0.6 BLEU of the production models ( Imitation in Table 2 ) .
For Nepali ?
English , we evaluate using FLoRes devtest ( Guzm ? n et al. , 2019 ) .
We compute BLEU scores using SacreBLEU ( Post , 2018 )
We query production systems with English news sentences and train imitation models to mimic their German outputs .
The imitation models closely imitate the production systems for both in - domain ( WMT newstes t 2014 ) and out - of- domain test data ( IWSLT TED talks ) .
best public system ( Guzm ? n et al. , 2019 ) .
Our imitation model reaches a nearly identical 22.0 BLEU .
OOD Evaluation and Functional Similarity Our imitation models have also not merely matched the production systems on in- domain data .
We test the English ?
German imitation models on IWSLT : the imitation models are always within 0.9 BLEU of the production systems ( IWSLT in Table 2 ) .
Finally , there is also a high inter-system BLEU between the imitation models and the production systems .
In particular , on the English ?
German WMT14 test set the inter-system BLEU is 65.6 , 67.7 , and 69.0 for Google , Bing , and Systran , respectively .
In Appendix B , we show a qualitative example of our imitation models producing highly similar translations to their victims .
Estimated Data Costs
We estimate that the costs of obtaining the data needed to train our English ?
German models is as little as $ 10 ( see Appendix C for full calculation ) .
Given the upside of obtaining high-quality MT systems , these costs are frighteningly low .
Attacking Production Systems
Thus far , we have shown that imitation models allow adversaries to steal black - box MT models .
Here , we show that imitation models can also be used to create adversarial examples for black - box MT systems .
Our attack code is available at https : //github.com / Eric-Wallace / adversarial-mt .
What are Adversarial Examples for MT ?
MT errors can have serious consequences , e.g. , they can harm end users or damage an MT system 's reputation .
For example , a person was arrested when their Arabic Facebook post meaning " good morning " was mistranslated as " attack Attack System English Input ( red = adversarial edit ) Predicted Translation ( blue = English meaning )
Targeted Flips Google
I am going to die , its over 100 ? F , help !
Ich werde sterben , es ist ? ber 100 ? F , Hilfe !
Google I am going to die , its over 102 ? F , help !
Ich werde sterben , es ist ? ber 22 ? C , Hilfe !
100 ? F ? 22 ? C ( =72 ? F ) Systran
I am feeling grey that HK decided to join China Ich f?hle mich grau , dass HK beschlossen hat , China beizutreten Systran I am feeling gre y that HK decided to join China Ich f?hle mich froh , dass HK beschlossen hat , China beizutreten " grau " ( gray ) ? " froh " ( happy )
Malicious Nonsense Google miei llll going ro tobobombier the Land Ich werde das Land bombardieren ( I will bomb the country )
We show examples of adversarial attacks that transfer to production MT systems as of April 2020 ( screenshots in Appendix G ) .
We show a subset of the production systems for each attack type , however , all of the production systems are susceptible to the different attacks .
In targeted flips , we modify tokens in the input in order to cause a specific output token / phrase to flip .
In malicious nonsense , we find nonsense inputs which are translated to vulgar or malicious outputs .
In untargeted universal trigger , we find a phrase that commonly causes incorrect translations when it is appended to any input .
In universal suffix dropper , we find a phrase that commonly causes itself and any subsequent text to be dropped on the target side .
Untargeted them " ( Hern , 2018 ) .
Additionally , Google was criticized when it mistranslated " sad " as " happy " when translating " I am sad to see Hong Kong become part of China " ( Klar , 2019 ) .
Although the public occasionally stumbles upon these types of egregious MT errors , bad actors can use adversarial attacks ( Szegedy et al. , 2014 ) to the target model and can compute gradients with respect to its inputs ( Ebrahimi et al. , 2018 ; Chaturvedi et al. , 2019 ) .
These gradients are used to generate attacks that flip output words ( Cheng et al. , 2020 ) , decode nonsense into arbitrary sentences ( Chaturvedi et al. , 2019 ) , or cause egregiously long translations .
Novelty of Our Attacks
We consider attacks against production MT systems .
Here , white - box attacks are inapplicable .
We circumvent this by leveraging the transferability of adversarial examples ( Papernot et al. , 2016 ; Liu et al. , 2017 ) : we generate adversarial examples for our imitation models and then apply them to the production systems .
We also design new universal ( inputagnostic ) attacks ( Moosavi - Dezfooli et al. , 2017 ; Wallace et al. , 2019 ) for MT : we append phrases that commonly cause errors or dropped content for any input ( described in Section 4.3 ) .
How We Generate Adversarial Examples
We first describe our general attack formulation .
We use a white - box , gradient - based method for constructing attacks .
Formally , we have white - box access to an imitation model f , a text input of tokens x , and an adversarial loss function L adv .
We consider different adversarial example types ; each type has its own L adv and initialization of x .
Our attack iteratively replaces the tokens in the input based on the gradient of the adversarial loss L adv with respect to the model 's input embeddings e.
We replace an input token at position i with the token whose embedding minimizes the first-order Taylor approximation of L adv : arg min e i ?V e i ? e i ?
e i L adv , ( 1 ) where V is the model 's token vocabulary and ?
e i L adv is the gradient of L adv with respect to the input embedding for the token at position i.
Since the arg min does not depend on e i , we solve : arg min e i ?V e i ?
e i L adv .
( 2 ) Computing the optimal e i can be computed using | V | d-dimensional dot products ( where d is the embedding dimension ) similar to Michel et al . ( 2019 ) .
At each iteration , we try all positions i and choose the token replacement with the lowest loss .
Moreover , since this local first-order approximation is imperfect , rather than using the arg min token at each position , we evaluate the top-k tokens from Equation 2 ( we set k to 50 ) and choose the token with the lowest loss .
Using a large value of k , e.g. , at least 10 , is critical to achieving strong results .
Types of Adversarial Attacks
Here , we describe the four types of adversarial examples we generate and their associated L adv .
( 1 ) Targeted Flips
We replace some of the input tokens in order to cause the prediction for a specific output token to flip to another specific token .
For example , we cause Google to predict " 22 ? C " instead of " 102 ? F " by modifying a single input token ( first section of Table 3 ) .
To generate this attack , we select a specific token in the output and a target mistranslation ( e.g. , " 100 ? F " ? " 22 ? C " ) .
We set L adv to be the cross entropy for that mistranslation token ( e.g. , " 22 ? C " ) at the position where the model currently outputs the original token ( e.g. , " 100 ? F " ) .
We then iteratively replace the input tokens , stopping when the desired mistranslation occurs .
( 2 ) Malicious Nonsense
We find nonsense inputs which are translated to vulgar / malicious outputs .
For example , " I miii llllll wgoing rr tobobombier the Laaand " is translated as " I will bomb the country " ( in German ) by Google ( second section of Table 3 ) .
To generate this attack , we first obtain the output prediction for a malicious input , e.g. , " I will kill you " .
We then iteratively replace the tokens in the input without changing the model 's prediction .
We set L adv to be the cross-entropy loss of the original prediction and we stop replacing tokens just before the prediction changes .
A possible failure mode for this attack is to find a paraphrase of the input-we find that this rarely occurs in practice .
( 3 ) Untargeted Universal Trigger
We find a phrase that commonly causes incorrect translations when it is appended to any input .
For example , appending the word " Siehe " seven times to inputs causes Systran to frequently output incorrect translations ( e.g. , third section of Table 3 ) .
( 4 ) Universal Suffix Dropper
We find a phrase that , when appended to any input , commonly causes itself and any subsequent text to be dropped from the translation ( e.g. , fourth section of Table 3 ) .
For attacks 3 and 4 , we optimize the attack to work for any input .
We accomplish this by averaging the gradient ?
e i L adv over a batch of inputs .
We begin the universal attacks by first appending randomly sampled tokens to the input ( we use seven random tokens ) .
For the untargeted universal trigger , we set L adv to be the negative cross entropy of the original prediction ( before the random tokens were appended ) , i.e. , we optimize the appended tokens to maximally change the model 's prediction from its original .
For the suffix dropper , we set L adv to be the cross entropy of the original prediction , i.e. , we try to minimally change the model 's prediction from its original .
Experimental Setup
We attack the English ?
German production systems to demonstrate our attacks ' efficacy on highquality MT models .
We show adversarial examples for manually -selected sentences in Table 3 . Quantitative Metrics
To evaluate , we report the following metrics .
For targeted flips , we We report the percent of inputs which are successfully attacked for our imitation models , as well as the percent of tokens which are changed for those inputs .
We then report the transfer rate : the percent of successful attacks which are also successful on the production MT systems .
pick a random token in the output that has an antonym in German Open WordNet ( https://github.
com/hdaSprachtechnologie/odenet ) and try to flip the model 's prediction for that token to its antonym .
We report the percent of inputs that are successfully attacked and the percent of the input tokens which are changed for those inputs ( lower is better ) .
4
For malicious nonsense , we report the percent of inputs that can be modified without changing the prediction and the percent of the input tokens which are changed for those inputs ( higher is better ) .
The untargeted universal trigger looks to cause the model 's prediction after appending the trigger to bear little similarity to its original prediction .
We compute the BLEU score of the model 's output after appending the phrase using the model 's original output as the reference .
We do not impose a brevity penalty , i.e. , a model that outputs its original prediction plus additional content for the appended text will receive a score of 100 .
For the universal suffix dropper , we manually compute the percentage of cases where the appended trigger phrase and a subsequent suffix are either dropped or are replaced with all punctuation tokens .
Since the universal attacks require manual analysis and additional computational costs , we attack one system per method .
For the untargeted universal trigger , we attack Systran .
For the universal suffix dropper , we attack Bing .
Evaluation Data
For the targeted flips , malicious nonsense , and untargeted universal trigger , we eval-uate on a common set of 200 examples from the WMT validation set ( newstest 2013 ) that contain a token with an antonym in German Open Word - Net .
For the universal suffix dropper , we create 100 sentences that contain different combinations of prefixes and suffixes ( full list in Appendix D ) .
Results : Attacks on Production Systems
The attacks break our imitation models and successfully transfer to production systems .
We report the results for targeted flips and malicious nonsense in Table 4 .
For our imitation models , we are able to perturb the input and cause the desired output in the majority ( > 3/4 ) of cases .
For the targeted flips attack , few perturbations are required ( usually near 10 % of the tokens ) .
Both attacks transfer at a reasonable rate , e.g. , the targeted flips attack transfers 23 % of the time for Systran .
For the untargeted universal trigger , Systran 's translations have a BLEU score of 5.46 with its original predictions after appending " Siehe " seven times , i.e. , the translations of the inputs are almost entirely unrelated to the model 's original output after appending the trigger phrase .
We also consider a baseline where we append seven random BPE tokens ; Systran achieves 62.2 and 58.8 BLEU when appending two different choices for the random seven tokens .
For the universal suffix dropper , the translations from Bing drop the appended phrase and the subsequent suffix for 76 of the 100 inputs .
To evaluate whether our imitation models are needed to generate transferable attacks , we also attack a Transformer Big model that is trained on the WMT14 training set .
The adversarial attacks generated against this model transfer to Google 8.8 % of the time - about half as often as our imitation model .
This shows that the imitation models , which are designed to be high-fidelity imitations of the production systems , considerably enhance the adversarial example transferability .
Defending Against Imitation Models
Our goal is not to provide a recipe for adversaries to perform real-world attack .
Instead , we follow the spirit of threat modeling - we identify vulnerabilities in NLP systems in order to robustly patch them .
To take first steps towards this , we design a new defense that slightly degrades victim model BLEU while more noticeably degrading imitation model BLEU .
To accomplish this , we repurpose Defending Against Imitation Models Figure 3 : A na?ve defense against model stealing equally degrades the BLEU score of the victim and imitation models ( gray line ) .
Better defenses are lower and to the right .
Our defense ( black line ) has a parameter ( BLEU match threshold ) that can be changed to tradeoff between the victim and the adversary 's BLEU .
We outperform the na?ve defense in all settings , e.g. , we degrade the victim 's BLEU from 34.6 ? 33.8 while degrading the adversary 's BLEU from 34.5 ? 32.7 . prediction poisoning ( Orekondy et al. , 2020 ) for MT : rather than having the victim output its original translation y , we have it output a different ( high -accuracy ) translation ? that steers the training of the imitation in the wrong direction .
Defense Objective Formally , assume the adversary will train their imitation model on the outputs of the victim model using a first-order optimizer with gradients g = ? ?t L( x , y ) , where ?
t is the current imitation model parameters , x is an input , y is the victim output , and L is the cross-entropy loss .
We want the victim to instead output a ?
whose gradient g = ? ?t L(x , ? ) maximizes the angular deviation with g , or equivalently minimizes the cosine similarity .
Training on this ?
effectively induces an incorrect gradient signal for ?
t .
Note that in practice the adversary 's model parameters ?
t is unknown to the victim .
Thus , we instead look to find a g that has a high angular deviation across ten different Transformer MT model checkpoints that are saved from ten different epochs .
To find ? , Orekondy et al. ( 2020 ) use information from the Jacobian .
Unfortunately , computing the Jacobian for MT is intractable because the number of classes for just one output token is on the order of 5,000 - 50,000 BPE tokens .
We instead design a search procedure to find ?.
Maximizing the Defense Objective
We first gen-erate the original output y from the victim model ( e.g. , the top candidate from a beam search ) and compute g using the ten Transformer model ensemble .
We then generate a diverse set of 100 alternate translations from the victim model .
To do so , we take the 20 best candidates from beam search , the 20 best candidates from diverse beam search ( Vijayakumar et al. , 2018 ) , 20 random samples , 20 candidates generated using top-k truncated sampling ( k = 10 ) from Fan et al. ( 2018 ) , and 20 candidates generated using nucleus sampling with p = 0.9 ( Holtzman et al. , 2020 ) .
Then , to largely preserve the model 's original accuracy , we compute the BLEU score for all 100 candidates using the model 's original output y as the reference , and we remove any candidate below a certain threshold ( henceforth BLEU Match threshold ) .
We finally compute the gradient g for all candidates using the model ensemble and output the candidate whose gradient maximizes the angular deviation with g. 5
In practice , generating the 100 candidates is done entirely in parallel , as is the computation of the gradient g.
Table 5 shows examples of ? at different BLEU Match thresholds .
For our quantitative results , we will sweep over different BLEU Match thresholds - lower thresholds will more severely degrade the victim 's accuracy but will have more freedom to incorrectly steer the imitation model .
BM ?
Text
Source andere orte im land hatten ? hnliche r?ume .
Target other places around the country had similar rooms .
y -- other places in the country had similar rooms .
? 88.0 24.1 ? some other places in the country had similar rooms .
? 75.1 40.1 ? other sites in the country had similar rooms .
? 72.6 42.1 ? another place in the country had similar rooms .
Table 5 : We show the victim model 's original translation y .
We then show three ? candidates , their BLEU Match ( BM ) with y and their angular deviation ( ? ) , i.e. , the arccosine of the cosine similarity between g and g.
Figure 4 in Appendix F shows a histogram of the angular deviations for the entire training set .
Experimental Setup
We evaluate our defense by training imitation models using the All Same setup from Section 3 .
We use BLEU Match thresholds of 70 , 80 , or 90 ( lower thresholds than 70 resulted in large BLEU decreases for the victim ) .
Results
Figure 3 plots the validation BLEU scores of the victim model and the imitation model at the different BLEU match thresholds .
Our defense degrades the imitation model 's BLEU ( e.g. , 34.5 ? 32.7 ) more than the victim model 's BLEU ( e.g. , 34.6 ? 33.8 ) .
6
The inter-system BLEU also degrades from the original 69.7 to 63.9 , 57.8 , and 53.5 for the 90 , 80 , and 70 BLEU Match thresholds , respectively .
Even though the imitation model 's accuracy degradation is not catastrophic when using our defense , it does allow the victim model to have a ( small ) competitive advantage over the adversary .
Adversarial Example Transfer
Our defense also implicitly inhibits the transfer of adversarial examples .
To evaluate this , we generate malicious nonsense attacks against the imitation models and transfer them to the victim model .
We use 400 examples from the IWSLT validation set for evaluation .
Without defending , the attacks transfer to the victim at a rate of 38 % .
Our defense can drop the transfer rates to 32.5 % , 29.5 % , and 27.0 % when using the 90 , 80 , and 70 BLEU match thresholds , respectively .
Also note that defenses may not be able to drive the transfer rate to 0 % : there is a baseline transfer rate due to the similarity of the architectures , input distributions , and other factors .
For example , we train two transformer models on distinct halves of IWSLT and observe an 11.5 % attack transfer rate between them .
Considering this as a very rough baseline , our defense can prevent about 20 - 40 % of the additional errors that are gained by the adversary using an imitation model .
Overall , our defense is a step towards preventing NLP model stealing ( see Appendix E for a review of past defenses ) .
Currently , our defense comes at the cost of extra compute ( it requires generating and backpropagating 100 translation hypotheses ) and lower BLEU .
We hope to develop more effective and scalable defenses in future work .
Conclusion
We demonstrate that model stealing and adversarial examples are practical concerns for production NLP systems .
Model stealing is not merely hy-pothetical : companies have been caught stealing models in NLP settings , e.g. , Bing copied Google 's search outputs using browser toolbars ( Singhal , 2011 ) .
Moving forward , we hope to improve and deploy defenses against adversarial attacks in NLP , and more broadly , we hope to make security and privacy a more prominent focus of NLP research .
