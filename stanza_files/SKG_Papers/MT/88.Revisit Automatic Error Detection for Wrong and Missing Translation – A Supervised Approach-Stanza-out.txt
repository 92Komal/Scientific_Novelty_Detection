title
Revisit Automatic Error Detection for Wrong and Missing Translation -A Supervised Approach
abstract
While achieving great fluency , current machine translation ( MT ) techniques are bottlenecked by adequacy issues .
To have a closer study of these issues and accelerate model development , we propose automatic detecting adequacy errors in MT hypothesis for MT model evaluation .
To do that , we annotate missing and wrong translations , the two most prevalent issues for current neural machine translation model , in 15000 Chinese - English translation pairs .
We build a supervised alignment model for translation error detection ( AlignDet ) based on a simple Alignment Triangle strategy to set the benchmark for automatic error detection task .
We also discuss the difficulties of this task and the benefits of this task for existing evaluation metrics .
Introduction Different translation errors impact translation comprehensibility and adequacy differently .
For example , wrong product or terminology translation can be considered more severe than missing translation in the eCommerce domain .
Though many machine translation evaluation ( MTE ) metrics have been proposed , most of them are not able to provide a direct connection between the score and the error class to emphasize the different impact on translation comprehension and adequacy .
Most MTE scores measure the similarity between the MT hypothesis and the reference using n-gram and are useful in providing immediate feedback on model performance for system development and objective evaluation for system comparison .
To further analyse the MT hypothesis , fine- grained translation analysis is normally conducted manually which involves expensive human effort ( Popovi ? and Ney , 2011 ; Popovi ? , 2011 b ; Fishel et al. , 2012 ; Vilar et al. , 2006 ) .
This motivates early unsupervised error detection methods ( Popovi ? and Ney , 2011 ) .
Such methods find the erroneous words by calculating the edit distance between the MT hypothesis and its corresponding reference .
However , such unsupervised methods suffer from low accuracy and inability to provide and distinguish certain error classes like Missing words and Wrong translation 1 ( c.f. Sec 3 ) .
On the other hand , the works on supervised methods are few due to limited training resources available .
The annotated corpus are mainly collected from outputs of Phrase - based Machine Translation systems and usually suffer from small quantity ( around 200 instances per language pair ) ( Fishel et al. , 2012 ) , noisy contents ( derived from student assignments ) ( Wisniewski et al. , 2014 ) or overabundant efforts ( additional human efforts are required on postediting the MT hypothesis ) ( Popovi ? and Arcan , 2016 ) , making the progress and development of this task challenging .
We believe a corpus annotated with different translation error classes will facilitate the research on translation error detection .
Therefore , we construct a high quality annotated corpus ( TransErr ) comprising 15000 Chinese -English translation pairs with inter-annotator agreement at 0.804 measured by Cohen's Kappa ( Cohen , 1960 ) .
Different from existing error detection works which focus on all error classes , we currently only take care of missing and wrong translation , the major errors related to adequacy , which is a wide - known issue in neural machine translation ( NMT ) ( Zheng et al. , 2019 ) .
The errors tags are annotated on source ( Chinese ) sentences to reflect the loyalty and adequacy with respect to the source .
Based on TransErr , we discuss the error distribution of dif-ferent systems and challenges of unsupervised error annotations using post-edits .
TransErr enables the study task of supervised MT error detection .
We benchmark this task by providing a simple yet efficient model called ( AlignDet ) .
AlignDet is based on our observation of the Alignment Triangle whose basic idea is that if a word is well translated , the translated word and the corresponding word in reference should be equivalent .
We implement such observation by employing both monolingual and bilingual alignment systems as well as various features .
We further provide error analyses of AlignDet to give more insight to this task .
To study the application of this task , we further conduct a discussion on the impact of different error classes on MT evaluation metrics based on WMT Chinese -English Direct Assessment ( DA ) corpus .
We find that embracing the results of our simple AlignDet model in existing MTE metrics helps to achieve significantly better correlation with human .
Our further analysis on gold standard error annotation suggests that wrong translation tends to produce worse translation .
This discovery gives further credence for the usefulness of our proposed adequacy -oriented error detection task .
In short , the paper is towards carrying out the first systematic study of supervised approaches for error detection , with the goal of accelerating the research and development of MT towards human translation quality .
Our systematic contribution lies on annotated data , supervised model and discussions of its potential to help existing machine translation evaluation task .
Specifically : ?
We propose a task of adequacy -oriented error detection for machine translation evaluation and contribute a high-quality corpus TransErr annotated by professional annotators for machine translation .
The corpus are available 2 . ?
We propose an AlignDet to set the benchmark of this task based on our key observation of Alignment Triangle .
As far as we know , this is the first model making use of all three alignments from source , MT hypothesis and reference concurrently .
?
We conduct various discussions on the challenges of this task and its potential to help existing machine translation evaluation task .
Related Work Existing automatic MTE focus on measuring the similarity between MT hypothesis and references .
Various dimensions have been considered , such as lexical level ( Papineni et al. , 2002 ; Snover et al. , 2009 ; Lo , 2017 ; Popovi ? , 2017 ) , syntax level ( Owczarzak et al. , 2007 ; Duma and Menzel , 2017 ; Liu and Gildea , 2005 ) , semantic level ( Stanojevi ? and Sima'an , 2015 ; Shimanaka et al. , 2018 ) and discourse level ( Guzm ? n et al. , 2014 ) .
However , such methods give an overall score to a system or give a relative ranking to a pair of systems , without any detailed insight of MT system output .
To get the fine-grained insights of the MT model performance , Popovi ? and Ney ( 2011 ) ; Popovi ?
et al. ( 2011a ) make the first step towards automatic translation error detection using unsupervised approach by performing monolingual alignment between a MT hypothesis and its corresponding reference through WER ( Levenshtein , 1966 ) and PER ( Tillmann et al. , 1997 ) . Popovi ? ( 2011 b ) further develop this algorithm into an open-source tool and demonstrates that the detected errors helps to build better evaluation metrics .
However , the edit distance algorithm is not robust ( c. f. Sec. 3.2 ) .
Zeman et al. ( 2011 ) improve the monolingual alignment by borrowing ideas from HMM ( Vogel et al. , 1996 ) but the error detection performance is still far from real application .
To facilitate this task , Wisniewski et al. ( 2014 ) derive corpus of 4,854 source sentences from the assignment of master student specializing on translation but the data turns out to be too noisy .
At the same time , edit distance is carried over to another shared task in WMT ( Callison - Burch et al. , 2012 ; Vaswani et al. , 2017 ) called word-level Quality Estimation ( QE ) ( Luong et al. , 2013 ; Han et al. , 2013 ; Wisniewski et al. , 2014 ; Kim et al. , 2017 ) .
In QE tasks , the edit distance is calculated between a MT hypothesis and its post-edits .
Word needs to be edited is tagged as BAD while the unedited words are tagged as GOOD .
The task requires participants to predict the label for each word .
Although similar to our settings , our error detection task differs from the word- level QE task in the following aspects : ?
The QE task is for confidence estimation ( Specia et al. , 2010 ) which estimates whether an output is good for an end user .
Our error detection task is for fine- grained evaluation and comparison during model development .
?
QE only have GOOD and BAD tag on the MT hypothesis .
It cannot make robust distinction on different error classes due to the intrinsic shortcomings of the edit distance algorithm .
?
Existing QE data does not have reliable label on the source sentence to indicate which source word cause the errors ( c.f. Sec. 3.2 ) , which is claimed as " particularly important to translation adequacy " in WMT 18 and WMT 19 3 .
TranErr Corpus and Analysis TransErr corpus comprises 15000 triples < source , MT hypothesis , reference > with source sentences extracted from LDC NIST MT evaluation sets { LDC2005T06 , NIST02 , NIST03 , NIST04 , NIST05 , NIST08 }.
MT hypothesis is obtained through an NMT engine trained on 25 million Chinese - English parallel sentences .
To study the correlation between error detection and human evaluation , we also conduct additional annotation on WMT17 Direct Assessment ( DA ) dataset containing 560 Chinese - English translation pairs .
Annotation is conducted by two professional translators who specialize on Chinese -English translation .
All three information < source , MT hypothesis , reference > are given during annotation .
Annotators are instructed to mark on the source the two adequacy related errors ( Missing & Wrong ) found in the MT hypothesis and to follow the minimum span principle , i.e. identifying the minimum number of words whose content are not ( correctly ) conveyed in the MT hypothesis regardless of fluency and grammatical correctness .
If a span in the source is incorrectly translated in the MT hypothesis , all words in that span will be marked as Wrong ( W ) ; if totally missing , it will be marked as Missing ( M ) .
Due to the intrinsic difference between the two languages , not all words in the source need to be translated in the 3 http://www.statmt.org/wmt19/
MT hypothesis .
In this case , we do NOT annotate them as missing .
We also have more stringent conditions for proper nouns and terminologies since their translations are more specific and domain dependent .
They have to be unerringly correct or otherwise , marked as Wrong Terminology ( WT ) or Missing Terminology ( MT 4 ) .
All correctly translated words are automatically assigned a label OK .
Annotators are first asked to annotate 100 sentence pairs followed by a discussion among the project team to resolve any disagreement .
After 5 such iterations , we start the formal annotation .
10 % overlapped instances are given to each annotator for monitoring the inter-annotation agreement ( IAA ) .
The disagreed instances are discussed every day .
We obtain an overall IAA of 0.804 measured by Cohen's Kappa .
The speed for both annotators is about 50 sentence pairs per hour .
Translation Error Analysis
Table 1 shows the error distribution of TransErr and WMT 17 corpus .
The corpora have different BLEU with TransErr having more missing ( M ) errors and WMT17 more wrong ( W ) errors .
As WMT17 contains output from multiple MT systems , we further investigate the errors distributions among the different systems , in particular the output from AFRL and NRC .
AFRL and NRC have comparable BLEU but different human evaluation .
Error distributions for the two corpora vary quite significantly with AFRL having more W errors ( 45 % W and 19 % WT ) while NRC has more M errors ( 31 % M and 31 % MT ) .
This demonstrates the importance of classifying translation errors and the need for the errors to be considered during evaluation .
Challenges of Automatic Translation Error Annotation
Though MTE and error detection are two different tasks , they are closely related .
Popovi ? and Ney ( 2011 ) QE and follow ( Popovi ? and Ney , 2011 ) to derive source annotation through two stages of alignments .
The first stage is monolingual alignment between post-edits and MT hypotheses through calculating their edit distance to find missing and wrong translations .
The second stage is bilingual alignment using Giza ++ ( Och and Ney , 2000 ) on post-edits and the sources to propagate the error labels on the post-edits to the corresponding sources .
Similar to our manual annotation , we assign MT and WT label for terminologies .
To get empirical study , we perform unsupervised annotation on our WMT17 dataset using the above-mentioned method .
Table 2 demonstrates its F1 score using the manual annotation as the gold standard .
We investigate and summarize the causes in Figure 1 : ?
The edit distance algorithm is not robust in error class detection since it is purely based on symbolic comparison .
( Ex. 1 ) illustrates a case where Byers 's is missing .
However , TER tool tags it as a substitution of the in the MT hypothesis because there is a perfect symbolic alignment before and after the erroneous word .
This intrinsic inability to detect missing and wrong translation is also discussed in ( Popovi ? and Ney , 2011 ) . ? Giza ++ cannot generate reliable alignments for low-frequency words ( Riley and Gildea , 2010 ) and such low-frequency words contribute to a large extent of erroneous words .
These misalignments propagate from the post-edits to the source sentence and lead to incorrect error tags .
( Ex. 2 ) gives an example , the Dettori should be aligned to daituli but was not aligned to any word by GIZA ++.
This leads to an incorrect OK tag in the source sentence .
As such , unsupervised error annotation presents many challenges even leveraging on post-edits .
Hence a set of more reliable annotated data will be able to contribute much to the error detection task .
AlignDet Model
To revive fine- grained translation error detection , we propose AlignDet , a supervised feature - based solution using TransErr to set the benchmark for this task .
We discuss our Alignment Triangle observation and briefly list the features we used .
In the subsequent discussions , we denote s i , t j and r k as the ith , jth and kth word in a source sequence S , MT hypothesis T and reference R separately .
Alignment Triangle Observation
In translation , adequacy can be simply defined as all contents in the source are reproduced correctly in the MT hypothesis .
As long as the MT hypothesis and the reference are semantically equivalent , such reproduction may not require the MT hypothesis and the reference to be exactly identical .
This motivates us to analyze the translation t j in T and r k in R for the same s i in S 5 , in the following error detection scenarios .
We refer t j and r k for the same s i as translation correspondence .
Pattern1 : s i has translation correspondence t j and r k .
It should be labeled as OK if t j and r k are semantically equivalent ; otherwise , it should be labeled as W. For example , in ( Ex. 3 ) from Figure 2 , the two words , swap and exchange , are equivalent in the given context , which should be marked as OK .
However , compared to ( Ex. 6 ) , students and comers are not equivalent in the given context , although they may be equivalent in higher abstraction , hence it is annotated as W. Pattern2 : s i does not have translation correspondence r k .
Assuming R does not have any adequacy issue , this would imply that s i does not need to be explicitly translated .
In this case , the omission of t j is an OK translation .
However , if it is explicitly translated in T as t j , we will still accept it as an OK translation though it may cause minor redundancy issue and we assume that it will not harm adequacy .
For example , either explicitly translation meiguo to USA or not in ( Ex. 4 ) should be fine .
Pattern3 : s i has translation correspondence r k but not t j .
This implies that translation of s i is missing and should be translated .
A M label will be given in this case .
( Ex. 5 ) shows an example of this .
jinzhu does not have t j but it is necessary for it to be translated as the translation correspondence is presence in R as r k .
The same rationale also applies to terminologies , except that they are marked with MT or WT and have to be unerringly translated for them to be marked as OK .
For the other cases not illustrated above , we assume that they are either not happening or do not affect the adequacy of the translation .
Features and Model
To capture the above observation , we build two sets of features , one for finding t he corresponding translation t j and r k for each s i , the other for checking the equivalence of t j and r k .
Here , we only briefly list the features adopted in our model as they are all commonly used standard features .
We give detailed description of all features in Appendix A.1. Giza ++ ( Och and Ney , 2000 ) is used to find the corresponding translation t j and r k for each s i .
However , as Giza ++ generates noisy alignments especially for low-frequency words , we propose a set of features to complement Giza ++ results .
The same features are applied on T and R , hence we only describe the feature for the alignment between S and T .
Suppose that s i is aligned to t j 6 .
These features are : s i 's POS tag ; a binary feature of whether NER tags of s i and t j are the same ; the corpus frequency and Giza ++ translation probability of s i and t j ; the similarity between t j and s i 's most frequent translation ; the number of alignedpairs among words linking to s i and t j in dependency tree .
We leverage on the state - of- the - art monolingual alignment tool Sultan et al . ( 2014 ) to check the equivalence of t j and r k .
The tool leverages on paraphrase lexicon ( Pavlick et al. , 2015 ) and dependency relations to find e quivalent expression between two sentences in the same language .
The following features are used : a binary feature indicating whether t j and r k are aligned by tool proposed by ( Sultan et al. , 2014 ) ; A binary feature indicating whether the NER tag of t j and r k are the same .
While this error detection problem can be modeled as sequential labelling task , we choose simple classification model as the benchmark .
Therefore , we use an Mutilayer Perceptron ( MLP ) as the detection model which takes all above features in for each source word s i to choose one of the labels in { OK , W , WT , M , MT } as the prediction .
Experiments
We conduct empirical experiments to evaluate the effectiveness of our error detection model , perform error analysis and discuss the challenges of this task .
We split our TransErr dataset into training and developing sets in a ratio of 9:1 and adopt the whole WMT DA dataset as the testing set .
Error detection results are reported in singleclass F1 .
All Chinese sentences are tokenized by THULAC ( Li and Sun , 2009 ) .
The rest of the pre-processings like POS , NER , parsing for both English and Chinese are performed by Stanford Corenlp ( Manning et al. , 2014 ) .
Baselines
We build a naive baseline , random model , to test the worst performance .
It randomly assigns a label to each source word according to the distribution of each class in the training set .
We then evaluate the performance of our proposed MLP model ( AlignDet ) and two recent models under WMT word- level QE shared - task : ? CEQE ( Hu et al. , 2018 )
Using a three - part neural network model , which encodes semantic factors , local context and global context successively .
? SHEF -bRNN
( Ive et al. , 2018 )
Adopting bidirectional recurrent neural network to learn a representation for < source , MT hypothesis > sentence pairs .
We replicate these two models utilizing original implementations but modify them to work in our 5 - label setting .
7
We also analyze our proposed model and its variant by ablating or substituting its components ?
Raw alignment Using only the raw results from the monolingual and bilingual alignment systems and NER on the source to examine the effect of alignment on the model . ?
No reference
We remove all features derived from the reference , for example , bilingual alignment features between S and R , and monolingual alignment features between T and R .
This is to test the efficacy of using only source information for translation performance evaluation .
Error Detection Results
Table 3 shows our error detection results on the developing set and test set .
Our random baseline ( Row ( 1 ) ) provides a preliminary insight for this error detection problem , with extremely low F1 scores ( nearly 0 ) except for the OK class .
It is because errors are scarcely distributed ( see Table 1 ) .
In addition , the QE models ( Row 2 , 3 ) do not perform well in error classification .
This is reasonable because they are not designed for error detection in our setting .
The overall better performance of our AlignDet model supports that our Alignment Triangle Observation and its contribution to the error detection task .
However , with raw alignment ( Row 4 vs Row 5 ) , the performance of AlignDet drops sharply .
It is because the alignment model , especially Giza + + , generates noisy results .
This demonstrates the efficacy of other features in rectifying Giza ++ results .
The comparison between Row ( 4 ) and Row ( 6 ) also demonstrates the importance of reference , especially for detecting W label .
It is because wrong translation is also likely to get aligned with the source as it can be the translation of the source words in some other contexts .
However , reference gives us a pointer to the correct word choice in the given context , hence helping determining W label .
Without reference , we may need to rely on more contextual modeling effort to detect a wrong translation .
To get more insight into the difficulty of this task , we perform error analysis on AlignDet .
We only examine the confusion matrix on test set due to the limitation of space in ( b ) Predict OK as M ( c ) Predict M as OK .
We illustrate the wrong prediction in Figure 3 . ( Ex. 7 ) demonstrates a case for " W as OK " .
Both strength ( the wrong translation in T ) and efforts ( the correct translation in R ) are aligned to the source word qiangdu by Giza ++.
Though both of them are possible translations for qiangdu , they are not semantically equivalent in this context .
Unfortunately , monolingual alignment tool aligns them as a pair , hence a more contextual and semantically - sensitive monolingual alignment is required .
( Ex. 8 ) demonstrates a case for " OK as M " .
In T , qiuyuan is translated to those and is not detected by our bilingual alignment .
This leads the model to treat it as not been translated .
This example demonstrates the constraint of bilingual alignment in finding the translation correspondence , which would require contextual - level understanding .
( Ex. 9 ) demonstrates a case for " M as OK " .
Similar to ( Ex. 8 ) , the mis-prediction is caused by the failure of bilingual alignment ( between chengban in S and in charge in R ) .
Our model regards chengban as omittable as bilingual alignment fails to align it to in charge in R .
As a result , though chengban is translated in R but not translated in T , we classify it as OK instead of M .
This example again demonstrates the important of alignment in error detection task .
Error Analysis
Our qualitative error analyses highlight the bottleneck of AlignDet model on the proposed align-ment features .
In a broader context , both bilingual alignment and monolingual alignment are still having open issues in the realm of NLP ( Sultan et al. , 2014 ) .
An intuitive solution is turning to soft alignment ( e.g. , attention ) using neural techniques ( Liu and Sun , 2015 ; Tamura et al. , 2014 ) .
This raises interesting and challenging questions on how to leverage on such approaches to capture our observations in finding corresponding translations t j and r k for each s i and comparing their equivalence ?
Neural models usually require large data and how to leverage on limited error annotation data ( say , 15000 sentence pairs ) and large parallel corpus for such purpose ?
6 Discussion : How Does Error Class Affect Existing MT Evaluation Metrics ?
This paper proposes an adequacy translation error detection model with the aim to study its contribution to MT .
We presume that fine grained MT evaluation is useful for assessing different properties of MT models .
Hence , we go one step further to discuss how fine - grained error detection benefits existing MT evaluation metrics using DA as the base for our evaluation .
To explore further , we define a Weight Adequacy Error Rate ( WAER ) following TER ( Snover et al. , 2006 ) : WAER = w1 # W + w2# WT + w3 # M + w4 # MT Len ( S ) ( 1 ) where uation metric .
As a common practice in BLEU and METEOR , we view WAER as a penalty to base metrics , which we refer as Score , and calculate the penalized score as follow : Score = Score * ( 1 ? WAER ) ( 2 )
To determine w 1 , w 2 , w 3 , w 4 , we build a development set of 300 instances , sampled from our TransErr , with overall translation quality scored by multiple judges who are professional at both English and Chinese .
To avoid bias , our error annotators of TransErr are excluded from this scoring .
We adjust w 1 , w 2 , w 3 , w 4 in this development set based on our gold annotation , such that the enhanced metrics achieves highest correlation with the average human score .
We examine three benchmark metrics ( BLEU , METEOR , ROUGE ) and the recent metrics submitted to WMT17 : BEER ( Stanojevi ? and Sima'an , 2015 ) , CUNI - TreeAggreg ( Mare ?ek et al. , 2017 ) , MEANT 2.0 , MEANT 2.0-nosrl ( Lo , 2017 ) , UHH TSKM ( Duma and Menzel , 2017 ) , NGRAM2VEC , BLEU2VEC sep ( T?ttar and Fishel , 2017 ) , CHRF ( Popovi ? , 2015 ) , CHRF ++ ( Popovi ? , 2017 ) .
We test their performance on WMT 17 DA dataset .
The results are in Table 5 . WAER is calculated by both system prediction results and the gold standard .
We observe that , with our error prediction , WAER is able to enhance almost all metrics substantially ( + 0.049 on average ) .
When calculating WAER using the gold standard , the performance is even higher ( + 0.089 on average ) .
This demonstrates the efficacy of error detection for enhancing existing direct assessment metrics .
Note that , we do not conclude that our WAER ( Eq. 1 ) and the way of enhancing existing Score ( Eq. 2 ) is optimal in terms of achieving better correlations with human judgment .
We also want to find out which error class affects the overall translation quality more .
Therefore , we study how the enhanced metrics correlate to human judgment when the weight of a single class of error changes .
The trend is shown in Figure 4 where y-axis is the average correlation of all metrics enhanced by WAER on gold standard .
X-axis is the weight of the observed error class .
To eliminate the contribution made by other error classes , weights of other error classes are set to 0 .
From Figure 4 , one can observe that wrong related error ( W and WT ) can help existing metrics achieve higher correlation to human judgment than missing -related error ( M and MT ) .
This suggests wrong translation is more severe than missing word .
This provides valuable feedback on the advancement of evaluation metric towards human evaluation .
Similarly , one can also observe that adequacy issue on terminologies is more severe than normal words and term-related errors need higher weights to achieve highest correlation .
Accurate translation of terminology is an issue in neural machine translation if its occurrence is low in the training data .
Accurate translation of terminology may be an area that warrens more work .
Conclusion
In this paper , we revive the problem of error detection task for fine - grained machine translation 950 evaluation purpose .
We contribute a high-quality dataset TransErr with Missing and Wrong translation manually annotated by professional translators to enable the development of supervised methods .
Based on TransErr , we benchmark this task by proposing a strong baseline , i.e. , Align - Det model , based on our Alignment Triangle observation .
We also conduct various discussions about this task such as the challenges of unsupervised methods for error annotation , the bottleneck of AlignDet model and the potential benefits of the error detection task for existing MT evaluation metrics task .
This work represents the preliminary work for more multi-faceted machine translation evaluation , focusing on multiple aspects instead of only a score or ranking , with the goal of push MT techniques to a higher standard .
However , there are areas for further exploration .
In the future , we will explore more advanced techniques such as neural networks for error detection .
We will also investigate how to make better evaluation metrics with the help of error detection .
We will also study how to improve machine translation model through error detection .
For example , how to improve MT models according to different class of errors .
Figure 1 : 1 Figure 1 : Examples of unsupervised annotation on WMT17 . ( S ) , ( P ) and ( T ) refer to source , post-edits and MT hypothesis respectively .
Solid line indicates correct alignment .
Solid line with a red cross refers to wrong alignment while dotted line refers to missing alignment .
This convention applies to the whole paper .
