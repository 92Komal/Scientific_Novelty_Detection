title
PATQUEST : Papago Translation Quality Estimation
abstract
This paper describes the system submitted by Papago team for the quality estimation task at WMT 2020 .
It proposes two key strategies for quality estimation : ( 1 ) task - specific pretraining scheme , and ( 2 ) task - specific data augmentation .
The former focuses on devising learning signals for pretraining that are closely related to the downstream task .
We also present data augmentation techniques that simulate the varying levels of errors that the downstream dataset may contain .
Thus , our PATQUEST models are exposed to erroneous translations in both stages of task -specific pretraining and finetuning , effectively enhancing their generalization capability .
Our submitted models achieve significant improvement over the baselines for Task 1 ( Sentence - Level Direct Assessment ; EN - DE only ) , and Task 3 ( Document - Level Score ) .
* Equal contribution ?
Work done during internship at Naver Corp .
Introduction
With the widespread use of machine translation systems , there is a growing need to evaluate translated results at low-cost .
The task of quality estimation ( QE ) addresses this issue , where the quality of a translation is predicted automatically given the source sentence and its translation .
The estimated quality can inform users about the reliability of the translation , or whether it needs to be post-edited .
Previous QE systems generally include pretraining and finetuning steps , where the former step involves masked language modeling ( MLM ) utilizing large parallel corpora , with the expectation that the models will learn cross-lingual relationships ( Kepler et al. , 2019 ; Kim et al. , 2019 ) .
The models are , in turn , finetuned with task -specific data .
However , while the pretraining step involves training data with near-perfect translations , lowquality translations are only introduced during the finetuning step .
In this work , we suggest two key strategies that could alleviate this pretrain- finetune discrepancy in QE tasks by : ( 1 ) adopting a task -specific pretraining objective which is close to that of the downstream task , and ( 2 ) generating abundant taskspecific erroneous sentence pairs and their learning signals .
Our approach , which is depicted in Figure 1 , is motivated from BLEURT ( Sellam et al. , 2020 ) , where we extend their general approach to the bilingual QE setting .
Our submitted systems achieve significant improvements in performance over the baseline systems on WMT20 Shared Tasks for QE ( Specia et al. , 2020 ) : an absolute gain of + 35.2 % in Pearson score for ( Task 1 ) Sentence -Level Direct Assessment ( EN - DE ) , and + 18.4 % in Pearson score for ( Task 3 ) Document - Level Score .
Sentence -Level QE : Direct Assessment
The task of sentence - level QE for direct assessment ( DA ) involves predicting the perceived quality of the translation given the source and the translated sentences .
Following the footsteps of the previous work on QE , our sentence - level system also utilizes the pretrained multilingual language models such as BERT ( Devlin et al. , 2018 ) and Cross-lingual Language Model ( XLM ) ( Conneau and Lample , 2019 ) .
As the size of the training corpus for the QE task is very limited ( 7 K sentence pairs ) , it is crucial to align these models closely to the task using more data in the form of task -specific pretraining .
As opposed to pretraining the models on parallel corpora using the standard MLM approach , we pretrain the models in a multi-task setting using learning signals and data that are arguably more task -specific similar to Sellam et al . ( 2020 ) .
Task -Specific Data Augmentation
In order to better align the pretrained models to the QE task , synthetic sentence pairs that contain various types of translation errors are generated from clean parallel corpora 1 . For each target sentence , we generate two perturbed sentences by separately applying one of the four methods described below .
Omitted Word
We randomly omit at most three words from the target-side , simulating inadequate translations .
Word Order Based on the part- of-speech ( POS ) tag for each word in the target sentence , and predefined sequences of POS patterns , we randomly swap two target words if those words match one of the patterns .
The POS patterns can be contiguous , e.g. , adjective -space-noun , or long-ranged , e.g. , noun-*- adjective .
When none of the patterns are matched , we randomly swap two words .
Lexical Selection
For each target sentence , we mask out at most three words randomly , and apply mask-filling via a German BERT model from Hugging Face 2 .
The purpose of this alteration is to generate fluent but somewhat inadequate target sentences .
Repeated Phrase
In order to simulate the repetition problem in translations generated by neural machine translation models , we alter the target sentence by adding a repetition of a random phrase within the sentence .
The length of the random phrase is at most three tokens .
1 Europarl v10 and News Commentary v15 2 bert- base-german-cased , https://huggingface.co/transformers/ pretrained_models.html
Task -Specific Learning Signals
As the goal of the downstream task is to predict the DA scores which represent the " perceived quality " of the translation , we need to consider pretraining signals that can capture the somewhat subjective notion of " good " and " bad " translations .
Consulting the related works , we prepared the three learning signals : ? SentenceBERT score ( Reimers and Gurevych , 2019 ) ? BERTScore ( Zhang et al. , 2019 ) , extended to multilingual setting ?
Target ( German ) Language Model ( GPT - 2 , Radford et al . ( 2019 ) ) score
For each sentence pair in the original bilingual corpora as well as the augmented ones , the three types of learning signals are computed , and later used in the task -specific pretraining .
SentenceBERT Score
For a given sentence , SentenceBERT produces a semantically meaningful sentence embedding that can be compared using a distance metric .
We note that when comparing the distance between two sentence vectors , the Kendall rank correlation coefficient ( Kendall , 1938 ) is computed instead of the cosine similarity measure as the former correlates better with the human judgement , possibly because it produces a more widespread range of scores than the latter especially when the dimension of the sentence vectors is high .
In our experiments , we used the publicly available multilingual SentenceBERT model released from UKPLab 3 that supports 13 languages including English and German .
Multilingual BERTScore While SentenceBERT score looks at the sentence embedding as a whole , BERTScore computes a similarity score for each token in the pair of sentences .
We include BERTScore as one of the learning signals because we feared that the meanpooling of the BERT - embedded tokens within the SentenceBERT model , while effective in extracting the overall meaning of the sentence , may overlook some of the small semantic details within the sentence .
However , as the original BERTScore is designed to work in monolingual setting , i.e. evaluating a translation against a reference sentence , it needs to be extended in multilingual setting using a multilingual BERT ( mBERT ) model .
Analogous to the original approach , the multilingual BERTScores can be computed in various ways depending on which side we are computing the maximum similarities from .
In our experiments , we devise a metric where we merge both the source - and target - side maximum similarities between tokens with the corresponding inverse document frequency ( IDF ) weighting ; thus , given a sequence of vectorized source and target tokens , s and t , we defined the mBERTScore of s and t to be : S s?t + S t?s s i ?s idf( s i ) + t j ?t idf( t j ) where S s?t = s i ?s idf( s i ) max t j ?t s i ?
t j S t?s = t j ?t idf( t j ) max s i ?s t j ?
s i
Target Language Model Score While SentenceBERT and multilingual BERTScore can be used as proxies for evaluating the " adequacy " of the translation , empirically , we noticed that they cannot seem to sufficiently represent the " fluency " of translated target sentence .
In other words , both metrics may assign high scores to the translated sentence if key source tokens are translated and present in the translation , even when the overall sentence may not be articulate .
To address this issue , the target language model ( GPT - 2 ) score is added to the set of learning signals .
We simply use the arithmetic mean of the tokenlevel predictions to produce the score for a target sentence .
We utilize the pretrained GPT - 2 model for German released by Zamia Brain 4 .
Model Architecture
We have two stages for task -specific training , i.e. first with the augmented data and the learning signals , and second with the provided QE dataset ( ref. Section 2.4 ) .
As the output to predict for each stage is different , we utilize the following two types of model architectures .
Figure 3 : The model architecture ( left ) for the taskspecific finetuning using the provided QE dataset .
For each concatenated vector computed within each Score Block ( c.f. Fig. 2 . ) , a Linear Block ( right ) is added on top of it .
The results from the Linear Blocks are concatenated and used to produce the final DA score .
Model for Task -Specific Pretraining
On top of the specific layer of the pretrained mBERT or XLM models , we attach a series of layers called " Score Block " for each type of learning signal as depicted in Figure 2 .
We utilize the 9th and 5th layer of the BERT and XLM models , respectively , as these layers are reported to be more semantically relevant ( Jawahar et al. , 2019 ; Zhang et al. , 2019 ) .
In addition to using the vector representation of the [ CLS ] token , utilizing the mean-pooled and max-pooled vectors from all tokens further improved the performance .
Model for Task -Specific Finetuning
Once the task-specific pretraining is completed , we begin the finetuning by adding layers above the concatenation layer within each Score Block , as shown in Figure 3 .
Thus , we have three concatenated vectors being fed to three " Linear Blocks " separately , whose purpose is to reduce the dimensions of the hidden representation , preparing it for the final regression layer .
We note that applying dropout ( Srivastava et al. , 2014 ) to these linear layers helps with the performance .
Task -Specific Training
We experiment with three different types of pretrained models : mBERT 5 , XLM trained with MLM ( XLM - MLM ) 6 , and XLM trained with causal language modeling ( XLM - CLM ) 7 . All of the pretrained models are available at Hugging Face .
Task -Specific Pretraining ( TSP )
As the size of the provided QE dataset is small , we make use of the existing parallel data as well as the error-induced synthetic data .
For the EN - DE bilingual dataset , we select a subset from this year 's training corpora for WMT News Translation Task , summing to just under 10 M sentence pairs ; for the synthetic dataset , the size is 3.4 M .
Given the concatenated source and target sentences as an input , the model for TSP is trained to predict the three types of learning signals in a multi-task setting by minimizing the sum of the mean squared error losses for each signal ( ref .
Task -Specific Finetuning ( TSF )
Once the model is trained with the augmented data , its parameters are loaded to the model for TSF ( ref. Figure 3 ) , and finetuned using the QE dataset .
This time , the model learns to predict the mean DA score .
3 Document - Level QE : MQM Scoring Given a source and its translated document , this task involves identifying translation errors and estimating the translation quality of the document based on the taxonomy of the Multidimensional Quality Metrics ( MQM ) 8 .
With the pre-defined MQM taxonomy , human annotators assess whether the translation satisfies the specifications , and from these annotations , an MQM score is obtained .
In this work , we focus on building a system that predicts the MQM score for a given pair of source and translated document .
The major difficulty that we encountered in this task was the lack of training data .
As the amount of provided data is limited ( 8,591 sentence pairs ) , a model that is solely finetuned on this small - scale data was not capable enough to differentiate sentences with varying level of errors .
To address this issue , we propose simple yet effective methods for task -specific data augmentation , and task -specific training framework 9 .
Task -Specific Data Augmentation
We generate erroneous sentence pairs and their pseudo- MQM scores from Europarl and QE training corpus in accordance with the MQM taxonomy .
Generating Erroneous Sentence Pairs
Out of the 45 error categories specified in QE annotations , we select five frequent categories for which we can automatically perturb the target -side of the parallel corpus at little cost .
More details on our data augmentation technique for each category are provided below .
Omitted Preposition
We introduce an error into the target -side of a sentence pair by randomly omitting one of the French prepositions that exist in the sentence .
Omitted Determiner
The same process is done for French determiners as for prepositions .
Wrong Preposition
We replace a French preposition with another one .
When more than one candidate exists , we choose one at random .
Word Order
We exploit grammatical pattern that most descriptive adjectives go after the noun in French sentences ( unlike English ones ) .
Using an in- house French POS tagger , we identify post-nominal adjectives and place them in front of the corresponding nouns so that they are now pre-nominal .
Lexical Selection
We mask - out target tokens at random positions , and substitute them with tokens predicted by the Camembert language model ( Martin et al. , 2020 ) .
Task -Specific Learning Signal
Once we introduce different types of errors into the target - side sentences , the next step is to obtain pseudo - MQM scores for the altered sentence pairs .
Two key elements for computing MQM score are the length of a text , and its total error severity as follows : Pseudo-MQM = 100 ( 1 ? 5.0 * n error + S N ) where N indicates the length of given target sentence and n error denotes the number of errors introduced in it .
We assign 5.0 , the most frequent severity , to each perturbation that we make .
If an error severity score , S , is assigned to the sentence by human annotators , we add this score to compute the total error severity score .
Model Architecture
We use pretrained mBERT or XLM 10 as initial parameters .
The concatenation of a source sentence and its corresponding target sentence with special symbol tokens is taken as input : [ CLS ] source [ SEP ] target [ SEP ] .
We experiment with two strategies for obtaining sentence embeddings .
First , we feed a hidden state vector corresponding to [ CLS ] token ( h [ CLS ] ) to a linear layer to compute a sentence - level MQM prediction of ? : ? = W h [ CLS ] + b where W and b are the weight matrix and bias vector of the linear layer , respectively .
For the other 10 xlm-mlm-enfr - 1024 method , we use the concatenation of a mean-pooled source representation ( s ? R n ) , mean-pooled target representation ( t ? R n ) and their element- wise differences ( |s ? t| ?
R n ) in an attempt to enlarge the model capacity : ? = W ? ReLU ( W r ( s , t , |s ? t | ) + b r ) + b where W r ? R 3n?n and b r are the weight matrix and bias vector of an intermediate dimensionreducing layer , respectively , and n denotes the dimension of hidden vectors .
W and b are the weight matrix and bias vector of the final linear layer .
Task -Specific Training
We suggest that the pretraining objective should be similar to that of the downstream task in order to mitigate the pretrain- finetune discrepancy ( Yang et al. , 2019 ) , and fully leverage the erroneous sentence pairs that we generated .
For this task , both phases minimize the mean-squared loss function : l = 1 K K k=1 y k ? ? 2 .
Task -Specific Pretraining ( TSP )
We utilize Europarl parallel corpus ( English - French ) to pretrain our submitted models 11 .
To acquire high quality data , we carried out the following filtering processes : ( 1 ) language detection ( filtering out non-English sentences in the sourceside , and non-French sentences in the target-side ) , ( 2 ) length ratio filtering ( eliminating sentence pairs with length ratio greater than 1.8 ) .
We assume that the remaining sentence pairs do not contain any translation error .
Therefore , we assign the total error severity score of zero to these pairs before the augmentation .
About 15.2 million examples 12 are generated with the above-mentioned data augmentation techniques .
The detailed examples are provided in Table 1 .
Task -Specific Finetuning ( TSF )
The next step is to finetune our model using the augmented QE train data .
Unlike Europarl corpus , we can fully leverage the MQM scores originally assigned to the QE training dataset .
We found that performing the data augmentation with three categories ( Omitted Determiner , Omitted Preposition , and Wrong Preposition ) effectively improves the performance .
The original QE training sentence pairs represent about 5 % of 169,997 sentence pairs obtained from the data augmentation .
We also provide the augmented examples for QE training data in Table 2 .
Since the learning objective is identical to that of the pretraining phase , we can simply train the same model with the augmented downstream task data .
Document - Level MQM Score
We specify that the models are trained at sentencelevel , learning to predict the non-truncated version of MQM scores which could take a range between negative infinity and 100 ; this is to avoid potential information loss that could arise from the truncation .
Given a document , the document- level MQM score is computed from its sentence - level MQM predictions in a closed form .
Afterwards , we truncate negative values to zero .
Experimental Results
Sentence -Level Task Table 3 shows the Pearson correlation coefficient between the predicted z-normalized DA scores and the reference scores on the development set .
We note that the number of parameters for PATQUEST - mBERT ( 724 M ) is greater than that of PATQUEST - XLM ( 616 M ) models , resulting in the difference in the correlation scores .
Nevertheless , computing the arithmetic mean of the scores produced by these three models improves the performance ( PATQUEST - ensemble ) .
The final result on the QE test set is shown in Table 4 .
We observe that finetuning the model with the additional error-induced synthetic data improves the performance as well as ensembling the models .
Our final submitted system ( PATQUESTensemble ) finished 4th out of the 15 submitted systems 13 in the final ranking of the sentence - level QE task for English - German .
In order to train a generally applicable QE system , we did not make use of the data such as internal information from the NMT models and in-domain Wikipedia texts that could be extracted from the provided Wikipedia titles .
Document - Level Task
The validation results on development set are shown in Table 5 . Both PATQUEST -mBERT and PATQUEST - XLM models use representations from [ CLS ] token .
We build another two models , PATQUEST - mBERT variant 1 and 2 , using the concatenations of mean-pooled source representations , mean-pooled target representations , and their element - wise differences .
Table 6 shows the test results of our submitted PATQUEST models .
For PATQUEST - ensemble , we compute an average from the four models enumerated in Table 5 .
In Table 7 , the effectiveness of our training scheme and data augmentation techniques is illustrated via an ablation study .
Note that " Pretrained mBERT ( A ) " in the table refers to the mBERT model that is finetuned on the original QE data without any task -specific training .
Both TSP and TSF enhance the generalization ability of model .
Note that the mBERT model trained via TSP and TSF , " A + TSP + TSF " , is the same model as PATQUEST - mBERT which itself achieves a significant improvement over the baselines as shown in Table 6 .
Our final system ( PATQUEST - ensemble ) submitted for the document- level QE task , came 1st out of the three submitted systems 14 . Similar to our sentence - level system , our document - level system also did not utilize any internal information from the NMT models and in-domain Wikipedia data tailored to the benchmark .
Conclusion
In this paper , we present a task -specific pretraining scheme for the QE task .
Our pretraining objective is devised so that it is closely related ( Task 1 ) or identical ( Task 3 ) to the finetuning objective .
In addition , the models are exposed to abundant amount of error-induced translations generated from large parallel corpora , effectively alleviating the issue of data scarcity .
Our proposed models yield significant improvement over the baseline systems for the two tasks .
Figure 1 : 1 Figure1 : Overview of our approach for Task 1 and 3 .
