title
SOURCE : SOURce -Conditional Elmo-style Model for Machine Translation Quality Estimation
abstract
Quality estimation ( QE ) of machine translation ( MT ) systems is a task of growing importance .
It reduces the cost of post-editing , allowing machine - translated text to be used in formal occasions .
In this work , we describe our submission system in WMT 2019 sentence - level QE task .
We mainly explore the utilization of pre-trained translation models in QE and adopt a bi-directional translation - like strategy .
The strategy is similar to ELMo , but additionally conditions on source sentences .
Experiments on WMT QE dataset show that our strategy , which makes the pre-training slightly harder , can bring improvements for QE .
In WMT - 2019 QE task , our system ranked in the second place on En- De NMT dataset and the third place on En-Ru NMT dataset .
Introduction
The quality of machine translation systems have been significantly improved over the past few years ( Chatterjee et al. , 2018 ) , especially with the development of neural machine translation ( NMT ) models ( Sutskever et al. , 2014 ; Bahdanau et al. , 2014 ) .
Despite such inspiring improvements , some machine translated texts are still error-prone and unreliable compared to those by professional humans .
It is often desirable to have human experts perform post-editing on machine - translated text to achieve a balance between cost and correctness .
Correspondingly , we may also want to develop automatic quality estimation systems to judge the quality of machine translation outputs , leading to the development of the Machine Translation Quality Estimation task .
The task of QE aims to evaluate the output of a machine translation system without access to reference translations .
It would allow human experts to concentrate * equal contribution on translations that are estimated of low-quality , further reducing post-editing cost .
In this work , we focus on sentence - level QE and describe our submission to the WMT 2019 QE task .
Sentence - level QE aims to predict a score for the entire source-translation pair that indicates the effort required for further post-editing .
The goals of the task are two -fold : 1 ) to predict the required post-editing cost , measured in HTER ( Snover et al. , 2006 ) ; 2 ) to rank all sentence pairs in descending translation quality .
In previous works , including the participating systems in previous WMT shared tasks , there have been various methods to tackle this problem .
Traditional linear models are based on handcrafted features , while recent state - of - the - art systems adopt end-to - end neural models ( Kim and Lee , 2016 ; .
The neural systems are usually composed of two modules : the bottom part is an MT - like source - target encoding model pre-trained with large parallel corpora , stacked with a top-level QE scorer based on the neural features extracted by the bottom model .
Especially , adopted the " Bilingual Expert " model for pre-training the bottom model and obtained several best results in WMT 2018 .
In this work , we improve the " Bilingual Expert " model with a SOURce -Conditional ELMo-style ( SOURCE ) strategy : instead of predicting target words based on contexts from both sides , we train two conditioned language ( translation ) models , each restricted to context from one side only .
This harder setting may force the model to condition more on the source .
Experiments show that this strategy can bring improvements for QE .
2 System
Basic Framework
We follow previous works and adopt the end-toend styled model for the QE scoring task .
The overall system architecture is shown in Figure 1 .
The system consists of two components : 1 ) a pretrained MT module which learns the representations of the source and target sentences , 2 ) a QE scorer which takes the representations from the MT module as inputs and predicts the translation quality score .
The MT module is pre-trained on large parallel corpus .
It is trained to predict each token in the translated sentence by using the information in source sentence and tokens in the translated sentence .
Details of the model will be described in Section 2.2 .
In the QE scorer module , the problem can be cast as a regression task , where the QE score is predicted given the source and target sentences .
The original inputs are encoded by the pre-trained MT module , whose outputs are taken as input features for this module .
We basically follow the model architecture of .
For each token , a quality vector is formed as : q j = Concat ( ? ? z j , ? ?
z j , e t j?1 , e t j+1 , f mm j ) , ( 1 ) where ? ? z j , ? ?
z j are state vectors produced by the bidirectional Transformer , and e t j?1 , e t j+1 are embedding vectors .
The " mismatching feature " f mm j is formed by extracting the score corresponding to y j , the highest score in the distribution , their difference , and an indicator of whether y j has the highest score .
After this , the quality vectors are viewed as another sequence and encoded by the Bi-LSTM / Transformer Quality Estimator to predict the QE score .
The loss function for training is mean squared error which is typical for regression tasks .
Pre-trained Translation Models Bilingual Expert
We start with a short description for the model of .
The model can be seen as a token - level reconstructionstyled translation model : each target word y j is predicted given a source sentence and all other target words {. . . , y j?1 , y j+1 , . . . }.
This setting is different to the traditional MT scenario where only previous target words can be seen .
The model uses the encoder-decoder architecture .
An encoder is applied over the source tokens to obtain the contextual representations of the source sentence .
A bidirectional pair of decoders ( one forward and one backward ) are adopted to encode the target translation sentence , while conditioning on the source sentence via attention mechanism .
Formally , for source tokens {x 1 , . . . , x ms } and translation tokens {y 1 , . . . , y mt } , the forward and backward target representations { ? ? z 1 , .
{ ? ? z 1 , . . . , ?
z mt } are computed as : c 1 , . . . , c ms = Encoder( x 1 , . . . , x ms ) , ? ? z 1 , . . . , ?
z mt = ? ? ? Decoder(y 1 , . . . , y mt , c 1 , . . . , c ms ) , ? ? z 1 , . . . , ?
z mt = ? ? ? Decoder(y 1 , . . . , y mt , c 1 , . . . , c ms ) .
Both encoder and decoders use Transformer ( Vaswani et al. , 2017 ) as their backbone for its better performances in machine translation tasks .
After obtaining these representations , the model is trained with the token reconstruction crossentropy loss for each target token with contextual information from both sides : log p(y j | y i =j , x ) = softmax ( ff ( [ ? ? z j ?1 ; ? ? z j + 1 ] ) ) .
( 2 ) Here " ff " denotes a feed-forward layer .
Note that we cannot use representations that capture y j , therefore , we use the forward representation of the previous token ? ? z j ?1 and the backward representation of the next token ? ? z j + 1 .
SOURCE
In the Bilingual Expert model , each target token is predicted given all target tokens on both sides .
However , this training scheme makes too much information visible to the model , such that the model could predict the target word even without seeing the source sentence .
For example , we can easily infer that the missing word in " He loves playing and his favorite basketball player is Michael Jordan " is " basketball " .
In another words , too much visible information on the target side provides an inductive bias that pushes the model towards learning a bi-directional language model instead of a translation - like model , by omitting the information on the source sentence .
We want to force our model to exploit the relationship between the source tokens and target tokens .
Thus , we no longer make the words on both sides visible to our model at the same time .
Instead , we separate the two directions , so that the model must predict each target word depending only on the source sentence and target words on one side .
More specifically , we compute two losses , 1 and 2 .
The cross-entropy loss 1 is derived by predicting the target word y j based on the source sentence {x 0 , . . . , x ms } and left-side target words {y 0 , . . . , y j?1 }. Another cross-entropy loss 2 is derived by predicting y j based on the source sentence and rightside target words {y j+1 , . . . , y mt } .
This training scheme corresponds to the strategy used in ELMo ( Peters et al. , 2018 ) , but the difference is that here we condition on additional source information , hence the name SOURce -Conditional Elmo-style ( SOURCE ) model .
Another method to force the model to attend more to source is using BERT ( Devlin et al. , 2018 ) , which masks several words and try to predict those words at once .
Inspired by the work of Cross-lingual BERT ( Lample and Conneau , 2019 ) , we choose to use the structure as shown in Figure 2 .
It can reduce the information seen by the decoder and force it to condition more on the source sentence .
Due to limitations on time and computing resources , we did not manage to produce successful results using BERT .
This would be an interesting and promising direction to explore in future work .
From empirical results of SOURCE , we find that although the prediction accuracy on MT parallel data decreases , the final performance on QE increases significantly .
This shows that decreasing the visible information makes token - prediction more difficult , and forces the model to learn more useful structures from the data , which in turn becomes features of higher quality for the QE task .
Model Ensemble
We perform model ensembling by stacking , which means we use the prediction results of different set as new features , and train a simple regression model to predict the actual development set labels .
Finally , the regression model is applied on the predictions of different models on test set .
We use ridge regression here as the regression model .
We also use grid search and cross-validation to select the regularization weight for ridge regression .
We train both the pre-trained MT module and the QE scorer module with different hyperparameters to produce different models for ensembling .
For the pre-trained MT module , toggled hyper-parameters include number of layers , number of self-attention heads , learning rate , label - smoothing rate , warm - up steps , and dropout rates .
For the QE scorer module , toggled hyperparameters include number of layers , hidden size , percentage of augmented data , encoder type ( LSTM or Transformer ) , and dropout rate .
Experiments
Settings
Our system is evaluated on the WMT18/19 QE sentence - level task .
The main metric is the Pearson 's r correlation score between predicted scores and ground - truth HTER scores .
There are other metrics including Mean Absolute Error ( MAE ) and Root Mean Squared Error ( RMSE ) for scoring and the Spearman 's ? rank correlation for ranking .
We evaluate our models on datasets in the WMT 18/19 shared task with different translation systems : WMT -18 En-De-SMT , WMT -18/19 En-De-NMT , WMT -19 En-Ru-NMT .
For experiments on WMT - 19 data , we report results based on official evaluation results .
Data
For the parallel data used in pre-training of the MT module , we collect large-scale parallel corpora for En-De and En- Ru from the WMT - 18 translation task .
Officially pre-processed data 1 are utilized .
To make it compatible with QE data , we re-escaped certain punctuation tokens .
To reduce the corpus size , we further apply a more strict filtering step by discarding sentence pairs with too many overlapping words in their source and target sentences ( > 0.9 for En-De , > 0.5 for En-Ru ) .
Finally , we obtain 32.1 M EN - De and 7.8 M En- Ru sentence pairs and mix it with the training set of the QE data ( using post-edited sentences as target ) .
Our mixing strategy is to mix one copy of QE data for every 1 M of parallel data .
The statistics of the mixed parallel data and QE data are summarized in Table 1 . Following , we also prepare artificial data via round-trip translation Grundkiewicz , 2016 , 2017 ) .
Since the QE data are obtained with two kinds of translation systems : SMT and NMT , we also prepare two kinds of artificial data .
For simplicity , we take the back - translated corpus by the Edinburgh 's translation system , 2 which contains 3.6 M back - translated sentences for En-De and 1.9 M for En-Ru .
We further train a SMT system with Moses and decode the English sentence back to German .
For NMT , we simply take a pre-trained NMT system ( also Edinburgh 's system 3 ) for decoding .
Implementation
We implement our system from scratch in Python with TensorFlow ( Abadi et al. , 2015 ) and OpenNMT ( Klein et al. , 2017 ) WMT -19 En-De-NMT and En-Ru-NMT
The official result from WMT - 19 is shown in Table 3 .
Our system achieves the second place on En-De and the third place on En-Ru .
It is worth mentioning that due to the limitation of computational resource , we train far fewer models for En- Ru than En- De , so it is reasonable that our system performs much better on the En- De dataset .
Conclusion and Discussion Empirical results indicate that decreasing the visible information makes token - prediction more difficult , and forces the model to learn more useful structures from the data , which in turn becomes features of higher quality for the QE task .
The experimental results on WMT - 18 shows the effectiveness of our SOURCE model as well as our stacking ensemble strategy .
According to the official evaluation results on WMT - 19 dataset , our ensemble SOURCE modela chieves the second place on En- De dataset and the third place on En- Ru dataset .
We will explore the BERT - style structure to better condition on source sentences in the future .
Figure 2 : 2 Figure 2 : Illustration of reconstruction loss for the token " W i " in different pre-training strategies .
a) In Bilingual Expert , one reconstruction loss is computed for each token , conditioned on the entire target context provided by the Forward and Backward decoders .
b)
With Elmo- Style , it is equivalent to training bidirectional translation models .
Two reconstruction losses are computed for each token , each only depending on one side of the context .
c)
With BERT - Style , certain inputs are masked out ( colored in grey ) and a masked - LM is learned .
One reconstruction loss is computed for each masked token .
