title
Sentiment after Translation : A Case-Study on Arabic Social Media Posts
abstract
When text is translated from one language into another , sentiment is preserved to varying degrees .
In this paper , we use Arabic social media posts as stand - in for source language text , and determine loss in sentiment predictability when they are translated into English , manually and automatically .
As benchmarks , we use manually and automatically determined sentiment labels of the Arabic texts .
We show that sentiment analysis of English translations of Arabic texts produces competitive results , w.r.t. Arabic sentiment analysis .
We discover that even though translation significantly reduces the human ability to recover sentiment , automatic sentiment systems are still able to capture sentiment information from the translations .
Introduction Automatic sentiment analysis of text , especially social media posts , has a number of applications in commerce , public health , and public policy development .
However , a vast majority of prior research on automatic sentiment analysis has been on English texts .
Furthermore , many sentiment resources essential to automatic sentiment analysis ( e.g. , sentiment lexicons ) exist only in English .
Thus there is a growing need for effective methods for analyzing text from other languages such as Arabic and Chinese , especially posts on social media .
There has also been marked progress in automatic translation of texts , especially from other languages into English .
Thus , instead of building source - language specific sentiment analysis systems , one can translate the texts into English and use an English sentiment analysis system .
However , it is widely believed that aspects of sentiment may be lost in translation , especially in automatic translation .
Though , the extent of this loss , in terms of drop in accuracy of automatic sentiment systems remains undetermined .
This paper analyzes several methods available in annotating non-English texts for sentiment : ?
Use a source - language sentiment analysis system .
?
Run an English sentiment analysis system on manually created English translations of source language text .
?
Run an English sentiment analysis system on automatically generated English translations of source language text .
In our experiments , we use Arabic social media posts as a specific instance of the source language text .
We use state- of- the - art Arabic and English sentiment analysis systems as well as a state - of- theart Arabic- to - English translation system .
We outline the advantages and disadvantages of each of the methods listed above , and more importantly conduct experiments to determine accuracy of sentiment labels obtained using each of these methods .
As benchmarks we use manually and automatically determined sentiment labels of the Arabic tweets .
These results will help users determine methods best suited for their particular needs .
Along the way , we answer several research questions such as : 1 . What sentiment prediction accuracy is expected when Arabic blog posts and tweets are translated into English ( using the current stateof - art techniques ) , and then run through a stateof - the- art English sentiment analysis system ?
2 . How does this performance compare with that of a current state - of- the - art Arabic sentiment system ?
3 . What is the loss in sentiment predictability when translating Arabic text into English automatically vs. manually ?
4 . How difficult is it for humans to determine sentiment of automatically translated text ?
5 . When dealing with translated text , which is more accurate at determining the sentiment of Arabic text : ( 1 ) automatic sentiment analysis of the translated text , or ( 2 ) human annotation of the translated text for sentiment ?
The inferences drawn from these experiments do not necessarily apply to language pairs other than Arabic- English .
Languages can differ significantly in terms of characteristics that impact accuracy of an automatic sentiment analysis system .
Our goal here specifically is to understand sentiment predictability of Arabic dialectal text on translation .
However , a similar set of experiments can be used for other language pairs as well to determine the impact of translation on sentiment .
Through our experiments on two different datasets , we show that sentiment analysis of English translations of Arabic texts produces competitive results , w.r.t. Arabic sentiment analysis .
We also show that translation ( both manual and automatic ) introduces marked changes in sentiment carried by the text ; positive and negative texts can often be translated into texts that are neutral .
We also find that certain attributes of automatically translated text that mislead humans with regards to the true sentiment of the source text , do not seem to affect the automatic sentiment analysis system .
In the process of developing these experiments to study how translation alters sentiment , we created a state - of- the - art Arabic sentiment analysis system by porting NRC - Canada 's competition winning system ( Kiritchenko et al. , 2014 ) to Arabic .
We also created a substantial amount of sentiment labeled data pertaining to Arabic social media texts and their English translations which is made freely available .
1 1 http://www.purl.com/net/ArabicSentiment
This is the first such resource where text in one language and its translations into another language ( both manually and automatically produced ) are each manually labeled for sentiment .
Related Work 2.1 Sentiment Analysis of English Social Media Sentiment analysis systems have been applied to many different kinds of texts including customer reviews , newspaper headlines ( Bellegarda , 2010 ) , novels ( Boucouvalas , 2002 ; Mohammad and Yang , 2011 ) , emails ( Liu et al. , 2003 ; Mohammad and Yang , 2011 ) , blogs ( Neviarouskaya et al. , 2011 ) , and tweets ( Mohammad , 2012 ) .
Often these systems have to cater to the specific needs of the text such as formality versus informality , length of utterances , etc .
Sentiment analysis systems developed specifically for tweets include those by Go et al . ( 2009 ) , Pak and Paroubek ( 2010 ) , Agarwal et al . ( 2011 ) , and Thelwall et al . ( 2011 ) .
A survey by Mart?nez-C?mara et al. ( 2012 ) provides an overview of the research on sentiment analysis of tweets .
In the last two years , several shared tasks on sentiment analysis were organized by the Conference on Semantic Evaluation Exercises ( SemEval ) , which allowed for comparison of different approaches on common datasets from different domains ( Wilson et al. , 2013 ; Rosenthal et al. , 2014 ; Pontiki et al. , 2014 ) .
The NRC - Canada system ( Kiritchenko et al. , 2014 ) ranked first in these competitions , and we use it in our experiments .
Details of the system are described in Section 6 .
Sentiment Analysis of Arabic Social Media Sentiment analysis of Arabic social media texts has several challenges .
The text is often in a regional Arabic dialect rather than Modern Standard Arabic ( MSA ) .
Unlike MSA which is a standardized form of Arabic , dialectal Arabic is the spoken form of Arabic and lacks strict writing standards .
The text often includes words from languages other than Arabic and multiple scripts may be used to express Arabic and foreign words .
In addition , Arabic is a morphologically complex language , thus having a lexicon of word-sentiment associations that covers all different surface forms becomes a cumbersome task .
Negation in MSA is expressed through negation par-ticles , but in some dialects ( Egyptian ) it is expressed using suffixes at the end of the word .
We refer the reader to Mourad and Darwish ( 2013 ) for more details on these issues .
There have been a few studies tackling sentiment analysis of Arabic texts ( Ahmad et al. , 2006 ; Badaro et al. , 2014 ) .
The ones most closely related to our work are the studies of sentiment analysis of Arabic social media ( Al - Kabi et al. , 2013 ; El-Beltagy and Ali , 2013 ; Mourad and Darwish , 2013 ; Abdul- Mageed et al. , 2014 ) .
Here we review existing Arabic sentiment analysis systems that were designed specifically for Arabic social media datasets .
Abdul - Mageed et al. ( 2014 ) trained an SVM classifier on a manually labeled dataset and applied a two -stage classification that first separates subjective from objective sentences and then classifies the subjective into positive or negative instances .
The authors have compiled several datasets from multiple social media resources that include chatroom messages , tweets , forum posts , and Wikipedia Talk pages .
However , these resources have not been made publicly available yet .
Mourad and Darwish ( 2013 ) trained SVM and Naive Bayes classifiers on Arabic tweets annotated by two native Arabic speakers .
We compare our system 's performance to theirs in Section 7 .
Refaee and Rieser ( 2014 b ) manually annotated tweets for sentiment by two native Arabic speakers .
They used an SVM to classify tweets in a twostage approach , polar vs neutral , then positive vs. negative .
The authors shared their data with us and we test our system on their dataset .
However , the dataset they provided us is a larger superset than the one they had originally used ( Refaee and Rieser , 2014a ) .
Thus , the results of sentiment systems on the two sets are not directly comparable .
Multilingual Sentiment Analysis
Work on multilingual sentiment analysis has mainly addressed mapping sentiment resources from English into morphologically complex languages .
Mihalcea et al. ( 2007 ) used English resources to automatically generate a Romanian subjectivity lexicon using an English -Romanian dictionary .
The generated lexicon is then used to classify Romanian text .
Wan ( 2008 ) translated Chinese customer reviews to English using a machine trans-lation system .
The translated reviews are then classified with a rule- based system that relies on English lexicons .
A higher accuracy is achieved by using ensemble methods and combining knowledge from Chinese and English resources .
Balahur and Turchi ( 2014 ) conducted a study to assess the performance of statistical sentiment analysis techniques on machine - translated texts .
Opinionbearing phrases from the New York Times text corpus ( 2002 ) ( 2003 ) ( 2004 ) ( 2005 ) were automatically translated using publicly available machine -translation engines ( Google , Bing , and Moses ) .
Then , the accuracy of a sentiment analysis system trained on original English texts was compared to the accuracy of the system trained on automatic translations to German , Spanish , and French .
The authors concluded that the quality of machine translation is sufficient for sentiment analysis to be performed on automatically translated texts without a substantial loss in accuracy .
Contrary to that work , our study uses both manual and automatic translations as well as both manual and automatic sentiment assignments to systematically examine the effect of translation on sentiment .
Additionally , we deal with noisy social media texts as opposed to more polished news media texts .
There exists research on using sentiment analysis to improve machine translation ( Chen and Zhu , 2014 ) , but that is beyond the scope of this paper .
Method for Determining Sentiment Predictability on Translation
In order to systematically study the impact of translation on sentiment analysis , we propose the following experimental setup : ?
Identify or compile an Arabic social media dataset .
We will refer to it as Ar. ( Ar comes from the first two letter of Arabic . ) ?
Manually translate
Ar into English .
We will refer to these English translations as En( Manl .
Trans . ) [ Manl . is for manual , and Trans . is for translations . ] ?
Automatically translate
Ar into English .
We will refer to these English translations as En( Auto .
Trans . ) [ Auto. is for automatic . ] ?
Manually annotate Ar. for sentiment .
We will refer to the sentiment - labeled dataset as Ar( Manl .
Sent . ) ?
Manually annotate all English datasets [ En( Manl.
Trans . ) and En( Auto.
Trans . ) ] for sentiment , creating En( Manl .
Trans. , Manl.Sent . ) and En( Auto .
Trans . , Manl. Sent. ) , respectively . ?
Run a state- of- the- art Arabic sentiment analysis system on Ar , creating Ar( Auto .
Sent . ) ?
Run a state- of- the - art English sentiment analysis system on all the English datasets [ En( Manl.
Trans . ) and En( Auto.
Trans . ) ] , creating En( Manl .
Trans. , Auto. Sent . ) and En( Auto .
Trans. , Auto. Sent. ) , respectively .
Figure 1 depicts this setup .
Once the various sentiment - labeled datasets are created , we can compare pairs of datasets to draw inferences .
For example , comparing the labels for Ar( Manl.
Sent . ) and En( Manl .
Trans . , Manl. Sent . ) will show how different the sentiment labels tend to be when text is translated from Arabic to English .
The comparison will also show , for example , whether positive tweets tend to often be translated into neutral tweets , and to what extent .
The results will also show how feasible it is to first translate Arabic text into English and then use automatic sentiment analysis ( Ar( Manl .
Sent . ) vs. En( Auto .
Trans. , Auto.Sent . ) ) .
In Section 8 , we provide an analysis of several such comparisons for two different Arabic social media datasets .
DATA : Since manual translation of text from Arabic to English is a costly exercise , we chose , for our experiments , an existing Arabic social media dataset that has already been translated - the BBN Arabic- Dialect / English Parallel Text ( Zbib et al. , 2012 ) . 2
It contains about 3.5 million tokens of Arabic dialect sentences and their English translations .
We use a randomly chosen subset of 1200 Levantine dialectal sentences , which we will refer to as the BBN posts or BBN dataset , in our experiments .
Additionally , we also conduct experiments on a dataset of 2000 tweets originating from Syria ( a country where Levantine dialectal Arabic is commonly spoken ) .
These tweets were collected in May 2014 by polling the Twitter API .
We will refer to this dataset as the Syrian tweets or Syrian dataset .
Note , however , that manual translations of the Syrian dataset are not available .
The experimental setup described above involves several component tasks : generating translations manually and automatically ( Section 4 ) , manually annotating Arabic and English texts for sentiment ( Section 5 ) , automatic sentiment analysis of English texts ( Section 6 ) , and automatic sentiment analysis of Arabic texts ( Section 7 ) .
Generating English Translations
The BBN dialectal Arabic dataset comes with manual translations into English .
We generate automatic translations of the Arabic BBN posts and the Syrian tweets , by training a multi-stack phrase - based machine translation system to translate from Arabic to English .
Our in - house system is quite similar to Cherry and Foster ( 2012 ) .
This statistical machine translation ( SMT ) system is trained on data from OpenMT 2012 .
We preprocess the training data by segmenting the Arabic source side of the training data with MADA 3.2 ( Habash et al. , 2009 ) , using Penn Arabic Treebank ( PATB ) segmentation scheme as recommended by El Kholy and Habash ( 2012 ) .
The Arabic script is further normalized by converting different forms of Alif and Ya to bare Alif and dotless Ya .
The different forms are used interchangeably , and normalization decreases the sparcity of Arabic tokens and improves translation .
The English side of the training data is lowercased and tokenized by stripping punctuation marks .
We set the decoder 's stack size to 10000 and distortion limit to 7 .
We replace the out-of- vocabulary words in the translated text with UNKNOWN token ( which is shown to the annotators ) .
The decoder 's log-linear model is tuned with MIRA ( Chiang et al. , 2008 ; Cherry and Foster , 2012 ) .
A KN - smoothed 5 gram language model is trained on the English Gigaword and the target side of the parallel data .
Creating sentiment labeled data in Arabic and English Manual sentiment annotations were performed on the crowdsourcing platform CrowdFlower 3 for three BBN datasets and two Syrian datasets : 1 . Original Arabic posts ( BBN and Syria datasets ) , annotated by Arabic speakers .
2 . Manual English translations of Arabic posts , annotated by English speakers ( only for BBN dataset ) .
3 . Automatic English translations of Arabic posts ( BBN and Syria datasets ) , annotated by English speakers .
Each post was annotated by at least ten annotators and the majority sentiment label was chosen .
Table 1 shows the class distribution of sentiment labels in various datasets .
Observe from rows a and d that neutral tweets constitute only about 10 % of the data in both BBN and Syria datasets .
The Syrian tweets have a much higher percentage of negative posts , whereas in the BBN data , the percentages of positive and negative posts are comparable .
( Arabic tweets in general tend to be much more skewed to the negative class than Arabic blog post sentences . )
Rows b , c , and e show that translated texts tend to lose some of the sentiment information and there is a relatively higher percentage of neutral instances in the translated text than in the original text .
For each post , we determine the count of the most frequent annotation divided by the total number of annotations .
This score is averaged for all posts to determine the inter-annotator agreement shown in the last column of Table 1 .
We use this agreement score as benchmark to compare performance of automatic sentiment systems ( described below ) .
English Sentiment Analysis
We use the English- language sentiment analysis system developed by NRC - Canada ( Kiritchenko et al. , 2014 ) in our experiments .
This system obtained highest scores in two recent international competitions on sentiment analysis of tweets - SemEval - 2013 Task 2 and SemEval - 2014 Task 9 ( Wilson et al. , 2013 ; Rosenthal et al. , 2014 ) .
We briefly describe the system below ; for more details , we refer the reader to Kiritchenko et al . ( 2014 ) .
A linear-kernel Support Vector Machine ( Chang and Lin , 2011 ) classifier is trained on the available training data .
The classifier leverages a variety of surface -form , semantic , and sentiment lexicon features described below .
The sentiment lexicon features are derived from existing , generalpurpose , manual lexicons , namely NRC Emotion Lexicon ( Mohammad and Turney , 2010 ; Mohammad and Turney , 2013 ) , Bing Liu's Lexicon ( Hu and Liu , 2004 ) , and MPQA Subjectivity Lexicon ( Wilson et al. , 2005 ) , as well as automatically generated , tweet -specific lexicons , Hashtag Sentiment Lexicon and Sentiment140 Lexicon ( Kiritchenko et al. , 2014 ) . 4
Generating English Sentiment Lexicon Ablation experiments in showed that their sentiment system benefited most from the use of the Hashtag Sentiment Lexicon .
The lexicon was created as follows .
A list of 77 seed words , which are synonyms of positive and negative , was compiled from the Roget 's Thesaurus .
Then , the Twitter API was polled to collect tweets that had these words as hashtags .
A tweet is considered positive if it has a positive hashtag and negative if it has a negative hashtag .
For each term in the tweet set , a sentiment score is computed by measuring the PMI ( pointwise mutual information ) between the term and the positive and negative categories : SenScore ( w ) = P M I ( w , pos ) ? P M I ( w , neg ) ( 1 ) where w is a term in the lexicon .
P M I ( w , pos ) is the PMI score between w and the positive class , and P M I ( w , neg ) is the PMI score between w and the negative class .
A positive SenScore ( w ) suggests that the word is associated with positive sentiment and a negative score suggests that the word is associated with negative sentiment .
The magnitude indicates the strength of the association .
Pre-processing and Feature Generation
The following pre-processing steps are performed .
URLs and user mentions are normalized to http://someurl and @someuser , respectively .
Tweets are tokenized and part-of-speech tagged with the CMU Twitter NLP tool ( Gimpel et al. , 2011 ) .
Then , each tweet is represented as a feature vector .
The features : - Word and character ngrams ; - POS : # occurrences of each part- of-speech tag ; - Negation : # negated contexts .
Negation also affects the ngram features : a word w becomes w NEG in a negated context ; - Automatic sentiment lexicons :
For each token w occurring in a tweet , its sentiment score score ( w ) is used to compute : # tokens with score ( w ) = 0 ; the total score = w?tweet score ( w ) ; the maximal score = max w?tweet score ( w ) ; the score of the last token in the tweet .
- Manually created sentiment lexicons :
For each of the three manual sentiment lexicons , the following features are computed : the sum of positive and the sum of negative scores for tweet tokens in affirmative contexts and in negated contexts , separately .
7 Arabic Sentiment Analysis
Building an Arabic Sentiment System
We built an Arabic sentiment analysis system by reconstructing the NRC - Canada English system to deal with Arabic text .
It extracts the same feature set as described in Section 6.2 .
We also generated a word-sentiment association lexicon as described in Section 6.1 , but for Arabic words from Arabic tweets ( more details in sub-section below ) .
We preprocess Arabic text by tokenizing with CMU Twitter NLP tool to deal with specific tokens such as URLs , usernames , and emoticons .
Then we use MADA to generate lemmas .
Finally , we normalize different forms of Alif and Ya to bare Alif and dotless
Ya to decrease token sparcity in Arabic datasets .
Generating Arabic Sentiment Lexicon
We translated 77 positive and negative seed words used to generate the English NRC Hashtag Sentiment Lexicon into Arabic using Google Translate .
Among the several translations provided by it , we chose words that were less ambiguous and tended to have strong sentiment in Arabic texts .
To increase the coverage of our seed list , we manually added different inflections for these translations .
We polled the Twitter API for the period of June to August 2014 and collected tweets with #( keyword ) .
After filtering out duplicate tweets and retweets , we ended up with 163,944 positive unique tweets and 37,848 negative unique tweets .
Then for each word w , SenScore ( w ) was calculated just as described in Section 6.1 .
Evaluation
We tested the Arabic sentiment system on two existing Arabic datasets ( Mourad and Darwish ( 2013 ) ( MD ) and Refaee and Rieser ( 2014 a ) ( RR ) ) and two newly sentiment - annotated Arabic datasets ( BBN and Syria ) .
Table 2 shows results of ten-fold crossvalidation experiments on each of the datasets .
For MD and RR , the presented results are for the twoclass problem ( positive vs. negative ) to allow for comparison with prior published results .
For BBN and Syria , the results are shown for the case where the system has to identify one of three classes : positive , negative , or neutral .
Human agreement scores are shown where available .
Note that the accuracy of our system is higher than previously published results on the MD dataset .
The only previously published results on the RR dataset are on a small subset ( about 1000 instances ) for which Refaee and Rieser ( 2014a ) obtained an accuracy of 87 % .
The results in Table 2 are for a larger dataset and so not directly comparable .
Sentiment After Translation
Using the methods and systems described in Sections 4 , 5 , 6 , and 7 , we generated all the manually and automatically labeled datasets mentioned in Section 3's Experimental Setup .
Table 3 shows the distribution of positive , negative , and neutral classes in datasets that have been automatically labeled with sentiment .
These percentages can be compared with those in Table 1 ( rows a and d ) which show the true sentiment distribution in the BBN and Syria datasets .
Observe that the automatic system has difficulty in assigning neutral class to posts .
This is probably because of the small percentage ( about 10 % ) of neutral tweets in the training data .
Also notice that the system predominantly guesses negative , which is also a reflection of the distribution in the training data .
The strong bias to negatives is lessened in the English translations .
Main Result : Tables 4 and 5 show how similar the sentiment labels are across various pairs of datasets for the BBN posts and the Syrian posts , respectively .
For example , row a. in Table 4 shows the comparison between Arabic tweets that were manually annotated for sentiment and those that were automatically labeled for sentiment by our Arabic sentiment analysis system .
Column 2 shows the percentage of instances where the sentiment labels match across the two datasets being compared .
For row a. the match percentage of 63.89 % represents the accuracy of the automatic sentiment analysis system on the Arabic BBN posts .
Row b. shows the difference in labels when text is manually translated from Arabic to English , even though sentiment labeling in both Arabic and English is done manually .
Observe that the two labels match only 71.31 % of the time .
However , the agreement among human sentiment annotators on original Arabic texts was only 73.8 % .
So , the English translation does affect sentiment , but not dramatically .
Row c. shows results for when the manually translated text is run through an English sentiment analysis system and the labels are compared against Ar( Manl .
Sent . )
Observe that the match for this pair is 68.65 % , which is not too much lower than 71.31 % obtained by manual sentiment labeling .
This shows 6 : Examples where the automatic translation was annotated a sentiment different the sentiment of the original Arabic tweet , but whose original sentiment was correctly predicted by the English sentiment system .
The manual translations are also listed for reference .
One reason why the automatic sentiment analysis system correctly annotates several automatically translated instances ( where manual annotations of the translation may fail ) , is that the system can learn an appropriate model even from mistranslated text - especially when automatic translation makes consistent errors .
For example , ( Oh God grant victory to ) has been consistently translated to God forsake .
All tweets having this phrase are correctly annotated as positive by our system , but were marked negative by the human annotators .
Caveats :
The automatic systems employed in these experiments , i.e. , Arabic sentiment analysis , English sentiment analysis , and SMT systems , exhibit state - of - the - art performance ; nevertheless , further improvements are possible .
The Arabic sentiment system will benefit from extended sentiment lexicons and features derived specifically for the Arabic language .
The English sentiment analysis system can be further adapted to the peculiarities of machine - translated texts , which are notably different from regular English .
The current translation system has been trained on non-tweet data that results in a high percentage of out - of- vocabulary words on our datasets .
In our experiments , we assumed that all texts are written in Levantine dialect of the Arabic language .
However , tweets can have a mixture of dialects or even a mixture of languages ( e.g. , Arabic and English ) .
Addressing these factors will give even more insight on how sentiment is altered on translation , in specific contexts .
Conclusions
We presented a set of experiments to systematically study the impact of English translation ( manual and automatic ) on sentiment analysis of Arabic social media posts .
Our experiments show that automatic sentiment analysis of English translations ( even of automatic translations ) can lead to competitive results -results that are similar to that obtained by current state - of- the - art Arabic sentiment analysis systems .
Our results also show that automatic sentiment analysis of automatic translations outperforms the manual sentiment annotations of the automatically translated text .
This suggests that SMT errors impact human perception of sentiment markedly more than automatic sentiment systems .
This is an interesting avenue for future exploration .
We also show that translated texts tend to lose some of the sentiment information and there is a relatively higher percentage of neutral instances in the translated text than in the original dataset .
The resources created as part of this project ( Arabic sentiment lexicons , Arabic sentiment annotations of social media posts , and English sentiment annotations of their translations ) are made freely available .
5 Figure 1 : 1 Figure 1 : Experimental setup to determine the impact of translation on sentiment .
We compare sentiment labels between Ar( Manl .
Sent . ) ( shown in a shaded box ) and other datasets shown on the right side of the figure .
Ar ( Manl.
Sent . ) is the original Arabic text manually annotated for sentiment .
Table 1 : 1 Class distribution ( in percentage ) of the sentiment annotated datasets .
positive negative neutral agreement BBN data a. Ar( Manl.
Sent ) 41.50 47.92 10.58 73.80 b. En( Manl.
Trans . , Manl.Sent ) 35.00 43.25 21.75 68.00 c. En( Auto.
Trans . , Manl.Sent ) 36.17 36.50 27.34 65.70 Syria data d. Ar( Manl.
Sent ) 22.40 67.50 10.10 79.00 e. En( Auto.
Trans . , Manl.Sent ) 14.25 66.15 19.60 76.10
Table 2 : 2 Accuracy ( in percentage ) of sentiment analysis ( SA ) systems on various Arabic social media datasets .
Arabic Sentiment Labeled Dataset MD RR BBN
Syria sentiment classes pos , neg pos , neg pos , neg , neu pos , neg , neu number of instances
1111 2681 1199 2000 Most frequent class baseline 66.06 68.92 47.95 67.50 Human agreement benchmark - - 73.82 79.05 Mourad and Darwish Arabic SA system 72.50 - - - Our Arabic SA system 74.62 85.23 63.89 78.65 pos neg neu BBN data a. Ar( Auto.
Sent ) 39.78 60.05 0.17 b. En( Manl.
Trans . , Auto.Sent ) 43.12 55.63 1.25 c. En( Auto.
Trans . , Auto.Sent ) 42.87 56.05 1.08 Syria data d. Ar( Auto.
Sent ) 20.60 75.30 4.10 e. En( Auto.
Trans . , Auto.Sent ) 24.75 69.75 5.50
Table 3 : 3 Class distribution ( in percentage ) resulting from automatic sentiment analysis .
1 . Bad auto . translation : mistranslation of ambiguous words Post negative Auto .
Trans .
the minimum taught me that more relatives clock neutral Manl .
Trans .
Life has taught me that most of the relatives are scorpions negative Trans .
you 're still good in front of the leakage of water existed from time positive Manl .
Trans .
Expect more good to come , water has been leaking since a long time negative Table 2 .
Bad auto .
translation : mistranslation of ambiguous words Post positive Auto .
Trans .
i wish i live in a place not cut off by snow negative Manl .
Trans .
I wish I live in a place where snow never stops falling positive 3 . Bad auto .
translation : sarcasm is hard to translate Post negative Auto .
https://catalog.ldc.upenn.edu/LDC2012T09
http://www.crowdflower.com
http://www.purl.com/net/lexicons
http://www.purl.com/net/ArabicSentiment
