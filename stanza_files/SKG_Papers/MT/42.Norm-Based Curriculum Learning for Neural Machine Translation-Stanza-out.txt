title
Norm -Based Curriculum Learning for Neural Machine Translation
abstract
A neural machine translation ( NMT ) system is expensive to train , especially with highresource settings .
As the NMT architectures become deeper and wider , this issue gets worse and worse .
In this paper , we aim to improve the efficiency of training an NMT by introducing a novel norm-based curriculum learning method .
We use the norm ( aka length or module ) of a word embedding as a measure of 1 ) the difficulty of the sentence , 2 ) the competence of the model , and 3 ) the weight of the sentence .
The normbased sentence difficulty takes the advantages of both linguistically motivated and modelbased sentence difficulties .
It is easy to determine and contains learning -dependent features .
The norm- based model competence makes NMT learn the curriculum in a fully automated way , while the norm- based sentence weight further enhances the learning of the vector representation of the NMT .
Experimental results for the WMT '14 English - German and WMT '17 Chinese - English translation tasks demonstrate that the proposed method outperforms strong baselines in terms of BLEU score ( + 1.17 / +1.56 ) and training speedup ( 2.22x/3.33 x ) .
Introduction
The past several years have witnessed the rapid development of neural machine translation ( NMT ) based on an encoder-decoder framework to translate natural languages ( Kalchbrenner and Blunsom , 2013 ; Sutskever et al. , 2014 ; Bahdanau et al. , 2015 ) .
Since NMT benefits from a massive amount of training data and works in a cross-lingual setting , it becomes much hungrier for training time than other natural language processing ( NLP ) tasks .
Based on self-attention networks ( Parikh et al. , 2016 ; Lin et al. , 2017 ) , Transformer ( Vaswani et al. , 2017 ) has become the most widely used architecture for NMT .
Recent studies on improving Transformer , e.g. deep models equipped with up to 30layer encoders ( Bapna et al. , 2018 ; Wu et al. , 2019 ; Zhang et al. , 2019a ) , and scaling NMTs which use a huge batch size to train with 128 GPUs , face a challenge to the efficiency of their training .
Curriculum learning ( CL ) , which aims to train machine learning models better and faster ( Bengio et al. , 2009 ) , is gaining an intuitive appeal to both academic and industrial NMT systems .
The basic idea of CL is to train a model using examples ranging from " easy " to " difficult " in different learning stages , and thus the criterion of difficulty is vital to the selection of examples .
Zhang et al. ( 2018 ) summarize two kinds of difficulty criteria in CL for NMT : 1 ) linguistically motivated sentence difficulty , e.g. sentence length , word frequency , and the number of coordinating conjunctions , which is easier to obtain ( Kocmi and Bojar , 2017 ; Platanios et al. , 2019 ) ; 2 ) model - based sentence difficulty , e.g. sentence uncertainties derived from independent language models or the models trained in previous time steps or epochs , which tends to be intuitively effective but costly ( Zhang et al. , 2017 ; Zhang et al. , 2019 b ; Zhou et al. , 2020 ) .
In this paper , we propose a novel norm-based criterion for the difficulty of a sentence , which takes advantage of both model - based and linguistically motivated difficulty features .
We observe that the norms of the word vectors trained on simple neural networks are expressive enough to model the two features , which are easy to obtain while possessing learning - dependent features .
For example , most of the frequent words and context- insensitive rare words will have vectors with small norms .
We know that .
Unlike existing CL methods for NMT , relying on a hand-crafted curriculum arrangement ( Zhang et al. , 2018 ) or a task - dependent hyperparameter ( Platanios et al. , 2019 ) , the proposed normbased model competence enables the model to arrange the curriculum itself according to its ability , which is beneficial to practical NMT systems .
We also introduce a novel paradigm to assign levels of difficulty to sentences , as sentence weights , into the objective function for better arrangements of the curricula , enhancing both existing CL systems and the proposed method .
Empirical results for the two widely - used benchmarks show that the proposed method provides a significant performance boost over strong baselines , while also significantly speeding up the training .
The proposed method requires slightly changing the data sampling pipeline and the objective function without modifying the overall architecture of NMT , thus no extra parameters are employed .
Background NMT uses a single large neural network to construct a translation model that translates a source sentence x into a target sentence y.
During training , given a parallel corpus D = { x n , y n } N n=1 , NMT aims to maximize its log-likelihood : ? = L( D ; ? 0 ) = arg max ?
0 N n=1 log P ( y n |x n ; ? 0 ) ( 1 ) where ?
0 are the parameters to be optimized during the training of the NMT models .
Due to the intractability of N , the training of NMT employs mini-batch gradient descent rather than batch gradient descent or stochastic gradient descent , as follows : B 1 , ? ? ? , B t , ? ? ? , B T = sample ( D ) ( 2 ) ? = L( B T ; L( B T ?1 ; ? ? ? L( B 1 , ? 0 ) ) ) ( 3 ) where T denotes the number of training steps and B t denotes the tth training batch .
In the training of the tth mini-batch , NMT optimizes the parameters ?
t?1 updated by the previous mini-batch .
CL supposes that if mini-batches are bucketed in a particular way ( e.g. with examples from easy to difficult ) , this would boost the performance of NMT and speed up the training process as well .
That is , upgrading the sample ( ? ) to B * 1 , ? ? ? , B * t , ? ? ? , B * T = sample * ( D ) ( 4 ) where the order from easy to difficult ( i.e. B * 1 ? B * T ) can be : 1 ) sentences with lengths from short to long ; 2 ) sentences with words whose frequency goes from high to low ( i.e. word rarity ) ; and 3 ) uncertainty of sentences ( from low to high uncertainties ) measured by models trained in previous epochs or pre-trained language models .
Table 1 shows the sentences of the training curricula provided by vanilla Transformer and the proposed method .
3 Norm-based Curriculum Learning
Norm- based Sentence Difficulty
Most NLP systems have been taking advantage of distributed word embeddings to capture the syntactic and semantic features of a word ( Turian et al. , 2010 ; Mikolov et al. , 2013 ) .
A word embedding ( vector ) can be divided into two parts : the norm and the direction : w = ||w | | norm ? w ||w| | direction ( 5 ) In practice , the word embedding , represented by w , is the key component of a neural model ( Liu et al. , 2019 a , b ) , and the direction w | |w | | can also be used to carry out simple word / sentence similarity and relation tasks .
However , the norm | | w | | is rarely considered and explored in the computation .
Surprisingly , the norm which is simply derived from a single model parameter , can also capture delicate features during the optimization of a model .
observe that in the word embedding model ( Mikolov et al. , 2013 ) , the word vector norm increases with a decrease of the word frequency , while polysemous words , such as " May " , tend to have an average norm weighted over its various contexts .
further conduct controlled experiments on word vector norm and find that besides the word frequency , the diversities of the context of the word are also a core factor to determine its norm .
The vector of a context- insensitive word is assigned a higher norm .
In other words , if a word is usually found in specific contexts , it should be regarded as a significant word ( Luhn , 1958 ) .
The word embedding model can exactly assign these significant words higher norms , even if some of them are frequent .
The sentences consisting of significant words share fewer commonalities with other sentences , and thus they can also be regarded as difficult - to - learn examples .
Figure 1 shows the relationship between the word vector norm and the word frequency in the English data of the WMT '14 English - German translation task .
The results stay consistent with prior works , showing that the rare words and significant words obtain a high norm from the word embedding model .
Motivated by these works and our preliminary experimental results , we propose to use the word vector norm as a criterion to determine the difficulty of a sentence .
Specifically , we first train a simple word embedding model on the training corpus , and then obtain an embedding matrix E w2v .
Given a source sentence x = x 1 , ? ? ? , x i , ? ? ? , x I , it can be mapped into distributed representations x 1 , ? ? ? , x i , ? ? ? , x I through E w2v .
The norm - based sentence difficulty is calculated as d( x ) = I i=1 ||x i || ( 6 ) Long sentences and sentences consisting of rare words or significant words tend to have a high sentence difficulty for CL .
The proposed norm-based difficulty criterion has the following advantages : 1 ) It is easy to compute since the training of a simple word embedding model just need a little time and CPU resources ;
2 ) Linguistically motivated features , such as word frequency and sentence length , can be effectively modeled ;
3 ) Model- based features , such as learning -dependent word significance , can also be efficiently captured .
Norm-based Model Competence
Besides finding an optimal sentence difficulty criterion , arranging the curriculum in a reasonable order is equally important .
As summarized by Zhang et al . ( 2019 b ) , there are two kinds of CL strategies : deterministic and probabilistic .
From their observations , probabilistic strategies are superior to deterministic ones in the field of NMT , benefiting from the randomization during mini-batch training .
Without loss of generality , we evaluate our proposed norm- based sentence difficulty with a typical probabilistic CL framework , that is , competencebased CL ( Platanios et al. , 2019 ) .
In this framework , a notion of model competence is defined which is a function that takes the training step t as input and outputs a competence value from 0 to 1 : 1 c ( t ) ? ( 0 , 1 ] = min( 1 , t 1 ? c 2 0 ? t + c 2 0 ) ( 7 ) where c 0 = 0.01 is the initial competence at the beginning of training and ?
t is a hyperparameter determining the length of the curriculum .
For the sentence difficulty , they use cumulative density function ( CDF ) to transfer the distribution of sentence difficulties into ( 0 , 1 ] :
The score of difficult sentences tends to be 1 , while that of easy sentences tends to be 0 .
The model uniformly samples curricula whose difficulty is lower than the model competence at each training step , thus making the model learn the curriculum in a probabilistic way .
d( x n ) ? ( 0 , 1 ] = CDF ( { d( x n ) } N n=1 ) n ( 8 ) 0 20K 40 K 60K 80K
One limitation of competence - based CL is that the hyperparameter ?
t is task- dependent .
In detail , for each system , it needs to first train a vanilla baseline model and then use the step reaching 90 % of its final performance ( BLEU score ) as the value of the length hyperparameter .
As we know , training an NMT baseline is costly , and arbitrarily initializing the value might lead to an unstable training process .
To alleviate this limitation and enable NMT to learn curricula automatically without human interference in setting the hyperparameter , it is necessary to find a way for the model to determine the length of a curriculum by itself , according to its competence , which should be independent of the specific task .
To this aim , we further introduce a norm-based model competence criterion .
Different from the norm-based difficulty using the word vector norm , the norm- based model competence uses the norm of the source embedding of the NMT model E nmt : m t = ||E nmt t || ( 9 ) where m t denotes the norm of E nmt at the tth training step , and we write m 0 for the initial value of the norm of E nmt .
This proposal is moti-vated by the empirical results shown in Figure 2 , where we show the BLEU scores and the norms of the source embedding matrix at each checkpoint of a vanilla Transformer model on the WMT '14 English - German translation task .
We found the trend of the growth of the norm m t to be very similar to that of the BLEU scores .
When m t stays between 15 K to 20K , which is about from twice to three times larger than the initial norm m 0 , both the growth of the norm and that of the BLEU score have slowed down .
It shows strong clues that m t is a functional metric to evaluate the competence of the model , and thus we can avoid the intractability of ?
t in Equation 7 : ?( t ) = min( 1 , ( m t ? m 0 ) 1 ? c 2 0 ? m m 0 + c 2 0 ) ( 10 ) where ?
m is a task - independent hyperparameter to control the length of the curriculum .
With this criterion , the models can , by themselves , fully automatically design a curriculum based on the feature ( norm ) .
At the beginning of the training , there is a lower m t , so the models tend to learn with an easy curriculum .
But with an increase of the norm m t , more difficult curricula will be continually added into the learning .
Norm- based Sentence Weight
In competence - based CL , the model uniformly samples sentences whose difficulty level is under the model competence , and then learns with the samples equally .
As a result , those simple sentences with low difficulty ( e.g. d( x ) < 0.1 ) are likely to be repeatedly used in the model learning .
This is somewhat counterintuitive and a waste of computational resources .
For example , when students are able to learn linear algebra , they no longer need to review simple addition and subtraction , but can keep the competence during the learning of hard courses .
On the other hand , a difficult ( long ) sentence is usually made up of several easy ( short ) sentences .
Thus , the representations of easy sentences can also benefit from the learning of difficult sentences .
To alleviate this limitation of competence - based CL and further enhance the learning from the curriculum of different levels of difficulty , we propose a simple yet effective norm- based sentence weight : w(x , t ) = ( d( x ) ?( t ) ) ?w ( 11 ) Compute norm- based model competence ?( t ) using Eq. 9 and 10 .
5 : Generate training batch B * t uniformly sampled from { x , y | d( x ) < ?( t ) , x , y ? D}.
6 : Compute norm- based length weight W = { w ( x , t ) | x , y ? B * t } using Eq. 11 .
7 : Update ? with batch loss E x,y ?B * t calculated by W and Eq. 12 .
8 : end for 9 : return ? where ? w is the scaling hyperparameter smoothing the weight , d( x ) is the norm- based sentence difficulty , and ?( t ) is the model competence .
For each training step t , or each model competence ?( t ) , the weight of a training example w(x , t ) is included in its objective function : l( x , y , t ) = ? log P ( y|x ) w ( x , t ) ( 12 ) where l( x , y , t ) is the training loss of an example x , y at the tth training step .
With the use of sentence weights , the models , at each training step , tend to learn more from those curricula whose difficulty is close to the current model competence .
Moreover , the models still benefit from the randomization of the mini-batches since the length weight does not change the curriculum sampling pipeline .
Overall Learning Strategy
Algorithm 1 illustrates the overall training flow of the proposed method .
Besides the component and training flow of vanilla NMT models , only some low-cost operations , such as matrix multiplication , have been included in the data sampling and objective function , allowing an easy implementation as a practical NMT system .
We have also found , empirically , that the training speed of each step is not influenced by the introduction of the proposed method .
Experiments
Data and Setup
We conducted experiments on the widely used benchmarks , i.e. the medium-scale WMT '14 English - German ( En- De ) and the large-scale WMT '17 Chinese -English ( Zh-En ) translation tasks .
For En- De , the training set consists of 4.5 M sentence pairs with English words and 113M German words .
The development is newstest13 and the test set is newstest14 .
For the Zh-En , the training set contains roughly 20 M sentence pairs .
The development is newsdev2017 and the test set is newstest2017 .
The Chinese data were segmented by jieba , 2 while the others were tokenized by the tokenize .
perl script from Moses .
3
We filtered the sentence pairs with a source or target length over 200 tokens .
Rare words in each data set were split into sub-word units ( Sennrich et al. , 2016 ) .
The BPE models were trained on each language separately with 32 K merge operations .
All of the compared and implemented systems are the base Transformer ( Vaswani et al. , 2017 ) using the open-source toolkit Marian ( Junczys - Dowmunt et al. , 2018 ) .
4
We tie the target input embedding and target output embedding ( Press and Wolf , 2017 ) .
The Adam ( Kingma and Ba , 2015 ) optimizer has been used to update the model parameters with hyperparameters ? 1 = 0.9 , ? 2 = 0.98 , ? = 10 ?9 . We use the variable learning rate proposed by Vaswani et al . ( 2017 ) with 16 K warm up steps and a peak learning rate 0.0003 .
We employed FastText ( Bojanowski et al. , 2017 ) 5 with its default settings to train the word embedding model for calculating the norm-based sentence difficulty ; an example is given in Figure 1 .
The hyperparameters ? m and ?
w controlling the norm-based model competence and norm-based sentence weight were tuned on the development set of En-De , with the value of 2.5 and 0.5 , respectively .
To test the adaptability of these two hyperparameters , we use them directly for the Zh-En translation task without any tuning .
We compare the proposed methods with the re-implemented During training , the mini-batch contains nearly 32 K source tokens and 32 K target tokens .
We evaluated the models every 2.5 K steps , and chose the best performing model for decoding .
The maximum training step was set to 100K for En-De and 150 K for Zh-En .
During testing , we tuned the beam size and length penalty ( Wu et al. , 2016 ) on the development data , using a beam size of 6 and a length penalty of 0.6 for En- De , and a beam size of 12 and a length penalty of 1.0 for Zh-En .
We report the 4 - gram BLEU ( Papineni et al. , 2002 ) score given by the multi-bleu.
perl script .
The codes and scripts of the proposed norm- based CL and our re-implemented competence - based CL are freely available at https://github.com/NLP2CT/ norm-nmt .
Main Results
Table 2 shows the results of the En- De translation task in terms of BLEU scores and training speedup .
Models ( 1 ) to ( 4 ) are the existing baselines of this translation benchmark .
Model ( 5 ) is our implemented base Transformer with 100K training steps , obtaining 27.64 BLEU scores on the test set .
By applying the competence - based CL with its proposed sentence rarity and square root competence function , i.e. model ( 6 ) , it reaches the performance of model ( 5 ) using 60 K training steps and also gets a better BLEU score .
For the proposed method , we first show the performance of each sub-module , that is : model ( 7 ) , which uses the norm-based model competence instead of the square root competence of model ( 6 ) ; model ( 8 ) , which uses the proposed norm- based sentence complexity instead of the sentence rarity of model ( 6 ) ; and model ( 9 ) , which adds the norm- based sentence weight to model ( 6 ) .
The results show that after applying each sub-module individually , both the BLEU scores and the learning efficiency are further enhanced .
Model ( 10 ) shows the results combining the three proposed norm- based methods for CL , i.e. the norm- based sentence difficulty , model competence , and sentence weight .
We call the combination of the proposed method norm- based CL .
It shows its superiority in the BLEU score , which has an increase of 1.17 BLEU scores compared to the Trans -
ID Model Dev. Test Updates Speedup Existing Baselines 11 Base Transformer ( Ghazvininejad et al. , 2019 ) - 23.74 -- 12 Big Transformer ( Ghazvininejad et al. , 2019 ) - 24.65 -- Our Implemented Baselines 13 Base Transformer ( Vaswani et al. , 2017 ) 22.29 23.69 150.0K 1.00x 14 13 + Competence - based CL ( Platanios et al. , 2019 ) former baseline , as well as speeding up the training process by a factor of 2.22 .
One can note that all of our implemented systems have the same number of model parameters ; besides , the training step of each model involves essentially the same execution time , resulting in a deployment -friendly system .
Effect of ? m and ? w Table 3 shows the effects of the two hyperparameters used in the proposed method .
For each experiment , we kept the other parameters unchanged and only adjusted the hyperparameter .
For ? m , controlling curriculum length , the higher the value , the longer the curriculum length .
When setting ?
m to 2.5 with the curriculum length of nearly 29 K steps , it achieves the best performance .
For ? w , the scaling sentence weight of the objective function , one achieves satisfactory results with a value of 0.5 , which maintains the right balance between the learning of simple and hard examples .
Results on the Large-scale NMT
Although the hyperparameters ? m and ?
w have been sufficiently validated on the En- De translation , the generalizability of the model trained using these two hyperparameters is still doubtful .
To clear up any doubts , we further conducted the experiments on the large-scale Zh- En translation without tuning these two hyperparameters , that is , directly using ? m = 2.5 and ? w = 0.5 .
Specifically , the only difference is the use of a large number of training steps in Zh-En , namely , 150K , for the purpose of better model fitting .
We first confirm the effectiveness of competencebased CL in large-scale NMT , that is model ( 14 ) , which shows both a performance boost and a training speedup .
Model ( 15 ) , which trains NMT with the proposed norm- based CL , significantly improves the BLEU score to 25.25 ( + 1.56 ) and speeds up the training by a factor of 3.33 , showing the generalizability of the proposed method .
The results
Source Last year a team from the University of Lincoln found that dogs turn their heads to the left when looking at an aggressive dog and to the right when looking at a happy dog .
Reference Letztes Jahr fand ein Team der Universit ? t von Lincoln heraus , dass Hunde den Kopf nach links drehen , wenn sie einen aggressiven
Hund ansehen , und nach rechts , wenn es sich um einen zufriedenen Hund handelt .
Vanilla
Im vergangenen Jahr stellte ein Team der Universit ?t Lincoln fest , dass Hunde beim Blick auf einen aggressiven
Hund nach links abbiegen .
NBCL Letztes Jahr fand ein Team von der Universit ? t von Lincoln heraus , dass Hunde ihren Kopf nach links drehen , wenn sie einen aggressiven
Hund sehen und rechts , wenn sie einen gl ?cklichen Hund sehen .
show that large-scale NMT obtains a greater advantage from an orderly curriculum with enhanced representation learning .
The proposed norm- based CL enables better and faster training of large-scale NMT systems .
Effect of Sentence Weight
As discussed in Section 3.3 , competence - based CL over-trains on the simple curriculum , which might lead a bias in the final translation .
To verify this , we quantitatively analysed the translations generated by different systems .
Figure 3
The results confirm our above assumption , although competence - based CL performs much better in translating simple sentences due to its overtraining , the translation of sentences of medium difficulty worsens .
However , the norm- based CL benefits from the norm- based sentence weight , successfully alleviating this issue by applying a scale factor to the loss of simple curricula in the objective function , leading to a consistently better translation performance over the vanilla Transformer .
To further prove the effectiveness of the proposed norm- based sentence weight , we explore the model integrating norm- based sentence weight with competence - based CL , and find that it can also strike the right balance between translating simple and medium -difficulty sentences .
A Case Study
Table 5 shows an example of a translation of a difficult sentence consisting of several similar clauses in the norm-based difficulty bucket .
We observe that the translation by the vanilla model omits translating the last clause , but NMT with norm- based CL translates the entire sentence .
The proposed method enhances the representation learning of NMT , leading to better understandings of difficult sentences , thus yielding better translations .
Related Work
The norm of a word embedding has been sufficiently validated to be highly correlated with word frequency .
and train a simple word embedding model ( Mikolov et al. , 2013 ) on a monolingual corpus , and find that the norm of a word vector is relevant to the frequency of the word and its context sensitivity : frequent words and words that are insensitive to context will have word vectors of low norm values .
For language generation tasks , especially NMT , there is still a correlation between word embedding and word frequency .
Gong et al. ( 2018 ) observe that the word embedding of NMT contains too much frequency information , considering two frequent and rare words that have a similar lexical meaning to be far from each other in terms of vector distance .
regard this issue as a representation degeneration issue that it is hard to learn expressive representations of rare words due to the bias in the objective function .
Nguyen and Chiang ( 2019 ) observe a similar issue during NMT decoding : given two word candidates with similar lexical meanings , NMT chooses the more frequent one as the final translation .
They attribute this to the norm of word vector , and find that target words with different frequencies have different norms , which affects the NMT score function .
In the present paper , for the sake of obtaining an easy and simple word vector norm requirement , we use the norm derived from a simple word embedding model .
In the future , we would like to test norms of various sorts .
There are two main avenues for future research regarding CL for NMT : sentence difficulty criteria and curriculum training strategies .
Regarding sentence difficulty , there are linguistically motivated features ( Kocmi and Bojar , 2017 ; Platanios et al. , 2019 ) and model - based features ( Zhang et al. , 2017 ; Zhang et al. , 2019 b ; Zhou et al. , 2020 ) .
Both types of difficulty criteria have their pros and cons , while the proposed norm- based sentence difficulty takes the best of both worlds by considering simplicity and effectiveness at the same time .
Regarding the training strategy , both deterministic ( Zhang et al. , 2017 ; Kocmi and Bojar , 2017 ) and probabilistic strategies ( Platanios et al. , 2019 ; Zhang et al. , 2019 b ; can be better than the other , depending on the specific scenario .
The former is easier to control and explain , while the latter enables NMT to benefit from the randomization of mini-batch training .
However , both kinds of strategy need to carefully tune the CL-related hyperparameters , thus making the training process somewhat costly .
In the present paper , we have designed a fully automated training strategy for NMT with the help of vector norms , removing the need for manual setting .
Conclusion
We have proposed a novel norm-based curriculum learning method for NMT by : 1 ) a novel sentence difficulty criterion , consisting of linguistically motivated features and learning - dependent features ; 2 ) a novel model competence criterion enabling a fully automatic learning framework without the need for a task - dependent setting of a feature ; and 3 ) a novel sentence weight , alleviating any bias in the objective function and further improving the representation learning .
Empirical results on the medium - and large-scale benchmarks confirm the generalizability and usability of the proposed method , which provides a significant performance boost and training speedup for NMT .
Figure 1 : 1 Figure1 : Word vector norm of the word embedding model trained on the WMT '14 English - German ( source side ) training data .
The x-axis is the word frequency , ranked in descending order .
Rare words and significant words have higher norms .
