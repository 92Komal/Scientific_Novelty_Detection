title
Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing
abstract
We frame the task of machine translation evaluation as one of scoring machine translation output with a sequence - to-sequence paraphraser , conditioned on a human reference .
We propose training the paraphraser as a multilingual NMT system , treating paraphrasing as a zero-shot translation task ( e.g. , Czech to Czech ) .
This results in the paraphraser 's output mode being centered around a copy of the input sequence , which represents the best case scenario where the MT system output matches a human reference .
Our method is simple and intuitive , and does not require human judgements for training .
Our single model ( trained in 39 languages ) outperforms or statistically ties with all prior metrics on the WMT 2019 segment - level shared metrics task in all languages ( excluding Gujarati where the model had no training data ) .
We also explore using our model for the task of quality estimation as a metric-conditioning on the source instead of the reference - and find that it significantly outperforms every submission to the WMT 2019 shared task on quality estimation in every language pair .
Word - level paraphraser log probabilities H( out | in ) sBLEU LASER
Copy Jason went to school at the University of Madrid . < EOS > -0 .
Introduction Machine Translation ( MT ) systems have improved dramatically in the past several years .
This is largely due to advances in neural MT ( NMT ) methods , but the pace of improvement would not have been possible without automatic MT metrics , which provide immediate feedback on MT quality without the time and expense associated with obtaining human judgments of MT output .
However , the improvements that existing automatic metrics helped enable are now causing the correlation between human judgments and automatic metrics to break down ( Ma et al. , 2019 ; Mathur et al. , 2020 ) especially for BLEU ( Papineni et al. , 2002 ) , which has been the de facto standard For example , the MT system output " Hi world " conditioned on the human reference " Hello world " is found to have token probabilities [ 0.3 , 0.6 ]
. metric since its introduction almost two decades ago .
The problem currently appears limited to very strong systems , but as hardware , modeling , and available training data improve , it is likely BLEU will fail more frequently in the future .
This could prove extremely detrimental if the MT community fails to adopt an improved metric , as good ideas could quietly be discarded or rejected from publication because they do not correlate with BLEU .
In fact , this may already be happening .
We propose using a sentential , sequence - tosequence paraphraser to force-decode and score MT outputs conditioned on their corresponding human references .
Our model implicitly represents the entire ( exponentially large ) set of potential paraphrases of a sentence , both valid and invalid ; by " querying " the model with a particular system out - put , we can use the model score to measure how well the system output paraphrases the human reference translation .
Our model is not trained on any human quality judgements , which are not available in many domains and / or language pairs .
The best possible MT output is one which perfectly matches a human reference ; therefore , for evaluation , an ideal paraphraser would be one with an output distribution centered around a copy of its input sentence .
We denote such a model a " lexically / syntactically unbiased paraphraser " to distinguish it from a standard paraphraser trained to produce output which conveys the meaning of the input while also being lexically and / or syntactically different from it .
For this reason , we propose using a multilingual NMT system as an unbiased paraphraser by treating paraphrasing as zero-shot " translation " ( e.g. , Czech to Czech ) .
We show that a multilingual NMT model is much closer to an ideal lexically / syntactically unbiased paraphraser than a generative paraphraser trained on synthetic paraphrases .
It also allows a single model to work in many languages , and can be applied to the task of " Quality estimation ( QE ) as a metric " ( Fonseca et al. , 2019 ) by conditioning on the source instead of the reference .
Figure 1 illustrates our method , which we denote Prism ( Probability is the metric ) .
We train a single model in 39 languages and show that it : ?
Outperforms or ties with prior metrics and several contrastive neural methods on the segment- level WMT 2019 MT metrics task in every language pair ; 1 ?
Is able to discriminate between very strong neural systems at the system level , addressing a problem raised at WMT 2019 ; and ?
Significantly outperforms all QE metrics submitted to the WMT 2019 QE shared task Finally , we contrast the effectiveness of our model when scoring MT output using the source vs the human reference .
We observe that human references substantially improve performance , and , crucially , allow our model to rank systems that are substantially better than our model at the task of translation .
This is important because it establishes that our method does not require building a state - of- theart multilingual NMT model in order to produce a state - of - the - art MT metric capable of evaluating state - of - the - art MT systems .
1 Except for Gujarati , where we had no training data .
We release our model , metrics toolkit , and preprocessed training data .
2 2 Related Work MT Metrics Early MT metrics like BLEU ( Papineni et al. , 2002 ) and NIST ( Doddington , 2002 ) use token - level n-gram overlap between the MT output and the human reference .
Overlap can also be measured at the character level ( Popovi ? , 2015 ( Popovi ? , , 2017 or using edit distance ( Snover et al. , 2006 ) .
Many metrics use word- and / or sentencelevel embeddings , including ReVal ( Gupta et al. , 2015 ) , RUSE ( Shimanaka et al. , 2018 ) , WMDO ( Chow et al. , 2019 ) , and ESIM ( Mathur et al. , 2019 ) . MEANT ( Lo and Wu , 2011 ) and MEANT 2.0 ( Lo , 2017 ) measure similarity between semantic frames and role fillers .
State - of- the- art methods including YiSi ( Lo , 2019 ) and BERTscore ( Zhang et al. , 2019 ( Zhang et al. , , 2020 rely on contextualized embeddings ( Devlin et al. , 2019 ) trained on large ( non- parallel ) corpora .
BLEURT ( Sellam et al. , 2020 ) applies fine tuning of BERT , including training on prior human judgements .
In contrast , our work exploits parallel bitext and does n't require training on human judgements .
Paraphrase Databases
Prior work explored using parallel bitext to identify phrase level paraphrases ( Bannard and Callison - Burch , 2005 ; Ganitkevitch et al. , 2013 ) including bitext in multiple language pairs ( Ganitkevitch and Callison - Burch , 2014 ) .
Paraphrase tables were , in turn , used in MT metrics to reward systems for paraphrasing words ( Banerjee and Lavie , 2005 ) or phrases ( Zhou et al. , 2006 ; Denkowski and Lavie , 2010 ) from the human reference .
Our work can be viewed as extending this idea to the sentence level , without having to enumerate the millions or billions of paraphrases ( Dreyer and Marcu , 2012 ) for each sentence .
Multilingual NMT Multilingual NMT
( Dong et al. , 2015 ) has been shown to rival performance of single language pair models in high- resource languages ( Aharoni et al. , 2019 ; Arivazhagan et al. , 2019 ) while also improving low-resource translation via transfer learning from higher - resource languages ( Zoph et al. , 2016 ; Nguyen and Chiang , 2017 ; Neubig and Hu , 2018 ) .
An extreme low-resource setting is where the system translates between languages seen during training , but in a language pair where it did not see any training Table 1 : Example token - level log probabilities from our model for various output sentences , conditioned on input sentence ( i.e. , human reference ) " Jason went to school at the University of Madrid . "
H( out |in ) denotes the average token - level log probability .
We observe that our model generally penalizes any deviations ( bolded ) from the input sentence , but tends to penalize deviations which change the meaning of the sentence or introduce a disfluency more harshly than those which are fluent and adequate .
Sentence - level BLEU with smoothing =1 ( " sBLEU " ) and LASER embedding cosine similarity ( " LASER " ) are shown for comparison .
We note that LASER appears fairly insensitive to disfluencies , and sentenceBLEU struggles to reward valid paraphrases .
data , denoted ' zero- shot ' translation .
Despite evidence that intermediate representations are not truly language - agnostic ( Kudugunta et al. , 2019 ) , zero-shot translation has been shown successful , especially between related languages ( Johnson et al. , 2017 ; Gu et al. , 2018 ; Pham et al. , 2019 ) . Generative Paraphrasing Sentential paraphrasing can be accomplished by training an MT system on paraphrase examples instead of translation pairs ( Quirk et al. , 2004 ) .
While natural paraphrase datasets do exist ( Quirk et al. , 2004 ; Coster and Kauchak , 2011 ; Fader et al. , 2013 ; Lin et al. , 2014 ; Federmann et al. , 2019 ) , they are somewhat limited .
An alternative is to start with much more plentiful bitext and back - translate one side into the language of the other to create synthetic paraphrases on which to train ( Prakash et al. , 2016 ; Wieting and Gimpel , 2018 ; Hu et al. , 2019 a , b, c ) . Tiedemann and Scherrer ( 2019 ) propose using paraphrasing as a way to measure the semantic abstraction of multilingual NMT .
They also propose using a multilingual NMT model as a generative paraphraser .
3 Semantic Similarity Parallel corpora in many language pairs have been used to produce fixed -size , multilingual sentence representations ( Schwenk and Douze , 2017 ; Wieting et al. , 2017 ; Artetxe and Schwenk , 2018 ; Wieting et al. , 2019 ; Raganato et al. , 2019 ) . LASER ( Artetxe and Schwenk , 2018 ) , for example , trains a variant of NMT with a fixed - size intermediate representation in 93 languages .
Embeddings produced by the encoder can be compared to measure intra-or interlingual semantic similarity .
Method
We propose using a paraphraser to force-decode and estimate probabilities of MT system outputs , conditioned on their corresponding human references .
Let p(y t |y i<t , x ) be the probability our paraphraser assigns to the t th token in output sequence y , given the previous output tokens y i<t and the input sequence x .
Table 1 shows an example of how token - level probabilities from our model ( described in ?4 ) penalize both fluency and adequacy errors given a human reference .
We consider two ways of combining token - level probabilities from the model- sequence - level log probability ( G ) and average token - level log probability ( H ) : G( y|x ) = | y | t=1 log p(y t |y i<t , x ) H ( y|x ) = 1 |y | G ( y | x )
Let sys denote an MT system output , ref denote a human reference , and src denote the source .
We expect scoring sys conditioned on ref to be most indicative of the quality of sys .
However , we also explore scoring ref conditioned on sys as we find qualitatively that output sentences which drop some meaning conveyed by the input sentence are penalized less harshly by the model than output sentences which contain extra information not present in the input .
Scoring in both directions to penalize the presence of information in one sentence but not the other is similar , in spirit , to methods which use bi-directional textual entailment as an MT metric ( Pad ?
et al. , 2009 ; Khobragade et al. , 2019 ) . 4
We postulate that the output sentence that best represents the meaning of an input sentence is , in fact , simply a copy of the input sentence , as precise word order and choice often convey subtle connotations .
As such , we seek a model whose output distribution is centered around a copy of the input sentence , which we denote a " lexically / syntactically unbiased paraphraser . "
While a standard generative paraphraser is trained to retain semantic meaning , it does not meet our criteria because it is simultaneously trained to produce output which is lexically / syntactically different than its input , a key element in generative paraphrasing ( Bhagat and Hovy , 2013 ) .
We propose using a multilingual NMT system as a lexically / syntactically unbiased paraphraser .
A multilingual NMT system consists of an encoder which maps a sentence in to an ( ideally ) languageagnostic semantic representation , and decoder to map that representation back to a sentence .
The model has only seen bitext in training , but we propose to treat paraphrasing as a zero-shot " translation " ( e.g. , Czech to Czech ) .
Because our model is multilingual , we can also score MT system output conditioned on the source sentence instead of the human reference .
This task is known as " quality estimation ( QE ) as a metric , " and was part of the WMT19 QE shared task ( Fonseca et al. , 2019 ) .
We use " Prism-ref " to denote our reference - based metric and " Prism-src " to denote our system applied as a QE metric .
Our final metric and QE metric are defined based on results on our development set ( see ?5.2 ) as follows : Prism-ref = 1 2 H(sys|ref ) + 1 2 H( ref|sys ) Prism-src = H(sys|src )
To obtain system- level scores , we average segmentlevel scores over all segments in the test set .
Experiments
We train a multilingual NMT model and explore the extent to which it functions as a lexically / syntactically unbiased paraphraser .
We then conduct several preliminary experiments on the WMT18 MT metrics data ( Ma et al. , 2018 ) to determine how to best utilize the token - level probabilities from the paraphraser , and report results on the WMT19 system - and segment - level metric tasks ( Ma et al. , 2019 ) and QE as a metric task ( Fonseca et al. , 2019 ) .
Data Preparation
Our method requires a model , which in turn relies heavily on the data on which it is trained , so we describe here the rationale behind the design decisions made regarding the training data .
Full details sufficient for replication are provided in Appendix B. Language -Agnostic Representations
To encourage our intermediate representation to be as language - agnostic as possible , we choose datasets with as much language pair diversity as possible ( i.e. , not just en-* and *- en ) , as Kudugunta et al . ( 2019 ) has shown that encoder representation is affected by both the source language and target language .
While it is common to append the target language token to the source sentence , we instead prepend it to the target sentence so that the encoder cannot do anything target - language specific with this tag .
At test time , we force- decode the desired language tag prior to scoring .
Noise NMT systems are known to be sensitive to noise , including sentence alignment errors ( Khayrallah and Koehn , 2018 ) , so we perform filtering with LASER ( Schwenk , 2018 ; Chaudhary et al. , 2019 ) .
We also perform language ID filtering using FastText ( Joulin et al. , 2016 ) to avoid training the decoder with incorrect language tags .
Aharoni et al. ( 2019 ) found that performance of zero-shot translation in a related language pair increased substantially when increasing the number of languages from 5 languages and 25 , with a performance plateau somewhere between 25 and 50 languages .
We view paraphrasing as zero-shot translation between sentences in the same language , so we expect to need a similar number of languages .
Number of Languages Copies
We filter sentence pairs with excessive copies and partial copies , as multiple studies ( Ott et al. , 2018 ; Khayrallah and Koehn , 2018 ) have noted that MT performance degrades substantially when systems are exposed to copies in training .
Model Training
We train a Transformer ( ( Eisele and Chen , 2010 ) .
The data processing described above and in Appendix B results in 99.8 M sentence pairs in 39 languages .
7
The most common language is English , at 16.7 % of our data , while the least common 20 languages account for 21.9 % .
Baselines and Contrastive Methods
We compare to all systems from the WMT19 shared metrics task , as well as BERTscore ( Zhang et al. , 2020 ) and the recent BLEURT method ( Sellam et al. , 2020 ) .
We also explore several contrastive methods .
Training details sufficient for replication for each model / baseline are given in Appendix C.
Generative Sentential Paraphraser
We compare scoring with our Prism model vs a standard , English-only paraphraser trained on the ParaBank 2 dataset ( Hu et al. , 2019 c ) .
ParaBank 2 contains ?
50 M synthetic paraphrastic pairs derived from back - translating a Czech- English corpus , and the authors report state - of - the - art paraphrasing results .
Auto-encoder Auto-encoders provide an alternative means of training seq2seq models , without the need for parallel bitext .
We compare to scoring with the " multilingual denoising pre-trained model " ( mBART ) of Liu et al . ( 2020 ) , as it works in all languages of interest .
LASER
We explore using the cosine distance between LASER embeddings of the MT output and human reference , using the pretrained 93 - language model provided by the authors .
8
We are particularly interested in LASER as it , like our model , is trained on parallel bitext in many languages .
Language Model
We find qualitatively that LASER is fairly insensitive to disfluencies ( see Table 1 ) , so we also explore augmenting it with language model ( LM ) scores of the system outputs .
We train a multilingual language model ( see Appendix C ) on the same data as our multilingual NMT system .
Paraphraser Bias
We expect that a lexically / syntactically unbiased measure of translation quality should ( on average ) increase with increased lexical similarity between a translation and reference .
To explore the extent to which Prism and the model trained on ParaBank 2 are biased , we consider average H(sys|ref ) as a function of binned lexical similarity ( approximated by sentBLEU , with smoothing =1 ) for all ( sys , ref ) pairs for all systems submitted to WMT19 in all language pairs into English .
We also contrast the conditional probabilities of three outputs for the same input : ( 1 ) the sequence generated by the model via beam search ; ( 2 ) a copy of the input ; and ( 3 ) a human paraphrase of the input .
Finally , we generate from the model using beam search and examine the outputs to see how much they differ from the inputs .
MT Metrics Evaluation
We report results and statistical significance using scripts released with the WMT19 shared task .
Segment - level performance is reported as the Kendall 's ?
variant used in the shared task , and system-level performance is reported as Pearson correlation with the mean of the human judgments .
Bootstrap resampling ( Koehn , 2004 ; Graham et al. , 2014 ) is used to estimate confidence intervals for each metric , and metrics with non-overlapping 95 % confidence intervals are identified as having a statistically significant difference in performance .
en- cs en-de en-fi en-gu en-kk en-lt en-ru en-zh de-cs de-fr fr-de BERTSCORE ( Zhang et al. , 2020 ) Table 2 : WMT19 segment- level human correlation ( ? ) , to non-English ( top ) and to English ( bottom ) .
Bold denotes top scoring method and any other methods with whose 95 % confidence interval overlaps with that of a top method .
?:WMT19 Metric Submission .
For brevity , only competitive baselines are shown .
For complete results see Appendix E. Our models were not trained on Gujarati ( gu ) .
" LASER + LM " denotes the optimal linear combination found on the development set .
Results
Paraphraser Bias Results
We find H( sys|ref ) increases monotonically with sentBLEU for the Prism model , but the model trained on ParaBank 2 has nearly the same scores for output with sentBLEU in the range of 60 to 100 ; however that range accounts for only about 8.5 % of all system outputs ( see Figure 2 ) .
We find that a copy of the input is almost as probable as beam search output for the Prism model .
In contrast , the model trained on ParaBank 2 prefers its own beam search output to a copy of the input .
Additionally , beam search from our model produces output which is more lexically similar to the input ( BLEU of 82.8 with respect to input , vs 31.9 for ParaBank 2 ) .
ParaBank 2 tends to change the output in ways which occasionally significantly alter the meaning of the sentence .
See Appendix
A for more details .
All of these findings support our hypothesis that our model is closer to an ideal lexically / syntactically unbiased paraphraser than the contrastive model trained on synthetic paraphrases .
Preliminary ( Development ) Results
We find that length - normalized log probability ( H ) slightly outperforms un-normalized log probability ( G ) .
When using the reference , we find an equal weighting of H(sys|ref ) and H( ref|sys ) to be approximately optimal , but we find that when using the source , H( src|sys ) does not appear to add useful information to H(sys|src ) .
Full results can be found in Appendix D .
These findings were used to select the Prism-ref and Prism-src definitions ( ?3 ) .
We find that the probability of sys as estimated by an LM , as well as and the cosine distance between LASER embeddings of sys and ref , both have decent correlation with human judgments and are complementary .
However , cosine distance between LASER embeddings of sys and src have only weak correlation .
Segment -Level Metric Results Segment-level metric results are shown in Table 2 .
On language pairs into non-English , we outperform prior work by a statistically significant margin in 7 of 11 language pairs 9 and are statistically tied for best in the rest , with the exception of Gujarati ( gu ) where the model had no training data .
Into English , our metric is statistically tied with the best prior work in every language pair .
Our metric tends to significantly outperform our contrastive LASER + LM and mBART methods , although LASER + LM performs surprisingly well in en-ru .
System -Level Metric Results
Table 3 shows system-level metric performance on the top four systems submitted to WMT19 compared to selected metrics .
While correlations are not high in all cases for Prism , they are at least all positive .
In contrast , BLEU has negative correlation in 5 language pairs , and BERTscore and YiSi - 1 variants are each negative in at least two .
BLEURT has positive correlations in all language pairs into English , but is English-only .
Note that Pearson 's correlation coefficient may be unstable in this setting ( Mathur et al. , 2020 ) .
For full top four system-level results see Appendix F .
We do not find the system-level results computed against all submitted MT systems ( see Appendix G ) to be particularly interesting ; as noted by Ma et al . ( 2019 ) , a single weak system can result in high overall system-level correlation even for a very poor metric .
QE as a Metric Results
We find that our reference-less Prism-src outperforms all QE as a metrics systems from the WMT19 shared task by a statistically significant margin , in every language pair at segment - level human correlation ( Table 4 ) , and outperforms or statistically ties at system-level human correlation ( Appendix G ) .
Analysis and Discussion
How helpful are human references ?
The fact that our model is multilingual allows us to explore the extent to which the human reference actually improves our model 's ability to judge MT system output , compared to using the source instead .
The underlying assumption with any MT metric is that the work done by the human translator makes it easier to automatically judge the quality of MT output .
However , if our model or the MT systems being judged were strong enough , we would expect this assumption to break down .
Comparing the performance of our method with access to the human reference ( Prism-ref ) vs our method with access to only the source ( Prism-src ) , we find that the reference - based method statistically outperforms the source - based method in all but one language pair .
We find the case where they are not statistically different , de-cs , to be particularly interesting : de-cs was the only language pair in WMT19 where the systems were unsupervised ( i.e. , did not use parallel training data ) .
As a result , it is the only language pair where our model outperformed the best WMT system at translation .
In most cases , our model is substantially worse at translation than the best WMT systems .
For example , in en-de and zh-en , two language pairs where strong NMT systems were especially problematic for MT metrics , the Prism model is 6.8 and 19.2 BLEU points behind the strongest WMT systems , respectively ( see Table 5 for the Prism model compared to the best system submitted in each WMT19 language pair ) .
Thus the performance difference between Prism-ref and Prism-src would suggest that the model needs no help in judging MT systems which are weaker than it is , but the human references are assisting our model in evaluating MT systems which are stronger than it is .
This means that we have not simply reduced the task of MT evaluation to that of building a state - of - the - art MT en-cs en-de en-fi en-gu en-kk en-lt en-ru en-zh de-cs de-fr fr-de BERTSCORE ( Zhang et al. , 2020 ) system .
We see that a good ( but not state - of - the - art ) multilingual NMT system can be a state - of - the - art MT metric and judge state - of - the - art MT systems .
Finally , with the exception of de-cs discussed above , we see statistically significant improvements for Prism-ref over Prism-src both into English ( where human judgments were referencebased ) and into non-English ( where human judgments were source- based ) .
This suggests that the high correlation of Prism-ref with human judgements is not simply the result of reference bias ( Fomicheva and Specia , 2016 ) .
Does paraphraser bias matter ?
Our lexically / syntactically unbiased paraphraser tends to outperforms the generative English-only ParaBank 2 paraphraser , but usually not by a statistically significant margin .
Analysis indicate the lexi-cal / syntactic bias is only harmful in somewhat infrequent cases where MT systems match or nearly match the reference , suggesting it would be more detrimental with stronger systems or multiple references .
Our multilingual training method is much simpler than the alternative of creating synthetic paraphrases and training individual models in 39 languages , and our model may benefit from transfer learning to lower - resource languages .
Does fluency matter ?
Despite NMT being very fluent , our results suggest that fluency is fairly discriminative , especially in non-English : LM scoring outperforms sentenceBLEU at segment- level correlation in 7/10 language pairs to non-English languages ( excluding Gujarati ) , for example .
This is consistent with recent findings that LM scores can be used to augment BLEU ( Edunov et al. , 2020 Table 5 : BLEU scores for our multilingual NMT system on WMT19 testsets , compared to best system from WMT19 .
Our multilingual system achieves state- ofthe - art performance as an MT metric despite substantially under performing all the best WMT19 MT systems at translation ( excluding unsupervised ) .
? : WMT systems were unsupervised ( no parallel data ) .
? : Multilingual system did not train on Gujarati ( gu ) .
Systems are not trained on the same data , so this should not be interpreted as a comparison between multilingual and single - language pair MT. ISO 639 - 1 language codes .
Can we measure adequacy and fluency separately ?
The proposed method significantly outperforms the contrastive LASER - based method in most language pairs , even when LASER is augmented with a language model .
This suggests that jointly optimizing a model for adequacy and fluency is better than optimizing them independently and combining after the fact - this is unsurprising given that neural MT has shown significant improvements over statistical MT , where a phrase table and language model were trained separately .
Can we train on monolingual data instead of bitext ?
The proposed method significantly outperforms scoring with the mBART auto-encoder , which is trained on large amounts of monolingual data , despite using substantially less compute power ( 1.3 weeks on 8 V100s for Prism vs 2.5 weeks on 256 V100s for mBART ) .
Conclusion and Future Work
We show that a multilingual NMT system can be used as a lexically / syntactically unbiased , multilingual paraphraser , and that the resulting paraphraser can be used as an MT metric and QE metric .
Our method achieves state - of - the - art performance on the most recent WMT shared metrics and QE tasks , without training on prior human judgements .
We release a single model which supports 39 languages .
To the best of our knowledge , we are the first to release a large multilingual NMT system , and we hope others follow suit .
We are optimistic our method will improve further as stronger multilingual NMT models become publicly available .
We compare our method to several contrastive methods and present analysis showing that we have not simply reduced the task of evaluation to that of building a state - of - the - art MT system ; the work done by the human translator to create references helps the evaluation model to judge systems that are stronger ( at translation ) than it is .
Nothing in our method is specific to sentencelevel MT .
In future work , we would like to extend Prism to paragraph - or document - level evaluation by training a paragraph - or document - level multilingual NMT system , as there is growing evidence that MT evaluation would be better conducted at the document level , rather than the sentence level ( L?ubli et al. , 2018 ) .
