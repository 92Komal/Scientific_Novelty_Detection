title
Learning Word Reorderings for Hierarchical Phrase- based Statistical Machine Translation
abstract
Statistical models for reordering source words have been used to enhance the hierarchical phrase - based statistical machine translation system .
Existing word reordering models learn the reordering for any two source words in a sentence or only for two continuous words .
This paper proposes a series of separate sub-models to learn reorderings for word pairs with different distances .
Our experiments demonstrate that reordering sub-models for word pairs with distance less than a specific threshold are useful to improve translation quality .
Compared with previous work , our method may more effectively and efficiently exploit helpful word reordering information .
Introduction
The hierarchical phrase - based model ( Chiang , 2005 ) is capable of capturing rich translation knowledge with the synchronous context-free grammar .
But selecting proper translation rules during decoding is a challenge as a huge number of hierarchical rules can be applied to one source sentence .
Chiang ( 2005 ) used a log-linear model to compute rule weights with features similar to Pharaoh ( Koehn et al. , 2003 ) .
However , to select appropriate rules , more effective criteria are required .
A lot of work has been done for better rule selection .
and used maximum entropy approaches to integrate rich contextual information for target side rule selection .
Cui et al. ( 2010 ) proposed a joint model to select hierarchical rules for both source and target sides .
Hayashi et al. ( 2010 ) demonstrated the effectiveness of using word reordering information within hierarchical phrase - based SMT by integrating Tromble and Eisner ( 2009 ) 's word reordering model into decoder as a feature , which estimates the probability of any two source words in a sentence being reordered during translating .
Feng et al. ( 2013 ) proposed a word reordering model to learn reorderings only for continuous words , which reduced computation cost a lot compared with Tromble and Eisner ( 2009 ) 's model and still achieved significant reordering improvement over the baseline system .
In this paper , we incorporate word reordering information into hierarchical phrase - based SMT by training a series of separate reordering submodels for word pairs with different distances .
We will demonstrate that the translation performance achieves consistent improvement as more sub-models for longer distance reorderings being integrated , but the improvement levels off quickly .
That means sub-models for reordering distance longer than a given threshold do not improve translation quality significantly .
Compared with previous models ( Tromble and Eisner , 2009 ; Feng et al. , 2013 ) , our method makes full use of helpful word reordering information and also avoids unnecessary computation cost for long distance reorderings .
Besides , our reordering model is learned by feed -forward neural network ( FNN ) for better performance and uses efficient caching strategy to further reduce time cost .
Phrase reordering models have also been integrated into hierarchical phrase - based SMT .
Phrase reordering models were originally developed for phrase - based SMT ( Koehn et al. , 2005 ; Zens and Ney , 2006 ; Ni et al. , 2009 ; and could not be used in hierarchical phrase - based model directly .
Nguyen and Vogel ( 2013 ) and Cao et al . ( 2014 ) proposed to integrate phrasebased reordering features into hierarchical phrasebased SMT .
However , their work limited to learning the reordering of continuous phrases .
For short phrases , in extreme cases , when phrase length is one , their model only learned reordering for continuous word pairs like Feng et al . ( 2013 ) 's work , while our model can be applied to word pairs with longer distances .
Our Approach Let e m 1 = e 1 , . . . , e m be a target translation of f l 1 = f 1 , . . . , f l and A be word alignments between e m 1 and f l 1 , our model estimates the reordering probability of the source sentence as follows : Pr f l 1 , e m 1 , A ? N n=1 i ,j:1?i<j?l, j?i=n
Pr f l 1 , e m 1 , A , i , j ( 1 ) where Pr f l 1 , e m 1 , A , i , j is the reordering probability of the word pair f i , f j during translating ;
N is the maximum distance for source word reordering , which is empirically determined by supposing that estimating reorderings longer than N does not improve translation performance any more .
Previous word reordering models ( Tromble and Eisner , 2009 ; Feng et al. , 2013 ) consider the reordering of a source word pair to be reversed or not .
When a source word is aligned to several uncontinuous target words , it can be hard to determine if a word pair is reversed or not .
They solved this problem by only using one alignment from multiple alignments and ignoring the others .
In contrast , our model handles all alignments as shown below .
Suppose that f i is aligned to ? i (?
i ? 0 ) target words .
When ? i > 0 , {a ik | 1 ? k ? ?
i } stands for the positions of target words aligned to f i .
If ?
i = 0 or ? j = 0 , Pr f l 1 , e m 1 , A , i , j = 1 , otherwise , Pr f l 1 , e m 1 , A , i , j = ?
i u=1 ? j v=1 Pr oijuv|fi?3 , ... , fj +3 , ea iu , ea jv where oijuv = 0 ( aiu ? ajv ) 1 ( aiu > ajv ) ( 2 ) We train a series of sub-models , M 1 , M 2 , . . . , M N Algorithm 1 Extract training instances .
Require : A pair of parallel sentence f l 1 and e m 1 with word alignments .
Ensure : Training examples for M1 , M2 , . . . , MN . for i = 1 to l ?
1 do for j = i + 1 to l do if j ? i ?
N then for u = 1 to ?i do for v = 1 to ?j do if aiu ?
ajv then fi?3 , ... , fj +3 , ea iu , ea jv , 0 is a negative instance for Mj?i else fi?3 , ... , fj +3 , ea iu , ea jv , 1 is a positive instance for Mj?i to learn reorderings for word pairs with different distances .
That means , for the word pair f i , f j with distance j ?
i = n , its reordering probability Pr o ijuv |f i?3 , ... , f j +3 , e a iu , e a jv is estimated by M n .
Different sub-models are trained and integrated into the translation system separately .
Each sub-model M n is implemented by an FNN , which has the same structure with the neural language model in ( Vaswani et al. , 2013 ) .
The input to M n is a sequence of n + 9 words : f i?3 , ... , f j +3 , e a iu , e a jv .
The input layer projects each word into a high dimensional vector using a matrix of input word embeddings .
Two hidden layers can combine all input data 1 .
The output layer has two neurons that give Pr o ijuv = 1 | f i?3 , ... , f j +3 , e a iu , e a jv and Pr o ijuv = 0 | f i?3 , ... , f j +3 , e a iu , e a jv .
The backpropagation algorithm is used to train these reordering sub-models .
The training instances for each sub-model are extracted from the word-aligned parallel corpus according to Algorithm 1 .
For example , the word pair " ?( wears ) ?( guy ) " in Figure 1 will be extracted as a positive instance for M 3 .
The input of this instance is as follows : " < s> <s> ? ? ? ? ? ? ? </ s> wears guy " , where < s> and </s> represent the beginning and ending of a sentence .
If a word never occurs or only occurs once in training corpus , we replace it with a special symbol < unk >.
Integration into the Decoder
In the hierarchical phrase - based model , a translation rule r is like : X ? ? , ? , ? where X is a nonterminal , ? and ? are respectively source and target strings of terminals and nonterminals , and ? is the alignment between nonterminals and terminals in ? and ?.
Each rule has several features and the feature weights are tuned by the minimum error rate training ( MERT ) algorithm ( Och , 2003 ) .
To integrate our model into the hierarchical phrase - based translation system , a new feature score n ( r ) is added to each rule r for each M n .
The score of this feature is calculated during decoding .
Note that these scores are correspondingly calculated for different sub-models M n and the sub-model weights are tuned separately .
Suppose that r is applied to the input sentence f l 1 , where ?
r covers the source span [ f ? , f ? ] ? ? contains nonterminals { X k | 1 ? k ? K} ?
X k covers the span [ f ? k , f ? k ]
Then scoren ( r ) = i , j ?S ?
K k=1 S k ?j?i=n log Pr f l 1 , e m 1 , A , i , j where S : { i , j |? ? i < j ? ?}
S k : { i , j |? k ? i < j ? ? k }
For example , if a rule " X1 X2 ? X1 guy X2 " is applied to the input sentence in Figure 1 , then [ f? , f ? ] = [ 1 , 5 ] ; [ f ?
1 , f ? 1 ] = [ 1 , 1 ] ; 2 , f ? 2 ] = [ 2 , 4 ] S ? K k=1 S k = 1 , 2 , 1 , 3 , 1 , 4 , 1 , 5 , 2 , 5 , 3 , 5 , 4 , 5 One concern in using target features is the computational efficiency , because reordering probabilities have to be calculated during decoding .
So we cache probabilities to reduce the expensive neural network computation in experiments .
Experiments
We evaluated the proposed approach for Chineseto-English ( CE ) and Japanese - to- English ( JE ) translation tasks .
The official datasets for the patent machine translation task at NTCIR -9 ( Goto et al. , 2011 )
In NTCIR -9 , the development and test sets were both provided for CE task while only the test set was provided for the JE task .
Therefore , we used the sentences from the NTCIR -8 JE test set as the development set for JE task .
The word segmentation was done by BaseSeg ( Zhao et al. , 2006 ; Zhao and Kit , 2008 ; Zhao and Kit , 2011 ; for Chinese and Mecab 2 for Japanese .
To learn neural reordering models , the training and development sets were put together to obtain symmetric word alignments using GIZA ++ ( Och and Ney , 2003 ) and the grow-diag-finaland heuristic ( Koehn et al. , 2003 ) .
The reordering instances extracted from the aligned training and development sets were used as the training and validation data respectively for learning neural reordering models .
Neural reordering models were trained by the toolkit NPLM ( Vaswani et al. , 2013 ) .
For CE task , training instances extracted from all the 1M sentence pairs were used to train neural reordering models .
For JE task , training instances were from 1 M sentence pairs that were randomly selected from all the 3.14 M sentence pairs .
We also implemented Hayashi et al . ( 2010 ) 's model for comparison .
The training instances for their model were extracted from the same sentence pairs as ours .
For each translation task , the recent version of the Moses hierarchical phrase - based decoder ( Koehn et al. , 2007 ) with the training scripts was used as the baseline system Base .
We used the default parameters for Moses .
A 5 - gram language model was trained on the target side of the training corpus by IRST LM Toolkit 3 with the improved Kneser - Ney smoothing .
We integrated our reordering models into Base .
Table 2 gives detailed translation results .
" Hayashi model " represents the method of ( Hayashi et al. , 2010 ) .
" M j 1 ( j = 1 , 2 , 3 , 4 ) " means that Base was augmented with the reordering scores calcuated from a series of sub-models M 1 to M j .
As shown in Table 2 , integrating only M 1 , which predicts reordering for two continuous source words , has already given BLEU improvement 1.8 % and 1.2 % over baseline on CE and JE , respectively .
As more sub-models for longer distance reordering being integrated , the translation performance improved consistently , though the improvement leveled off quickly .
For CE and JE tasks , M n with n ?
3 and n ?
4 , respectively , cannot give further performance improvement at any significant level .
Why did the improvement level off quickly ?
In other words , why do long distance reordering models have a much less leverage over translation performance than short ones ?
First , the prediction accuracy decreases as the reordering distance increasing .
Table 3a gives classification accuracies on the validation data for each sub-model .
The reason for accuracy decreasing is that the input size of sub-model grows as reordering distance increasing .
Namely , long distance reordering needs to consider more complicated context .
Second , we attribute the influence decrease of the longer reordering models to the redundancy of the predictions among different reordering models .
For example , in Figure 1 , when word pairs " ?( guy ) ?( is ) " and " ?( is ) ?( James ) " are both predicted to be not reversed , the reordering for " ?( guy ) ?( James ) " can be logically determined to be not reversed without further reordering model prediction .
That means , sometimes , a long distance word reordering can be determined by a series of shorter word reordering pairs .
But still , some predictions for longer reordering are useful .
For example , the reordering of " ?( wears ) ?( guy ) " cannot be determined when " ?( wears ) ?( glasses ) " is predicted to be not reversed and " ?( glasses ) ?( guy ) " is reversed .
This is the reason why translation performance improves as more sub-models being integrated .
As shown in Table 2 , with 4 sub-models being integrated , our model improved baseline system significantly and also outperformed Hayashi model clearly .
It is easy to understand , since our model was trained by feed -forward neural network on a high dimensional space and incorporated rich context information , while Hayashi model used the averaged perceptron algorithm and simple features .
pairs with all different distances are used as training data .
By using separate sub-models , we can train each sub-model one by one and stop when translation performance cannot be improved any more .
However , despite of efficiency , one unified model will theoretically have better performance than separate sub-models since separate sub-models do not share training instances and the unified model will suffer less from data sparsity .
So , we did some extra experiments and trained a neural network which had the same structure as M 4 to learn reorderings for all word pairs with distance 4 or less , instead of using 4 separate neural networks .
A specific word null was used since word pairs with distance 1,2,3 do not have enough inputs for M 4 .
The significance test results showed that translation performance had no significant difference between one unified model and multiple sub-models .
This is because the training corpus for our model is quite large , so separate training sets are sufficient for each submodel to learn the reorderings well .
Besides , using neural networks to learn these sub-models on a continuous space can relieve the data sparsity problem to some extent .
Note that if we only integrate M 4 into Base , the translation quality of Base can be improved in our preliminary experiments .
But M 4 cannot predict reorderings for word pairs with distance less than 4 .
So M 3 1 will be still needed for predicting reorderings of word pairs with distance 1,2,3 .
But after M 3 1 being integrated , M 4 will not be needed due to the redundancy of the predictions among
Conclusion
In this paper , we propose to enhance hierarchical phrase - based SMT by training a series of separate sub-models to learn reorderings for word pairs with distances less than a specific threshold , based on the experimental fact that longer distance reordering models are not quite helpful for translation quality .
Compared with Hayashi et al . ( 2010 ) 's work , our model is much more efficient and keeps all helpful word reordering information .
Besides , our reordering model is learned by feed -forward neural network and incorporates rich context information for better performance .
On both Chinese-to-English and Japanese- to - English translation tasks , the proposed model outperforms the previous model significantly .
Figure 1 : 1 Figure 1 : A Chinese-English sentence pair .
