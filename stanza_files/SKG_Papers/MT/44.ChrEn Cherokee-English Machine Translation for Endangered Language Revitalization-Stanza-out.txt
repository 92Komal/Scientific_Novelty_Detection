title
ChrEn : Cherokee -English Machine Translation for Endangered Language Revitalization
abstract
Cherokee is a highly endangered Native American language spoken by the Cherokee people .
The Cherokee culture is deeply embedded in its language .
However , there are approximately only 2,000 fluent first language Cherokee speakers remaining in the world , and the number is declining every year .
To help save this endangered language , we introduce ChrEn , a Cherokee - English parallel dataset , to facilitate machine translation research between Cherokee and English .
Compared to some popular machine translation language pairs , ChrEn is extremely low-resource , only containing 14 k sentence pairs in total .
We split our parallel data in ways that facilitate both in - domain and out-of- domain evaluation .
We also collect 5 k Cherokee monolingual data to enable semi-supervised learning .
Besides these datasets , we propose several Cherokee -English and English - Cherokee machine translation systems .
We compare SMT ( phrase- based ) versus NMT ( RNN - based and Transformer - based ) systems ; supervised versus semi-supervised ( via language model , back - translation , and BERT / Multilingual - BERT ) methods ; as well as transfer learning versus multilingual joint training with 4 other languages .
Our best results are 15.8/12.7 BLEU for in- domain and 6.5/5.0 BLEU for out -of- domain Chr-En / En- Chr translations , respectively , and we hope that our dataset and systems will encourage future work by the community for Cherokee language revitalization .
1
Introduction
The Cherokee people are one of the indigenous peoples of the United States .
Before the 1600s , they lived in what is now the southeastern United States ( Peake Raymond , 2008 ) .
Today , there are three federally recognized nations of Cherokee Src. ? ? ? ? , ? ? ? ? ? ?. Ref .
They are not of the world , even as I am not of the world .
SMT
It was not the things upon the earth , even as I am not of the world .
NMT
I am not the world , even as I am not of the world .
people : the Eastern Band of Cherokee Indians ( EBCI ) , the United Keetoowah Band of Cherokee Indians ( UKB ) , and the Cherokee Nation ( CN ) .
The Cherokee language , the language spoken by the Cherokee people , contributed to the survival of the Cherokee people and was historically the basic medium of transmission of arts , literature , traditions , and values ( Nation , 2001 ; Peake Raymond , 2008 ) .
However , according to the Tri-Council Res. No. 02-2019 , there are only 2,000 fluent first language Cherokee speakers left , and each Cherokee tribe is losing fluent speakers at faster rates than new speakers are developed .
UNESCO has identified the dialect of Cherokee in Oklahoma is " definitely endangered " , and the one in North Carolina is " severely endangered " .
Language loss is the loss of culture .
CN started a 10 - year language revitalization plan ( Nation , 2001 ) in 2008 , and the Tri-Council of Cherokee tribes declared a state of emergency in 2019 to save this dying language .
To revitalize Cherokee , language immersion programs are provided in elementary schools , and second language programs are offered in universities .
However , students have difficulty finding exposure to this language beyond school hours ( Albee , 2017 ) .
This motivates us to build up English ( En ) to Cherokee ( Chr ) machine translation systems so that we could automatically translate or aid human translators to translate English materials to Cherokee .
Chr-to - En is also highly meaningful in helping spread Cherokee history and culture .
Therefore , in this paper , we contribute our effort to Cherokee revitalization by constructing a clean Cherokee - English parallel dataset , ChrEn , which results in 14,151 pairs of sentences with around 313 K English tokens and 206K Cherokee tokens .
We also collect 5,210 Cherokee monolingual sentences with 93 K Cherokee tokens .
Both datasets are derived from bilingual or monolingual materials that are translated or written by first- language Cherokee speakers , then we manually aligned and cleaned the raw data .
2 Our datasets contain texts of two Cherokee dialects ( Oklahoma and North Carolina ) , and diverse text types ( e.g. , sacred text , news ) .
To facilitate the development of machine translation systems , we split our parallel data into five subsets : Train / Dev/ Test / Out-dev/ Out-test , in which Dev/ Test and Out-dev / Out-test are for indomain and out-of- domain evaluation respectively .
See an example from ChrEn in Table 1 and the detailed dataset description in Section 3 .
The translation between Cherokee and English is not easy because the two languages are genealogically disparate .
As shown in Figure 1 , Cherokee is the sole member of the southern branch of the Iroquoian language family and is unintelligible to other Iroquoian languages , while English is from the West Germanic branch of the Indo-European language family .
Cherokee uses a unique 85 character syllabary invented by Sequoyah in the early 1820s , which is highly different from English 's alphabetic writing system .
Cherokee is a polysynthetic language , meaning that words are composed of many morphemes that each have independent meanings .
A single Cherokee word can express the meaning of several English words , e.g. , ? ( widatsinegisi ) , or I am going off at a distance to get a liquid object .
Since the semantics are often conveyed by the rich morphology , the word orders of Cherokee sentences are vari-able .
There is no " basic word order " in Cherokee , and most word orders are possible ( Montgomery - Anderson , 2008 ) , while English generally follows the Subject- Verb-Object ( SVO ) word order .
Plus , verbs comprise 75 % of Cherokee , which is only 25 % for English ( Feeling , 1975 ( Feeling , , 1994 .
Hence , to develop translation systems for this low-resource and distant language pair , we investigate various machine translation paradigms and propose phrase - based ( Koehn et al. , 2003 ) Statistical Machine Translation ( SMT ) and RNNbased ( Luong et al. , 2015 ) or Transformer - based ( Vaswani et al. , 2017 ) Neural Machine Translation ( NMT ) systems for both Chr-En and En- Chr translations , as important starting points for future works .
We apply three semi-supervised methods : using additional monolingual data to train the language model for SMT ( Koehn and Knowles , 2017 ) ; incorporating BERT ( or Multilingual - BERT ) ( Devlin et al. , 2019 ) representations for NMT ( Zhu et al. , 2020 ) , where we introduce four different ways to use BERT ; and the back -translation method for both SMT and NMT ( Bertoldi and Federico , 2009 ; Lambert et al. , 2011 ; Sennrich et al. , 2016 b ) .
Moreover , we explore the use of existing X - En parallel datasets of 4 other languages ( X = Czech / German / Russian / Chinese ) to improve Chr-En / En-Chr performance via transfer learning ( Kocmi and Bojar , 2018 ) or multilingual joint training ( Johnson et al. , 2017 ) .
Empirically , NMT is better than SMT for in- domain evaluation , while SMT is significantly better under the out-of- domain condition .
RNN - NMT consistently performs better than Transformer - NMT .
Semi-supervised learning improves supervised baselines in some cases ( e.g. , back - translation improves out -of- domain Chr-En NMT by 0.9 BLEU ) .
Even though Cherokee is not related to any of the 4 languages ( Czech / German / Russian / Chinese ) in terms of their language family trees , surprisingly , we find that both transfer learning and multilingual joint training can improve Chr-En / En - Chr performance in most cases .
Especially , transferring from Chinese -English achieves the best in - domain Chr-En performance , and joint learning with English - German obtains the best in - domain En - Chr performance .
The best results are 15.8/12.7 BLEU for in - domain Chr-En / En-Chr translations ; and 6.5/5.0 BLEU for outof-domain Chr-En / En-Chr translations .
Finally , we conduct a 50 - example human ( expert ) evalua-tion ; however , the human judgment does not correlate with BLEU for the En-Chr translation , indicating that BLEU is possibly not very suitable for Cherokee evaluation .
Overall , we hope that our datasets and strong initial baselines will encourage future works to contribute to the revitalization of this endangered language .
Related Works Cherokee Language Revitalization .
In 2008 , the Cherokee Nation launched the 10 - year language preservation plan ( Nation , 2001 ) , which aims to have 80 % or more of the Cherokee people be fluent in this language in 50 years .
After that , a lot of revitalization works were proposed .
Cherokee Nation and the EBCI have established language immersion programs and k-12 language curricula .
Several universities , including the University of Oklahoma , Stanford University , etc. , have begun offering Cherokee as a second language .
However , given Cherokee has been rated at the highest level of learning difficulty ( Peake Raymond , 2008 ) , it is hard to be mastered without frequent language exposure .
As mentioned by Crystal ( 2014 ) , an endangered language will progress if its speakers can make use of electronic technology .
Currently , the language is included among existing Unicode-compatible fonts , is supported by Gmail , and has a Wikipedia page .
To revitalize Cherokee , a few Cherokee pedagogical books have been published ( Holmes and Smith , 1976 ; Joyner , 2014 ) , as well as several online learning platforms .
3 Feeling ( 2018 ) provided detailed English translations and linguistic analysis of a number of Cherokee stories .
A Digital Archive for American Indian Languages Preservation and Perseverance ( DAILP ) has been developed for transcribing , translating , and contextualizing historical Cherokee language documents ( Bourns , 2019 ; Cushman , 2019 ) .
4
However , the translation between Cherokee and English still can only be done by human translators .
Given that only 2,000 fluent first - language speakers are left , and the majority of them are elders , it is important and urgent to have a machine translation system that could assist them with translation .
Therefore , we introduce a clean Cherokee - English parallel dataset to facilitate machine translation development and propose multiple translation systems as starting points of future works .
We hope our work could attract more attention from the NLP community in helping to save and revitalize this endangered language .
An initial version of our data and its implications was introduced in ( Frey , 2020 ) .
Note that we are not the first to propose a Cherokee - English parallel dataset .
There is Chr-En parallel data available on OPUS ( Tiedemann , 2012 ) .
5
The main difference is that our parallel data contains 99 % of their data and has 6 K more examples from diverse domains .
Low -Resource Machine Translation .
Even though machine translation has been studied for several decades , the majority of the initial research effort was on high- resource translation pairs , e.g. , French - English , that have large-scale parallel datasets available .
However , most of the language pairs in the world lack large-scale parallel data .
In the last five years , there is an increasing research interest in these low-resource translation settings .
The DARPA 's LORELEI language packs contain the monolingual and parallel texts of three dozen languages that are considered as low-resource ( Strassel and Tracey , 2016 ) .
Riza et al. ( 2016 ) proposed several low-resource Asian language pairs .
Lakew et al. ( 2020 ) and Duh et al . ( 2020 ) proposed benchmarks for five and two low-resource African languages , respectively .
Guzm ?n et al. ( 2019 ) introduced two low-resource translation evaluation benchmarks : Nepali-English and Sinhala- English .
Besides , most low-resource languages rely on the existing parallel translations of the Bible ( Christodouloupoulos and Steedman , 2015 ) .
Because not many low-resource parallel datasets were publicly available , some low-resource machine translation research was done by sub-sampling high- resource language pairs ( Johnson et al. , 2017 ; Lample et al. , 2018 ) , but it may downplay the fact that low-resource translation pairs are usually distant languages .
Our ChrEn dataset can not only be another open resource of low-resource MT research but also challenge MT methods with an extremely morphology rich language and a distant language pair .
Two methods have been largely explored by existing works to improve low-resource MT .
One is semi-supervised learning to use monolingual data ( Gulcehre et al. , 2015 ; Sennrich et al. , 2016 b 2018 ; Johnson et al. , 2017 ) .
We explore both of them to improve Chr-En / En - Chr translations .
Data Description
It is not easy to collect substantial data for endangered Cherokee .
We obtain our data from bilingual or monolingual books and newspaper articles that are translated or written by first- language Cherokee speakers .
In the following , we will introduce the data sources and the cleaning procedure and give detailed descriptions of our data statistics .
Parallel Data Fifty - six percent of our parallel data is derived from the Cherokee New Testament .
Other texts are novels , children 's books , newspaper articles , etc .
These texts vary widely in dates of publication , the oldest being dated to 1860 .
Additionally , our data encompasses both existing dialects of Cherokee : the Overhill dialect , mostly spoken in Oklahoma ( OK ) , and the Middle dialect , mostly used in North Carolina ( NC ) .
These two dialects are mainly phonologically different and only have a few lexical differences ( Uchihara , 2016 ) .
In this work , we do not explicitly distinguish them during translation .
The left pie chart of Figure 2 shows the parallel data distributions over text types and dialects , and the complete information is in Table 14 of Appendix A.1 .
Many of these texts were translations of English materials , which means that the Cherokee structures may not be 100 % natural in terms of what a speaker might spontaneously produce .
But each text was translated by people who speak Cherokee as the first language , which means there is a high probability of grammaticality .
These data were originally available in PDF version .
We apply the Optical Character Recog- nition ( OCR ) via Tesseract OCR engine 6 to extract the Cherokee and English text .
Then our co-author , a proficient second - language speaker of Cherokee , manually aligned the sentences and fixed the errors introduced by OCR .
This process is time - consuming and took several months .
The resulting dataset consists of 14,151 sentence pairs .
After tokenization , 7 there are around 313 K English tokens and 206 K
Cherokee tokens in total with 14 K unique English tokens and 38 K unique Cherokee tokens .
Notably , the Cherokee vocabulary is much larger than English because of its morphological complexity .
This casts a big challenge to machine translation systems because a lot of Cherokee tokens are infrequent .
To facilitate machine translation system development , we split this data into training , development , and testing sets .
As our data stems from limited sources , we find that if we randomly split the data , some phrases / sub-sentences are repeated in training and evaluation sets , so the trained models will over -fit to these frequent patterns .
Considering that low-resource translation is usually accompanied by out -of- domain generalization in real-world applications , we provide two groups of development / testing sets .
We separate all the sentence pairs from newspaper articles , 512 pairs in total , and randomly split them in half as out -of-domain development and testing sets , denoted by Out-dev and Out-test .
The remaining sentence pairs are randomly split into in- domain Train , Dev , and Test .
About 13.3 % of unique English tokens and 37.7 % of unique Cherokee tokens in Dev have not appeared in Train , while the percentages are 42.1 % and 67.5 % for Out-dev , which shows the difficulty of the out-of- domain generalization .
Table 2 contains more detailed statistics ; notably , the average sentence length of Cherokee is much shorter than English , which demonstrates that the semantics are morphologically conveyed in Cherokee .
Note that Cherokee - English parallel data is also available on OPUS ( Tiedemann , 2012 ) , which has 7.9K unique sentence pairs , 99 % of which are the Cherokee New Testament that are also included in our parallel data , i.e. , our data is bigger and has 6 K more sentence pairs that are not sacred texts ( novels , news , etc. ) .
The detailed comparison will be discussed in A.2 .
Monolingual Data
In addition to the parallel data , we also collect a small amount of Cherokee monolingual data , 5,210 sentences in total .
This data is also mostly derived from Cherokee monolingual books .
8
As depicted by the right pie chart in Figure 2 , the majority of monolingual data are also sacred text , which is Cherokee Old Testament , and it also contains two -dialect Cherokee texts .
Complete information is in Table 15 of Appendix A.1 .
Similarly , we applied OCR to extract these texts .
However , we only manually corrected the major errors introduced by OCR .
Thus our monolingual data is noisy and contains some lexical errors .
As shown in Table 2 , there are around 93 K Cherokee tokens in total with 20 K unique Cherokee tokens .
This monolingual data has a very small overlap with the parallel data ; about 72 % of the unique Cherokee tokens are unseen in the whole parallel data .
Note that most of our monolingual data have English translations , i.e. , it could be converted to parallel data .
But it requires more effort from Cherokee speakers and will be part of our future work .
For now , we show how to effectively use this monolingual data for semi-supervised gains .
Models
In this section , we will introduce our Cherokee -English and English - Cherokee translation systems .
Adopting best practices from low-resource machine translation works , we propose both Statistical Machine Translation ( SMT ) and Neural Machine Translation ( NMT ) systems , and for NMT , we test both RNN - based and Transformer - based models .
We apply three semi-supervised methods : training language model with additional monolingual data for SMT ( Koehn and Knowles , 2017 ) , incorporating BERT or Multilingual - BERT representations into NMT ( Zhu et al. , 2020 ) , and back -translation for both SMT and NMT ( Bertoldi and Federico , 2009 ; Sennrich et al. , 2016 b ) .
Further , we explore transfer learning ( Kocmi and Bojar , 2018 ) from and multilingual joint training ( Johnson et al. , 2017 ) with 4 other languages ( Czech / German / Russian / Chinese ) for NMT .
SMT Supervised SMT .
SMT was the mainstream of machine translation research before neural models came out .
Even if NMT has achieved state - of - theart performance on many translation tasks , SMT is still very competitive under low-resource and out-of- domain conditions ( Koehn and Knowles , 2017 ) .
Phrase - based SMT is a dominant paradigm of SMT ( Koehn et al. , 2003 ) .
It first learns a phrase table from the parallel data that translates source phrases to target .
Then , a reordering model learns to reorder the translated phrases .
During decoding , a scoring model scores candidate translations by combining the weights from translation , reordering , and language models , and it is tuned by maximizing the translation performance on the development set .
A simple illustration of SMT is shown in Figure 3 . Note that , as Cherokee and English have different word orders ( English follows SVO ; Cherokee has variable word orders ) , one Cherokee phrase could be translated into two English words that are far apart in the sentence .
This increases the difficulty of SMT that relies on phrase correspondence and is not good at distant word reordering ( Zhang et al. , 2017 ) .
We implement our SMT systems by Moses ( Koehn et al. , 2007 ) . Semi-Supervised SMT .
Previous works have shown that SMT can be improved by two semisupervised methods : ( 1 ) A big language model ( Koehn and Knowles , 2017 ) , i.e. , a language model trained with big target - side monolingual data ; ( 2 ) Synthesizing bilingual data by backtranslating monolingual data ( Bertoldi and Federico , 2009 ; Lambert et al. , 2011 ) .
Using our Cherokee monolingual data and the publicly available English monolingual data , we test these two methods .
For the first method , we use both parallel and monolingual data to train the language model ; for the second method , we back - translate target - language monolingual data into the source language and then combine them with the training set to retrain a source - target SMT model .
NMT Supervised NMT .
NMT has mostly dominated recent machine translation research .
Especially when a large amount of parallel data is available , NMT surpasses SMT by a large margin ; moreover , NMT is good at generating fluent translations because of its auto-regressive generation nature .
Koehn and Knowles ( 2017 ) pointed out the poor performance of NMT under low-resource and out - of- domain conditions ; however , recent work from Sennrich and Zhang ( 2019 ) showed that low-resource NMT can be better than SMT by using proper training techniques and hyperparameters .
NMT models usually follow encoderdecoder architecture .
The encoder encodes the source sentence into hidden representations , then the decoder generates the target sentence word by word by " reading " these representations , as shown in Figure 3 .
We investigate two paradigms of NMT implementations : RNN - based model ( Bahdanau et al. , 2015 ) and Transformer - based model ( Vaswani et al. , 2017 ) .
We implement both of them via OpenNMT ( Klein et al. , 2017 ) . For RNN - NMT , we follow the global attentional model with general attention proposed by Luong et al . ( 2015 ) . For Transformer - NMT , we mainly follow the architecture proposed by Vaswani et al . ( 2017 ) except applying layer normalization before the selfattention and FFN blocks instead of after , which is more robust ( Baevski and Auli , 2019 ) . Semi-Supervised NMT .
NMT models can often be improved when more training data is available ; therefore , a lot of works have studied semisupervised approaches that utilize monolingual data to improve translation performance .
Similar to SMT , we mainly investigate two semisupervised methods .
The first is to leverage pretrained language models .
Early works proposed shallow or deep fusion methods to rerank NMT outputs or add the language model 's hidden states to NMT decoder ( Jean et al. , 2015 ; Gulcehre et al. , 2015 ) .
Recently , the large-scale pre-trained language model , BERT ( Devlin et al. , 2019 ) , has achieved impressive success in many NLP tasks .
Zhu et al . ( 2020 ) showed that incorporating the contextualized BERT representations can significantly improve translation performances .
Following but different from this work , we explore four different ways to incorporate BERT representations into NMT models for English - Cherokee translation only .
9
As depicted in Figure 4 Note that 3 ? and 4 ? will not be applied simultaneously , and all the combination of these four methods are treated as hyper-parameters , details are in Appendix B.4 .
In general , we hope BERT representations can help encoder understand English sentences better and thus improve translation performance .
We also test Multilingual - BERT ( Devlin et al. , 2019 ) to see if a multilingual pre-trained model can generalize better to a newly encountered language .
The second semi-supervised method we try is again the back -translation method .
Sennrich et al. ( 2016 b ) has shown that applying this method on NMT obtains larger improvement than applying it on SMT , and it works better than the shallow or deep fusion methods .
Transferring & Multilingual NMT .
Another important line of research is to improve lowresource translation performance by incorporating knowledge from other language pairs .
As mentioned in Section 1 , Cherokee is the sole member of the southern branch of the Iroquoian language family , so it seems that Cherokee is not " genealogically " related to any high- resource languages in terms of their language family trees .
However , it is still interesting to see whether the translation knowledge between other languages and English can help with the translation between Cherokee and English .
Hence , in this paper , we will explore two ways of leveraging other language pairs : Transfer learning and Multilingual joint training .
Kocmi and Bojar ( 2018 ) proposed a simple and effective continual training strategy for the transfer learning of translation models .
This method will first train a " parent " model using one language pair until convergence ; then continue the training using another language pair , so as to transfer the translation knowledge of the first language pair to the second pair .
Johnson et al. ( 2017 ) introduced the " many - to- one " and " one-to-many " methods for multilingual joint training of X- En and En-X systems .
They achieve this by simply combining training data , except for the " one-tomany " method , every English sentence needs to start with a special token to specify the language to be translated into .
We test both the transferring and multilingual methods for Chr-En / En - Chr translations with 4 other X- En / En-X language pairs ( X=Czech / German / Russian / Chinese ) .
Results
Experimental Details
We randomly sample 5K - 100K sentences ( about 0.5 - 10 times the size of the parallel training set ) from News Crawl 2017 10 as our English monolingual data .
We randomly sample 12K -58 K examples ( about 1 - 5 times the size of parallel training set ) for each of the 4 language pairs ( Czech / German / Russian / Chinese-English ) from News Commentary v13 of WMT2018 11 and Bibleuedin ( Christodouloupoulos and Steedman , 2015 ) on OPUS 12 .
We apply tokenizer and truecaser from Moses ( Koehn et al. , 2007 ) .
We also apply the BPE tokonization ( Sennrich et al. , 2016 c ) , but instead of using it as default , we treat it as hyperparameter .
For systems with BERT , we apply the WordPiece tokenizer ( Devlin et al. , 2019 ) .
We compute detokenized and case-sensitive BLEU score ( Papineni et al. , 2002 ) using SacreBLEU ( Post , 2018 ) . 13
We implement our SMT systems via Moses ( Koehn et al. , 2007 ) . SMT denotes the base system ; SMT + bigLM represents the SMT system that uses additional monolingual data to train its language model ; SMT with back - translation is denoted by SMT + BT .
Our NMT systems are implemented by OpenNMT toolkit ( Klein et al. , 2017 ) .
Two baselines are RNN - NMT and Transformer - NMT .
For En-Chr , we also test adding BERT or Multilingual - BERT representations ( Devlin et al. , 2019 ) , NMT + BERT or NMT + mBERT , and with back - translation , NMT + BT .
For Chr-En , we only test NMT + BT , treating the English monolingual data size as hyper-parameter .
For both En-Chr and Chr-En , we test Transfer learning from and Multilingual joint training with 4 other languages denoted by NMT +X ( T ) and NMT +X ( M ) respectively , where X = Czech / German / Russian / Chinese .
We treat the X - En data size as hyper-parameter .
All other detailed model designs and hyperparameters are introduced in Appendix B .
Quantitative Results
Our main experimental results are shown in Table 3 and Table 4 mance is poor compared with the results of some high- resource translations ( Sennrich et al. , 2016a ) , which means that current popular SMT and NMT techniques still struggle to translate well between Cherokee and English especially for the out-ofdomain generalization .
Chr - En vs. En- Chr. Overall , the Cherokee -English translation gets higher BLEU scores than the English - Cherokee translation .
It is reasonable because English has a smaller vocabulary and simpler morphology ; thus , it is easier to generate .
SMT vs. NMT .
For in- domain evaluation , the best NMT systems surpass SMT for both translation directions .
It could result from our extensive architecture hyper-parameter search ; or , it supports our conjecture that SMT is not necessarily better than NMT because of the different word orders .
But , SMT is dominantly better than NMT for out - of- domain evaluation , which is consistent with the results in Koehn and Knowles ( 2017 ) .
RNN vs. Transformer .
Transformer -NMT performs worse than RNN - NMT , which contradicts the trends of some high- resource translations ( Vaswani et al. , 2017 ) .
We conjecture that Transformer architecture is more complex than RNN and thus requires larger -scale data to train properly .
We also notice that Transformer models are very sensitive to hyper-parameters , so it can be possibly improved after a more extensive hyperparameter search .
The best Transformer - NMT has a 5 - layer encoder / decoder and 2 - head attention , which is smaller -scale than the model used for high- resource translations ( Vaswani et al. , 2017 ) .
Another interesting observation is that previous works have shown applying BPE and using a small vocabulary by setting minimum word frequency are beneficial for low-resource translation ( Sennrich et al. , 2016 c ; Sennrich and Zhang , 2019 ) ; however , these techniques are not always being favored during our model selection procedure , as shown in Appendix B.4 .
Supervised vs. Semi-supervised .
As shown in Table 3 , using a big language model and backtranslation both only slightly improve SMT baselines on both directions .
For English - Cherokee translation , leveraging BERT representations improves RNN - NMT by 0.4/0.5 BLEU points on Dev/ Test .
Multilingual -BERT does not work better than BERT .
Back - translation with our Cherokee monolingual data barely improves performance for both in - domain and out -of- domain evaluations , probably because the monolingual data is also out - of- domain , 72 % of the unique Cherokee tokens are unseen in the whole parallel data .
For Cherokee - English translation , back - translation improves the out-of- domain evaluation of RNN - NMT by 0.9/0.9 BLEU points on Out-dev / Out-test , while it does not obviously improve in-domain evaluation .
A possible reason is that the English monolingual data we used is news data that is not of the same domain as Dev / Test but closer to Outdev / Out - test so that it helps the model to do domain adaptation .
We also investigate the influence of the English monolingual data size .
We find that all of the NMT +BT systems perform best when only using 5 K English monolingual data , see Figure 5 in Appendix B.5 .
that even though the 4 languages are not related to Cherokee , their translation knowledge can still be helpful .
Transferring from the Chinese-English model and joint training with English - German data achieve our best in - domain Cherokee -English and English - Cherokee performance , respectively .
However , there is barely an improvement on the out-of- domain evaluation sets , even though the X- En / En-X data is mostly news ( same domain as Outdev / Out-test ) .
On average , multilingual joint training performs slightly better than transfer learning and usually prefers a larger X- En / En-X data size ( see details in Appendix B.4 ) .
Transferring vs. Multilingual .
Qualitative Results Automatic metrics are not always ideal for natural language generation ( Wieting et al. , 2019 ) .
As a new language to the NLP community , we are also not sure if BLEU is a good metric for Cherokee evaluation .
Therefore , we conduct a smallscale human ( expert ) pairwise comparison by our coauthor between the translations generated by our NMT and SMT systems .
We randomly sample 50 examples from Test or Out-test , anonymously shuffle the translations from two systems , and ask our coauthor to choose which one they think is better .
15
As shown in Table 5 , human preference does not always follow the trends of BLEU scores .
For English - Cherokee translation , though the RNN - NMT + BERT ( N5 ) has a better BLEU score than SMT + BT ( S3 ) ( 12.2 vs. 9.9 ) , it is liked less by humans ( 21 vs. 29 ) , indicating that BLEU is possibly not a suitable for Cherokee evaluation .
A detailed study is beyond the scope of this paper but is an interesting future work direction .
15
The author , who conducted this human study , was not involved in the development of MT systems .
Table 5 : Human comparison between the translations generated from our NMT and SMT systems .
If A vs. B , " Win " or " lose " means that the evaluator favors A or B. Systems IDs correspond to the IDs in Table 3 .
Conclusion and Future Work
In this paper , we make our effort to revitalize the Cherokee language by introducing a clean Cherokee - English parallel dataset , ChrEn , with 14 K sentence pairs ; and 5 K Cherokee monolingual sentences .
It not only can be another resource for low-resource machine translation research but also will help to attract attention from the NLP community to save this dying language .
Besides , we propose our Chr-En and En- Chr baselines , including both SMT and NMT models , using both supervised and semi-supervised methods , and exploring both transfer learning and multilingual joint training methods with 4 other languages .
Experiments show that SMT is significantly better and NMT under out - of- domain condition while NMT is better for in- domain evaluation ; and the semi-supervised learning , transfer learning , and multilingual joint training can improve simply supervised baselines .
Overall , our best models achieve 15.8/12.7 BLEU for in - domain Chr-En / En-Chr translations and 6.5/5.0 BLEU for outof-domain Chr-En / En- Chr translations .
We hope these diverse baselines will serve as useful strong starting points for future work by the community .
Our future work involves converting the monolingual data to parallel and collecting more data from the news domain .
Cherokee : the Overhill dialect , most widely spoken in Oklahoma ( OK ) , and the Middle dialect , most widely used in North Carolina ( NC ) .
