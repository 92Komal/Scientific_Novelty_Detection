title
Context-Dependent Translation Selection Using Convolutional Neural Network
abstract
We propose a novel method for translation selection in statistical machine translation , in which a convolutional neural network is employed to judge the similarity between a phrase pair in two languages .
The specifically designed convolutional architecture encodes not only the semantic similarity of the translation pair , but also the context containing the phrase in the source language .
Therefore , our approach is able to capture context-dependent semantic similarities of translation pairs .
We adopt a curriculum learning strategy to train the model : we classify the training examples into easy , medium , and difficult categories , and gradually build the ability of representing phrases and sentencelevel contexts by using training examples from easy to difficult .
Experimental results show that our approach significantly outperforms the baseline system by up to 1.4 BLEU points .
Introduction Conventional statistical machine translation ( SMT ) systems extract and estimate translation pairs based on their surface forms ( Koehn et al. , 2003 ) , which often fail to capture translation pairs which are grammatically and semantically similar .
To alleviate the above problems , several researchers have proposed learning and utilizing semantically similar translation pairs in a continuous space ( Gao et al. , 2014 ; Zhang et al. , 2014 ; Cho et al. , 2014 ) .
The core idea is that the two phrases in a translation pair should share the same semantic meaning and have similar ( close ) feature vectors in the continuous space .
The above methods , however , neglect the information of local contexts , which has been proven to be useful for disambiguating translation candidates during decoding Marton and Resnik , 2008 ) .
The matching scores of translation pairs are treated the same , even they are in different contexts .
Accordingly , the methods fail to adapt to local contexts and lead to precision issues for specific sentences in different contexts .
To capture useful context information , we propose a convolutional neural network architecture to measure context-dependent semantic similarities between phrase pairs in two languages .
For each phrase pair , we use the sentence containing the phrase in source language as the context .
With the convolutional neural network , we summarize the information of a phrase pair and its context , and further compute the pair 's matching score with a multi-layer perceptron .
We discriminately train the model using a curriculum learning strategy .
We classify the training examples according to the difficulty level of distinguishing the positive candidate from the negative candidate .
Then we train the model to learn the semantic information from easy ( basic semantic similarities ) to difficult ( context- dependent semantic similarities ) .
Experimental results on a large-scale translation task show that the context-dependent convolutional matching ( CDCM ) model improves the performance by up to 1.4 BLEU points over a strong phrase - based SMT system .
Moreover , the CDCM model significantly outperforms its context- independent counterpart , proving that it is necessary to incorporate local contexts into SMT .
Contributions .
Our key contributions include : ? we introduce a novel CDCM model to capture context-dependent semantic similarities between phrase pairs ( Section 2 ) ; ? we develop a novel learning algorithm to train the CDCM model using a curriculum learning strategy ( Section 3 ) .
Context-Dependent Convolutional Matching Model
The model architecture , shown in Figure 1 , is a variant of the convolutional architecture of .
It consists of two components : ? convolutional sentence model that summarizes the meaning of the source sentence and the target phrase ; ? matching model that compares the two representations with a multi-layer perceptron ( Bengio , 2009 ) .
Let ? be a target phrase and f be the source sentence that contains the source phrase aligning to ?.
We first project f and ?
into feature vectors x and y via the convolutional sentence model , and then compute the matching score s(x , y ) by the matching model .
Finally , the score is introduced into a conventional SMT system as an additional feature .
Convolutional sentence model .
As shown in Figure 1 , the model takes as input the embeddings of words ( trained beforehand elsewhere ) in f and ?.
It then iteratively summarizes the meaning of the input through layers of convolution and pooling , until reaching a fixed length vectorial representation in the final layer .
In Layer - 1 , the convolution layer takes sliding windows on f and ?
respectively , and models all the possible compositions of neighbouring words .
The convolution involves a filter to produce a new feature for each possible composition .
Given a k-sized sliding window i on f or ? , for example , the jth convolution unit of the composition of the words is generated by : c i ( 1 , j ) = g ( ? i ( 0 ) ) ? ?( w ( 1 , j ) ? ? i ( 0 ) + b ( 1 , j ) ) ( 1 ) where ? g( ? ) is the gate function that determines whether to activate ?( ? ) ; ? ?(? ) is a non-linear activation function .
In this work , we use ReLu ( Dahl et al. , 2013 ) as the activation function ; ? w ( 1 , j ) is the parameters for the jth convolution unit on Layer - 1 , with matrix , 1 ) , . . . , w ( 1 , J ) ] ; W ( 1 ) = [ w ( 1 ? ? i ( 0 ) is a vector constructed by concatenating word vectors in the k-sized sliding widow i ; ? b ( 1 , j ) is a bias term , with vector B ( 1 ) = [ b ( 1,1 ) , . . . , b ( 1 , J ) ] .
To distinguish the phrase pair from its context , we use one additional dimension in word embeddings : 1 for words in the phrase pair and 0 for the others .
After transforming words to their tagged embeddings , the convolutional sentence model takes multiple choices of composition using sliding windows in the convolution layer .
Note that sliding windows are allowed to cross the boundary of the source phrase to exploit both phrasal and contextual information .
In Layer - 2 , we apply a local max-pooling in non-overlapping 1 ? 2 windows for every convolution unit c ( 2 , j ) i = max {c ( 1 , j ) 2 i , c ( 1 , j ) 2 i+1 } ( 2 ) In Layer - 3 , we perform convolution on output from Layer - 2 : c i ( 3 , j ) = g ( ? i ( 2 ) ) ? ?( w ( 3 , j ) ? ? i ( 2 ) + b ( 3 , j ) ) ( 3 )
After more convolution and max-pooling operations , we obtain two feature vectors for the source sentence and the target phrase , respectively .
Matching model .
The matching score of a source sentence and a target phrase can be measured as the similarity between their feature vectors .
Specifically , we use the multi-layer perceptron ( MLP ) , a nonlinear function for similarity , to compute their matching score .
First we use one layer to combine their feature vectors to get a hidden state h c : h c = ?( w c ? [ x fi : y ?j ] + b c ) ( 4 )
Then we get the matching score from the MLP : s( x , y ) = M LP ( h c ) ( 5 ) 3 Training
We employ a discriminative training strategy with a max-margin objective .
Suppose we are given the following triples y + , y ? ) from the oracle , where x , y + , y ? are the feature vectors for f , ?+ , ? respectively .
We have the ranking - based loss as objective : L ? ( x , y + , y ? ) = max ( 0 , 1+s( x , y ? )?s( x , y + ) ) ( 6 ) where s(x , y ) is the matching score function defined in Eq. 5 , ? consists of parameters for both the convolutional sentence model and MLP .
The model is trained by minimizing the above objective , to encourage the model to assign higher matching scores to positive examples and to assign lower scores to negative examples .
We use stochastic gradient descent ( SGD ) to optimize the model parameters ?.
We train the CDCM model with a curriculum strategy to learn the contextdependent semantic similarity at the phrase level from easy ( basic semantic similarities between the source and target phrase pair ) to difficult ( context- dependent semantic similarities for the same source phrase in varying contexts ) .
Curriculum Training Curriculum learning , first proposed by Bengio et al . ( 2009 ) in machine learning , refers to a sequence of training strategies that start small , learn easier aspects of the task , and then gradually increase the difficulty level .
It has been shown that the curriculum learning can benefit the nonconvex training by giving rise to improved generalization and faster convergence .
The key point is that the training examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts , and gradually more complex ones .
For each positive example ( f , ?+ ) , we have three types of negative examples according to the difficulty level of distinguishing the positive example from them : ?
Easy : target phrases randomly chosen from the phrase table ; ?
Medium : target phrases extracted from the aligned target sentence for other non-overlap source phrases in the source sentence ; ?
Difficult : target phrases extracted from other candidates for the same source phrase .
We want the CDCM model to learn the following semantic information from easy to difficult : ? the basic semantic similarity between the source sentence and target phrase from the easy negative examples ; ? the general semantic equivalent between the source and target phrase pair from the medium negative examples ; ? the context-dependent semantic similarities for the same source phrase in varying contexts from the difficult negative examples .
Alg For each curriculum ( lines 12 - 16 ) , we compute the gradient of the loss objective L ? and learn ? using the SGD algorithm .
Note that we meanwhile update the word embeddings to better capture the semantic equivalence across languages during training .
If the loss function L ? reaches a local minima or the iterations reach the predefined number , we terminate this curriculum .
Related Work
Our research builds on previous work in the field of context- dependent rule matching and bilingual phrase representations .
There is a line of work that employs local contexts over discrete representations of words or phrases .
For example , , and Marton and Resnik ( 2008 ) employed within-sentence contexts that consist of discrete words to guide rule matching .
Wu et al . ( 2014 ) exploited discrete contextual features in the source sentence ( e.g. words and part- of-speech tags ) to learn better bilingual word embeddings for SMT .
In this study , we take into account all the phrase pairs and directly compute phrasal similarities with convolutional representations of the local contexts , integrating the strengths associated with the convolutional neural networks ( Collobert and Weston , 2008 ) .
In recent years , there has also been growing interest in bilingual phrase representations that group phrases with a similar meaning across different languages .
Based on that translation equivalents share the same semantic meaning , they can supervise each other to learn their semantic phrase embeddings in a continuous space ( Gao et al. , 2014 ; Zhang et al. , 2014 ) .
However , these models focused on capturing semantic similarities between phrase pairs in the global contexts , and neglected the local contexts , thus ignored the useful discriminative information .
Alternatively , we integrate the local contexts into our convolutional matching architecture to obtain context-dependent semantic similarities .
Meng et al. ( 2015 ) and Zhang ( 2015 ) have proposed independently to summary source sentences with convolutional neural networks .
However , they both extend the neural network joint model ( NNJM ) of Devlin et al . ( 2014 ) to include the whole source sentence , while we focus on capturing context-dependent semantic similarities of translation pairs .
Experiments
Setup
We carry out our experiments on the NIST Chinese -English translation tasks .
Our training data contains 1.5 M sentence pairs coming from LDC dataset .
1 We train a 4 - gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit ( Stolcke , 2002 ) with modified Kneser -Ney Smoothing ( Kneser and Ney , 1995 ) .
We use the 2002 NIST MT evaluation test data as the development data , and the 2004 , 2005 NIST MT evaluation test data as the test data .
We use minimum error rate training ( Och , 2003 ) to optimize the feature weights .
For evaluation , case- insensitive NIST BLEU ( Papineni et al. , 2002 ) is used to measure translation performance .
We perform a significance test using the sign-test approach ( Collins et al. , 2005 )
For training the neural networks , we use 4 convolution layers for source sentences and 3 convolution layers for target phrases .
For both of them , 4 pooling layers ( pooling size is 2 ) are used , and all the feature maps are 100 .
We set the sliding window k = 3 , and the learning rate ? = 0.02 .
All the parameters are selected based on the development data .
We train the word embeddings using a bilingual strategy similar to Yang et al . ( 2013 ) , and set the dimension of the word embeddings be 50 .
To produce high-quality bilingual phrase pairs to train the CDCM model , we perform forced decoding on the bilingual training sentences and collect the used phrase pairs .
Evaluation of Translation Quality
We have two baseline systems : ?
Baseline :
The baseline system is an opensource system of the phrase - based model - Moses ( Koehn et al. , 2007 ) with a set of common features , including translation models , word and phrase penalties , a linear distortion model , a lexicalized reordering model , and a language model .
? CICM ( context- independent convolutional matching ) model : Following the previous works ( Gao et al. , 2014 ; Zhang et al. , 2014 ; Cho et al. , 2014 ) , we calculate the matching degree of a phrase pair without considering any contextual information .
Each unique phrase pair serves as a positive example and a randomly selected target phrase from the phrase table is the corresponding negative example .
The matching score is also introduced into Baseline as an additional feature .
Table 1 summaries the results of CDCMs trained from different curriculums .
No matter from which curriculum it is trained , the CDCM model significantly improves the translation quality on the overall test data ( with gains of 1.0 BLEU points ) .
The best improvement can be up to 1.4 BLEU points on MT04 with the fully trained CDCM .
As expected , the translation performance is consistently increased with curriculum growing .
This indicates that the CDCM model indeed captures the desirable semantic information by the curriculum learning from easy to difficult .
Comparing with its context-independent counterpart ( CICM , Row 2 ) , the CDCM model shows significant improvement on all the test data consistently .
We contribute this to the incorporation of useful discriminative information embedded in the local context .
In addition , the performance of CICM is comparable with that of CDCM 1 .
This is intuitive , because both of them try to capture the basic semantic similarity between the source and target phrase pair .
One of the hypotheses we tested in the course of this research was disproved .
We thought it likely that the difficult curriculum ( CDCM 3 that distinguishs the correct translation from other candidates for a given context ) would contribute most to the improvement , since this circumstance is more consistent with the real decoding procedure .
This turned out to be false , as shown in Table 1 .
One possible reason is that the " negative " examples ( other candidates for the same source phrase ) may share the same semantic meaning with the positive one , thus give a wrong guide in the supervised training .
Constructing a reasonable set of negative examples that are more semantically different from the positive one is left for our future work .
Conclusion
In this paper , we propose a context-dependent convolutional matching model to capture semantic similarities between phrase pairs that are sensitive to contexts .
Experimental results show that our approach significantly improves the translation performance and obtains improvement of 1.0 BLEU scores on the overall test data .
Integrating deep architecture into contextdependent translation selection is a promising way to improve machine translation .
In the future , we will try to exploit contextual information at the target side ( e.g. , partial translations ) .
Figure 1 : 1 Figure 1 : Architecture of the CDCM model .
The convolutional sentence model ( bottom ) summarizes the meaning of the tagged sentence and target phrase , and the matching model ( top ) compares the representations using a multi-layer perceptron . " / " indicates all - zero padding turned off by the gating function .
