title
Maximum Expected BLEU
Training of Phrase and Lexicon Translation Models
abstract
This paper proposes a new discriminative training method in constructing phrase and lexicon translation models .
In order to reliably learn a myriad of parameters in these models , we propose an expected BLEU score- based utility function with KL regularization as the objective , and train the models on a large parallel dataset .
For training , we derive growth transformations for phrase and lexicon translation probabilities to iteratively improve the objective .
The proposed method , evaluated on the Europarl German- to - English dataset , leads to a 1.1 BLEU point improvement over a state - of - the - art baseline translation system .
In IWSLT 2011 Benchmark , our system using the proposed method achieves the best Chinese-to - English translation result on the task of translating TED talks .
Introduction Discriminative training is an active area in statistical machine translation ( SMT ) ( e.g. , Och et al. , 2002 , 2003 , Liang et al. , 2006 , Blunsom et al. , 2008 , Chiang et al. , 2009 , Foster et al , 2010 , Xiao et al .
2011 . Och ( 2003 ) proposed using a loglinear model to incorporate multiple features for translation , and proposed a minimum error rate training ( MERT ) method to train the feature weights to optimize a desirable translation metric .
While the log-linear model itself is discriminative , the phrase and lexicon translation features , which are among the most important components of SMT , are derived from either generative models or heuristics ( Koehn et al. , 2003 , Brown et al. , 1993 .
Moreover , the parameters in the phrase and lexicon translation models are estimated by relative frequency or maximizing joint likelihood , which may not correspond closely to the translation measure , e.g. , bilingual evaluation understudy ( BLEU ) ( Papineni et al. , 2002 ) .
Therefore , it is desirable to train all these parameters to directly maximize an objective that directly links to translation quality .
However , there are a large number of parameters in these models , making discriminative training for them non-trivial ( e.g. , Liang et al. , 2006 , Chiang et al. , 2009 . Liang et al. ( 2006 ) proposed a large set of lexical and Part- of - Speech features and trained the model weights associated with these features using perceptron .
Since many of the reference translations are non-reachable , an empirical local updating strategy had to be devised to fix this problem by picking a pseudo reference .
Many such non-desirable heuristics led to moderate gains reported in that work .
Chiang et al. ( 2009 ) improved a syntactic SMT system by adding as many as ten thousand syntactic features , and used Margin Infused Relaxed Algorithm ( MIRA ) to train the feature weights .
However , the number of parameters in common phrase and lexicon translation models is much larger .
In this work , we present a new , highly effective discriminative learning method for phrase and lexicon translation models .
The training objective is an expected BLEU score , which is closely linked to translation quality .
Further , we apply a Kullback - Leibler ( KL ) divergence regularization to prevent over-fitting .
For effective optimization , we derive updating formulas of growth transformation ( GT ) for phrase and lexicon translation probabilities .
A GT is a transformation of the probabilities that guarantees strict non-decrease of the objective over each GT iteration unless a local maximum is reached .
A similar GT technique has been successfully used in speech recognition ( Gopalakrishnan et al. , 1991 , Povey , 2004 , He et al. , 2008 .
Our work demonstrates that it works with large scale discriminative training of SMT model as well .
Our work is based on a phrase - based SMT system .
Experiments on the Europarl German-to - English dataset show that the proposed method leads to a 1.1 BLEU point improvement over a strong baseline .
The proposed method is also successfully evaluated on the IWSLT 2011 benchmark test set , where the task is to translate TED talks ( www.ted.com).
Our experimental results on this open-domain spoken language translation task show that the proposed method leads to significant translation performance improvement over a state - of - the - art baseline , and the system using the proposed method achieved the best single system translation result in the Chineseto - English MT track .
Related Work
One best known approach in discriminative training for SMT is proposed by Och ( 2003 ) .
In that work , multiple features , most of them are derived from generative models , are incorporated into a log-linear model , and the relative weights of them are tuned discriminatively on a small tuning set .
However , in practice , this approach only works with a handful of parameters .
More closely related to our work , Liang et al . ( 2006 ) proposed a large set of lexical and Part- of - Speech features in addition to the phrase translation model .
Weights of these features are trained using perceptron on a training set of 67 K sentences .
In that paper , the authors pointed out that forcing the model to update towards the reference translation could be problematic .
This is because the hidden structure such as phrase segmentation and alignment could be abused if the system is forced to produce a reference translation .
Therefore , instead of pushing the parameter update towards the reference translation ( a.k.a. bold updating ) , the author proposed a local updating strategy where the model parameters are updated towards a pseudo-reference ( i.e. , the hypothesis in the n-best list that gives the best BLEU score ) .
Experimental results showed that their approach outperformed a baseline by 0.8 BLEU point when using monotonic decoding , but there was no significant gain over a stronger baseline with a full-distortion model .
In our work , we use the expectation of BLEU scores as the objective .
This avoids the heuristics of picking the updating reference and therefore gives a more principal way of setting the training objective .
As another closely related study , Chiang et al . ( 2009 ) , Tromble et al. , 2008 and lattice - based MERT .
In these earlier work , however , the phrase and lexicon translation models used remained unchanged .
Another line of research that is closely related to our work is phrase table refinement and pruning .
Wuebker et al. ( 2010 ) proposed a method to train the phrase translation model using Expectation - Maximization algorithm with a leave- one - out strategy .
The parallel sentences were forced to be aligned at the phrase level using the phrase table and other features as in a decoding process .
Then the phrase translation probabilities were estimated based on the phrase alignments .
To prevent overfitting , the statistics of phrase pairs from a particular sentence was excluded from the phrase table when aligning that sentence .
However , as pointed out by Liang et al ( 2006 ) , the same problem as in the bold updating existed , i.e. , forced alignment between a source sentence and its reference translation was tricky , and the proposed alignment was likely to be unreliable .
The method presented in this paper is free from this problem .
Phrase - based Translation System
The translation process of phrase - based SMT can be briefly described in three steps : segment source sentence into a sequence of phrases , translate each source phrase to a target phrase , re-order target phrases into target sentence ( Koehn et al. , 2003 ) .
In decoding , the optimal translation ? given the source sentence F is obtained according to ?
= argmax ! ? ? ? ( 1 ) where ? ? ? = 1 ? ? ? ! log ? ! ( ? , ? ) ! ( 2 ) and ? = ? ? ! log ? ! ( ? , ? ) ! ! is the normalization denominator to ensure that the probabilities sum to one .
Note that we define the feature functions {? ! ( ? , ? ) } in log domain to simplify the notation in later sections .
Feature weights ? = {? ! } are usually tuned by MERT .
Features used in a phrase - based system usually include LM , reordering model , word and phrase counts , and phrase and lexicon translation models .
Given the focus of this paper , we review only the phrase and lexicon translation models below .
Phrase translation model A set of phrase pairs are extracted from wordaligned parallel corpus according to phrase extraction rules ( Koehn et al. , 2003 ) .
Phrase translation probabilities are then computed as relative frequencies of phrases over the training dataset .
i.e. , the probability of translating a source phrase ? to a target phrase ? is computed by ? ? ? = ?(? , ? ) ?(? ) ( 3 ) where ?(? , ? ) is the joint counts of ? and ? , and ?(? ) is the marginal counts of ?.
In translation , the input sentence is segmented into K phrases , and the source - to - target forward phrase ( FP ) translation feature is scored as : ? ! " ? , ? = ? ? ! ? ! ! ( 4 ) where ? ! and ? ! are the k-th phrase in E and F , respectively .
The target- to-source ( backward ) phrase translation model is defined similarly .
Lexicon translation model
There are several variations in lexicon translation features ( Ayan and Dorr 2006 , Koehn et al. , 2003 , Quirk et al. , 2005 .
We use the word translation table from IBM Model 1 ( Brown et al. , 1993 ) and compute the sum over all possible word alignments within a phrase pair without normalizing for length ( Quirk et al. , 2005 ) .
The source- to- target forward lexicon ( FL ) translation feature is : ? ! " ? , ? = ? ? ! , ! ? ! , ! ! ! ! ( 5 ) where ? ! , ! is the m-th word of the k-th target phrase ? ! , ? ! , ! is the r-th word in the k-th source phrase ? ! , and ?(? ! , ! |? ! , ! ) is the probability of translating word ? ! , ! to word ? ! , ! .
In IBM model 1 , these probabilities are learned via maximizing a joint likelihood between the source and target sentences .
The target- to-source ( backward ) lexicon translation model is defined similarly .
Maximum Expected-BLEU
Training
Objective function
We denote by ?
the set of all the parameters to be optimized , including forward phrase and lexicon translation probabilities and their backward counterparts .
For simplification of notation , ? is formed as a matrix , where its elements {? ! " } are probabilities subject to ? ! " ! = 1 . E.g. , each row is a probability distribution .
The utility function over the entire training set is defined as : ?(? ) = ? ? (? ! , ? , ? ! |? ! , ? , ? ! ) ?(? ! , ? ! * ) ! !!! ! ! , ? , ! ! ( 6 ) where N is the number of sentences in the training set , ? ! * is the reference translation of the n-th source sentence ? ! , and ? ! ? ?(? ! ) that denotes the list of translation hypotheses of ? ! .
Since the sentences are independent with each other , the joint posterior can be decomposed : ? ? ? ! , ? , ? ! ? ! , ? , ? ! = ? ? ? ! ? ! ! !!! ( 7 ) and ? ? ? ! ? ! is the posterior defined in ( 2 ) , the subscript ?
indicates that it is computed based on the parameter set ?. ? ? is proportional ( with a factor of N ) to the expected sentence BLEU score over the entire training set , i.e. , after some algebra , ?(? ) = ? ? (? ! |? ! ) ?(? ! , ? ! * ) ! ! ! !!!
In a phrase - based SMT system , the total number of parameters of phrase and lexicon translation models , which we aim to learn discriminatively , is very large ( see Table 1 ) .
Therefore , regularization is critical to prevent over-fitting .
In this work , we regularize the parameters with KL regularization .
KL divergence is commonly used to measure the distance between two probability distributions .
For the whole parameter set ? , the KL regularization is defined in this work as the sum of KL divergence over the entire parameter space : ?(? ! ||? ) = ? ! " ! log ? ! " ! ? ! " ! ! ( 8 ) where ? ! is a constant prior parameter set .
In training , we want to improve the utility function while keeping the changes of the parameters from ? ! at minimum .
Therefore , we design the objective function to be maximized as : ? ? = log ? ? ? ? ? ?(? ! ||? ) ( 9 ) where the prior model ? ! in our approach is the relative - frequency - based phrase translation model and the maximum- likelihood - estimated IBM model 1 ( word translation model ) .
? is a hyperparameter controlling the degree of regularization .
Optimization
In this section , we derived GT formulas for iteratively updating the parameters so as to optimize objective ( 9 ) .
GT is based on extended Baum-Welch ( EBW ) algorithm first proposed by Gopalakrishnan et al . ( 1991 ) and commonly used in speech recognition ( e.g. , He et al. 2008 ) .
Extended Baum-Welch Algorithm Baum- Eagon inequality ( Baum and Eagon , 1967 ) gives the GT formula to iteratively maximize positive - coefficient polynomials of random variables that are subject to sum-to-one constants .
Baum - Welch algorithm is a model update algorithm for hidden Markov model which uses this GT .
Gopalakrishnan et al. ( 1991 ) extended the algorithm to handle rational function , i.e. , a ratio of two polynomials , which is more commonly encountered in discriminative training .
Here we briefly review EBW .
Assuming a set of random variables ? = {? ! " } that subject to the constraint that ii ) Derive GT formula for ? ? ? ! " ! = 1 ? ! " = ? ! " ! ?(? ) ? ! " ?!?!
+ ? ? ? ! " ! ? ! " ! ?(? ) ? ! " ?!?! ! + ? ( 11 ) where D is a smoothing factor .
GT of Translation Models
Now we derive the GTs of translation models for our objective .
Since maximizing ? ? is equivalent to maximizing ? ! ? , we have the following auxiliary function : ? ? = ?(? ) ? !!?! "(? ! ||? ) ( 12 ) After substituting ( 2 ) and ( 7 ) into ( 6 ) , and drop optimization irrelevant terms in KL regularization , we have ? ? in a rational function form : ? ? = ? ? ? ? ? ?(? ) ( 13 ) where ? ? = ? ! ! ! ? ! , ? ! ! ! !!! ! ! , ? , ! ! , ? ? = ? ! " !! ! " ! ! ! , and ? ? = ? ! ! ! ? ! , ? ! ! !!! ? ? ! , ? ! * ! !!! ! ! , ? , ! ! are all positive polynomials of ?.
Therefore , we can follow the two steps of EBW to derive the GT formulas for ?.
If we denote by ? ! " the probability of translating the source phrase i to the target phrase
Then , the updating formula is ( derivation omitted ) : ? ! " = ? ! " (? ! , ? , ? , ? ) ! ! ! + ? ? ? ! " ? ! " ! + ? ! ? ! " ! ? ! " (? ! , ? , ? , ? ) ! ! ! ! + ? ? ? ! " + ? ! ( 14 ) where ? ! " = ?/? ! " and ? ! " ? ! , ? , ? , ? = ? ?!
? ! ? ! ? ? ? ! , ? ! * ? ? ! ? ? ?(? ! , ! = ? , ? ! , ! = ? ) ! .
In which ? ! ? takes a form similar to ( 6 ) , but is the expected BLEU score for sentence n using models from the previous iteration .
? ! , ! and ? ! , ! are the kth phrases of ? ! and ? ! , respectively .
The smoothing factor set of ? ! according to the Baum- Eagon inequality is usually far too large for practical use .
In practice , one general guide of setting ? ! is to make all updated value positive .
Similar to ( Povey 2004 ) , we set ? ! by ? ! = max ( 0 , ? ! " (? ! , ? , ? , ? ) ! ! ! ! ) ( 15 ) to ensure the denominator of ( 15 ) is positive .
Further , we set a low-bound of ? ! as max ! { ! ! ! " ! ! , ! , ! , ! ! ! ! ! ! " ! } to guarantee the numerator to be positive .
We denote by ? ! " the probability of translating the source word i to the target word j .
Then following the same derivation , we get the updating formula for forward lexicon translation model : ? ! " = ? ! " (? ! , ? , ? , ? ) ! ! ! + ? ? ? ! " ? ! " ! + ? ! ? ! " ! ? ! " (? ! , ? ! , ? , ? ) ! ! ! ! + ? ? ? ! " + ? ! ( 16 ) where ? ! " = ?/? ! " and ? ! " ? ! , ? , ? , ? = ? ?!
? ! ? ! ? ? ? ! , ? ! * ? ? ! ? ? ?(? ! , ! , ! = ? ) ! ! ? ? , ? , ? , ? , and ? ? , ? , ? , ? = ?(! ! , ! , ! !!) !!(! ! , ! , ! |! ! , ! , ! ) ! !!(! ! , ! , ! |! ! , ! , ! ) ! , in which ? ! , ! , ! and ? ! , ! , ! are the r-th and m-th word in the k-th phrase of the source sentence ? ! and the target hypothesis ? ! , respectively .
Value of ? ! is set in a way similar to ( 15 ) .
GTs for updating backward phrase and lexicon translation models can be derived in a similar way , and is omitted here .
Implementation issues
Normalizing ?
The posterior ? ?!
? ! ? ! in the model updating formula is computed according to ( 2 ) .
In decoding , only the relative values of ? matters .
However , the absolute value will affect the posterior distribution , e.g. , an overly large absolute value of ? would lead to a very sharp posterior distribution .
In order to control the sharpness of the posterior distribution , we normalize ? by its L1 norm : ? ! = ? ! |? ! | ! ( 17 )
Computing the sentence BLEU sore
The commonly used BLEU - 4 score is computed by ?-- 4 = BP ? exp 1 4 log ? ! ! !!!
( 18 ) In the updating formula , we need to compute the sentence - level ? ? ! , ? ! * .
Since the matching count may be sparse at the sentence level , we smooth raw precisions of high-order n-grams by : ? ! = #(?--? ? ) + ? ? ? ! ! #(?--? ) + ? ( 19 ) where ? ! ! is the prior value of ? ! , ? is a smoothing factor usually takes a value of 5 and ? ! ! can be set by ? ! ! = ? !!! ? ? !!! ? !!! , for n = 3 , 4 . ? ! and ? ! are estimated empirically .
Brevity penalty ( BP ) also plays a key role .
Instead of clip it at 1 , we use a non-clipped BP , ? = ? (!! ! ! ) , for sentence - level BLEU 1 .
We further scale the reference length , r , by a factor such that the total length of references on the training set equals that of the baseline output 2 .
Training procedure
The parameter set ? is optimized on the training set while the feature ? are tuned on a small tuning set 3 . Since ? and ? affect the training of each other , we train them in alternation .
I.e. , at each iteration , we first fix ? and update ? , then we re-tune ? given the new ?.
Due to mismatch between training and tuning data , the training process might not always
Therefore , we need a validation set to determine the stop point of training .
At the end , ? and ? that give the best score on the validation set are selected and applied to the test set .
Fig. 1 gives a summary of the training procedure .
Note that step 2 and 4 are parallelize - able across multiple processors .
Evaluation
In evaluating the proposed method , we use two separate datasets .
We first describe the experiments with the Europarl dataset ( Koehn 2002 ) , followed by the experiments with IWSLT - 2011 task ( Federico et al. , 2011 ) .
Experimental setup in the Europarl task
In evaluating the proposed method , we use two separate datasets .
First , we conduct experiments on the Europarl German- to - English dataset .
The training corpus contains 751 K sentence pairs , 21 words per sentence on average .
2000 sentences are provided in the development set .
We use the first 1000 sentences for ? tuning , and the rest for validation .
The test set consists of 2000 sentences .
3 Usually , the tuning set matches the test condition better , and therefore is preferable for ? tuning .
To build the baseline phrase - based SMT system , we first perform word alignment on the training set using a hidden Markov model with lexicalized distortion ( He 2007 ) , then extract the phrase table from the word aligned bilingual texts ( Koehn et al. , 2003 ) .
The maximum phrase length is set to four .
Other models used in the baseline system include lexicalized ordering model , word count and phrase count , and a 3 - gram LM trained on the English side of the parallel training corpus .
Feature weights are tuned by MERT .
A fast beam-search phrasebased decoder ( Moore and Quirk 2007 ) is used and the distortion limit is set to four .
Details of the phrase and lexicon translation models are given in Table 1 .
This baseline achieves a BLEU score of 26. 22 % on the test set .
This baseline system is also used to generate a 100 - best list of the training corpus during maximum expected BLEU training .
Translation model # parameters Phrase models ( fore .
& back . )
9.2 M Lexicon model ( IBM - 1 src-to-tgt ) 12.9 M Lexicon model ( IBM - 1 tgt-to-src ) 11.9 M Table 1 . Summary of phrase and lexicon translation models
Experimental results on the Europarl task During training , we first tune the regularization factor ? based on the performance on the validation set .
For simplicity reasons , the tuning of ? makes use of only the phrase translation models .
Table 2 reports the BLEU scores and gains over the baseline given different values of ?.
The results highlight the importance of regularization .
While ? = 5?10 !!
gives the best score on the validation set , the gain is shown to be substantially reduced to merely 0.2 BLEU point when ? = 0 , i.e. , no regularization .
We set the optimal value of ? = 5?10 !!
in all remaining experiments .
Fixing the optimal regularization factor ? , we then study the relationship between the expected 1 .
Build the baseline system , estimate { ? , ? }. 2 . Decode N- best list for training corpus using the baseline system , compute ?(? ! , ? ! * ) .
3 . Results on the Europarl German-to - English dataset .
The BLEU measures from various settings of maximum expected BLEU training are compared with the baseline , where * denotes that the gain over the baseline is statistically significant with a significance level > 99 % , measured by paired bootstrap resampling method proposed by Koehn ( 2004 ) .
Experiments on the IWSLT2011 benchmark
As the second evaluation task , we apply our new method described in this paper to the 2011 Chinese - to - English machine translation benchmark ( Federico et al. , 2011 ) .
The main focus of the IWSLT2011 Evaluation is the translation of TED talks ( www.ted.com).
These talks are originally given in English .
In the Chinese-to - English translation task , we are provided with human translated Chinese text with punctuations inserted .
The goal is to match the human transcribed English speech with punctuations .
This is an open-domain spoken language translation task .
The training data consist of 110K sentences in the transcripts of the TED talks and their translations , in English and Chinese , respectively .
Each sentence consists of 20 words on average .
Two development sets are provided , namely , dev2010 and tst2010 .
They consist of 934 sentences and 1664 sentences , respectively .
We use dev2010 for ? tuning and tst2010 for validation .
The test set tst2011 consists of 1450 sentences .
In our system , a primary phrase table is trained from the 110K TED parallel training data , and a 3 gram LM is trained on the English side of the parallel data .
We are also provided additional outof-domain data for potential usage .
From them , we train a secondary 5 - gram LM on 115 M sentences of supplementary English data , and a secondary phrase table from 500K sentences selected from the supplementary UN corpus by the method proposed by Axelrod et al . ( 2011 ) .
In carrying out the maximum expected BLEU training , we use 100 - best list and tune the regularization factor to the optimal value of ? = 1?10 !! .
We only train the parameters of the primary phrase table .
The secondary phrase table and LM are excluded from the training process since the out-of- domain phrase table is less relevant to the TED translation task , and the large LM slows down the N-best generation process significantly .
At the end , we perform one final MERT to tune the relative weights with all features including the secondary phrase table and LM .
The translation results are presented in Table 4 .
The baseline is a phrase - based system with all features including the secondary phrase table and LM .
The new system uses the same features except that the primary phrase table is discriminatively trained using maximum expected - BLEU and GT optimization as described earlier in this paper .
The results are obtained using the two -stage training schedule , including six iterations for training phrase translation models and two iterations for training lexicon translation models .
The results in Table 4 show that the proposed method leads to an improvement of 1.2 BLEU point over the baseline .
This gives the best single system result on this task .
Summary
The contributions of this work can be summarized as follows .
First , we propose a new objective function ( Eq. 9 ) for training of large-scale translation models , including phrase and lexicon models , with more parameters than all previous methods have attempted .
The objective function consists of 1 ) the utility function of expected BLEU score , and 2 ) the regularization term taking the form of KL divergence in the parameter space .
The expected BLEU score is closely linked to translation quality and the regularization is essential when many parameters are trained at scale .
The importance of both is verified experimentally with the results presented in this paper .
Second , through non-trivial derivation , we show that the novel objective function of Eq. ( 9 ) is amenable to iterative GT updates , where each update is equipped with a closed - form formula .
Third , the new objective function and new optimization technique are successfully applied to two important machine translation tasks , with implementation issues resolved ( e.g. , training schedule and hyper-parameter tuning , etc. ) .
The superior results clearly demonstrate the effectiveness of the proposed algorithm . , and assume ?(? ) and ?(? ) are two positive polynomial functions of ? , a GT of ? for the rational function ? ? = !(? ) !(? ) can be obtained through the following two steps : i ) Construct the auxiliary function : ? ? = ? ? ? ? ? ! ? ? ( 10 ) where ? ! are the values from the previous iteration .
Increasing f guarantees an increase of r , i.e. , ? ? > 0 and ? ? ? ? ? = ! ! ? ? ? ? ? ? .
Figure 1 . 1 Figure 1 .
The max expected - BLEU training algorithm .
3 . set ? = ? , ? ! = ?.
4 . Max expected BLEU training a .
Go through the training set .
i. Compute ? ?! (? ! |? ! ) and ? ! ( ? ) . ii. Accumulate statistics {?}. b. Update : ? ! ? ? by one iteration of GT .
5 . MERT on the tuning set : ? ! ? ?. 6 . Test on the validation set using { ? , ? }. 7 . Go to step 3 unless training converges or reaches a certain number of iterations .
8 . Pick the best { ? , ? } on the validation set .
sentence - level BLEU ( Exp. BLEU ) score of N-best lists and the corpus-level BLEU score of 1 - best translations .
The conjectured relationship between the two is important in justifying our use of the former as the training objective .
Fig.2 shows these two scores on the training set over training iterations .
Since the expected BLEU is affected by ? strongly , we fix the value of ? in order to make the expected BLEU comparable across different iterations .
From Fig.2 it is clear that the expected BLEU score correlates strongly with the real BLEU score , justifying its use as our training objective .
Figure 2 . 2 Figure 2 . Expected sentence BLEU and 1 - best corpus BLEU on the 751K sentence of training data .
Figure 3 . 3 Figure 3 . BLEU scores on the validation set as a function of the GT training iteration in two -stage training of both the phrase translation models ( PT ) and the lexicon models ( LEX ) .
The BLEU scores on training phrase models are shown in blue , and on training lexicon models in red .
Table 2 . 2 Results on degrees of regularizations .
BLEU scores are reported on the validation set .
? denotes the gain over the baseline .
Test on Validation Set ? % ?% Baseline 26.70 -- ? = 0 ( no regularization ) 26.91 +0.21 ? = 1?10 !! 27.31 +0.61 ? = 5?10 !! 27.44 +0.74 ? = 10?10 !! 27.27 +0.57
This is to better approximate corpus-level BLEU , i.e. , as discussed in ( Chiang , et al. , 2008 ) , the per-sentence BP might effectively exceed unity in corpus-level BLEU computation .
2
This is to focus the training on improving BLEU by improving n-gram match instead of by improving BP , e.g. , this makes the BP of the baseline output already being perfect .
