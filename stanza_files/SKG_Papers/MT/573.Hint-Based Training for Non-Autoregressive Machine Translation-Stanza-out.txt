title
Hint - Based Training for Non-Autoregressive Machine Translation
abstract
Due to the unparallelizable nature of the autoregressive factorization , AutoRegressive Translation ( ART ) models have to generate tokens sequentially during decoding and thus suffer from high inference latency .
Non-AutoRegressive Translation ( NART ) models were proposed to reduce the inference time , but could only achieve inferior translation accuracy .
In this paper , we proposed a novel approach to leveraging the hints from hidden states and word alignments to help the training of NART models .
The results achieve significant improvement over previous NART models for the WMT14 En-De and De- En datasets and are even comparable to a strong LSTMbased ART baseline but one order of magnitude faster in inference .
Introduction Neural machine translation has attracted much attention in recent years ( Bahdanau et al. , 2014 ( Bahdanau et al. , , 2016 Kalchbrenner et al. , 2016 ; Gehring et al. , 2016 ) .
Given a sentence x = ( x 1 , . . . , x Tx ) from the source language , the straight - forward way for translation is to generate the target words y = ( y 1 , . . . , y Ty ) one by one from left to right .
This is also known as the AutoRegressive Translation ( ART ) models , in which the joint probability is decomposed into a chain of conditional probabilities : P ( y|x ) = ?
Ty t=1 P ( y t |y <t , x ) , ( 1 )
While the ART models have achieved great success in terms of translation quality , the time consumption during inference is still far away from satisfactory .
During training , the predictions at different positions can be estimated in parallel since the ground truth pair ( x , y ) is exposed to the model .
However , during inference , the model
The work was performed at Microsoft Research Asia .
has to generate tokens sequentially as y <t must be inferred on the fly .
Such autoregressive behavior becomes the bottleneck of the computational time ( Wu et al. , 2016 ) .
In order to speed up the inference process , a line of works begin to develop non-autoregressive translation models .
These models break the autoregressive dependency by decomposing the joint probability with P ( y|x ) = P ( T y | x ) ?
Ty t=1 P ( y t | x ) .
( 2 )
The lost of autoregressive dependency largely hurt the consistency of the output sentences , increase the difficulty in the learning process and thus lead to a low quality translation .
Previous works mainly focus on adding different components into the NART model to improve the expressiveness of the network structure to overcome the loss of autoregressive dependency ( Gu et al. , 2017 ; Lee et al. , 2018 ; .
However , the computational overhead of new components will hurt the inference speed , contradicting with the goal of the NART models : to parallelize and speed up neural machine translation models .
To tackle this , we proposed a novel hint- based method for NART model training .
We first investigate the causes of the poor performance of the NART model .
Comparing with the ART model , we find that : ( 1 ) the positions where the NART model outputs incoherent tokens will have very high hidden states similarity ; ( 2 ) the attention distributions of the NART model are more ambiguous than those of ART model .
Therefore , we design two kinds of hints from the hidden states and attention distributions of the ART model to help the training of the NART model .
The experimental results show that our model achieves significant improvement over the NART baseline models and is even comparable to a strong ART baseline in Wu et al . ( 2016 ) . Figure 1 : Case study : the above three figures visualize the hidden state cosine similarities of different models .
The axes correspond to the generated target tokens .
Each pixel shows the cosine similarities cos ij between the last layer hidden states of the i-th and j-th generated tokens , where the diagonal pixel will always be 1.0 .
Approach
In this section , we first describe the observations on the ART and NART models , and then discuss what kinds of information can be used as hints to help the training of the NART model .
We follow the network structure in Vaswani et al . ( 2017 ) , use a copy of the source sentence as decoder input , remove the attention masks in decoder self-attention layers and add a positional attention layer as suggested in Gu et al . ( 2017 ) .
We provide a visualization of ART and NART models we used in Figure 3 and a detailed description of the model structure in Appendix .
Observation : Illed States and Attentions According to the case study in Gu et al . ( 2017 ) , the translations of the NART models contain incoherent phrases ( e.g. repetitive words ) and miss meaningful tokens on the source side , while these patterns do not commonly appear in ART models .
After some empirical study , we find two nonobvious facts that lead to this phenomenon .
First , we visualize the cosine similarities between decoder hidden states of a certain layer in both ART and NART models for sampled cases .
Mathematically , for a set of hidden states r 1 , . . . , r T , the pairwise cosine similarity can be derived by cos ij = r i , r j /( r i ? r j ) .
We then plot the heatmap of the resulting matrix cos .
A typical example is shown in Figure 1 , where the cosine similarities in the NART model are larger than those of the ART model , indicating that the hidden states across positions in the NART model are " similar " .
Positions with highly - correlated hidden states tend to generate the same word and make the NART model output repetitive tokens , e.g. , the yellow area on the top-left of Figure 1 ( b ) , while this does not happen in the ART model ( Figure 1 ( a ) ) .
According to our statistics , 70 % of the cosine similarities between hidden states in the ART model are less than 0.25 , and 95 % are less than 0.5 .
Second , we visualize the encoder-decoder attentions for sampled cases , shown in Figure 2 .
Good attentions between the source and target sentences are usually considered to lead to accurate translation while poor ones may cause wrong output tokens ( Bahdanau et al. , 2014 ) .
In Figure 2 ( b ) , the attentions of the ART model almost covers all source tokens , while the attentions of the NART model do not cover " farm " but with two " morning " .
This directly makes the translation result worse in the NART model .
These phenomena inspire us to use the intermediate hidden information in the ART model to guide the learning process of the NART model .
Hints from the ART teacher Model
Our study motivates us to leverage the intermediate hidden information from an ART model to improve the NART model .
We focus on how to define hints from a well - trained ART teacher model and use it to guide the training process of a NART student model .
We study layer - to - layer hints and assume both the teacher and student models have an M - layer encoder and an N - layer decoder , despite the difference in stacked components .
Without the loss of generality , we discuss our method on a given paired sentence ( x , y ) .
In real experiments , losses are averaged over all training data .
For the teacher model , we use a tr t ,l , h as the encoder- to- decoder attention distribution of h-th head in the l-th decoder layer at position t , and use r tr t,l as the output of the l-th decoder layer after feed forward network at position t.
Correspondingly , a st t , l , h and r st t ,l are used for the student model .
We propose a hint- based training framework that contains two kinds of hints :
Hints from hidden states
The discrepancy of hidden states motivates us to use hidden states of the ART model as a hint for the learning process of the NART model .
One straight - forward method is to regularize the L 1 or L 2 distance between each pair of hidden states in ART and NART models .
However , since the network components are quite different in ART and NART models , applying the straight - forward regression on hidden states hurts the learning process and fails .
Therefore , we design a more implicit loss to help the student refrain from the incoherent translation results by acting towards the teacher in the hidden-state level : L hid = 2 ( T y ? 1 ) T y N Ty?1 s=1
Ty t=s + 1 N l=1 ?( d st , d tr ) , where d st = cos ( r st s , l , r st t , l ) , d tr = cos ( r tr s,l , r tr t , l ) , and ? is a penalty function .
In particular , we let ?( d st , d tr ) = ? ? ? ? ? ? log ( 1 ? d st ) , if d st ? ? st and d tr ? ? tr ; 0 , else , where ?1 ? ? st , ? tr ?
1 are two thresholds controlling whether to penalize or not .
We design this loss since we only want to penalize hidden states that are highly similar in the NART model , but not similar in the ART model .
We have tested several choices of ? log ( 1? d st ) , e.g. , exp ( d st ) , from which we find similar experimental results .
Hints from word alignments
We observe that meaningful words in the source sentence are sometimes untranslated by the NART model , and the corresponding positions often suffer from ambiguous attention distributions .
Therefore , we use the word alignment information from the ART model to help the training of the NART model .
In particular , we minimize KL - divergence between the per-head encoder-to- decoder attention distributions of the teacher and the student to encourage the student to have similar word alignments to the teacher model , i.e .
L align = 1 T y N H Ty t=1 N l=1 H h=1 D KL ( a tr t, l , h a st t , l , h ) .
Our final training loss L is a weighted sum of two parts stated above and the negative loglikelihood loss L nll defined on bilingual sentence pair ( x , y ) , i.e.
L = L nll + ?L hid + ?L align , ( 3 ) where ? and ? are hyperparameters controlling the weight of different loss terms .
Experiments
Experimental Settings
The evaluation is on two widely used public machine translation datasets : IWSLT14 German-to- English ( De-En ) ( Huang et al. , 2017 ; Bahdanau et al. , 2016 ) and WMT14 English - to - German ( En - De ) dataset ( Wu et al. , 2016 ; Gehring et al. , 2017 ) .
To compare with previous works , we also reverse WMT14 English - to - German dataset and obtain WMT14 German- to - English dataset .
We pretrain Transformer ( Vaswani et al. , 2017 ) as the teacher model on each dataset , which achieves 33.26/27.30/31.29 in terms of BLEU ( Papineni et al. , 2002 ) in IWSLT14 De-En , WMT14 En-De and De- En test sets .
The student model shares the same number of layers in encoder / decoder , size of hidden states / embeddings and number of heads as the teacher models ( Figure 3 ) .
Following Gu et al. ( 2017 ) ; Kim and Rush ( 2016 ) , we replace the target sentences by the decoded output of the teacher models .
Hyperparameters ( ? st , ? tr , ? , ? ) for hint - based learning are determined to make the scales of three loss components similar after initialization .
We also employ label smoothing of value ls = 0.1 ( Szegedy et al. , 2016 ) in all experiments .
We use Adam optimizer and follow the setting in Vaswani et al . ( 2017 ) . Models for WMT14/IWSLT14 tasks are trained on 8/1 NVIDIA M40 GPUs respectively .
The model is based on the open-sourced tensor2tensor ) .
1 More settings can be found in Appendix .
Inference During training , T y does not need to be predicted as the target sentence is given .
During testing , we have to predict the length of the target sentence for each source sentence .
In many languages , the length of the target sentence can be roughly estimated from the length of the source sentence .
We choose a simple method to avoid the computational overhead , which uses input length to deter - 1 Open-source code can be found at https://github.
com / zhuohan123 /hint -nart mine target sentence length : T y = T x + C , where C is a constant bias determined by the average length differences between the source and target training sentences .
We can also predict the target length ranging from [ ( T x +C ) ?B , ( T x + C ) + B ] , where B is the halfwidth .
By doing this , we can obtain multiple translation results with different lengths .
Note that we choose this method only to show the effectiveness of our proposed method and a more advanced length estimation method can be used to further improve the performance .
Once we have multiple translation results , we additionally use our ART teacher model to evaluate each result and select the one that achieves the highest probability .
As the evaluation is fully parallelizable ( since it is identical to the parallel training of the ART model ) , this rescoring operation will not hurt the non-autoregressive property of the NART model .
Experimental Results
We compare our model with several baselines , including three ART models , the fertility based ( FT ) NART model ( Gu et al. , 2017 ) , the deterministic iterative refinement based ( IR ) NART model ( Lee et al. , 2018 ) , and the Latent Transformer ( LT ; which is not fully nonautoregressive by incorporating an autoregressive sub-module in the NART model architecture .
The results are shown in the According to our empirical analysis , the percentage of repetitive words drops from 8.3 % to 6.5 % by our proposed methods on the IWSLT14 De - En test set , which is a 20 % + reduction .
This shows that our proposed method effectively improve the quality of the translation outputs .
We also provide several case studies in Appendix .
Finally , we conduct an ablation study on IWSLT14 De - En task .
As shown in hints from word alignments provide an improvement of about 1.6 BLEU points , and the hints from hidden states improve the results by about 0.8 BLEU points .
We also test these models on a subsampled set whose source sentence lengths are at least 40 .
Our model outperforms the baseline model by more than 3 BLEU points ( 20.63 v.s. 17.48 ) .
Conclusion
In this paper , we proposed to use hints from a well - trained ART model to enhance the training of NART models .
Our results on WMT14 En-De and De- En significantly outperform previous NART baselines , with one order of magnitude faster in inference than ART models .
In the future , we will focus on designing new architectures and training methods for NART models to achieve comparable accuracy as ART models .
Figure 2 : 2 Figure 2 : Case study : the above three figures visualize the encoder-decoder attention weights of different models .
The x-axis and y-axis correspond to the source and generated target tokens respectively .
The attention distribution is from a single head of the third layer encoder-decoder attention , which is the most informative one according to our observation .
Each pixel shows attention weights ?
ij between the i-th source token and j-th target token .
