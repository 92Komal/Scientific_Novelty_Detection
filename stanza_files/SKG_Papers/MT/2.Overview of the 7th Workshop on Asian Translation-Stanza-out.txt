title
Overview of the 7th Workshop on Asian Translation
abstract
This paper presents the results of the shared tasks from the 7th workshop on Asian transla tion ( WAT 2020 ) .
For the WAT2020 , 20 teams participated in the shared tasks and 14 teams submitted their translation results for the hu man evaluation .
We also received 12 research paper submissions out of which 7 were ac cepted .
About 500 translation results were submitted to the automatic evaluation server , and selected submissions were manually eval uated .
Introduction
The Workshop on Asian Translation ( WAT ) is an open evaluation campaign focusing on Asian lan guages .
Following the success of the previous workshops WAT2014WAT2019 ( Nakazawa et al. , 2019 ) , WAT2020 brings together machine transla tion researchers and users to try , evaluate , share and discuss brandnew ideas for machine transla tion .
We have been working toward practical use of machine translation among all Asian countries .
For the 7th WAT , we included the following new tasks : ? Hindi / Thai / Malay / Indonesian ?
English IT and Wikinews tasks ?
Odia ?
English task ?
Bengali / Hindi / Malayalam / Tamil / Telugu / Marathi / Gujarati ? English Indic multilin gual tasks ?
English ?
Japanese multimodal tasks ?
English ?
Japanese documentlevel transla tion tasks
All the tasks are explained in Section 2 .
WAT is a unique workshop on Asian language translation with the following characteristics : ?
Open innovation platform
Due to the fixed and open test data , we can re peatedly evaluate translation systems on the same dataset over years .
WAT receives sub missions at any time ?
i.e. , there is no submis sion deadline of translation results w.r.t auto matic evaluation of translation quality .
?
Domain and language pairs WAT is the world 's first workshop that targets scientific paper domain , and Chinese ?
Japanese and Korean ?
Japanese language pairs .
In the future , we will add more Asian languages such as Vietnamese , Lao and so on .
?
Evaluation method Evaluation is done both automatically and manually .
Firstly , all submitted translation re sults are automatically evaluated using three metrics : BLEU , RIBES and AMFM .
Among them , selected translation results are assessed by two kinds of human evaluation : pairwise evaluation and JPO adequacy evaluation .
Tasks
ASPEC Task ASPEC was constructed by the Japan Science and Technology Agency ( JST ) in collaboration with the National Institute of Information and Com munications Technology ( NICT ) .
The corpus con sists of a JapaneseEnglish scientific paper abstract corpus ( ASPECJE ) , which is used for ja?en subtasks , and a Japanese Chinese scientific paper excerpt corpus ( ASPECJC ) , which is used for ja?zh subtasks .
The statistics for each corpus are shown in Table 1 .
ASPECJE
The training data for ASPECJE was constructed by NICT from approximately two million JapaneseEnglish scientific paper abstracts owned by JST .
The data is a comparable corpus and sentence correspondences are found automatically using the method from Utiyama and Isahara ( 2007 ) .
Each sentence pair is accompanied by a similarity score calculated by the method and a field ID that indicates a scientific field .
The correspondence between field IDs and field names , along with the frequency and occurrence ratios for the training data , are described in the README file of ASPECJE .
The development , developmenttest and test data were extracted from parallel sentences from the JapaneseEnglish paper abstracts that exclude the sentences in the training data .
Each dataset con sists of 400 documents and contains sentences in each field at the same rate .
The document align ment was conducted automatically and only doc Lang Train Dev DevTest TestN zhja 1,000,000 2,000 2,000 5,204 koja 1,000,000 2,000 2,000 5,230 enja 1,000,000 2,000 2,000 5,668 Lang TestN1 TestN2 TestN3 TestEP zhja 2,000 3,000 204 1,151 koja 2,000 3,000 230 enja 2,000 3,000 668 - Table 2 : Statistics for JPC uments with a 1to1 alignment are included .
It is therefore possible to restore the original docu ments .
The format is the same as the training data except that there is no similarity score .
ASPECJC ASPECJC is a parallel corpus consisting of Japanese scientific papers , which come from the literature database and electronic journal site J STAGE by JST , and their translation to Chinese with permission from the necessary academic as sociations .
Abstracts and paragraph units are se lected from the body text so as to contain the high est overall vocabulary coverage .
The development , developmenttest and test data are extracted at random from documents con taining single paragraphs across the entire corpus .
Each set contains 400 paragraphs ( documents ) .
There are no documents sharing the same data across the training , development , development test and test sets .
JPC Task JPO Patent Corpus ( JPC ) for the patent tasks was constructed by the Japan Patent Office ( JPO ) in collaboration with NICT .
The corpus consists of ChineseJapanese , KoreanJapanese and English Japanese patent descriptions whose International Patent Classification ( IPC ) sections are chemistry , electricity , mechanical engineering , and physics .
At WAT2020 , the patent tasks has two sub tasks : normal subtask and expression pattern sub task .
Both subtasks use common training , develop ment and developmenttest data for each language pair .
The normal subtask for three language pairs uses four test data with different characteristics : ? testN : union of the following three sets ?
The definition of data use is shown in Table 4 . Participants submit the translation results of one or more of the test data .
The sentence pairs in each data are identified in the same manner as that for ASPEC using the method from ( Utiyama and Isahara , 2007 ) .
Mixeddomain Task
ALT and UCSY Corpus
The parallel data for MyanmarEnglish translation tasks at WAT2020 consists of two corpora , the ALT corpus and UCSY corpus .
?
The ALT corpus is one part from the Asian Language Treebank ( ALT ) project ( Riza et al. , 2016 ) , consisting of twenty thousand MyanmarEnglish parallel sentences from news articles .
?
The UCSY corpus ( Yi Mon Shwe Sin and Khin Mar Soe , 2018 ) is constructed by the NLP Lab , University of Computer Studies , Yangon ( UCSY ) , Myanmar .
The corpus con sists of 200 thousand MyanmarEnglish par allel sentences collected from different do mains , including news articles and textbooks .
The ALT corpus has been manually segmented into words ( Ding et al. , , 2019 , and the UCSY corpus is unsegmented .
A script to tokenize the Myanmar data into writing units is released with the data .
The automatic evaluation of Myanmar translation results is based on the tokenized writ ing units , regardless to the segmented words in the ALT data .
However , participants can make a use of the segmentation in ALT data in their own man ner .
The detailed composition of training , develop ment , and test data of the MyanmarEnglish trans lation tasks are listed in Table 5 .
Notice that both of the corpora have been modified from the data used in WAT2018 .
ALT and ECCC Corpus
The parallel data for KhmerEnglish translation tasks at WAT2020 consists of two corpora , the ALT corpus and ECCC corpus .
The ALT corpus has been manually segmented into words , and the ECCC corpus is unsegmented .
A script to tokenize the Khmer data into writing units is released with with the data .
The automatic evaluation of Khmer trans lation results is based on the tokenized writing units , regardless to the segmented words in the ALT data .
However , participants can make a use of the segmentation in ALT data in their own man ner .
The detailed composition of training , develop ment , and test data of the KhmerEnglish transla tion tasks are listed in Table 6 .
NICTSAP Task
This year , we created a new task for joint multi domain multilingual neural machine translation in volving 4 lowresource Asian languages : Thai ( Th ) , Hindi ( Hi ) , Malay ( Ms ) , Indonesian ( Id ) .
En glish ( En ) is the source or the target language for the translation directions being evaluated .
The pur pose of this task was to test the feasibility of multi domain multilingual solutions for extremely low resource language pairs and domains .
Naturally the solutions could be onetomany , manytoone or manytomany NMT models .
The domains in question are Wikinews and IT ( specifically , Soft ware Documentation ) .
The total number of evalu ation directions are 16 ( 8 for each domain ) .
There is very little clean and publicly available data for these domains and language pairs and thus we en couraged participants to not only utilize the small Asian Language Treebank ( ALT ) parallel corpora ( Thu et al. , 2016 ) corpora 2 ( Buschbeck and Exel , 2020 ) and encour aged participants to consider GNOME , UBUNTU and KDE corpora from OPUS .
In Table 7 we give statistics of the aforementioned corpora which we used for the organizer 's baselines .
Note that we do not list 3 all available corpora here and participants were not restricted from using any corpora as long as they are freely available .
News Commentary Task
For the Russian ?
Japanese task we asked partic ipants to use the JaRuNC corpus 4 ( Imankulova et al. , 2019 ) which belongs to the news commen tary domain .
This dataset was manually aligned and cleaned and is trilingual .
It can be used to evaluate Russian ?
English translation quality as well but this is beyond the scope of this years subtask .
Refer to Table 8 for the statis tics of the indomain parallel corpora .
In addi tion we encouraged the participants to use outof domain parallel corpora from various sources such as KFTT , 5 JESC , 6 TED , 7 ASPEC , 8 UN , 9 Yan dex 10 and Russian ?
English newscommentary corpus 11 .
This year we also encouraged partici pants to use any corpora from WMT 2020 12 in volving Japanese , Russian and English as long as it did not belong to the news commentary domain to prevent any test set sentences from being inten tionally seen during training .
Indic Multilingual Task
In 2018 , we had organized an Indic languages task ( Nakazawa et al. , 2018 ) but due to lack of reli able evaluation corpora we discontinued it in WAT 2019 .
However , in 2020 , high quality publicly available evaluation ( and training ) corpora became available which motivated us to relaunch the task .
The Indic task involves mixed domain corpora for evaluation consisting of various articles composed by Indian Prime Minister .
The languages involved are Hindi ( Hi ) , Marathi ( Mr ) , Tamil ( Ta ) , Telugu ( Te ) , Gujarati ( Gu ) , Malayalam ( Ml ) , Bengali ( Bg ) and English ( En ) .
English is either the source or the target language during evaluation leading to a total of 14 translation directions .
The objec tive of this task , like the Indic languages task in 2018 , was to evaluate the performance of multilin gual NMT models .
The desired solution could be onetomany , manytoone or manytomany NMT models .
We provided a filtered version of the PM India dataset 13 and further encouraged the use of the CVITPIB dataset 14 .
Our organizer 's baselines used the PMI and PIB corpora for training .
De tailed statistics for the aforementioned corpora can be found in Table 9 .
We also listed additional sources of corpora for participants to use .
See Ap pendix B for details .
UFAL ( EnOdia ) Task
This task introduced this year at WAT2020 and the first Odia ?
English machine translation shared task running in any conference .
For Odia ?
English translation task we asked the partic ipants to use OdiEnCorp 2.0 ( Parida et al. , 2020 ) . 15
The statistics of the corpus are given in Table 10 .
English ?
Hindi MultiModal Task For English ?
Hindi multimodal translation task we asked the participants to use the updated ver sion 1.1 of Hindi Visual Genome corpus ( HVG , Parida et al. , 2019 a , b) . 16
The update consisted in correcting primarily the Hindi , i.e. the target side of the corpus .
The statistics of HVG 1.1 are given in rectangular region highlighting a part of the im age , the original English caption of this region and the Hindi reference translation .
Depending on the track ( see 2.9.1 below ) , some of these item compo nents are available as the source and some serve as the reference or play the role of a competing can didate solution .
Since HVG 1.0 was used already in WAT 2019 , all the data were publicly available before WAT 2020 .
We instructed the participants to use only the Training and DTest sections and avoid using ETest and CTest which are the official test sets for the task this year .
The English ?
Hindi multimodal task includes three tracks as illustrated in Figure 1 :
English ?
Hindi MultiModal Task Tracks
1 . TextOnly Translation ( labeled " TEXT " in WAT official tables ) :
The participants are asked to translate short English captions ( text ) into Hindi .
No visual information can be used .
On the other hand , additional text resources are permitted ( but they need to be specified in the corresponding system description paper ) .
2 . Hindi Captioning ( labeled " HI " ) :
The partici pants are asked to generate captions in Hindi for the given rectangular region in an input image .
MultiModal Translation ( labeled " MM " ) : Given an image , a rectangular region in it and an English caption for the rectangular region , the participants are asked to translate the En glish text into Hindi .
Both textual and visual information can be used .
Here we use the MeCab tokenizer to count Japanese tokens .
*
Four of the original English sentences are actually broken so we did not provide their translations .
Japanese ?
English MultiModal Tasks
The goal of Japanese ?
English multimodal task 17 is to improve translation performance with the help of another modality ( images ) associated with in put sentences .
For both English ?
Japanese and Japanese ?
English tasks , we use the Flickr30k En tities Japanese ( F30 kEntJp ) dataset ( Nakayama et al. , 2020 ) .
This is an extended dataset of the Flickr30 k 18 and Flickr30k Entities 19 datasets where manual Japanese translations are added .
No tably , it has the annotations of manytomany phrasetoregion correspondences in both English and Japanese captions , which are expected to strongly supervise multimodal grounding and pro vide new research directions .
We summarize the statistics of our dataset in Ta
There are two settings of submission : with and without resource constraints .
In the constrained setting , external resources such as additional data and pretrained models ( with external data ) are not allowed to use , except for pretrained convo lutional neural networks ( for visual analysis ) and basic linguistic tools such as taggers , parsers , and morphological analyzers .
As the baseline system to compute the Pairwise score , we implement the textonly model in ( Nishihara et al. , 2020 ) under the constrained setting .
Documentlevel Translation Task
In WAT2020 , we set up 2 documentlevel transla tion tasks : ParaNatCom and BSD .
Documentlevel Business Scene Dialogue Translation
There are a lot of readytouse parallel corpora for training machine translation systems , however , most of them are in written languages such as web crawl , newscommentary , patents , scientific papers and so on .
Even though some of the paral lel corpora are in spoken language , they are mostly spoken by only one person ( TED talks ) or contain a lot of noise ( OpenSubtitle ) .
Most of other MT evaluation campaigns adopt the written language , monologue or noisy dialogue parallel corpora for their translation tasks .
Traditional ASPEC trans lation tasks are sentencelevel and the translation quality of them seem to be saturated .
We think it 's high time to move on to documentlevel evaluation .
For the first year , WAT uses BSD Corpus 22 ( The Business Scene Dialogue corpus ) for the dataset including training , development and test data .
Par ticipants of this taks must get a copy of BSD corpus by themselves .
IITB Hindi-English task
In this task we use IIT Bombay EnglishHindi Cor pus which contains EnglishHindi parallel corpus as well as mono lingual Hindi corpus collected from a variety of sources and corpora ( Bojar et al. , 2014 ) .
This cor pus had been developed at the Center for Indian Language Technology , IIT Bombay over the years .
The corpus is used for mixed domain tasks hi?en .
The statistics for the corpus are shown in Table 13 .
Participants
Table 14 shows the participants in WAT2020 .
The table lists 14 organizations from various countries , including Japan , India , Singapore , China , Ireland , and Switzerland .
493 translation results by 20 teams were submit ted for automatic evaluation and about 121 trans lation results by 14 teams were submitted for the human evaluation .
Table 15 shows tasks for which each team submitted results by the deadline .
The human evaluation was conducted only for the tasks with the check marks in " human eval " line .
Baseline Systems Human evaluations of most of WAT tasks were conducted as pairwise comparisons between the translation results for a specific baseline system and translation results for each participant 's sys tem .
That is , the specific baseline system was the standard for human evaluation .
At WAT 2020 , we adopted a neural machine translation ( NMT ) with attention mechanism as a baseline system .
The NMT baseline systems consisted of pub licly available software , and the procedures for building the systems and for translating using the systems were published on the WAT web page .
23
We also have SMT baseline systems for the tasks that started at WAT 2017 or before 2017 .
The base line systems are shown in Tables 16 , 17 , and 18 . SMT baseline systems are described in the WAT 2017 overview paper ( Nakazawa et al. , 2017 ) .
The commercial RBMT systems and the online transla tion systems were operated by the organizers .
We note that these RBMT companies and online trans lation companies did not submit themselves .
Be cause our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate , the system IDs of these systems are anonymous in this paper .
Tokenization
We used the following tools for tokenization .
For ASPEC , JPC , TDDC , JIJI , ALT , UCSY , ECCC , and IITB ? Juman version 7.0 24 for Japanese segmenta tion .
( Kunchukuttan , 2020 ) for Indic language segmentation .
?
The tools included in the ALT corpus for Myanmar and Khmer segmentation .
? subwordnmt 28 for all languages .
When we built BPEcodes , we merged source and target sentences and we used 100,000 for s op tion .
We used 10 for vocabularythreshold when subwordnmt applied BPE .
For News Commentary ?
The Moses toolkit for English and Russian only for the News Commentary data .
Table 14 : List of participants who submitted translations for the human evaluation in WAT2020 ( Note : teams with '* ' marks did not submit their system description papers , therefore the evaluation results are UNOFFICIAL according to our policy ) Multimodal ASPEC JPC JIJI EnHi EnJa Team ID CJ JC EJ JE CJ JC KJ JK JE TX HI MM EJ JE TMU ? ? ? NHKNES ? ODIANLP ? ? KyotoU+ECNU ? ? goku20 ? ? ? ? ? ? * HWTSC ? ? * iiitsc ? CNLPNITS ? ? human eval ? ? ? ? ? ? ? ? ? ? ? ? ? ? Indic Multilingual Team ID EnBnBnEnEnHiHiEnEnMlMlEnEnTaTaEnEnTeTeEnEnGuGuEnEnMrMrEn
NICT5 ? ? ? ? ? ? ? * cvitmt ? ? ? ? ? ? ? ? ? ? ? ? ? ? ODIANLP ? ? ? ? ? ? ? ? ? ? ? ? ? ? HWTSC ? ? ? ? ? ? ? ? ? ? ? ? ? ? human eval ? ? ? ? UFAL EnOdia BSD Hinden Multilingual Multidomain ( IT / Wikinews )
Team ID EnOd OdEn EJ JE EnHiHiEnEnHiHiEnEnThThEnEnMsMsEnEnInInEn NICT5 ?/? ?/? ?/? /? ?/? ?/? ?/? ?/?
* cvitmt ? ? ODIANLP ? ? goku20 ? ? WT ? ? utmrt ? ? adaptdcu ? DEEPNLP ? ? human eval ? ? ? ? ? ? Table 15 : Submissions for each task by each team .
E , J , C , and K denote English , Japanese , Chinese , and Korean respectively .
The human evaluation was conducted only for the tasks with the check marks in " human eval " line .
? Mecab 29 for Japanese segmentation .
?
Corpora are further processed by ten sor2tensor 's internal pre / postprocessing which includes subword segmentation .
Indic and NICTSAP Tasks ?
For the Indic task we did not perform any ex plicit tokenization of the raw data . ?
For the NICTSAP task we only character seg mented the Thai corpora as it was the only lan guage for which character level BLEU was to be computed .
Other languages corpora were not preprocessed in any way .
?
Any subword segmentation or tokenization was handled by the internal mechanisms of tensor2tensor .
For English ?
Hindi MultiModal and UFAL EnOdia Tasks ? Hindi Visual Genome 1.1 and OdiEnCorp 2.0 comes untokenized and we did not use or rec ommend any specific external tokenizer .
?
The standard OpenNMTpy subword seg mentation was used for pre / postprocessing for the baseline system and each participant used what they wanted .
For English ?
Japanese MultiModal Tasks ?
For English sentences , we applied lowercase , punctuation normalization , and the Moses to kenizer .
?
For Japanese sentences , we used KyTea for word segmentation .
Baseline NMT Methods
We used the following NMT with attention for most of the tasks .
We used Transformer ( Ten sor2Tensor , Vaswani et al. , 2017 ) for the News Commentary and English ?
Tamil tasks and Trans former ( OpenNMTpy ) for the Multimodal task .
NMT with Attention
We used OpenNMT ( Klein et al. , 2017 ) as the implementation of the baseline NMT systems of NMT with attention ( System ID : NMT ) .
We used the following OpenNMT configuration .
29 https://taku910.github.io/mecab/ ?
encoder_type = brnn ? brnn_merge = concat ? src_seq_length = 150 ? tgt_seq_length = 150 ? src_vocab_size = 100000 ? tgt_vocab_size = 100000 ? src_words_min_frequency = 1 ? tgt_words_min_frequency = 1
The default values were used for the other system parameters .
We used the following data for training the NMT baseline systems of NMT with attention .
?
All of the training data mentioned in Section 2 were used for training except for the AS PEC Japanese - English task .
For the ASPEC Japanese - English task , we only used train 1.txt , which consists of one million parallel sentence pairs with high similarity scores .
?
All of the development data for each task was used for validation .
Transformer ( Tensor2 Tensor )
For the News Commentary task , we used ten sor2tensor 's 30 implementation of the Transformer ( Vaswani et al. , 2017 ) and used default hyperpa rameter settings corresponding to the " base " model for all baseline models .
The baseline for the News Commentary task is a multilingual model as described in Imankulova et al . ( 2019 ) which is trained using only the indomain parallel corpora .
We use the token trick proposed by ( Johnson et al. , 2017 ) to train the multilingual model .
As for the Indic and NICTSAP tasks , we used tensor2tensor to train manytoone and oneto many models where the latter were trained with the aforementioned token trick .
We used default hyperparameter settings corresponding to the " big " model .
Since the NICTSAP task involves two do mains for evaluation ( Wikinews and IT ) we used a modification of the token trick technique for do main adaptation to distinguish between corpora for different domains .
In our case we used tokens such as 2alt and 2 it to indicate whether the sen tences belonged to the Wikinews or IT domain , re spectively .
For both tasks we used 32,000 sepa rate subword vocabularies .
We trained our mod els on 1 GPU till convergence on the development set BLEU scores , averaged the last 10 checkpoints 30 https://github.com/tensorflow/ tensor2tensor ( separated by 1000 batches ) and performed decod ing with a beam of size 4 and a length penalty of 0.6 .
Transformer ( OpenNMTpy )
For the English ?
Hindi Multimodal and UFAL EnOdia tasks , we used the Transformer model ( Vaswani et al. , 2018 ) as implemented in OpenNMTpy ( Klein et al. , 2017 ) and used the " base " model with default parameters for the multimodal task baseline .
We have generated the vocabulary of 32 k subword types jointly for both the source and target languages .
The vocabulary is shared between the encoder and decoder .
Multimodal Transformer with Supervised Attention
As the baselines for the English ?
Japanese
Mul timodal tasks , we implement five models de scribed in ( Nishihara et al. , 2020 )
Automatic Evaluation
Procedure for Calculating Automatic Evaluation Score
We evaluated translation results by three met rics : BLEU ( Papineni et al. , 2002 ) , RIBES ( Isozaki et al. , 2010 ) and AMFM ( Banchs et al. , 2015 ) . BLEU scores were calculated using multi-bleu.perl in the Moses toolkit ( Koehn et al. , 2007 ) . RIBES scores were calculated using RIBES .py version 1.02.4 .
31 AMFM scores were calculated using scripts created by the technical collaborators listed in the WAT2020 web page .
32
All scores for each task were calculated using the corresponding reference translations .
Before the calculation of the automatic evalua tion scores , the translation results were tokenized or segmented with tokenization / segmentation tools for each language .
For Japanese segmenta tion , we used three different tools : Juman version 7.0 ( Kurohashi et al. , 1994 ) , KyTea 0.4.6 ( Neubig et al. , 2011 ) with full SVM model 33 and MeCab 0.996 ( Kudo , 2005 ) with IPA dictionary 2.7.0 .
34 For Chinese segmentation , we used two different tools : KyTea 0.4.6 with full SVM Model in MSR model and Stanford Word Segmenter ( Tseng , 2005 ) version 20140616 with Chinese Penn Treebank ( CTB ) and Peking University ( PKU ) model .
35 For Korean segmentation , we used mecabko .
36 For Myanmar and Khmer segmen tations , we used myseg.py 37 and kmseg.py 38 . For English and Russian tokenizations , we used tokenizer .
perl 39 in the Moses toolkit .
For Indonesian and Malay tokenizations , we used tokenizer .
perl as same as the English tok enization .
For Thai tokenization , we segmented the whole character separately .
For Bengali , Gujarati , Hindi , Marathi , Malayalam , Odia , Tamil , and Telugu tokenizations , we used Indic NLP Library 40 ( Kunchukuttan , 2020 ) .
The detailed procedures for the automatic evaluation are shown on the WAT2020 evaluation web page .
41
Automatic Evaluation System
The automatic evaluation system receives transla tion results by participants and automatically gives evaluation scores to the uploaded results .
As shown in Figure 2 , the system requires participants to provide the following information for each sub mission : ?
Human Evaluation : whether or not they sub mit the results for human evaluation ?
JPC{N , N1 , N2, N3, EP}zh-ja , JPC { N, N1 , N2,N3}ja-zh , JPC{N , N1 , N2,N3}ko-ja , JPC {N , N1 , N2,N3}ja-ko , JPC { N , N1, N2,N3}en -ja , and JPC{N , N1 , N2 , N3 }ja- en indicate the patent tasks with JPO Patent Corpus .
JPCN1 { zh-ja , ja-zh , ko-ja , ja-ko , en-ja , ja-en} are the same tasks as JPC { zh -ja , ja-zh , ko-ja , ja-ko , en-ja , ja-en} in WAT2015 -WAT2017 .
AMFM is not calculated for JPC {N , N2,N3 } tasks .
Human evaluation :
If you want to submit the file for human evaluation , check the box " Human Evaluation " .
Once you upload a file with checking " Human Evaluation " you cannot change the file used for human evaluation .
When you submit the translation results for human evaluation , please check the checkbox of " Publish " too .
You can submit two files for human evaluation per task .
One of the files for human evaluation is recommended not to use other resources , but it is not compulsory .
Other : Team Name , Task , Used Other Resources , Method , System Description ( public ) , Date and Time ( JST ) , BLEU , RIBES and AMFM will be disclosed on the Evaluation Site when you upload a file checking " Publish the results of the evaluation " .
You can modify some fields of submitted data .
Read " Guidelines for submitted data " at the bottom of this page .
Evaluation scores of translation results that partic ipants permit to be published are disclosed via the WAT2020 evaluation web page .
Participants can also submit the results for human evaluation using the same web interface .
This automatic evaluation system will remain available even after WAT2020 .
Anybody can reg ister an account for the system by the procedures described in the registration web page .
42
Back to top
Additional Automatic Scores in MultiModal and UFAL EnOdia Tasks
For the multimodal task and UFAL EnOdia task , several additional automatic metrics were run aside from the WAT evaluation server , namely : BLEU ( this time calculated by Moses scorer 43 ) , characTER ( Wang et al. , 2016 ) , chrF3 ( Popovi ? , 2015 ) , TER ( Snover et al. , 2006 ) , WER , PER and CDER ( Leusch et al. , 2006 ) .
Except for chrF3 and characTER , we ran Moses tokenizer 44 on the can didate and reference before scoring .
For all error metrics , i.e. metrics where better scores are lower , we reverse the score by taking 1 ? x and indicate this by prepending " n " to the metric name .
With this modification , higher scores always indicate a better translation result .
Also , we multiply all met ric scores by 100 for better readability .
These additional scores document again , that BLEU implementations ( and the underlying tok enization schemes ) heavily vary in their outcomes .
The scores are thus comparable only within each of the metric variation , even if it is supposed to be the same " BLEU " .
In Table 22 , we highlight with a special symbol whenever the ranking in one of the metrics differs from the toptobottom sorting of the scores .
Last year , a number of these met ric , including our BLEU vs. official WAT BLEU ( BLEU w in Table 22 ) lead to varying rankings .
This year , the system differences are probably suf ficiently big in the optics of these metrics that only nCharacTER in the ETest textonly ( " EV TEXT " ) scoring differs .
Human Evaluation
In WAT2020 , we conducted 2 kinds of human eval uations : pairwise evaluation ( only for JaEn multi modal translation task , Section 6.1 ) and JPO ad equacy evaluation ( other than HiEn multimodal translation task , Section 6.2 ) and a pairwise varia tion of direct assessment ( Section 6.4 ) for the HiEn multimodal task .
42 http://lotus.kuee.kyoto-u.ac.jp/WAT/
WAT2020/registration/index.html 43 https://github.com/moses-smt/mosesdecoder/ blob/master/mert/evaluator.cpp 44 https://github.com/moses-smt/mosesdecoder/ blob/master/scripts/tokenizer/tokenizer.perl
Pairwise Evaluation
We conducted pairwise evaluation for participants ' systems submitted for human evaluation .
The sub mitted translations were evaluated by a profes sional translation company and Pairwise scores were given to the submissions by comparing with baseline translations ( described in Section 4 ) .
Sentence Selection and Evaluation
For the pairwise evaluation , we randomly selected 400 sentences from the test set of each task .
We used the same sentences as the last year for the continuous subtasks .
Baseline and submitted trans lations were shown to annotators in random order with the input source sentence .
The annotators were asked to judge which of the translations is bet ter , or whether they are on par .
Voting
To guarantee the quality of the evaluations , each sentence is evaluated by 5 different annotators and the final decision is made depending on the 5 judge ments .
We define each judgement j i ( i = 1 , ? ? ? , 5 ) as : j i = ? ? ?
1 if better than the baseline ?1 if worse than the baseline 0 if the quality is the same
The final decision D is defined as follows using S = ? j i : D = ? ? ? win ( S ? 2 ) loss ( S ? ?2 ) tie ( otherwise )
Pairwise Score Calculation
Suppose that W is the number of wins compared to the baseline , L is the number of losses and T is the number of ties .
The Pairwise score can be calculated by the following formula : P airwise = 100 ? W ? L W + L + T From the definition , the Pairwise score ranges be tween 100 and 100 .
JPO Adequacy Evaluation
We conducted JPO adequacy evaluation for the top two or three participants ' systems of pairwise eval uation for each subtask .
45
The evaluation was car ried out by translation experts based on the JPO 45
The number of systems varies depending on the subtasks .
5 All important information is transmitted correctly . ( 100 % ) 4 Almost all important information is transmitted cor rectly . ( 80 % - ) 3 More than half of important information is transmit ted correctly .
( 50 % - ) 2 Some of important information is transmitted cor rectly .
( 20 % - ) 1 Almost all important information is NOT transmit ted correctly . ( - 20 % ) Table 19 : The JPO adequacy criterion adequacy evaluation criterion , which is originally defined by JPO to assess the quality of translated patent documents .
Sentence Selection and Evaluation
For the JPO adequacy evaluation , the 200 test sen tences were randomly selected from the test sen tences .
For the Newswire ( JIJI ) task test set II , ar ticles were randomly selected from the context of test set II until the number of the test sentences that were contained in the selected articles became 200 .
For each test sentence , input source sentence , translation by participants ' system , and reference translation were shown to the annotators .
For the Newswire ( JIJI ) task test set II , input source sen tences were shown in articles , which means that not only input source sentences but also their con text were shown to the evaluators .
The evaluators considered the context of the input sentences to evaluate the translations .
To guarantee the qual ity of the evaluation , each sentence was evaluated by two annotators .
Note that the selected sentences are basically the same as those used in the previous workshop ( WAT2019 ) .
Evaluation Criterion Table 19 shows the JPO adequacy criterion from 5 to 1 .
The evaluation is performed subjectively .
" Important information " represents the technical factors and their relationships .
The degree of im portance of each element is also considered to eval uate .
The percentages in each grade are rough in dications for the transmission degree of the source sentence meanings .
The detailed criterion is de scribed in the JPO document ( in Japanese ) .
46 46 http://www.jpo.go.jp/shiryou/toushin/ chousa/tokkyohonyaku_hyouka.htm
Manual Evaluation for the UFAL ( EnOdia ) Task
The user interface for our annotation for each of the tracks is illustrated in Figure 3 , and Figure 4 .
The interpretation of these judgements is carried out as described in the following Section 6.4 .
Manual Evaluation for the English ?
Hindi MultiModal Task
The evaluations of the three tracks of the multi modal task and also the UFAL EnOdia task follow the Direct Assessment ( DA , Graham et al. , 2016 ) technique by asking annotators to assign a score from 0 to 100 to each candidate .
The score is as signed using a slider with no numeric feedback , the scale is therefore effectively continuous .
After a certain number of scored items , it is assumed that each of the annotators stabilizes in their scoring cri teria .
The collected DA scores can be either directly averaged for each system and track ( denoted " Ave " ) , or first standardized per annotator across all annotation tasks and then averaged ( " Ave Z " ) .
The standardization removes the effect of individ ual differences in the range of scores assigned : the scores are scaled so that the average score of each individual annotator across all tasks he or she an notated is 0 and the standard deviation is 1 .
Our evaluation differs from the basic DA in the following respects : ( 1 ) we run the evaluation bilin gually , i.e. we require the annotators to understand the source English sufficiently to be able to assess the adequacy of the Hindi translation , ( 2 ) we ask the annotators to score two distinct segments at once , while the original DA displays only one can didate at a time .
The main benefit of bilingual evaluation is that the reference is not needed for the evaluation .
In stead , the reference can be included among other candidates and the manual evaluation allows us to directly compare the performance of MT to human translators .
The dual judgment ( scoring two candidates at once ) was added experimentally last year .
The advantage is saving some of the annotators ' time ( they do not need to read the source or examine the picture again ) and the chance to better stabi lize their score assignments by seeing another can didate output .
This essentially direct pairwise com parison could be useful especially for systems very close in their performance .
47
The user interface for our annotation for each of the tracks is illustrated in Figures 5 to 7 .
By default , the position of the slider appears to be at the " worst " score but technically , the user inter face is capable of distinguishing if the annotator has touched the slider at all or not .
The default score is 1 while the lowest score that the annota tor can assign is 0 .
Table 20 provides an overview of the usage of sliders in annotation for the 6 Hindi annotators ( H * , each scoring 1256 outputs ) and 6 Odia annotators ( O * , each scoring 576 outputs ) .
The Hindi anno tation was carried out first and we observed that the default value was left untouched ( " Unscored " ) rather often , in up to 45.6 % of cases for the an notator H2 .
Given this large proportion , we de cided to consider two interpretations of the default score 1 : we either disregard these scorings , assum ing that the annotator forgot about that particular slider , or we interpret the scoring as " Worst " , i.e. merging the " Unscored " and " Min " cases .
For the 47 For the full statistical soundness of the subsequent inter pretation of DA scores , the judgments should be independent of each other .
We explicitly ask our annotators to judge the candidates independently but the dependence cannot be de nied .
Whether the violation of the independence assumption is offset by the benefit of obtaining more stable judgements is yet to be analyzed Odia task , we urged the annotators to touch every slider .
The low " Unscored " rates indicate that this reminder helped and only very few items were for gotten .
It may seem surprising that many annotators used the very highest score ( " Max " ) .
This is possi ble because the sentences are often short and sim ple and also because human translations are in cluded in the scoring .
The big differences in the us age of the extreme values however justify the need for score standardization .
In the " textonly " evaluation , one English text ( source ) and two Hindi translations ( candidate 1 and 2 ) are shown to the annotators .
In the " multi modal " evaluation , the annotators are shown both the image and the source English text .
The first question is to validate if the source English text is a good caption for the indicated area .
For two translation candidates , the annotators are asked to independently indicate to what extent the meaning is preserved .
The " Hindi captioning " evaluation shows only the image and two Hindi candidates .
The annotators are reminded that the two captions should be treated independently and that each of them can consider a very different aspect of the re gion .
Evaluation Results
In this section , the evaluation results for WAT2020 are reported from several perspectives .
Some of the results for both automatic and human evalu ations are also accessible at the WAT2020 web site .
48
Official Evaluation Results
Figures 8 and 9 show the official evaluation results of ASPEC subtasks , Figures 10 , 11 , 12 , 13 , 14 and 15 show those of JPC subtasks , Figure 16 shows that of JIJIc subtask , Figures 17 and 18 21 .
The weights for the weighted ?
( Cohen , 1968 ) is defined as | Evaluation1 ? Evaluation 2 |/4 .
The automatic scores for the multimodal and UFAL EnOdia tasks along with the WAT evalua tion server BLEU scores are provided in Table 22 .
For each of the test sets of the multimodal task ( ETest , CTest ) , the scores are comparable across all the tracks ( textonly , captioning or multimodal translation ) because of the underlying set of refer ence translations is the same .
The scores for the captioning task will be however very low because captions generated independently of the English source caption are very likely to differ from the ref erence translation .
For multimodal task , Table 23 shows the man ual evaluation scores for all valid system submis sions .
As mentioned above , we used the reference translation as if it was one of the competing sys tems , see the rows " Reference " in the table .
The annotation was fully anonymized , so the annota tors had no chance of knowing if they are scoring human translation or MT output .
The UFAL EnOdia task has its official manual scores listed in Table 24 .
Findings
ASPEC Task
There is only one team ( KyotoU + ECNU ) who par ticipated ASPEC task this year .
KyotoU+ECNU team participated Japanese ?
Chinese translation subtasks .
They achieved the stateoftheart auto matic evaluation scores , however , the human eval uation scores are below those of last year 's ( see Fig ure 8 and 9 .
Strictly speaking , the human evalua tion scores of this year and last year are not directly comparable because the evaluators might be differ ent .
They trained a lot of different NMT models which exploit 1 ) different training data ( outof domain external data , back / forwardtranslation of Japaneseside of ASPECJE ) , 2 ) different S2S frameworks ( LSTM , ConvS2S , Transformer and Ligntconv ) and 3 ) different model capacities ( dif ferent hyperparameter settings ) .
They also tried to use mBART .
Among all the models , the ones which exploit data augmentation by back / forward translation performed the best .
They also tried to combine various NMT models trained above .
The BLEU score improves a lot ( about 0.5 to 1.5 points ) by adding first 2 or 3 models , however , the impact is getting smaller ( about 0.1 to 0.2 points improve ment ) after that .
From the results , we can say that using exter nal resources and combining various models both improves the automatic evaluation scores because of the generalization effect or improvement of flu ency , but they might have a bad effect on lexical choice of technical term translations which directly affect the human evaluation scores .
JPC Task
Two teams participated in the JPC task ?
goku20 submitted their systems for all language pairs ( J?E , J?C , and J?K ) and TMU submitted their systems for the K?J pair .
goku20 used baseline Transformer models and mBART models , which were pretrained on largescale monolingual cor pus in 25 languages , finetuned on the JPO cor pus .
TMU used baseline Transformer models , en hanced models with Hanja loss to obtain close embeddings of SinoKorean ( Hanja ) and Sino Japanese ( Kanji ) words , and domain adaptation models finetuned for each domain on source test sentences and target sentences translated by their domainspecific model .
Domain indicates section based on IPC : chemistry , electricity , mechanical engineering , or physics .
We discuss results on testN data as follows .
For J?C and J?E , goku20 's ensemble Transformer models achieved higher BLEU scores than the best systems in previous years ' WAT except for sys tems using additional resources .
According to goku20 's experiments , single mBART models out performed single Transformer models for several language pairs but ensemble mBART models un derperformed ensemble Transformer models for all language pairs .
For K?J , TMU 's domain adap tation model achieved the highest BLEU score , while their Hanja loss + domain adaptation models ( and their unofficial baseline Transformer model ) achieved similar scores .
According to TMU 's ex periments , single Hanja loss model slightly im proved single Transformer model but no difference was observed between ensemble versions of both types of models .
As for JPO adequacy evaluation , goku20 submitted ensemble mBART models for all language pairs and TMU submitted ensemble domain adaptation model and ensemble domain adaptation + Hanja loss model for K?J.
All sys tems by both teams achieved high adequacy scores close to or better than 4.5 .
Thus , evaluation results in this year demon strated high translation accuracy by Transformer models similarly to WAT2019 's results .
Although both teams reported improvements by their ad ditional techniques , their enhanced models per formed similarly or worse than strong Transformer baselines if ensemble models were used .
Newswire ( JIJI ) Task
There were two submissions to the Japaneseto English task from the NHKNES team .
The two submissions were translations without using con text and translations using context .
The team ad dressed the problem of translating zero subject sen tences in Japanese into English by extracted sub jects and topics from source context using deep analysis and added them to the input sentences as context .
The automatic evaluation scores and the human evaluation scores of JPO Adequacy were improved by using context .
Although official train ing data does not contain contextual information , the team used external training data that contained contextual information .
NICTSAP
Task Despite the novelty of the task and the availabil ity of clean evaluation data for Wikinews and Soft ware Documentation domain , we had only 1 sub mission from the " NICT5 " team .
They submitted a manytomany model which in most cases , sig nificantly outperformed the organizers baselines which were onetomany and manytoone models .
Thai to English translation was significantly lower ( around 10 BLEU ) compared to all other transla tion directions .
Human evaluation was not per formed and at present it is difficult to draw any conclusions on the translation quality due to lack of participation .
Indic Multilingual Task
Of the several submissions we collected from 4 teams ( excluding organizers ) , the best translations , as measured by BLEU , were submitted by " HW TSC " .
The BLEU scores for most translation direc tions for this team were significantly higher com pared to the other participants .
With regards to au tomatic evaluation scores , translation into English was observed to be substantially better than trans lation into the Indic languages .
This is understand able because Indic languages are morphologically richer than English .
Furthermore , translation qual ity to and from Dravidian languages such as Tamil , Telugu and Malayalam was observed to be the least when compared to the translation quality to and from the IndoAryan languages Bengali , Marathi , Hindi and Gujarati .
Given that the Dravidian lan guages are morphologically richer than the Indo Aryan ones , making them hard to translate or trans late into .
Marathi is a special language which ex hibits some properties of Dravidian languages such as agglutination making it morphologically richer than the other IndoAryan languages .
This causes translation quality to and from Marathi to be lower than the translation quality to and from the other IndoAryan languages .
Human evaluation was done for Hindi-English and Bengali-English ( both directions ) which re vealed that higher BLEU scores often did not correlate with what humans considered as higher quality translations .
A deeper look showed that while " HWTSC " had significantly higher BLEU scores , the percentage of sentences that were per fectly translated ( a rating of 5 by human evalu ators ) were substantially lower than the percent age of sentences that were perfectly translated by the team " cvitmt " .
For all human evaluated di rections , translations by " cvitmt " was rated to be the best despite lagging behind in terms of BLEU when compared with " HWTSC " .
This indicates that human evaluation and automatic evaluation fo cus on different aspects of translation quality and thus both types of evaluation should be performed in order to better evaluate the quality of transla tions .
UFAL ( EnOdia ) Task
This year , four teams participated in this new task .
For the English ?
Odia translation task , we received 10 submissions from four teams ( ex cluding organizers ) which includes five submis sions from the team " cvit " , two submissions from the teams " ODIANLP " and " ADAPT " and a sin gle submission from the team " NLPRL " .
For the Odia ?
English translation task , we received 6 submissions from three teams ( excluding orga nizers ) which includes three submisions from the team " cvit " , two submissions from the team " ODI ANLP " , and a single submission from the team " NLPRL " .
The team " ODIANLP " obtained the high est BLEU score for both English ?
Odia and Odia ?
English translation tasks .
Manual evaluation of EnOdia Task is provided in Table 24 .
Regardless the exact interpretation of the rankings ( see the discussion in the following section on Hindi MultiModal task ) , manual trans lation is the best , followed by " cvit " in the transla tion into Odia and by " ODIANLP " in the transla tion into English .
English ?
Hindi MultiModal Task
This year three teams participated in the different subtasks ( TEXT , MM , and HI ) of the English ?
MultiModal task .
The team " ODIANLP " ob tained the highest BLEU score for the textonly translation ( TEXT ) for both the evaluation ( ETest ) and challenge ( CTest ) test set .
For the caption ing and multimodal subtasks ( HI and MM ) , we re ceived only one submission from the teams " ODI ANLP " and " CNLPNITS " , respectively .
In order to make the evaluation better grounded , we included also the outputs of the best system in each of the subtasks in the annotation .
In manual ranking , the 2020 systems thus compete not only with the reference translation but also with the win ner from 2019 .
It should be noted that a revision of ETest and CTest files was carried out between the years but the source English did not really change .
49
The 2019 system outputs could be thus directly used in this year 's evaluation .
Table 23 in the appendix presents the results of the manual annotation .
We compare the two inter pretations , either ignoring items where the slider was not touched by the annotator , or interpreting it as the lowest value .
The final ranking of the sys tems do not change ( despite substantial changes in the actual average scores ) .
Similarly , the standard ization of the scores ( " Ave " vs . " Ave Z " ) do not change the overall ranking of the systems .
Across the tasks , 2020 systems perform better than the best system from 2019 .
The reference translation generally scores much better than the best system in each task , except for textonly trans lation of the ETest .
Same as last year , the best sys tem comes out marginally better than the reference .
Interestingly , IDIAP system ID 2956 , which sur passed the reference in 2019 ended up fourth this year .
This can be explained by the different ran dom choice of evaluated sentences this year , but is still clearly illustrates that textonly competition is tight .
Across the various tasks , each of the three par ticipating teams got its medal .
aware model ( utmrt 2 ) , but it did not improve the translation quality compared to the model without context ( utmrt 1 ) .
From these results , finetuning also requires a substantial amount of training data with context .
Another interesting point is that smaller ( 6,000 ) subword units leads to better translation quality , which is reported by adaptdcu .
We currently do not have any idea about the reason of this result .
It is worth investigating the results deeper in the future .
IITB Hindi-English Task
This year we received two submissions for English to Hindi translation and one submission for Hindi to English translation , all from one team ( " WT " ) .
For English to Hindi translation , the submissions had about 1 to 2 BLEU points higher than the best submissions of 2018 and 2019 .
The AMFM scores were substantially higher by approximately 16 to 17 points compared to previous years .
How ever the human evaluation for adequacy showed that the translation quality was slightly worse than the best translation quality in 2019 and comparable to the best translation quality in 2018 .
For the re verse direction , Hindi to English , there was an ex plosive growth in translation quality as measured by BLEU .
Where the best submission of 2019 had a BLEU score of 22.91 , the best ( and only ) sub mission of 2020 had a BLEU score of 29.59 .
Hu man evaluation ( adequacy ) , revealed that the over all adequacy score is less than the best adequacy score in 2019 .
Deeper investigation showed that this is due to most translations in 2020 being of average quality compared to the best translations in 2019 ( 42.00 % in 2020 versus 17.17 % in 2019 ) .
On the other hand the number of perfectly rated translations dropped substantially from 48.00 % in 2019 to 30 % in 2020 .
This explains why the over all human evaluation score for the best 2019 trans lations is higher than the one for the 2020 transla tions .
This discrepancy between BLEU score and human evaluation shows the importance of man ual investigations of translation instead of blindly relying on automatic scores .
Conclusion and Future Perspective
This paper summarizes the shared tasks of WAT2020 .
We had 14 participants worldwide who submitted their translation results for the human evaluation , and collected a large number of use ful submissions for improving the current machine translation systems by analyzing the submissions and identifying the issues .
For the next WAT workshop , we will include several new datasets and new languages ( Arabic and additional Indic languages ) .
We are also plan ning to provide documentlevel test sets for some translation tasks because sentencelevel translation quality is almost saturated for some tasks .
We also plan to have a new shared task named " Restricted Translation " task where we will investigate trans lation with restricted target vocabularies .
