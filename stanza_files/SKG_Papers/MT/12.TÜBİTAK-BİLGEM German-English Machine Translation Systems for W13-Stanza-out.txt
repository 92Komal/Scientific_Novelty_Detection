title
T ÜB İTAK -B İLGEM German-English Machine Translation Systems for WMT'13
abstract
This paper describes T ?B ?TAK -B ?LGEM statistical machine translation ( SMT ) systems submitted to the Eighth Workshop on Statistical Machine Translation ( WMT ) shared translation task for German - English language pair in both directions .
We implement phrase - based SMT systems with standard parameters .
We present the results of using a big tuning data and the effect of averaging tuning weights of different seeds .
Additionally , we performed a linguistically motivated compound splitting in the Germanto- English SMT system .
Introduction T ?B ?TAK -B ?LGEM participated for the first time in the WMT '13 shared translation task for the German- English language pairs in both directions .
We implemented a phrase - based SMT system by using the entire available training data .
In the German-to - English SMT system , we performed a linguistically motivated compound splitting .
We tested different language model ( LM ) combinations by using the parallel data , monolingual data , and Gigaword v4 .
In each step , we tuned systems with five different tune seeds and used the average of tuning weights in the final system .
We tuned our systems on a big tuning set which is generated from the last years ' ( 2008 , 2009 , 2010 , and 2012 ) development sets .
The rest of the paper describes the details of our systems .
German-English
Baseline
All available data was tokenized , truecased , and the maximum number of tokens were fixed to 70 for the translation model .
The Moses open SMT toolkit ( Koehn et al. , 2007 ) was used with MGIZA ++ ( Gao and Vogel , 2008 ) with the standard alignment heuristic grow-diag-final ( Och and Ney , 2003 ) for word alignments .
Good - Turing smoothing was used for phrase extraction .
Systems were tuned on newstest2012 with MERT ( Och , 2003 ) and tested on newstest 2011 .
4 gram language models ( LMs ) were trained on the target side of the parallel text and the monolingual data by using SRILM ( Stolcke , 2002 ) toolkit with Kneser - Ney smoothing ( Kneser and Ney , 1995 ) and then binarized by using KenLM toolkit ( Heafield , 2011 ) .
At each step , systems were tuned with five different seeds with latticesamples .
Minimum Bayes risk decoding ( Kumar and Byrne , 2004 ) and - drop-unknown parameters were used during the decoding .
This configuration is common for all of the experiments decribed in this paper unless stated otherwise .
Table 1 shows the number of sentences used in system training after the clean-corpus process .
We trained two baseline systems in order to assess the effects of this year 's new parallel data , commoncrawl .
We first trained an SMT system by using only the training data from the previous WMT shared translation tasks that is europarl and news -commentary ( Baseline1 ) .
As the second baseline , we also included the new parallel data commoncrawl only in the translation model ( Base-line2 ) .
Then , we included commoncrawl corpus both to the translation model and the language model ( Baseline3 ) .
Data
Number of sentences
Bayesian Alignment
In the original IBM models ( Brown et al. , 1993 ) , word translation probabilities are treated as model parameters and the expectation - maximization ( EM ) algorithm is used to obtain the maximumlikelihood estimates of the parameters and the resulting distributions on alignments .
However , EM provides a point-estimate , not a distribution , for the parameters .
The Bayesian alignment on the other hand takes into account all values of the model parameters by treating them as multinomial - distributed random variables with Dirichlet priors and integrating over all possible values .
A Bayesian approach to word alignment inference in IBM Models is shown to result in significantly less " garbage collection " and a much more compact alignment dictionary .
As a result , the Bayesian word alignment has better translation performances and obtains significant BLEU improvements over EM on various language pairs , data sizes , and experimental settings ( Mermer et al. , 2013 ) .
We compared the translation performance of word alignments obtained via Bayesian inference to those obtained via EM algorithm .
We used a a Gibbs sampler for fully Bayesian inference in HMM alignment model , integrating over all possible parameter values in finding the alignment distribution by using Baseline3 word alignments for initialization .
Development Data in Training Development data from the previous years ( i.e. newstest08 , newstest09 , newstes t10 ) , though being a small set of corpus ( 7 K sentences ) , is in- domain data and can positively affect the translation system .
In order to make use of this data , we experimented two methods : i ) adding the development data in the translation model as described in this section and ii ) using it as a big tuning set for tuning the parameters more efficiently as explained in the next section .
Similar to including the commoncrawl corpus , we first add the development data both to the training and language models by concatenating it to the biggest corpus europarl ( DD ( tm +lm ) ) and then we removed this corpus from the language models ( DD ( tm ) ) .
Results in Table 4 show that including the development data both the tranining and language model increases the performance in development set but decreases the performance in the test set .
Including the data only in the translation model shows a very slight improvement in the test set .
System
Tuning with a Big Development Data
The second method of making use of the development data is to concatenate it to the tuning set .
As a baseline , we tuned the system with newstest12 as mentioned in Section 2.1 .
Then , we concatenated the development data of the previous years with the newstest12 and built a big tuning set .
Finally , we obtained a tuning set of 10 K sentences .
We excluded the newstest11 as an internal test set to see the relative improvements of different systems .
Table 5 shows the results of using a big tuning set .
Tuning the system with a big tuning set resulted in a 0.13 BLEU points improvement .
System
Effects of Different Language Models
In this set of experiments , we tested the effects of different combinations of parallel and monolingual data as language models .
As the baseline , we trained three LMs , one from each parallel corpus as europarl , news - commentary , and commoncrawl and one LM from the monolingual data newsshuffled ( Baseline3 ) .
We then trained two LMs , one from the whole parallel data and one from the monolingual data ( 2LMs ) .
German Preprocessing In German , compounding is very common .
From the machine translation point of view , compounds increase the vocabulary size with high number of the singletons in the training data and hence decrease the word alignment quality .
Moreover , high number of out-of- vocabulary ( OOV ) words in tuning and test sets results in several German words left as untranslated .
A well -known solution to this problem is compound splitting .
Similarly , having different word forms for a source side lemma for the same target lemma causes the lexical redundancy in translation .
This redundancy results in unnecessary large phrase translation tables that overload the decoder , as a separate phrase translation entry has to be kept for each word form .
For example , German definite determiner could be marked in sixteen different ways according to the possible combinations of genders , case and number , which are fused in six different tokens ( e.g. , der , das , die , den , dem , des ) .
Except for the plural and genitive cases , all these forms are translated to the same English word " the " .
In the German preprocessing , we aimed both normalizing lexical redundancy and splitting German compounds with corpus driven splitting algorithm based on Koehn and Knight ( 2003 ) .
We used the same compound splitting and lexical redundancy normalization methods described in Allauzen et al . ( 2010 ) and Durgar El-Kahlout and Yvon ( 2010 ) with minor in-house changes .
We used only " addition " ( e.g. , -s , -n , -en , -e , -es ) and " truncation " ( e.g. , -e , -en , -n ) affixes for compound splitting .
We selected minimum candidate length to 8 and minimum split length to 4 .
By using the Treetagger ( Schmid , 1994 ) output , we included linguistic information in compound splitting such as not splitting named entities and foreign words ( CS1 ) .
We also experimented adding # as a delimiter for the splitted words except the last word ( e.g. , Finanzkrisen is splitted as finanz# krisen ) ( CS2 ) .
On top of the compound splitting , we applied the lexical redundancy normalization ( CS + Norm1 ) .
We lemmatized German articles , adjectives ( only positive form ) , for some pronouns and for nouns in order to remove the lexical redundancy ( e.g. , Bildes as Bild ) by using the finegrained part-of-speech tags generated by RFTagger ( Schmid and Laws , 2008 ) .
Similar to CS2 , We tested the delimited version of normalized words ( CS + Norm2 ) .
Table 7 shows the results of compound splitting and normalization methods .
As a result , normalization on top of compounding did not perform well .
Besides , experiments showed that compound word decomposition is crucial and helps vastly to improve translation results 0.43 BLEU points on average over the best system described in Section 2.5 .
System
Average of Weights
As mentioned in Section 2.1 , we performed tuning with five different seeds .
We averaged the five tuning weights and directly applied these weights during the decoding .
Table 8 shows that using the average of several tuning weights performs better than each individual tuning ( 0.2 BLEU points ) .
System
Other parameters
In addition to the experiments described in the earlier sections , we removed the - drop-unknown parameter which gave us a 0.5 BLEU points improvement .
We also included the monotone - atpunctuation , - mp in decoding .
We handled outof-vocabulary ( OOV ) words by lemmatizing the OOV words .
Moreover , we added all development data in training after fixing the parameter weights as described in Section 2.7 .
Although each of these changes increases the translation scores each gave less than 0.1 BLEU point improvement .
Table 9 shows the results of the final system after including all of the approaches except the ones described in Section 2.2 and 2.3 .
System
English - German For English - to - German translation system , the baseline setting is the same as described in Section 2.1 .
We also added the items that showed positive improvement in the German to English SMT system such as using 2 LMs , tuning with five seeds and averaging tuning parameters , using - mp , and not using - drop-unknown .
Final System and Results Table 11 shows our official submission scores for German-English SMT systems submitted to the WMT '13 .
System newstest13 De- En 25.60 En-De 19.28 Table 11 : German-English Official Test Submission .
Conclusion
In this paper , we described our submissions to WMT '13 Shared Translation Task for German - English language pairs .
We used phrase - based systems with a big tuning set which is a combination of the development sets from last four years .
We tuned the systems on this big tuning set with five different tunes .
We averaged these five tuning weights in the final system .
We trained 4 - gram language models one from parallel data and one from monolingual data .
Moreover , we trained a 4 - gram language model with Gigaword v4 for German- to - English direction .
For Germanto- English , we performed a different compound splitting method instead of the Moses splitter .
We obtained a 1.7 BLEU point increase for Germanto- English SMT system and a 0.5 BLEU point increase for English - to - German SMT system for the internal test set newstest2011 .
Finally , we submitted our German-to - English SMT system with a BLEU score 25.6 and English - to - German SMT system with a BLEU score 19.3 for the official test set newstest2013 .
Table 1 : 1 Parallel Corpus .
Europarl 1908574 News -Commentary 177712 Commoncrawl 726458
