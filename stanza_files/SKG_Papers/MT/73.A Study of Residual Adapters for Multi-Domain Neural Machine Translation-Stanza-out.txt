title
A Study of Residual Adapters for Multi-Domain Neural Machine Translation
abstract
Domain adaptation is an old and vexing problem for machine translation systems .
The most common and successful approach to supervised adaptation is to fine - tune a baseline system with in - domain parallel data .
Standard fine-tuning however modifies all the network parameters , which makes this approach computationally costly and prone to overfitting .
A recent , lightweight approach , instead augments a baseline model with supplementary ( small ) adapter layers , keeping the rest of the model unchanged .
This has the additional merit to leave the baseline model intact and adaptable to multiple domains .
In this paper , we conduct a thorough analysis of the adapter model in the context of a multidomain machine translation task .
We contrast multiple implementations of this idea using two language pairs .
Our main conclusions are that residual adapters provide a fast and cheap method for supervised multi-domain adaptation ; our two variants prove as effective as the original adapter model and open perspective to also make adapted models more robust to label domain errors .
Introduction Owing to multiple improvements , Neural Machine Translation ( NMT ) ( Kalchbrenner and Blunsom , 2013 ; Sutskever et al. , 2014 ; Bahdanau et al. , 2015 ; Vaswani et al. , 2017 ) nowadays delivers useful outputs for many language pairs .
However , as many deep learning models , NMT systems need to be trained with sufficiently large amounts of data to reach their best performance .
Therefore , the quality of the translation of NMT models is still limited in low-resource language or domain conditions ( Duh et al. , 2013 ; Zoph et al. , 2016 ; Koehn and Knowles , 2017 ) .
While many approaches have been proposed to improve the quality of NMT models in low-resource domains ( see the recent survey of Chu and Wang ( 2018 ) ) , full fine-tuning ( Luong and Manning , 2015 ; Neubig and Hu , 2018 ) of a generic baseline model remains the dominant supervised approach when adapting NMT models to specific domains .
Under this view , building adapted systems is a two -step process : ( a ) one first trains NMT with the largest possible parallel corpora , aggregating texts from multiple , heterogeneous sources ; ( b ) assuming that in- domain parallel documents are available for the domain of interest , one then adapts the pre-trained model by resuming training with the sole in - domain corpus .
It is a conjecture that the pretrained model constitutes a better initialization than a random one , especially when adaptation data is scarce .
Indeed , studies of transfer learning for NMT such as Artetxe et al . ( 2020 ) ; Aji et al. ( 2020 ) have confirmed this claim in extensive experiments .
Full fine-tuning , that adapts all the parameters of a baseline model usually significantly improves the quality of the NMT for the chosen domain .
However , it also yields large losses in translation quality for other domains , a phenomenon referred to as " catastrophic forgetting " in the neural network literature ( McCloskey and Cohen , 1989 ) .
Therefore , a fully fine- tuned model is only useful to one target domain .
As the number of domains to handle grows , training , and maintaining a separate model for each task can quickly become tedious and resource-expensive .
Several recent studies ( e.g. ( Vilar , 2018 ; Wuebker et al. , 2018 ; Michel and Neubig , 2018 ; Bapna and Firat , 2019 ) ) have proposed more lightweight schemes to perform domain adaptation , while also preserving the value of pre-trained models .
Our main inspiration is the latter work , whose proposal relies on small adapter components that are plugged in each hidden layer .
These adapters are trained only with the in-domain data , keeping the pre-trained model frozen .
Because these additional adapters are very small compared to the size of the baseline model , their use significantly reduces the cost of training and maintaining fine-tuned models , while delivering a performance that remains close to that of full fine-tuning .
In this paper , we would like to extend this architecture to improve NMT in several settings that still challenge automatic translation , such as translating texts from multiple topics , genre , or domains , in the face of unbalanced data distributions .
Furthermore , as the notion of " domains " is not always well established , another practical setting is the translation of texts mixing several topics / domains .
An additional requirement is to translate texts from domains unseen in training , based only on the unadapted system , which should then be made as strong as possible .
In this context , our main contribution is a thorough experimental study of the use of residual adapters for multi-domain translation .
We notably explore ways to adjust and / or regularize adapter modules to handle situations where the adaptation data is very small .
We also propose and contrast two new variants of the residual architecture : in the first one ( highway residual adapters ) , adaptation still affects each layer of the architecture , but its effect is delayed till the last layer , thus making the architecture more modular and adaptive ; our second variant ( gated residual adapters ) exploits this modularity and enables us to explore ways to improve performance in the face of train-test data mismatch .
We experiment with two language pairs and report results that illustrate the flexibility and effectiveness of these architectures .
Residual adapters
In this section , we describe the basic version of the residual adapter architectures ( Houlsby et al. , 2019 ; Bapna and Firat , 2019 ) , as well as two novel variants of this model .
Basic architecture
The computation of adapter layers
Our reference architecture is the Transformer model of Vaswani et al . ( 2017 ) , which we assume contains a stack of layers both on the encoder and the decoder sides .
Each layer contains two subparts , an attention layer , and a dense layer .
Details vary from one implementation to another , we simply contend here that each layer i ?
{ 1 . . .
L} ( in the encoder or the decoder ) computes a transform of a fixed - length sequence of d-dimensional input vectors h i into a sequence of output vectors h i+1 as follows ( LN denotes the ( sub ) layer normalization , ReLU is the " rectified linear unit " operator ) : h i 0 = LN ( h i ) h i 1 = W i db h i 0 + a i 1 h i 2 = ReLU ( h i 1 ) h i 3 = W i bd h i 2 + a i 2 hi = h i 3 + h i .
Overall , the i th adapter is thus parameterized by matrices W i db ?
R d?b , W i bd ?
R b?d , bias vectors b i 1 ? R b , b i 2 ? R d , with b the dimension of the adapter .
For the sake of brevity , we will simply denote h i 3 = ADAP ( i ) ( h i ) , and ? ADAP ( i ) the corresponding set of parameters .
The " adapted " hidden vectors hi 1 ?i? L?1 , where L is the number of layers , will then be the input of the ( i + 1 ) th layer ; hL is passed to the decoder if it belongs to the encoder side , or is the input of output layer if it belongs to the decoder side .
Note that zeroing out all adapters enables us to recover the basic Transformer , with hi = h i for all i .
In the experiments of Section 3 , we use 2 ? L = 12 residual adapters , one for each of the L = 6 attention layers of the encoder and similarly for the decoder .
1
Design space and variants
This general architecture leaves open many design choices pertaining to the details of the network organization , the training procedure , and the corresponding objective function .
The first question is the number of adapter layers .
While in principle , all Transformer layers can be subject to adaptation , it is nonetheless worthwhile to consider simpler adaptation schemes , which would only alter a limited number of layers .
Such strategy might be especially relevant when the training data contains very small domains , as in the experiments of Section 3 , and for which a complete adaptation may not be necessary or / and or prone to overfitting .
Likewise , it might be meaningful to explore ways to share subsets of adapters across domains .
This , in turn , raises the issue of which layer ( s ) to adapt , a question that can be approached in the light of recent analyses of Transformers models , which conjecture that the higher layers encode global patterns with a more " semantic " interpretation , while the lower layers encode local patterns akin to morpho-syntactic information ( Raganato and Tiedemann , 2018 ) .
A related question concerns the regularization of adapter layers to mitigate overfitting .
Reducing the number of adapters , or their dimensions , is simple , but such choices are difficult to optimize numerically - an issue that becomes important as the number of domain grows .
Less naive alternatives can also be entertained , such as applying weight decay or layer regularization to the adapter .
Implementing these requires to modify the objective function in a way that still allows for a smooth optimization problem .
For instance , weight decay applies a penalization on the weights of the adapters , complementing the cross-entropy term with a function of the norm of the parameters : L = 1 # ( x , y ) x,y ( ? log ( P ( y| x ) ) ) + ? i?{1,..,6}?{enc , dec} ? ADAP ( i ) 2
An alternative scheme is layer regularization , which penalizes the output of the adapters , corresponding to the following objective : L = 1 # ( x , y ) x,y ( ? log ( P ( y|x ) ) + ? i?{1,..,6}?{enc , dec} ADAP ( i ) ( h i ( x , y ) )
2 ) Finally , another independent design choice relates to the training strategy for adapters .
A first option is to generalize supervised domain adaptation to multi-domain adaptation and to proceed in two steps : ( a ) train a generic model with all the available data ; ( b ) train each adapter layer with domain-specific data , keeping the generic model parameters unchanged .
Another strategy is to adopt the view of Dredze and Crammer ( 2008 ) , where the multi-domain setting is viewed as an instance of multi-task learning ( Caruana , 1997 ) with each domain corresponding to a specific task .
This suggests training all the parameters from scratch , as we would do in a multi-task mode .
The generic parameters will still depend on all the available data , while each adapter will only be trained with the corresponding in - domain data .
Highway Residual Adapters
In the basic architecture described in Section 2.1 , the computation performed by lower level layers will impact all the subsequent layers .
In this section , we introduce an alternative implementation of the same idea , which however delays the adaptation of each layer to the last layer ( of the encoder or the decoder ) as depicted on Figure 1 .
While the basic architecture performs adaptation in sequence , we propose here to perform it in parallel .
In this version , only the last hidden vector of the encoder ( decoder ) is thus modified according to : hL = h L + 1?i?L ADAP i ( h i ) ( 1 )
One obvious benefit of this variant is that it allows us to reuse the hidden vectors h i of all hidden layers when computing an adapted output for several domains during the inference .
In this situation , the forward step needs only to compute the hidden vectors h i once for the inner encoder layers , before an adapted sequence of vectors is computed at the topmost layer .
Therefore , we can fine - tune the model to multiple domains at once without recomputing h i .
This variant also opens the way to more parameter sharing across adapters , a perspective that we will not explore further in this work .
Instead , we use it to develop a second variation of the adapter model , that is presented in the next section .
Gated Residual Adapters
The basic architecture presented above rests on a rather simplistic view of " domains " as made of well - separated and unrelated pieces of texts that are processed independently during adaptation .
Likewise , when translating test documents , one needs to choose between either using one specific domainadapted model or resorting to the generic model .
In this context , using wrong domain labels can have a strong ( negative ) effect on translation performance .
Therefore , we would like to design a version of residual adapters that is more robust to such domain errors .
This variant , called the gated residual adapter model , relies on the training of a supplementary component that will help decide whether to activate , on a word per word basis , a given residual layer and to regulate the strength of this activation .
To this end , we extend the highway version of residual adapters as follows .
Formally , we replace the adapter computation of equation ( 1 ) and take the adapted hidden ( topmost ) layer to be computed as ( this is for domain k ) : hL = h L + 1?i?L ADAP i k ( h i ) z k ( h L ) , ( 2 ) where the scalar 3 Experimental settings z k ( h L [ t ] ) ? [ 0 ,
Data and metrics
We perform our experiments with two translation pairs involving multiple domains : English - French ( En? Fr ) and English - German ( En?De ) .
For the former pair , we use texts 3 initially from 6 domains , corresponding to the following data sources : the UFAL Medical corpus V1.0 ( M E D ) 4 , the European Central Bank corpus ( B A N K ) ( Tiedemann , 2012 ) ; The JRC - Acquis Communautaire corpus ( L A W ) ( Steinberger et al. , 2006 ) , documentations for KDE , Ubuntu , GNOME and PHP from Opus collection ( Tiedemann , 2009 )
We randomly select in each corpus a development and a test set of 1,000 lines each and keep the rest for training .
8 Development sets help choose the best model according to the average BLEU score ( Papineni et al. , 2002 ) . 9
Baseline architectures Using Transformers ( Vaswani et al. , 2017 ) implemented in OpenNMT - tf 10 ( Klein et al. , 2017 ) , we train the following baselines : ?
a generic model trained on a concatenation of all corpora , denoted Mixed ; ? a fine-tuned model ( Luong and Manning , 2015 ; Freitag and Al - Onaizan , 2016 ) , based on the Mixed system , further trained on each domain with early stopping when the development BLEU score stops increasing during 3 consecutive epochs .
M E D L A W B A N K I T T A L K R E L N E W S B A N K E C O M E D G O V N E W S T O U R W E B 4 ( 0 . For all En?
Fr models , we set the embeddings size and the hidden layers size to 512 .
Transformers use multi-head attention with 8 heads in each of the 6 layers ; the inner feedforward layer contains 2,048 cells .
Residual adapters additionally use an adaptation block in each layer , composed of a 2layer perceptron , with an inner ReLU activation function operating on normalized entries of dimension b = 1024 .
Bapna and Firat ( 2019 ) showed that the performance of adapted models increases with respect to the size of the inner dimension and obtained performance close to the full fine - tuned model with b = 1024 , which is twice as large as the dimension of a Transformer layer .
We used the same setting in our experiments .
Training uses a batch size of 12,288 tokens ; optimization uses Adam with parameters ?
1 = 0.9 , ? 2 = 0.98 and Noam decay ( warmup steps = 4 , 000 ) , and a dropout rate of 0.1 for all layers .
For the Mixed model , we use an initial learning rate of 1.0 and take the concatenation of the validation sets of 6 domains for development .
In the fine-tuning experiments , we continue training using Mixed as starting point , using the same learning rate schedule , and continuing the incrementation of the number of steps .
In the multi-task training , we use the same learning rate schedule as for Mixed : for each iteration , we sample a domain a probability proportional to its size ; we then sample a batch of 12,288 tokens that is used to update the shared parameters and the parameters of the corresponding adapter .
Models for En?De are larger and rely on embeddings as well as hidden layers of size 1024 ; each Transformers layer contains 16 attention heads ; the inner feedforward layer contains 4,096 cells .
Adapter modules have the same architecture as for the other language pair , except for their size , which is doubled ( b = 2 , 048 ) .
Multi-domain systems
In this section , we evaluate several proposals from the literature on multi-domain adaptation and compare them to full fine - tuning on the one hand , and to two variants of the residual adapter architecture on the other hand .
The reference methods included in our experiments are the following : ? a system using " domain control " ( Kobus et al. , 2017 ) .
In this approach , domain information is introduced either as an additional token for each source sentence ( DC - Tag ) or in the form of a supplementary feature for each word ( DC - Feat ) ; ? a system using lexicalized domain representations ( Pham et al. , 2019 ) : word embeddings are composed of a generic and a domainspecific part ( LDR ) ; ? the three proposals of Britz et al . ( 2017 ) .
TTM is a feature - based approach where the domain tag is introduced as an extra word on the target side .
The training uses reference tags and inference is performed with predicted tags , just like for regular target words .
DM is a multi-task learner where a domain classifier is trained on top of the MT encoder , so as to make it aware of domain differences ; ADM is the adversarial version of DM , pushing the encoder towards learning domain-independent source representations .
These methods only use domain labels in training .
Model / Domain
The two variants of the residual adapter model included in this first round of experiment have been presented in Section 2.1 : Res -Adap is the multidomain version of the approach of Bapna and Firat ( 2019 ) based on a two -step training procedure ; while Res-Adap - MT is the " multi-task " version , where the parameters of the generic model and of the adapters are jointly learned from scratch .
We also report results for the same system , using the the parameters of the Mixed model as initialization ( Res - Adap - MT + ) .
11 Because of the limit of our computational resources , we restrict the experiments in this section to the En? Fr task .
Results are in Table 3 . M E D L A W B A N K T A L K I T R E L A V G These results first show that full fine-tuning outperforms all other methods for the in-domain test sets .
However , Res - Adap is able to reduce the gap with this approach for several domains , showing the effectiveness of residual adapters .
The " multi-task " variant is slightly less effective in our experiments than the basic version , where optimization is performed in two steps .
As it turns out , using residual adapters proves here better on average than the other reference multi-domain systems ; it is also much better than the generic system for translating data from known domains , outperforming the Mixed system by more than 4 BLEU points in average .
Gains are especially large for small domains such as L A W and R E L. Comparing training schemes ( Res - Adap vs Res-Adap - MT vs Res-Adap - MT + ) suggests that the simultaneous learning of all parameters 11
This system also includes a layer dropout policy that cancels adapter layers with probability 0.5 is detrimental to performance in our settings : we see that the 2- step procedure implemented in Res-Adap always yields the best scores , even when Res-Adap - MT is initialized with good parameter values .
This may be because in this setting , the adapters have access to a stable version of the generic system .
The last line ( Res - Adap - MT ( gen ) ) gives the results for a Res-Adap - MT trained system in which we cancel the adapter in inferencecomparing this to Mixed shows how differently the generic parts of these two systems behave .
Varying the positions and number of residual adapters Tables 4 - 5 report BLEU scores for 6 domains in each language pair : M E D , L A W , B A N K , T A L K , I T and R E L for En?Fr ; G O V , E C O , T O U R , B A N K , M E D and N E W S for En?De .
We first see that for the latter direction , the basic version Res -Adap also outperforms the mixed baseline on average , with large gains for the small domains T O U R , B A N K and comparable results for the other domains .
By varying the number and position of residual adapters ( see Section 2.1 ) , we then contrast several implementations .
Because the set of possible configurations is large , we only perform experiments for layers i = 2 , 4 , 6 ( both for the encoder and decoder ) .
Two settings are considered : keeping just one adapter or keeping the three .
The trend is the same for the two language directions : suppressing adapters always hurts the overall performance , albeit by a small margin : having six adapters is better than three , which is better than keeping only one .
With only one adapter active , we observe small , insignificant changes in performance when varying the adapter 's depth .
Regularizing fine-tuning
The translation from English into German includes two domains ( T O U R and B A N K ) that are extremely small and account only for a very small fraction of the training data ( respectively for 0.039 % and 0.022 % of the total number of sentences ) .
Finetuning on these domains can lead to serious overfitting .
We assess two well -known regularization techniques for adapter modules , that could help mitigate this problem : weight decay and layer regularization .
For each method , the optimal hyper-parameter ?
( weight decay or layer regularization coefficient , see Section 2.1.2 ) are chosen by grid search in a small set of values ( { 10 ?3 , 10 ?4 , 10 ?5 } ) .
Results in Tables 4 and 5 show that regularizing the adapter model can positively impact the test performance for the smallest domains ( this is especially clear for weight - decay ( Res - Adap - WD ) in En?De ) , at the cost however of a small drop in performance for the other domains .
Using layer regularization proves here to be comparatively less effective .
Finding better ways to set the regularization parameters , for instance by varying ? for each domain based on the available supervision data , is left for future work .
Highway and Gated Residual Adaptaters
We now turn to the evaluation of our new architectural variants : Highway residual adapters Res-Adap - HW on the one hand , and Gated residual adapters Res-Adap - Gated on the other hand .
We use the same domains and settings as before , focusing here exclusively on the language direction En?Fr .
To also evaluate the robustness with respect to out - of- domain examples , we perform two additional experiments .
We first generate translations with erroneous ( more precisely : randomly assigned ) domain information : the corresponding results appear in Table 6 under column R N D .
We also compute translation for a domain unseen in training ( N E W S ) as follows .
For each sentence of this test set , we automatically evaluate the closest domain , 12 then use the predicted domain label to compute the translation .
This is an error-prone pro-cess , which also challenges the robustness of our multi-domain systems .
Results are in Table 6 .
A first observation is that for domains seen in training , our variants Res-Adap - HW and Res-Adap - Gated achieve BLEU scores that are on a par to those of the original version ( Res - Adap ) , with insignificant variations across test sets .
The two other settings are instructive in several ways : they first clearly illustrate the brittleness of domain-adapted systems , for which large drops in performance ( more than 15 BLEU points on average ) are observed when the domain label is randomly chosen .
Our gated variant however proves much more robust than the other adaptation strategy and performs almost on par to the generic system for that test condition .
The same trend holds for the unseen N E W S domain , with Res-Adap - Gated being the best domain adapted system in our set , outperforming the other variants by about 2 BLEU points .
Related Work Training with data from multiple , heterogeneous sources is a common scenario in natural language processing ( Dredze and Crammer , 2008 ; Finkel and Manning , 2009 ) .
It is thus no wonder that the design of multi-domain systems has been proposed for many tasks .
In this short survey , we exclusively focus on machine translation ; it is likely that similar methods ( parameter sharing , instance selection / weighting , adversarial training , etc ) have also been proposed for other tasks .
Early approaches to multi-domain MT were proposed for statistical MT , either considering multiple data sources ( eg. Banerjee et al .
( 2010 ) ; Clark et al . ( 2012 ) ; Sennrich et al . ( 2013 ) ; Huck et al. ( 2015 ) ) or domains containing several topics ( Eidelman et al. , 2012 ; Hasler et al. , 2014 ) .
Two main strategies emerge : feature - based methods , where domain labels are integrated through supplementary features ; and instance - based methods , involving a measure of similarity between train and test domains .
The former approach has also been adapted to NMT : Kobus et al . ( 2017 ) ; Tars and Fishel ( 2018 ) use an additional domain feature in an RNN model , in the form of an extra domain- token or of additional domain-features associated with each word .
Chen et al. ( 2016 ) apply domain control on the target side , using a topic vector to describe the Model / Domain Model / Domain 2018 ) are two recent representatives of the instance - based approach : for each test sentence , a small adaptation corpus is collected based on similarity measures and used to fine - tune a mix-domain model .
As shown in the former work , also adapting the training regime on a per sentence basis is crucial to make these techniques really effective .
Model / Domain M E D L A W B A N K T A L K I T R E L A V G P A R A M G O V E C O T O U R B A N K M E D N E W S A V G P A R A M S M E D L A W B A N K T A L K I T R E L A V G R N D N E W S Mixed Finally , note that a distinct evolution of the residual adapter model of Bapna and Firat ( 2019 ) is presented in Sharaf et al . ( 2020 ) , where meta-learning techniques are used to make fine-tuning more effective in a standard domain-adaptation setting .
Conclusion and outlook
In this paper , we have performed an experimental study of the residual adapter architecture in the context of multi-domain adaptation , where the goal is to build one single system that ( a ) performs well for domain seen in training , ideally as well as full fine- tuning ; ( b ) is also able to robustly handle translations for new , unseen domains .
We have shown that this architecture allowed us to quickly adapt a model to a specific domain , delivering BLEU performance than are much better than the generic , mixed domain baseline , and close the gap with the full- finetuning approach , at a modest computa-tional cost .
Several new variants have been introduced and evaluated for two language directions : if none that able to clearly surpass the baseline , residual adapter models , they provide directions for improving this model in practical settings : unbalanced data condition , noise in label domains , etc .
In our future work , we would like to continue the development of the gated variant , which , it seems to us , provides a flexible and robust tool to address the various challenges of multi-domain machine translation .
Figure 1 : 1 Figure 1 : Highway residual adapter network
