title
Neural Machine Translation Between Myanmar ( Burmese ) and Rakhine ( Arakanese )
abstract
This work explores neural machine translation between Myanmar ( Burmese ) and Rakhine ( Arakanese ) .
Rakhine is a language closely related to Myanmar , often considered a dialect .
We implemented three prominent neural machine translation ( NMT ) systems : recurrent neural networks ( RNN ) , transformer , and convolutional neural networks ( CNN ) .
The systems were evaluated on a Myanmar - Rakhine parallel text corpus developed by us .
In addition , two types of word segmentation schemes for word embeddings were studied : Word - BPE and Syllable - BPE segmentation .
Our experimental results clearly show that the highest quality NMT and statistical machine translation ( SMT ) performances are obtained with Syllable - BPE segmentation for both types of translations .
If we focus on NMT , we find that the transformer with Word - BPE segmentation outperforms CNN and RNN for both Myanmar -Rakhine and Rakhine - Myanmar translation .
However , CNN with Syllable - BPE segmentation obtains a higher score than the RNN and transformer .
Introduction
The Myanmar language includes a number of mutually intelligible Myanmar dialects , with a largely uniform standard dialect used by most Myanmar standard speakers .
Speakers of the standard Myanmar may find the dialects hard to follow .
The alternative phonology , morphology , and regional vocabulary cause some problems in communication .
Machine translation ( MT ) has so far neglected the importance of properly handling the spelling , lexical , and grammar divergences among language varieties .
In the Republic of the Union of Myanmar , there are many ethnical groups , and dialectal varieties exist within the standard Myanmar language .
To address this problem , we are developing a Myanmar and Rakhine dialectal corpus with monolingual and parallel text .
We conducted statistical machine translation ( SMT ) experiments and obtained results similar to previous research ( Oo et al. , 2018 ) .
Deep learning revolution brings rapid and dramatic change to the field of machine translation .
The main reason for moving from SMT to neural machine translation ( NMT ) is that it achieved the fluency of translation that was a huge step forward compared with the previous models .
In a trend that carries over from SMT , the strongest NMT systems benefit from subtle architecture modifications and hyperparameter tuning .
NMT models have advanced the state of the art by building a single neural network that can learn representations better ( Sutskever et al. , 2014a ) .
Other authors ( Rikters et al. , 2018 ) conducted experiments with different NMTs for less-resourced and morphologically rich languages , such as Estonian and Russian .
They compared the multi-way model performance to one - way model performance , by using different NMT architectures that allow achieving state - of - the - art translation .
For the multiway model trained using the transformer network architecture , the reported improvement over the baseline methods was + 3.27 bilingual evaluation understudy ( BLEU ) points .
( Honnet et al. , 2017 ) proposed solutions for the machine translation of a family of dialects , Swiss German , for which parallel corpora are scarcee .
The authors presented three strategies for normalizing Swiss German input to address the regional and spelling diversity .
The results show that character - based neural machine translation was the most promising strategy for text normalization and that in combination with phrase - based statistical machine translation it achieved 36 % BLEU score .
In their study , NMT outperformed SMT .
In our study , we performed the first comparative NMT analysis of Myanmar dialectal language with three prominent architectures : recurrent neural network ( RNN ) , convolutional neural network ( CNN ) , and transformer .
We investigated the translation quality of the corresponding hyper-parameters ( batch size , learning rate , cell type , and activation function ) in machine translation between the standard Myanmar and national varieties of the same group of languages .
In addition , we used two types of segmentation schemes : word byte pair encoding ( Word - BPE ) segmentation and syllable byte pair encoding ( Syllable - BPE ) segmentation .
We compared the performance of this method to SMT and NMT experiments with the RNN , transformer , and CNN .
We found that the transformer with Word - BPE segmentation outperformed both CNN and RNN for both Myanmar -Rakhine and Myanmar - Rakhine translations .
We also found that CNN with Syllable - BPE segmentation obtained a higher score compared with RNN and the transformer .
Rakhine Language Rakhine ( Arakanese ) is one of the eight national ethnic groups in the Republic of the Union of Myanmar .
The Arakan was officially altered to " Rakhine " in 1989 and is located on a narrow coastal strip on the west of Myanmar , 300 miles long and 50 to 20 miles wide .
The total population in all countries is nearly 3 million .
The Rakhine language has been studied by researchers .
L.F- Taylor 's " The Dialects of Burmese " described comparative pronunciation , sentence construction , and grammar usage in Rakhine , Dawei , In - tha , Taung-yoe , Danu , and Yae .
Professor Denise Bernot , in " The vowel system of Arakanese and Tavoyan , " mainly emphasized the vowels of standard Myanmar and Tavoyan ( Dawei ) in 1965 .
In " Three Burmese Dialects " ( 1969 ) , the linguist John Okell studied the spoken language of Myanmar , Dawei , and In-tha : specifically , usage of grammar and vowel differences ( OKELL , 1995 ) .
Although the Rakhine language used the script as Arakanese or Rakkhawanna Akkhara before at least the 8th century A.D. , the current Rakhine script is nearly the same as the Myanmar script .
Generally , the Arakanese language is mutually intelligible with the Myanmar language and has the same word order ( namely , subject - object - verb ( SOV )
Difference between the Rakhine and standard Myanmar language
The Rakhine language is a largely monosyllabic and analytic language , with a SOV word order , and it uses the Myanmar script .
It is considered by some to be a dialect of the Myanmar language , though it differs significantly from the standard Myanmar language in its vocabulary and includes loan words from Bengali , Hindi , and English .
Compared with the Myanmar language , the speech of the Rakhine language is likely to be closer to the written form .
The Rakhine language notably retains an /r/ sound that has become / j / in the Myanmar language .
Rakhine speakers pronounce the medial " ? " as " Yapint " ( i.e. , / j / sound ) and the medial " ? " as " Rayit " ( i.e. , /r/ sound ) .
Moreover , Myanmar vowel " ? " ( / e/ sound ) is pronounced as " ? " ( / i/ sound ) in Rakhine language .
Thus , for example , the word " dog " in the Myanmar language is written as " ? ? " ( Khwe ) , and in the Rakhine language it is written as " ? ? ? " ( khwii ) .
Similarly , Rakhine pronounce " ? " ( / e:/ ) for Myanmar pronunciation of " ? " ( / ai / ) syllable .
Thus , Myanmar word " ? ? ? " ( peh-hinn ) ( pea curry in English ) is pronounced " ? ? " ( pay-hinn ) in the Rakhine language .
Some Pali words are also used in the Rakhine language .
For example , the word " guest " of Myanmar monks " ? ? " ( Agantu ) is used in normal speech of Rakhine and it is similar to the normal Myanmar word " guest , " " ? ? " ( Ai thay ) .
In summary , the most significant differences between the Rakhine and Myanmar languages are in their pronunciation and vocabulary ; there are no grammatical differences .
Segmentation
Word Segmentation
In both Myanmar and Rakhine texts , spaces are used to separate the phrases for easier reading .
The spaces are not strictly necessary and are rarely used in short sentences .
There are no clear rules for using spaces .
Thus , spaces may ( or may not ) be inserted between words , phrases , and even between root words and their affixes .
Although Myanmar sentences of ASEAN - MT corpus ( Boonkwan and Supnithi , 2013 ) are already segmented , we have to consider some rules for manual word segmentation of Rakhine sentences .
We defined Rakhine " word " to be a meaningful unit .
Affix , root word , and suffix ( s ) are separated such as " ? ? " , " ? ? ? " , " ? ? ? ? " . Here , " ? " ( " eat " in English ) is a root word and the others are suffixes for past and future tenses .
As Myanmar language , Rakhine plural nouns are identified by the following particle .
We added a space between the noun and the following particle : for example a Rakhine word " ? ? ? ? ? " ( ladies ) is segmented as two words " ? ? ? ? " and the particle " ? " .
In Rakhine grammar , particles describe the type of noun and are used after a number or text number .
For example , a Rakhine word " ? ? ? ? ? ? ? " ( " two coins " in English ) is segmented as " ? ? ? ? ? ? ? " .
In our manual word segmentation rules , compound nouns are considered as one word .
Thus , a Rakhine compound word " ? ? " + " ? ? " ( " money " + " bag " in English ) is written as one word " ? ? ? " ( " wallet " in English ) .
Rakhine adverb words such as " ? " ( " really " in English ) , " ? " ( " quickly " in English ) are also considered as one word .
The following is an example of word segmentation for a Rakhine sentence in our corpus , and the meaning is " Among the four air conditioners in our room , two are out of order . "
Unsegmented sentence : ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
Segmented sentence : ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
Syllable Segmentation Generally , Myanmar words are composed of multiple syllables , and most of the syllables are composed of more than one character .
Syllables are also the basic units for pronunciation of Myanmar words .
If we only focus on consonant - based syllables , the structure of the syllable can be described with Backus normal form ( BNF ) as follows : Syllable := CM W [ CK ] [ D ]
Here , C stands for consonants , M for medials , V for vowels , K for vowel killer character , and D for diacritic characters .
Myanmar syllable segmentation can be done with a rule-based approach ( Maung and Makami , 2008 ; Thu et al. , 2013 ) , finite state automaton ( FSA ) ( Hlaing , 2012 ) , or regular expressions ( RE ) ( https://github.com/ye-kyawthu/sylbreak).
In our experiments , we used RE - based Myanmar syllable segmentation tool named " sylbreak . "
( Sennrich et al. , 2016 ) proposed a method to enable open-vocabulary translation of rare and unknown words as a sequence of subword units representing BPE algorithm ( Gage , 1994 ) .
The input is a monolingual corpus for a language ( one side of the parallel training data , in our case ) and starts with an initial vocabulary , the characters in the text corpus .
The vocabulary is updated using an iterative greedy algorithm .
In every iteration , the most frequent bigram ( based on the current vocabulary ) in the corpus is added to the vocabulary ( the merge operation ) .
The corpus is again encoded using the updated vocabulary , and this process is repeated for a predetermined number of merge operations .
The number of merge operations is the only hyperparameter of the system that needs to be tuned .
A new word can be segmented by looking up the learnt vocabulary .
For instance , a new word " rocket , " ? ? ? ? may be segmented as ?@@ ?
? ? ? ? after looking up the learnt vocabulary , assuming ? and ? ? ? ? ? as BPE units learnt during the training .
Byte-Pair-Encoding
Encoder-Decoder Models for NMT
The core idea is to encode a variable - length input sequence of tokens into a sequence of vector representations , and then decode these representations into a sequence of output tokens .
Formally , with a given sentence p(y t | Y 1:t?1 , X ; ? ) = sof tmax ( W o st + b o ) , ( 1 ) where W o scales to the dimension of the target vocabulary V trg .
Stacked RNN with attention
The encoder consists of a bidirectional RNN followed by a stack of unidirectional RNNs .
An RNN at layer l produces a sequence of hidden states h l 1 . . . h l n : h l i = f enc ( h l?1 i , h l i?1 ) , ( 2 ) where f rnn is some non-linear function , such as a gated recurrent unit ( GRU ) or long shortterm memory ( LSTM ) cell , and h l?1 i is the output from the lower layer at step i .
The bidirectional RNN on the lowest layer uses embeddings E S x i as input and concatenates the hidden states of a forward and a reverse RNN : h 0 i = [ ? ? h 0 i ; ? ? h 0 i ] .
With deeper networks , learning becomes increasingly difficult ( Hochreiter et al. , 2001 ; Pascanu et al. , 2012 ) , and residual connections of the form ( He et al. , 2016 ) . h l i = h l?1 i + f enc ( h l?1 i , h l i?1 ) become essential
The decoder consists of an RNN to predict one target word at a time through a state vector s : s t = f dec ( [ E T y t?1 ; st ?1 ] , s t?1 ) , ( 3 ) where f dec is a multilayer RNN , s t?1 the previous state vector , and st ?1 the sourcedependent attentional vector .
Providing the attentional vector as an input to the first decoder layer is also called input feeding ( Luong et al. , 2015 ) .
The initial decoder hidden state is a non-linear transformation of the last encoder hidden state : s 0 = tanh ( W init h n + b init ) .
The attentional vector st combines the decoder state with a context vector c t : st = tanh ( W s[s t ; c t ] ) , ( 4 ) where c t is a weighted sum of encoder hidden states : c t = ?
n i=1 ? ti h i .
The attention vector ?
t is computed by an attention network ( Bahdanau et al. , 2014 ; Luong et al. , 2015 ) : ? ti = softmax ( score ( s t , h i ) ) score(s , h ) = v ? a tanh ( W u s + W v h ) . ( 5 )
Self-Attentional Transformer
The transformer model ( Vaswani et al. , 2017 ) uses attention to replace recurrent dependencies , making the representation at time step i independent from the other time steps .
This requires the explicit encoding of positional information in the sequence by adding fixed or learned positional embeddings to the embedding vectors .
The encoder uses several identical blocks consisting of two core sublayers : self-attention and a feedforward network .
The self-attention mechanism is a variation of the dot-product attention ( Luong et al. , 2015 ) generalized to three inputs : query matrix Q ? R n?d , key matrix K ? R n?d , and value V ? R n?d , where d denotes the number of hidden units .
( Vaswani et al. , 2017 ) further extend attention to multiple heads , allowing for focusing on different parts of the input .
A single head u produces a context matrix C u = softmax ( QW Q u ( KW K u ) T ? d u ) VW V u , ( 6 ) where matrices W Q u , W K u , and W V u are in R d?du .
The final context matrix is given by concatenating the heads , followed by a linear transformation : C = [ C 1 ; . . . ; C h ]W O .
The form in Equation 6 suggests parallel computation across all time steps in a single large matrix multiplication .
Given a sequence of hidden states h i ( or input embeddings ) , concatenated to H ?
R n?d , the encoder computes selfattention using Q = K = V = H .
The second subnetwork of an encoder block is a feedforward network with ReLU activation defined as FFN ( x ) = max ( 0 , xW 1 + b 1 ) W 2 + b 2 , ( 7 ) which is also easily parallelizable across time steps .
Each sublayer , self-attention and feedforward network , is followed by a postprocessing stack of dropout , layer normalization , and residual connection .
The decoder uses the same self-attention and feedforward networks subnetworks .
To maintain auto-regressiveness of the model , self-attention is masked out on future time steps according to ( Vaswani et al. , 2017 ) .
In addition to self-attention , a source attention layer , which uses the encoder hidden states as key and value inputs , is added .
Given decoder hidden states S ? R m?s and the encoder hidden states of the final encoder layer H l , source attention is computed as in Equation 5 with Q = S , K = H l , V = H l .
As in the encoder , each sublayer is followed by a postprocessing stack of dropout , layer normalization ( Ba et al. , 2016 ) , and residual connection .
Fully Convolutional Models
The convolutional model ( Gehring et al. , 2017 ) uses convolutional operations and also dispenses with recurrence .
Hence , input embeddings are again augmented with explicit positional encodings .
The convolutional encoder applies a set of ( stacked ) convolutions that are defined as h l i = v( W l [ h l?1 i?k/2 ? ; . . . ; h l?1 i+?k/2 ? ] + b l ) + h l?1 i , ( 8 ) where v is a non-linearity such as a gated linear unit ( Gehring et al. , 2017 ; Dauphin et al. , 2016 ) , and W l ?
R dcnn? kd are the convolutional filters .
To increase the context window captured by the encoder architecture , multiple layers of convolutions are stacked .
To maintain sequence length across multiple stacked convolutions , inputs are padded with zero vectors .
The decoder is similar to the encoder but adds an attention mechanism to every layer .
The output of the target side convolution s l * t = v( W l [ s l?1 t?k + 1 ; . . . ; sl?1 t ] + b l ) ( 9 ) is combined to form S * and then fed as an input to the attention mechanism of Equation 6 with a single attention head and Q = S * , K = H l , V = H l , resulting in a set of context vectors c t .
The full decoder hidden state is a residual combination with the context such that sl t = s l * t + c t + sl ? 1 t ( 10 ) To avoid convolving over future time steps at time t , the input is padded to the left .
Experiments
Corpus Preparation and Statistics
We used 18,373 Myanmar sentences ( with no name entity tags ) of the ASEAN - MT Parallel Corpus ( Boonkwan and Supnithi , 2013 ) , which is a parallel corpus in the travel domain .
It contains six main categories : people ( greeting , introduction , and communication ) , survival ( transportation , accommodation , and finance ) , food ( food , beverages , and restaurants ) , fun ( recreation , traveling , shopping , and nightlife ) , resource ( number , time , and accuracy ) , special needs ( emergency and health ) .
Manual translation into the Rakhine language was done by native Rakhine students from two Myanmar universities , and the translated corpus was checked by the editor of a Rakhine newspaper .
Word segmentation for Rakhine was done manually , and there are exactly 123,018 words in total .
We used 14,076 sentences for training , 2,485 sentences for development , and 1,812 sentences for evaluation .
Moses SMT system
We used the Moses toolkit ( Koehn et al. , 2007 ) for training the operation sequence model ( OSM ) statistical machine translation systems .
We did not consider phrase - based statistical machine translation ( PBSMT ) and hierarchical phrase - based statistical machine translation ( HPBSMT ) , because the OSM approach achieved the highest BLEU ( Papineni et al. , 2002 ) and RIBES ( Isozaki et al. , 2010 ) scores among three approaches ( Oo et al. , 2018 ) for both Myanmar - Rakhine to Rakhine - Myanmar statistical machine translations .
The word-segmented ( i.e. , Syllable - BPE and Word - BPE ) source language was aligned with the word-segmented target language using GIZA ++.
The alignment was symmetrized by grow-diag-final and heuristic .
The lexicalized reordering model was trained with the msd-bidirectional - fe option .
We used KenLM ( Heafield , 2011 ) for training the 5 gram language model with modified Kneser - Ney discounting .
Minimum error rate training ( MERT ) ( Och , 2003 ) was used to tune the decoder parameters , and the decoding was done using the Moses decoder ( version 2.1.1 ) ( Koehn et al. , 2007 ) .
We used the default settings of Moses for all experiments .
Batch
Framework for NMT
An open-source sequence - to-sequence toolkit for NMT written in Python ( Hieber et al. , 2017 ) and built on Apache MXNET ( Chen et al. , 2015 ) , the toolkit offers scalable training and inference for the three most prominent encoder-decoder architectures : attentional recurrent neural network ( Schwenk , 2012 ; Kalchbrenner and Blunsom , 2013 ; Sutskever et al. , 2014 b ) , self-attentional transformers ( Vaswani et al. , 2017 ) , and fully convolutional networks ( Gehring et al. , 2017 ) .
Training Details
We used the Sockeye toolkit , which is based on MXNet , to train NMT models .
The initial learning rate is set to 0.0001 .
If the per-formance on the validation set has improved for 8 checkpoints , the learning rate is multiplied by 32 checkpoints .
All the neural networks have eight layers .
For RNN Seq2Seq , the encoder has one bi-directional LSTM and six stacked unidirectional LSTMs , and the encoder is a stack of eight unidirectional LSTMs .
The size of hidden states is 512 .
We apply layer - normalization and label smoothing ( 0.1 ) in all models .
We tie the source and target embeddings .
The dropout rate of the embeddings and transformer blocks is set to ( 0.1 ) .
The dropout rate of RNNs is ( 0.2 ) .
The attention mechanism in the transformer has eight heads .
We applied three different batch sizes ( 128 , 256 , and 512 ) for RNN , Transformer , and CNN network architectures .
The learning rate varies from 0.0001 to 0.0005 .
Two memory cell types GRU and LSTM were used for the RNN and transformer .
Moreover , two activation functions were applied to the CNN architecture .
The comparison between Syllable - BPE and Word - BPE segmentation schemes was done for both SMT ( i.e. , OSM ) and NMT ( RNN , Transformer , and CNN ) techniques .
All experiments are run on NVIDIA Tesla K80 24GB GDDR5 .
We trained all models for the maximum number of epochs using the AdaGrad and adaptive moment estimation ( Adam ) optimizer .
The BPE segmentation models were trained with a vocabulary of 8,000 .
Evaluation
We used automatic criteria to evaluate the machine translation output .
The metric BLEU ( Papineni et al. , 2002 ) measures the adequacy of the translation between language pairs , such as Myanmar and English .
The Higher BLEU scores are better .
Before computing BLEU , the translations were decomposed into their constituent syllables to ensure that the results are cross-comparable .
Results and Discussion
The BLEU score results for three NMT approaches ( RNN , Transformer , and CNN ) with three batch sizes ( 128 , 256 , and 512 ) for Syllable - BPE segmentation scheme are shown in Table 1 . Bold numbers indicate the highest BLEU score among different batch sizes .
CNN achieved the highest BLEU scores for both Myanmar-Rakhine and Rakhine - Myanmar translations .
However , the transformer architecture achieved the top BLEU scores for Word - BPE segmentation schemes for both Myanmar -Rakhine and Rakhine - Myanmar neural machine translations ( see Table 2 ) .
From the experimental results of Table 1 and Table 2 , we noticed that RNN and Transformer NMT with Syllable - BPE have a decreased translation performance for batch size 512 .
Thus , we used batch size 256 for further experiments with the RNN and transformer architectures .
The NMT performance of the RNN and transformer with Syllable - BPE segmentation schemes together with different learning rates ( from 0.0001 to 0.0005 ) and two different memory cell types ( GRU and LSTM ) can be seen in Table 3 .
From these BLEU scores of the RNN and transformer approaches , LSTM gave the highest NMT performance for both Myanmar - Rakhine dialect translation and vice versa .
To observe the maximum translation performance of CNN architecture , we extended experiments by using two activation functions ( ReLu and Soft - ReLu ) , two batch sizes ( 128 and 256 ) , and five learning rates ( from 0.0001 to 0.0005 ) ( see Table 4 ) .
Here , bold numbers indicate the highest BLEU scores of each batch size .
From these results , we can clearly see that Soft - ReLu achieved the highest BLEU scores for both Myanmar to Rakhine and Rakhine to Myanmar translations .
We found that the training processes with learning rate 0.0004 and 0.0005 were stopped for the batch size 128 for both ReLu and Soft - ReLu activation functions .
We also made a comparison between SMT and NMT , and the results can be seen in Table 5 .
In this study , we run only OSM approach for the SMT experiments based on the our previous SMT work ( Oo et al. , 2018 ) .
The Table 5 presents that although CNN achieved the top BLEU score ( 83.75 ) for Myanmar to Rakhine translation , OSM gave the best BLEU ( 84.36 ) score for Rakhine to Myanmar translation .
Furthermore , we also found that Syllable - BPE segmentation scheme is working well for both SMT and NMT for Myanmar - Rakhine dialect language pair .
As shown in the experimental results of Table 1 to Table 5 , our dialect NMT experiments give significantly higher BLEU scores than other SMT on different language pairs such as Myanmar - Chinese , Myanmar-German , Myanmar - Japanese , Myanmar-Malaysian , Myanmar-Korean , Myanmar -Spanish , Myanmar - Thai , Myanmar-Vietnamese
( Thu et al. , 2016 ) , and also for NMT on Myanmar-English ( Sin and Soe , 2018 ) .
As we discussed in Section 3 , Rakhine and Myanmar have the same word order of SOV and also share a lot of vocabulary .
For these reasons , we assume that both SMT and NMT systems reach a very high machine translation performance .
Conclusion
This paper presents the first study of the neural machine translation between Standard Myanmar and Rakhine ( a spoken Myanmar dialect ) .
We implemented three prominent NMT systems : RNN , transformer , and CNN .
The systems were evaluated on a Myanmar - Rakhine parallel text corpus that we are developing .
We also investigated two types of segmentation schemes ( Word - BPE segmentation and Syllable - BPE segmentation ) .
Our results clearly show that the highest performance of SMT and NMT was obtained with Syllable - BPE segmentation for both Myanmar-Rakhine and Rakhine - Myanmar translation .
If we only focus on NMT , we find that the transformer with Word - BPE segmentation outperforms CNN and RNN for both Myanmar -Rakhine and Rakhine - Myanmar .
We also find that CNN with syllable - BPE segmentation obtains a higher BLEU score compared with the RNN and transformer .
In the near future , we plan to conduct a further study with a focus on NMT models with one more subword segmentation scheme SentencePiece for Myanmar - Rakhine NMT .
Moreover , we intend to investigate SMT and NMT approaches for other Myanmar dialect languages , such as Myeik and Dawei . | X ) as a target language sequence model , conditioning the probability of target word y t on target history Y 1:t?1 and source sentence X. Both x i and y t are integer IDs given by the source and target vocabulary mapping , V src and V trg , built from the training data tokens and represented as one- hot vectors x i ? { 0 , 1 } | Vsrc | and y t ? { 0 , 1 } | Vtrg | .
These are embedded into e-dimensional vector representations ( Vaswani et al. , 2017 ) E S x i and E T y t , using embedding matrices R e?|Vsrc | and E T ? R e?|Vtrg | .
The target sequence is factorized as p( Y | X ; ? ) = ? m t=1 p(y t | Y 1:t?1 , X ; ? ) .
The model , parameterized by ? , consists of an encoder and decoder part , which vary depending on the model architecture .
p(y t | Y 1:t?1 , X ; ? ) is parameterized via a softmax output layer over some decoder representations
St : X = x 1 , ? , x n and target sentence Y = y 1 , ? , y m , an NMT system models p( Y
