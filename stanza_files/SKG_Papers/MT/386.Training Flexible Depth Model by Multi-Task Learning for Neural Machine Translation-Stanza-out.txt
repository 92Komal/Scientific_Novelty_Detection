title
Training Flexible Depth Model by Multi-Task Learning for Neural Machine Translation
abstract
The standard neural machine translation model can only decode with the same depth configuration as training .
Restricted by this feature , we have to deploy models of various sizes to maintain the same translation latency , because the hardware conditions on different terminal devices ( e.g. , mobile phones ) may vary greatly .
Such individual training leads to increased model maintenance costs and slower model iterations , especially for the industry .
In this work , we propose to use multi-task learning to train a flexible depth model that can adapt to different depth configurations during inference .
Experimental results show that our approach can simultaneously support decoding in 24 depth configurations and is superior to the individual training and another flexible depth model training method -- LayerDrop .
Introduction
As neural machine translation models become heavier and heavier ( Vaswani et al. , 2017 ) , we have to resort to model compress techniques ( e.g. , knowledge distillation ( Hinton et al. , 2015 ; Kim and Rush , 2016 ) ) to deploy smaller models in devices with limited resources , such as mobile phones .
However , a practical challenge is that the hardware conditions of different devices vary greatly .
To ensure the same calculation latency , customizing distinct model sizes ( e.g. , depth , width ) for different devices is necessary , which leads to huge model training and maintenance costs ( Yu et al. , 2019 ) .
For example , we need to distill the pretrained large model into N individual small models .
The situation becomes worse for the industry when considering more translation directions and more frequent model iterations .
An ideal solution is to train a single model that can run in different model sizes .
Such attempts have been explored in SlimNet ( Yu et al. , 2019 ) and LayerDrop ( Fan et al. , 2020 ) .
SlimNet allows running in four width configurations by joint training of these width networks , while LayerDrop can decode with any depth configuration by applying Dropout ( Srivastava et al. , 2014 ) on layers during training .
In this work , we take a further step along the line of flexible depth network like LayerDrop .
As shown in Figure 1 , we first demonstrate that when there is a large gap between the predefined layer dropout during training and the actual pruning ratio during inference , LayerDrop 's performance is poor .
To solve this problem , we propose to use multitask learning to train a flexible depth model by treating each supported depth configuration as a task .
We reduce the supported depth space for the aggressive model compression rate and propose an effective deterministic sub-network assignment method to eliminate the mismatch between training and inference in LayerDrop .
Experimental results on deep Transformer ( Wang et al. , 2019 ) show that our approach can simultaneously support decoding in 24 depth configurations and is superior to the individual training and LayerDrop .
2 Flexible depth model and LayerDrop
Flexible depth model
We first give the definition of flexible depth model ( FDM ) : given a neural machine translation model M M ?N whose encoder depth is M and decoder depth is N , in addition to ( M, N ) , if M M ?N can also simultaneously decode with different depth configurations ( m i , n i ) k i=1 where m i ?
M and n i ?
N and obtain the comparable performance with independently trained model M m i ?n i , we refer to M M ?N as a flexible depth model with a capacity of k .
We notice that although a pretrained vanilla Transformer can force decoding with any depth , its performance is far behind the independently trained model 1 . Therefore , the vanilla Transformer does not belong to FDM .
LayerDrop
In NMT , both encoder and decoder are generally composed of multiple layers with residual connections , which can be formally described as : x i+ 1 = x i + Layer ( x i ) .
( 1 ) To make the model robust to pruned layers ( shallower networks ) , LayerDrop proposed by Fan et al . ( 2020 ) , applies structured dropout over layers during training .
A Bernoulli distribution associated with a pre-defined parameter p ? [ 0,1 ] controls the drop rate .
It modifies Eq. 1 as : x i+1 = x i + Q i * Layer( x i ) ( 2 ) where P r( Q i = 0 ) = p and P r( Q i = 1 ) = 1 ? p. In this way , the l-th layer theoretically can take any proceeding layer as input , rather than just the previous one layer ( l ? 1 - th layer ) .
At runtime , given the desired layer - pruning ratio p = 1 ? D inf / D where D inf is the number of layers actually used in decoding and D is the total number of layers , LayerDrop selects to remove the d-th layer such that : d ? 0 ( mod 1 p ) ( 3 ) 1 BLEU score is only 0.14 if we ask the vanilla Transformer with M=12 and N=6 to decode with M=1 and N=1 directly .
However , an individual trained model with M=1 and N=1 can obtain 30.36 .
LayerDrop 's problem for flexible depth Although LayerDrop can play a good regularization effect when training deep Transformer ( Fan et al. , 2020 ) , we argue that this method is not suitable for FDM .
As illustrated in Figure 1 , we demonstrate that LayerDrop suffers a lot when there is a large gap between the pre-defined layer dropout p in training and the actual pruning ratio p at runtime .
We attribute it to two aspects : 1 . Huge sub-network space in training .
Consider a D-layer network , because each layer can be masked or not , up to 2 D sub-networks are accessible during training , which is a major challenge when D is large .
Mismatch between training and inference .
As opposite to training , LayerDrop uses a deterministic sub-network at inference when given the layer pruning ratio p ( See Eq. 3 ) , which leads to a mismatch between training and inference .
For example , for D=6 and D inf =3 , there are D D inf sub-network candidates during training , while only one of them is used in decoding .
Algorithm 1 : Training Flexible Depth Model by Multi-Task Learning .
Reduce depth space .
For depth D , in principle , LayerDrop can be pruned to any depth of ?( D ) = { 0 , 1 , 2 , . . . , D}. However , consider the actual situation of model compression for resourcelimited devices , it is unnecessary if the compressing rate is too low , e.g. , D ? D -1 .
Therefore , for an aggressive compress rate , we replace the entire space ?( D ) with the set of all positive divisors of D 2 : 1 pre-train M M ?N on training data D ; 2 generate distillation data D by M M ?N ; 3 M M ?N ? M M ?N ; 4 for t in 1 , 2 , . . . , T do 5 B ? sample batch from D ; 6 gradient G ? 0 ; 7 for ( m i , n i ) in ?( M ) ? ?( N ) do 8 SN e , SN d ? F( m i , M ) , F( n i , N ) ; 9 Feed B into network ( SN e , SN d ) ; ?( D ) = { d | D %d = 0 , 1 ? d ? D } ( 4 )
The physical meaning of ?( D ) is to compress every D/d layers into one layer , where d ? ?( D ) .
Guideline for deterministic sub-network assignment .
The use of deterministic sub-networks is critical to maintaining the consistency between training and inference .
However , for each d ? ?( D ) , it is not trivial to decide which d layers should be selected to construct the d-layer subnetwork .
Here we propose two metrics to guide the procedure .
The first is task balance ( TB ) , whose motivation is to make every layer have as uniform tasks as possible .
We use the standard deviation of the number of tasks per layer to measure it quantitatively : TB = i? [ 1 , D ] t( i ) ? t 2 D ( 5 ) 2 For the diversity of depth configuration , we assume that D is not a prime number in this work .
where t( i ) is the number of tasks in which the i-th layer participates and t = d ? ?( D ) d D .
The second is average layer distance ( ALD ) , which requires the distance between adjacent layers in the subnetwork SN ( d ) = { L a 1 , L a 2 , . . . , L a d } should be large .
For example , for a 6 - layer network , if we want to build a 2 - layer sub-network , it is unreasonable to select { L 1 , L 2 } directly because the features extracted by adjacent layers are semantically similar ( Peters et al. , 2018 ; Raganato and Tiedemann , 2018 ) .
Therefore , we use the average distance between layers in all sub-networks as the metric : ALD = d? ?( D ) a i , a i + 1 ?SN ( d ) | a i+1 ? a i | Z ( 6 ) where Z = d? ?( D ) ( d ? 1 ) is the normalization item .
Proposed method .
Guided by these two metrics , we design an effective sub-network assignment method Optimal .
We record the usage state s i of each layer to ensure not to put too many tasks on the same layer .
At initialization , we set s i as Alive .
For d ? ?( D ) , Optimal prioritizes to process large depth .
Optimal uniformly assigns one layer for every c = D/d layers to make ALD high .
In each chunk , we pick the middle layer of ceil ( c / 2 ) ? 1 ( called MiddleLeft ) .
Note that , LayerDrop uses the leftmost layer in each chunk ( called Left ) , as shown in Eq.
3 . Although Left and MiddleLeft have the same ALD , we found that there is a large gap in TB .
For example , when D=12 , Left 's TB is 1.5 , which is much higher than MiddleLeft 's 0.78 ( lower is better ) .
Then , Optimal records which layers are used and picks the less used layers as much as possible .
Each used layer is marked as Dead .
If current alive layers cannot accommodate the picked depth d , we pass it and choose a smaller d until the alive layers are sufficient , or reset all layers as Alive .
Training .
Algorithm 1 describes the training process of our method .
During training , compared with individual training and LayerDrop from scratch , our FDM finetunes on the individually pretrained M M ?N and uses sequence - level knowledge distillation ( Seq - KD ) ( Kim and Rush , 2016 ) to help shallower networks training .
We note that in conventional Seq - KD , the student model cannot finetune on the teacher model directly because the two models have different sizes .
However , FDM allows models with different depths to share the same parameters , and finetuning on the pre-trained teacher model also promotes model convergence .
M N 1 2 3 6 Base ? LD ? M T Base ? LD ? M T Base ? LD ? M T Base ? LD ? M
Experiments
Setup
We conducted experiments on IWSLT '14 German ?
English ( De? En , 160k ) following the same setup as Wu et al . ( 2019 ) .
To verify FDM 's efficiency , we train all models with a deep encoder to contain more tasks .
Specifically , we train a PreNorm Transformer ( Wang et al. , 2019 ) with M=12 and N=6 .
See Appendix
A for the details .
We mainly compare our method MT with the two baselines :
Results and Analysis Main results .
As shown in Table 1 , we compared Baseline , LayerDrop and our MT in all tasks .
Although LayerDrop outperforms our method 3 Original LayerDrop in Fan et al. ( 2020 ) samples a batch to update the model , while we modify it by accumulating 6?4=24 batches to keep the training cost comparable with Baseline and MT .
Also , more samples improve Layer - Drop 's performance .
For example , the average BLEU score in 24 tasks with one batch and 24 batches is 32.31 and 34.18 , respectively .
We report TB and ALD on encoder side .
? denotes the lower the better , while ? is on contrary .
Note that , unlike the standard BLEU score , BLEU 6?4 is more difficult to change significantly because it is scaled of the number of tasks .
Strategy when a few layers pruned , we can see that MT is the winner in most tasks ( 20 / 24 ) .
It indicates that our method is superior to LayerDrop for FDM training and demonstrates the potential to substitute a dozen models with different depths to just one model .
Besides , in line with Fan et al . ( 2020 ) , it is interesting to see the FDM without any pruning outperforms the individually trained model ( see M=12 , N=6 ) , which is obvious evidence that jointly training of various depth models has a good regularization effect .
Knowledge distillation .
Table 2 shows average BLEU scores of 24 tasks when training a flexible depth model with / without Seq-KD .
It is clear that using distillation data helps FDM training in all systems , which is in line with the previous singlemodel compression study ( Kim and Rush , 2016 ) .
According to Zhou et al . ( 2020 ) , Seq -KD makes the training data distribution smoother , so we suspect that FDM benefits from Seq - KD because of the difficulty of multi-task learning .
Sub-layer assigiment strategy .
Training efficiency .
Our multi-task learning needs to accumulate gradients on all tasks , and its cost is linearly related to the number of tasks .
Actually , we can sample fewer tasks instead of enumerating them all .
For example , randomly sam - 5 .
First of all , we can see that more training costs can obtain better performance .
Compared with reducing tasks and reducing batches , we found that the former is a better choice .
In particular , sampling more depths on the encoder side is more important than the decoder side , which is consistent with the recent observation in Wang et al . ( 2019 ) that encoder is more important than decoder in terms of translation performance .
Conclusion
We demonstrated LayerDrop is not suitable for FDM training because of ( 1 ) the huge sub-network space in training and ( 2 ) the mismatch between training and inference .
Then we proposed to use multi-task learning to mitigate it .
Experimental results show that our approach can decode with up to 24 depth configurations and obtain comparable or better performance than individual training and LayerDrop .
In the future , we plan to explore more effective FDM training methods , and combining flexible depth and width is also one of the attractive directions .
10 Collect gradient g by Back - Propa. ; 11 G ? G + g ; 12 end 13 Optimize M M ?N with gradient G ; 14 end 15 Return M M ?N
