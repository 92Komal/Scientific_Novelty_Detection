title
When Does Unsupervised Machine Translation Work ?
abstract
Despite the reported success of unsupervised machine translation , the field has yet to examine the conditions under which the methods succeed and fail .
We conduct an extensive empirical evaluation using dissimilar language pairs , dissimilar domains , and diverse datasets .
We find that performance rapidly deteriorates when source and target corpora are from different domains , and that stochasticity during embedding training can dramatically affect downstream results .
We advocate for extensive empirical evaluation of unsupervised MT systems to highlight failure points and encourage continued research on the most promising paradigms .
Towards this goal , we release our preprocessed dataset to stress -test systems under multiple data conditions .
Introduction Machine translation ( MT ) has progressed rapidly since the advent of neural machine translation ( NMT ) ( Kalchbrenner and Blunsom , 2013 ; Bahdanau et al. , 2015 ; and is better than ever for languages for which ample high-quality bitext exists .
Conversely , MT for lowresource languages remains a great challenge due to a dearth of parallel training corpora and poor quality bitext from esoteric domains .
To address this , several authors have proposed unsupervised MT techniques , which rely only on monolingual text for training ( e.g. , Ravi and Knight , 2011 ; Yang et al. , 2018 ; Artetxe et al. , 2018c ; Hoshen and Wolf , 2018 ; Lample et al. , 2018a , b ; Artetxe et al. , 2018 b . Recent unsupervised MT results appear promising , but they primarily report results for the highresource languages for which traditional MT already works well .
The limits of these methods are so far under-explored .
For unsupervised MT to be a viable path for low-resource machine translation , the field must determine ( 1 ) if it works outside highly - controlled environments , and ( 2 ) how to effectively evaluate newly - proposed training paradigms to pursue those which are promising for real-world low-resource scenarios .
Unsupervised MT methods must work ( 1 ) on different scripts and between dissimilar languages , ( 2 ) with imperfect domain alignment between source and target corpora , ( 3 ) with a domain mismatch between training data and the test set , and ( 4 ) on the low-quality data of real low-resource languages .
These factors reflect the real-life challenges of lowresource translation .
Our main contribution is an extensive analysis of unsupervised MT with regards to factors ( 1 ) - ( 3 ) above .
1 We find that ( a ) translation performance rapidly deteriorates when source and target corpora are from different domains , ( b ) stochasticity during word embedding training can dramatically affect downstream bilingual lexicon induction ( BLI ) and translation performance , and ( c ) like in the bilingual lexicon induction literature , unsupervised MT performance declines when source and target languages are dissimilar .
While ( 4 ) is not the focus of this paper , we do observe very low performance on an authentic low-resource language pair , corroborating previous studies ( Guzm ? n et al. , 2019 ) .
Finally , as there are no standard evaluation protocols to ensure that unsupervised MT systems are robust to the types of data anomalies ubiquitous in low-resource translation settings , we advocate for extensive empirical evaluation of unsupervised MT systems to highlight failure points and encourage continued research on the most promising paradigms .
We first discuss related work in Section 2 , followed by a detailed overview of the unsupervised MT architecture in Section 3 .
In Section 4 , we discuss our research questions , followed by our evaluation methodology and datasets in Sections 5 and 6 .
Section 7 presents our findings , and Section 8 discusses the results .
We conclude in Section 9 .
Related Work Bilingual Lexicon Induction Unsupervised MT methods can be thought of as an end-to - end extension of work inducing bilingual lexicons from monolingual corpora .
Bilingual lexicon induction ( BLI ) using non-parallel data has a rich history , beginning with corpus statistic and decipherment methods ( e.g. , Rapp , 1995 ; Fung , 1995 ; Knight , 2000 , 2002 ; Haghighi et al. , 2008 ) , continuing to modern neural methods to create crosslingual word embeddings ( e.g. Mikolov et al. , 2013a ; , see Ruder et al. ( 2019 for a survey ) which form a critical component of stateof - the - art unsupervised MT systems .
S?gaard et al. ( 2018 ) determine that monolingual embedding spaces of similar languages are not typically isomorphic as was previously believed , and that bilingual dictionary induction " depends heavily on ... the language pair , the comparability of the monolingual corpora , and the parameters of the word embedding algorithms . " argue that unsupervised approaches are unsuccessful with dissimilar languages and domains , and that unsupervised performance has been overly lauded because the conditions under which they were compared with supervised baselines were inequitable .
Evaluation of Embedding Spaces
While a modest body of literature has examined the quality of cross-lingual word embeddings ( CLEs ) by measuring performance on BLI , evaluate on downstream natural language tasks , underlining the importance of fullsystem evaluation .
The authors conclude that " the quality of CLE models is largely task - dependent and that overfitting the models to the BLI task can result in deteriorated performance in downstream tasks . "
Similarly , Doval et al. ( 2019 ) investigate cross-lingual natural language inference .
Evaluation of Unsupervised MT Liu et al. ( 2020 ) helpfully re-define unsupervised machine translation into three distinct categories : ( 1 ) no bitext whatsoever , ( 2 ) the target language pair is linked through bitext via a pivot language , and ( 3 ) no linkage through a pivot language , but bitexts exists for * some * language and the target language .
The authors analyze their multilingual pretraining method with respect to other similar training paradigms ( Conneau and Lample , 2019 ; Song et al. , 2019 ) and evaluate unsupervised MT performance when using backtranslation ( Definition 1 ) or language transfer after finetuning on related bitext ( Definition 3 ) .
In unsupervised MT with no bitext , Lample et al . ( 2018 b ) ablate their PBSMT system , finding that initial phrase table quality is critical and that performance suffers when the language model is trained with less data .
They tweak their NMT embedding initialization method , such as using separatelytrained BPE instead of joint , and word embeddings instead of BPE token embeddings .
They report the results of dropping part of their loss function and making minor changes to the NMT architecture on downstream BLEU score .
Concurrently to our work , Kim et al . ( 2020 ) arrived at similar conclusions to us using a different autoencoder / duallearning unsupervised MT approach based on crosslingual language model pretraining ( Conneau and Lample , 2019 ) ; this complements our experiments and corroborates our results .
Background : Unsupervised MT
Our experiments employ the models of Artetxe et al . ( 2018 b
Another approach to unsupervised MT involves pretraining a bilingual or multilingual model on monolingual text on a general task before finetuning on translation .
Such methods include crosslingual language model pretraining ( Conneau and Lample , 2019 ) , masked sequence - to-sequence pretraining ( Song et al. , 2019 ) , and multilingual denoising pretraining ( Liu et al. , 2020 ) , and have shown promise .
For instance , Liu et al . ( 2020 ) record the first good results on the low-resource Sinhala - English and Nepali-English pairs .
While pretraining and multilingual methods are not the subject of this work , they warrant future evaluation .
Training begins with two monolingual corpora which are not necessarily related in any way ( i.e. they are not assumed to be parallel nor comparable text ) .
First , word embeddings are trained independently for each corpus , resulting in a source and a target embedding space .
Specifically , after preprocessing , Artetxe et al . ( 2018 b ) train two statistical language models using KenLM ( Heafield , 2011 ) , one for the source language and one for the target .
They use phrase2vec 4 ( Artetxe et al. , 2018 b ) , an extension of Mikolov et al . ( 2013 b ) 's skip-gram model , 5 to generate phrase embeddings for 200,000 unigrams , 400,000 bigrams , and 400,000 trigrams .
Next , source and target word embeddings are aligned into a common cross-lingual embedding space .
They run VecMap 6 ( Artetxe et al. , 2018a ) which calculates a linear mapping of one space to another based on the intuition that phrases with similar meaning should have similar neighbors regardless of language .
Given a matrix of source word embeddings X and target word embeddings Z which have been length - normalized , meancentered , then length - normalized again , VecMap calculates M x = XX T and M z = ZZ T .
Each cell M x ij and M z ij is the cosine similarity between words X i and X j , and Z i and Z j , respectively .
M x and M z are symmetric , and if the monolingual vector spaces were fully isometric , M x and M z would be identical besides rows and columns being permuted .
Each row of M x and M z is a similarity distribution .
To exploit this , each row of ?
M x and 4 https://github.com/artetxem/ phrase2vec 5 https://github.com/tmikolov/word2vec 6 https://github.com/artetxem/vecmap ?
M z is sorted ( they find that using the square root works better empirically ) , and length - normalized , mean-centered , and length - normalized again .
For each row i in sorted ( ?
M x ) , they find the row j of sorted ( ?
M z ) that is its nearest neighbor , and assign X i = Z j in the initial translation dictionary D. A cell D ij = 1 if words X i , and Z j are translations of one another , and 0 otherwise .
Next , there is an iterative process of calculating the optimal linear mappings and extracting an updated dictionary .
For calculating the mapping , the goal is to find the linear transformations W x and W z which maximize the cosine similarity of the words that are translations of one another as defined by the dictionary D , over the entire dictionary : arg max Wx , Wz i j ( D ij ) ( ( X i W x ) ? ( Z j W z ) )
From there , they calculate M = XW x W T z Z T , whereby each cell in M is the cosine similarity of word X i and Z j after their transformations with W x and W z .
To avoid poor local optima , they stochastically zero-out some cells of M with probability p = 0.9 , decreasing over time .
The final score for each potential translation candidate is calculated using Cross-domain Similarity Local Scaling ( CSLS ) to mitigate the hubness problem .
CSLS utilizes cosine similarity , which is taken from M .
For each pair of words X i and Z j , the new dictionary cell D ij = 1 if the CSLS score between X i and Z j is the highest over all other words in Z , and D ij = 0 otherwise .
The dictionary is created in both directions , and concatenated .
Readers are directed to Artetxe et al . ( 2018a ) for further details .
The next step extracts an initial phrase - table for use in a SMT system .
They use the softmax over the cosine similarity of the 100 nearest - neighbors of each source phrase embedding as the phrase translation probabilities .
This is done in both directions : ( f |e ) = e ( cos ( e , f ) / ? ) f e ( cos ( e , f ) / ? )
For the target embedding with the highest cosine similarity , the phrases are aligned , and unigram translation probabilities are multiplied to become the lexical weighting .
Combining the preliminary phrase table with a distortion penalty and language model produces the initial unsupervised phrase - based SMT system ( Koehn et al. , 2007 ) .
The SMT model weights are tuned using a variant of MERT ( Och , 2003 ) designed for unsupervised scenarios , which uses 10,000 parallel sentences generated via backtranslation ( Sennrich et al. , 2016 a ) .
The SMT model then undergoes three rounds of iterative backtranslation .
extend their 2018 work by adding a critical " NMT hybridization " final step , which achieves significant gains over SMT alone .
7
An NMT system is trained using backtranslated output from SMT for one epoch .
On the next epoch , a small number of sentences are backtranslated with the newly - trained NMT system and concatenated with a slightly smaller fraction of SMT - generated bitext .
The procedure continues for 30 epochs , gradually increasing the percentage of synthetic training data created by the NMT system until all of the training data is NMT - generated .
The NMT system is trained for an additional 30 epochs of iterative backtranslation using data generated fully by the NMT system of the previous epoch .
The test set is translated with beam search using an ensemble of models saved at every tenth epoch ( six total ) , resulting in BLEU scores of 33.2 and 26.4 ( SacreBLEU ( Post , 2018 ) ) on newstest2014 for French -English and German-English , respectively .
We run Artetxe et al. ( 2018 b 's implementation for our experiments .
Specifically , neural models are Transformer - big ( Vaswani et al. , 2017 ) trained with fairseq on one NVIDIA GeForce GTX 1080 Ti GPU .
Models use shared embeddings , the Adam optimizer with ?
1 = 0.9 , ? 2 = 0.98 ( Kingma and Ba , 2015 ) , label smoothing , initial learning rate of 1e - 07 warming up for 4000 steps to 5e - 04 before decaying , and dropout ( Srivastava et al. , 2014 ) probability of 0.3 .
We set optimizer delay to 4 to simulate 4 GPUs .
To elucidate the performance gap due to the unsupervised architecture , we build a standard supervised NMT system using the same neural architecture described above .
We train until performance on the development set ceases to improve for 10 epochs .
To parallel the unsupervised setup , we translate the test set using an ensemble of 6 models ;
We perform ensemble selection by performance on a validation set , selecting the best-performing checkpoint along with 5 previous checkpoints .
Research Questions
Existing unsupervised translation methods work well on languages which are similar to each other , use the same Roman script , and have an ample amount of monolingual news data available ( which matches the test set domain ) .
Questions remain as to whether unsupervised methods will be useful on authentic low-resource settings where few or none of the aforementioned properties hold .
Namely , does unsupervised MT work with : ? dissimilar languages ?
? dissimilar source and target domains ?
? diverse datasets ?
? authentic low-resource language pairs ?
Such questions reflect the reality of authentic low-resource translation , and are those which must be adequately resolved for unsupervised MT to be a viable alternative to traditional translation methods for the most difficult language pairs .
Evaluation of Unsupervised MT
We perform an extensive empirical evaluation of unsupervised MT .
Our evaluation protocol stresstests an unsupervised MT system under varying conditions to reveal its points of strength and failure .
Systems should be judged on how well they perform : ( 1 ) on dissimilar languages , ( 2 ) on increasingly divergent domains between source and target corpora , ( 3 ) on diverse datasets , and ( 4 ) on authentic low-resource language pairs where data quality is typically low .
Namely , we : 1 . Choose 2 language pairs , at least one of which where the source and target languages utilize different scripts .
2 . Choose 3 datasets of different domains , at least one of which is parallel bitext .
3 . Perform at least one experiment for each language pair under each of the following data conditions : ?
Originally parallel ?
Not originally parallel ?
Different domain for source and target .
4 . Choose 2 true low-resource language pairs .
5 . Judge the system based on performance in all tested scenarios .
The data conditions above are designed measure how well a system performs in regards to the research questions of Section 4 .
Namely , success on a variety of languages with different scripts and linguistic structure indicates robustness to dissimilar languages ; success on multiple datasets of different domains indicates that the system is not specifically designed for one domain at the expense of others , and performs well even when training and test data do not match perfectly ; Step # 3 evaluates performance on increasingly divergent domains between source and target data ; and Step # 4 is the true test - whether the system succeeds on authentic low-resource language pairs .
Datasets
Training datasets used in our reinvestigation of the unsupervised MT system presented in are shown in Table 1 .
We focus on Russian - English ( Ru-En ) and French -English ( Fr-En ) tasks and include as reference Sinhala - English ( Si-En ) and Nepali-English ( Ne-En ) as well .
Following Section 5 , we evaluate the same system under various ablated data setups : ?
The " Supervised " condition is the standard MT training setup which uses parallel bitext . ?
In the " Parallel " condition , an unsupervised MT system is trained on a corpus that was originally parallel ( i.e. UN corpus ) , now being treated as two separate monolingual corpora . ?
In contrast , the " Disjoint " setting splits data from a parallel corpus into two disjoint halves , using the first half of the source -side corpus and the second half of the target - side corpus . ?
In the " Different Domain " ( Diff .
Dom. ) setting , source and target monolingual corpora come from different domains .
This is a realistic setting in low-resource scenarios , and is expected to be much more difficult than the " Disjoint " setting . ?
" News crawl " ( News ) and " Common Crawl " ( CC ) settings determine whether the system can flexibly handle diverse datasets .
Specifics of the datasets used are described in subsequent subsections .
is the condition most similar to ( Artetxe et al. , 2018 b . Src ( M ) and Trg ( M ) columns are the token counts , in millions .
" Supervised " count is in BPE tokens .
All others are token counts for SMT ( pre - BPE ) .
the subsections below are before preprocessing , whereas Table 1 reflects the data remaining after the preprocessing procedure of Artetxe et al . ( 2018 b ) .
We will release the preprocessed data splits for others to compare their results with ours .
United Nations
The United Nations Parallel Corpus ( UN ) ( Ziemski et al. , 2016 )
We additionally extract the first 100 million French and Russian tokens for CC experiments .
Preprocessing Training data is preprocessed separately for each unsupervised experiment as part of Artetxe et al . ( 2018 b ) 's training pipeline .
Data is deduplicated , and tokenized and truecased using scripts from Moses ( Koehn et al. , 2007 ) .
Sentences with less than 3 tokens or more than 80 tokens are discarded , and sentences are shuffled .
Ten thousand sentences are removed to form a development set .
To begin the NMT phase , a joint BPE ( Sennrich et al. , 2016 b ) vocabulary of 32000 tokens is learned .
Source - and target - side corpora are backtranslated using the final model from the SMT phase , and all data then has BPE applied .
8 For supervised experiments , training data is tokenized and truecased , and then a joint BPE ( Sennrich et al. , 2016 b ) vocabulary of 32000 tokens is learned .
After applying BPE , the data is cleaned using Moses ' clean-corpus-n.perl , discarding sentences under 3 and greater than 80 tokens .
Vocabulary Overlap of Training Sets
A vocabulary of unigrams was collected for each target- side ( English ) corpus , which includes tokens that appear at least 10 times , for a maximum of 200,000 unigrams .
Of approximately 144,000 such unique tokens between UN - A and UN - B from the Fr- En UN corpus , the corpora share 54.1 % .
These corpora are used in the Disjoint condition .
The respective vocabulary overlap for UN - A and CC from the Diff .
Dom condition for Fr- En is 25.7 % .
For UN - B vs. CC for Fr-En , they share 25.3 % .
Statistics are analogous for Ru-En .
Test and Validation Sets Ru - En models are tested on newstest2019 .
Fr- En models are tested on newstest2014 .
Supervised models use newstest2018 ( Ru-En ) or new-stest2013 ( Fr-En ) for validation .
For Si-En and Ne-En , we use the Wikipedia dev and devtest sets from Guzm ? n et al . ( 2019 ) . 9
For supervised models , we select the ensemble with best performance on newstest 2017 ( Ru-En ) or newstest 2012 ( Fr-En ) .
7 Reinvestigation of Artetxe et. al. Next , we set up a series of experiments to assess the questions posed in Section 4 .
Results are presented in Tables 3 and 4 .
Unsupervised Quality Loss
The Supervised ( " Sup. " ) column of Table 3 shows performance of a standard Transformer - big architecture on parallel bitext for Ru-En and Fr-En .
Assuming that supervised translation will always outperform unsupervised , these scores represent a ceiling to quantify how much potential quality is lost using an unsupervised architecture .
Sup. Parallel Disjoint Diff. Dom. Corpus A / A A / A A / B A / CC * Ru-En 26.9 23.7 ( - 3.2 ) 21.2 ( - 5.7 ) 0.7 ( - 26.2 ) Fr- En 29.9 27.6 ( - 2.3 ) 27.0 ( - 2.9 ) 3.9 ( - 26.0 )
Table 3 : Unsupervised MT performance on a single run using the United Nations ( UN ) dataset .
" Diff.
Dom. " uses UN data as source and Common Crawl ( * ) as target .
" Sup. " is supervised with UN parallel data .
A / A refers to UN training dataset A used on the source and target sides , for example .
Scored using Sacre-BLEU ( Post , 2018 ) on newstest2019 ( Ru-En ) and new-stest2014 ( Fr-En ) . .
The supervised models and those in the Parallel column use the same datasets 10 and can therefore be directly compared .
We observe a BLEU score drop of ?3.2 for Ru - En versus a drop of ?2.3 for Fr- En when using the unsupervised architecture .
This minor quality loss represents a strong result for unsupervised MT ; however , the question is whether the results will remain strong as we gradually make the monolingual corpora less similar .
Investigating Our Research Questions
Does unsupervised machine translation work for : ( 1 ) Dissimilar language pairs ?
We conduct experiments in French and Russian into English .
Whereas French and English share the same Roman script and common linguistic origin , Russian is a Slavic language that uses the Cyrillic script .
The results presented in Tables 3 and 4 indicate that unsupervised MT is more difficult when writing script and language family differs .
Across the board , we observe that the ?BLEU between supervised and unsupervised performance is wider for Ru - En than for Fr-En , particularly for News and Common Crawl datasets .
For instance , whereas Fr- En loses 2.9 BLEU in the Supervised versus Disjoint setups ( which use comparable data ) , Ru - En loses 5.7 BLEU .
While we acknowledge that in general one should not compare BLEU scores across language pairs or datasets , this gap suggests that unsupervised MT may behave differently for different language pairs .
( 2 ) Dissimilar domains ?
We investigate the effects of domain similarity between source and target training corpora .
For each language , we observe the difference in perfor -10 Differences in token count are due to the different preprocessing detailed in Section 6.4 . mance on Table 3 of the Parallel , Disjoint , and Diff .
Dom. columns .
Because training data in the Parallel condition was originally parallel , these experiments have the highest possible domain match between source and target data .
Since Disjoint data was extracted from the same corpus but was not parallel , source and target can be thought of as having very slightly different domains .
We observe a minor performance drop between Parallel and Disjoint experiments , which is more pronounced for Ru-En .
Examining the Diff.
Dom. column , however , the performance contrast is stark .
While both language pairs obtain respectable BLEU scores in the 20s when domains match in Parallel and Disjoint conditions , performance drops sharply when training set domains are mismatched - scoring 3.9 BLEU for Fr- En and 0.7 for Ru-En .
( A subsequent run of Fr- En scored 17.4 , addressed in Section 7.4 ) .
The fault is not with either side of the training corpus alone - Parallel / Disjoint experiments from Table 3 which use UN data alone and CC experiments in Table 4 which use Common Crawl data alone perform acceptably - it is when the two datasets are paired as source-target in Diff .
Dom. conditions that performance rapidly deteriorates .
( 3 ) Diverse datasets ?
( Post , 2018 ) on newstest2019 ( Ru-En ) , newstest2014 ( Fr-En ) , and the FLoRes Wikipedia evaluation sets ( Si- En , Ne-En ) ( Guzm ? n et al. , 2019 ) . UN
Table 4 shows the results of experiments using three different training datasets .
News crawl matches the domain of the test set exactly .
UN data has a moderate domain match with the test set , and CC matches the least .
Not unexpectedly , most experiments where training and test domain match perform better than when there is a domain mismatch .
The exception is the News experiment for Ru-En , where the model performs considerably worse than the UN condition despite having a stronger domain match .
Notably , News has approx-imately 2 - 3x less data than UN for each language pair .
We suspect that for Fr-En , the relative ease of unsupervised translation for this language pair allowed the strong domain match with the test set to outweigh the lower amount of data .
On the other hand , the relative difficulty of unsupervised MT in Ru - En made the system suffer too greatly in the lower - resource condition , to where it could not compensate with domain match .
( 4 ) A true low-resource pair ?
Facebook recently released test sets for Sinhala-English and Nepali-English , true low-resource language pairs which not only lack bitext , but monolingual data is of poor quality .
These languages do not share a script or language family with English , and the data is out - of- domain with the English data .
This reflects a real-world low-resource scenario where we would hope to benefit from unsupervised MT .
We observe extremely poor results in Table 4 , with Si- En achieving a BLEU score of 0.2 , and 0.4 Ne- En. Guzm ?n et al. ( 2019 ) achieve similarly poor results for these language pairs without using supplemental data from a related language .
BLEU During Training Figure 2 shows translation performance for the experiments in Tables 3 and 4 at various steps during the unsupervised machine translation pipeline .
Most SMT models improve performance slightly as a result of unsupervised MERT tuning , and more substantially after three rounds of iterative backtranslation .
Substantial improvement occurs as a result of NMT training for all models except the degenerate Diff .
Dom conditions .
Training Stability
One challenge with unsupervised methods is training stability : stochasticity during training can give substantially different results due to the iterative bootstrap nature of the training process .
In their analysis of unsupervised methods for generating CLEs , note considerable instability in performance on BLI .
Defining failure as having a mean average precision ( MAP ) of < 0.05 on all training runs , Iterative Closest Point ( Hoshen and Wolf , 2018 ) fails for ?21 % of language pairs , Gromov-Wasserstein Alignment ( Alvarez - Melis and Jaakkola , 2018 ) for ?46 % , and MUSE for ?54 % .
VecMap ( Artetxe et al. , 2018a ) succeeds for all language pairs , leading Glava ?
et al. to deem it the most robust .
Artetxe et al. ( 2018a ) demonstrate their robustness over other methods in their work .
When counting successful runs as achieving > 5.0 % accuracy , VecMap is successful 10/10 times for three language pairs .
Hartmann et al. ( 2019 ) also investigate instability in vector space alignment methods .
After training phrase embeddings for each experiment , we run VecMap on the generated embedding spaces ten additional times and indeed find little fluctuation in BLI between runs .
When rerunning the full pipeline for each experiment , however , we observe considerable instability in two experiments which dramatically affects downstream results .
We build a gold-standard bilingual dictionary of 2000 word pairs from Wikipedia data ( Wo? k and Marasek , 2014 ) available publicly on OPUS ( Tiedemann , 2012 ) , and run the first four steps of the unsupervised training procedure additional times for each experiment .
Table 5 contains the summary results of 10 - 11 runs of each experiment .
Tables 3 and 4 present the results of the single first run of each experiment .
Whereas the majority have consistent accuracy on bilingual lexicon between runs as seen in 5 ) .
The unsupervised pipeline begins with preprocessing ( deterministic , except shuffling and random selection of development set ) , language model training with KenLM ( Heafield , 2011 ) ( deterministic ) , followed by phrase embedding training using phrase2vec ( non- deterministic ) , and then embedding space mapping with VecMap ( nondeterministic ) .
Because performance on reruns of VecMap alone was stable while holding the rest of the system constant , we must conclude that the dramatic instability is caused by either a poor embedding initialization from phrase2vec / word2vec , or VecMap 's inability to handle certain monolingual vector space configurations .
Apparently , the initial formation of monolingual vector spaces dramatically affects VecMap 's ability to converge to a good solution , which in turn results in highly variable downstream translation performance .
To observe the relationship between BLI accuracy and downstream BLEU score , we direct the reader to Figure 3 , where BLI accuracy after the VecMap phase of experiments from Tables 3 and 4 are displayed in relation to the final BLEU score .
Discussion Except in the Diff. Dom. condition , unsupervised MT performance for Fr- En is impressive and suggests that sentence alignment may not be required for successful MT under ideal conditions .
Ru - En results are also impressive , but show that unsupervised MT still struggles when language pairs are dissimilar , especially when data amount is reduced .
The gap between Disjoint and Diff.
Dom. con - ditions is perhaps the most striking result in our experiments .
It suggests that one cannot naively collect monolingual corpora without considering their relative domain similarity ; this may be a challenge in low-resource conditions , where there is less flexibility with data sources .
Vuli ?
et al. ( 2019 ) make a similar claim about unsupervised CLEs , stating " UNSUPERVISED methods are able to yield a good solution only when there is no domain mismatch and for the pair with two most similar languages ( English - Spanish ) , again questioning their robustness and portability to truly low-resource and more challenging setups " .
Furthermore , the extremely poor results of Ne- En and Si- En reflect the reality of low-resource translation ; the compound negative effects of language dissimilarity , domain mismatch between monolingual corpora , domain mismatch with the test set , and low amounts of low-quality data .
It is the " worst of all worlds " but reflects how current models might perform on the use cases for which they are needed .
These challenges highlight the importance of evaluating unsupervised MT under varying realistic data conditions .
Our evaluation is a step towards this goal , and identifies multiple areas for improvement .
A critical step in state - of- the - art unsupervised MT is methods for creating CLEs .
Several authors have pointed out that " mapping " methods like VecMap assume that monolingual vector spaces are structurally similar , but that this " approximate isomorphism assumption " is increasingly tenuous as languages and domains diverge ( S?gaard et al. , 2018 ; Ormazabal et al. , 2019 ; Patra et al. , 2019 ) . Patra et al. ( 2019 ) find this for Fr- En and Ru - En specifically , the languages examined in this work .
Nakashole and Flauger ( 2018 ) argue that while linearity may hold within local " neighborhoods " of the vector space , the global mapping is non-linear .
S?gaard et al. ( 2018 ) use their eigenvector similarity metric to show a strong correlation between vector space similarity and BLI performance .
Analysis of the CLEs from our experiments demonstrate a relationship between BLI performance and downstream BLEU on the translation task .
Coupled with our empirical evidence , the works cited in this section suggest that nonisometric vector spaces lead to poor quality translation .
Factors observed in our experiments that lead to lower quality translation can be attributable to a " weak isomorphism " between the monolingual vector spaces .
Dissimilar languages means increasingly different distributional characteristics of words .
Data from different domains naturally have different word frequencies and distributional characteristics , which become more pronounced as domains diverge .
Because mapping methods rely on structural similarity of vector spaces , experiments using either UN or CC data alone had acceptable downstream performance , where as combining the datasets as source and target resulted in extremely poor translation .
We observe the critical effect of word embedding initialization on BLI performance and downstream BLEU , suggesting that stochasticity during word embedding creation can cause resulting vector spaces to be more or less isomorphic .
Finally , more data can give a more accurate distribution of words in comparison with the true distribution in the language , leading to a more realistic monolingual vector space .
With less data , word embeddings are dependent on the smaller training sample , which may not match the test set or reflect true distributional properties of the language .
Combining all of these negative factors likely leads to highly nonisomorphic monolingual embedding spaces , as demonstrated by the very poor Si-En and Ne - En results .
Conclusion & Future Work Progress in unsupervised MT has been impressive , achieving performance near its supervised counterparts under some scenarios .
That said , evaluating current approaches under broader settings and datasets reveals that unsupervised MT struggles in realistic low-resource scenarios .
As stated by Lample et al . ( 2018 b ) , " It 's an open question whether there are more effective instantiations of these principles [ underlying recent successes in fully unsupervised MT ] or other principles altogether " .
In this work , we find that there is room for improvement to become robust to ( 1 ) dissimilar languages pairs , ( 2 ) dissimilar domains , ( 3 ) diverse datasets , and ( 4 ) the low-quality data of true low-resource languages - factors ubiquitous in low-resource language pairs for which unsupervised MT is intended .
We find that ( a ) performance rapidly declines when source and target corpora are from different domains , and ( b ) stochasticity during word embedding training can dramatically affect downstream translation results .
The latter is a yet unexplored research area .
Future work should also evaluate pretraining methods in bilingual and multilingual training contexts .
Finally , we argue for extensive evaluation of unsupervised MT systems under varying data conditions to assess failure cases and encourage pursuit of promising paradigms .
Doing so is a step towards solving the real-world problems of low-resource machine translation .
as representative of state- ofthe - art for the class of unsupervised MT methods that bootstrap from cross-lingual word embeddings .
Recent work such as Lample et al . ( 2018 b ) is based on similar concepts .
For our purposes , unsupervised MT follows Liu et al . ( 2020 ) 's Definition ( 1 ) from Section 2 , where no bitext exists .
