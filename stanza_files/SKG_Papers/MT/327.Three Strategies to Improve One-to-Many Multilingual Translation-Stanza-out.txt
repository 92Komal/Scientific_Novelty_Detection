title
Three Strategies to Improve One-to - Many Multilingual Translation
abstract
Due to the benefits of model compactness , multilingual translation ( including many - toone , many - to- many and one-to-many ) based on a universal encoder-decoder architecture attracts more and more attention .
However , previous studies show that one - to -many translation based on this framework cannot perform on par with the individually trained models .
In this work , we introduce three strategies to improve one - to -many multilingual translation by balancing the shared and unique features .
Within the architecture of one decoder for all target languages , we first exploit the use of unique initial states for different target languages .
Then , we employ language - dependent positional embeddings .
Finally and especially , we propose to divide the hidden cells of the decoder into shared and language - dependent ones .
The extensive experiments demonstrate that our proposed methods can obtain remarkable improvements over the strong baselines .
Moreover , our strategies can achieve comparable or even better performance than the individually trained translation models .
Introduction Encoder-decoder based neural machine translation ( NMT ) has achieved the new state - of - the - art due to powerful end-to - end modeling ( Sutskever et al. , 2014 ; Bahdanau et al. , 2015 ; Wu et al. , 2016 ; . Under this end-to- end framework , many researchers attempt to improve the translation quality between two languages by exploiting monolingual data ( Sennrich et al. , 2016 ; Zhang and Zong , 2016 ) , taking advantage of both NMT and statistical machine translation ( Wang et al. , 2017a ; Tang et al. , 2016 ; Zhao et al. , 2018 ; Zhou et al. , 2017 ) and so on .
Another research direction about how to perform multilingual translation within this encoderdecoder architecture has recently drawn more and more attention Dong et al. , 2015 ; Luong et al. , 2016 ; Johnson et al. , 2017 ; Firat et al. , 2016 b ) .
In multilingual translation scenarios , one can employ multi-task learning framework to perform many - to - one or one - to -many translation using multiple encoders or multiple decoders ( Luong et al. , 2016 ; Dong et al. , 2015 ) . Firat et al. ( 2016 a ) and Lu et al . ( 2018 ) further propose to share a universal attention mechanism for many - to -many translations .
In these methods , encoder or decoder is language dependent and network parameters increase linearly with the number of languages .
Johnson et al. ( 2017 ) and Ha et al . ( 2016 ) present an appealing approach in which a universal encoder- decoder framework is designed for manyto-one , many - to - many and one - to -many multilingual translation tasks .
The network model is compact and the model size does not grow as the number of languages increases .
However , Johnson et al. ( 2017 ) observe that only the many - toone paradigm can achieve better translation results than the individually trained models .
For the other two paradigms , there are various degrees of quality degradation .
In this work , we focus on one-tomany multilingual translation under the universal encoder-decoder framework and attempt to boost its performance while maintaining the model compactness .
To this end , we propose three strategies which exploit the unique features of each target language and keep as many parameters shared as possible .
First , we design two special labels at the tail of encoder and the head of decoder to mark the target language and guide the generation of different target languages .
Then , we introduce languagedependent positional embeddings into the bottom layer of the decoder network and correspondingly the structural difference between target languages can be well captured .
Finally and especially , we propose a new parameter - sharing mechanism in which we divide the hidden units of each decoder layer into shared and language - dependent ones .
We verify the effectiveness of our proposed methods on two one- to-many tasks : Chineseto-English / Japanese translation and English-to - German / French translation .
The experimental results demonstrate that the three strategies can significantly outperform the baseline multilingual models and they can achieve comparable or even better performance than the individually trained translation models .
Specifically , our contributions in this paper are two -fold : ?
The proposed three strategies can take advantage of unique features of each target language while sharing the network parameters as many as possible .
?
The extensive experiments on multiple translation tasks show that the three proposed strategies improve the translation quality .
Moreover , the effects of the strategies are complementary and the combined one can perform on par with or better than the individually optimized translation models .
Background
Our proposed approach can be applied to any encoder-decoder architecture .
Considering the excellent translation performance of Transformer network ( Vaswani et al. , 2017 ) , we implement our method entirely based on it in this work .
Transformer consists of stacked encoder and decoder layers .
The encoder maps an input sequence x = ( x 1 , x 2 , ? ? ? , x n ) to a sequence of continuous representations z = ( z 1 , z 2 , ? ? ? , z n ) whose size varies with respect to the source sentence length .
The decoder generates an output sequence y = ( y 1 , y 2 , ? ? ? , y m ) from the continuous representations z.
Since the Transformer network contains no recurrence , positional embeddings are used in model to make use of sequence order .
The encoder and decoder are trained to maximize the conditional probability of target sequence given a source sequence : L ( ? ) = N t=1 log P ( y t |y <t , x ; ? ) ( 1 ) For the sake of brevity , we refer the reader to Vaswani et al . ( 2017 ) for more details regarding the architecture .
Method Description
In this section , we introduce our general strategies for extending the transformer network to one-tomany translation task .
We decompose the probability of the target sequences into the products of per token probabilities in all translation forms : L ( ? ) = M t=1 N l l=1 log ( P ( y l t |x , y l <t ; ? ) ) ( 2 ) where M is number of target languages , and P ( y l t |x , y l <t ; ? ) denotes the translation probability of t-th word of the l-th target language .
Note that the translation process for all target languages uses the same parameter set ?.
Our methods mainly concentrate on improving one - to -many multilingual translation by designing new decoder structure under the universal encoder-decoder framework .
The idea is to exploit the shared and unique features of different target languages , and we respectively propose three strategies including special label initialization , language - dependent positional embedding and a new parameter - sharing mechanism .
Special Label Initialization
In the universal encoder-decoder network for oneto-many multilingual translation ( Johnson et al. , 2017 ) , a special token ( e.g. en2 fr ) is added at the end of the source sentence to indicate the translation direction .
Although it is an effective mechanism , we find that the initial states of the decoder are very important to guide the generation process for different target languages .
In order to enhance the model , we utilize another special languagedependent label at the beginning of the decoder and we regard it as the first generated token of the target language ( e.g. 2 fr ) .
Language-dependent Positional Embedding Positional embeddings give the model the sense of which part of the sequence is currently being dealt with .
Intuitively , different target languages should have different positional embeddings to distinguish the structural difference between multiple target languages .
Therefore , we design languagedependent positional embeddings in the universal encoder-decoder multilingual translation .
For the fixed embedding method ( Vaswani et al. , 2017 ) , sine ( x ) and cosine ( x ) functions are used to generate positional embeddings .
In this case , we introduce trigonometric functions with different orders or offsets on the decoder to distinguish different target languages .
For the dynamic embedding method ( Gehring et al. , 2017 ) , we equip the target inputs by embedding the absolute position of different languages separately .
Shared and Language-dependent Hidden Units per Layer
In the universal encoder-decoder multilingual translation , the hidden layers of the decoder are responsible for generating different target language sentences .
As a result , the hidden layers should embody some language - dependent information .
In this work , we propose to divide the hidden units of each decoder layer into shared units and language - dependent ones .
On the one hand , shared units can learn the commonality of languages and enable one - to -many translation to share the network parameters as many as possible .
On the other hand , language - dependent units are capable of capturing the characteristic of each specific language .
Figure 1 gives a brief description of our proposed strategy .
For instance , in training step for one target language ( tar - 1 ) , we tune the shared units and the language - dependent units of tar -1 , and mask out other parts .
In decoding step , we only use the shared and language - dependent hidden units of target language tar -1 to predict translation results .
Experiments Settings
In this section , we test the proposed methods on two one- to -many translation tasks , including ( i ) Chinese ?
English / Japanese in general domain , and ( ii ) English ?
French / German in WMT14 task .
Chinese ?
English / Japanese
For this translation task , the training sets of Chinese-to- English ( briefly , Zh?En ) and Chinese-to - Japanese ( briefly , Zh?Ja ) both contain about 10 million parallel corpora .
We evaluate our methods on NIST03-06 ( MT03 - 06 ) for Zh?
En translation and 400 sentences extracted from our general corpus for Zh?
Ja translation .
English ?
French / German
The training set consists of about 4.5 million bilingual sentence pairs in WMT14 English - German ( briefly , En?De ) task and about 36 million sentence pairs in WMT14 English - French ( briefly , En?Fr ) task 1 . We use the combination of newstest2012 and newstest2013 as our validation set , and we use newstest2014 as our test set on En?De and En? Fr tasks .
We adopt the tensor2tensor 2 library for training and evaluating our basic Transformer translation model .
We use wordpiece method ( Wu et al. , 2016 ; Schuster and Nakajima , 2012 ) to encode source side sentences and the combination of target side sentences .
The vocabulary size is 37,000 for both sides .
We train our models using configuration transformer big adopted by Vaswani et al . ( 2017 ) , which contains a 6 - layer encoder and a 6layer decoder with 1024 - dimensional hidden representations .
During training , each mini-batch on one GPU contains a set of sentence pairs with roughly 3,072 source and 3,072 target tokens .
We use Adam optimizer ( Kingma and Ba , 2014 ) with ? 1 =0.9 , ? 2 =0.98 , and = 10 ?9 .
For our model , we train for 400,000 steps on one machine with 8 NVIDIA Tesla M40 GPUs .
Results and Analysis
We show the results of one- to-many translation experiments using our proposed strategies .
The translation performance is evaluated by case-insensitive BLEU4 for Zh?
En translation , character - level BLEU5 for Zh ?
Ja translation , and case-sensitive BLEU4 ( Papineni et al. , 2002 ) for En?De / Fr translation task .
Our Strategies vs. Baseline Table 1 reports the main translation results of Zh?En / Ja and En?De / Fr translation tasks .
We conduct universal one-to-many translation using Table 1 : Translation performance of our methods on Zh?En / Ja and En?De / Fr tasks .
Indiv means translation model of individual pair .
O2 M is the our baseline system .
1 , 2 and 3 denote our proposed three strategies of special label initialization , language - dependent positional embedding and the new parameter - sharing mechanism separately .
2 ( Dyn ) and 2 ( Fixed ) represent the two ways of language - dependent positional embedding method .
For shared and language - dependent method , we set one - half of hidden units as shared units , and for another half , we use a quarter hidden units to denote two output languages respectively .
Johnson et al. ( 2017 ) method on Transformer framework as our baseline system ( briefly , O2M method ) .
From the first two lines , we can see that the O2M method cannot perform on par with the individually trained systems in most cases .
We mentioned before that our goal is to improve the universal one - to -many multilingual translation framework while maintaining the parameter sharing property .
We can observe from the table that all our proposed strategies ( last part in Table 1 ) improve the performance compared to the baseline ( O2M ) .
Specifically , the combined use of three strategies performs best and it can achieve the improvements up to 1.96 BLEU points ( 45.51 vs. 43.55 on Zh?En MT04 ) .
As for languagedependent positional embedding , we find that both fixed and dynamic styles perform similarly .
Our ultimate goal is to make the universal oneto-many framework as good as or better than the individually trained systems .
Table 1 demonstrates some encouraging results .
It is shown in the table that the universal one - to - many architecture enhanced with our strategies can outperform the individually trained models on three out of four language translations ( Zh?En , Zh?Ja , En?Fr ) .
The results verify the effectiveness of our proposed methods .
Comparison of Shared Unit Size
For the new parameter - sharing mechanism , it is an open question to decide how many hidden units should be shared and how many ones should be language dependent .
To figure out this question , we further conduct an experiment to investigate different settings .
For example , we keep a quarter of the hidden units of each decoder layer as shared and make the left three quarters evenly distributed to different target languages .
Figure 2 reports the results .
We can observe different trends for different language pairs .
On the En?De / Fr translation task , the performance is best when we share one - half of the hidden units .
In contrast , it obtains the best results when we share only 37.5 % of hidden units on Zh?En / Ja translation .
It indicates that similar languages ( De / Fr ) can share more hidden units and languages with a great difference ( En / Ja ) may share less hidden units .
Related Work
In this work , we explore the balancing problem of shared and unique parameters , and attempt to incorporate the language - dependent presentation features to distinguish different target languages under the scenario of one- to -many multilingual translation .
Multilingual translation has been extensively studied in Dong et al . ( 2015 ) , Firat et al. ( 2016 a ) , Luong et al. ( 2016 ) and Johnson et al . ( 2017 ) .
Owing to excellent translation performance and ease of use , many researchers ( Blackwood et al. , 2018 ; Lakew et al. , 2018 ) have conduct translation of multiple languages based on the framework of Johnson et al . ( 2017 ) and Ha et al . ( 2016 ) .
As for low-resource translation scenario Wang et al. , 2017 b ) , similar to above method , Gu et al . ( 2018 ) enable sharing of lexical and sentence representation across multiple languages especially for lowresource multilingual NMT .
Different from previous methods , our work mainly focuses on improving the one-to -many multilingual translation framework while sharing as many parameters as possible .
Conclusion
In this paper , we have proposed three effective strategies to improve the universal one - to -many multilingual translation , including special label initialization , language - dependent positional embedding and a new parameter - sharing mechanism .
The empirical experiments on four language pairs demonstrate that our strategies can obtain significant improvement over the strong baseline , and can achieve comparable or even better results than the individually trained models .
For future work , we plan to extend our strategies on many - to -many multilingual translation scenarios , and explore other effective strategies to balance parameter sharing .
Figure 1 : 1 Figure1 : The hidden units of decoder network .
Blue part represents the shared units , and yellow , green and red parts denote different language - dependent units respectively .
