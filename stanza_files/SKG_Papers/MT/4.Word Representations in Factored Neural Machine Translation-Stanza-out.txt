title
Word Representations in Factored Neural Machine Translation
abstract
Translation into a morphologically rich language requires a large output vocabulary to model various morphological phenomena , which is a challenge for neural machine translation architectures .
To address this issue , the present paper investigates the impact of having two output factors with a system able to generate separately two distinct representations of the target words .
Within this framework , we investigate several word representations that correspond to different distributions of morpho-syntactic information across both factors .
We report experiments for translation from English into two morphologically rich languages , Czech and Latvian , and show the importance of explicitly modeling target morphology .
Introduction Open vocabularies remain a challenge for Neural Machine Translation ( NMT ) ( Cho et al. , 2014 ; Bahdanau et al. , 2015 ) , both for linguistic and computational reasons .
From a linguistic standpoint , morphological variation and lexical productivity cause word forms unseen in training to occur in source texts , which may also require to generate novel target word forms .
Using very large input / output vocabularies partially mitigates these issues , yet may cause serious instability ( when computing embeddings of rare or unseen words ) and complexity issues ( when dealing with large softmax layers ) .
Several proposals have been put forward to address these problems , which are particularly harmful when one language is a morphologically rich *
Both authors have contributed equally to this work .
language ( MRL ) , exhibiting larger token / type ratio than is observed for English .
One strategy is to improve NMT 's internal procedures : for instance by using a structured output layer ( Mnih and Hinton , 2008 ) or by altering the training or decoding criteria ( Jean et al. , 2015 ) .
An alternative approach is to work with representations designed to remove some variations via source -side or target - side normalization procedures ; or more radically to consider character - based representations ( Ling et al. , 2015 ; Luong and Manning , 2016 ; Costa-juss ? and Fonollosa , 2016 ) , which are however much more costly to train , and make long distance dependencies even longer .
None has however been as successful as the recent proposal of Sennrich et al . ( 2016 b ) which seems to achieve a right balance between a limited vocabulary size and an ability to translate a fully open vocabulary .
In a nutshell , this approach decomposes source and target tokens into smaller units of variable length ( using what is now termed as a " Byte Pair Encoding " or BPE in short ) : this means that ( a ) all source tokens can be represented as a sequence of such units , which crucially are all seen in training ; ( b ) all possible target words can also be generated ; ( c ) the size of the output layer can be set to remain within tractable limits ; ( d ) most frequent words are kept as BPE units , which preserves the locality of many dependencies .
In this work , we consider possible ways to extend this approach by also supplying target -side linguistic information in order to help the system generate correct target word forms .
Our proposal relies on two distinct components ( a ) linguistically or data-driven normalization procedures manipulating various source and target word segmentations , as well as eg. multiple factors on the target side ( see ? 4 ) , and ( b ) a neural architecture equipped with a dual output layer to predict the target in two simpler tasks generating the lexi-cal unit and the morphological information ( ? 3 ) .
These components are assessed separately and in conjunction using translation from English into two MRLs : Czech and Latvian .
Our experiments show improvement over a strong ( Denkowski and Neubig , 2017 ) BPE - to-BPE baseline , incorporating ensemble of models and backtranslated data ( ? 5 ) .
Overall , they suggest that BPE representations , which loosely simulates concatenative morphological processes , is complementary to feature - based morphological representations .
Related Work Translating from and into MRLs has recently attracted some attention from the research community , as these languages compound a number of difficulties for automatic translation , such as the need to analyze or generate word forms unseen in training , or to handle variation in word order .
To mitigate the unknown word problem , a first approach consists in translating into target stems ( Minkov et al. , 2007 ; Toutanova et al. , 2008 ) ; the right form is then selected from the full paradigms in a second step using a classifier .
Target words may also be represented as lemmas complemented with side information .
Bojar ( 2007 ) ; Bojar and Kos ( 2010 ) ; Bojar et al. ( 2012 ) use such a representation for two statistical MT systems : the first one translates from English into Czech lemmas decorated with source-side information and the second one performs a monotone translation into fully inflected Czech .
Fraser et al. ( 2012 ) propose a target morphology normalization for German words represented as lemmas followed by a sequence of morphological tags and introduce a linguistically motivated selection of these when translating from English .
The selection step consists in predicting the tags that have been removed during normalization , using a specific Conditional Random Field ( CRF ) model for each morphological attribute to predict .
Finally , word forms are produced via look - up in a morphological dictionary .
This approach is extended by Weller et al . ( 2013 ) , who takes verbal subcategorization frames into account , thus enabling the CRFs to make better predictions .
Note that Burlot et al . ( 2016 ) and El Kholy and Habash ( 2012 b , a ) propose related approaches respectively for translating into Czech and Arabic .
Factored word representations have also been considered in neural language models ( Niehues et al. , 2016 ; Alexandrescu and Kirchhoff , 2006 ; Wu et al. , 2012 ) , and more recently in a neural machine translation architecture as input features ( Sennrich and Haddow , 2016 ) and in the output by separating the lemma and morphological factors ( Garc? a- Mart ? nez et al. , 2016 ) .
One contribution of the current paper is the investigation of new variants of the latter architecture .
There have been other attempts with dual training objectives in NMT .
In ( Chen et al. , 2016 ) , a guided alignment training using topic information of the sentence as a second objective helps the decoder to improve the translation .
Multi-task and multilingual learning in NMT have also been considered in several papers ( Luong et al. , 2015 ; Dong et al. , 2015 ; , where training batches have to carefully balance tasks and language pairs .
In contrast to these approaches , our factored NMT ( FNMT ) system produces several outputs simultaneously .
Model Architectures
The baseline NMT system used in this paper is an implementation of a standard NMT model with attention mechanism ( Bahdanau et al. , 2015 ) .
It consists of a sequence to sequence encoderdecoder of two recurrent neural networks ( RNN ) , one used by the encoder and the other by the decoder .
This architecture integrates a bidirectional RNN encoder ( see bottom left part with green background of Figure 1 ) .
Each input sentence word x i ( i ? 1 . . . N with N the source sequence length ) is encoded into an annotation a i by concatenating the hidden states of a forward and a backward RNN .
Each annotation a 1 . . . a N thus represents the whole sentence with a focus on the word ( s ) being processed .
The decoder is based on a conditional gated recurrent unit ( GRU ) made of two GRUs interleaved with the attention mechanism .
The attention mechanism computes a context vector C j as a convex combination of annotation vectors , where the weights of each annotation are computed locally using a feed-forward network .
The decoder RNN takes as input the embedding of the previous output word in the first GRU , the context vector C j in the second GRU and its hidden state .
The softmax output layer is connected to the network through a non-linear layer which takes as input the embedding of the previous output word as well as the context vector and the output of the decoder from the second GRU ( both adapted with a linear trans-formation , respectively , L C and L R ) .
Finally , the output probabilities for each word in the target vocabulary are computed with a softmax .
The word with the highest probability is the translation output at each time step .
The encoder and the decoder are trained jointly to maximize the conditional probability of the reference translation .
The Factored NMT system of Garc?a- Mart ? nez et al. ( 2016 ) is an extension of the standard NMT architecture that allows the system to generate several output symbols at the same time , as presented in Figure 1 . DECODER ENCODER Attention Mechanism x1 x2 xN ? ? ? x1 x2 xN ? ? ? ? ? ? Annotations Bidirectional RNN GRU1 GRU2 a1 aN
The encoder and the attention mechanism of the Factored NMT are the same as the standard NMT model .
However , the decoder has been modified to produce multiple outputs .
The two outputs are constrained to have the same length .
The decoder feedback is also modified to use information from the multiple output streams .
The concatenation of the embeddings of the pair of generated symbols is used to feed the decoder 's cGRU at each timestep .
Two types of FNMT models have been used for this work .
Their architecture differ after the generation of the decoder state .
The first model contains a single hidden- to- output ( h2o ) layer which is used by the two separate softmax .
This layer uses the context vector , the decoder 's hidden state and the concatenation of the embeddings of the previous generated tokens .
The second model is one contribution of the current work .
As shown in Figure 1 ) , it contains two separated h2o layers .
They are similar to the h2o layer in the first model except that instead of using the concatenation of the embeddings of the previously generated factors , each h2o layer receives only the embedding of the factor it is generating .
The two separated h2o layers allow the system to have more weights specialized for each output .
Word Representations
This paper focuses on the question of word representations , which we understand not only in terms of word segmentation , but also as the quantity of morpho-syntactic information encoded in a word .
We introduce three representations varying in the quantity of grammatical information they contain : ? fully inflected words : this is a baseline setup where all the lexical and grammatical information is encoded in a single factor .
? normalized words : only a well chosen subset of morphological features is kept in the first factor ; the second factor corresponds to the Part of Speech ( PoS ) .
? lemmas : the output splits the lexical content of the word ( first factor : lemma ) and its grammatical content ( second factor : PoS ) .
These differences are illustrated in Table 1 .
Normalizing Word Forms
Translating from English into a MRL is made difficult by linguistic divergences , as English lacks many of the morphological contrasts that exist in the MRL .
Normalization is needed to reduce the morphological variability on the MRL side so as to limit the number of types in the target , and to mitigate sparsity issues .
This strategy is used for instance by Burlot et al . ( 2016 ) who remove the case mark from Czech nouns , which is not predictable from their English counterpart ( s ) .
Normalization is usually performed using handcrafted rules and requires expert knowledge for each language pair .
In this paper , normalized words are obtained with an automatic data-driven method 1 introduced in ( Burlot and Yvon , 2017 b ) .
In a nutshell , this method performs a clustering of the MRL vocabulary by grouping together words that tend to share the same translation ( s ) in English .
This translational similarity is based on the conditional entropy of lexical translation models estimated , for each MRL word form , using automatic word alignments .
The clustering procedure merges two words whenever the resulting cluster does not increase the conditional entropy , which ensures a minimal loss of information during the whole process .
The actual normalization algorithm is delexicalized and operates at the level of PoS .
Each word is represented as a lemma , a coarse PoS and a sequence of morphological tags ( e.g. ko?ka+ Noun + Sing + Accusative ) .
Translational similarities are computed on such words and are combined to provide a PoS - level similarity between two tag sequences .
Successive merge operations group into one cluster different such tag sequences .
As a result of this procedure , we represent words as a lemma and a cluster identificator ( ID ) taking the form of a coarse PoS and an arbitrary integer , such as ko ?ka+ Noun +7 in Table 1 .
In this example , the cluster ID Noun +7 stands for a set of fine- grained PoS , such as { Sing + Nominative , Sing + Accusative , . . . }.
This representation introduces a direct correspondence between the first and the second factor in our architecture , since the former ( the cluster ID ) constraints the set of possible values of the latter ( the fine- grained PoS ) , which is notably used in our constrained decoding procedure ( ? 5.4 ) .
Word Representation Setup
The example of Table 1 shows that words are also varying along a second dimension : in addition to considering unsegmented lexical units ( be it fully inflected words , normalized words or lemmas ) , we also investigate the impact of a segmentation of these units using BPE ( Sennrich et al. , 2016 b ) .
In this scenario , BPE segmentation is performed on fully inflected words and lemmas .
For its application to normalized words , the cluster ID was considered as a minimal unit that cannot be segmented ( just like any other character ) , in order to avoid segmentations like ko?ka+ No - un +7 .
For these setups , the PoS information ( second factor ) is replicated for all subparts of a word .
We finally use an alternative representation with normalized words to which BPE segmentation is applied and cluster IDs are systematically split from the lemma .
Whenever the FNMT system predicts a lemma in the first factor , it is forced to predict a null PoS in the second factor .
On the other hand , when a split cluster ID is predicted , the second factor should output an actual PoS .
This specific treatment of the second factor is expected to give the system a better ability to map a word to a compatible PoS , thus avoiding , for instance , the prediction of a verbal PoS for the Czech noun ko ?ka ( cat ) .
These different word representations imply a progressive reduction of the target vocabulary .
We computed the vocabulary size of Czech on the parallel data used to train the systems ( ? 5.1 ) over unsegmented words .
We thus have 2.1 M fully inflected words , 1.9 M normalized words , 1.5 M normalized words with split clusters ( lemmas and clusters ) , and 1.4 M lemmas .
Experiments
We introduce here the experimental setup for all the reported systems translating from English into Czech and Latvian .
Data and Preprocessing Our experimental setting follows the guidelines of the WMT '17 2 news translation task .
The preprocessing of English data relies on in-house tools ( D?chelotte et al. , 2008 ) .
All the Czech data were tokenized and truecased the Moses toolkit ( Koehn et al. , 2007 ) .
PoS-tagging was performed with Morphodita ( Strakov ?
et al. , 2014 ) .
The pre-processing of Latvian was provided by Tilde .
3 Latvian PoS-tags were obtained with the LU MII Tagger ( Paikens et al. , 2013 ) .
For English - to - Czech , the parallel data used consisted in nearly 20 M sentences from a subset of WMT data relevant to the news domain :
Newscommentary , Europarl and specific categories of the Czeng corpus ( news , paraweb , EU , fiction ) .
Newstest - 2015 was used for validation and the systems are tested on Newstest - 2016 and 2017 .
The normalization of the Czech data was trained on the parallel data used to train the MT systems , except Czeng fiction and paraweb subcorpora , which amounts to over 10 M sentences .
A part of these systems was also trained on synthetic parallel data ( Sennrich et al. , 2016a ) ( see ? 6 ) .
The Czech monolingual corpus News - 2016 was backtranslated to English using the single best system provided by the University of Edinburgh from WMT '16 .
4
In order to prevent learning from being too biased towards the synthetic source of this set , we used initial bitext parallel data as well .
We added five copies 5 of News-commentary and the news subcorpus from Czeng , as well as 5 M sentences from the Czeng EU corpus randomly selected after running modified Moore - Lewis filtering with XenC ( Rousseau , 2013 ) .
The English-to - Latvian systems used all the parallel data provided at WMT '17 .
The DCEP corpus was filtered with the Microsoft sentence aligner 6 and using modified Moore-Lewis .
We kept the best 1 M sentences , which led to a total of almost 2 M parallel sentences .
The systems were validated on 2 k sentences held out from the LETA corpus and we report results on Newsdev -2017 and newstest - 2017 .
The normalization of Latvian data was trained on the same parallel sentences used to train the MT systems .
Training was carried out for a part of these systems on synthetic parallel data .
We used a backtranslation of the monolingual corpora news - 2015 and 2016 provided by the University of Edinburgh ( Moses system ) .
To these corpora were added 10 copies of the LETA corpus , as well as 2 copies of Europarl and Rapid .
Bilingual BPE models for each language pair and system setup were learned on the bitext parallel data .
90 k merge operations were performed to obtain the final vocabularies .
For ( F ) NMT models , the vocabulary size of the second factors is only 1.5 k for Czech and 376 for Latvian .
The number of parameters in ( F ) NMT systems increases around 2.5 % for Czech and 7 % in Latvian .
System Setup
Only sentences with a maximum length of 50 were kept in the training data , except for the setup where cluster IDs were split in normalized words .
In this case , we set the maximum length to 100 .
For the training of all models , we used NMTPY , a Python toolkit based on Theano ( Caglayan et al. , 2017 ) and available as free software 7 .
We used the standard NMT system on fully inflected words and the FNMT architecture described in ?
3 on all other word representations .
All systems ( F ) NMT systems have an embedding dimension of 512 and hidden states of dimension 1024 for both the encoder and the decoder .
Dropout is enabled on source embeddings , context vector , as well as output layers .
When training starts , all parameters are initialized with Xavier ( Glorot and Bengio , 2010 ) .
In order to slightly speed up the training on the actual parallel data , the learning rate was set to 0.0004 , patience to 30 with validation every 20 k updates .
On the synthetic data , we finally set the learning rate to 0.0001 and performed validation every 5 k updates .
These systems were tuned with Adam optimizer ( Kingma and Ba , 2014 ) and have been training for approximately 1 month .
Reinflection
The factored systems predict at each time step a lexical unit and a PoS-tag , which requires a nontrivial additional step producing sentences in a fully inflected language .
We refer to this process as reinflection .
Given a lexical unit and a PoS-tag , word forms are retrieved with a dictionary look - up .
In the context of MRL , deterministic mappings from a lemma and a PoS to a form are very rare .
Instead , the dictionary often contains several word forms corresponding to the same lexical unit and morphological analysis .
A first way to solve this ambiguity is to simply compute unigram frequencies of each word form , which was done over all the monolingual data available at WMT '17 for both Czech and Latvian .
During a dictionary look - up , ambiguities can then be solved by taking the most frequent word form .
The downside of this procedure is that it ignores important information given by the target monolingual context .
For instance , the Czech preposition s ( with ) will have different forms according to the right- side context : s tebou ( with you ) , but se mnou ( with me ) .
A solution is to let an inflected - word - based system select the correct word form from the dictionary .
To this end , k- best hypotheses from the dictionary are generated .
Given a sentence containing lemmas and PoS , we perform a beam search going through each word and keeping at each step the k-best reinflection hypotheses according to the unigram model mentioned above .
For Czech reinflection , we used the Morphodita generator ( Strakov ?
et al. , 2014 ) .
Since we had no such tool for Latvian , all monolingual data available at WMT '17 were automatically tagged using the LU MII Tagger ( Paikens et al. , 2013 ) and we gathered the result in a look - up table .
As one could expect , we obtained a large table ( nearly 2.5 M forms ) in which we observed a lot of noise .
Constrained Decoding
The factored system described in ?
3 outputs a lexical unit and a PoS - tag at each time step .
A peculiarity of this system is that the predictions of both factors are independent .
There is only a weak dependency due to the fact that both share the same decoder state and context vector .
As a consequence , the best hypothesis for the first factor can well be incompatible with the best hypothesis for the second factor , and the risks of such mismatches only get worse when top -n hypotheses are considered , as in beam search .
Our constrained decoding procedure aims at enforcing a strong consistency between factors .
Each word in the target vocabulary is first associated with a specific set of PoS-tags .
The decoding procedure is modified as follows : for each candidate target word , we only retain the compatible PoS tags , and select the top -n hypotheses to be kept in the beam from this filtered list .
This constraint ensures that the beam search does not evaluate incompatible pairs of factors .
( e.g. the PoS Preposition and the word cat ) .
With a dictionary , creating such a mapping is trivial for full lemmas , but less obvious in the case of BPE units .
Since the latter can be generated from different words having different grammatical classes , the size of the set of possible PoS can grow quickly .
For normalized words , things are much easier and do not even require a dictionary , as the mapping between cluster IDs and compatible PoS is learnt during the normalization process ( see ? 4.1 ) .
Thus constrained decoding was only performed for ( a ) unsegmented lemmas , and ( b ) unsegmented and segmented normalized words .
Automatic Evaluation Results are reported using the following automatic metrics : BLEU ( Papineni et al. , 2002 ) , BEER ( Stanojevi ? and Sima'an , 2014 ) which tunes a large number of features to maximize the human ranking correlation at sentence level and Charac-TER ( Wang et al. , 2016 ) , a character - level version of TER which has shown a high correlation with human rankings ( Bojar et al. , 2016 ) .
Each score on fully inflected word systems is averaged from two independent runs ( for both single and ensembled models ) .
Experiments with Bitext
The results using the bitext provided at the WMT '17 the evaluation campaign are presented in Table 2 for English - to - Czech 8 and in Table 3 for English -to - Latvian .
We can observe that using the constrained decoding consistently improves the results , except when using split clusters .
In this last case , the system is forced to predict a PoS in the second factor whenever it has generated a cluster ID in the first factor .
Since there is a reduced quantity of such cluster IDs , the model has no difficulty to learn the constraints by itself and therefore to map a cluster ID exclusively to a specific subset of PoS .
In the Latvian lemma setup , we observe that the improvement using constrained decoding is lower than for Czech ( see Table 3 ) , which is probably due to the quality of the noisy look - up table we have created for Latvian ( see ? 5.1 ) .
Note that we have no such dependency on the lexical resources at decoding time for the normalized word setups , where improvements are comparable across both language pairs .
The systems using BPE tokens significantly outperform word- level systems , which confirms the analysis of Sennrich et al . ( 2016 b ) .
The results show that BPE units are even more efficient when applied to normalized words , providing significant improvements over segmented inflected words of 1.79 and 1.85 BLEU points for Czech , and 0.78 and 1.06 for Latvian .
The lemma representation was tested with the two FNMT models presented in ?
3 , one model using a single hidden - to- output layer ( single h2o layer ) and the other model using two separated hidden - to - output layers ( separated h2o layers ) .
We observe mixed results , here : the system with the single h2o layer has slightly better results for the word- to - word systems , but the BPE - to - BPE factored lemma system obtains better performance with the separated h2o layers architecture .
For that reason , we decided to only use the separated h2o layers architecture for the next set of experiments involving synthetic data which is the aim of the next section .
Experiments with Selected Bitext and Synthetic Data Table 4 and 5 show the results of using selected parts of bitext and synthetic parallel data ( see section 5.1 ) for both language pairs .
Each model trained with a selection of bitext and synthetic data was initialized with the parameters of its counterpart trained on bitext .
The BPE vocabulary used was the same as in the model used for initialization , which led the systems to generate unknown words .
In our experiments , we forced the decoder to avoid unknown token generation .
By using synthetic data , we are able to obtain a large improvement for all systems , which is in line with ( Sennrich et al. , 2016 a ) .
We notice that the contrasts present in the previous section between the various word representations are less clear now .
The baseline system ( first two rows ) is the system which benefits the most from the additional data with + 5.7 and + 6.9 BLEU for Czech and Latvian .
The performance of factored systems has also increased , but to a lesser extent , leading to slightly worse results compared to the baseline system .
This situation changes when the reinflected hypotheses are rescored .
We are then able to surpass the baseline system with normalized words .
The two language pairs react differently to kbest hypotheses rescoring ( + k- best rescored in the tables ) .
For Czech , this has nearly no impact on translation quality according to the metrics , whereas it provides an important improvement in Latvian : + 2.03 and + 0.84 BLEU in the split cluster setup .
Note that this specific setup gives the best score we could achieve on newsdev - 2017 , without n-best rescoring or model ensembling .
We interpret this situation as a result of the difference in quality observed for the Czech and Latvian dictionaries used for reinflection .
Indeed , since Morphodita contains exclusively useful Czech reinflection candidates , a simple unigram model is sufficient to select the right word forms , making the generation of 10 - best reinflection hypotheses useless .
9
On the other hand , the hypotheses returned by the look - up table we have used to generate Latvian word forms were noisy and required a rescoring from an MT system based on fully inflected words .
10
We obtained the best results for this Latvian setup by generating the 100 - best reinflection hypotheses , which provides less dependency on the quality of the dictionary and relies more on the knowledge learned by a word-formaware system .
Despite the fact that such a rescoring procedure is costly in terms of computational time , we observe that it can be a helpful solution when no resources of quality are available .
Czech n-best reinflection , as opposed to kbest , turned out to be efficient , bringing the lemma-based systems to the level of the baselines and even above for the normalized word setups .
Whereas it does not improve with Latvian normalized words , we observe a positive impact on the lemma-based systems .
We assume that rescoring the n-best list is a way to rely on an inflectedword - based system to make important decisions related to translation , as opposed to the much simpler monolingual process of reinflection mentioned above .
Latvian split-cluster models seem to have nothing to learn from such systems .
Factored norm performs best among all the presented models , showing consistent BLEU improvements over the baselines of 0.25 and 0.56 for Czech , and 0.57 and 0.89 for Latvian .
We finally notice that ensembling two models slightly reduces those contrasts , and lemma-based systems are the ones that benefit the most from model ensembling .
Conclusions are not easy to draw , since across the different setups , the level of indepen -
It is important to note that automatic metrics may not do justice to the lexical and grammatical choices made by the factored systems .
In an attempt to focus on the grammaticality of the FNMT systems , we conducted a qualitative analysis of the outputs .
7 Qualitative Evaluation
Attention in Factored Systems
In a factored NMT setup , the attention mechanism distributes weights across all positions in the input sentence in order to make two predictions , one for each factor , which is an important difference from single-objective NMT .
An illustration of the impact of this difference is shown in Figure 2 for the ensembles of two English - to - Czech models introduced in ?
6 . In this sentence , the system based on fully inflected words ( translation on the top ) erroneously predicts the verbal present tense in nevyh ?b? ( does not avoid ) .
We can see that the target subword unit nevy@@ is rather strongly linked to the source did n't , which allowed the system to correctly predict negative polarity .
On the other hand , the ending of the verb ? is not linked by attention to this same source word , from which the morphological feature of past should have been conveyed .
We observe in ( a ) that the lemma-based system attention aligns the target position to both the source auxil-iary did n't and the lexical verb 's first subword unit shir@@ , which enables the successful prediction of the right lemma and morphology , i.e. negation ( N ) and past ( R ) .
The normalized word based system in ( b ) shows an even more explicit modelization of this morphological phenomenon .
While the lemma nevyh ? bat@@ is strongly related to the same English segment shir@@ , it is only slightly linked to the English auxiliary .
did n't is instead clearly associated to the cluster ID V+20 that gathers negative past tense PoS-tags , enabling the right prediction in the second factor .
In this last setup , the system has to deal , at each time step in the output sentence , with either a lexical phenomenon or a grammatical one .
Target - side grammatical phenomena being more explicitly modeled in factored NMT , it is generally easier for the attention mechanism to spot an English grammatical word ( auxiliary , preposition , negative particle , etc. ) , which enables a better prediction in the second factor output .
We assume that this peculiarity ensures a better source - to- target grammatical adequacy .
Measuring Morphological Consistency
We provide here an attempt to understand more systematically whether an a priori intuition of factored NMT systems is verified .
The intuition is that dividing the task of translating a sentence into two easier joint tasks , namely the prediction of a lexical unit and of a set of morphological features , should encourage the system to produce a higher level of grammaticality .
To this end , we have used a part of the test suite ( a ) ( b ) a nevy@@ h?b@@ ? se jim , and did n't shir@@ k in getting and did n't shir@@ k in getting a vyh ?
bat se ten , aby J^----------VpYS---XR-NA
P7-X4-------PDZS3-------Z:----------J,----------a nevy@@ h?b@@ ? se jim , and did n't shir@@ k in getting and did n't shir@@ k in getting a@@ + J+ 3 vyh?bat@@ + V+20 se@@ + P+65 on@@ + P+71 , @@ + Z+0 aby@@ + J+4 --J^------------VpMP---XR-NA
--P7-X4---------PPXP3--3------Z:------------J,---------- provided by Burlot and Yvon ( 2017a ) , who propose an evaluation of the morphological competence of a machine translation system performed on an automatically produced test suite .
For each source test sentence from a monolingual corpus ( the base ) , several variants are generated , containing exactly one difference with the base , and focusing on a specific target lexeme of the base .
We took the part of the test labeled as " C-set " that focuses on a word in the base sentence and produces variants containing synonyms and antonyms of this word .
Thus the consistency of morphological choices is tested over lexical variation ( eg. synonyms and antonyms all having the same tense ) and the success is measured based on the average normalized entropy of morphological features in the set of target sentences .
The systems used are the ensembles of two models introduced in ?
6 ( the inflected word system is our best system for each language pair ) .
The results of this procedure are shown in Table 6 .
Entropy demonstrates how confident a system is wrt .
a specific morphological feature across synonyms and antonyms .
While NMT systems on fully inflected words are well -known to produce fluent outputs , we always observe a lower entropy with the factored systems over all features , except for the lemma-based system on Czech verbs .
This tends to show that the prediction of any morphological feature is more confident when it is explicitly modeled by a separate objective focused on morphology , disregarding lexical variations .
Conclusion
In this paper , we have presented various models integrating factored word representations for neural machine translation systems .
Additionally to results with automatic metrics reporting significant improvements over a strong baseline , we provided a qualitative analysis focusing on the grammatical competence of FNTM systems that showed the benefits of explicitly modeling morpho-syntactic information .
Our experiments have shown that the cluster ID from the morphological normalization of target words brings useful information to the system by enabling a better correspondence of both factors ' predictions .
This specificity , as well as the improvements given by constrained decoding , brings us to future work focusing on the modelization of a stronger dependency of the second factor towards the first one in the FNMT architecture .
Figure 1 : 1 Figure 1 : Factored NMT system .
