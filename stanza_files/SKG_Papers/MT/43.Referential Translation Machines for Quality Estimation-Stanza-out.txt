title
Referential Translation Machines for Quality Estimation
abstract
We introduce referential translation machines ( RTM ) for quality estimation of translation outputs .
RTMs are a computational model for identifying the translation acts between any two data sets with respect to a reference corpus selected in the same domain , which can be used for estimating the quality of translation outputs , judging the semantic similarity between text , and evaluating the quality of student answers .
RTMs achieve top performance in automatic , accurate , and language independent prediction of sentence - level and word - level statistical machine translation ( SMT ) quality .
RTMs remove the need to access any SMT system specific information or prior knowledge of the training data or models used when generating the translations .
We develop novel techniques for solving all subtasks in the WMT13 quality estimation ( QE ) task ( QET 2013 ) based on individual RTM models .
Our results achieve improvements over last year 's QE task results ( QET 2012 ) , as well as our previous results , provide new features and techniques for QE , and rank 1st or 2nd in all of the subtasks .
Introduction Quality Estimation Task ( QET ) ( Callison - Burch et al. , 2012 ; Callison - Burch et al. , 2013 ) aims to develop quality indicators for translations and predictors without access to the references .
Prediction of translation quality is important because the expected translation performance can help in estimating the effort required for correcting the translations during post-editing by human translators .
Bicici et al. ( 2013 ) develop the Machine Translation Performance Predictor ( MTPP ) , a state- ofthe- art , language independent , and SMT system extrinsic machine translation performance predictor , which achieves better performance than the competitive QET baseline system ( Callison - Burch et al. , 2012 ) by just looking at the test source sentences and becomes the 2nd overall after also looking at the translation outputs in QET 2012 .
In this work , we introduce referential translation machines ( RTM ) for quality estimation of translation outputs , which is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain .
RTMs reduce our dependence on any task dependent resource .
In particular , we do not use the baseline software or the SMT resources provided with the QET 2013 challenge .
We believe having access to glass - box features such as the phrase table or the n-best lists is not realistic especially for use-cases where translations may be provided by different MT vendors ( not necessarily from SMT products ) or by human translators .
Even the prior knowledge of the training corpora used for building the SMT models or any other model used when generating the translations diverges from the goal of independent and unbiased prediction of translation quality .
Our results show that we do not need to use any SMT system dependent information to achieve the top performance when predicting translation output quality .
Referential Translation Machine ( RTM ) Referential translation machines ( RTMs ) provide a computational model for quality and semantic similarity judgments using retrieval of relevant training data ( Bic ?ici and Yuret , 2011a ; Bic ?ici , 2011 ) as interpretants for reaching shared semantics ( Bic ?ici , 2008 ) . RTMs achieve very good performance in judging the semantic similarity of sentences ( Bic ?ici and van Genabith , 2013a ) and we can also use RTMs to automatically assess the correctness of student answers to obtain better results ( Bic ?ici and van Genabith , 2013 b ) than the state - of- the- art ( Dzikovska et al. , 2012 ) . RTM is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain .
RTM can be used for predicting the quality of translation outputs .
An RTM model is based on the selection of common training data relevant and close to both the training set and the test set of the task where the selected relevant set of instances are called the interpretants .
Interpretants allow shared semantics to be possible by behaving as a reference point for similarity judgments and providing the context .
In semiotics , an interpretant I interprets the signs used to refer to the real objects ( Bic ?ici , 2008 ) . RTMs provide a model for computational semantics using interpretants as a reference according to which semantic judgments with translation acts are made .
Each RTM model is a data translation model between the instances in the training set and the test set .
We use the FDA ( Feature Decay Algorithms ) instance selection model for selecting the interpretants ( Bic ?ici and Yuret , 2011a ) from a given corpus , which can be monolingual when modeling paraphrasing acts , in which case the MTPP model ( Section 2.1 ) is built using the interpretants themselves as both the source and the target side of the parallel corpus .
RTMs map the training and test data to a space where translation acts can be identified .
We view that acts of translation are ubiquitously used during communication :
Every act of communication is an act of translation ( Bliss , 2012 ) .
Step 2 selects the interpretants , I , relevant to the instances in the combined training and test data .
Steps 3 and 4 use I to map train and test to a new space where similarities between translation acts can be derived more easily .
Step 5 trains a learning model M over the training features , F train , and Step 6 obtains the predictions .
RTM relies on the representativeness of I as a medium for building translation models for translating between train and test .
Our encouraging results in the QET challenge provides a greater understanding of the acts of translation we ubiquitously use when communicating and how they can be used to predict the performance of translation , judging the semantic similarity between text , and evaluating the quality of student answers .
RTM and MTPP models are not data or language specific and their modeling power and good performance are applicable across different domains and tasks .
RTM expands the applicability of MTPP by making it feasible when making monolingual quality and similarity judgments and it enhances the computational scalability by building models over smaller but more relevant training data as interpretants .
The Machine Translation Performance Predictor ( MTPP )
In machine translation ( MT ) , pairs of source and target sentences are used for training statistical MT ( SMT ) models .
SMT system performance is affected by the amount of training data used as well as the closeness of the test set to the training set .
MTPP is a state- of - theart and top performing machine translation performance predictor , which uses machine learning models over features measuring how well the test set matches the training set to predict the quality of a translation without using a reference translation .
MTPP measures the coverage of individual test sentence features and syntactic structures found in the training set and derives feature functions measuring the closeness of test sentences to the available training data , the difficulty of translating the sentence , and the presence of acts of translation for data transformation .
MTPP Features for Translation Acts MTPP uses n-gram features defined over text or common cover link ( CCL ) ( Seginer , 2007 ) structures as the basic units of information over which similarity calculations are made .
Unsupervised parsing with CCL extracts links from base words to head words , resulting in structures representing the grammatical information instantiated in the training and test data .
Feature functions use statistics involving the training set and the test sentences to determine their closeness .
Since they are language independent , MTPP allows quality estimation to be performed extrinsically .
We extend MTPP in its learning module , the features included , and their representations .
Categories for the 308 features ( S for source , T for target ) used are listed below where the number of features are given in {# } and the detailed descriptions for some of the features are presented in . ? Coverage { 110 } : Measures the degree to which the test features are found in the training set for both S ( { 56 } ) and T ( { 54 } ) .
I , ( Brown et al. , 1993 ) . ? IBM2 Alignment Features { 11 } : Calculates the sum of the entropy of the distribution of alignment probabilities for S ( s?S ?p log p for p = p( t | s ) where s and t are tokens ) and T , their average for S and T , the number of entries with p ? 0.2 and p ? 0.01 , the entropy of the word alignment between S and T and its average , and word alignment log probability and its value in terms of bits per word .
? Minimum Bayes Retrieval Risk { 4 } : Calculates the translation probability for the translation having the minimum Bayes risk among the retrieved training instances .
? Sentence Translation Performance { 3 } : Calculates translation scores obtained according to q(T , R ) using BLEU ( Papineni et al. , 2002 ) , NIST ( Doddington , 2002 ) , or F 1 ( Bic ?ici and Yuret , 2011 b ) for q. ?
Character n-grams { 4 } : Calculates cosine between character n-grams ( for n=2,3,4,5 ) obtained for S and T ( B?r et al. , 2012 ) . ? LIX { 2 } : Calculates the LIX readability score ( Wikipedia , 2013 ; Bj?rnsson , 1968 ) for S and T. 1 For retrieval closeness , we use FDA instead of dice for sentence selection .
We also improve FDA 's instance selection score by scaling with the length of the sentence ( Bic ?ici and Yuret , 2011a ) .
IBM2 alignments and their probabilities are obtained by first obtaining IBM1 alignments and probabilities , which become the starting point for the IBM2 model .
Both models are trained for 25 to 75 iterations or until convergence .
Quality Estimation Task Results
We participate in all of the four challenges of the quality estimation task ( QET ) ( Callison - Burch et al. , 2013 ) , which include English to Spanish ( en-es ) and German to English translation directions .
There are two main categories of challenges : sentence - level prediction ( Task 1 . * ) and word-level prediction ( Task 2 ) .
Task 1.1 is about predicting post-editing effort ( PEE ) , Task 1.2 is about ranking translations from different systems , Task 1.3 is about predicting post-editing time ( PET ) , and Task 2 is about binary or multi-class classification of word-level quality .
For each task , we develop RTM models using the parallel corpora and the LM corpora distributed by the translation task ( WMT13 ) ( Callison - Burch et al. , 2013 ) and the LM corpora provided by LDC for English and Spanish 2 .
The parallel corpora contain 4.3 M sentences for de-en with 106 M words for de and 111 M words for en and 15 M sentences for en-es with 406 M words for en and 455 M words for 1 LIX =
A B + C 100 A , where A is the number of words , C is words longer than 6 characters , B is words that start or end with any of " . " , " : " , " ! " , " ? " similar to ( Hagstr ? m , 2012 ) .
2 English Gigaword 5th , Spanish Gigaword 3rd edition .
es .
We do not use any resources provided by QET including data , software , or baseline features since they are SMT system dependent or language specific .
Instance selection for the training set and the language model ( LM ) corpus is handled by a parallel implementation of FDA ( Bic ?ici , 2013 ) .
We tokenize and true-case all of the corpora .
The true-caser is trained on all of the training corpus using Moses ( Koehn et al. , 2007 ) .
We prepare the corpora by following this procedure : tokenize ? train the true-caser ? true-case .
Since we do not know the best training set size that will maximize the performance , we rely on previous SMT experiments ( Bic ?ici and Yuret , 2011a ; Bic ?ici and Yuret , 2011 b ) and quality estimation challenges ( Bic ?ici and van Genabith , 2013a ; Bic ?ici and van Genabith , 2013 b ) to select the proper training set size .
For each training and test sentence provided in each subtask , we choose between 65 and 600 sentences from the parallel training corpora to be added to the training set , which creates roughly 400K sentences for training .
We add the selected training set to the 8 million sentences selected for each LM corpus .
The statistics of the training data selected by the parallel FDA and used as interpretants in the RTM models is given in Table 2 .
Evaluation
In this section , we describe the metrics we use to evaluate the learning performance .
Let y i represent the actual target value for instance i , ? the mean of the actual target values , ? i the value estimated by the learning model , and ?
the mean of the estimated target values , then we use the following metrics to evaluate the learning models : ? Mean Absolute Error ( MAE ) : | | = n i=1 | ?i ?y i | n ? Relative Absolute Error ( RAE ) : ? ? | | = n i=1 | ?i ?y i | n i=1 |?y i | ? Root Mean Squared Error : RMSE = n i=1 ( ? i ?y i ) 2 n ? DeltaAvg : ?( V , S ) = 1 | S|/2?1 | S| /2 n=2 n?1 k=1 s? k i=1 q i V ( s ) | k i=1 q i | ? Correlation : r = n i=1 ( ?i ? ? ) ( y i ? ) ? n i=1 ( ?i ? ? ) 2 ? n i=1 ( y i ? )
2 DeltaAvg ( Callison - Burch et al. , 2012 ) calculates the average quality difference between the scores for the top n ?
1 quartiles and the overall quality for the test set .
Relative absolute error measures the error relative to the error when predicting the actual mean .
We use the coefficient of determination , R 2 = 1 ? n i=1 ( ?i ? y i ) 2 / n i=1 ( ? ? y i ) 2 , during optimization where the models are regression based and higher R 2 values are better .
Task 1 : Sentence-level Prediction of Quality
In this subsection , we develop techniques for the prediction of quality at the sentence - level .
We first discuss the learning models we use and how we optimize them and then provide the results for the individual subtasks and the settings used .
Learning Models and Optimization
The learning models we use for predicting the translation quality include the ridge regression ( RR ) and support vector regression ( SVR ) with RBF ( radial basis functions ) kernel .
Both of these models learn a regression function using the features to estimate a numerical target value such as the HTER score , the F 1 score ( Bic ?ici and Yuret , 2011 b ) , or the PET score .
We also use these learning models after a feature subset selection with recursive feature elimination ( RFE ) ( Guyon et al. , 2002 ) or a dimensionality reduction and mapping step using partial least squares ( PLS ) ( Specia et al. , 2009 ) , both of which are described in .
The learning parameters that govern the behavior of RR and SVR are the regularization ? for RR and the C , ? , and ? parameters for SVR .
We optimize the learning parameters , the number of features to select , and the number of dimensions used for PLS .
More detailed description of the optimization process is given in .
In our submissions , we only used the results we obtained from SVR and SVR after PLS ( SVRPLS ) since they perform the best during training .
Optimization can be a challenge for SVR due to the large number of parameter settings to search .
In this work , we decrease the search space by selecting ? close to the theoretically optimal values .
We select ?
close to the standard deviation of the noise in the training set since the optimal value for ? is shown to have linear dependence to the noise level for different noise models ( Smola et al. , 1998 ) .
We use RMSE of RR on the training set as an estimate for the noise level ( ? of noise ) and the following formulas to obtain the ? with ? = 3 : ? = ? ? ln n n ( 1 ) and the C ( Cherkassky and Ma , 2004 ; Chalimourda et al. , 2004 ) : C = max (|? + 3 ? y | , |? ? 3 ? y | ) ( 2 ) Since the C obtained could be low ( Chalimourda et al. , 2004 ) , we use a range of C values in addition to the obtained C value including C values with a couple of ?
y values larger .
Table 3 lists the RMSE of the RR model on the training set and the corresponding ? and C values for different subtasks .
We also present the optimized parameter values for SVR and SVRPLS .
Table 3 shows that , empirically , Equation 1 and Equation 2 gives results close to the best parameters found after optimization .
Task 1.1 involves the prediction of the case insensitive translation edit rate ( TER ) scores obtained by TERp ( Snover et al. , 2009 ) and their ranking .
In contrast , we derive features over sentences that are true-cased .
We obtain the rankings by sorting according to the predicted TER scores .
Table 4 presents the learning performance on the training set using the optimized parameters .
We are able to significantly improve the results when compared with the QET 2012 ( Callison - Burch et al. , 2012 ) and our previous results especially in terms of MAE and RAE .
The results on the test set are given in Table 5 . Rank lists the overall ranking in the task .
RTMs with SVR PLS learning is able to achieve the top rank in this task .
= ( c ? d ) / n( n? 1 ) 2 = c?d c+d where a pair is concordant , c , if the ordering agrees , discordant , d , if their ordering disagrees , and neither concordant nor discordant if their rankings are equal .
We use sentence - level F 1 scores ( Bic ?ici and Yuret , 2011 b ) as the target to predict .
We use F 1 because it can be easily interpreted and it correlates well with human judgments ( more than TER ) ( Bic ?ici and Yuret , 2011 b ; Callison - Burch et al. , 2011 ) .
We also found that the ? of the rankings obtained according to the F 1 score over the training set ( 0.2040 ) is better than BLEU ( Papineni et al. , 2002 ) ( 0.1780 ) and NIST ( Doddington , 2002 ) ( 0.1907 ) for de-en .
Table 6 presents the learning performance on the training set using the optimized parameters .
Learning F 1 becomes an easier task than learning TER as observed from the results but we have significantly more training instances .
We use the SVR model for predicting the F 1 scores on the training set and the test set .
MAE is a more important performance metric here since we want to be as precise as possible when predicting the actual performance .
Our next goal is to learn a threshold for judging if two translations are equal over the predicted F 1 scores .
This threshold is used to determine whether we need to alter the ranking .
We try to mimic the human decision process when determining two translations are equivalent .
On some occasions where the sentences are close enough , humans give them equal ranking .
This is also related to the granularity of the differences visible with a 1 to 5 ranking schema .
We compared different threshold formulations and used the following condition in our submissions to decide whether the ranking of item i in a set S of translations , i ?
S , should be different : j =i F 1 ( j ) ? F 1 ( i ) |j ? i| /|S| > t , ( 3 ) where t is the optimized threshold minimizing the following loss for n training instances : n i=1 ? ( f ( t , q i ) , r i ) ( 4 ) where f ( t , q i ) is a function returning rankings based on the threshold t and the quality scores for instance i , q i and ?
( r j , r i ) calculates the ? score based on the rankings r j and r i .
For both de-en and en-es subtasks , we found the thresholds obtained to be very similar or the same .
The optimized values are given in Table 7 .
On the test set , we used the same threshold , t = 0.00275 for both de-en and en-es , which is a little higher than the optimal t to prevent overfitting .
We believe that human judgments of linguistic equality and the corresponding thresholds we learned in this work can be useful for developing better automatic evaluation metrics and can improve the correlation of the scores obtained with human judgments ( as we did here ) .
The results on the test set are given in Table 8 .
We are also able to achieve the top ranking in this task .
The results on the test set are given in Table 10 .
We are able to become the 2nd best system according to MAE in this task .
Task 2 : Word-level Prediction of Quality
In this subsection , we develop a learning model , global linear models with dynamic learning rate ( GLMd ) , for the prediction of quality at the wordlevel where the word- level quality is a binary ( K : keep , C : change ) or multi-class classification problem ( K : keep , S : substitute , D : delete ) .
We first discuss the GLMd learning model , then we present the word - level features we use , and then present our results on the test set .
w = w + ? (?( x i , y i ) ? ?( x i , ? ) ) , ( 5 ) where ?
returns a global representation for instance i and the weights are updated by ? = exp( log 10 ( 3 ?1 / 0 ) ) with ?1 and 0 representing the error of the previous and first iteration respectively .
? decays the amount of the change during weight updates at later stages and prevents large fluctuations with updates .
We used both the GLM model and the GMLd models in our submissions .
Word-level Features
We introduce a number of novel features for the prediction of word-level translation quality .
In broad categories , these word- level features are : ? CCL : Uses CCL links . ?
Word context : Surrounding words . ?
Word alignments :
Alignments , their probabilities , source and target word contexts .
?
Length :
Word lengths , n-grams over them .
?
Location : Location of the words .
? Prefix and Suffix : Word prefixes , suffixes .
?
Form : Capital , contains digit or punctuation .
We found that CCL links are the most discriminative feature among these .
In total , we used 511 K features for binary and 637 K for multi-class classification .
The learning curve is given in Figure 1 .
The results on the test set are given in Table 11 . P , R , and A stand for precision , recall , and accuracy respectively .
We are able to become the 2nd according to A in this task .
Contributions Referential translation machines achieve top performance in automatic , accurate , and language independent prediction of sentence - level and wordlevel statistical machine translation ( SMT ) quality .
RTMs remove the need to access any SMT system specific information or prior knowledge of the training data or models used when generating the translations .
We develop novel techniques for solving all subtasks in the quality estimation ( QE ) task ( QET 2013 ) based on individual RTM models .
Our results achieve improvements over last year 's QE task results ( QET 2012 ) , as well as our previous results , provide new features and techniques for QE , and rank 1st or 2nd in all of the subtasks .
? Synthetic Translation Performance { 6 } : Calculates translation scores achievable according to the n-gram coverage .
? Length { 7 } : Calculates the number of words and characters for S and T and their average token lengths and their ratios .
?
Feature Vector Similarity { 16 } : Calculates similarities between vector representations .
? Perplexity { 90 } : Measures the fluency of the sentences according to language models ( LM ) .
We use both forward ( { 30 } ) and backward ( { 15 } ) LM features for S and T. ? Entropy { 9 } : Calculates the distributional similarity of test sentences to the training set over top N retrieved sentences .
? Retrieval Closeness { 24 } : Measures the degree to which sentences close to the test set are found in the selected training set , I , using FDA ( Bic ?ici and Yuret , 2011a ) .
? Diversity { 6 } : Measures the diversity of cooccurring features in the training set .
? IBM1 Translation Probability { 16 } : Calculates the translation probability of test sentences using the selected training set ,
develops global learning models ( GLM ) , which rely on Viterbi decoding , perceptron learning , and flexible feature definitions .
We extend the GLM by parallel perceptron training ( McDonald et al. , 2010 ) and dynamic learning with adaptive weight updates in the perceptron learning algorithm :
Figure 1 : 1 Figure 1 : Learning curve with the parallel GLM and GLMd models .
Binary
Table 1 : 1 Table 1 lists the statistics of the data used in the training and test sets for the tasks .
Data statistics for different tasks .
The number of words is listed after tokenization .
Task sents words Test sents Train 1.1 1.2 ( de-en ) 1.2 ( en-es ) 1.3 & 2 2254 32730 22338 803 63 K ( en ) 762 K ( de ) 528 K ( en ) 18 K ( en ) 67 K ( es ) 786 K ( en ) 559 K ( es ) 20 K ( es ) 500 1810 1315 284
Table 2 : 2 Statistics of the training data used as interpretants in the RTM models in thousands ( K ) of sentences or millions ( M ) of words .
Table 3 : 3 Optimal parameters predicted by Equation 1 and Equation2 and the optimized parameter values , ? and ? for SVR and SVRPLS and the number of dimensions ( # dim ) for SVRPLS .
3.2.2 Task 1.1 : Scoring and Ranking for Post-Editing Effort
Table 4 : 4 Tas k1.1 results on the training set .
Task 1.1 RR RR PLS SVR SVR PLS 0.4305 0.6569 0.1305 0.1003 0.7284 R 2 r RMSE MAE RAE 0.3510 0.5965 0.1393 0.1086 0.7888 0.4232 0.6509 0.1313 0.1023 0.7430 0.4394 0.6647 0.1295 0.0967 0.7023
Table 5 : 5 Tas k1.1 results on the test set .
Ranking CNGL SVRPLS CNGL SVR Scoring CNGL SVRPLS CNGL SVR DeltaAvg 11.09 9.88 MAE 13.26 13.85 r 0.55 0.51 RMSE Rank Rank 1 4 16.82 3 17.28 8 3.2.3 Task 1.2 : Ranking Translations from Different Systems
Task 1.2 involves the prediction of the ranking among up to 5 translation outputs produced by dif- ferent MT systems .
Evaluation is done against the human rankings using the Kendall 's ? corre - lation ( Callison - Burch et al. , 2013 ) : ?
Table 6 : 6 Tas k1.2 results on the training set .
de-en en-es Task 1.2 RR SVR RR SVR R 2 0.6320 0.7953 0.1169 0.0733 0.5535 r RMSE MAE RAE 0.7528 0.8692 0.0958 0.0463 0.3494 0.5101 0.7146 0.1569 0.1047 0.6323 0.4819 0.7018 0.1613 0.0973 0.5873
Table 7 : 7 Task1.2 optimized thresholds and the corresponding comparisons that were found to be equal ( # same ) over all comparisons ( # all ) .
Table 8 : 8 Task1.2 results on the test set .
Ties penalized model de-en CNGL SVRPLS F1 0.17 ? CNGL SVR F1 0.17 en-es CNGL SVRPLS F1 0.15 CNGL SVR F1 0.13
Ties ignored model ? de-en CNGL SVRPLS F1 0.17 CNGL SVR F1 0.17 en-es CNGL SVRPLS F1 0.16 CNGL SVR F1 0.13 Rank 3 4 1 2 Rank 3 4 2 3 3.2.4 Task 1.3 : Predicting Post-Editing Time
Task 1.3 involves the prediction of the post-editing time ( PET ) for a translator to post- edit the MT out - put .
Table 9 presents the learning performance on the training set using the optimized parameters .
Task 1.3 RR RR PLS SVR SVR PLS 0.5316 0.7604 62.6031 33.5490 0.5682 R 2 r RMSE MAE RAE 0.4463 0.6702 68.0628 39.5250 0.6694 0.5917 0.7716 58.4464 35.8759 0.6076 0.4062 0.6753 70.4853 36.5132 0.6184
Table 9 : 9 Tas k1.3 results on the training set .
Table 10 : 10 Tas k1.3 results on the test set .
Task 1.3 CNGL SVR CNGL SVRPLS 49.6161 MAE 49.2121 RMSE CNGL SVRPLS 86.6175 CNGL SVR 90.3650 Rank 3 4 Rank 4 7
Table 11 : 11
Task 2 results on the test set .
A CNGL dGLM .7146 .7392 .9261 .8222 P R F1 Rank ( A ) 2 CNGL GLM .7010 .7554 .8581 .8035 5 Multi-class A Rank CNGL dGLM .7162 3 CNGL GLM .7116 4
