title
Unsupervised Compositional Translation of Multiword Expressions
abstract
This article describes a dependency - based strategy that uses compositional distributional semantics and cross-lingual word embeddings to translate multiword expressions ( MWEs ) .
Our unsupervised approach performs translation as a process of word contextualization by taking into account lexico-syntactic contexts and selectional preferences .
This strategy is suited to translate phraseological combinations and phrases whose constituent words are lexically restricted by each other .
Several experiments in adjective-noun and verb-object compounds show that mutual contextualization ( co-compositionality ) clearly outperforms other compositional methods .
The paper also contributes with a new freely available dataset of English - Spanish MWEs used to validate the proposed compositional strategy .
Introduction
In the field of compositional distributional semantics there have been some interesting research , though not too much , making use of a syntaxsensitive vector space to compose the meaning of phrases and sentences ( Erk and Pad ? , 2008 ; Thater et al. , 2010 ; Erk et al. , 2010 ; Weir et al. , 2016 ) .
In those approaches , dependency - based combination of vectors enables words to be disambiguated as a process of contextualization .
More precisely , given two words , a and b , related by a syntactic dependency ( r ) , the meaning of the corresponding composite expression is actually two contextualized senses : a , which is the contextualized sense of a resulting from combing this word with the selectional restrictions imposed by b in relation r ; and b , which stands for the contextualized sense of b as a result of combining this word with the restrictions imposed by a in r.
Moving towards a multilingual scenario , the objective of this paper is to apply this unsupervised method to a bilingual vector space so as to model translation as a process of compositional contextualization .
In this regard , we first create contextualized vectors using selectional preferences , and then we generate possible translations by taking advantage of crosslingual word embeddings learned from monolingual corpora .
The results of several experiments in English - Spanish adjective -noun and verb-object compounds show that mutual contextualization ( or co-compositionality ) clearly outperforms other compositional methods .
Additionally , this paper also contributes with a new freely available dataset of 273 English - Spanish compound equivalents .
This new resource contains multiword expressions ( MWEs ) with different degrees of semantic compositionality ( free combinations such as use a computer , collocations - for instance , hard drug- , light - verb construction - e.g. , take a cab- , or idioms such as lone wolf ) , which are useful to evaluate translation strategies using compositional approaches .
It is worth noting that MWEs can fall into a wide spectrum of compositionality , from compositional compounds to idiomatic expressions ( Cordeiro et al. , 2019 ) .
To restrict the object of study , in this article , we focus on a specific subset of MWEs : adjective -noun and verb-noun compounds .
The rest of this article is organized as follows .
Section 2 describes the compositional translation method .
In Section 3 we describe the English - Spanish dataset and use it to evaluate the proposed strategy .
Then , some related work is presented in Section 4 .
Finally , Section 5 addresses conclusions , drawbacks of the strategy and future work .
Compositional Translation with Cross-Lingual Embeddings
The proposed method consists of two main tasks : i ) the construction of contextualized word mean-ing by means of a syntax -sensitive compositional distributional strategy ( see 2.1 ) ; ii ) word contextualization in a bilingual vector space allowing the translation of compounds ( See 2.2 ) .
We will focus on the translation two -word compounds encoded through a single syntactic dependency .
Compositional Distributional Meaning
We abandon the traditional choice of representing the meaning of a phrase or sentence as a single vector .
In our approach , the meaning of a composite expression is represented by a contextualized vector for each constituent word rather than by a single vector standing for the entire expression ( Erk and Pad ? , 2008 ; Weir et al. , 2016 ; Gamallo , 2017 ) .
This is in accordance with the main postulates of Dependency Grammar which only defines linguistic categories for words and relations , but not for composite units such as phrases or sentences .
Let us take the dependency ( r , h , d ) , where r is a binary relation between the head word , h , and the dependent one , d .
This dependency can be used to yield two lexico-syntactic contexts : ( ?r , h ) ( 1 ) ( ?r , d ) ( 2 ) where ?r and ?r are the head and dependent roles of relation r , respectively .
The tuple in 1 represents a lexico-syntactic context of word d while tuple 2 is a context of h .
Given these two contexts , the meaning of a binary dependency is represented by two contextualized vectors : h ( ?r, d ) and d ( ?r, h ) , which are defined as follows : h ( ?r, d ) = h + d ?r ( 3 ) d ( ?r, h ) = d + h ?r ( 4 ) where h ?r and d ?r are vectors representing selectional preferences , more precisely , h ?r stands for the selectional preferences imposed by the head , h , to the dependent word , d , and d ?r represents those imposed by the dependent one to the head .
So , the contextualized sense of a word is the result of adding ( by component - wise vector sum ) its direct vector with another one representing the selectional preferences imposed by the word linked to it in the syntactic dependency .
Head and depen-dent selectional preferences are defined as follows : h ?r = 1 N d:(?r, d ) ? Sal ?r ( h ) d ( 5 ) d ?r = 1 N h:(? r, h ) ?
Sal ?r ( d ) h ( 6 ) where Sal ?r ( h ) and Sal ?r ( d ) are two sets of salient contexts : the most salient contexts of the head , h , with the role ?r and the salient contexts of the dependent d with the role ?r , N being the cardinality of each set .
The set of salient contexts of a word consists of its top -N contexts ranked using a lexical association measure ( e.g. , PPMI , loglikelihood , etc ) .
The top -N contexts are considered to be the most salient and informative for the given word .
The summation runs through the lemmas that make up the salient contexts in equations 5 and 6 .
Equation 5 defines the head preferences and Equation 6 the dependent preferences .
Let us take an example .
The dependency ( amod , drug , hard ) , from the compound " hard drugs " , gives rise to two contextualized senses : drug ( ? amod , hard ) = drug + hard ? amod ( 7 ) hard ( ? amod , drug ) = hard + drug ? amod ( 8 )
The resulting vector in Equation 7 is the contextualized sense of drug as being modified by the adjective hard , while the vector in 8 represents the contextualized sense of hard when it modifies the noun drug .
The selectional preferences imposed by the noun ( head preferences ) , noted drug ? amod , are actually the result of adding the vectors of the most representative ( salient ) adjectives modifying that noun , divided by the number of representative adjectives .
Intuitively , it represents the main properties of drugs , for instance , psychoactive , hallucinogenic and illicit are the three more salient adjectives modifying the noun drug in our experiments .
On the other hand , the selectional preferences imposed by the adjective ( dependent preferences ) , and noted hard ( ? amod , drug ) , are the result of adding the vectors of the most representative nouns modified by the adjective , divided by the number of representative nouns .
So , it represents the set of most salient hard things ; for example , bop , disc and rock are the three most salient nouns modified by the adjective hard in our corpus .
English dependency Spanish candidates ( amod , drug , hard ) ( amod , medicamento , duro ) , ( amod , medicamento , dif?cil ) ( amod , medicamento , f?cil ) , ( amod , medicamento , imposible ) ( amod , medicamento , arduo ) , ( amod , droga , duro ) ( amod , droga , dif?cil ) , ( amod , droga , f?cil ) ( amod , droga , imposible ) , ( amod , droga , arduo ) ( amod , estupefaciente , duro ) , ( amod , estupefaciente , dif?cil ) ( amod , estupefaciente , f?cil ) , ( amod , estupefaciente , imposible ) ( amod , estupefaciente , arduo ) , ( amod , coca ? na , duro ) ( amod , coca?na , dif?cil ) , ( amod , coca?na , f?cil ) ( amod , coca? na , imposible ) , ( amod , coca? na , arduo ) ( amod , f?rmaco , duro ) , ( amod , f?rmaco , dif?cil ) ( amod , f?rmaco , f?cil ) , ( amod , f?rmaco , imposible ) ( amod , f?rmaco , arduo ) Table 1 : 25 Spanish candidate translations of the English collocation " hard drug " .
Only the one in bold is an acceptable translation .
The English drug was translated into Spanish by : medicamento ( medicine ) , droga ( narcotic ) , estupefaciente ( narcotic ) , coca? na ( cocaine ) , and f?rmaco ( medicine ) .
And the adjective hard was translated by : duro ( hard ) , dif?cil ( difficult ) , f?cil ( easy ) , imposible ( impossible ) , and arduo ( arduous ) .
We added the most common English translation of each Spanish word so that readers who do not know Spanish will understand the ambiguity issue .
Compositional Translation of Dependencies
The compositional translation of an expression syntactically codified in a binary dependency consists of three steps : i ) generation of translation candidates in the target language , ii ) construction of the compositional meaning of the source dependency and the candidates in the target language , and iii ) selection of the most similar candidate to the source dependency .
The input of the system is a dependency in the source language which is expanded into a set of candidate translations in the target language by making use of a translation lexicon automatically built with cross-lingual embeddings and Cosine similarity .
For instance , let us take an English - Spanish translation lexicon and select the five most similar nouns to drug and the five most similar adjectives to hard .
Taking into account these translations , the English dependency ( amod , drug , hard ) is expanded in the 5x5 Spanish candidates shown in Table 1 .
Once the candidates have been generated , the next step is to build the compositional vectors ( contextualized senses ) of both the input dependency and translation candidates , by making use of the algorithm used in the previous sub-section ( 2.1 ) and the cross-lingual embeddings of the previous step .
Finally , the compositional vectors of the candidates are compared pairwise with the source compositional vectors by means of cosine similarity and the most similar is selected .
For the binary dependency in the source language , a translation candidate is selected by computing the contextualized translation measure , CT , which selects the most similar dependency in the target language by comparing the degree of similarity between heads and dependents in both languages .
More precisely , given a dependency ( r , h , d ) in the source language , its translation into the target language is computed as follows : CT ( r , h , d ) = ( 9 ) arg max ( r , h , d ) ? S ( h ( ?r, d ) , h ( ?r , d ) ) + S ( d ( ?r, h ) , d ( ?r , h ) ) 2 where ( r , h , d ) is any target dependency belonging to the set of translation candidates , ?.
The first S computes the similarity between the two compositional vectors derived from the contextualized heads in the two languages .
The second one computes the similarity between the vectors derived from the contextualized dependent words .
So , CT is nothing more than the overall similarity between two composite expressions , which is the addition mean of the similarity scores obtained by comparing their head - based and dependent - based compositional vectors .
The resulting translation is , thus , the composite expression belonging to ? with the highest overall similarity score .
Experiments
To have an idea about the quality of compositional vectors , most of the research done so far has made use of monolingual datasets prepared to measure the correlation between individual human similarity scores and the system 's predictions ( Mitchell and Lapata , 2008 ; .
Nonetheless , we consider that translation of composite expressions and MWEs is a more reliable way of evaluating the quality of compositional strategies .
For instance , it is not clear whether blue car is semantically closer to red car than to yellow car , however , no one doubts that the Spanish translation of red car is coche rojo .
In order to allow an evaluation based on compositional translation , we have created two bilingual datasets with MWEs syntactically coded by means of two dependencies : adjective-noun ( amod ) and verb-noun ( vobj ) .
Test Datasets
To evaluate our compositional translation algorithm , it is required a bilingual resource containing a set of phrases with a simple syntactic structure in the source language with their possible translations into the target language .
As there is no such resource , we decided to generate it by taking advantage of a free list of multilingual MWEs which was obtained using parallel corpora ( Garcia , 2018 ) .
The method presented in the referred paper extracts candidates of syntactic collocations using PPMI and frequency thresholds , and then identifies multilingual equivalents using bilingual word embeddings .
From this resource , we selected 200 English - Spanish examples : 100 bilingual equivalents of adj-noun ( amod ) collocations ( e.g. , facial hair ) , and 100 verb-object ( vobj ) examples ( e.g. , take [ a ] cab ) .
These lists were manually reviewed and enlarged with more possible translations , obtaining a final resource of 273 English - Spanish pairs ( 92 amod expressions with 143 translations , and 83 vobj English examples with 130 Spanish equivalents ) .
It is worth mentioning that as these lists were built using statistical association measures they contain not only phraseological combinations , but also other expressions with different degrees of se-mantic compositionality : free combinations ( use [ a ] computer ) , true collocations ( e.g. , deep condolence , and also light - verb constructions such as take [ a ] cab ) , terms ( sulfuric acid ) , quasiidioms ( buy [ the ] silence ) , or idioms ( lone wolf ) ( Mel '?uk , 1998 ) . 1
Thus , this variety of expressions converts the lists into a valuable resource for evaluating the translation of adj-noun and verbobject instances .
2
Corpora and Distributional Models
In order to build bilingual compositional vectors , we made use of English and Spanish wikipedias ( dumps files of December 2018 ) , with 21 and 5 billion words , respectively .
The two wikipedias were PoS tagged and syntactically analyzed with LinguaKit ( Gamallo et al. , 2018 ) .
The syntactically analyzed corpus was the basis for the elaboration of the salient lexico-syntactic contexts with which we constructed selectional preferences and contextualized vectors .
Preliminary experiments were performed to find the best configuration , which was set to 50 salient contexts per lemma / PoS tag pair .
Bilingual embeddings were created with VecMap ( Artetxe et al. , 2018a ) by using the supervised configuration and an open available English - Spanish dictionary , Apertium , containing 6,249 nouns , verbs , and adjectives .
3 . To make the evaluation fairer , we have removed from the dictionary all English words belonging to the test datasets .
The original embeddings mapped by VecMap were created with Word2Vec , configured with CBOW algorithm , window 5 , and 300 dimensions ( Mikolov et al. , 2013 b ) .
Word2 Vec was applied on PoS tagged wikipedias and each token was coded as a lemma / tag pair .
The bilingual mapped models with lemma / tag embeddings are made freely available .
4
Translation Candidates
Using the bilingual vectors built from Wikipedia , each English word appearing in the test datasets was associated with the 10 most similar Spanish words and , so , each English binary dependency of the dataset was expanded with 10x10 candidate 1 Note , however , that in ambiguous cases , the compositional translation was preferred ( e.g. , cut [ a ] cable ) .
2 Both datasets have been added as suplementary material to the submission 3 https://github.com/apertium/ apertium-trunk 4 https://ufile.io/lrze1 ( anonymous account ) Spanish dependencies .
It means that each English expression was compared with 100 Spanish translation candidates .
It is worth pointing out that the correct translation is not always present in the 100 candidates .
Yet , previous experiments allowed us to verify that increasing the number of translation candidates did not improve the final results .
Evaluation
To evaluate our compositional strategy , CT ( head+ dep ) , which combines both head and dependent contextualized words ( see equation 9 ) , we compared its performance to five other approaches : CT ( head ) , which only considers the contextualized head ; CT ( dep ) , which only takes into account the contextualized dependent word ; mult , which combines the vectors of the two related words by pairwise multiplication ; add , which combines vectors by pairwise addition ; and corpus , which implements the corpus-based strategy described in ( Grefenstette , 1999 ) by just selecting the most frequent translation candidates in the Spanish corpus .
All strategies but corpus use the same bilingual word embeddings and the same similarity measure ( cosine ) between compositional vectors .
Additionally , we also included UNdreaMT in the evaluation .
UNdreaMT is a recent neural machine translation system which uses monolingual corpora and cross-lingual word embeddings to learn translation models in an unsupervised way ( Artetxe et al. , 2018 b ) .
In the learning process , UNdreaMT applies backtranslation and uses a single shared encoder for both languages .
To compare our compositional strategy with UNdreaMT , this system was trained with exactly the same monolingual corpora and word embeddings used by the other models .
As UNdreaMT works with surface structures ( and not dependency pairs ) , we adapted the input to not harm the system ( e.g. , package , bring ? bring the package ) .
Also , we manually modified the output to adapt it to the gold- standard format ( e.g. , b?sico instinto ? instinto , b?sico ) .
Table 2 shows the results of all these methods on the two datasets ( amod and vobj ) described above .
The table shows the accuracy , which is the number of correct translations divided by the number of different English expressions ( source language ) in each dataset .
It is worth noting the significant difference between the proposed strategy , CT ( head+ dep ) , and the rest of methods .
The two methods based on just one contextualized word , CT ( head ) and CT ( dep ) , obtain similar scores to the well - known baselines , mult and add , as well as to the unsupervised MT strategy implemented with UNdreaMT .
However , all these systems reached values far below those obtained by CT ( head+ dep ) combining the two contextualizations within the dependency .
Going into more detail , vector addition ( add ) outperforms vector multiplication ( mult ) in the two datasets , and also the contextualized dependent word performs better than the contextualized head in the two datasets .
Finally , corpus gets the lowest values of all the compared methods .
System
Error Analysis
We carried out an error analysis of the CT ( head+ dep ) model to know in detail in what types of expressions our strategy fails .
So every wrong translation of the system was analyzed and classified into the following five error types ( see Table 3 for quantitative results ) :
DistSimil : the most frequent errors arose from the distributional strategy ( they are common in other vector-based approaches ) , since words belonging to different semantic relations ( e.g. , antonyms ) may have very similar vectors .
In our experiments , CT ( head + dep ) translated male victim by v?ctima femenina ( female victim ) , or take a cab as tomar un furg ? n ( take a van ) .
Conventions : another frequent source of errors was the generation of expressions which do not collocate , e.g. , they do not follow the conventions of the target language , even if the meaning is transparent .
In this regard , fill a report was translated by llenar un informe instead of rellenar un informe ( both verbs in Spanish mean to fill , but llenar is most used for physical objects , e.g. , llenar el vaso , fill the glass ) .
Similarly , the system generated evidencia verdadera ( instead of evidencia real ) from real evidence .
Translation : 11 % of the errors were approximate translations which do not appear in the dataset .
This includes some combinations which may have slightly different meaning ( depending on the context ) , such as pr?xima d?cada and siguiente d?cada ( from next decade ) , and cases of polysemy : share a cell , where cell may refer to a biological cell ( c?lula in Spanish ) , and a room in a prison or a part of a spreadsheet ( both translated as celda ) .
Idiomacity : some non-compositional expressions were not correctly translated , such as lone wolf ( which usually refers to a person and not to an animal ) , which was translated as lobo indefenso ( vulnerable or defenseless wolf ) .
Data processing : finally , few errors emerged from problems in the data ( or in its preprocessing : tokenization , lemmatization , etc. ) .
As an example , the noun in industrial area was translated by area ( which does not exist in Spanish ) instead of ?rea .
Discussion on Co-Compositionality
The high accuracy reached by the strategy based on the two contextualizations seems to verify the co-compositionality hypothesis ( Pustejovsky , 1995 ) , which states that the head word imposes selectional restrictions on the dependent one , while this one also imposes its restrictions on the former .
It follows that a syntactic dependency between two words carries two complementary selective functions , each one imposing its own selectional pref-erences .
These two functions allow the two related words to mutually disambiguate or discriminate the sense of each other by co-composition However , co-compositionality has not been considered by many formal semantic approaches .
In most approaches to formal semantics , inspired by Categorial Grammar , the interpretation of composite expressions such as " hard drug " relies on a rigid function - argument structure .
In an adjectivenoun construction , the adjective denotes an unary function applied to the noun denotation .
Any syntactic dependency between two lexical words is generally represented in the semantic space as the assignment of an argument to a lexical function which impose its selectional preferences .
There is just one direction in the process of contextualization : the word representing the lexical function contextualizes ( imposes its preferences to ) the word representing the passive argument .
This one - way compositional procedure is also present in some work on distributional compositional semantics ( Baroni et al. , 2014 ; .
Unfortunately , a comparison with these one - way strategies has not been possible because they have not yet been applied to compositional translation .
Related Work
The proposed compositional method integrates three different tasks : to build compositional vectors representing the contextualized sense of composite expressions ; to build cross-lingual word embeddings from monolingual corpora ; to propose contextualized translations with compositional and cross-lingual vectors .
The basic approach to distributional composition is to combine vectors of two syntactically related words with arithmetic operations : addition and component - wise multiplication ( Mitchell and Lapata , 2008 , 2009 .
This approach is not strictly compositional since it does not take into account the syntactic structure underlying the expression .
It does not consider the functionargument relationship underlying compositionality in Categorial Grammar approaches ( Montague , 1970 ) .
Other approaches propose compositional models inspired by Categorial Grammar .
Some induce the compositional meaning of functional words from examples adopting regression techniques commonly used in machine learning ( Ba-roni and Zamparelli , 2010 ; Krishnamurthy and Mitchell , 2013 ; Baroni , 2013 ; Baroni et al. , 2014 ) , and others use tensor products for composition ( Coecke et al. , 2010 ; .
Although compositional , none of them is based on co-compositional strategy , like ours .
There are also studies making use of neuralbased approaches , namely bidirectional long short - term memory networks , to deal with word contextualization ( Melamud et al. , 2016 ; McCann et al. , 2017 ; Peters et al. , 2018 ) .
However , word contextualization is not defined by means of syntax - based compositional functions , as they do not consider the syntactic functions of the constituent words .
As has been said , our compositional approach is inspired by the work described in Erk and Pad ? ( 2008 ) and Erk et al . ( 2010 ) , in which second order vectors represent selectional preferences and each word combination gives rise to two contextualized word senses .
More recently , Weir et al. ( 2016 ) describe a similar approach where the meaning of a sentence is represented by the contextualized senses of its constituent words .
Each word occurrence is modeled by what they call anchored packed dependency tree , which is a dependency - based graph that captures the full sentential context of the word .
The main drawback of this context - based approach is its critical tendency to build very sparse word representations .
Our approach is an attempt to join the main ideas of these syntax - sensitive models ( namely , the use of selectional preferences and two returning word senses per combination ) in order to apply them to contextualized translation .
The method proposed in this paper also relies on count- based techniques to build bilingual vectors from monolingual corpora ( Fung and McKeown , 1997 ; Rapp , 1999 ; Saralegi et al. , 2008 ; Ansari et al. , 2014 ) .
Neural - based strategies also have been used to learn translation equivalents from word embeddings ( Mikolov et al. , 2013a ; Artetxe et al. , 2016 Artetxe et al. , , 2018a .
They learn a linear mapping between embeddings in two languages that minimizes the distances between equivalences listed in a bilingual dictionary .
Finally , many approaches to compositional translation of phrases and composite terms consist in decomposing the source term into atomic components , translating these components into the target language and recomposing the translated com-ponents into target terms ( Delpech et al. , 2012 ; Tanaka and Baldwin , 2003 ; Grefenstette , 1999 ) .
Selection of the best translation candidate is performed by means of corpus-based searching .
However , this strategy has not yielded good results in the experiments described in the previous section .
Our translation approach also follows the decomposing strategy but , unlike the works cited above , we use compositional / contextualized vectors to select the best candidate instead of basic corpus-based frequencies .
Conclusions
In this article , we tried to show that it is possible to apply compositional distributional semantics on a bilingual vector space to propose contextualized translations .
However , the proposed contextualization method has several drawbacks that need to be addressed in future work .
First , it will be necessary to deal with fertile translations , i.e. translations in which the target term has a different number of words ( and so a different syntactic structure ) than the source one .
For this purpose , we will expand the set of translation candidates by making use of a great variety of extraction strategies as , for instance , a Mel'?uk - based strategy consisting of identifying similar words to the base of a collocation ( Mel '?uk , 1998 ) .
Second , our method does not distinguish between compositional and non-compositional expressions .
It will probably be necessary to first identify the degree of compositionality of the source MWE before choosing the compositional translation strategy that best suits that expression ( Cordeiro et al. , 2019 ) .
And third , increasingly complex expressions consisting of more than one dependency will have to be dealt with .
For this purpose , the method will have to be generalized to any input sentence with any syntactic structure , giving rise to an unsupervised machine translation approach .
ators ( BBVA Foundation ) , Juan de la Ciervaincorporaci ?n grant ( IJCI - 2016-29598 ) .
Finally , we gratefully acknowledge the support of NVIDIA Corporation with the donation of two Titan Xp GPUs used for this research .
Table 3 : 3 Error classification ( type and percentage ) of the CT ( head + dep ) system .
Total values are the microaverage .
Type amod vobj Total DistSimil 42.86 66.67 53.85 Convention 21.43 25 23.08 Translation 14.29 8.33 11.54 Idiomacity 14.29 0 7.69 DataProcess 7.14 0 3.85
