title
Incorporating Structural Alignment Biases into an Attentional Neural Translation Model
abstract
Neural encoder-decoder models of machine translation have achieved impressive results , rivalling traditional translation models .
However their modelling formulation is overly simplistic , and omits several key inductive biases built into traditional models .
In this paper we extend the attentional neural translation model to include structural biases from word based alignment models , including positional bias , Markov conditioning , fertility and agreement over translation directions .
We show improvements over a baseline attentional model and standard phrase - based model over several language pairs , evaluating on difficult languages in a low resource setting .
Introduction Recently , models of end-to- end machine translation based on neural network classification have been shown to produce excellent translations , rivalling or in some cases surpassing traditional statistical machine translation systems ( Kalchbrenner and Blunsom , 2013 ; Sutskever et al. , 2014 ; Bahdanau et al. , 2015 ) .
This is despite the neural approaches using an overall simpler model , with fewer assumptions about the learning and prediction problem .
Broadly , neural approaches are based around the notion of an encoder-decoder ( Sutskever et al. , 2014 ) , in which the source language is encoded into a distributed representation , followed by a decoding step which generates the target translation .
We focus on the attentional model of translation ( Bahdanau et al. , 2015 ) which uses a dynamic representation of the source sentence while allowing the decoder to attend to different parts of the source as it generates the target sentence .
The attentional model raises intriguing opportunities , given the correspondence between the notions of attention and alignment in traditional word - based machine translation models ( Brown et al. , 1993 ) .
In this paper we map modelling biases from word based translation models into the attentional model , such that known linguistic elements of translation can be better captured .
We incorporate absolute positional bias whereby word order tends to be similar between the source sentence and its translation ( e.g. , IBM Model 2 and ( Dyer et al. , 2013 ) ) , fertility whereby each instance of a source word type tends to be translated into a consistent number of target tokens ( e.g. , IBM Models 3 , 4 , 5 ) , relative position bias whereby prior preferences for monotonic alignments / attention can be encouraged ( e.g. , IBM Model 4 , 5 and HMM - based Alignment ( Vogel et al. , 1996 ) ) , and alignment consistency whereby the attention in both translation directions are encouraged to agree ( e.g. symmetrisation heuristics ( Och and Ney , 2003 ) or joint modelling ( Liang et al. , 2006 ; Ganchev et al. , 2008 ) ) .
We provide an empirical analysis of incorporating the above structural biases into the attentional model , considering low resource translation scenario over four language - pairs .
Our results demonstrate consistent improvements over vanilla encoder - ( Bahdanau et al. , 2015 ) .
The encoder is shown below the decoder , and the edges connecting the two corresponding to the attention mechanism .
Aller Anfang RNN Attentional Decoder
Heavy edges denote a higher attention weight , and these values are also displayed in matrix form , with one row for each target word .
decoder and attentional model in terms of the perplexity and BLEU score , e.g. up to 3.5 BLEU points when re-ranking the candidate translations generated by a state - of - the - art phrase based model .
The attentional model of translation
We start by reviewing the attentional model of translation ( Bahdanau et al. , 2015 ) , as illustrated in Fig. 1 , before presenting our extensions in ?3 .
Encoder
The encoding of the source sentence is formulated using a pair of RNNs ( denoted bi - RNN ) one operating left-to - right over the input sequence and another operating right - to- left , h ? i = RNN ( h ? i?1 , r ( s ) s i ) h ?
i = RNN ( h ? i+ 1 , r ( s ) s i ) where h ?
i and h ?
i are the RNN hidden states .
The left-to- right RNN function is defined as h ?
i = tanh W ? si r ( s ) s i + W ? sh h ? i?1 + b ? s ( 1 ) where h ? 0 ? R H is a learned parameter vector , as are R ( s ) ? R V S ?E , W ? si ?
R H?E , W ? sh ?
R H?H and b ?
s ?
R H , with H the number of hidden units , V S the size of the source vocabulary and E the word embedding dimensionality .
1 Each source word is 1 Similarly , h ? 0 ? R H , W ? si ?
R H?E , W ? sh ?
R H?H , b ? s ?
R H are the parameters of the right- to- left RNN .
Note that we use a long short term memory unit ( Hochreiter and Schmidhuber , 1997 ) in place of the RNN , shown here for simplicity of exposition .
then represented as a pair of hidden states , one from each RNN , e i = h ?
i h ? i .
This encodes not only the word but also its left and right context , which can provide important evidence for its translation .
A crucial question is how this dynamic sized matrix E = [ e 1 , e 2 , . . . , e I ] ?
R I?H can be used in the decoder to generate the target sentence .
As with Sutskever 's encoder-decoder , the target sentence is created left-to - right using an RNN , while the encoded source is used to bias the process as an auxiliary input .
The mechanism for this bias is by attentional vectors , i.e. vectors of scores over each source sentence location , which are used to aggregate the dynamic source encoding into a fixed length vector .
Decoder
The decoder operates as a standard RNN over the translation t , formulated as follows g j = tanh W ( th ) g j?1 + W ( ti ) r ( t ) tj?1 + W ( ta ) c j ( 2 ) u j = tanh g j + W ( uc ) c j + W ( ui ) r ( t ) tj?1 ( 3 ) t j ? softmax W ( ou ) u j + b ( to ) ( 4 ) where the decoder RNN is defined analogously to Eq 1 but with an additional input , the source attention component c j ?
R 2H and weighting matrix W ( ta ) ? R H?2H .
The hidden state of the recurrence is then passed through a single hidden layer 2 ( Eq 3 ) in combination with the source attention and target word using weighting matrices W ( uc ) ? R H?2H and W ( ui ) ? R H?E .
In Eq 4 this vector is transformed to be target vocabulary sized , using weight matrix W ( ou ) ?
R V T ?H and bias b ( to ) ?
R V T , after which a softmax is taken , and the resulting normalised vector used as the parameters of a Categorical distribution in generating the next target word .
The presentation above assumes a simple RNN is used to define the recurrence over hidden states , however we can easily use alternative formulations of recurrent networks including multiplelayer RNNs , gated recurrent units ( GRU ; Cho et al . ( 2014 ) ) , or long short-term memory ( LSTM ; Hochreiter and Schmidhuber ( 1997 ) ) units .
These more advanced methods allow for more efficient learning of more complex concepts , particularly long distance effects .
Empirically we found LSTMs to be the best performing , and therefore use these units herein .
The last key detail is the attentional component c j in Eqs 2 and 3 , which is defined as follows f ji = v tanh W ( ae ) e i + W ( ah ) g j?1 ( 5 ) ? j = softmax ( f j ) c j = i ?
ji e i with the scalars f ji denoting the compatibility between the target hidden state g j?1 and the source encoding e i .
This is defined as a neural network with one hidden layer of size A and a single output , parameterised by W ( ae ) ? R A?2H , W ( ah ) ?
R A?H and v ? R A .
The softmax then normalises the scalar compatibility values such that for a given target word j , the values of ?
j can be interpreted as alignment probabilities to each source location .
Finally , these alignments are used to to reweight the source components E to produce a fixed length context representation .
Training of this model is done by minimising the cross-entropy of the target sentence , measured word - by - word as for a language model .
We use standard stochastic gradient optimisation using the back - propagation technique for computation of partial derivatives according to the chain rule .
Incorporating Structural Biases
The attentional model , as described above , provides a powerful and elegant model of translation in which alignments between source and target words are learned through the implicit conditioning context afforded by the attention mechanism .
Despite its elegance , the attentional model omits several key components of a traditional alignment models such as the IBM models ( Brown et al. , 1993 ) and Vogel 's hidden Markov Model ( Vogel et al. , 1996 ) as implemented in the GIZA ++ toolkit ( Och and Ney , 2003 ) .
Combining the strengths of this highly successful body of research into a neural model of machine translation holds potential to further improve modelling accuracy of neural techniques .
Below we outline methods for incorporating these factors as structural biases into the attentional model .
Position bias First we consider position bias , based on the observation that a word at a given relative position in the source tends to align to a word at a similar relative position in the target , i I ? j J ( Dyer et al. , 2013 ) .
Related , the IBM model 2 learns discrete mappings between positions i and j conditioned on sentence lengths I and J .
We include a position bias through redefining the pre-normalised attention scalars f ji in Eq 5 as : f ji = v tanh W ( ae ) e i + W ( ah ) g j?1 + W ( ap ) ?( j , i , I ) ( 6 ) where the new component in the input is a simple feature function of the positions in the source and target sentences and the source length , ?( j , i , I ) = log ( 1 + j ) , log ( 1 + i ) , log ( 1 + I ) and W ( ap ) ? R A?3 .
We exclude the target length J as this is unknown during decoding , as a partial translation can have several ( infinite ) different lengths .
The use of the log ( 1 + ? ) function is to avoid numerical instabilities from widely varying sentence lengths .
The non-linearity in Eq 6 allows for complex functions of these inputs to be learned , such as relative positions and approximate distance from the diagonal , as well as their interactions with the other inputs ( e.g. , to learn that some words are exceptional cases where a diagonal bias should not apply ) .
Markov condition
The HMM model of translation ( Vogel et al. , 1996 ) is based on a Markov condition over alignment random variables , to allow the model to learn local effects such as when i ?
j is aligned then it is likely that i +
1 ? j + 1 or i ? j + 1 . These correspond to local diagonal alignments or one- to-many alignments , respectively .
In general , there are many correlations between the alignments of a word and the alignments of the preceding word .
Markov conditioning can also be incorporated in a similar manner to positional bias , by augmenting the attentional input from Eqs 5 and 6 to include : f ji = v tanh . . . + W ( am ) ? 1 ( ? j?1 ; i ) ( 7 ) where . . . abbreviates the e i , g j?1 and ? components from Eq 6 , and ? 1 ( ? j?1 ) provides a fixed dimensional representation of the attention state for the preceding word .
It is not immediately obvious how to incorporate the previous attention vector as ? is dynamically sized to match the source sentence length , thus using it directly would not generalise over sentences of different lengths .
For this reason , we make a simplification by just considering local moves offset by ?k positions , that is , 2 k + 1 ) .
Our approach is likely to capture the most important alignments patterns forming the backbone of the alignment HMM , namely monotone , 1 - to - many , and local inversions .
? 1 ( ? j?1 ; i ) = ? j?1 , i?k , .. , ? j?1 , i , .. , ? j?1 , i+k with W ( am ) ? R A ?(
Fertility Fertility is the propensity for a word to be translated as a consistent number of words in the other language , e.g. , Iseseisvusdeklaratsioon ( Et ) translates as 3 - 4 words in English , namely ( the ) Declaration of Independence .
Fertility is a central component in the IBM models 3 - 5 ( Brown et al. , 1993 ) .
Incorporating fertility into the attentional model is a little more involved , and we present two techniques for doing so .
Local fertility
First we consider a feature - based technique , which includes the following features ? 2 ( ? <j ; i ) = ? ? j <j ? j , i?k , .. , j <j ? j , i , .. , j <j ? j , i+1 ? ? and the corresponding feature weights , i.e. , W ( af ) ? R A ?( 2k + 1 ) .
These sums represent the total alignment score for the surrounding source words , similar to fertility in a traditional latent variable model , which is the sum over binary alignment random variables .
A word which already has several alignments can be excluded from participating in more alignments , thus combating the garbage collection problem .
Conversely words that tend to need high fertility can be learned through the interactions between these features and the word and context embeddings in Eq 7 .
Global fertility A second , more explicit , technique for incorporating fertility is to include this as a modelling constraint .
Initially we considered a soft constraint based on the approach in ( Xu et al. , 2015 ) , where an image captioning model was biased to attend to every pixel in the image exactly once .
In our setting , the same idea can be applied through adding a regularisation term to the training objective of the form i 1 ? j ? j, i 2 .
However this method is overly restrictive : enforcing that every word is used exactly once is not appropriate in translation where some words are likely to be dropped ( e.g. , determiners and other function words ) , while others might need to be translated several times to produce a phrase in the target language .
3
For this reason we develop an alternative method , based around a contextual fertility model , p( f i |s , i ) = N ?( e i ) , ? 2 ( e i ) which scores the fertility of source word i , defined as f i = j ? j , i , using a normal distribution 4 parameterised by ? and ?
2 , both positive scalar valued non-linear functions of the source word encoding e i .
This is incorporated into the training objective as an additional additive term , i log p( f i |s , i ) , for each training sentence .
This formulation allows for greater consistency in translation , through e.g. , learning which words tend to be omitted from translation , or translate as several words .
Compared to the fertility model in IBM 3 - 5 ( Brown et al. , 1993 ) , ours uses many fewer parameters through working over vector embeddings , and moreover , the BiRNN encoding of the source means that we learn context- dependent fertilities , which can be useful for dealing with fixed syntactic patterns or multi-word expressions .
Bilingual Symmetry
So far we have considered a conditional model of the target given the source , modelling p( t | s ) .
However it is well established for latent variable translation models that the alignments improve if p( s|t ) is also modelled and the inferences of both directional models are combined - evidenced by the symmetrisation heuristics used in most decoders , and also by explicit joint agreement training objectives ( Liang et al. , 2006 ; Ganchev et al. , 2008 ) .
The rationale is that both models make somewhat independent errors , so an ensemble stands to gain from variance reduction .
We propose a method for joint training of two directional models as pictured in Figure 2 .
Training twinned models involves optimising L = ? log p( t |s ) ? log p( s|t ) + ?B where , as before , we consider only a single sentence pair , for simplicity of notation .
This corresponds to a pseudo-likelihood objective , with the B linking the two models .
5
The B component considers the alignment ( attention ) matrices , ? s?t ? R J?I and ? t?s ?
R I?J , and attempts to make these close to one another for both translation directions ( see Fig. 2 ) .
To achieve this , we use a ' trace bonus ' , inspired by ( Levinboim et al. , 2015 ) , formulated as B = ? tr (? s?t ? s?t ) = j i ? s?t i , j ? s?t j , i .
As the alignment cells are normalised using the softmax and thus take values in [ 0,1 ] , the trace term is bounded above by min( I , J ) which occurs when the two alignment matrices are transposes of each
Experiments Datasets .
We conducted our experiments with four language pairs , translating between English ? Romanian , Estonian , Russian and Chinese .
These languages were chosen to represent a range of translation difficulties , including languages with significant morphological complexity ( Estonian , Russian ) .
We focus on a ( simulated ) low resource setting , where only a limited amount of training data is available .
This serves to demonstrate the robustness and generalisation of our model on sparse data - something that has not yet been established for neural models with millions of parameters with vast potential for over-fitting .
Table 1 shows the statistics of the training sets .
6 For Chinese - English , the data comes from the BTEC corpus , where the number of training sentence pairs is 44,016 .
We used ' devset1 2 ' and ' devset 3 ' as the development and test sets , respectively , and in both cases used only the first reference for evaluation .
For Romanian and Estonian , the data come from the Europarl corpus ( Koehn , 2005 ) , where we used 100K sentence pairs for training , and 3 K for development and 2 K for testing .
7 The Russian - English data was taken from a web derived corpus ( Antonova and Misyurev , 2011 ) .
The dataset is split into three parts using the same technique as for the Europarl sets .
During the preprocessing stage we lower -cased and tokenized the data , and excluded sentences longer than 30 words .
For the Europarl data , we also removed sentences containing headings and other meeting formalities .
8 Models and Baselines .
We have implemented our neural translation model with linguistic features in C ++ using the CNN library .
9
We compared our proposed model against our implementations of the attentional model ( Bahdanau et al. , 2015 ) and encoder-decoder architecture ( Sutskever et al. , 2014 ) .
As the baseline , we used a state - of - the - art phrase - based statistical machine translation model built using Moses ( Koehn et al. , 2007 ) with the standard features : relative -frequency and lexical translation model probabilities in both directions ; distortion model ; language model and word count .
We used KenLM ( Heafield , 2011 ) to create 3 - gram language models with Kneser - Ney smoothing on the target side of the bilingual training corpora .
Evaluation Measures .
Following previous work ( Kalchbrenner and Blunsom , 2013 ; Sutskever et al. , 2014 ; Bahdanau et al. , 2015 ; Neubig et al. , 2015 ) , we evaluated all neural models using test set perplexities and translation results , as well as in an additional re-ranking setting , using BLEU ( Papineni et al. , 2002 ) measure .
We applied bootstrap resampling ( Koehn , 2004 ) to measure statistical significance , p < 0.05 , of our models compared to a baseline .
For re-ranking , we generated 100 - best translations using the baseline phrase - based model , to which we added log probability features from our neural models alongside all the features of the underlying phrase - based model .
We trained the re-ranking models using MERT ( Och , 2003 ) on development sets with 100 - best translations .
Analysis of Alignment Biases
We start by investigating the effect of various linguistic constraints , described in Section 3 , on the attentional model .
Table 2 presents the perplexity of trained models for Chinese ?
English translation .
For comparison , we report the results of an encoderdecoder - based neural translation model ( Sutskever et al. , 2014 ) q q q q q q q q q q qq qq q qq qq qq qq qq qq qq q q qq q q q 0 2 4 embedding , 512 hidden , and 256 alignment dimensions .
For each model , we also report the number of its parameters .
Models are trained end-to - end using stochastic gradient descent ( SGD ) , allowing up to 20 epochs .
We use a held - out development set for regularisation by early stopping , which terminated the training after 5 - 10 epochs for most cases .
As expected , the vanilla attentional model greatly improves over encoder-decoder ( perplexity of 4.77 vs. 5.35 ) , clearly making good use of the additional context .
Adding the combined positional bias , local fertility , and Markov structure ( denoted by + align ) further decreases the perplexity to 4.56 .
Adding the global fertility ( + glofer ) is detrimental , however , increases perplexity to 5.20 .
Interestingly , global fertility helps to reduce the perplexity ( to 4.31 ) when used with the pre-training setting ( + align + glofer - pre ) .
In this case , it is refining an already excellent model from which reliable global fertility estimates can be obtained .
This finding is consistent with the other languages , see Figure 3 which shows typical learning curves of different variants of the attentional model .
Note that when global fertility is added to the vanilla attentional model with alignment features , it significantly slows down training as it limits exploration in early training iterations , however it does bring a sizeable win when used to fine - tune a pre-trained model .
Finally , the bilingual symmetry also helps to reduce the perplexity scores when used with the alignment features , however , does not combine well with global fertility ( + align+sym + glofer-pre ) .
This is perhaps an unsurprising result as both methods impose a often-times similar regularising effect over the attention matrix .
Figure 4 illustrates the different attention matri-ces inferred by the various model variants .
Note the difference between the base attentional model and its variant with alignment features ( ' + align ' ) , where more weight is assigned to diagonal and 1 - to - many alignments .
Global fertility pushes more attention to the sentinel symbols ?s? and ?/s?.
Determiners and prepositions in English show much lower fertility than nouns , while Estonian nouns have even higher fertility .
This accords with Estonian morphology , wherein nouns are inflected with rich case marking , e.g. , n?ukoguga has the cogitative - ga suffix , meaning ' with ' , and thus translates as several English words ( with the council ) .
The right-most column corresponds to joint symmetric training , with many more confident attention values especially for consistent 1 - to - many alignments ( difficult in English and raskeid in Estonian , an adjective in partitive case meaning some difficult ) .
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
Experimental Results
The perplexity results of the neural models for the two translation directions across the four language pairs are presented in Table 3 .
In all cases , our work achieves lower perplexities compared to the vanilla attentional model and the encoder-decoder architecture , owing to the linguistic constraints .
We also obtained similar patterns of improvements when decoding , using a greedy decoding strategy , as shown in Table 4 .
The exception was for en?ru , where the addition of the global fertility ( in addition to the other aligment features ) was detrimental , resulting in a decrease in BLEU score ( 5.94 ? 5.26 ) .
This may be due to highly noisy nature of the web text corpus of Russian - English language pair , compared to the much cleaner sources for the other language pairs .
Greedy decoding does not appear to be competitive for neural models trained on small parallel corpora , not reaching the level of a phrase - based baseline ( see Table 5 ) .
Despite this , however , these models still provide substantial gains when used for reranking ( as shown in Table 5 ) for translating into English from the other four languages .
We compare reranking settings using the log probabilities produced by our model as additional features 10 vs. using log probabilities from the vanilla attentional model and the encoder-decoder .
The re-rankers based on our model are significantly better than the rest for Chinese and Estonian , and on par with the other for Russian and Romanian ?
English .
In all cases our model has performance at least 1 BLEU point better than the baseline phrase - based system .
It is worth not - ing that for Chinese - English , our re-ranker leads to a substantial increase of almost 3 BLEU points .
5 Related Work Kalchbrenner and Blunsom ( 2013 ) were the first to propose a full neural model of translation , using a convolutional network as the source encoder , followed by an RNN decoder to generate the target translation .
This was extended in Sutskever et al . ( 2014 ) , who replaced the source encoder with an RNN using a Long Short - Term Memory ( LSTM ) and leveraged the last hidden RNN states as source context for generating the output .
Inspired by this , Bahdanau et al . ( 2015 ) introduced the notion of " attention " to the model , whereby the source context can dynamically change during the decoding process to attend to the most relevant parts of the source sentence .
Further , Luong et al. ( 2015 ) refined the attention mechanism to be more local , by constraining attention to a text span , whose words ' representations are averaged .
Similar in spirit to our work , recent research has proposed different ways of leveraging the attention history to incorporate alignment structural biases .
( Luong et al. , 2015 ) made use of the attention vector of the previous position when generating the attention vector for the next position .
Feng et al. ( 2016 ) added another recurrent structure for the attention mechanism to enhance its memorization capabilities and capture long- range dependencies between the attention vectors .
Tu et al. ( 2016 ) proposed a coverage vector to keep track of the attention history , hence refining future attentions .
Finally , Cheng et al. ( 2015 ) proposed a similar agreement - based joint training for bidirectional attention - based neural machine translation , and showed significant improvements in BLEU for the large data French ?
English translation .
Conclusion
We have shown that the attentional model of translation does not capture many well known properties of traditional word - based translation models , and proposed several ways of imposing these as structural biases on the model .
We show improvements across several challenging language pairs in a low-resource setting , as well as in perplexity , translation and reranking evaluations .
In future work we intend to investigate the model performance on larger-scale datasets , and incorporate further linguistic information , such as morphological representations .
Figure 1 : 1 Figure1 : Attentional model of translation ( Bahdanau et al. ,
