title
Addressing Word-order Divergence in Multilingual Neural Machine Translation for Extremely Low Resource Languages
abstract
Transfer learning approaches for Neural Machine Translation ( NMT ) trains a NMT model on an assisting language - target language pair ( parent model ) which is later fine-tuned for the source language - target language pair of interest ( child model ) , with the target language being the same .
In many cases , the assisting language has a different word order from the source language .
We show that divergent word order adversely limits the benefits from transfer learning when little to no parallel corpus between the source and target language is available .
To bridge this divergence , we propose to pre-order the assisting language sentences to match the word order of the source language and train the parent model .
Our experiments on many language pairs show that bridging the word order gap leads to major improvements in the translation quality in extremely low-resource scenarios .
Introduction Transfer learning for multilingual Neural Machine Translation ( NMT ) ( Zoph et al. , 2016 ; Dabre et al. , 2017 ; Nguyen and Chiang , 2017 ) attempts to improve the NMT performance on the source to target language pair ( child task ) using an assisting source language ( assisting to target language translation is the parent task ) .
Here , the parent model is trained on the assisting and target language parallel corpus and the trained weights are used to initialize the child model .
If source- target language pair parallel corpus is available , the child model can further be fine-tuned .
The weight initialization reduces the requirement on the training data for the source- target language pair by transferring knowledge from the parent task , thereby improving the performance on the child task .
However , the divergence between the source and the assisting language can adversely impact the benefits obtained from transfer learning .
Multiple studies have shown that transfer learning works best when the languages are related ( Zoph et al. , 2016 ; Nguyen and Chiang , 2017 ; Dabre et al. , 2017 ) .
Zoph et al. ( 2016 ) studied the influence of language divergence between languages chosen for training the parent and the child model , and showed that choosing similar languages for training the parent and the child model leads to better improvements from transfer learning .
Several studies have tried to address the lexical divergence between the source and the target languages either by using Byte Pair Encoding ( BPE ) as basic input representation units ( Nguyen and Chiang , 2017 ) or character - level NMT system ( Lee et al. , 2017 ) or bilingual embeddings ( Gu et al. , 2018 ) .
However , the effect of word order divergence and its mitigation has not been explored .
In a practical setting , it is not uncommon to have source and assisting languages with different word order .
For instance , it is possible to find parallel corpora between English ( SVO word order ) and some Indian ( SOV word order ) languages , but very little parallel corpora between Indian languages .
Hence , it is natural to use English as an assisting language for inter-Indian language translation .
To address the word order divergence , we propose to pre-order the assisting language sentences ( SVO ) to match the word order of the source language ( SOV ) .
We consider an extremely resourceconstrained scenario , where there is no parallel corpus for the child task .
From our experiments , we show that there is a significant increase in the translation accuracy for the unseen source - target language pair .
Related Work
To the best of our knowledge , no work has addressed word order divergence in transfer learning for multilingual NMT .
However , some work exists for other NLP tasks in a multilingual setting .
For Named Entity Recognition ( NER ) , Xie et al. ( 2018 ) use a self-attention layer after the Bi-LSTM layer to address word-order divergence for Named Entity Recognition ( NER ) task .
The approach does not show any significant improvements , possibly because the divergence has to be addressed before / during construction of the contextual embeddings in the Bi-LSTM layer .
Joty et al. ( 2017 ) use adversarial training for cross-lingual questionquestion similarity ranking .
The adversarial training tries to force the sentence representation generated by the encoder of similar sentences from different input languages to have similar representations .
Pre-ordering the source language sentences to match the target language word order has been found useful in addressing word-order divergence for Phrase - Based SMT ( Collins et al. , 2005 ; Ramanathan et al. , 2008 ; Navratil et al. , 2012 ; . For NMT , Ponti et al. ( 2018 ) and Kawara et al . ( 2018 ) have explored preordering .
Ponti et al. ( 2018 ) demonstrated that by reducing the syntactic divergence between the source and the target languages , consistent improvements in NMT performance can be obtained .
On the contrary , Kawara et al . ( 2018 ) reported drop in NMT performance due to pre-ordering .
Note that these works address source-target divergence , not divergence between source languages in multilingual NMT scenario .
Proposed Solution Consider the task of translating for an extremely low-resource language pair .
The parallel corpus between the two languages , if available may be too small to train an NMT model .
Similar to Zoph et al . ( 2016 ) , we use transfer learning to overcome data sparsity between the source and the target languages .
We choose English as the assisting language in all our experiments .
In our resource-scarce scenario , we have no parallel corpus for training the child model .
Hence , at test time , the source language sentence is translated using the parent model after performing a word- byword translation from source to the assisting language using a bilingual dictionary .
Since the source language and the assisting language ( English ) have different word order , we hypothesize that it leads to inconsistencies in the con -
Before Reordering After Reordering textual representations generated by the encoder for the two languages .
Specifically , given an English sentence ( SVO word order ) and its translation in the source language ( SOV word order ) , the encoder representations for words in the two sentences will be different due to different contexts of synonymous words .
This could lead to the attention and the decoder layers generating different translations from the same ( parallel ) sentence in the source or assisting language .
This is undesirable as we want the knowledge to be transferred from the parent model ( assisting source ?
target ) to the child model ( source?target ) .
S In this paper , we propose to pre-order English sentences ( assisting language sentences ) to match the source language word-order and train the parent model on the pre-ordered corpus .
Table 1 shows one of the pre-ordering rules ( Ramanathan et al. , 2008 ) used along with an example sentence illustrating the effect of pre-ordering .
This will ensure that context of words in the parallel source and assisting language sentences are similar , leading to consistent contextual representations across the source languages .
Pre-ordering may also be beneficial for other word order divergence scenarios ( e.g. , SOV to SVO ) , but we leave verification of these additional scenarios for future work .
Experimental Setup
In this section , we describe the languages experimented with , datasets used , the network hyperparameters used in our experiments .
Languages :
We experimented with English ?
Hindi translation as the parent task .
English is the assisting source language .
Bengali , Gujarati , Marathi , Malayalam and Tamil are the source languages , and translation from these to Hindi constitute the child tasks .
Hindi , Bengali , Gujarati and Marathi are Indo-Aryan languages , while Malayalam and Tamil are Dravidian languages .
All these languages have a canonical SOV word order .
Datasets : For training English -Hindi NMT systems , we use the IITB English - Hindi parallel corpus ( Kunchukuttan et al. , 2018 ) ( 1.46 M sentences from the training set ) and the ILCI English - Hindi parallel corpus ( 44.7 K sentences ) .
The ILCI ( Indian Language Corpora Initiative ) multilingual parallel corpus ( Jha , 2010 ) 1 spans multiple Indian languages from the health and tourism domains .
We use the 520- sentence dev-set of the IITB parallel corpus for validation .
For each child task , we use 2 K sentences from ILCI corpus as test set .
Network :
We use OpenNMT - Torch ( Klein et al. , 2018 ) to train the NMT system .
We use the standard encoder-attention - decoder architecture ( Bahdanau et al. , 2015 ) with input-feeding approach ( Luong et al. , 2015 ) .
The encoder has two layers of bidirectional LSTMs with 500 neurons each and the decoder contains two LSTM layers with 500 neurons each .
We use a mini-batch of size 50 and a dropout layer .
We begin with an initial learning rate of 1.0 and continue training with exponential decay till the learning rate falls below 0.001 .
The English input is initialized with pre-trained fast - Text embeddings ( Grave et al. , 2018 ) 2 . English and Hindi vocabularies consists of 0.27 M and 50 K tokens appearing at least 2 and 5 times in the English and Hindi training corpus respectively .
For representing English and other source languages into a common space , we translate each word in the source language into English using a bilingual dictionary ( we used Google Translate to get single word translations ) .
In an end-to- end solution , it would be ideal to use bilingual embeddings or obtain word - by - word translations via bilingual embeddings ( Xie et al. , 2018 ) .
However , publicly available bilingual embeddings for English -Indian languages are not good enough for obtaining good-quality , bilingual representations ( Smith et al. , 2017 ; Jawanpuria et al. , 2019 ) and publicly available bilingual dictionaries have limited coverage .
The focus of our study is the in- fluence of word-order divergence on Multilingual NMT .
We do not want bilingual embeddings quality or bilingual dictionary coverage to influence the experiments , rendering our conclusions unreliable .
Hence , we use the above mentioned largecoverage bilingual dictionary .
Pre-ordering :
We use CFILT - preorder 3 for prereordering English sentences .
It contains two preordering configurations : ( 1 ) generic rules ( G ) that apply to all Indian languages ( Ramanathan et al. , 2008 ) , and ( 2 ) hindi-tuned rules ( HT ) which improves generic rules by incorporating improvements found through error analysis of English - Hindi reordering ( Patel et al. , 2013 ) .
The Hindituned rules improve translation for other English to Indian language pairs too .
Results
We experiment with two scenarios : ( a ) an extremely resource scarce scenario with no parallel corpus for child tasks , ( b ) varying amounts of parallel corpora available for child task .
No Parallel Corpus for Child Task
The results from our experiments are presented in the scores .
We observe that both the pre-ordering models significantly improve the translation quality over the no-preordering models for all the language pairs .
The results support our hypothesis that word-order divergence can limit the benefits of multilingual translation .
Thus , reducing the word order divergence improves translation in extremely low-resource scenarios .
An analysis of the outputs revealed that preordering significantly reduced the number of UNK tokens ( placeholder for unknown words ) in the test output ( Table 3 ) .
We hypothesize that due to word order divergence between English and Indian languages , the encoder representation generated is not consistent leading to decoder generating unknown words .
However , the pre-ordered models generate better encoder representations leading to lesser number of UNK tokens and better translation , which is also reflected in the BLEU scores and Table 4 .
Parallel Corpus for Child Task
We study the impact of child task parallel corpus on pre-ordering .
To this end , we finetune the parent task model with the child task parallel corpus .
Table 5 shows the results for Bengali-Hindi , Gujarati-Hindi , Marathi-Hindi , Malayalam -Hindi , and Tamil-Hindi translation .
We observe that pre-ordering is beneficial when almost no child task corpus is available .
As the child task corpus increases , the model learns the on edit distance , hence it can handle morphological variations and cognates ( Virpioja and Gr?nroos , 2015 ) . word order of the source language ; hence , the non pre-ordering models perform almost as good as or sometimes better than the pre-ordered ones .
The non pre-ordering model is able to forget the wordorder of English and learn the word order of Indian languages .
We attribute this behavior of the non pre-ordered model to the phenomenon of catastrophic forgetting ( McCloskey and Cohen , 1989 ; French , 1999 ) which enables the model to learn the word-order of the source language when sufficient child task parallel corpus is available .
We also compare the performance of the finetuned model with the model trained only on the available source -target parallel corpus with randomly initialized weights ( No Transfer Learning ) .
Transfer learning , with and without pre-ordering , is better compared to training only on the small source - target parallel corpus .
Conclusion
In this paper , we show that handling word-order divergence between the source and assisting languages is crucial for the success of multilingual NMT in an extremely low-resource setting .
We show that pre-ordering the assisting language to match the word order of the source language significantly improves translation quality in an extremely low-resource setting .
If pre-ordering is not possible , fine - tuning on a small source - target parallel corpus is sufficient to overcome word order divergence .
While the current work focused on Indian languages , we would like to validate the hypothesis on a more diverse set of languages .
We would also like to explore alternative methods to address word-order divergence which do not re-quire expensive parsing of the assisting language corpus .
Further , use of pre-ordering to address word-order divergence for multilingual training of other NLP tasks can be explored .
Table 1 : 1 Example showing transitive verb before and after reordering ( Adapted from Chatterjee et al . ( 2014 ) )
