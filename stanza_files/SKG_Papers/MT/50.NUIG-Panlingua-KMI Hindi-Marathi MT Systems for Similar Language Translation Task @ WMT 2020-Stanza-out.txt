title
NUIG - Panlingua-KMI Hindi â†” Marathi MT Systems for Similar Language Translation Task @ WMT 2020
abstract
NUIG - Panlingua -KMI submission to WMT 2020 seeks to push the state- of- the - art in the Similar language translation task for the Hindi ?
Marathi language pair .
As part of these efforts , we conducted a series of experiments to address the challenges for translation between similar languages .
Among the 4 MT systems prepared for this task , 1 PBSMT systems were prepared for Hindi ?
Marathi each and 1 NMT systems were developed for Hindi ? Marathi using Byte Pair Encoding ( BPE ) of subwords .
The results show that different architectures in NMT could be an effective method for developing MT systems for closely related languages .
Our Hindi-Marathi NMT system was ranked 8 th among the 14 teams that participated and our Marathi-Hindi NMT system was ranked 8 th among the 11 teams participated for the task .
Introduction Developing automated relations between closely related languages is a contemporary concern especially in the domain of Machine Translation ( MT ) .
Hindi and Marathi exhibit a significant overlap in their vocabularies and strong syntactic plus lexical similarities .
These striking similarities seem promising in enhancing the possibility of mutual inter-comprehension within closely related languages .
However , automated translation between such closely related languages is a rather challenging task .
The linguistic similarities and regularities in morphological variations and orthography motivate the use of character - level translation models , which have been applied to translation ( Vilar et al. , 2007 ; and transliteration ( Matthews , 2007 ; Chakravarthi et al. , 2019a ; Chakravarthi , 2020 ) .
In the past few years , neural machine translation systems have achieved outstanding performance with high resource languages , with the help of open source toolkit such as OpenNMT ( Klein et al. , 2017 ) , Marian ( Junczys - Dowmunt et al. , 2018 ) and Neamtus ( Sennrich et al. , 2017 ) , which provide various ways of experimenting with the use of different features and architectures , yet it fails to achieve the same results with low resource languages ( Chakravarthi et al. , 2018 ( Chakravarthi et al. , , 2019 b .
However , Sennrich and Zhang ( 2019 ) revisited the NMT models and tuned hyperparameters , changed network architectures to optimize NMT for low-resource conditions and concluded that low-resource NMT is very sensitive to hyper-parameters such as Byte Pair Encoding ( BPE ) vocabulary size , word dropout , and others .
This paper is an extension of our work Ojha et al . ( 2019 ) submitted to WMT 2019 similar language translation task .
Therefore our team adapted methods of the low resource setting for NMT proposed by Sennrich and Zhang ( 2019 ) to explore the following broad objectives : ? to compare the performance of SMT and NMT in case of closely related , relatively lowresourced language pairs , and ? to findout how to leverage the accuracy of NMT in closely related languages using BPE into subwords .
? to analyze the effects of data quality in performance of the systems .
System Description
This section provides an overview of the systems developed for the WMT 2020 Shared Task .
In these experiments , the NUIG - Panlingua - KMI team explored two different approaches : phrase - based statistical ( Koehn et al. , 2003 ) , and neural method for Hindi-Marathi and Marathi-Hindi language pairs .
In all the submitted systems , we use the Moses ( Koehn et al. , 2007 ) and Nematus ( Sennrich et al. , 2017 ) toolkit for developing statistical and neural machine translation systems respectively .
The preprocessing was done to handle noise in data ( for example , different language sentences , non - UTF characters etc ) , the details of which are provided in section 3.1
Phrase - based SMT Systems
These systems were built on the Moses open source toolkit using the KenLM ( Heafield , 2011 ) language model and GIZA ++ ( Och and Ney , 2003 ) aligner .
' Grow-diag-final - and heuristic ' parameters were used to extract phrases from the corresponding parallel corpora .
In addition to this , KenLM was used to build 5 - gram language models .
Neural Machine Translation System Nematus was used to build 2 NMT systems .
As we mentioned in an earlier section , at first data was pre-processed at subwords level with BPE for neural translation , and then the system was trained using Nematus toolkit .
Most of the system features were adopted from ( Sennrich et al. , 2017 ; Koehn and Knowles , 2017 ) ( see section 3.3.2 ) .
Assessment Assessment of these systems was done on the standard automatic evaluation metrics : BLEU ( Papineni et al. , 2002 ) , Rank - based Intuitive Bilingual Evaluation Score ( RIBES ) ( Isozaki et al. , 2010 ) and Translation Error Rate ( TER ) ( Snover et al. , 2006 ) .
Experiments
This section briefly describes the experiment settings for developing the systems .
Data Preparations
The parallel data-set for these experiments was provided by the WMT Similar Translation Shared Task 1 organisers and the Marathi monolingual data-set was taken from WMT 2020 Shared Task : Parallel Corpus Filtering for Low-Resource Conditions .
2
The parallel data was sub-divided into training , tuning , and monolingual sets , as detailed in Table 1 .
However , the shared data was very noisy .
To enhance the data quality , the team had to undertake an extensive pre-processing session focused on identifying and cleaning the data-sets .
Out of 43274 training sentences , the Hindi corpus had Telugu sentences while the Marathi corpus had Meitei sentences intermingled as shown in first row ( Figure 1 ) .
The parallel data had more than 1192 lines that were not comparable with each other as shown in second and third row ( Figure 1 ) , where some Hindi sentences had only half the sentences translated in Marathi ( second row ) and some had blank spaces against their Marathi counter parts ( third row ) .
The translation quality of the parallel data was also not up to mark .
In fact , the team could locate a few instances of synthetic data .
There were a few sentences where character encoding was an issue , hence were completely unintelligible .
Pre-processing
The following pre-processing steps were performed as part of the experiments : a) Both corpora were tokenized and cleaned ( sentences of length over 80 words were removed ) .
b)
For neural translation , training , validation and test data was prepossessed into subwords BPE format .
This format was utilised to prepare BPE and vocabulary further used .
All these processes were performed using Moses scripts .
However , the tokenization was done by the RGNLP team tokenizer ( Ojha et al. , 2018 ) and Indic nlp library .
3
These tokenizers were used since Moses does not provide a tokenizer for Indic languages .
Also the RGNLP tokenizer ensured that the canonical Unicode representation of the characters are retained .
Development of the NUIG - Panlingua- KMI MT Systems After removing noisy and pre-processing data , the following steps were followed to build the NUIG - Panlingua - KMI MT systems :
Building Primary MT Systems :
As previously mentioned , the Hindi-Marathi and Marathi-Hindi PBSMT systems were built as the primary submission using Moses .
The language model was built first , using KenLM .
For Marathi-Hindi and Hindi-Marathi language pairs , the lan-guage models were trained on 5 - gram .
After that , the systems were built independently and combined in a loglinear scheme in which each model was assigned a different weight using the Minimum Error Rate Training ( Och , 2003 ) tuning algorithm .
To train and tune the systems , we used 40454 and 1411 parallel sentences , respectively , for all language pairs .
Building Contrastive MT Systems :
As mentioned in the previous section , Nematus toolkit was used to develop the NMT systems .
The training was done on subword and character -level .
All the NMT experiments were carried out only with a data-set that contained sentences with length of up to 80 words .
The neural model is trained on 5000 epochs , using Adam with a default learning rate of 0.002 , dropout at 0.01 and mini-batches of 80 and the batch size for the validation was 40 .
Vocabulary size of 30000 for both Marathi-Hindi and Hindi-Marathi language pairs was extracted .
Remaining parameters were limited with the use of default hyper-parameters configuration .
Evaluation
All the systems were evaluated using the reference set provided by the shared task organizers .
The standard MT evaluation metrics , BLEU ( Papineni et al. , 2002 ) score , RIBES ( Isozaki et al. , 2010 ) and TER ( Snover et al. , 2006 ) , were used for automatic evaluation .
These results were prepared on the Primary and Contrastive system submission which are mentioned in the
Results
Overall we see varying performance among the system submitted to the task , with some performing much better out - of- sample than others .
The NUIG - Panlingua - KMI subword NMT system took 8 th position for both Hindi-Marathi and Marathi-Hindi language pair , across 14 teams .
Our subword NMT systems for Marathi-Hindi language pair showed better results in terms of all the three metrics ( 17.39 in BLEU , 58.84 in RIBES and 81.15 in TER ) while the Hindi-Marathi language pair scored 9.76 in BLEU , 52.18 in RIBES and 91.24 in TER .
Across both the language pairs , subword based NMT performed better than PBSMT as its accuracy rate was higher in BLEU and lower in TER metrics , shown in Table 2 .
Analysis
We used the reference set provided by the shared task organizers to evaluate both PBSMT and NMT systems .
Even though subword based NMT system could take advantage of the shared features among similar languages , challenges in translating a few linguistics structures acted as a constraint .
Example 1 shown in Figure 2 is one of the challenging structures that the system was unable to translate .
In these sentences the systems could not capture the correct tense and aspect which is past perfect in source sentence whereas the NMT system translated it as simple past .
The second most common challenging structures that needed special attention were the postpositions as shown in Example 2 and 3 in the figure .
In most cases , the system overgeneralised the sentences in Marathi and generated unnecessary postposition phrases in Hindi as in Example 2 .
Similarly , we can see in Example 3 while translating from Hindi to Marathi both PBSMT and NMT systems used wrong post-positions .
Conclusion
Our experiment results reveal that subword based NMT could take advantage of the relation between the similar language to boost the accuracy of neural machine translations system in low resource data settings .
As BPE units are variable - length units and the vocabularies used are much smaller than morpheme and word - level model , the problem of data sparsity does not occur .
On the contrary , it provides an appropriate context for translation between similar languages .
However , the quality of data used to train the systems does affect the quality of translation .
Thus , we could conclude that shared features between two languages could be an advantage to leverage the accuracy of NMT systems for closely related languages .
Figure 1 : 1 Figure 1 : Examples of discrepancies in Hindi-Marathi parallel data
