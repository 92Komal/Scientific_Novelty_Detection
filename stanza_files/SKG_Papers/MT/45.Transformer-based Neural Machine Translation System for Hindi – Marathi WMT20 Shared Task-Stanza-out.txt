title
Transformer - based Neural Machine Translation System for Hindi - Marathi : WMT20 Shared Task
abstract
This paper reports the results for the Machine Translation ( MT ) system submitted by the NL - PRL team for the Hindi - Marathi Similar Translation Task at WMT 2020 .
We apply the Transformer - based Neural Machine Translation ( NMT ) approach on both translation directions for this language pair .
The trained model is evaluated on the corpus provided by shared task organizers , using BLEU , RIBES , and TER scores .
There were a total of 23 systems submitted for Marathi to Hindi and 21 systems submitted for Hindi to Marathi in the shared task .
Out of these , our submission ranked 6th and 9th , respectively .
Introduction
In the last decade and a half , neural machine translation ( NMT ) ( Sutskever et al. , 2014 ) has achieved great success in automatically translating human language text , outperforming statistical machine translation ( SMT ) ( Koehn et al. , 2003 ) .
Both the system require very large corpus sizes to train and evaluate the results .
They , however , do n't work very well for low resource data ( He et al. , 2016 ; Koehn and Knowles , 2017 ; Dowling et al. , 2018 ) .
Translation from or to low resource languages is the major challenges faced by today 's NMT systems .
Different methods have been proposed to overcome the data sparsity problem for low resource languages by researchers around the world .
These include using monolingual data ( Wu et al. , 2019 ) , fine-tuning ( Miceli Barone et al. , 2017 ) the high resource monolingual and parallel data on low resource data , back translation ( Hoang et al. , 2018 ) , etc .
They succeed up to some extent , but the success is limited , as the reported results show when compared to those for resource rich languages .
In this paper , we use the Transformer networkbased NMT system ( Vaswani et al. , 2017 ) because it is among the state of the art models for machine translation .
The work reported for this shared task is an extension of the work done by ( Kumar and Singh , 2019 ) for similar languages task for 2019 , which had also used a transformer based NMT system .
Similar Languages
Two languages are considered similar or closely related if they are close relatives in terms of the linguistic family of the linguistic family tree ( or forest ) , or if the speakers of the two languages are in close contact over a long period of time .
Contact over a long period leads to the exchange of cognates and loanwords between the speakers , sometimes even grammatical constructs .
Leveraging the close similarity of languages is one way to overcome the problem of data scarcity .
Using similar features between such languages and improving translation is one of the directions for research for low resource machine translation .
For this submission , the motives behind conducting the shared task experiments are : ?
To find out whether it is advantageous to use transformer - based NMT for similar languages .
?
Whether using the SentencePiece 1 library without tokenization is beneficial for translation between similar languages or not .
Submitted System
We submitted two systems , namely , Marathi? Hindi and Hindi? Marathi .
Both are the NMT systems trained on a Transformer ( Vaswani et al. , 2017 ) network .
In this experiment , we did not tokenize data using any tokenizer .
We directly applied SentencePiece library on the corpus .
We found that directly applying SentencePiece for preprocessing of data gives a better result .
Since both the languages come under the category of morphologically rich and similar languages , directly applying SentencePiece on their corpus is advantageous .
SentencePiece breaks the sentences into morphemes and phonemes .
It extracts loanwords and cognate pairs .
Breaking of sentences into subwords helps the neural translation network to learn better translations , and to generalize this knowledge to translate and produce unseen words , partly due to jointly developing the subword vocabulary .
Data
We trained the model on total 49434 number of Hindi - Marathi parallel corpus which belongs to three domains : News , PM India and Indic WordNet .
Validation is done on total 1411 sentences .
For testing , a total of 1941 sentences were used .
Experiment setup
We used fairseq 2 sequence to sequence encoderdecoder framework to train and evaluate the system .
For hyper- parameter settings , we used the settings reported by ( Guzm ? n et al. , 2019 ) as these setting work well on low resource languages .
Table 1 gives the hyper-parameter settings .
Results
Task organizers evaluate the systems using three evaluation metric : BLEU ( Papineni et al. , 2002 ) , RIBES ( Isozaki et al. , 2010 ) Rate ( TER ) ( Snover et al. , 2006 ) .
We report the evaluation scores in table 2 .
Conclusion
In this paper , we perform experiments for translation between two similar languages : Hindi and Marathi .
We submitted two systems : Marathi?
Hindi and Hindi ?
Marathi , which were evaluated using BLEU , RIBES and TER .
We found that SentencePiece works well for similar languages because it helps the Transformer in capturing the relations between two languages by providing morphemes , phonemes , cognate pairs , loanwords , etc .
There were a total 23 systems submitted for Marathi ? Hindi and 21 systems submitted for Hindi ?
Marathi in the shared task .
Out of these , our system ranked 6th and 9th for Marathi ? Hindi and Hindi ?
Marathi , respectively , considering the BLEU scores .
Table 1 : 1 Hyperparameters used in our experiment Parameters Value Encoder and decoder layers 5 Encoder embedding dimension 512 Decoder embedding dimension 512 Encoder attention heads 2 Decoder attention heads 2 Dropout 0.4 Attention dropout 0.2 Optimizer Adam Learning rate scheduler inverse sqrt Learning rate 1e- 3 Minimum learning rate 1e- 9 Adam-betas ( 0.9 , 0.98 )
Number of epochs 100
