title
Detecting Translation Direction : A Cross-Domain Study
abstract
Parallel corpora are constructed by taking a document authored in one language and translating it into another language .
However , the information about the authored and translated sides of the corpus is usually not preserved .
When available , this information can be used to improve statistical machine translation .
Existing statistical methods for translation direction detection have low accuracy when applied to the realistic out - of- domain setting , especially when the input texts are short .
Our contributions in this work are threefold : 1 ) We develop a multi-corpus parallel dataset with translation direction labels at the sentence level , 2 ) we perform a comparative evaluation of previously introduced features for translation direction detection in a cross-domain setting and 3 ) we generalize a previously introduced type of features to outperform the best previously proposed features in detecting translation direction and achieve 0.80 precision with 0.85 recall .
Introduction Translated text differs from authored text ( Baker , 1993 ) .
The main differences are simplification , explicitation , normalization and interference ( Volansky et al. , 2013 ) .
Statistical classifiers have been trained to detect Translationese 1 . Volansky et al. ( 2013 ) state two motivations for automatic detection of Translationese : empirical validation of Translationese linguistic theories and improving statistical machine translation ( Kurokawa et al. , 2009 ) .
Most of the prior work focus on in- domain Translationese detection ( Baroni and Bernardini , 2006 ; Kurokawa et al. , 2009 ) .
That is , the training and test set come from the same , usually narrow , domain .
Cross-domain Translationese detection serves the two stated motivations better than in-domain detection .
First , automatic classification validates linguistic theories only if it works independent of the domain .
Otherwise , the classifier could perform well by memorizing lexical terms unique to a specific domain without using any linguistically meaningful generalizations .
Second , a Translationese classifier can improve statistical machine translation in two ways : 1 ) By labeling the parallel training data with translation direction 2 ; 2 ) By labeling input sentences to a decoder at translation time and use matching models .
The accuracy of the classifier is the main factor determining its impact on statistical machine translation .
Most parallel or monolingual training data sources do not contain translation direction meta-data .
Also , the input sentences at translation time can be from any domain .
Therefore , a cross-domain setting for translation direction detection is more appropriate for improving statistical machine translation as well .
We develop a crossdomain training and test data set and compare some of the linguistically motivated features from prior work ( Kurokawa et al. , 2009 ; Volansky et al. , 2013 ) in this setting .
In addition , we introduce a new bilingual feature that outperforms all prior work in both in - domain and cross-domain settings .
Our work also differs from many prior works by focusing on sentence level , rather than block level classification .
Although Kurokawa et al. ( 2009 ) compare sentence level versus block level detection accuracy , most other research focuses on block level detection ( Baroni and Bernardini , 2006 ; Volansky et al. , 2013 ) .
Sentence level classification serves the stated motivations above better than block level classification .
For empirical validation of linguistic theories , features that are detectable at the sentence level are more linguistically meaningful than block level statistics .
Sentence level detection is also more appropriate for labeling decoder input as well as some statistical machine translation training data .
In the rest of the paper , we first review prior work on sentence level and cross-domain translation direction detection .
In Section 3 we motivate the selection of features used in this study .
Next , we describe our cross-domain data set and the classification algorithm we use to build and evaluate models given a set of features .
Experimental results are presented in Section 5.2 .
2 Related Work Volansky et al. ( 2013 ) provide a comprehensive list of monolingual features used for Translationese detection .
These features include POS n-grams , character n-grams , function word frequency , punctuation frequency , mean word length , mean sentence length , word n-grams and type / token ratio .
We are aware of only one prior work that presented a crossdomain evaluation .
Koppel and Ordan ( 2011 ) use a logistic regression classifier with function word unigram frequencies to achieve 92.7 % accuracy with ten fold cross validation on the EuroParl ( Koehn , 2005 ) corpus and 86.3 % on the IHT corpus .
However testing the EuroParl trained classifier on the IHT corpus yields an accuracy of 64.8 % ( and the accuracy is 58.8 % when the classifier is trained on IHT and tested on EuroParl ) .
The classifiers in this study are trained and tested on text blocks of approximately 1500 tokens , and there is no comparative evaluation of models using different feature sets .
We are also aware of two prior works that investigate Translationese detection accuracy at the sentence level .
First Kurokawa et al ( 2009 ) periments .
For sentence level translation direction detection they reach F-score of 77 % using word n-grams and stay slightly below 70 % F-score with POS n-grams using an SVM classifier .
Second , Eetemadi and Toutanova ( 2014 ) leverage word alignment information by extracting POS tag minimal translation units ( MTUs ) ( Quirk and Menezes , 2006 ) along with an online linear classifier trained on the Hansard English - French corpus to achieve 70.95 % detection accuracy at the sentence level .
Feature Sets
The goal of our study is to compare novel and previously introduced features in a cross-domain setting .
Due to the volume of experiments required for comparison , for an initial study , we select a limited number of feature sets for comparison .
Prior works claim POS n-gram features capture linguistic phenomena of translation and should generalize across domains ( Kurokawa et al. , 2009 ; Eetemadi and Toutanova , 2014 ) .
We chose source and target POS n-gram features for n = 1 . . . 5 to test this claim .
Another feature we have chosen is from the work of Eetemadi and Toutanova ( 2014 ) where they achieve higher accuracy by introducing POS MTU 3 n-gram features .
POS MTUs incorporate source and target side information in addition to word alignment .
Prior work has also claimed lexical features such as word ngrams do not generalize across domains due to corpus specific vocabulary ( Volansky et al. , 2013 ) .
We test this hypothesis using source and target word ngram features .
Using n-grams of length 1 through 5 we run 45 ( nine data matrix entries times n-gram lengths of five ) experiments for each feature set mentioned above .
In addition to the features mentioned above , we Durrani et al. , 2014 ) .
Finally , we also include source and target Brown cluster n-grams as a comparison point to better understand their effectiveness compared to POS n-grams and their contribution to the effectiveness of Brown cluster MTUs .
Given these 8 feature types summarized in Table 1 , n-gram lengths of up to 5 and the 3 ? 3 data matrix explained in the next section , we run 360 experiments for this cross-domain study .
Data , Preprocessing and Feature Extraction
We chose the English - French language pair for our cross-domain experiments based on prior work and availability of labeled data .
Existing sentenceparallel datasets used for training machine translation systems , do not normally contain goldstandard translation direction information , and additional processing is necessary to compile a dataset with such information ( labels ) .
Kurokawa et al ( 2009 ) extract translation direction information from the English - French Hansard parallel dataset using speaker language tags .
We use this dataset , and treat the two sections " main parliamentary proceedings " and " committee hearings " as two different corpora .
These two corpora have slightly different domains , although they share many common topics as well .
We additionally choose a third corpus , whose domain is more distinct from these two , from the Eu-roParl English - French corpus .
Islam and Mehler ( 2012 ) provided a customized version of Europarl with translation direction labels , but this dataset only contains sentences that were authored in English and translated to French , and does not contain examples for which the original language of authoring was French .
We thus prepare a new dataset from EuroParl and will make it publicly available for use .
The original unprocessed version of Eu-roParl ( Koehn , 2005 ) contains speaker language tags ( original language of authoring ) for the French and English sides of the parallel corpus .
We filter out inconsistencies in the corpus .
First , we filter out sections where the language tag is missing from one or both sides .
We also filter out sections with conflicting language tags .
Parallel sections with different number of sentences are also discarded to maintain sentence alignment .
This leaves us with three data sets ( two Hansard and one EuroParl ) with translation direction information available , and which contain sentences authored in both languages .
We hold out 10 % of each data set for testing and use the rest for training .
Our 3?3 corpus data matrix consists of all nine combinations of training on one corpus and testing on another ( Table 2 ) .
Preprocessing First , we clean all data sets using the following simple techniques .
?
Sentences with low alphanumeric density are discarded . ?
A character n-gram based language detection tool is used to identify the language of each sentence .
We discard sentences with a detected language other than their label .
?
We discard sentences with invalid unicode characters or control characters .
?
Sentences longer than 2000 characters are excluded .
Next , an HMM word alignment model ( Vogel et al. , 1996 ) trained on the WMT English - French corpus ( Bojar et al. , 2013 ) word-aligns sentence pairs .
We discard sentence pairs where the word alignment fails .
We use the Stanford POS tagger ( Toutanova and Manning , 2000 ) for English and French to tag all sentence pairs .
A copy of the alignment file with words replaced with their POS tags is also generated .
French and English Brown clusters are trained separately on the French and English sides of the WMT English - French corpus ( Bojar et al. , 2013 ) .
The produced models assign cluster IDs to words in each sentence pair .
We create a copy of the alignment file with cluster IDs instead of words as well .
Feature Extraction
The classifier of our choice ( Section 5 ) extracts ngram features with n specified as an option .
In preparation for classifier training and testing , feature extraction only needs to produce the unigram features while preserving the order ( n- grams of higher length are automatically extracted by the classifier ) .
POS , word , and Brown cluster n-gram features are generated by using the respective representation for sequences of tokens in the sentences .
For POS and Brown cluster MTU features , the sequence of MTUs is defined as the left-to - right in source order sequence ( due to reordering , the exact enumeration order of MTUs matters ) .
For example , for the sentence pair in Figure 1 , the sequence of Brown cluster MTUs is : 73 ? ( 390,68 ) , 208?24 , 7689?3111 , 7321?1890 , 2?16 .
Experiments
We chose the Vowpal Wabbit ( Langford et al. , 2007 ) ( VW ) online linear classifier since it is fast , scalable and it has special ( bag of words and n-gram generation ) options for text classification .
We found that VW was comparable in accuracy to a batch logistic regression classifier .
For training and testing the classifier , we created balanced datasets with the same number of training examples in both di-rections .
This was achieved by randomly removing sentence pairs from the English to French direction until it matches the French to English direction .
For example , 636 k sentence pairs are randomly chosen from the 2,930 k sentence pairs in English to French Hansard - Committees corpus to match the number of examples in the French to English direction .
Evaluation Method
We are interested in comparing the performance of various feature sets in translation direction detection .
Performance evaluation of different classification features objectively is challenging in the absence of a downstream task .
Specifically , depending on the preferred balance between precision and recall , different features can be superior .
Ideally an ROC graph ( Fawcett , 2006 ) visualizes the tradeoff between precision and recall and can serve as an objective comparison between different classification feature sets .
However , it is not practical to present ROC graphs for 360 experiments .
Hence , we resort to the Area Under the ROC graph ( AUC ) measure as a good measure to provide an objective comparison .
Theoretically , the area under the curve can be interpreted as the probability that the classifier scores a random negative example higher than a random positive example ( Fawcett , 2006 ) .
As a point of reference , we also provide F-scores for experimental settings that are comparable to the prior work reviewed in Section 2 .
1 for experiment label description .
Results tures with n-gram lengths of 1 through 5 .
Graphs on the diagonal correspond to in-domain detection and demonstrate higher performance compared to off diagonal graphs .
This confirms the basic assumption that cross-domain translation direction detection is a more difficult task .
The overall performance is also higher when trained on the Hansard corpus and tested on Hansard - Commitee and vice versa .
This is because the Hansard corpus is more similar to the Hansard - Committees corpus compared to the EuroParl corpus .
It is also observable that the variation in performance of different features diminishes as the training and test corpora become more dissimilar .
For instance , this phenomenon can be observed on the second row of graphs where the features are most spread out when tested on the Hansard corpus .
They are less spread out when tested on the Hansard - Committees corpus , and compressed together when tested on the Eu-roParl corpus .
The same phenomenon can be observed for classifiers trained on other corpora .
For different feature types , different n-gram order of the features is best , depending on the feature granularity .
To make it easier to observe patterns in the performance of different feature types , Figure 3 shows the performance for each feature type and each train-test corpus combination as a single point , by using the best n-gram order for that feature / data combination .
Each of the 9 train / test data combinations is shown as a curve over feature types .
We can see that MTU features ( which look at both languages at the same time ) outperform individual source or target features ( POS or Brown cluster ) for all datasets .
Brown clusters are unsupervised and can provide different levels of granularity .
On the other hand , POS tags usually provide a fixed granularity and require lexicons or labeled data to train .
We see that Brown clusters outperform corresponding POS tags across data settings .
As an example , when training and testing on the Hansard corpus FRA .
BC outperforms FRA .POS by close to 20 AUC points .
Lexical features outperform monolingual POS and Brown cluster features in most settings although their advantages diminish as the training and test corpus become more dissimilar .
This is somewhat contrary to prior claims that lexical features will not generalize well across domains - we see that lexical features do capture important generalizations across domains and models that use only POS tag features have lower performance , both in and out-of- domain .
Figure 4 shows the rank of each feature amongst 107
Conclusion and Future Work From among eight studied sets of features , Brown cluster MTUs were the most effective at identifying translation direction at the sentence level .
They were superior in both in - domain and cross-domain settings .
Although English - Lexical features did not perform as well as Brown cluster MTUs , they performed better than most other methods .
In future work , we plan to investigate lexical MTUs and to consider feature sets containing any subset of the eight or more basic feature types we have considered here .
With these experiments we hope to gain further insight into the performance of feature sets in in out out - of- domain settings and to improve the state - of - the - art in realistic translation direction detection tasks .
Additionally , we plan to use this classifier to extend the work of Twitto - Shmuel ( 2013 ) by building a more accurate and larger parallel corpus labeled for translation direction to further improve SMT quality .
Figure 1 : 1 Figure 1 : POS Tagged and Brown Cluster Aligned Sentence Pairs
