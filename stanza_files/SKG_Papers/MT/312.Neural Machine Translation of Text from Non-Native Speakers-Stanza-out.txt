title
Neural Machine Translation of Text from Non-Native Speakers
abstract
Neural Machine Translation ( NMT ) systems are known to degrade when confronted with noisy data , especially when the system is trained only on clean data .
In this paper , we show that augmenting training data with sentences containing artificially - introduced grammatical errors can make the system more robust to such errors .
In combination with an automatic grammar error correction system , we can recover 1.0 BLEU out of 2.4 BLEU lost due to grammatical errors .
We also present a set of Spanish translations of the JFLEG grammar error correction corpus , which allows for testing NMT robustness to real grammatical errors .
Introduction Neural Machine Translation ( NMT ) is undeniably a success story : public benchmarks ( Bojar et al. , 2016 ) are dominated by neural systems , and neural approaches are the de facto option for industrial systems ( Wu et al. , 2016 ; Hassan Awadalla et al. , 2018 ; Crego et al. , 2016 ; Hieber et al. , 2018 ) .
Even under low-resource conditions , neural models were recently shown to outperform traditional statistical approaches ( Nguyen and Chiang , 2018 ) .
However , there are still several shortcomings of NMT that need to be addressed : a ( nonexhaustive ) list of six challenges is discussed by Koehn and Knowles ( 2017 ) , including outof-domain testing , rare word handling , the widebeam problem , and the large amount of data needed for learning .
An additional challenge is robustness to noise , both during training and at inference time .
In this paper , we study the effect of a specific type of noise in NMT : grammatical errors .
We primarily focus on errors that are made by non-native ?
Equal contribution .
Work performed at the University of Notre Dame .
source - language speakers ( as opposed to dialectal language , SMS or Twitter language ) .
Not only is this linguistically important , but we believe that it would potentially have great social impact .
Our contributions are three -fold .
First , we confirm that NMT is vulnerable to source - side noise when trained on clean data , losing up to 3.6 BLEU on our test set .
This is consistent with previous work , yet orthogonal to it , since we use more realistic noise for our experiments .
Second , we explore training methods that can deal with noise , and show that including noisy synthetic data in the training data makes NMT more robust to handling similar types of errors in test data .
Combining this simple method with an automatic grammar correction system , we find that we can recover 1.5 BLEU .
Third , we release Spanish translations of the JFLEG corpus , 1 a standard benchmark for English Grammar Error Correction ( GEC ) systems .
We also release all other data and code used in this paper .
Our additional annotations on both the JFLEG corpus and the English WMT data will enable the evaluation of the robustness of NMT systems on realistic , natural noise : a robust system would ideally produce the same output when presented with either the original or the noisy source sentence .
We hope that our datasets will become a benchmark for noise-robust NMT , because we believe that deployed systems should also be able to handle source -side noise .
Data
We focus on NMT from English to Spanish .
We choose English to be our source-side language because there exist English corpora annotated with grammar corrections , which we can use as a source of natural noise .
Moreover , since English is probably the most commonly spoken non-native language ( Lewis et al. , 2009 ) , our work could be directly applicable to several translation applications .
Our choice of Spanish as a target language enables us to have access to existing parallel data and easily create new parallel corpora ( see below , ?2.3 ) .
For all experiments , we use the Europarl English - Spanish dataset ( Koehn , 2005 ) as our training set .
In the synthetic experiments of Section ?2.2 , we use the newstest2012 and new-stest2013 as dev and test sets , respectively .
Furthermore , to test our translation methods on real grammatical errors , we introduce a new collection of Spanish translations of the JFLEG corpus ( ?2.3 ) .
Grammar Error Correction Corpora
To our knowledge , there are five publicly available corpora of non-native English that are annotated with corrections , which have been widely used for research in Grammar Error Correction ( GEC ) .
The NUS Corpus of Learner English ( NUCLE ) contains essays written by students at the National University of Singapore , corrected by two annotators using 27 error codes ( Dahlmeier et al. , 2013 ) .
It has become the main benchmark for GEC , as it was used in the CoNLL GEC Shared Tasks ( Ng et al. , , 2014 .
Other corpora include the Cambridge Learner Corpus First Certificate in English FCE corpus ( Yannakoudakis et al. , 2011 ) , which is only partially public , the Lang - 8 corpus ( Tajiri et al. , 2012 ) , which was harvested from online corrections , and the AESW 2016 Shared Task corpus , which contains corrections on texts from scientific journals .
The last corpus is the JHU FLuency - Extended GUG corpus ( JFLEG ) ( Napoles et al. , 2017 ) .
This corpus covers a wider range of English proficiency levels on the source side , and its correction annotations include extended fluency edits rather than just minimal grammatical ones .
That way , the corrected sentence is not just grammatical , but also guaranteed to be fluent .
Synthetic grammar errors Ideally , we would train a translation model to translate grammatically noisy language by training it on parallel data with grammatically noisy language .
Since , to our knowledge , no such data exist in the quantities that would be needed , an al- ternative is to add synthetic grammatical noise to clean data .
An advantage of this approach is that controlled introduction of errors allows for finegrained analysis .
This is a two-step process , similar to the methods used in the GEC literature for creating synthetic data based on confusion matrices ( Rozovskaya et al. , 2014 ; Rozovskaya and Roth , 2010 ; Xie et al. , 2016 ; Sperber et al. , 2017 ) .
First , we mimic the distribution of errors found in real data , and then introduce errors by applying rulebased transformations on automatic parse trees .
The first step involves collecting error statistics on real data .
Conveniently , the NUCLE corpus has all corrections annotated with 27 error codes .
We focus on five types of errors , with the last four being the most common in the NUCLE corpus : ? drop : randomly deleting one character from the sentence .
we obtain the probability of a noun being replaced with its singular or plural form .
For sva errors , the probability that a present tense verb is replaced with its third - person-singular ( 3SG ) or not -3SG form .
An additional sva error that we included is the confusion between the appropriate form for the verb ' to be ' in the past tense ( ' was ' and ' were ' ) .
The second step involves applying the noiseinducing transformations using our collected statistics as a prior .
We obtained parses for each sentence using the Berkeley parser ( Petrov et al. , 2006 ) .
The parse tree allows us to identify candidate error positions in each sentence ( for example , the beginning of a noun phrase without a determiner , were one could be inserted ) .
For each error type we introduced exactly one error per sentence , wherever possible , which we believe matches more realistic scenarios than previous work .
It also allows for controlled analysis of the behaviour of the NMT system ( see Section 4 ) .
For each error and each sentence , we first identify candidate positions ( based on the error type and the parse tree ) and sample one of them based on the specific error distribution statistics .
Then , we sample and introduce a specific error using the corresponding probability distribution from the confusion matrix .
( In the case of drop , nn , and sva errors , we only need to sample the position and only insert / substitute the corresponding error . )
If no candidate positions are found ( for example , a sentence does n't have a verb that can be substituted to produce a sva error ) then the sentence remains unchanged .
Following the above procedure , we added errors in our training , dev , and test set ( henceforth referred to as [ error ] ) .
Basic statistics on our produced datasets can be found in Table 2 , while example sentences are shown in Table 3 .
Furthermore , we created training and dev sets that mix clean and noisy data .
The clean + [ error ] training sets are the concatenation of each [ error ] with the clean data , effectively including a clean and a noisy version of each sentence pair .
We also created a training and dev dataset with mixed error types , in our attempt to study the effect of including all noise types during training .
The mix-all dataset includes each training pair six times : once with the original ( clean ) sentence as the source , and once for every possible error .
We experimented with a mixed dataset that included each training sentence once , with the number of noisy sentences being proportional to the real error distributions of the NUCLE dataset , but obtained results similar to the [ error ] datasets .
JFLEG -es : Spanish translations of JFLEG
The JFLEG corpus consists of a dev and test set ( no training set ) , with 747 and 754 English sentences , respectively , collected from non-native English speakers .
Each sentence is annotated with four different corrections , resulting in four ( fluent and grammatical ) reference sentences .
About 14 % of the sentences do not include any type of error , with the source and references being equivalent .
We created translations of the JFLEG corpus that allow us to evaluate how well NMT fares compared to a human translator , when presented with noisy input .
We will refer to the augmented JF - LEG corpus as JFLEG -es .
Two professional translators were tasked with producing translations for the dev and the test set , respectively .
The translators were presented only with the original erroneous sentences ; they did not have access to the correction annotations .
Spanish used in the Europarl corpus ) .
There exist cases where a translator might choose to preserve a source-side error when producing the translation , such as translation of literary works where it 's possible that grammar or fluency errors are intentional ; however , our translators were explicitly asked not to do that .
The exact instructions were as follows :
Please translate the following sentences .
Note that some sentences will have grammatical errors or typos in English .
Do n't try to translate the sentences word for word ( e.g. replicate the error in Spanish ) .
Instead , try to translate it as if it was a grammatical sentence , and produce a fluent grammatical Spanish sentence that captures its meaning .
Experiments
In this section , we provide implementation details and the results of our NMT experiments .
For convenience , we will refer to each model with the same name as the dataset it was trained on ; e.g. the mix-all model will refer to the model trained on the mix-all dataset .
Implementation Details
All data are tokenized , truecased , and split into subwords using Byte Pair Encoding ( BPE ) with 32,000 operations ( Sennrich et al. , 2016 ) .
We filter the training set to only contain sentences up to 80 words .
Our LSTM models are implemented using DyNet ( Neubig et al. , 2017 ) , and our transformer models using PyTorch ( Paszke et al. , 2017 ) .
The transformer model uses 6 layers , 8 attention heads , the dimension for embeddings and positional feedforward are 512 and 2048 respectively .
The sublayer computation sequence follows the guidelines from Chen et al . ( 2018 ) .
Dropout probability is set to 0.2 ( also in the source embeddings , following Sperber et al . ( 2017 ) ) .
We use the learning rate schedule in Vaswani et al . ( 2017 ) with warm - up steps of 24000 but only decay the learning rate until it reaches 10 ?5 as inspired by .
For testing , we select the model with the best performance on the dev set corresponding to the test set .
At inference time , we use a beam size of 4 with length normalization ( Wu et al. , 2016 ) with a weight of 0.6 .
Results
We report the results obtained with the transformer model , as they were consistently better than the LSTM one .
All the result tables for the LSTM models can be found in the Appendix .
The performance of our systems on the synthetic WMT test sets , as measured by detokenized BLEU ( Papineni et al. , 2002 ) , is summarized in Table 4 .
When the system is trained only on clean data ( first row ) and tested on noisy data , it unsurprisingly exhibits degraded performance .
We observe significant drops in the range of 1.0-3.6 BLEU .
WMT Training Set En
The largest drop ( more than 3.5 BLEU ) is observed with nn errors in the source sentence .
This is not unreasonable : nouns almost always carry content significant for translation .
Especially when translating into Spanish , a noun number change can , and apparently does , also affect the rest of the sentence significantly , for example , by influencing the conjugation of a subsequent verb .
The second- largest drop ( more than 3.0 BLEU points ) is observed in the case of drop errors .
This is also to be expected ; typos produce outof-vocabulary ( OOV ) words , which in the case of BPE are usually segmented to a most likely rarer subword sequence than the original correct word .
We find that a training regime that includes both clean and noisy sentences ( [ clean+error ) results in better systems across the board .
Importantly , these models manage to perform en par with the clean model on the clean test set .
Since the original training set is part of the [ clean + error training sets , this behavior is expected .
We conclude , thus , that including the full clean dataset during training is important for performance on clean data - one cannot just train on noisy data .
The [ clean+error ] systems exhibit a notable pattern : their BLEU scores are generally similar to the clean system on all test sets , except for the test set that matches their training set errors ( highlighted in Table 4 ) , where they generally obtain the best performance .
The mix-all model is our best system on all test sets ( except drop ) and on average .
Unlike the [ clean+error ] systems , it outperforms the clean model on all noisy test sets and not only on a specific one .
On average , using the mix-all training set leads to an improvement of 0.4 BLEU over the clean model and 0.1 ? 0.7 BLEU over the [ clean+error ] models .
Furthermore , the mix-all model exchibits the smallest performance standard deviation of all models , averaging over all test sets .
This is another indication that our system is more robust to multiple source-side variations .
We further explore this intuition in Section 4 .
On the more realistic JFLEG - es dev and test sets , we observe same trends but at a smaller scale , as shown in Table 5 .
Our mix-all model generally achieves comparable results when presented with each of the four reference corrections of the test set ( corX columns ) .
However , when we use the noisy source sentence as input ( No corr column ) our mix-all model obtains 1.4 BLEU improvements over the clean model .
The difference between the performance of the models when presented with clean and noisy input is another indicator for robustness .
On the JFLEG - es test set , the noisy source results in a ?3.1 BLEU point drop for the clean model , while the drop for our mixall model is smaller , at ?1.7 BLEU points .
In addition , we experimented with using an automatic error-corrected source as input to our sys - tem ( column Auto corr of Table 5 ) .
We used the publicly available JFLEG outputs of the ( almost ) state - of - the - art model of Junczys - Dowmunt and Grundkiewicz ( 2016 ) as inputs to our NMT system .
3
This experiment envisions a pipeline where the noisy source is first automatically corrected and then translated .
As expected , this helps the clean model ( by + 1.1 BLEU ) , but our mixall training helps even further ( by another + 0.8 BLEU ) .
Interestingly , the automatic GEC system only helps in the test set , while there are no improvements in the dev set .
Naturally , since automatic GEC systems are imperfect , the performance of this pipeline still lags behind translating on clean data .
Analysis
We attempt an in- depth analysis of the impact of the different source -side error types on the behavior of our NMT system , when trained on clean data and tested on the artificial noisy data that we created .
Art Errors
Table 6 shows the difference of the BLEU scores obtained on the sentences , broken down by the type of article error that was introduced .
The first observation is that in all cases the difference is negative , meaning that we get higher BLEU scores when testing on clean data .
Encouragingly , there is practically no difference when we substitute ' a ' with ' an ' or ' an ' with ' a ' ; the model seems to have learned very similar representations for the two indefinite articles , and as a result such an error has no impact on the produced output .
However , we observe larger performance drops when substituting indefinite articles with the definite one and vice versa ; since the target language makes the same article distinction as the source language , any article source error is propagated to the produced translation .
Prep Errors
Due to the large number of prepositions , we cannot present a full analysis of preposition errors , but highlights are shown in Table 7 .
Deleting a correct preposition or inserting a wrong one leads to performance drops of 1.2 and 0.8 BLEU points for the clean model , but drops of 0.4 and 0.7 for the mix-all model .
Nn and Sva Errors
We found no significant performance difference between the different nn errors .
Incorrectly pluralizing a noun has the same adverse effect as singularizing it , leading to performance reductions of over 4.0 and 3.5 BLEU points respectively .
We observe a similar behavior with sva errors : each error type leads to roughly the same performance degradation .
Related Work
The effect of noise in NMT was recently studied by Khayrallah and Koehn ( 2018 ) 2018 ) evaluated the robustness of word embeddings against word scrambling noise , and showed that performance in downstream tasks like POS - tagging and MT is especially hurt .
Sakaguchi et al. ( 2017a ) studied word scrambling and the Cmabrigde Uinervtisy ( Cambridge University ) effect , where humans are able to understand the meaning of sentences with scrambled words , performing word recognition ( word level spelling correction ) with a semi-character RNN system .
Focusing only on character - level NMT models , Belinkov and Bisk ( 2018 ) showed that they exhibit degraded performance when presented with noisy test examples ( both artificial and natural occurring noise ) .
In line with our findings , they also showed that slightly better performance can be achieved by training on data artificially induced with the same kind of noise as the test set .
Sperber et al. ( 2017 ) proposed a noiseintroduction system reminiscent of WER , based on insertions , deletions , and substitutions .
An NMT system tested on correct transcriptions achieves a BLEU score of 55 ( 4 references ) , but tested on the ASR transcriptions it only achieves a BLEU score of 35.7 .
By introducing similar noise in the training data , they were able to make the NMT system slightly more robust .
Interestingly , they found that the optimal amount of noise on the training data is smaller than the amount of noise on the test data .
The notion of linguistically plausible corruption is also explored by Li et al . ( 2017 ) tively ) .
When training with these noisy datasets , they obtained better performance on several text classification tasks .
Furthermore , in accordance with our results , their best system is the one that combines different types of noise .
We present a summary of relevant previous work in Table 8 . Synthetic errors refer to noise introduced according an artificially created distribution , and natural errors refer to actual errorful text produced by humans .
As for semi-natural , it refers to either noise introduced according to a distribution learned from data ( as in our work ) , or to errors that are learned from data but introduced according to an artificial distribution ( as is part of the work of Belinkov and Bisk ( 2018 ) ) .
We consider our work to be complementary to the works of Heigold et al .
( 2018 ) ; Belinkov and Bisk ( 2018 ) , and Sperber et al . ( 2017 ) .
However , there are several important differences : 1 . Belinkov and Bisk ( 2018 ) and Sperber et al . ( 2017 ) train their NMT systems on fairly small datasets : 235 K ( Fr-En ) , 210 K ( De-En ) , 122 K ( Cz-En ) , and 138 K sentences ( Es - En ) respectively .
Even though they use systems like Nematus ( Sennrich et al. , 2017 ) or XNMT ( Neubig et al. , 2018 ) Czech case ) but they substitute all possible correct words with their erroneous version , ending up with datasets with more than 40 % of the tokens being noisy .
For that reason , we refer to it as semi-natural noise in Table 8 .
Meanwhile , Sperber et al. ( 2017 ) test on the outputs of an ASR system that has a WER of 41.3 % .
For comparison , in the JF - LEG datasets , we calculated that only about 3.5 % - 5 % of the tokens are noisy - the average Levenshtein distance of a corrected reference and its noisy source is 13 characters .
3 . The word scrambling noise , albeit interesting , could not be claimed to be applicable to realistic scenarios , especially when applied to all words in a sentence .
The solution Belinkov and Bisk ( 2018 ) suggested and Sperber et al . ( 2017 ) discussed is a character - or spelling - aware model for producing word - or subword - level embeddings .
We suspect that such a solution would indeed be appropriate for dealing with typos and other characterlevel noise , but not for more general grammatical noise .
Our method could potentially be combined with GloVe ( Pennington et al. , 2014 ) or fastText ( Bojanowski et al. , 2017 ) embeddings that can deal with slight spelling variations , but we leave this for future work .
On the other side , Grammar Error Correction has been extensively studied , with significant incremental advances made recently by treating GEC as an MT task : among others , Junczys - Dowmunt and Grundkiewicz ( 2016 )
Conclusion
In this work , we studied the effect of grammatical errors in NMT .
We not only confirmed previous findings , but also expanded on them , showing that realistic human- like noise in the form of specific grammatical errors also leads to degraded performance .
We added synthetic errors on the English WMT training , dev , and test data ( including dev and test sets for all WMT 18 evaluation pairs ) , and have released them along with the scripts necessary for reproducing them .
We also produced Spanish translations of the JFLEG corpus , so that future NMT systems can be properly evaluated on real noisy data . , who explored noisy situations during training due to webcrawled data .
This type of noise includes misaligned , mistranslated , or untranslated sentences which , when used during training , significantly degrades the performance of NMT .
Unlike our
