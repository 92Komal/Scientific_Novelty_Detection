title
Bayesian Word Alignment for Statistical Machine Translation
abstract
In this work , we compare the translation performance of word alignments obtained via Bayesian inference to those obtained via expectation - maximization ( EM ) .
We propose a Gibbs sampler for fully Bayesian inference in IBM Model 1 , integrating over all possible parameter values in finding the alignment distribution .
We show that Bayesian inference outperforms EM in all of the tested language pairs , domains and data set sizes , by up to 2.99 BLEU points .
We also show that the proposed method effectively addresses the well - known rare word problem in EM - estimated models ; and at the same time induces a much smaller dictionary of bilingual word-pairs .
Introduction
Word alignment is a crucial early step in the training of most statistical machine translation ( SMT ) systems , in which the estimated alignments are used for constraining the set of candidates in phrase / grammar extraction ( Koehn et al. , 2003 ; Chiang , 2007 ; Galley et al. , 2006 ) .
State - of- the- art word alignment models , such as IBM Models ( Brown et al. , 1993 ) , HMM ( Vogel et al. , 1996 ) , and the jointly - trained symmetric HMM ( Liang et al. , 2006 ) , contain a large number of parameters ( e.g. , word translation probabilities ) that need to be estimated in addition to the desired hidden alignment variables .
The most common method of inference in such models is expectation - maximization ( EM ) ( Dempster et al. , 1977 ) or an approximation to EM when exact EM is intractable .
However , being a maxi-mization ( e.g. , maximum likelihood ( ML ) or maximum a posteriori ( MAP ) ) technique , EM is generally prone to local optima and overfitting .
In essence , the alignment distribution obtained via EM takes into account only the most likely point in the parameter space , but does not consider contributions from other points .
Problems with the standard EM estimation of IBM Model 1 was pointed out by Moore ( 2004 ) and a number of heuristic changes to the estimation procedure , such as smoothing the parameter estimates , were shown to reduce the alignment error rate , but the effects on translation performance was not reported .
Zhao and Xing ( 2006 ) note that the parameter estimation ( for which they use variational EM ) suffers from data sparsity and use symmetric Dirichlet priors , but they find the MAP solution .
Bayesian inference , the approach in this paper , have recently been applied to several unsupervised learning problems in NLP Johnson et al. , 2007 ) as well as to other tasks in SMT such as synchronous grammar induction ( Blunsom et al. , 2009 ) and learning phrase alignments directly ( DeNero et al. , 2008 ) .
Word alignment learning problem was addressed jointly with segmentation learning in Xu et al . ( 2008 ) , Nguyen et al. ( 2010 , and Chung and Gildea ( 2009 ) .
The former two works place nonparametric priors ( also known as cache models ) on the parameters and utilize Gibbs sampling .
However , alignment inference in neither of these works is exactly Bayesian since the alignments are updated by running GIZA + + ( Xu et al. , 2008 ) or by local maximization ( Nguyen et al. , 2010 ) .
On the other hand , Chung and Gildea ( 2009 ) apply a sparse Dirichlet prior on the multinomial parameters to prevent overfitting .
They use variational Bayes for inference , but they do not investigate the effect of Bayesian inference to word alignment in isolation .
Recently , Zhao and Gildea ( 2010 ) proposed fertility extensions to IBM Model 1 and HMM , but they do not place any prior on the parameters and their inference method is actually stochastic EM ( also known as Monte Carlo EM ) , a ML technique in which sampling is used to approximate the expected counts in the E-step .
Even though they report substantial reductions in alignment error rate , the translation BLEU scores do not improve .
Our approach in this paper is fully Bayesian in which the alignment probabilities are inferred by integrating over all possible parameter values assuming an intuitive , sparse prior .
We develop a Gibbs sampler for alignments under IBM Model 1 , which is relevant for the state - of - the - art SMT systems since : ( 1 ) Model 1 is used in bootstrapping the parameter settings for EM training of higherorder alignment models , and ( 2 ) many state - of- theart SMT systems use Model 1 translation probabilities as features in their log-linear model .
We evaluate the inferred alignments in terms of the end-toend translation performance , where we show the results with a variety of input data to illustrate the general applicability of the proposed technique .
To our knowledge , this is the first work to directly investigate the effects of Bayesian alignment inference on translation performance .
Bayesian Inference with IBM Model 1 Given a sentence - aligned parallel corpus ( E , F ) , let e i ( f j ) denote the i-th ( j- th ) source ( target ) 1 word in e ( f ) , which in turn consists of I ( J ) words and denotes the s-th sentence in E ( F ) .
2 Each source sentence is also hypothesized to have an additional imaginary " null " word e 0 .
Also let V E ( V F ) denote the size of the observed source ( target ) vocabulary .
In Model 1 ( Brown et al. , 1993 ) , each target word 1 We use the " source " and " target " labels following the generative process , in which E generates F ( cf. Eq. 1 ) .
2 Dependence of the sentence- level variables e , f , I , J ( and a and n , which are introduced later ) on the sentence index s should be understood even though not explicitly indicated for notational simplicity .
f j is associated with a hidden alignment variable a j whose value ranges over the word positions in the corresponding source sentence .
The set of alignments for a sentence ( corpus ) is denoted by a ( A ) .
The model parameters consist of a V E ? V F table T of word translation probabilities such that t e , f = P ( f |e ) .
The joint distribution of the Model - 1 variables is given by the following generative model 3 : P ( E , F , A ; T ) = s P ( e) P ( a|e ) P ( f |a , e ; T ) ( 1 ) = s P ( e ) ( I + 1 ) J J j=1 t ea j , f j ( 2 ) In the proposed Bayesian setting , we treat T as a random variable with a prior P ( T ) .
To find a suitable prior for T , we re-write ( 2 ) as : P ( E , F , A|T ) = s P ( e ) ( I + 1 ) J V E e=1 V F f =1 ( t e , f ) n e , f ( 3 ) = V E e=1 V F f =1 ( t e , f ) N e , f s P ( e ) ( I + 1 ) J ( 4 ) where in ( 3 ) the count variable n e ,f denotes the number of times the source word type e is aligned to the target word type f in the sentence - pair s , and in ( 4 ) N e , f = s n e , f .
Since the distribution over {t e , f } in ( 4 ) is in the exponential family , specifically being a multinomial distribution , we choose the conjugate prior , in this case the Dirichlet distribution , for computational convenience .
For each source word type e , we assume the prior distribution for t e = t e,1 ? ? ?
t e, V F , which is itself a distribution over the target vocabulary , to be a Dirichlet distribution ( with its own set of hyperparameters ?
e = ? e,1 ? ? ? ? e, V F ) independent from the priors of other source word types : t e ? Dirichlet ( t e ; ? e ) f j |a , e , T ? Multinomial ( f j ; t ea j )
We choose symmetric Dirichlet priors identically for all source words e with ?
e , f = ? = 0.0001 to obtain a sparse Dirichlet prior .
A sparse prior favors distributions that peak at a single target word and penalizes flatter translation distributions , even for rare words .
This choice addresses the well -known problem in the IBM Models , and more severely in Model 1 , in which rare words act as " garbage collectors " ( Och and Ney , 2003 ) and get assigned excessively large number of word alignments .
Then we obtain the joint distribution of all ( observed + hidden ) variables as : P ( E , F , A , T ; ? ) = P ( T ; ? ) P ( E , F , A|T ) ( 5 ) where ? = ? 1 ? ? ? ? V E .
To infer the posterior distribution of the alignments , we use Gibbs sampling ( Geman and Geman , 1984 ) .
One possible method is to derive the Gibbs sampler from P ( E , F , A , T ; ? ) obtained in ( 5 ) and sample the unknowns A and T in turn , resulting in an explicit Gibbs sampler .
In this work , we marginalize out T by : P ( E , F , A ; ? ) = T P ( E , F , A , T ; ? ) ( 6 ) and obtain a collapsed Gibbs sampler , which samples only the alignment variables .
Using P ( E , F , A ; ? ) obtained in ( 6 ) , the Gibbs sampling formula for the individual alignments is derived as : 4 P ( a j = i|E , F , A ?j ; ?) = N ?j e i , f j + ? e i , f j V F f =1 N ?j e i , f + V F f =1 ? e i , f ( 7 ) where the superscript ?j denotes the exclusion of the current value of a j .
The algorithm is given in Table 1 . Initialization of A in Step 1 can be arbitrary , but for faster convergence special initializations have been used , e.g. , using the output of EM ( Chiang et al. , 2010 ) .
Once the Gibbs sampler is deemed to have converged after B burn - in iterations , we collect M samples of A with L iterations in - between 5 to estimate P ( A|E , F ) .
To obtain the Viterbi alignments , which are required for phrase extraction ( Koehn et al. , 2003 ) , we select for each a j the most frequent value in the M collected samples .
Input : E , F ; Output : K samples of A 1 Initialize A 2 for k = 1 to K do 3 for each sentence - pair s in ( E , F ) do 4 for j = 1 to J do 5 for i = 0 to I do 6 Calculate P ( a j = i| ? ? ? ) according to ( 7 ) 7 Sample a new value for a j Table 1 : Gibbs sampling algorithm for IBM Model 1 ( implemented in the accompanying software ) .
Experimental Setup For Turkish ?
English experiments , we used the 20K - sentence travel domain BTEC dataset ( Kikui et al. , 2006 )
For each language pair , we trained standard phrase - based SMT systems in both directions ( including alignment symmetrization and log-linear model tuning ) using Moses ( Koehn et al. , 2007 ) , SRILM ( Stolcke , 2002 ) , and ZMERT ( Zaidan , 2009 ) tools and evaluated using BLEU ( Papineni et al. , 2002 ) .
To obtain word alignments , we used the accompanying Perl code for Bayesian inference and GIZA ++ ( Och and Ney , 2003 ) for EM .
For each translation task , we report two EM estimates , obtained after 5 and 80 iterations ( EM - 5 and EM - 80 ) , respectively ; and three Gibbs sampling estimates , two of which were initialized with those two EM Viterbi alignments ( GS - 5 and GS - 80 ) and a third was initialized naively 9 ( GS - N ) .
Sampling settings were B = 400 for T?E , 4000 for C?E and 8000 for A?E ; M = 100 , and L = 10 .
For reference , we also report the results with IBM Model 4 alignments ( M4 ) trained in the standard bootstrapping regimen of 1 5 H 5 3 3 4 3 .
Results
Table 2 compares the BLEU scores of Bayesian inference and EM estimation .
In all translation tasks , Bayesian inference outperforms EM .
The improvement range is from 2.59 ( in Turkish - to- English ) up to 2.99 ( in English - to- Turkish ) BLEU points in travel domain and from 0.16 ( in English - to - Czech ) up to 0.85 ( in English -to- Arabic ) BLEU points in news domain .
Compared to the state- of- the- art IBM Model 4 , the Bayesian Model 1 is better in all travel domain tasks and is comparable or better in the news domain .
Fertility of a source word is defined as the number of target words aligned to it .
inates " excessive " alignments ( fertility ? 8 ) 10 .
The number of distinct word-pairs induced by an alignment has been recently proposed as an objective function for word alignment ( Bodrumlu et al. , 2009 ) .
Small dictionary sizes are preferred over large ones .
Table 4 shows that the proposed inference method substantially reduces the alignment dictionary size , in most cases by more than 50 % .
Conclusion
We developed a Gibbs sampling - based Bayesian inference method for IBM Model 1 word alignments and showed that it outperforms EM estimation in terms of translation BLEU scores across several language pairs , data sizes and domains .
As a result of this increase , Bayesian Model 1 alignments perform close to or better than the state - of- the- art IBM Model 4 .
The proposed method learns a compact , sparse translation distribution , overcoming the wellknown " garbage collection " problem of rare words in EM - estimated current models .
Table 3 3 Each target word was aligned to the source candidate that co-occured the most number of times with that target word in the entire parallel corpus .
shows the distribution of fertilities in alignments obtained from different methods .
Compared to EM estimation , in - cluding Model 4 , the proposed Bayesian inference dramatically reduces " questionable " high-fertility ( 4 ? fertility ? 7 ) alignments and almost entirely elim - 9
