title
Cheap Translation for Cross-Lingual Named Entity Recognition
abstract
Recent work in NLP has attempted to deal with low-resource languages but still assumed a resource level that is not present for most languages , e.g. , the availability of Wikipedia in the target language .
We propose a simple method for crosslingual named entity recognition ( NER ) that works well in settings with very minimal resources .
Our approach makes use of a lexicon to " translate " annotated data available in one or several high resource language ( s ) into the target language , and learns a standard monolingual NER model there .
Further , when Wikipedia is available in the target language , our method can enhance Wikipedia based methods to yield state - of - the - art NER results ; we evaluate on 7 diverse languages , improving the state - of - the - art by an average of 5.5 % F1 points .
With the minimal resources required , this is an extremely portable crosslingual NER approach , as illustrated using a truly low-resource language , Uyghur .
Introduction
In recent years , interest in the natural language processing ( NLP ) community has expanded to include multilingual applications .
Although this uptick of interest has produced diverse annotated corpora , most languages are still classified as lowresource .
In order to build NLP tools for lowresource languages , we either need to annotate data ( a costly exercise , especially for languages with few native speakers ) , or find a way to use annotated data in other languages in service to the cause .
We refer to the latter techniques as crosslingual techniques .
In this paper , we address cross-lingual named 2 show that this improvement continues to a wide variety of languages .
The baseline is a simple direct transfer model .
The previous state- of- the-art ( SOA ) is entity recognition ( NER ) .
Prior methods ( described in detail in Section 2 ) depend heavily on limited and expensive resources such as Wikipedia or large parallel text .
Concretely , there are about 3800 written languages in the world .
1 Wikipedia exists in about 280 languages , but most versions are too sparse to be useful .
Parallel text may be found on an ad-hoc basis for some languages , but it is hardly a general solution .
Religious texts , such as the Bible and the Koran , exist in many languages , but the unique domain makes them hard to use .
This leaves the vast majority of the world 's languages with no general method for NER .
We propose a simple solution that requires only minimal resources .
We translate annotated data in a high- resource language into a low-resource language , using just a lexicon .
2
We refer to this as cheap translation , because in general , lexicons are much cheaper and easier to find than parallel text ( Mausam et al. , 2010 ) .
One of the biggest efforts at gathering lexicons is Panlex ( Kamholz et al. , 2014 ) , which has lexicons for 10,000 language varieties available to download today .
The quality and size of these dic-tionaries may vary , but in Section 5.3 we showed that even small dictionaries can give improvements .
If there is no dictionary , or if the quality is poor , then the Uyghur case study outlined in Section 6 suggests that effort is best spent in developing a high-quality dictionary , rather than gathering questionable -quality parallel text .
We show that our approach gives non-trivial scores across several languages , and when combined with orthogonal features from Wikipedia , improves on state - of - the - art scores .
Table 1 compares a simple direct transfer baseline , the previous state - of - the - art in cross-lingual NER , and our proposed algorithm .
For these languages , we beat the baseline by 25.4 points , and the state - of - the - art by 5.9 points .
In addition , we found that translating from a language related to the target language gives a further boost .
We conclude with a case study of a truly low-resource language , Uyghur , and show a good score , despite having almost no target language resources .
Related Work
There are two main branches of work in crosslingual NLP : projection across parallel data , and language independent methods .
Projection Projection methods take a parallel corpus between source and target languages , annotate the source side , and push annotations across learned alignment edges .
Assuming that source side annotations are of high quality , success depends largely on the quality of the alignments , which depends , in turn , on the size of the parallel data .
There is work on projection for POS tagging ( Yarowsky et al. , 2001 ; Das and Petrov , 2011 ; Duong et al. , 2014 ) , NER ( Wang and Manning , 2014 ; Kim et al. , 2012 ; Ehrmann et al. , 2011 ; Ni and Florian , 2016 ) , mention detection ( Zitouni and Florian , 2008 ) , and parsing ( Hwa et al. , 2005 ; Mc - Donald et al. , 2011 ) .
For NER , the received wisdom is that parallel projection methods work very well , although there is no consensus on the necessary size of the parallel corpus .
Most approaches require millions of sentences , with a few exceptions which require thousands .
Accordingly , the drawback to this approach is the difficulty of finding any parallel data , let alone millions of sentences .
Religious texts ( such as the Bible and the Koran ) exist in a large number of languages , but the domain is too far removed from typical target domains ( such as newswire ) to be useful .
As a simple example , the Bible contains almost no entities tagged as ' organization ' .
We approach the problem with the assumption that little to no parallel data is available .
Language Independent
The second common tool for cross-lingual NLP is to use language independent features .
This is often called direct transfer , in the sense that a model is trained on one language and then applied without modification on a dataset in a different language .
Lexical or lexical - derived features are typically not used unless there is significant vocabulary overlap between languages .
experiments with direct transfer of dependency parsing and NER , and showed that using word cluster features can help , especially if the clusters are forced to conform across languages .
The cross-lingual word clusters were induced using large parallel corpora .
Building on this work , T?ckstr ? m ( 2012 ) focuses solely on NER , and includes experiments on self-training and multi-source transfer for NER .
link words and phrases to entries in Wikipedia and use page categories as features .
They showed that these wikifier features are strong language independent features .
We build on this work , and use these features in our experiments .
Bharadwaj et al. ( 2016 ) build a transfer model using phonetic features instead of lexical features .
These features are not strictly languageindependent , but can work well when languages share vocabulary but with spelling variations , as in the case of Turkish , Uzbek , and Uyghur .
Others
In a technique similar to ours , Carreras et al . ( 2003 ) use Spanish resources for Catalan NER .
They translate the features in the weight vector , which has the flavor of a language independent model with the lexical features of a projection model .
Our work is a natural extension of this paper , but explores these techniques on many more languages , showing that with some modifications , it has a broad applicability .
Further , we experiment with orthogonal features , and with combining multiple source languages to get state of the art results on standard datasets .
Irvine and Callison- Burch ( 2016 ) build a machine translation system for low-resource languages by inducing bilingual dictionaries from monolingual texts .
Koehn and Knight ( 2001 ) experiment with varying knowledge levels on the task of translating German nouns in a small parallel German - English corpus .
A lexicon along with monolingual text can correctly translate 79 % of the nouns in the evaluation set .
They reach a score of 89 % when a parallel corpus is available along with a lexicon , but also comment on the scarcity of parallel corpora .
The main takeaways from the viewpoint of our work are a ) word level translation can be effective , at least for nouns , and b ) obtaining the correct word pair is more difficult than choosing between a set of options .
Our method : Cheap Translation
We create target language training data by translating source data into the target language .
It is effectively the same as standard phrase - based statistical machine translation systems ( such as MOSES ( Koehn et al. , 2007 ) ) , except that the translation table is not induced from expensive parallel text , but is built from a lexicon , hence the name cheap translation .
The entries in our lexicon contain word- to - word translations , as well as word - to - phrase , phraseto-word , and phrase - to - phrase translations .
Entries typically do not have any further information , such as part of speech or sense disambiguation .
The standard problems related to ambiguity in language apply : a source language word may have several translations , and several source language words may have the same translation .
We are mostly concerned with the problem of multiple translations of a source language word .
For example , in the English - Spanish lexicon , the English word woman translates into about 50 different words , with meanings ranging from woman , to female golfer , to youth .
Although all candidates might be technically correct , we are interested in the most prominent translation .
To estimate this , we gathered cooccurrence counts of each sourcetarget word pair in the lexicon .
For Spanish , in the case of woman , the most probable translation is mujer , because it shows up in other contexts in the dictionary , such as farm woman or young woman , whereas translations such as joven cooccur infrequently with woman .
We normalize these cooc- Algorithm 1 Our translation algorithm Input D E : Annotated data in E L : Lexicon between E-F Returns D F : Annotated data in F 1 : for ?w i ?
D E do 2 : p = w i w i +1 ...w i+ j
Window of size j 3 : while p not in L and j ?
0 do end while 7 : if p in L then 8 : if | L [ p ] | > 1 then 9 : resolve with LM and prominence 10 : end if 11 : D F add ( L [ p ] , labels of p ) 12 : else 13 : D F add ( w i , label of w i ) 14 : end if 15 : Increment i by length of p 16 : end for currence counts in each candidate set , and call this the prominence score .
With these probabilities in hand , we have effectively constructed a phrase translation table .
We use a simple greedy decoding method ( as shown in Algorithm 1 ) where options from the lexicon are resolved by a language model multiplied by the prominence score of each option .
We use SRILM ( Stolcke et al. , 2002 ) trained on Wikipedia ( although any large monolingual corpus will do ) .
During decoding , once we have chosen a candidate , we copy all labels from the source phrase to the target phrase .
Since the translation is phraseto- phrase , we can copy gold labels directly , 3 without worrying about getting good alignments .
The result is annotated data in the target language .
Notice that the algorithm allows for no reordering beyond what exists in the phrase - to - phrase entries of the lexicon .
Compared to phrasetables learned from massive parallel corpora , our lexicon - based phrase tables are not large enough or expressive enough for robust reordering .
We leave explorations of reordering to future work .
See Figure 1 for a representative example of translation from English to Turkish , with a human translation as reference .
There are sin -
Experimental Setup
Before we describe our experiments , we describe some of the tools we used .
Lexicons
We use lexicons provided by ( Rolston and Kirchhoff , 2016 ) , which are harvested from PanLex , Wiktionary , and various other sources .
There are 103 lexicons , each mapping between English and a target language .
These vary in size from 56 K entries to 1.36 M entries , as shown in the second row of Table 2 .
There are also noisy translations .
Some entries consist of a single English letter , some are morphological endings , others are misspellings , others are obscure translations of metaphors , and still others are just wrong .
Datasets
We use data from CoNLL2002/2003 shared tasks ( Tjong Kim Sang , 2002 ; Tjong Kim Sang and De Meulder , 2003 ) .
The 4 languages represented are English , German , Spanish , and Dutch .
All training is on the train set , and testing is on the test set ( TestB ) .
The evaluation metric for all experiments is phrase level F1 , as explained in Tjong Kim Sang ( 2002 ) .
In order to experiment on a broader range of languages , we also use data from the RE - FLEX ( Simpson et al. , 2008 ) , and LORELEI projects .
From LORELEI , we use Turkish and Hausa 4 From REFLEX , we use Bengali , Tamil , and Yoruba .
5
We use the same set of test documents as used in .
We also use Hindi and Malayalam data from FIRE 2013 , 6 pre-processed to contain only PER , ORG , and LOC tags .
While several of these languages are decidedly high- resource , we limit the resources used in order to show that our techniques will work in truly low-resource settings .
In practice , this means generating training data where high-quality manually annotated data is already available , and using dictionaries where translation is available .
NER Model
In all of our work we use the Illinois NER system ( Ratinov and Roth , 2009 ) with standard features ( forms , capitalization , affixes , word prior , word after , etc. ) as our base model .
We train Brown clusters on the entire Wikipedia dump for any given language ( again , any monolingual corpus will do ) , and include the multilingual gazetteers and wikifier features proposed in .
Experiments
We performed two different sets of experiments : first translating only from English , then translating from additional languages selected to be similar to the target language .
Translation from English
We start by translating from the highest resourced language in the world , English .
We first show that our technique gives large improvement over a simple baseline , then combine with orthogonal features , then compare against a ceiling obtained with Google Translate .
Baseline Improvement
To get the baseline , we trained a model on English CoNLL data ( train set ) , and applied the model directly to the target language , mismatching lexical features notwithstanding .
We did not use gazetteers in this approach .
For the non-Latin script languages , Tamil and Bengali , we transliterated the entire English corpus into the target script .
These results are in Table 2 , row " Baseline " .
In our approach ( " Cheap Translation " ) , for each test language , we translated the English CoNLL data ( train set ) into that language .
The first row of Table 2 shows the coverage of each dictionary .
For example , in the case of Spanish , 90.94 % of the words were translated into Spanish .
This gives an average of 14.6 points F1 improvement over the baseline .
This shows that simple translation is surprisingly effective across the board .
The improvement is most noticeable for Bengali and Tamil , which are languages with non-Latin script .
This mostly shows that the trivial baseline does n't work across scripts , even with transliteration .
Spanish shows the least improvement over the baseline , which may be because English and Spanish are so similar that the baseline is already high .
We found that we needed to normalize the Yoruba text ( that is , remove all pronunciation symbols on vowels ) in order to make the data less sparse .
Since the training data for Bengali and Tamil never shares a script with the test data , we omit using the word surface form as a feature .
This is indicated by the ? in Table 2 . Brown clusters , which implicitly use the word form , are still used .
Wikifier Features
Now we show that our approach is also orthogonal to other approaches , and can be combined with great effect .
Wikifier features are obtained by grounding words and phrases to English Wikipedia pages , and using the categories of the linked page as NER features for the surface text .
Our approach can be naturally combined with wikifier features .
We show results in Table 2 , in the row marked ' Cheap Translation + Wiki ' .
Using wikifier features improves scores for all 7 languages .
Further , for all languages we beat , with an average of 3.92 points F1 improvement .
For the three European languages ( Dutch , German , and Spanish ) , we have an average improvement of 4.8 points F1 over .
This may reflect the fact that English is more closely related to European languages than Indian or African languages , in terms of lexical similarities , word order , and spellings and distribution of named entities .
This suggests that it is advantageous to select a source language similar to the target language ( by some definition of similar ) .
We explore this hypothesis in Section 5.2 .
Google Translate
Since we are performing translation , we compared against a high-quality translation system to get a ceiling .
We used Google Translate to translate the English CoNLL training data into the target language , sentence by sentence .
We aligned the source-target data using fast align ( Dyer et al. , 2013 ) , and projected labels across alignments .
7 Since this is high-quality translation , we treat it as an upper bound on our technique , but with the caveat that the alignments can be noisy given the relatively small amount of text .
This introduces a source of noise that is not present in our technique , but the loss from this noise is small compared to the gain from the high-quality translation .
As with the other approaches , we found that Brown cluster features were an important signal .
Surprisingly , Google Translate beats our basic approach with a margin of only 4.3 points .
Despite the na?vete of our approach , we are relatively close to the ceiling .
Further , Google Translate is limited to 103 languages , whereas our approach is limited only by available dictionaries .
In lowresource settings , such as the one presented in Section 6 , Google Translate is not available , but dictionaries are available , although perhaps only by pivoting through a high- resource language .
Translation from Similar Languages
Observing that English as a source works well for European languages , but not as well for non-European languages , we form a key hypothesis : cheap translation between similar languages should be better than between different languages .
There are several reasons for this .
First , similar languages should have similar word orderings .
Since we do no reordering in translation , this means the target text has a better chance of a Table 3 : This shows the language selection results .
In each row , we see the target language , and the languages used for training .
For example , when testing on Dutch , we train on German and English .
These scores came from WALS .
coherent ordering .
Second , in case of dictionary misses , vocabulary common between languages will be correct in the target language .
This requires two new resources : annotated data in a similar language S , and a lexicon that maps from S to T , the target language .
Data in other languages
For most target languages , English is not the closest language , and it is likely that there exists an annotated dataset in a closer language .
There are annotated datasets available in many languages with a diversity of script and family .
We have datasets annotated in about 10 different languages , although more exist .
One caveat is that the source dataset must have a matching tagset with the target dataset .
At present , we accept this as a limitation , with the understanding that there is a common set of coarse- grained tags that is widely used ( PER , ORG , LOC ) .
We leave further exploration to future work .
Pivoting Lexicons
Although we cannot expect to find lexicons between all pairs of languages , we can usually expect that a language will have at least one lexicon with a high- resource language .
Often that language is English .
We can use this high- resource language as a pivot to transitively create an S-T dictionary , although perhaps with some loss of precision .
Assume we want a Turkish -Bengali lexicon and we have only English -Bengali and English - Turkish lexicons .
We collect all English words that appear in both dictionaries .
Each such English word has two sets of candidate translations , one set in Turkish , the other in Bengali .
To create transitive pairs , we take the Cartesian product of these two sets of candidate translations .
This will create too many entries , some of which will be incorrect , but usually the correct entry is there .
Notice also that the resulting dictionary contains only those English words that appear in both original dictionaries .
If either of the original dictionaries is small , the result will be smaller still .
Source Selection and Combination
To choose a related source language , we used syntactic features of languages retrieved from the World Atlas of Language Structures ( WALS ) ( Dryer and Haspelmath , 2013 ) .
Each language is represented as a binary vector with each index indicating presence or absence of a syntactic feature in that language .
We used the feature set called syntax knn , which includes 103 syntactic features , such as subject before verb , and possessive suffix , and uses k-Nearest Neighbors to predict missing values .
We measure similarity as cosine distance between language vectors .
In the absence of criteria for a similarity cutoff , we chose to include only the top most similar language as source for that target language .
The results of this similarity calculation are shown in Table 3 .
For example , when the target language is Dutch , German is the closest .
We also included English in the training , as the highest resource language , and with the highest quality dictionaries .
Results
Our results are in Table 2 , in the row named ' Best Combination ' .
The average over all languages surpasses the English- source average by 5.4 points , and also beats .
We also add wikifier features , and report results in row ' Best Com-bination + Wiki . '
This shows improvement on all but Spanish , with an average improvement of 5.58 points F1 over .
To the best of our knowledge , these are the best cross -language settings scores for all these datasets .
While these scores are lower than those seen on typical NER tasks ( 70 - 90 % F1 ) , we emphasize first that cross-lingual scores will necessarily be much lower than monolingual scores , and second that these are the best available given the setup .
Dictionary Ablation
The most expensive resource we require is a lexicon .
In this section , we briefly explore what effect the size of the lexicon has on the end result .
Using Turkish , we vary the size of the dictionary by randomly removing entries .
The sizes vary from no entries to full dictionary ( rows ' Baseline ' and ' Cheap Translation ' in Table 2 , respectively ) , with several gradations in the middle .
With each reduced dictionary , we translate from English to generate Turkish training data as in Section 5.1 .
As before , we train an NER model on the generated data , and test on the Turkish test data .
Results are shown in Figure 2 .
Interestingly , we see improvement over the baseline even with only 500 entries .
This improvement continues until 125 K entries .
It is important to note that only a small number of dictionary entries - words that typically show up in the contexts of named entities , such as president , university or town - are likely to be useful .
The larger the dictionary , the more likely these valuable entries are present .
Further , our random removal process may unfairly prioritize less common words , compared to a manually compiled dictionary which would prioritize common words .
It is likely that a small Figure 2 : Effect of dictionary size on F1 score for Turkish .
Each column is an experiment with a randomly reduced dictionary .
The orange bars represent how much of the corpus is translated .
but carefully constructed manual dictionary could have a large impact .
6 Case study : Uyghur
We have shown in the previous sections that our method is effective across a variety of languages .
However , all of the tested languages have some resources , most notably , Google Translate and reasonably sized Wikipedias .
In this section , we show that our methods hold up on a truly low-resource language , Uyhgur .
Uyghur is a language native to northwest China , with about 25 million speakers .
8
It is a Turkic language , and is related most closely to Uzbek , although it uses an Arabic writing system .
Uyghur is not supported by Google Translate , and the Uyghur Wikipedia has less than 3,000 articles .
In contrast , the smallest Wikipedia size language in our test set is Yoruba , with 30 K articles .
Because of the small Wikipedia size , we do not use any wikifier features .
We did this work as part of the NIST LoReHLT evaluation in the summer of 2016 .
The official evaluation scores were calculated over a set of 4500 Uyghur documents .
Each team was given the unannotated version of those documents , with the task being to submit annotations on that set .
Our official scores are reported in Table 4 , and compared with Bharadwaj et al . ( 2016 ) .
After the evaluation , NIST released 199 of the annotated evaluation documents , called the unse - questered set .
In this section , we will drill into the various methods we used to build the transfer model , and report finer - grained results using the unsequestered set .
The following are some of the language -specific techniques we employed .
?
Dictionary
The dictionary provided for Uyghur from Rolston and Kirchhoff ( 2016 ) had only 5 K entries , so we augmented this with the dictionary provided in the LORELEI evaluation , which resulted in 116 K entries .
?
Name Substitution
As with Bengali and Tamil , very few names were translated .
We found transliteration models were too noisy , so instead , we gathered a list of gazetteers from Uyghur Wikipedia , categorized by tag type ( PER , LOC , GPE , ORG ) .
Upon encountering an untranslatable NE , we replaced it with a randomly selected NE from the gazetteer list corresponding to the tag .
This led to improbable sentences like John Kerry has joined the Baskin Robbins , but it meant that NEs were fluent in the target text .
?
Stemming
We created a very simple stemmer for Uyghur .
This consists of 45 common suffixes sorted longest first .
For each Uyghur word in a corpus , we removed all possible suffixes ( Uyghur words can take multiple suffixes ) .
We stemmed all train and test data .
We report results in Table 5 .
The first row is from a monolingual model trained on 158 documents in the unsequestered set , and tested on the remaining 41 .
All other rows test on the complete unsequestered set .
The next section , ' Standard Translation ' , refers to the method described above .
Notably , we do not use stemming for train or test data here .
As with Bengali and Tamil , we omit form features .
We translate from English , Turkish , and Uzbek , which are the closest languages predicted by WALS .
Next , we incorporated language specific methods .
The scores we get from training on English , Turkish and Uzbek all go up because the stemming makes the features more dense .
Next we generated dictionaries using observations over Uyghur and Uzbek , and we used non-native speakers to annotate Uyghur data .
Language Specific Dictionary Induction
We began by romanizing Uyghur text into the Uyghur Latin alphabet ( ULY ) so we could read it .
We noticed that Uzbek and Uyghur are very similar , sharing a sizable amount of vocabulary , and several morphological rules .
However , while there is a shared vocabulary , the words are usually spelled slightly differently .
For example , the word for " southern " is " janubiy " in Uzbek and " jenubiy " in Uyghur .
We tried several ideas for gathering a mapping for this shared vocabulary : manual mapping , editdistance mapping , and cross-lingual CCA with word vectors .
Manual mapping :
We manually translated about 100 words often found around entities , such as president , and university Edit-distance mapping :
We gathered ( Uyghur , Uzbek ) word pairs with low-edit distance , using a modified edit-distance algorithm that allowed cer-tain substitutions at zero cost .
For example , this discovered such pairs as pokistan- pakistan and telegraph -t?l? graf .
Cross-lingual CCA with word vectors :
We projected Uyghur and Uzbek monolingual vectors into a shared semantic space , using CCA ( Faruqui and Dyer , 2014 ) .
We used the list of low editdistance word pairs as the dictionary for the projection .
Once all the vectors were in the same space , we found the closest Uyghur word to each Uzbek word .
Results Scores are in Table 5 .
Interestingly , the language specific methods evaluated individually did not improve much over the generic word translation methods .
But with all language specific methods combined , ' All Lang .
Spec. ' , the score increased by nearly 10 points , suggesting that the different training data covers many angles .
To the best of our knowledge , there are no published scores on the unsequestered data set .
Our best score is comparable to the score of our evaluation submission on the unsequestered dataset .
Conclusion
We have shown a novel cross-lingual method for generating NER data that gives significant improvement over state - of - the - art on standard datasets .
The method benefits from annotated data in many languages , combines well with orthogonal features , and works even when resources are virtually nil .
The simplicity and minimal use of resources makes this approach more portable than all previous approaches .
p = w i w i + 1 ...w i+ j 6 :
