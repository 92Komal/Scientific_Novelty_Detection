title
Are Two Heads Better than One ?
Crowdsourced Translation via a Two -Step Collaboration of Non-Professional Translators and Editors
abstract
Crowdsourcing is a viable mechanism for creating training data for machine translation .
It provides a low cost , fast turnaround way of processing large volumes of data .
However , when compared to professional translation , naive collection of translations from non-professionals yields low-quality results .
Careful quality control is necessary for crowdsourcing to work well .
In this paper , we examine the challenges of a two-step collaboration process with translation and post-editing by non-professionals .
We develop graphbased ranking models that automatically select the best output from multiple redundant versions of translations and edits , and improves translation quality closer to professionals .
Introduction Statistical machine translation ( SMT ) systems are trained using bilingual sentence - aligned parallel corpora .
Theoretically , SMT can be applied to any language pair , but in practice it produces the state - of - art results only for language pairs with ample training data , like English -Arabic , English - Chinese , French - English , etc.
SMT gets stuck in a severe bottleneck for many minority or ' low resource ' languages with insufficient data .
This drastically limits which languages SMT can be successfully applied to .
Because of this , collecting parallel corpora for minor languages has become an interesting research challenge .
There are various options for creating training data for new language pairs .
Past approaches have examined harvesting translated documents from the web ( Resnik and Smith , 2003 ; Uszkoreit et al. , 2010 ; Smith et al. , 2013 ) , or discovering parallel fragments from comparable corpora ( Munteanu and Marcu , 2005 ; Abdul -Rauf and Schwenk , 2009 ; Smith et al. , 2010 ) .
Until relatively recently , little consideration has been given to creating parallel data from scratch .
This is because the cost of hiring professional translators is prohibitively high .
For instance , Germann ( 2001 ) hoped to hire professional translators to create a modest sized 100,000 word Tamil - English parallel corpus , but were stymied by the costs and the difficulty of finding good translators for a short-term commitment .
Recently , crowdsourcing has opened the possibility of translating large amounts of text at low cost using non-professional translators .
Facebook localized its web site into different languages using volunteers ( TechCrunch , 2008 ) .
DuoLingo turns translation into an educational game , and translates web content using its language learners ( von Ahn , 2013 ) .
Rather than relying on volunteers or gamification , NLP research into crowdsourcing translation has focused on hiring workers on the Amazon Mechanical Turk ( MTurk ) platform ( Callison - Burch , 2009 ) .
This setup presents unique challenges , since it typically involves non-professional translators whose language skills are varied , and since it sometimes involves participants who try to cheat to get the small financial reward ( Zaidan and Callison - Burch , 2011 ) .
A natural approach for trying to shore up the skills of weak bilinguals is to pair them with a native speaker of the target language to edit their translations .
We review relevant research from NLP and human-computer interaction ( HCI ) on collaborative translation processes in Section 2 .
To sort good translations from bad , researchers often solicit multiple , redundant translations and then build models to try to predict which translations are the best , or which translators tend to produce the highest quality translations .
The contributions of this paper are : ?
An analysis of the difficulties posed by a twostep collaboration between editors and translators in Mechanical Turk-style crowdsourcing environments .
Editors vary in quality , and poor editing can be difficult to detect . ?
A new graph - based algorithm for selecting the best translation among multiple translations of the same input .
This method takes into account the collaborative relationship between the translators and the editors .
Related work
In the HCI community , several researchers have proposed protocols for collaborative translation efforts ( Morita and Ishida , 2009 b ; Morita and Ishida , 2009a ; Hu , 2009 ; Hu et al. , 2010 ) .
These have focused on an iterative collaboration between monolingual speakers of the two languages , facilitated with a machine translation system .
These studies are similar to ours in that they rely on native speakers ' understanding of the target language to correct the disfluencies in poor translations .
In our setup the poor translations are produced by bilingual individuals who are weak in the target language , and in their experiments the translations are the output of a machine translation system .
1 Another significant difference is that the HCI studies assume cooperative participants .
For instance , Hu et al . ( 2010 ) recruited volunteers from the International Children 's Digital Library ( Hourcade et al. , 2003 ) who were all well intentioned and participated out a sense of altruism and to build a good reputation among the other volunteer translators at childrenslibrary.org .
Our setup uses anonymous crowd workers hired on Mechanical Turk , whose motivation to participate is financial .
Bernstein et al. ( 2010 ) characterized the problems with hiring editors via MTurk for a word processing application .
Workers were either lazy ( meaning they made only minimal edits ) or overly zealous ( meaning they made many unnecessary edits ) .
Bernstein et al. ( 2010 ) addressed this problem with a three step find-fix -verify process .
In the first step , workers click on one word or phrase that needed to be corrected .
In the next step , a separate group of workers proposed correc-tions to problematic regions that had been identified by multiple workers in the first pass .
In the final step , other workers would validate whether the proposed corrections were good .
Most NLP research into crowdsourcing has focused on Mechanical Turk , following pioneering work by Snow et al . ( 2008 ) who showed that the platform was a viable way of collecting data for a wide variety of NLP tasks at low cost and in large volumes .
They further showed that non-expert annotations are similar to expert annotations when many non-expert labelings for the same input are aggregated , through simple voting or through weighting votes based on how closely non-experts matched experts on a small amount of calibration data .
MTurk has subsequently been widely adopted by the NLP community and used for an extensive range of speech and language applications ( Callison - Burch and Dredze , 2010 ) .
Although hiring professional translators to create bilingual training data for machine translation systems has been deemed infeasible , Mechanical Turk has provided a low cost way of creating large volumes of translations ( Callison - Burch , 2009 ; . For instance , Zbib et al . ( 2012 ; Zbib et al. ( 2013 ) translated 1.5 million words of Levine Arabic and Egyptian Arabic , and showed that a statistical translation system trained on the dialect data outperformed a system trained on 100 times more MSA data .
Post et al . ( 2012 ) used MTurk to create parallel corpora for six Indian languages for less than $ 0.01 per word .
MTurk workers translated more than half a million words worth of Malayalam in less than a week .
Several researchers have examined the use of active learning to further reduce the cost of translation Ambati , 2012 ; Bloodgood and Callison- Burch , 2010 ) .
Crowdsourcing allowed real studies to be conducted whereas most past active learning were simulated .
Pavlick et al. ( 2014 ) conducted a large-scale demographic study of the languages spoken by workers on MTurk by translating 10,000 words in each of 100 languages .
Chen and Dolan ( 2012 ) examined the steps necessary to build a persistent multilingual workforce on MTurk .
This paper is most closely related to previous work by Zaidan and Callison - Burch ( 2011 ) , who showed that non-professional translators could approach the level of professional translators .
They solicited multiple redundant translations from dif-Urdu translator :
According to the territory 's people the pamphlets from the Taaliban had been read in the announcements in all the mosques of the Northern Wazeerastan .
English post-editor :
According to locals , the pamphlet released by the Taliban was read out on the loudspeakers of all the mosques in North Waziristan .
LDC professional :
According to the local people , the Taliban 's pamphlet was read over the loudspeakers of all mosques in North Waziristan .
Table 1 : Different versions of translations .
ferent Turkers for a collection of Urdu sentences that had been previously professionally translated by the Linguistics Data Consortium .
They built a model to try to predict on a sentence - by-sentence and Turker - by - Turker which was the best translation or translator .
They also hired US - based Turkers to edit the translations , since the translators were largely based in Pakistan and exhibited errors that are characteristic of speakers of English as a language .
Zaidan and Callison- Burch ( 2011 ) observed only modest improvements when incorporating these edited translation into their model .
We attempt to analyze why this is , and we proposed a new model to try to better leverage their data .
Crowdsourcing Translation Setup
We conduct our experiments using the data collected by Zaidan and Callison -Burch ( 2011 ) .
This data set consists 1,792 Urdu sentences from a variety of news and online sources , each paired with English translations provided by non-professional translators on Mechanical Turk .
Each Urdu sentence was translated redundantly by 3 distinct translators , and each translation was edited by 3 separate ( native English -speaking ) editors to correct for grammatical and stylistic errors .
In total , this gives us 12 non-professional English candidate sentences ( 3 unedited , 9 edited ) per original Urdu sentence .
52 different
Turkers took part in the translation task , each translating 138 sentences on average .
In the editing task , 320 Turkers participated , averaging 56 sentences each .
For comparison , the data also includes 4 different reference translations for each source sentence , produced by professional translators .
Table 1 gives an example of an unedited translation , an edited translation , and a professional translation for the same sentence .
The translations provided by translators on MTurk are generally done conscientiously , preserving the meaning of the source sentence , but typically contain simple mistakes like misspellings , typos , and awkward word choice .
English-speaking editors , despite having no knowledge of the source language , are able to fix these errors .
In this work , we show that the collaboration design of two heads - non-professional Urdu translators and nonprofessional English editors -yields better translated output than would either one working in isolation , and can better approximate the quality of professional translators .
Analysis
We know from inspection that translations seem to improve with editing ( Table 1 ) .
Given the data from MTurk , we explore whether this is the case in general :
Do all translations improve with editing ?
To what extent does the individual translator and the individual editor effect the quality of the final sentence ?
sentences are very similar .
To capture the quality ( " professionalness " ) of a translation , we take the average TER of the translation against each of our gold translations .
That is , we define TER gold of translation t as T ER gold = 1 4 4 i=1 T ER ( gold i , t ) ( 1 ) where a lower TER gold is indicative of a higher quality ( more professional - sounding ) translation .
We first look at editors along two dimensions : their aggressiveness and their effectiveness .
Some editors may be very aggressive ( they make many changes to the original translation ) but still be ineffective ( they fail to bring the quality of the translation closer to that of a professional ) .
We measure aggressiveness by looking at the TER between the pre-and post-edited versions of each editor 's translations ; higher TER implies more aggressive editing .
To measure effectiveness , we look at the change in TER gold that results from the editing ; negative ?TER gold means the editor effectively improved the quality of the translation , while positive ?TER gold means the editing actually brought the translation further from our gold standard .
Figure 1 shows the relationship between these two qualities for individual editor / translation pairs .
We see that while most translations require only a few edits , there are a large number of translations which improve substantially after heavy editing .
This trend conforms to our intuition that editing is most useful when the translation has much room for improvement , and opens the question of whether good editors can offer improvements to translations of all qualities .
To address this question , we split our translations into 5 bins , based on their TER gold .
We also split our editors into 5 bins , based on their effectiveness ( i.e. the average amount by which their editing reduces TER gold ) .
Figure 2 shows the degree to which editors at each level are able to improve the translations from each bin .
We see that good editors are able to make improvements to translations of all qualities , but that good editing has the greatest impact on lower quality translations .
This result suggests that finding good editor / translator pairs , rather than good editors and good translators in isolation , should produce the best translations overall .
Figure 3 gives an example of how an initially medium-quality translation , when combined with good editing , produces a better result than the higher -quality translation paired with mediocre editing .
Problem Formulation
The problem definition of the crowdsourcing translation task is straightforward : given a set of candidate translations for a source sentence , we want to choose the best output translation .
This output translation is the result of the combined translation and editing stages .
Therefore , our method operates over a heterogeneous network that includes translators and post-editors as well as the translated sentences that they produce .
We frame the problem as follows .
We form two graphs : the first graph ( G T ) represents Turkers ( translator / editor pairs ) as nodes ; the second graph ( G C ) represents candidate translated and Figure 3 : Three alternative translations ( left ) and the edited versions of each ( right ) .
Each edit on the right was produced by a different editor .
Order reflects the TER gold of each translation , with the lowest TER gold on the top .
Some translators receive low TER gold scores due to superficial errors , which can be easily improved through editing .
In the above example , the middle - ranked translation ( green ) becomes the best translation after being revised by a good editor .
post-edited sentences ( henceforth " candidates " ) as nodes .
These two graphs , G T and G C are combined as subgraphs of a third graph ( G T C ) .
Edges in G T C connect author pairs ( nodes in G T ) to the candidate that they produced ( nodes in G C ) .
Together , G T , G C , and G T C define a co-ranking problem ( Yan et al. , 2012a ; Yan et al. , 2011 b ; Yan et al. , 2012 b ) with linkage establishment ( Yan et al. , 2011a ; Yan et al. , 2012 c ) , which we define formally as follows .
Let G denote the heterogeneous graph with nodes V and edges E. Let G = ( V ,E ) = ( V T , V C , E T , E C , E T C ) .
G is divided into three subgraphs , G T , G C , and G T C . G C = ( V C , E C ) is a weighted undirected graph representing the candidates and their lexical relationships to one another .
Let V C denote a collection of translated and edited candidates , and E C the lexical similarity between the candidates ( see Section 4.3 for details ) .
G T = ( V T , E T ) is a weighted undirected graph representing collaborations between Turkers .
V T is the set of translator / editor pairs .
Edges E T connect translator / editor pairs in V T which share a translator and / or editor .
Each collaboration ( i.e. each node in V T ) produces a candidate ( i.e. a node in V C ) .
G T C = ( V T C , E T C ) is an unweighted bipartite graph that ties G T and G C together and represents " authorship " .
The graph G consists of nodes V T C = V T ? V C and edges E T C connecting each candidate with its authoring translator / post-editor pair .
The three sub-networks ( G T , G C , and G T C ) are illustrated in Figure 4 .
Inter - Graph Ranking
The framework includes three random walks , one on G T , one on G C and one on G T C .
A random walk on a graph is a Markov chain , its states being the vertices of the graph .
It can be described by a stochastic square matrix , where the dimension is the number of vertices in the graph , and the entries describe the transition probabilities from one vertex to the next .
The mutual reinforcement framework couples the two random walks on G T and G C that rank candidates and Turkers in isolation .
The ranking method allows us to obtain a global ranking by taking into account the intra- / inter-component dependencies .
In the following sections , we describe how we obtain the rankings on G T and G C , and then move on to discuss how the two are coupled .
Our algorithm aims to capture the following intuitions .
A candidate is important if 1 ) it is similar to many of the other proposed candidates and 2 ) it is authored by better qualified translators and / or post-editors .
Analogously , a translator / editor pair is believed to be better qualified if 1 ) the editor is collaborating with a good translator and vice versa and 2 ) the pair has authored important candidates .
This ranking schema is actually a reinforced process across the heterogeneous graphs .
We use two vectors c = [?( c ) ] | c|?1 and t = [ ? ( t ) ] | t |?1 to denote the saliency scores ?(. ) of candidates and Turker pairs .
The above-mentioned intuitions can be formulated as follows : ? Homogeneity .
We use adjacency matrix ?
Heterogeneity .
We use an adjacency matrix [ ? ] | c|?|t| and [ W ] | t |?| c| to describe the authorship between the output candidate and the producer Turker pair from both of the candidate - to - Turker and Turker-to - candidate perspectives .
c ? ?
T t , t ? W T c ( 3 ) All affinity matrices will be defined in the next section .
By fusing the above equations , we can have the following iterative calculation in matrix forms .
For numerical computation of the saliency scores , the initial scores of all sentences and Turkers are set to 1 and the following two steps are alternated until convergence to select the best candidate .
Step 1 : compute the saliency scores of candidates , and then normalize using - 1 norm . c ( n ) = ( 1 ? ?) M T c ( n?1 ) + ? ? t ( n?1 ) c ( n ) = c ( n ) /||c ( n ) || 1 ( 4 ) Step 2 : compute the saliency scores of Turker pairs , and then normalize using - 1 norm .
t ( n ) = ( 1 ? ?)N T t ( n?1 ) + ? W c ( n?1 ) t ( n ) = t ( n ) /||t ( n ) || 1 ( 5 ) where ? specifies the relative contributions to the saliency score trade- off between the homogeneous affinity and the heterogeneous affinity .
In order to guarantee the convergence of the iterative form , we must force the transition matrix to be stochastic and irreducible .
To this end , we must make the c and t column stochastic ( Langville and Meyer , 2004 ) . c and t are therefore normalized after each iteration of Equation ( 4 ) and ( 5 ) .
Intra-Graph Ranking
The standard PageRank algorithm starts from an arbitrary node and randomly selects to either follow a random out - going edge ( considering the weighted transition matrix ) or to jump to a random node ( treating all nodes with equal probability ) .
In a simple random walk , it is assumed that all nodes in the transitional matrix are equi-probable before the walk starts .
Then c and t are calculated as : c = ?M
T c + ( 1 ? ? ) 1 | V C | ( 6 ) and t = ?N T t + ( 1 ? ? ) 1 | V T | ( 7 ) where 1 is a vector with all elements equaling to 1 and the size is correspondent to the size of V C or V T . ? is the damping factor usually set to 0.85 , as in the PageRank algorithm .
Affinity Matrix Establishment
We introduce the affinity matrix calculation , including homogeneous affinity ( i.e. , M , N ) and heterogeneous affinity ( i.e. , ? , W ) .
As discussed , we model the collection of candidates as a weighted undirected graph , G C , in which nodes in the graph represent candidate sentences and edges represent lexical relatedness .
We define an edge 's weight to be the cosine similarity between the candidates represented by the nodes that it connects .
The adjacency matrix M describes such a graph , with each entry corresponding to the weight of an edge .
F( c i , c j ) = c i ? c j ||c i ||||c j ||
M ij = F( c i , c j ) k F ( c i , c k ) ( 8 ) where F (. ) is the cosine similarity and c is a term vector corresponding to a candidate .
We treat a candidate as a short document and weight each term with tf.idf ( Manning et al. , 2008 ) , where tf is the term frequency and idf is the inverse document frequency .
The Turker graph , G T , is an undirected graph whose edges represent " collaboration . "
Formally , let t i and t j be two translator / editor pairs ; we say that pair t i " collaborates with " pair t j ( and therefore , there is an edge between t i and t j ) if t i and t j share either a translator or an editor ( or share both a translator and an editor ) .
Let the function I( t i , t j ) denote the number of " collaborations " ( # col ) between t i and t j .
I ( t i , t j ) = # col ( e ij ? E T ) 0 otherwise , ( 9 ) Then the adjacency matrix N is then defined as N ij = I( t i , t j ) k I ( t i , t k ) ( 10 )
In the bipartite candidate - Turker graph G T C , the entry E T C ( i , j ) is an indicator function denoting whether the candidate c i is generated by t j : A ( c i , t j ) = 1 ( e ij ? E T C ) 0 otherwise ( 11 ) Through E T C we define the weight matrices Wij and ? ij , containing the conditional probabilities of transitions from c i to t j and vice versa : Wij = A( c i , t j ) k A ( c i , t k ) , ?ij = A ( c i , t j ) k A ( c k , t j ) ( 12 ) 5 Evaluation
We are interested in testing our random walk method , which incorporates information from both the candidate translations and from the Turkers .
We want to test two versions of our proposed collaborative co-ranking method : 1 ) based on the unedited translations only and 2 ) based on the edited sentences after translator / editor collaborations .
Metric Since we have four professional translation sets , we can calculate the Bilingual Evaluation Understudy ( BLEU ) score ( Papineni et al. , 2002 ) for one professional translator ( P1 ) using the other three ( P2,3,4 ) as a reference set .
We repeat the process four times , scoring each professional translator against the others , to calculate the expected range of professional quality translation .
In the following sections , we evaluate each of our methods by calculating BLEU scores against the same four sets of three reference translations .
Therefore , each number reported in our experimental results is an average of four numbers , corresponding to the four possible ways of choosing 3 of the 4 reference sets .
This allows us to compare the BLEU score achieved by our methods against the BLEU scores achievable by professional translators .
Baselines
As a naive baseline , we choose one candidate translation at random for each input Urdu sentence .
oracle scores .
The first oracle operates at the segment level on the sentences produced by translators only : for each source segment , we choose from the translations the one that scores highest ( in terms of BLEU ) against the reference sentences .
The second oracle is applied similarly , but chooses from the candidates produced by the collaboration of translator / post-editor pairs .
The third oracle operates at the worker level : for each source segment , we choose from the translations the one provided by the worker whose translations ( over all sentences ) score the highest on average .
The fourth oracle also operates at the worker level , but selects from sentences produced by translator / post-editor collaborations .
These oracle methods represent ideal solutions under our scenario .
We also examine two voting - inspired methods .
The first method selects the translation with the minimum average TER ( Snover et al. , 2006 ) against the other translations ; intuitively , this would represent the " consensus " translation .
The second method selects the translation generated by the Turker who , on average , provides translations with the minimum average TER .
Results
A summary of our results in given in Table 2 .
As expected , random selection yields bad performance , with a BLEU score of 30.52 .
The oracles indicate that there is usually an acceptable translation from the Turkers for any given sentence .
Since the oracles select from a small group of only 4 translations per source segment , they are not overly optimistic , and rather reflect the true potential of the collected translations .
On average , the reference translations give a score of 42.38 .
To put this in perspective , the output of a state - of- the - ( Zaidan and Callison - Burch , 2011 ) .
The approach which selects the translations with the minimum average TER ( Snover et al. , 2006 ) against the other three translations ( the " consensus " translation ) achieves BLEU scores of 35.78 .
Using the raw translations without post-editing , our graph - based ranking method achieves a BLEU score of 38.89 , compared to Zaidan and Callison - Burch ( 2011 ) ' s reported score of 28.13 , which they achieved using a linear feature - based classification .
Their linear classifier achieved a reported score of 39.06 2 when combining information from both translators and editors .
In contrast , our proposed graph - based ranking framework achieves a score of 41.43 when using the same information .
This boost in BLEU score confirms our intuition that the hidden collaboration networks between candidate translations and transltor / editor pairs are indeed useful .
Parameter Tuning
There are two parameters in our experimental setups : ? controls the probability of starting a new random walk and ? controls the coupling between the candidate and Turker subgraphs .
We set the damping factor ? to 0.85 , following the standard PageRank paradigm .
In order to determine a value for ? , we used the average BLEU , computed against the professional refer- ence translations , as a tuning metric .
We experimented with values of ?
ranging from 0 to 1 , with a step size of 0.05 ( Figure 5 ) .
Small ? values place little emphasis on the candidate / Turker coupling , whereas larger values rely more heavily on the coranking .
Overall , we observed better performance with values within the range of 0.05 - 0.15 .
This suggests that both sources of information - the candidate itself and its authors - are important for the crowdsourcing translation task .
In all of our reported results , we used the ? = 0.1 . Analysis
We examine the relative contribution of each component of our approach on the overall performance .
We first examine the centroid- based ranking on the candidate sub-graph ( G C ) alone to see the effect of voting among translated sentences ; we denote this strategy as plain ranking .
Then we incorporate the standard random walk on the Turker graph ( G T ) to include the structural information but without yet including any collaboration information ; that is , we incorporate information from G T and G C without including edges linking the two together .
The co-ranking paradigm is exactly the same as the framework described in Section 3.2 , but with simplified structures .
Finally , we examine the two-step collaboration based candidate - Turker graph using several variations on edge establishment .
As before , the nodes are the translator / post-editor working pairs .
We investigate three settings in which 1 ) edges connect two nodes when they share only a translator , 2 ) edges connect two nodes when they share only a post-editor , and 3 ) edges connect two nodes when they share either a translator or a post-editor .
These results are summarized in Table 3 .
Interestingly , we observe that when modeling the linkage between the collaboration pairs , connecting Turker pairs which share either a translator or the post-editor achieves better performance than connecting pairs that share only translators or connecting pairs which share only editors .
This result supports the intuition that a denser collabo-ration matrix will help propagate saliency to good translators / post-editors and hence provides better predictions for candidate quality .
Conclusion
We have proposed an algorithm for using a twostep collaboration between non-professional translators and post-editors to obtain professionalquality translations .
Our method , based on a co-ranking model , selects the best crowdsourced translation from a set of candidates , and is capable of selecting translations which near professional quality .
Crowdsourcing can play a pivotal role in future efforts to create parallel translation datasets .
In addition to its benefits of cost and scalability , crowdsourcing provides access to languages that currently fall outside the scope of statistical machine translation research .
In future work on crowdsourced translation , further benefits in quality improvement and cost reduction could stem from 1 ) building ground truth data sets based on high-quality Turkers ' translations and 2 ) identifying when sufficient data has been collected for a given input , to avoid soliciting unnecessary redundant translations .
Figure 1 : Figure 2 : 12 Figure 1 : Relationship between editor aggressiveness and effectiveness .
Each point represents an editor / translation pair .
Aggressiveness ( x- axis ) is measured as the TER between the pre-edit and post-edit version of the translation , and effectiveness ( y- axis ) is measured as the average amount by which the editing reduces the translation 's TER gold .
While many editors make only a few changes , those who make many changes can bring the translation substantially closer to professional quality .
