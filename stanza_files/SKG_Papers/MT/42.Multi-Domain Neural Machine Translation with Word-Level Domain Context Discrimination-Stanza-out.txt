title
Multi-Domain Neural Machine Translation with Word-Level Domain Context Discrimination
abstract
With great practical value , the study of Multidomain Neural Machine Translation ( NMT ) mainly focuses on using mixed - domain parallel sentences to construct a unified model that allows translation to switch between different domains .
Intuitively , words in a sentence are related to its domain to varying degrees , so that they will exert disparate impacts on the multi-domain NMT modeling .
Based on this intuition , in this paper , we devote to distinguishing and exploiting word - level domain contexts for multi-domain NMT .
To this end , we jointly model NMT with monolingual attention - based domain classification tasks and improve NMT as follows :
1 ) Based on the sentence representations produced by a domain classifier and an adversarial domain classifier , we generate two gating vectors and use them to construct domain-specific and domain-shared annotations , for later translation predictions via different attention models ;
2 ) We utilize the attention weights derived from target -side domain classifier to adjust the weights of target words in the training objective , enabling domain-related words to have greater impacts during model training .
Experimental results on Chinese-English and English - French multi-domain translation tasks demonstrate the effectiveness of the proposed model .
Source codes of this paper are available on Github https://github.com/DeepLearnXMU/WDCNMT.
Introduction
In recent years , neural machine translation ( NMT ) has achieved great advancement ( Nal and Phil , 2013 ; Sutskever et al. , 2014 ; Bahdanau et al. , 2015 ) .
However , two difficulties are encountered in the practical applications of NMT .
On the one hand , training a NMT model for a spe- cific domain requires a large quantity of parallel sentences in such domain , which is often not readily available .
Hence , the much more common practice is to construct NMT models using mixed - domain parallel sentences .
In this way , the domain-shared translation knowledge can be fully exploited .
On the other hand , the translated sentences often belong to multiple domains , thus requiring a NMT model general to different domains .
Since the textual styles , sentence structures and terminologies in different domains are often remarkably distinctive , whether such domainspecific translation knowledge is effectively preserved could have a direct effect on the performance of the NMT model .
Therefore , how to simultaneously exploit the exclusive and shared translation knowledge of mixed - domain parallel sentences for multi-domain NMT remains a challenging task .
To tackle this problem , recently , researchers have carried out many constructive and in- depth studies ( Kobus et al. , 2016 ; Zhang et al. , 2016 ; Pryzant et al. , 2017 ; Farajian et al. , 2017 ) .
However , most of these studies mainly focus on the utilization of domain contexts as a whole in NMT , while ignoring the discrimination of domain contexts at finer- grained level .
In each sentence , some words are closely associated with its domain , while others are domain-independent .
Intuitively , these two kinds of words play differ-ent roles in multi-domain NMT , nevertheless , they are not being distinguished by the current models .
Take the sentence shown in Figure 1 for example .
The Chinese words " ' OE ? "( congress ) , " AE Y " ( bills ) , " \ " ( inclusion ) , and " AE ? " ( agenda ) are frequently used in Laws domain and imply the Laws style of the sentence , while other words in this sentence are common in all domains and they mainly indicate the semantic meaning of the sentence .
Thus , it is reasonable to distinguish and encode these two types of words separately to capture domain-specific and domain-shared contexts , allowing the exclusive and shared knowledge to be exploited without any interference from the other .
Meanwhile , the English words " priority " , " government " , " bill " and " agenda " are also closely related to Laws domain .
To preserve the domain-related text style and idioms in generated translations , it is also reasonable for our model to pay more attention to these domain-related words than the others during model training .
On this account , we believe that it is significant to distinguish and explore word- level domain contexts for multi-domain NMT .
In this paper , we propose a multi-domain NMT model with word-level domain context discrimination .
Specifically , we first jointly model NMT with monolingual attention - based domain classification tasks .
In source-side domain classification and adversarial domain classification tasks , we perform two individual attention operations on source-side annotations to generate the domainspecific and domain-shared vector representations of source sentence , respectively .
Meanwhile , an attention operation is also placed on target -side hidden states to implement target -side domain classification .
Then , we improve NMT with the following two approaches : ( 1 ) According to the sentence representations produced by source-side domain classifier and adverisal domain classifier , we generate two gating vectors for each source annotation .
With these two gating vectors , the encoded information of source annotation is selected automatically to construct domain-specific and domain-shared annotations , both of which are used to guide translation predictions via two attention mechanisms ; ( 2 ) Based on the attention weights of the target words from target - side domain classifier , we employ word - level cost weighting strategy to refine our model training .
In this way , domain-specific target words will be assigned greater weights than others in the objective function of our model .
Our work demonstrates the benefits of separate modeling of the domain-specific and domainshared contexts , which echoes with the successful applications of the multi-task learning based on shared - private architecture in many tasks , such as discourse relation recognition , word segmentation ) , text classification ( Liu et al. , 2017a ) , and image classification .
Overall , the main contributions of our work are summarized as follows : ?
We propose to construct domain-specific and domain-shared source annotations from initial annotations , of which effects are respectively captured for translation predictions . ?
We propose to adjust the weights of target words in the training objective of NMT according to their relevance to different domains .
?
We conduct experiments on large-scale multi-domain Chinese-English and English - French datasets .
Experimental results demonstrate the effectiveness of our model .
Model Figure 2 illustrates the architecture of our model , which includes a neural encoder equipped with a domain classifier and an adversarial domain classifier , and a neural decoder with two attention models and a target -side domain classifier .
Neural Encoder
As shown in the lower part of Figure 2 , our encoder leverages the sentence representations produced by these two classifiers to construct domain-specific and domain-shared annotations from initial ones , preventing the exclusive and shared translation knowledge from interfering with each other .
In our encoder , the input sentence x=x 1 , x 2 , ... , x N are first mapped to word vectors and then fed into a bidirectional GRU ( Cho et al. , 2014 ) to obtain ? ? h = ? ? h 1 , ? ? h 2 , ... , ? ? h N and ? ? h = ? ? h 1 , ? ? h 2 , ... , ? ? h N in the left-to- right and right - to - left directions , respectively .
These two sequences are then concatenated as h i = { ? ? h ? i , ? ? h ? i } ? to ? ? ? Decoder ? ? ? ? ? ? ? Domain Classifier ? ? ? ? ? ? ? ? Encoder ? Domain-Specific Annotations ? ? ? ? Domain-Shared Annotations ? ? ? ? Domain Classifier ? ? ? ? E r ( ) ? ? ? ? ? ? ? ? ? ? E s ( ) E r ( y ) ? ? ? ? ? ? ? Adversarial Domain Classifier ? ? ? ? ? ? ? ? Figure 2 : The architecture illustration of our model .
Note that our two source-side domain classifiers are used to produce domain-specific and domain-shared annotations , respectively , and our target - side domain classifier is only used during model training .
two attention - like aggregators to generate the semantic representations of sentence x , denoted by the vectors E r ( x ) and E s ( x ) , respectively .
Based on these two vectors , we employ the same neural network to model two classifiers with different context modeling objectives :
One is a domain classifier that aims to distinguish different domains in order to generate domain-specific source -side contexts .
It is trained using the objective function J s dc ( x ; ? s dc ) = log p( d|x ; ? s dc ) , where d is the domain tag of x and ?
s dc is its parameter set .
The other is an adversarial domain classifier capturing source-side domainshared contexts .
To this end , we train it using the following adversarial loss functions : J s1 adc ( x ; ? s1 adc ) = log p( d|x ; ? s1 adc , ? s2 adc ) , ( 1 ) J s2 adc ( x ; ? s2 adc ) = H ( p ( d|x ; ? s1 adc , ? s2 adc ) ) , ( 2 ) where H ( p ( ? ) ) =?
K k=1 p k ( ? ) log p k ( ? ) is an en- tropy of distribution p( ? ) with K domain labels , ? s1 adc and ?
s2 adc denote the parameters of softmax layer and the generation layer of E s ( x ) in this classifier , respectively .
By this means , E r ( x ) and E s ( x ) are expected to encode the domain-specific and domain-shared semantic representations of x , respectively .
It should be noted that our utilization of domain classifiers is similar to adversarial training used in ( Pryzant et al. , 2017 ) which injects domain-shared contexts into annotations .
However , by contrast , we introduce domain classifier and adversarial domain classifier simultaneously to distinguish different kinds of contexts for NMT more explicitly .
Here we describe only the modeling procedure of the domain classifier , while it is also applicable to the adversarial domain classifier .
Specifically , E r ( x ) is defined as follows : E r ( x ) = N i=1 ? i h i , ( 3 ) where ?
i = exp( e i ) N i ? exp( e i ? ) , e i = ( v a ) ? tanh ( W a h i ) , and v a and W a are the relevant attention parameters .
Then , we feed E r ( x ) into a fully connected layer with ReLU function ( Ballesteros et al. , 2015 ) , and then pass its output through a softmax layer to implement domain classification p(?|x ; ? s dc ) =sof tmax ( W s? dc ReLU ( E r ( x ) ) + b s dc ) , ( 4 ) where W s dc and b s dc are softmax parameters .
Domain-Specific and Domain- Shared Annotations .
Since domain-specific and domain-shared contexts have different effects on NMT , and thus should be distinguished and separately captured by NMT model .
Specifically , we first leverage the sentence representations E r ( x ) and E s ( x ) to generate two gating vectors , g r i and g s i , for annotation h i in the following way : g r i = sigmoid ( W ( 1 ) gr E r ( x ) + W ( 2 ) gr h i + b gr ) , ( 5 ) g s i = sigmoid ( W ( 1 ) gs E s ( x ) + W ( 2 ) gs h i + b gs ) , ( 6 ) where W * gr , W * gs , b gr and b gs denote the relevant matrices and bias , respectively .
With these two vectors , we construct domain-specific and domain-shared annotations h r i and h s i from h i : h r i = g r i ? h i , ( 7 ) h s i = g s i ? h i . ( 8 )
Neural Decoder
The upper half of Figure 2 illustrates the architecture of our decoder .
In particular , with the attention weights of target words from the domain classifier , we employ word - level cost weighting strategy to refine model training .
Formally , our decoder applies a nonlinear function g( * ) to define the conditional probability of translation y=y 1 , y 2 , ... , y M : p( y|x ) = M j=1 p(y j |x , y <j ) = M j=1 g(y j?1 , s j , c r j , c s j ) , ( 9 ) where the vector s j denotes the GRU hidden state .
It is updated as s j = GRU ( s j?1 , y j?1 , c r j , c s j ) . ( 10 )
Here the vectors c r j and c s j represent the domainspecific and domain-shared contexts , respectively .
Domain-Specific and Domain- Shared Context Vectors .
When generating y j , we define c r j as a weighted sum of the domain-specific annotations { h r i } : c r j = N i=1 exp( e r j , i ) N i ? = 1 exp( e r j , i ? ) ? h r i , ( 11 ) where e r j, i = a( s j?1 , h r i ) , and a ( * ) is a feedforward neural network .
Meanwhile , we produce c s j from the domain-shared annotations {h s i } as in Eq. 11 .
By introducing c r j and c s j into s j , our decoder is able to distinguish and simultaneously exploit two types of contexts for translation predictions .
Domain Classifier .
We equip our decoder with a domain classifier with parameters ? tdc , which maximizes the training objective i.e. , J t dc ( y ; ? t dc ) = log p( d|y ; ? t dc ) .
To do this , we also apply attention operation to produce the domain- aware semantic representation E r ( y ) of y , E r ( y ) = M j=1 ? j s j , ( 12 ) where ? j = exp( e j ) M j ? exp( e j ? ) , e j = ( v b ) ? tanh ( W b s j ) , and v b and W b are the related parameters .
Likewise , we stack a domain classifier on top of E r ( y ) .
Note that this classifier is only used in model training to infer attention weights of target words .
These weights measure their semantic relevance to different domains and can be utilized to adjust their cost weights in NMT training objective .
NMT Training Objective with Word-Level Cost Weighting .
Formally , we define the objective function of NMT as follows : J nmt ( x , y ; ? nmt ) = M j=1 ( 1 + ? j ) log p(y j |x , y <j ; ? nmt ) , ( 13 ) where ? j is the attention weight of y j obtained by Eq. ( 12 ) , and ? nmt denotes the parameter set of NMT .
By this scaling strategy , domainspecific words are emphasized with a bonus , while domain-shared words are updated as usual .
Please note that scaling costs with a multiplicative scalar essentially changes the magnitude of parameter update but without changing its direction ( Chen et al. , 2017a ) .
Besides , although our scaling strategy is similar to the cost weighting proposed by Chen et al . ( 2017a ) , our approach differs from it in two aspects :
First , we employ wordlevel cost weighting rather than sentence - level one to refine NMT training ;
Second , our approach is less time - consuming for multi-domain NMT .
Overall Training Objective Given a mixed - domain training corpus D = { ( x , y , d ) } , we train the proposed model accord-ing to the following objective function : J ( D ; ? ) = ( x , y , d ) ?
D
{ J nmt ( x , y ; ? nmt ) + J s dc ( x ; ? s dc ) + J t dc ( y ; ? t dc ) ( 14 ) + J s1 adc ( x ; ? s1 adc ) + ? ? J s2 adc ( x ; ? s2 adc ) } where J nmt ( * ) , J s dc ( * ) , J t dc ( * ) and J s * adc ( * ) are the objective functions of NMT , source-side domain classifier , target - side domain classifier , and source-side adversarial domain classifier , respectively , ?={?
nmt , ? s dc , ?
t dc , ? s1 adc , ? s2 adc } , and ? is the hyper-parameter for adversarial learning .
Particularly , to ensure encoding accuracy of domain-shared contexts , we follow to adopt an alternative two - phase strategy in training , where we alternatively optimize J ( D ; ? ) with ? s1 adc and {?-?
s1 adc } respectively fixed at a time .
Experiment
To investigate the effectiveness of our model , we conducted multi-domain translation experiments on Chinese-English and English - French datasets .
Setup Datasets .
For Chinese -English translation , our data comes from UM - Corpus ( Tian et al. , 2014 ) and LDC 1 .
To ensure data quality , we chose only the parallel sentences with domain label Laws , Spoken , and Thesis from UM - Corpus , and the LDC bilingual sentences related to News domain as our dataset .
We used randomly selected sentences from UM - Corpus and LDC as development set , and combined the test set of UM - Corpus and randomly selected sentences from LDC to construct our test set .
For English - French translation , we conducted experiments on the datasets of OPUS corpus 2 , containing sentence pairs from Medical , News , and Parliamentary domains .
We also divided these datasets into training , development and test sets .
Table 1 provides the statistics of the corpora used in our experiments .
We performed word segmentation on Chinese sentences using Stanford Segmenter 3 , and tokenized English and French sentences using MOSES script 4 . Then , we employed Byte Pair Encoding ( Sennrich et al. , 2016 ) to convert all words into subwords .
The translation quality was evaluated by case-sensitive BLEU ( Papineni et al. , 2002 ) . Contrast Models .
Since our model is essentially a standard attentional NMT model enhanced with word- level domain contexts , we refer to it as + WDC .
We compared it with the following models , namely : ? OpenNMT 5 . A famous open-source NMT system used widely in the NMT community trained on mix-domain training set .
? DL4NMT -single ( Bahdanau et al. , 2015 ) .
A reimplemented attentional NMT trained on a single domain dataset .
? DL4NMT - mix ( Bahdanau et al. , 2015 ) .
A reimplemented attentional NMT trained on mix-domain training set .
? DL4NMT -finetune ( Luong and Manning , 2015 ) .
A reimplemented attentional NMT which is first trained using out - of- domain training corpus and then fine- tuned using indomain dataset .
?
+ Domain Control ( + DC ) ( Kobus et al. , 2016 ) .
It directly introduces embeddings of source domain tag to enrich annotations of encoder .
? + Multitask Learning ( + ML1 ) ( Dong et al. , 2015 ) .
It adopts a multi-task learning framework that shares encoder representation and separates the decoder modeling of different domains .
? + Multitask Learning ( + ML2 ) ( Pryzant et al. , 2017 ) .
This model jointly trains NMT with domain classification via multitask learning .
? + Adversarial Discriminative Mixing ( + ADM ) ( Pryzant et al. , 2017 ) .
It employs adversarial training to achieve the domain adaptation in NMT .
? + Target Token Mixing ( + TTM ) ( Pryzant et al. , 2017 ) .
This model is similar to + DC , with the only difference that it enriches source annotations by adding target - side domain tag rather than source-side one .
Note that our model uses two annotation sequences , thus we also compared it with the aforementioned models with two times of hidden state size ( 2?hd ) .
To further examine the effectiveness of the proposed components in our model , we also provided the performance of the following variants of our model : ? + WDC ( S ) .
It only exploits the source-side word-level domain contexts for multi-domain NMT . ? + WDC ( T ) .
It only employ word - level cost weighting on the target side to refine the model training .
Implementation Details .
Following the common practice , we only used the training sentences within 50 words to efficiently train NMT models .
Thus , 85.40 % and 88.96 % of the Chinese-English and English - French parallel sentences were covered in our experiments .
In addition , we set the vocabulary size for Chinese -English and English - French as 32,000 and 32,000 , respectively .
In doing so , our vocabularies covered 99.97 % Chinese words and 99.99 % English words of the Chinese-English corpus , and almost 100 % English words and 99.99 % French words of the English - French corpus , respectively .
We applied Adam ( Kingma and Ba , 2015 ) to train models and determined the best model parameters based on the model performance on development set .
The used hyper-parameter were set as follows : ? 1 and ?
2 of Adam as 0.9 and 0.999 , word embedding dimension as 500 , hidden layer size as 1000 , learning rate as 5 ? 10 ?4 , batch size as 80 , gradient norm as 1.0 , dropout rate as 0.1 , and beamsize as 10 .
Other settings were set following ( Bahdanau et al. , 2015 ) . Overall Evaluation of the Chinese-English translation task .
2? hd = two times of hidden state size .
Results on Chinese-English Translation
We first determined the optimal hyper-parameter ? ( see Eq. ( 14 ) ) on the development set .
To do this , we gradually varied ? from 0.1 to 1.0 with an increment of 0.1 in each step .
Since our model achieved the best performance when ?=0.1 , hence , we set ?=0.1 for all experiments thereafter .
Table 2 shows the overall experimental results .
Using almost the same hyper-parameters , our reimplemented DL4NMT outperforms OpenNMT in all domains , demonstrating that our baseline is competitive in performance .
Moreover , on all test sets of different domains , our model significantly outperforms other contrast models no matter which hyper-parameters they use .
Furthermore , we arrive at the following conclusions :
First , our model surpasses DL4NMT - single , DL4NMT - mix and DL4NMT - finetune , all of which are commonly used in domain adaptation for NMT .
Please note that DL4NMT - finetune requires multiple adapted NMT models to be constructed , while ours is a unified one that works well in all domains .
Second , compared with + DC , + ML2 and + ADM which all exploit source -side domain contexts for multi-domain NMT , our + WDC ( S ) still ) " ( Formation ) , " ? { " ( Method ) , " ? 4 " ( Seal ) , " O ? " ( Calculation ) , " ? " ( Experiment ) are strengthened by g r i , while most of the domainshared words " " ( of ) and " ? " ( and ) are focused by g s i . exhibits better performance .
This is because that these models focus on one aspect of domain contexts , while our model considers both domainspecific and domain-shared contexts on the source side .
Third , + WDC ( T ) also outperforms DL4NMT , revealing that it is reasonable and effective to emphasize domain-specific words in model training ..
Last , + WDC achieves the best performance when compared with both + WDC ( S ) and + WDC ( T ) .
Therefore , we believe that word- level domain contexts on the both sides are complementary to each other , and utilizing them simultaneously is beneficial to multi-domain NMT .
Experimental Analysis
Furthermore , we conducted several visualization experiments to empirically analyze the individual effectiveness of the added model components .
Visualizations of Gating Vectors
We first visualized the gating vectors g r i and g s i to quantify their effects on extracting domainspecific and domain-shared contexts from initial source-side annotations .
Since both g r i and g s i are high dimension vectors , which are difficult to be visualized directly , we followed and Zhou et al . ( 2017 ) to visualize their individual contributions to the final output , which can be The visualization of the sentence representations and their corresponding average annotations , where the triangle - shaped ( purple ) , circle-shaped ( red ) , square- shaped ( green ) and pentagonal - shaped ( blue ) points denote News , Laws , Spoken and Thesis sentences , respectively .
approximated by their first derivatives .
Figure 3 shows the first derivative heat maps for two example sentences in Laws and Thesis domain , respectively .
We can observe that without any loss of semantic meanings from source sentences , most of the domain-specific words are strengthened by g r i , while most of the domainshared words , especially function words , are focused by g s i .
This result is consistent with our expectation for the function of two gating vectors .
Visualizations of Sentence Representations and Annotations Furthermore , we applied the hypertools ( Heusser et al. , 2018 ) to visualize the sentence representations E r ( x ) and E s ( x ) , and the domain-specific and domain-shared annotation sequences {h r i } N i=1 and {h s i } N i=1 .
Here we represent each annotation sequence with its average vector in the figure .
As shown in Figure 4 ( a ) and ( b ) , the sentence representation vectors and the average annotation vectors of different domains are clearly distributed in different regions .
By contrast , their distributions are much more concentrated in Figure 4 ( c ) and ( d ) .
Thus , we conclude that our model is able to distinctively learn domain-specific and domainshared contexts .
Moreover , from Figure 4 those of the other domains , this may be caused by the more formal and consistent sentence styles in Laws domain .
Illustrations of Domain-Specific Target Words Lastly , for each domain , we presented the top ten target words with the highest weights learned by our target -side domain classifier .
To do this , we calculated the average attention weight of each word in the training corpus as its corresponding domain weight .
As is clearly shown in Table 3 that most listed target words are closely related to their domains .
This result validates the aforementioned hypothesis that some words are domain-dependent while others are domain-independent , and our targetside domain classifier is capable of distinguishing them with different attention weights .
Results on English - French Translation Likewise , we determined the optimal ?=0.1 on the development set .
Table 4 gives the results of English - French multi-domain translation .
Similar to the previous experimental result in Section 3.2 , our model continues to achieve the best performance compared to all contrast models using two different hidden state size settings , which demonstrates again that our model is effective and general to different language pairs in multi-domain NMT .
Related Work
In this work , we study on multi-domain machine translation in the field of domain adaptation for machine translation , which has attracted great attention since SMT ( Clark et al. , 2012 ; Huck et
The first category is to transfer out - of- domain knowledge to in-domain translation .
The conventional method is fine-tuning , which first trains the model on out - of- domain dataset and then finetunes it on in- domain dataset ( Luong and Manning , 2015 ; Zoph et al. , 2016 ; Servan et al. , 2016 ) . Freitag and Al- Onaizan ( 2016 ) ( 2016 ) used the topic information of input sentence as an additional input to decoder .
Zhang et al. ( 2016 ) enhanced the word representation by adding its topic embedding .
However , these methods require to have explicit document boundaries between training data , which unfortunately do not exist in most datasets .
Overall , our work is related to the second type of approach with ( Pryzant et al. , 2017 ) and ( Chen et al. , 2017a ) most related to ours .
Unlike ( Pryzant et al. , 2017 ) applying adversarial training to only capture domain-shared translation knowledge , we further exploit domain-specific translation knowledge for multi-domain NMT .
Also , in sharp contrast to ( Chen et al. , 2017a ) , our model not only exploits the source-side word- level domain contexts differently , but also employs a word- level cost weighting strategy for multi-domain NMT .
Conclusion and Future Work
In this work , we have explored how to utilize word- level domain contexts for multi-domain NMT .
By jointly modeling NMT and domain classification tasks , we utilize the sentence representations of source-side domain classifier and ad-versarial domain classifier to construct domainspecific and domain-shared source annotations , which are then exploited by decoder .
Moreover , using the attentional weights of target -side domain classifier , we adjust the weights of target words in the training objective to refine model training .
Experimental results and in- depth analyses demonstrate the effectiveness of the proposed model .
In the future , we would like to extend the proposed word - level cost weighting strategy to source words .
Besides , our method is also general to other NMT models .
Therefore , we plan to apply our method to the NMT with complex architectures , for example , lattice - to-sequence NMT , hierarchy - to- sequence NMT ( Su et al. , 2018 ) , NMT with context - aware encoder and Transformer ( Vaswani et al. , 2017 ) and so on .
Figure 1 : 1 Figure 1 : Word-level correlation heat map to Laws domain for a Chinese ( CH ) - English ( EN ) parallel sentence .
