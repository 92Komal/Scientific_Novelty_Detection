title
LIUM 's Contributions to the WMT2019 News Translation Task : Data and Systems for German â†”French Language Pairs
abstract
This paper describes the neural machine translation ( NMT ) systems of the LIUM Laboratory developed for the French ?
German news translation task of the Fourth Conference on Machine Translation ( WMT 2019 ) .
The chosen language pair is included for the first time in the WMT news translation task .
We describe how the training and the evaluation data was created .
We also present our participation in the French ?
German translation directions using self-attentional Transformer networks with small and big architectures .
Introduction
Since the start of the WMT translation shared tasks in 2006 , English has been involved in the majority of translation directions .
Few exceptions have been seen in 2012 and 2013 where Czech was also proposed as source and target for several language pairs .
This overwhelming disparity is due to the fact that English is available in large quantity , in both monolingual and bilingual corpora .
We think that this may be problematic for research purposes since considering English ( either as source or target language ) may hide many linguistic problems .
For example , considering gender agreement , which does not exist in English , translating from English is harder because of the lack of source side information , and translating towards English is simpler since the agreement should be ignored .
Generally speaking , English is a rather morphologically impoverished language , for instance having few gender agreement cases or conjugated verb forms .
This contrasts with French and German where number and gender agreements are very frequent .
That is why we introduced two new translation directions involving two European languages , namely French and German .
DE ?
FR language pair Training data
The training data for this language pair was created by cross-matching the training data from the previous WMT shared tasks for the EN - FR and EN - DE language pairs .
The details of the corpora are provided in Table 1 in which we provide the original sizes of EN - FR and EN - DE corpora and the extracted parallel corpora in DE - FR .
Overall , we were able to create a German - French parallel corpus with 153.2 M and 171.1 M words respectively .
Development and test data
The data collected for the FR ?
DE language pair has been created from several online news websites .
The development and test sets have been created from news articles in both French and German .
The development set is the fruit of a collaboration with the Faculty of Literature and Humanities of the University of Le Mans during several Digital Humanities ( DH ) lab sessions .
The purpose of these quality sessions is twofold : on the first hand , students would learn and comprehend the inherent concepts of using a computer assisted translation ( CAT ) tool in the context of DH classes ( Baillot et al. , 2019 ) .
On the other hand , the translated data is intended to be used for Machine Translation research purposes .
This process led to a 1512 sentences 1 development corpus distributed during the WMT2019 shared task .
While creating the development data we intentionally mixed ( to some degree ) the translation directions , therefore 462 sentences were translated from French to German and the reverse for the remaining 1050 sentences .
LIUM Submissions
All our systems are constrained as we only used the supplied parallel data ( described in table 1 ) with additional back - translations created from a subset of the monolingual news data made available by the shared task organizers .
Model Description
For our submissions we used the Transformer ( Vaswani et al. , 2017 ) sequence - to-sequence model as implemented in fairseq ( Ott et al. , 2019 ) .
Transformer is the state of the art NMT model which rely on a multi-headed attention applied as self-attention to source and target sentences .
Our models are based on both small and big Transformer configurations .
All experiments with the big transformer are models with 6 blocks in the encoder and decoder networks following the configuration described in ( Ott et al. , 2018 ) .
With respect to the small transformer model , we also used a 6 blocks encoder and decoder network with an embedding layer of size 512 , a feed-forward layer with an inner dimension of 1024 , and a multiheaded attention with 4 attention heads .
We use a vocabulary of 35 K units based on a joint source and target byte pair encoding ( Sennrich et al. , 2016 ) .
We set the batch size to 2048 tokens and maximum sentence length to 150 BPE units , in order to fit the big Transformer configuration to our GPUs ( NVIDIA GeForce GTX 1080 Ti with 11 GB RAM ) .
Data Preparation
Our preparation pipeline consists of a preprocessing step performed using scripts from Moses ( Koehn et al. , 2007 ) .
We replace the unicode punctuation , normalize the punctuation and remove the non-printing characters before the tokenization .
After the tokenization step , we perform a cleaning stage where all source and target sentences with an overlapping rate higher than 65 % are deleted .
Statistics of the training corpora after the cleaning process are presented in table 2 .
These values should be contrasted with those of table 1 to assess the effect of the cleaning process .
As it can be seen from tables 1 and 2 , the effect of the cleaning step is more pronounced for the noisy parallel corpora ( i.e. ParaCrawl and Common Crawl ) .
For the europarl - v7 corpus , more than a thousand lines are removed after cleaning which mainly corresponds to English sentences in both languages : FR and DE as well as sentences with long lists of numbers .
In addition to the available parallel data , we have used monolingual News Crawl articles as additional synthetic bilingual data .
We used only news 2018 from which we selected a subpart based on cross-entropy data selection method ( Moore and Lewis , 2010 ) .
Data selection was performed with the europarl corpus as in-domain data and using the XenC Toolkit ( Rousseau , 2013 ) .
By doing this , we were able to extract 3.4 M German sentences out of the 38.6 M sentences of the monolingual German 2018 News Crawl corpus .
Similarly , 3.3 M sentences were extracted out of the 8.2 M monolingual French 2018 News Crawl .
Experiments and Results
In this section , we first present the results for German to French translation direction followed by the French to German direction .
We use BLEU as evaluation metric ( Papineni et al. , 2002 ) and all reported scores are calculated using case-sensitive detokenized BLEU with multi-bleu.pl .
All results use beam search with a beam width of 12 and length penalty of 1 .
German to French
In this section we present the results for German to French direction .
We have tried three different configurations differentiated by the training data used to create the NMT system .
For each of these configurations , we trained a small and a big transformer model .
Given the prior knowledge about the noisy quality of the ParaCrawl corpus , we first tried to train some NMT systems with all available parallel data from table 3 except ParaCrawl .
Table 4 contains the results for this setting .
We report the results with the best checkpoint and an ensembledecoding with 2 and 5 checkpoints for small and big Transformer versions .
As expected , the big transformer outperforms the small version and we obtain an improvement of 1.69 BLEU point for the ensemble - decoding of 5 checkpoints .
Asterisk ( * ) in Table 4 marks our submitted model for German to French official evaluation .
This model obtains a BLEU score of 33.4 .
Our best system with back -translation was also submitted after the evaluation deadline and obtain a BLEU score of 34.6 .
French to German
We performed the same set of experiments as German to French .
Table 7 shows the BLEU scores when NMT systems are trained without the ParaCrawl corpus .
Unlike the German to French direction , only a small improvement is observed by using the big transformer architecture compared to the small one ( 21 As for the DE ?
Fr direction , we also trained systems by adding ParaCrawl data and results are presented in Table 9 .
As was formerly the case with DE ?
Fr , no improvement is observed by adding the Paracrawl corpus to the small transformer model .
The model works less well than without Paracrawl and a drop of 0.4 % BLEU points is observed when we compare the " + Ensemble ( x5 ) " of small transformer models from tables 7 and 8 .
For the big transformer model there is an improvement of 0.76 BLEU point when the Paracrawl corpus is included in the training data .
Conclusion
In this paper , we presented the LIUM participation to the WMT2019 news translation shared task .
This year we have added for the first time the French - German language pair to the WMT news translation task .
The parallel training data were created by cross-matching the EN - FR and EN - DE training data from previous WMT shared tasks .
The LIUM has participated in the German ?
French translation task with an ensemble of neural machine translation models based on the Transformer architecture .
Our models were trained using a cleaned subset of the provided training dataset , and synthetic parallel data generated from the provided monolingual corpora .
Table 1 : 1 Training corpora statistics ( number of sentences ) for FR?DE News translation shared task .
The second line of each cell corresponds to the number of tokens in French followed by the number of tokens in German .
been followed for the test set creation : 335 of the 1701 test sentences have been produced from French documents and the 1366 remaining pairs from German documents .
We note that 756 out of the German 1366 German sentences in the test set have been translated into French by professional translators 2 .
The dev and test sets are freely distributed and available for download 3 .
The same process has
