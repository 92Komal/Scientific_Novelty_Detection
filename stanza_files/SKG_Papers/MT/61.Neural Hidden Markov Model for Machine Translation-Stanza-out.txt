title
Neural Hidden Markov Model for Machine Translation
abstract
This work aims to investigate alternative neural machine translation ( NMT ) approaches and thus proposes a neural hidden Markov model ( HMM ) consisting of neural network - based alignment and lexicon models .
The neural models make use of encoder and decoder components , but drop the attention component .
The training is end-to-end and the standalone decoder is able to provide comparable performance with the state - of - the - art attention - based models on three different translation tasks .
Introduction Attention - based neural translation models ( Bahdanau et al. , 2015 ; Luong et al. , 2015 ) attend to specific positions on the source side to generate translation .
Using the attention component provides significant improvements over the pure encoder-decoder sequence - to-sequence approach ( Sutskever et al. , 2014 ) that uses no such attention mechanism .
In this work , we aim to compare the performance of attention - based models to another baseline , namely , neural hidden Markov models .
The neural HMM has been successfully applied in the literature on top of conventional phrasebased systems ( Wang et al. , 2017 ) .
In this work , our purpose is to explore its application in standalone decoding , i.e. the model is used to generate and score candidates without assistance from a phrase - based system .
Because translation is done standalone using only neural models , we still refer to this as NMT .
In addition , while Wang et al . ( 2017 ) applied feedforward networks to model alignment and translation , the recurrent structures proposed in this work surpass the feedforward variants by up to 1.3 % in BLEU .
By comparing neural HMM and attention - based NMT , we shed light on the role of the attention component .
To this end , we use an alignmentbased model that has a recurrent bidirectional encoder and a recurrent decoder , but use no attention component .
We replace the attention mechanism by a first-order HMM alignment model .
Attention levels are deterministic normalized similarity scores part of the architecture design of an otherwise fully supervised classifier .
HMM -style alignments on the other hand are discrete random variables and ( unlike attention levels ) must be marginalized .
Once alignments are marginalized , which is tractable for a first-order HMM , parameters can be estimated to attain a local optimum of log-likelihood of observations as usual .
Motivation
In attention - based approaches , the alignment distribution is used to select the positions in the source sentence that the decoder attends to during translation .
Thus the alignment model can be considered as an implicit part of the translation model .
On the other hand , separating the alignment model from the lexicon model has its own advantages :
First of all , this leads to more flexibility in modeling and training :
The models can not only be trained separately , but they can also have different model types , such as neural models , count- based models , etc .
Second , the separation avoids propagating errors from one model to another .
In attention - based systems , the translation score is based on the alignment distribution , in which errors can be propagated from the alignment part to the translation part .
Third , probabilistic treatment to alignments in NMT typically implies an extended degree of interpretability ( e.g. one can inspect posteriors ) and control over the model ( e.g. one can impose priors over alignments and lexical distributions ) .
Neural Hidden Markov Model Given a source sentence f J 1 = f 1 ...f j ...f
J and a target sentence e I 1 = e 1 ...e i ...e
I , where j = b i is the source position aligned to the target position i , we model translation using an alignment model and a lexicon model : p( e I 1 |f J 1 ) = b I 1 p( e I 1 , b I 1 |f J 1 ) ( 1 ) := b I 1 I i=1 p( e i | b i 1 , e i?1 0 , f J 1 ) lexicon model ? p( b i | b i?1 1 , e i?1 0 , f J 1 ) alignment model ( 2 ) Instead of predicting the absolute source position b i , we use an alignment model Wang et al . ( 2017 ) applied feedforward neural networks for modeling the lexicon and alignment probabilities .
In this work , we would like to model these distributions using recurrent neural networks ( RNN ) .
RNNs have been shown to outperform feedforward variants in language and translation modeling .
This is mainly due to that RNN can handle arbitrary input lengths and thus include unbounded context information .
Unfortunately , the recurrent hidden layer cannot be easily applied for the neural hidden Markov model , since it will significantly complicate the computation of forward - backward messages when running Baum- Welch .
Nevertheless , we can apply long short-term memory ( LSTM ) ( Hochreiter and Schmidhuber , 1997 ) structure for source and target words embedding .
With this technique we can take the essence of LSTM RNN and do not break any sequential generative model assumptions .
p(?
i |b i?1 1 , e i?1 0 , f J 1 ) that predicts the jump ?
i = b i ? b i?1 .
Our models are close in structure to the model proposed in Luong et al . ( 2015 ) , where we have a component that encodes the source sentence , and another that encodes the target sentence .
As shown in Figure 1 , we use a source side bidirectional LSTM embedding h j = ? ? h j + ? ? h j , where ? ? h j = LSTM ( W , f j , ? ? h j?1 ) and ? ? h j = LSTM ( V , f j , ? ? h j+1 ) , as well as a target side LSTM embedding s i?1 = LSTM ( U , e i?1 , s i ?2 ) .
h j , ? ? h j , ? ? h j and s i?1 , s i?2 are vectors , W , V and U are weight matrices .
Before the non-linear hidden layers , there is a projection layer which f1 ? ? ? fj?1 fj fj +1 ? ? ? fJ e1 ? ? ? ei?2 ei?1 ? ? s i?1 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? h j ? ? h j p( ei|hj , si?1 , ei?1 ) Figure 1 : The architecture of our neural networks with LSTM RNN on source and target side .
concatenates h j , s i?1 and e i?1 .
Then the neural network - based lexicon model is given by p( e i | b i 1 , e i?1 0 , f J 1 ) := p( e i |h j , s i?1 , e i?1 ) ( 3 ) and the neural network - based alignment model p( b i | b i?1 1 , e i?1 0 , f J 1 ) := p(?
i |h j , s i?1 , e i?1 ) ( 4 ) where j = b i?1 .
The training criterion is the logarithm of sentence posterior probabilities over training sentence pairs ( F r , E r ) , r = 1 , ... , R : arg max ?
r log p ?
( E r | F r ) ( 5 )
The derivative for a single sentence pair ( F , E ) = ( f J 1 , e I 1 ) is : ? ? log p ?
( E|F ) = j , j i p i ( j , j|f J 1 , e I 1 ; ? ) ? ? ? log p( j , e i | j , e i?1 0 , f J 1 ; ? ) ( 6 ) with HMM posterior weights p i ( j , j|f J 1 , e I 1 ; ? ) , which can be computed using the forwardbackward algorithm .
The entire training procedure can be summarized as backpropagation in an EM framework : 1 . compute : ? the posterior HMM weights ? the local gradients ( backpropagation )
update neural network weights
In the decoding stage we still calculate the sum over alignments and apply a target- synchronous beam search for the target string .
The auxiliary quantity for each unknown partial string e i 0 is specified as Q( i , j ; e i 0 ) .
During search , the partial hypothesis is extended from e i?1 0 to e i 0 : Q( i , j ; e i 0 ) = j p( j , e i | j , e i?1 0 , f J 1 ) ? Q( i ? 1 , j ; e i?1 0 ) ( 7 )
The decoder is shown in Algorithm 1 .
In the innermost loop ( line 11 - 13 ) , alignments are hypothesized and used to calculate the auxiliary quantity Q( i , j ; e i 0 ) .
Then for each source position j , the lexical distribution over the full target vocabulary is computed ( line 14 ) .
The distributions are accumulated ( Q( i ; e i 0 ) = j Q( i , j ; e i 0 ) , line 16 ) , then sorted ( line 18 ) and the best candidate translations ( arg max e i Q ( i ; e i 0 ) ) lying within the beam are used to expand the partial hypotheses ( line 19 - 23 ) .
cache is a two -dimensional list of size J ? | V src | ( source vocabulary size ) , which is used to cache the current quantities .
Whenever a partial hypothesis in the beam ends with the sentence end symbol ( < EOF > ) , the counter will be increased by 1 ( line 26 - 28 ) .
The translation is terminated if the counter reaches the beam size or hypothesis sentence length reaches three times the source sentence length ( line 6 ) .
If a hypothesis stops but its score is worse than other hypotheses , it is eliminated from the beam , but it still contests non-terminated hypotheses .
During comparison the scores are normalized by hypothesis sentence length .
Note that we have no explicit coverage constraints .
This means that a source position can be revisited many times , whereby creating one - to - many alignment cases .
This also allows unaligned source words .
In the neural HMM decoder , word alignments are estimated and scored according to the distribution calculated by the neural network alignment model , leading alignment decisions to become part of the beam search .
The search space consists of both alignment and translation decisions .
In contrast , the search space in attentionbased decoding consists only of translation decisions .
The decoding complexity is O( J 2 ? I ) ( J = source sentence length , I = target sentence length ) return GETBEST ( hyps ) 33 : end function compared to O( J ? I ) for attention - based models .
These are theoretical complexities of decoding on a CPU only considering source and target sentence lengths .
In practice , the size of the neural network must also be taken into account , and there are some optimized matrix multiplications for decoding on a GPU .
In general , the decoding speed of our model is about 3 times slower than that of a standard attention model ( 1.07 sentences per second vs .
3.00 sentences per second ) on a single GPU .
This is still an initial decoder and we did not spend much time on accelerating its decoding yet .
The optimization of our decoder would be a promising future work .
Experiments
The experiments are conducted on the WMT 2017 German ?
English and Chinese ?
English translation tasks , which consist of 5 M and 23 M parallel sentence pairs respectively .
Translation quality is measured with the case sensitive BLEU ( Papineni et al. , 2002 ) and TER ( Snover et al. , 2006 ) metric on newstests 2017 , which contain 3004 ( German?
English ) and 2001 ( Chinese ?
English ) sentence pairs .
For German and English preprocessing , we use the Moses tokenizer with hyphen splitting , and perform truecasing with Moses scripts ( Koehn et al. , 2007 ) . For German ?
English subword segmentation , we use 20 K joint BPE operations .
For the Chinese data , we segment it using the Jieba 1 segmenter .
We then learn a BPE model on the segmented Chinese , also using 20 K merge operations .
During training , sentences with a length greater than 50 subwords are filtered out .
Attention - Based System
The attention - based systems are trained with Sockeye ( Hieber et al. , 2017 ) , which implement an attentional encoder-decoder with small modifications to the model in Bahdanau et al . ( 2015 ) .
The encoder and decoder word embeddings are of size 620 .
The encoder consists of a bidirectional layer with 1000 LSTMs with peephole connections to encode the source side .
We use Adam ( Kingma and Ba , 2015 ) as optimizer with a learning rate of 0.001 , and a batch size of 50 .
The network is trained with 30 % dropout for up to 500 K iterations and evaluated every 10 K iterations on the development set with BLEU .
Decoding is done using beam search with a beam size of 12 .
In the end the four best models are averaged as described in 1 https://github.com/fxsjy/jieba the beginning of Junczys - Dowmunt et al. ( 2016 ) .
Neural Hidden Markov Model
The entire neural hidden Markov model is implemented in TensorFlow ( Abadi et al. , 2016 ) .
The feedforward models have three hidden layers of sizes 1000 , 1000 and 500 respectively , with a 5 word source window and a 3 - gram target history .
200 nodes are used for word embeddings .
The output layer of the neural lexicon model consists of around 25 K nodes for all subword units , while the neural alignment model has a small output layer with 201 nodes , which reflects that the aligned position can jump within the scope from ?100 to 100 .
Apart from the basic projection layer , we also applied LSTM layers for the source and target words embedding .
The embedding layers have 350 nodes and the size of the projection layer is 800 ( 400 + 200 + 200 , Figure 1 ) .
We use Adam as optimizer with a learning rate of 0.001 .
Neural lexicon and alignment models are trained with 30 % dropout and the norm of the gradient is clipped with a threshold 1 ( Pascanu et al. , 2014 ) .
In decoding we use a beam size of 12 and the element - wise average of all weights of the four best models also results in better performance .
Results
We compare the neural HMM approach ( Subsection 5.2 ) with the state - of - the - art attention - based approach ( Subsection 5.1 ) on different translation tasks .
The results are presented in Table 1 . Compare to the model presented in Wang et al . ( 2017 ) , switching to LSTM models has a clear advantage , which improves the FFNN - based system by up to 1.3 % BLEU and 1.8 % TER .
It seems that the HMM model benefits from richer features , such as LSTM states , which are very similar to what an attention mechanism would require .
We actually WMT 2017 # free German ?
English English ?
German Chinese ?
English parameters BLEU [ % ] TER [ % ] BLEU [ % ] TER [ % ] BLEU [ % ] expected it to do with less , the reason being that alignment distributions get refined a posteriori and so they do not have to be as strong a priori .
We can also observe that the performance of our approach is comparable with the state - of - the - art attentionbased system with 25 M more parameters on all three tasks .
Alignment Analysis
We show an example from the German ?
English newstest 2017 in Figure 2 , along with the attention and alignment matrices .
We can observe that the neural network - based HMM could generate a more clear alignment path compared to the attention weights .
In this example , it can exactly estimate the alignment positions for words wanted and of .
Discussion
We described a novel formulation for a neural network - based machine translation system , which applied neural networks to the conventional hidden Markov model .
The training is end-to-end , the model is monolithic and can be used as a standalone decoder .
This results in a more modern and efficient way to use HMM in machine translation and enables neural networks to benefit from HMMs .
Experiments show that replacing attention with alignment does not improve the translation performance of NMT significantly .
One possible reason is that alignment may fail to capture relevant contexts as attention does .
While alignment aims to identify translation equivalents between two lan-guages , attention is designed to find relevant context for predicting the next target word .
Source words with high attention weights are not necessarily translation equivalents of the target word .
Although using alignment does not lead to significant improvements in terms of BLEU over attention , we think alignment - based NMT models are still useful for automatic post editing and developing coverage - based models .
These might be interesting future directions to explore .
+ SCORES ( hyp , j ) ?p align ( f j , j ? j )
Figure 2 : 2 Figure 2 : Attention weight and alignment matrices visualized in heat map form .
Generated by the attention NMT baseline , GIZA ++ and the neural hidden Markov model .
Table 1 : 1 Experimental results on WMT 2017 German ?
English and Chinese ?
English test sets .
All models are trained without synthetic data .
Single model is used for decoding .
TER [ % ]
1 ( Wang et al. , 2017 ) but applied in decoding instead of rescoring 2
This work 3 ( Bahdanau et al. , 2015 ) with small modifications ( Section 5.1 )
