title
Name-aware Machine Translation
abstract
We propose a Name-aware Machine Translation ( MT ) approach which can tightly integrate name processing into MT model , by jointly annotating parallel corpora , extracting name - aware translation grammar and rules , adding name phrase table and name translation driven decoding .
Additionally , we also propose a new MT metric to appropriately evaluate the translation quality of informative words , by assigning different weights to different words according to their importance values in a document .
Experiments on Chinese-English translation demonstrated the effectiveness of our approach on enhancing the quality of overall translation , name translation and word alignment over a high-quality MT baseline 1 .
Introduction
A shrinking fraction of the world 's Web pages are written in English , therefore the ability to access pages across a range of languages is becoming increasingly important .
This need can be addressed in part by cross-lingual information access tasks such as entity linking ( McNamee et al. , 2011 ; Cassidy et al. , 2012 ) , event extraction ( Hakkani - Tur et al. , 2007 ) , slot filling ( Snover et al. , 2011 ) and question answering ( Parton et al. , 2009 ; Parton and McKeown , 2010 ) .
A key bottleneck of highquality cross-lingual information access lies in the performance of Machine Translation ( MT ) .
Traditional MT approaches focus on the fluency and accuracy of the overall translation but fall short in their ability to translate certain content words including critical information , especially names .
1 Some of the resources and open source programs developed in this work are made freely available for research purpose at http://nlp.cs.qc.cuny.edu/NAMT.tgz
A typical statistical MT system can only translate 60 % person names correctly .
Incorrect segmentation and translation of names which often carry central meanings of a sentence can also yield incorrect translation of long contexts .
Names have been largely neglected in the prior MT research due to the following reasons : ?
The current dominant automatic MT scoring metrics ( such as Bilingual Evaluation Understudy ( BLEU ) ( Papineni et al. , 2002 ) ) treat all words equally , but names have relative low frequency in text ( about 6 % in newswire and only 3 % in web documents ) and thus are vastly outnumbered by function words and common nouns , etc .. ?
Name translations pose a greater complexity because the set of names is open and highly dynamic .
It is also important to acknowledge that there are many fundamental differences between the translation of names and other tokens , depending on whether a name is rendered phonetically , semantically , or a mixture of both . ?
The artificial settings of assigning low weights to information translation ( compared to overall word translation ) in some largescale government evaluations have discouraged MT developers to spend time and explore resources to tackle this problem .
We propose a novel Name-aware MT ( NAMT ) approach which can tightly integrate name processing into the training and decoding processes of an end-to - end MT pipeline , and a new name - aware metric to evaluate MT which can assign different weights to different tokens according to their importance values in a document .
Compared to previous methods , the novel contributions of our approach are :
Baseline MT
As our baseline , we apply a high- performing Chinese -English MT system ( Zheng , 2008 ; Zheng et al. , 2009 ) based on hierarchical phrase - based translation framework ( Chiang , 2005 ) .
It is based on a weighted synchronous context-free grammar ( SCFG ) .
All SCFG rules are associated with a set of features that are used to compute derivation probabilities .
The features include : ? Relative frequency in two directions P ( ?|? ) and P ( ?|? ) , estimating the likelihoods of one side of the rule r : X ?< ? , ? > translating into the other side , where ? and ? are strings of terminals and non-terminals in the source side and target side .
Non-terminals in ? and ? are in one- to - one correspondence .
?
Lexical weights in two directions : P w ( ?|? ) and P w ( ?|? ) , estimating likelihoods of words in one side of the rule r : X ?< ? , ? > translating into the other side ( Koehn et al. , 2003 ) . ?
Phrase penalty : a penalty exp ( 1 ) for a rule with no non-terminal being used in derivation .
?
Rule penalty : a penalty exp ( 1 ) for a rule with at least one non-terminal being used in derivation .
?
Glue rule penalty : a penalty exp ( 1 ) if a glue rule used in derivation .
?
Translation length : number of words in translation output .
Our previous work showed that combining multiple LMs trained from different sources can lead to significant improvement .
The LM used for decoding is a log-linear combination of four word n-gram LMs which are built on different English corpora ( details described in section 5.1 ) , with the LM weights optimized on a development set and determined by minimum error rate training ( MERT ) , to estimate the probability of a word given the preceding words .
All four LMs were trained using modified Kneser - Ney smoothing algorithm ( Chen and Goodman , 1996 ) and converted into Bloom filter LMs ( Talbot and Brants , 2008 ) supporting memory map .
The scaling factors for all features are optimized by minimum error rate training algorithm to maximize BLEU score ( Och , 2003 ) .
Given an input sentence in the source language , translation into the target language is cast as a search problem , where the goal is to find the highest - probability derivation that generates the source -side sentence , using the rules in our SCFG .
The source-side derivation corresponds to a synchronous targetside derivation and the terminal yield of this targetside derivation is the output of the system .
We employ our CKY - style chart decoder , named SRInterp , to solve the search problem .
Name-aware MT
We tightly integrate name processing into the above baseline to construct a NAMT model .
Figure 1 depicts the general procedure .
Training
This basic training process of NAMT requires us to apply a bilingual name tagger to annotate parallel training corpora .
Traditional name tagging approaches for single languages cannot address this requirement because they were all built on data and resources which are specific to each language without using any cross-lingual features .
In addition , due to separate decoding processes the results on parallel data may not be consistent across languages .
We developed a bilingual joint name tagger ( Li et al. , 2012 ) based on conditional random fields that incorporates both monolingual and cross-lingual features and conducts joint inference , so that name tagging from two languages can mutually enhance each other and therefore inconsistent results can be corrected simultaneously .
This joint name tagger achieved 86.3 % bilingual pair F-measure with manual alignment and 84.4 % bilingual pair F-measure with automatic alignment as reported in ( Li et al. , 2012 ) .
Given a parallel sentence pair we first apply Giza ++ ( Och and Ney , 2003 ) to align words , and apply this join - t bilingual name tagger to extract three types of names : ( Person ( PER ) , Organization ( ORG ) and Geo-political entities ( GPE ) ) from both the source side and the target side .
We pair two entities from two languages , if they have the same entity type and are mapped together by word alignment .
We ignore two kinds of names : multi-word names with conflicting boundaries in two languages and names only identified in one side of a parallel sentence .
We built a NAMT system from such nametagged parallel corpora .
First , we replace tagged name pairs with their entity types , and then use Giza + + and symmetrization heuristics to regenerate word alignment .
Since the name tags appear very frequently , the existence of such tags yields improvement in word alignment quality .
The re-aligned parallel corpora are used to train our NAMT system based on SCFG .
Since the joint name tagger ensures that each tagged source name has a corresponding translation on the target side ( and vice versa ) , we can extract SCFG rules by treating the tagged names as non-terminals .
However , the original parallel corpora contain many high- frequency names , which can already be handled well by the baseline MT .
Some of these names carry special meanings that may influence translations of the neighboring words , and thus replacing them with non-terminals can lead to information loss and weaken the translation model .
To address this issue , we merged the name- replaced parallel data with the original parallel data and extract grammars from the combined corpus .
For example , given the following sentence pair : ? -? ? e ? ?e ? ? . ?
China appeals to world for non involvement in Angola conflict . after name tagging it becomes ?
GPE ? e ? ?e GPE ? . ? GPE appeals to world for non involvement in GPE conflict .
Both sentence pairs are kept in the combined data to build the translation model .
Decoding During decoding phase , we extract names with the baseline monolingual name tagger described in ( Li et al. , 2012 ) from a source document .
Its performance is comparable to the best reported results on Chinese name tagging on Automatic Content Extraction ( ACE ) data ( Ji and Grishman , 2006 ; Florian et al. , 2006 ; Zitouni and Florian , 2008 ; Nguyen et al. , 2010 ) .
Then we apply a state - of - the - art name translation system to translate names into the target language .
The name translation system is composed of the following steps :
In our NAMT framework , we add the following extensions to name translation .
We developed a name origin classifier based on Chinese last name list ( 446 name characters ) and name structure parsing features to distinguish Chinese person names and foreign person names ( Ji , 2009 ) , so that pinyin conversion is applied for Chinese names while name transliteration is applied only for foreign names .
This classifier works reasonably well in most cases ( about 92 % classification accuracy ) , except when a common Chinese last name appears as the first character of a foreign name , such as " 1 ? " which can be translated either as " Jolie " or " Zhu Li " .
For those names with fewer than five instances in the training data , we use the name translation system to provide translations ; for the rest of the names , we leave them to the baseline MT model to handle .
The joint bilingual name tagger was also exploited to mine bilingual name translation pairs from parallel training corpora .
The mapping score between a Chinese name and an English name was computed by the number of aligned tokens .
A name pair is extracted if the mapping score is the highest among all combinations and the name types on both sides are identical .
It is necessary to incorporate word alignment as additional constraints because the order of names is often changed after translation .
Finally , the extracted 9,963 unique name translation pairs were also used to create an additional name phrase table for NAMT .
Manual evaluation on 2,000 name pairs showed the accuracy is 86 % .
The non-terminals in SCFG rules are rewritten to the extracted names during decoding , therefore allow unseen names in the test data to be translated .
Finally , based on LMs , our decoder exploits the dynamically created phrase table from name translation , competing with originally extracted rules , to find the best translation for the input sentence .
Name- aware MT Evaluation Traditional MT evaluation metrics such as BLEU ( Papineni et al. , 2002 ) and Translation Edit Rate ( TER ) ( Snover et al. , 2006 ) assign the same weights to all tokens equally .
For example , incorrect translations of " the " and " Bush " will receive the same penalty .
However , for crosslingual information processing applications , we should acknowledge that certain informationally critical words are more important than other common words .
In order to properly evaluate the translation quality of NAMT methods , we propose to modify the BLEU metric so that they can dynamically assign more weights to names during evaluation .
BLEU considers the correspondence between a system translation and a human translation : BLEU = BP ? exp N n=1 w n log p n ( 1 ) where BP is brevity penalty defined as follows : BP = 1 if c > r , e ( 1?r/ c ) if c ? r. ( 2 ) where w n is a set of positive weights summing to one and usually uniformly set as w n = 1/N , c is the length of the system translation and r is the length of reference translation , and p n is modified n-gram precision defined as : p n = C?Candidates n-gram ?C
Count clip ( n- gram ) C ? Candidates n-gram ?C
Count clip ( n- gram ) ( 3 ) where C and C are translation candidates in the candidate sentence set , if a source sentence is translated to many candidate sentences .
As in BLEU metric , we first count the maximum number of times an n-gram occurs in any single reference translation .
The total count of each candidate n-gram is clipped at sentence level by its maximum reference count .
Then we add up the weights of clipped n-grams and divide them by the total weight of all n-grams .
Based on BLEU score , we design a name - aware BLEU metric as follows .
Depending on whether a token t is contained in a name in reference translation , we assign a weight weight t to t as follows : D ) , if t never appears in names weightt = 1 ? e ? tf ( t , d ) ? idf ( t , 1 + P E Z , if t occurs in name ( s ) ( 4 ) where P E is the sum of penalties of non-name tokens and Z is the number of tokens within all names : P E = t never appears in names e ? tf ( t , d ) ? idf ( t , D ) ( 5 )
In this paper , the tf ? idf score is computed at sentence level , therefore , D is the sentence set and each d ?
D is a sentence .
The weight of an n-gram in reference translation is the sum of weights of all tokens it contains .
weight ngram = t?ngram weight t ( 6 ) Next , we compute the weighted modified ngram precision Count weight?clip ( n- gram ) as follows : Count weight?clip ( n- gram ) = if the ngram i is correctly translated weight ngram i ( 7 ) The Count clip ( n- gram ) in the equation 3 is substituted with above Count weight?clip ( n- gram ) .
When we sum up the total weight of all n-grams of a candidate translation , some n-grams may contain tokens which do not exist in reference translation .
We assign the lowest weight of tokens in reference translation to these rare tokens .
We also add an item , name penalty N P , to penalize the output sentences which contain too many or too few names : NP = e ?( u v ?1 ) 2 /2 ? ( 8 ) where u is the number of name tokens in system translation and v is the number of name tokens in reference translation .
Finally the name- aware BLEU score is defined as : BLEUNA = BP ? N P ? exp N n=1 wn log wpn ( 9 )
This new metric can also be applied to evaluate MT approaches which emphasize other types of facts such as by simply replacing name tokens by other fact tokens .
Experiments
In this section we present the experimental results of NAMT compared to the baseline MT .
Data Set
We used a large Chinese - English MT training corpus from various sources and genres ( including newswire , web text , broadcast news and broadcast conversations ) for our experiments .
We also used some translation lexicon data and Wikipedia translations .
The majority of the data sets were collected or made available by LDC for U. Four LMs , denoted LM1 , LM2 , LM3 , and LM4 , were trained from different English corpora .
LM1 is a 7 - gram LM trained on the tar-get side of Chinese-English and Egyptian Arabic - English parallel text , English monolingual discussion forums data R1 - R4 released in BOLT Phase 1 ( LDC2012E04 , LDC2012E16 , LDC2012E21 , LDC2012E54 ) , and English Gigaword Fifth Edition ( LDC2011T07 ) .
LM2 is a 7 - gram LM trained only on the English monolingual discussion forums data listed above .
LM3 is a 4 - gram LM trained on the web genre among the target side of all parallel text ( i.e. , web text from pre-BOLT parallel text and BOLT released discussion forum parallel text ) .
LM4 is a 4 - gram LM trained on the English broadcast news and conversation transcripts released under the DARPA GALE program .
Note that for LM4 training data , some transcripts were quick transcripts and quick rich transcripts released by LDC , and some were generated by running flexible alignment of closed captions or speech recognition output from LDC on the audio data ( Venkataraman et al. , 2004 ) .
In order to demonstrate the effectiveness and generality of our approach , we evaluated our approach on seven test sets from multiple genres and domains .
We asked four annotators to annotate names in four reference translations of each sentence and an expert annotator to adjudicate results .
The detailed statistics and name distribution of each test data set is shown in Table 1 .
The percentage of names occurred fewer than 5 times in training data are listed in the brackets in the last column of the table .
Overall Performance
Besides the new name - aware MT metric , we also adopt two traditional metrics , TER to evaluate the overall translation performance and Named Entity Weak Accuracy ( NEWA ) ( Hermjakob et al. , 2008 ) to evaluate the name translation performance .
TER measures the amount of edits required to change a system output into one of the reference translations .
Specifically : TER = # of edits average # of reference words ( 10 ) Possible edits include insertion , substitution deletion and shifts of words .
The NEWA metric is defined as follows .
Using a manually assembled name variant table , we also support the matching of name variants ( e.g. , " World Health Organization " and " WHO " ) .
For better comparison with NAMT , besides the original baseline , we develop the other baseline system by adding name translation table into the phrase table ( NPhrase ) .
Table 2 presents the performance of overall translation and name translation .
We can see that except for the BOLT3 data set with BLEU metric , our NAMT approach consistently outperformed the baseline system for all data sets with all metrics , and provided up to 23.6 % relative error reduction on name translation .
According to Wilcoxon Matched - Pairs Signed - Ranks
Test , the improvement is not significant with BLEU metric , but is significant at 98 % confidence level with all of the other metrics .
The gains are more significant for formal genres than informal genres mainly because most of the training data for name tagging and name translation were from newswire .
Furthermore , using external name translation table only did not improve translation quality in most test sets except for BOLT2 .
Therefore , it is important to use name- replaced corpora for rule extraction to fully take advantage of improved word alignment .
Many errors from the baseline MT approach oc-curred because some parts of out - of- vocabulary names were mistakenly segmented into common words .
For example , the baseline MT system mistakenly translated a person name " Y ? ? ( Sun Honglei ) " into " Sun red thunder " .
In informal genres such as discussion forums and web blogs , even common names often appear in rare forms due to misspelling or morphing .
For example , " e8l ( Obama ) " was mistakenly translated into " Ma Olympic " .
Such errors can be compounded when word re-ordering was applied .
For example , the following sentence : " ? " ? / : 'J / i y ( Guo Meimei 's strength really is formidable , I really admire her ) " was mistakenly translated into " Guo the strength of the America and the America also really strong , ah , really admire her " by the baseline MT system because the person name " ? ( Guomeimei ) " was mistakenly segmented into three words " ? ( Guo ) " , " ? ( the America ) " and " ? ( the America ) " .
But our NAMT approach successfully identified and translated this name and also generated better overall translation : " Guo Meimei 's power is also really strong , ah , really admire her " .
Name- aware BLEU vs The Human Evaluation
In order to investigate the correlation between name - aware BLEU scores and human judgment results , we asked three bi-lingual speakers to judge our translation output from the baseline system and the NAMT system , on a Chinese subset of 250 sentences ( each sentence has two corresponding translations from baseline and NAMT ) extracted randomly from 7 test corpora .
The annotators rated each translation from 1 ( very bad ) to 5 ( very good ) and made their judgments based on whether the translation is understandable and conveys the same meaning .
We computed the name - aware BLEU scores on the subset and also the aggregated average scores from human judgments .
Figure 2 shows that NAMT consistently achieved higher scores with both name - aware BLEU metric and human judgement .
Furthermore , we calculated three Pearson product-moment correlation coefficients between human judgment scores and name - aware BLEU scores of these two MT systems .
Give the sample size and the correlation coefficient value , the high significance value of 0.99 indicates that nameaware BLEU tracks human judgment well .
Word Alignment
It is also important to investigate the impact of our NAMT approach on improving word alignment .
We conducted the experiment on the Chinese-English Parallel Treebank ( Li et al. , 2010 ) with ground -truth word alignment .
The detailed procedure following NAMT framework is as follows : ( 1 ) Ran the joint bilingual name tagger ; ( 2 ) Replaced each name string with its name type ( PER , ORG or GPE ) , and ran Giza ++ on the replaced sentences ; ( 3 ) Ran Giza ++ on the words within each name pair .
( 4 ) Merged ( 2 ) and ( 3 ) to produce the final word alignment results .
In order to compare with the upper-bound gains , we also measured the performance of applying ground - truth name tagging with the above procedures .
The experiment results are shown in Table 3 .
For the words within names , our approach provided significant gains by enhancing F-measure from 46.0 % to 50.3 % .
Only 10.6 % words are within names , therefore the upper-bound gains on overall word alignment is only 1.3 % .
Our joint name tagging approach achieved 0.4 % ( statistically significant ) improvement over the baseline .
In Figure 3 we categorized the sentences according to the percentage of name words in each sentence and measured the improvement for each category .
We can clearly see that as the sentences include more names , the gains achieved by our approach tend to be greater .
Remaining Error Analysis
Although the proposed model has significantly enhanced translation quality , some challenges remain .
We analyze some major sources of the remaining errors as follows .
1 . Name Structure Parsing .
We found that the gains of our NAMT approach were mainly achieved for names with one or two components .
When the name structure becomes too complicated to parse , name tagging and name translation are likely to produce errors , especially for long nested organizations .
For example , " ?0 ? ?b ? @ " ( Anti-malfeasance Bureau of Gutian County Procuratorate ) consists of a nested organization name with a GPE as modifier : " ? 0 ? ?b " ( Gutian County Procuratorate ) and an ORG name : " ? @ " ( Anti-malfeasance Bureau ) .
2 . Name abbreviation tagging and translation .
Some organization abbreviations are also difficult to extract because our name taggers have not incorporated any coreference resolution techniques .
For example , without knowing that " FAW " refers to " First Automotive Works " in " FAW has also utilized the capital market to directly finance , and now owns three domestic listed companies " , our system mistakenly labeled it as a GPE .
The same challenge exists in name alignment and translation ( for example , " i ( Min Ge ) " refers to " -? Zi} ?X " ( Revolutionary Committee of the Chinese Kuomintang ) .
3 . Cross-lingual information transfer English monolingual features normally generate higher confidence than Chinese features for ORG names .
On the other hand , some good propagated Chinese features were not able to correct English results .
For example , in the following sentence pair : " 9n - ?
OET ? r ? " ... ( in accordance with the tripartite agreement reached by China , Laos and the UNHCR on ) ... " , even though the tagger can successfully label " T ? r/ UNHCR " as an organization because it is a common Chinese name , English features based on previous GPE contexts still incorrectly predicted " UNHCR " as a GPE name .
Related Work
Two types of humble strategies were previously attempted to build name translation components which operate in tandem and loosely integrate into conventional statistical MT systems :
1 . Pre-processing : identify names in the source texts and propose name translations to the MT system ; the name translation results can be simply but aggressively transferred from the source to the target side using word alignment , or added into phrase table in order to enable the LM to decide which translations to choose when encountering the names in the texts .
Heuristic rules or supervised models can be developed to create " do-not- translate " list ( Babych and Hartley , 2003 ) or learn " when - to- transliterate " ( Hermjakob et al. , 2008 ) .
2 . Post-processing : in a cross-lingual information retrieval or question answering framework , online query names can be utilized to obtain translation and post-edit MT output ( Parton et al. , 2009 ; Parton and McKeown , 2010 ; Parton et al. , 2012 ) .
It is challenging to decide when to use name translation results .
The simple transfer method ensures all name translations appear in the MT output , but it heavily relies on word alignment and does not take into account word re-ordering or the words found in a name 's context ; therefore it could mistakenly break some context phrase structures due to name translation or alignment errors .
The LM selection method often assigns an inappropriate weight to the additional name translation table because it is constructed independently from translation of context words ; therefore after weighted voting most correct name translations are not used in the final translation output .
Our experimental results 2 confirmed this weakness .
More importantly , in these approaches the MT model was still mostly treated as a " black - box " because neither the translation model nor the LM was updated or adapted specifically for names .
Recently the wider idea of incorporating semantics into MT has received increased interests .
Most of them designed some certain semantic representations , such as predicate - argument structure or semantic role labeling ( Wu and Fung , 2009 ; Liu and Gildea , 2009 ; Meyer et al. , 2011 ; Bojar and Wu , 2012 ) , word sense disambiguation ( Carpuat and Wu , 2007 b ; Carpuat and Wu , 2007a ) and graph-structured grammar representation ( Jones et al. , 2012 ) .
Lo et al. ( 2012 ) proposed a semantic role driven MT metric .
However , none of these work declaratively exploited results from information extraction for MT .
Some statistical MT systems ( e.g. ( Zens et al. , 2005 ) , ( Aswani and Gaizauskas , 2005 ) ) have attempted to use text normalization to improve word alignment for dates , numbers and job titles .
But little reported work has shown the impact of joint name tagging on overall word alignment .
Most of the previous name translation work combined supervised transliteration approaches with LM based re-scoring ( Knight and Graehl , 1998 ; Al - Onaizan and Knight , 2002 ; Huang et al. , 2004 ) .
Some recent research used comparable corpora to mine name translation pairs ( Feng et al. , 2004 ; Kutsumi et al. , 2004 ; Udupa et al. , 2009 ; Ji , 2009 ; Fung and Yee , 1998 ; Rapp , 1999 ; Shao and Ng , 2004 ; Lu and Zhao , 2006 ; Hassan et al. , 2007 ) .
However , most of these approaches required large amount of seeds , suffered from Information Extraction errors , and relied on phonetic similarity , context co-occurrence and document similarity for re-scoring .
In contrast , our name pair mining approach described in this paper does not require any machine translation or transliteration features .
Conclusions and Future Work
We developed a name - aware MT framework which tightly integrates name tagging and name translation into training and decoding of MT .
Experiments on Chinese-English translation demonstrated the effectiveness of our approach over a high-quality MT baseline in both overall translation and name translation , especially for formal genres .
We also proposed a new name - aware evaluation metric .
In the future we intend to improve the framework by training a discriminative model to automatically assign weights to combine name translation and baseline translation with additional features including name confidence values , name types and global validation evidence , as well as conducting LM adaptation through bilingual topic modeling and clustering based on name annotations .
We also plan to jointly optimize MT and name tagging by propagating multiple word segmentation and name annotation hypotheses in lattice structure to statistical MT and conduct latticebased decoding ( Dyer et al. , 2008 ) .
Furthermore , we are interested in extending this framework to translate other out - of- vocabulary terms .
Figure 1 : 1 Figure 1 : Architecture of Name-aware Machine Translation System .
( 1 ) Dictionary matching based on 150,041 name translation pairs ; ( 2 ) Statistical name transliteration based on a structured perceptron model and a character based MT model ( Dayne and Shahram , 2007 ) ; ( 3 ) Context information extraction based re-ranking .
S. DARPA Translingual Information Detection , Extraction and Summarization ( TIDES ) program , Global Autonomous Language Exploitation ( GALE ) program , Broad Operational Language Translation ( BOLT ) program and National Institute of Standards and Technology ( NIST ) MT evaluations .
The training corpus includes 1,686,458 sentence pairs .
The joint name tagger extracted 1,890,335 name pairs ( 295,087 Persons , 1,269,056 Geopolitical entities and 326,192 Organizations ) .
Figure 2 : 2 Figure 2 : Scores based on Automatic Metrics and Human Evaluation .
Figure 3 : 3 Figure 3 : Word alignment gains according to the percentage of name words in each sentence .
Table 1 : 1 Statistics and Name Distribution of Test Data Sets .
NEWA = Count # of correctly translated names Count # of names in references ( 11 )
Table 2 : 2 Translation Performance ( % ) .
Table 3 : 3 Impact of Joint Bilingual Name Tagging on Word Alignment ( % ) .
Words Method P R F Baseline Giza ++ 69.8 47.8 56.7 Overall Joint Name 70.4 48.1 57.1 Words Tagging Ground-truth 71.3 48.9 58.0 Name Tagging ( Upper-bound ) Words Baseline Giza ++ 86.0 31.4 46.0 Within Joint Name 77.6 37.2 50.3 Names Tagging
