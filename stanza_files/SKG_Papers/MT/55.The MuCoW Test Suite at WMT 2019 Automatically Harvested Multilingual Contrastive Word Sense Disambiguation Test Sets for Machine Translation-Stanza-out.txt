title
The MUCOW test suite at WMT 2019 : Automatically harvested multilingual contrastive word sense disambiguation test sets for machine translation
abstract
Supervised Neural Machine Translation ( NMT ) systems currently achieve impressive translation quality for many language pairs .
One of the key features of a correct translation is the ability to perform word sense disambiguation ( WSD ) , i.e. , to translate an ambiguous word with its correct sense .
Existing evaluation benchmarks on WSD capabilities of translation systems rely heavily on manual work and cover only few language pairs and few word types .
We present MU - COW , a multilingual contrastive test suite that covers 16 language pairs with more than 200 000 contrastive sentence pairs , automatically built from word-aligned parallel corpora and the wide -coverage multilingual sense inventory of BabelNet .
We evaluate the quality of the ambiguity lexicons and of the resulting test suite on all submissions from 9 language pairs presented in the WMT19 news shared translation task , plus on other 5 language pairs using pretrained NMT models .
The MUCOW test suite is available at http://github.
com/Helsinki-NLP / MuCoW .
Introduction Neural Machine Translation ( NMT ) has provided impressive advances in translation quality , leading to a discussion whether translations produced by professional human translators can still be distinguished from the output of NMT systems , and to what extent automatic evaluation measures can reliably account for these differences ( Hassan Awadalla et al. , 2018 ; L?ubli et al. , 2018 ; Toral et al. , 2018 ) .
One answer to this question lies in the development of so-called test suites ( Burchardt et al. , 2017 ) or challenge sets ( Isabelle et al. , 2017 ) that focus on particular linguistic phenomena that are known to be difficult to evaluate with simple reference - based metrics such as BLEU .
Existing test suites focus e.g. on morphosyntactic and syn-tactic divergences between source and target language ( Burchardt et al. , 2017 ; Burlot and Yvon , 2017 ; Isabelle et al. , 2017 ; Sennrich , 2017 ; Burlot et al. , 2018 ; Macketanz et al. , 2018 ) or on discourse phenomena ( Guillou and Hardmeier , 2016 ; Bawden et al. , 2018 ; Guillou et al. , 2018 ) .
Another linguistic phenomenon that is challenging for translation is lexical ambiguity ( Liu et al. , 2018 ; Marvin and Koehn , 2018 ) , i.e. , words of the source language that have multiple translations in the target language representing different meanings .
Recently , Rios Gonzales et al. ( 2017 ) introduced a lexical ambiguity benchmark called Con-traWSD that is based on contrastive translation pairs : a sentence containing an ambiguous source word is paired with the correct reference translation and with a modified translation in which the ambiguous word has been replaced by a word of a different sense .
Contrastive evaluation makes use of the ability of NMT systems to score given translations : a contrast is considered successfully detected if the reference translation obtains a higher score than an artificially modified translation .
However , all these test suites require significant amounts of expert knowledge and manual work for identifying the divergences and compiling the examples , which typically limits their coverage to a small number of language pairs and directions .
For example , the test sets built by Rios Gonzales et al . ( 2017 ) cover only 65 ambiguous words for two language pair directions .
In this paper , we present a languageindependent method for automatically building ContraWSD - style test suites .
It involves the following steps : ( 1 ) identify ambiguous source words and their translations ; ( 2 ) cluster the translations into senses ; ( 3 ) select sentences with ambiguous words and create contrast pairs .
The setup proposed by Rios
As a result , we make available two variants of MUCOW , a multilingual contrastive word sense disambiguation test suite for machine translation .
The scoring variant covers 11 language pairs with a total of almost 240 000 sentence pairs .
The translation variant covers 9 language pairs with a total of 15 600 sentences .
The data and scoring scripts are available at https://github.
com / Helsinki-NLP / MuCoW .
Building MUCOW
In this section , we describe the three steps needed to create a MUCOW test suite and illustrate them with some German ?
English examples .
Step 1 : Identify ambiguous source words and their translations
We first compile a list of source language words that have a large number of distinct translations .
For this , we apply the eflomal word alignment tool ( ?stling and Tiedemann , 2016 ) that were aligned at least 10 times each with at least two distinct target words .
We use parallel corpora from the OPUS collection ( Tiedemann , 2012 ) , 1 counting only one - to - one word alignment links .
Table 1
We query BabelNet with each source word and take the intersection of the alignment - inferred target words and the BabelNet - inferred target words .
Crucially , we group the remaining target words according to the BabelNet sense clusters .
Finally , we combine those clusters that share at least one common target word .
Table 2 shows an example .
Step 2 b : Refine sense clusters with sense embeddings
It is known that lexical resources such as Babel - Net tend to suffer from overly fine granularity of their sense inventory ( Navigli , 2006 ; Palmer et al. , 2007 ) .
We therefore introduce an additional merging step : i ) we associate each Babel synset with an embedding , ii ) compute pairwise cosine similarities between synsets , iii ) and merge them if their embedding similarity is higher than a threshold ?.
Choosing a good Babel synset embedding and an optimal threshold is a difficult task .
We evaluated three Babel synset vector representations , using the existing German ?
English ContraWSD test suite as gold standard : Nasari ( Camacho - Collados et al. , 2016 ) is a vector representation built by combining the knowledge from Wikipedia and WordNet with word embeddings .
SW2V ( Mancini et al. , 2017 ) is a neural model that learns word and synset embeddings in a shared vector space exploiting a shallow graph - based disambiguation algorithm .
FastText -Centroid ( FT - C ) :
We also include a synset embedding representation by looking up the FastText word embeddings ( Bojanowski et al. , 2017 ) for all words in a synset and computing their centroid .
Note that Nasari and SW2V embeddings are tied to the ( language - independent ) BabelNet synset IDs and can therefore be applied in a straightforward way to non-English target languages .
2
As a baseline , we use the synset clusters obtained from Section 2.2 .
We compute precision and recall scores for all three embedding methods with ?
threshold values ranging from 0.15 to 0.65 with a 0.05 step size .
An inferred synset was considered correct if all its lexicalisations ( if present ) occurred in a single gold synset , and no lexicalisations of a gold synset were found in a different inferred synset .
In other words , an inferred synset was considered wrong if it had been falsely merged or if it had falsely been kept separate from another one .
Figure 1 shows the precision and recall curves .
All refinement methods improve precision , whereas recall only decreases at low thresholds .
Figure 2 shows F 1 and F 0.5 scores ; we deem the latter more sensible in the present setting as high precision is more important to us than high recall .
The FT -C and SW2V methods perform best at lower thresholds , while Nasari works best at high thresholds .
An additional manual evaluation was carried out with 50 random German words 3 and four settings that obtained high F 1 or F 0.5 scores .
As shown in Table 3 , the SW2V method with a threshold set at 0.3 obtained the highest precision value by a large margin and therefore also the best F 0.5 score .
We chose this setting for all languages .
Source words that end up with a single synset as a result of this step are discarded .
2.4 Step 3 : Selecting sentences and creating contrast pairs ( Scoring variant only )
We use the synset lexicon built in the previous step to guide the creation of contrast pairs .
We extract sentence pairs from the parallel corpora and group them by source word and target word sense .
We restrict the extraction process to sentences longer than 10 words and skip sentences in which the source or target item occurs more than once .
From this set , we randomly choose 20 instances of each sense from various corpus sources .
For each extracted sentence pair , a contrastive sentence pair is produced by keeping the source sentence identical , but replacing the target word in the target sentence by another lexicalisation from a different synset .
While this entirely automatic setup could give rise to inconsistencies which would require manual correction as in Rios Gonzales et al . ( 2017 ) , we argue that BabelNet constraints already provide some filtering ( for example mostly keeping number constant ) .
Given our aim to scale up to a large number of languages , the need for human intervention would make the creation of a large scale multilingual benchmark difficult and costly .
Statistics
We apply the three steps presented above to all to -English translation directions that were part of the Conference of Machine Translation ( WMT ) news translation task over the last years .
Table 4 summarizes the statistics of these resources .
The average number of senses per source word ranges between 2.0 and 2.11 ( 2.36 - 2.4 for ContraWSD ) .
The lexicons for the Baltic languages are small due to the small size of available parallel corpora .
Measuring machine translation WSD capability with MUCOW
The aim of MUCOW is to examine the ability of current machine translation systems to choose the ( Sennrich et al. , 2017 b ) . 5
The upper half of Table 5 reports ContraWSD and MUCOW accuracy scores as well as BLEU scores computed on the WMT17 test set .
The ranking of the three models is consistent across the three tasks .
Interestingly , the Transformer model ( trained on far less data than the Nematus model ) scores much better on the two test suites than the BLEU score would suggest , confirming the findings by Tang et al . ( 2018 ) .
The University of Edinburgh also makes available their NMT models for other WMT16 and WMT17 language pairs .
6 MUCOW accuracy scores of these models are shown in the lower half of Table 5 together with the WMT test set BLEU scores reported by the authors ( Sennrich et al. , 2016 a ( Sennrich et al. , , 2017a .
Even though we only assess the confidence of an NMT system in detecting the right sense of a single word within a sentence , the results show that WSD is still an issue in MT - even in stateof - the - art-systems - that requires further study .
Translation test suites for WMT 2019
As mentioned in Section 1 , the WMT test suite call requires a different setup that does not rely on scoring capabilities of the participating systems .
Therefore , we modified step ( 3 ) of our method to conform with these requirements , analogously to the modification of ContraWSD by .
As a beneficial side effect , we were also able to include language pairs with non-English target languages .
7
The changes to step ( 3 ) are the following : ?
The sentence pairs were filtered more aggressively .
We only kept sentence pairs in which both the source and target words were tagged as NOUNs by the respective UDPipe part-ofspeech tagger ( Straka and Strakov ? , 2017 ) . ?
Source sentences stemming from one of the WMT training corpora were excluded .
We only used sentences from the following OPUS corpora : Books , Tatoeba , TED2013 , EUBookstore and OpenSubtitles 2018 . ?
We only kept synsets for which we found at least 4 example sentences , and we retained at most 10 example sentences per sense . ?
If as a result of the above filters , all but one senses of a source word were removed , we removed the source word entirely .
?
We distinguished between in- domain and out-of- domain synsets .
A synset is considered out - of- domain if more than half of its example sentences come from OpenSubti-tles2018 .
The intuition behind this distinction is that most participating systems will be tuned towards the news domain and thus will not handle features of colloquial speech reliably .
?
We disregarded the automatically generated contrastive sentences .
We built the translation variant of MUCOW for 9 translation directions of the news task .
Table 6 shows some statistics .
The resulting test suites contain sentences of the source language together with the following metadata : the ambiguous source word , the list of correct target words ( the correct target synset ) , the list of incorrect target words ( the incorrect target synset ) , and information about the domain of the synsets .
Table 7 shows an example .
The source language sentences were sent ( without metadata ) to the WMT participants as part of the test set , and we received the translations for evaluation .
WMT 2019 test suite results
In order to assess the translation output of the WMT participants , we check whether any of the correct or incorrect target words listed in the metadata file can be identified in the tokenized and lowercased translation output .
Although the sentences have been selected to contain the uninflected base form both in the We used the Turku neural lemmatizer with pretrained models ( Kanerva et al. , 2019 ) .
For Lithuanian , as no pretrained model was available , we trained one using the respective available data from the Universal Dependencies project .
9 Examples that contained both correct and incorrect target words were counted as incorrect .
Recall = # examples with correct target words # total examples
For each language pair , EN?CS , EN?DE , EN?FI , EN?RU and EN?LT , results are shown respectively in Tables 9 to 13 .
Overall , we observe that systems perform quite well in WSD , achieving high precision overall .
For some translation directions , there is a big gap between in - domain and out - of- domain synsets , showing clearly that systems tuned towards news translation struggle to identify the right sense when tested on a different domain .
At the same time , online systems are more robust to domain mismatch , which is likely due to their use of a much larger variety of training data .
Interestingly , the Czech- English task shows opposite results , with online systems performing better on in-domain synsets than research systems .
Interestingly enough , having English as source side yields better overall precision comparing with English as target side .
One possible explanation could be found in the difficulty to obtain better encoder representations for morphologically rich languages .
Recall is better with English on the target side due to higher coverage ( Table 8 ) .
It would have been instructive to compare the MUCOW results with automatic or manual evaluation scores on the official WMT19 test set , but unfortunately , such scores were not available in time for all systems .
Conclusion
In this paper , we have presented MUCOW , an automatically built WSD test suite for machine translation that relies on large parallel corpora , the multilingual lexical resource BabelNet and language - independent synset embeddings .
We used the proposed benchmark to assess the WSD ability of NMT systems following two evaluation protocols : scoring both reference and contrastive translations with pretrained NMT models , and as translation test suite for the WMT19 news shared task .
We find that state - of- the - art and fine- tuned NMT systems still present some drawbacks on handling ambiguous words , especially when evaluated on out - of- domain data and when the encoder has to deal with a morphologically rich language .
It will be particularly instructive to see how well the WSD test suite results correlate with human evaluation scores and with recently proposed evaluation metrics that are based on semantic representations of the translations ( Gupta et al. , 2015 ; Shimanaka et al. , 2018 ) .
As future work we plan to further extend the test suite including more languages and parallel data , and make use of the contrastive sentences as adversarial examples during training .
Figure 1 : 1 Figure 1 : Precision ( dashed ) and recall ( solid lines ) values for different sense embeddings and thresholds .
