title
A Large Scale Distributed Syntactic , Semantic and Lexical Language Model for Machine Translation
abstract
This paper presents an attempt at building a large scale distributed composite language model that simultaneously accounts for local word lexical information , mid-range sentence syntactic structure , and long-span document semantic content under a directed Markov random field paradigm .
The composite language model has been trained by performing a convergent N-best list approximate EM algorithm that has linear time complexity and a followup EM algorithm to improve word prediction power on corpora with up to a billion tokens and stored on a supercomputer .
The large scale distributed composite language model gives drastic perplexity reduction over ngrams and achieves significantly better translation quality measured by the BLEU score and " readability " when applied to the task of re-ranking the N-best list from a state - of - theart parsing - based machine translation system .
Introduction
The Markov chain ( n- gram ) source models , which predict each word on the basis of previous n-1 words , have been the workhorses of state - of - the - art speech recognizers and machine translators that help to resolve acoustic or foreign language ambiguities by placing higher probability on more likely original underlying word strings .
Research groups ( Brants et al. , 2007 ; Zhang , 2008 ) have shown that using an immense distributed computing paradigm , up to 6 grams can be trained on up to billions and trillions of words , yielding consistent system improvements , but Zhang ( 2008 ) did not observe much improvement beyond 6 - grams .
Although the Markov chains are efficient at encoding local word interactions , the n-gram model clearly ignores the rich syntactic and semantic structures that constrain natural languages .
As the machine translation ( MT ) working groups stated on page 3 of their final report ( Lavie et al. , 2006 ) , " These approaches have resulted in small improvements in MT quality , but have not fundamentally solved the problem .
There is a dire need for developing novel approaches to language modeling . "
Wang et al. ( 2006 ) integrated n-gram , structured language model ( SLM ) ( Chelba and Jelinek , 2000 ) and probabilistic latent semantic analysis ( PLSA ) ( Hofmann , 2001 ) under the directed MRF framework ( Wang et al. , 2005 ) and studied the stochastic properties for the composite language model .
They derived a generalized inside-outside algorithm to train the composite language model from a general EM ( Dempster et al. , 1977 ) by following Jelinek 's ingenious definition of the inside and outside probabilities for SLM ( Jelinek , 2004 ) with 6th order of sentence length time complexity .
Unfortunately , there are no experimental results reported .
In this paper , we study the same composite language model .
Instead of using the 6th order generalized inside-outside algorithm proposed in ( Wang et al. , 2006 ) , we train this composite model by a convergent N-best list approximate EM algorithm that has linear time complexity and a follow - up EM algorithm to improve word prediction power .
We conduct comprehensive experiments on corpora with 44 million tokens , 230 million tokens , and 1.3 billion tokens and compare perplexity results with n-grams ( n=3,4,5 respectively ) on these three corpora , we obtain drastic perplexity reductions .
Finally , we ap- ply our language models to the task of re-ranking the N-best list from Hiero ( Chiang , 2005 ; Chiang , 2007 ) , a state - of - the - art parsing - based MT system , we achieve significantly better translation quality measured by the BLEU score and " readability " .
Composite language model
The n-gram language model is essentially a word predictor that given its entire document history it predicts next word w k+1 based on the last n-1 words with probability p( w k+1 | w k k?n+ 2 ) where w k k?n+ 2 = w k?n+ 2 , ? ? ? , w k .
The SLM ( Chelba and Jelinek , 1998 ; Chelba and Jelinek , 2000 ) uses syntactic information beyond the regular n-gram models to capture sentence level long range dependencies .
The SLM is based on statistical parsing techniques that allow syntactic analysis of sentences ; it assigns a probability p( W , T ) to every sentence W and every possible binary parse T .
The terminals of T are the words of W with POS tags , and the nodes of T are annotated with phrase headwords and non-terminal labels .
Let W be a sentence of length n words to which we have prepended the sentence beginning marker < s > and appended the sentence end marker </ s> so that w 0 =<s> and w n+1 =</s >.
Let W k = w 0 , ? ? ? , w k be the word k-prefix of the sentence - the words from the beginning of the sentence up to the current position k and W k T k the word - parse k-prefix .
A word- parse k-prefix has a set of exposed heads h ?m , ? ? ? , h ?1 , with each head being a pair ( headword , non-terminal label ) , or in the case of a root-only tree ( word , POS tag ) .
An m-th order SLM ( m- SLM ) has three operators to generate a sentence : WORD - PREDICTOR predicts the next word w k+1 based on the m left-most exposed headwords h ?1 ?m = h ?m , ? ? ? , h ?1 in the word- parse k-prefix with prob- ability p( w k +1 |h ?1 ?m ) , and then passes control to the TAGGER ; the TAGGER predicts the POS tag t k+1 to the next word w k+1 based on the next word w k+1 and the POS tags of the m left-most exposed headwords h ?1 ?m in the word- parse k-prefix with probability p( t k+1 |w k+1 , h ?m .tag , ? ? ? , h ?1 .tag ) ; the CONSTRUCTOR builds the partial parse T k from T k?1 , w k , and t k in a series of moves ending with NULL , where a parse move a is made with probability p( a|h ?1 ?m ) ; a ? A= {( unary , NTlabel ) , ( adjoinleft , NTlabel ) , ( adjoin-right , NTlabel ) , null} .
Once the CONSTRUCTOR hits NULL , it passes control to the WORD - PREDICTOR .
See detailed description in ( Chelba and Jelinek , 2000 ) .
A PLSA model ( Hofmann , 2001 ) is a generative probabilistic model of word-document cooccurrences using the bag-of-words assumption described as follows : ( i ) choose a document d with probability p( d ) ; ( ii ) SEMANTIZER : select a semantic class g with probability p( g |d ) ; and ( iii ) WORD - PREDICTOR : pick a word w with probability p( w | g ) .
Since only one pair of ( d , w ) is being observed , as a result , the joint probability model is a mixture of log-linear model with the expression p( d , w ) = p ( d ) g p( w | g ) p ( g | d ) .
Typically , the number of documents and vocabulary size are much larger than the size of latent semantic class variables .
Thus , latent semantic class variables function as bottleneck variables to constrain word occurrences in documents .
When combining n-gram , m order SLM and PLSA models together to build a composite generative language model under the directed MRF paradigm ( Wang et al. , 2005 ; Wang et al. , 2006 ) , the TAGGER and CONSTRUCTOR in SLM and SEMANTIZER in PLSA remain unchanged ; however the WORD - PREDICTORs in n-gram , m-SLM and PLSA are combined to form a stronger WORD - PREDICTOR that generates the next word , w k +1 , not only depending on the m left-most exposed headwords h ?1 ?m in the word- parse k-prefix but also its n-gram history w k k?n+ 2 and its semantic content g k +1 .
The parameter for WORD - PREDICTOR in the composite n-gram / m-SLM / PLSA language model becomes p( w k +1 | w k k?n+2 h ?1 ?m g k + 1 ) .
The resulting composite language model has an even more complex dependency structure but with more expressive power than the original SLM .
Figure 1 illustrates the structure of a composite n-gram / m-SLM / PLSA language model .
The composite n-gram / m-SLM / PLSA language model can be formulated as a directed MRF model ( Wang et al. , 2006 )
Training algorithm Under the composite n-gram / m-SLM / PLSA language model , the likelihood of a training corpus D , a collection of documents , can be written as L( D , p ) = Y d?D Y l X G l X T l Pp( W l , T l , G l |d ) !!! ( 1 ) where ( W l , T l , G l , d ) denote the joint sequence of the lth sentence W l with its parse tree structure T l and semantic annotation string G l in document d .
This sequence is produced by a unique sequence of model actions : WORD - PREDICTOR , TAGGER , CONSTRUCTOR , SEMANTIZER moves , its probability is obtained by chaining the probabilities of these moves Pp ( W l , T l , G l | d ) = Y g?G 0 @ p( g |d ) # ( g , W l , G l , d ) Y h ?1 , ? , h ?m ?H Y w, w ?1 , ? , w ?n+1 ?V p( w|w ?1 ?n+1 h ?1 ?m g ) #( w ? 1 ?n+1 wh ?1 ?m g, W l , T l , G l , d ) Y t?O p( t|wh ?1 ?m .tag ) # ( t, wh ?1 ?m .tag , W l , T l , d ) Y a?A p( a|h ?1 ?m ) #( a , h ?1 ?m , W l , T l , d ) ! where # ( g , W l , G l , d ) is the count of seman - tic content g in semantic annotation string G l of the lth sentence W l in document d , #( w ?1 ?n+1 wh ?1 ?m g , W l , T l , G l , d ) is the count of n-grams , its m most recent exposed headwords and semantic content g in parse T l and semantic annotation string G l of the lth sentence W l in document d , # ( twh ?1 ?m .tag , W l , T l , d ) is the count of tag t predicted by word w and the tags of m most recent exposed headwords in parse tree T l of the lth sentence W l in document d , and finally #( ah ?1 ?m , W l , T l , d ) is the count of constructor move a conditioning on m exposed headwords h ?1 ?m in parse tree T l of the lth sentence W l in document d .
The objective of maximum likelihood estimation is to maximize the likelihood L( D , p ) respect to model parameters .
For a given sentence , its parse tree and semantic content are hidden and the number of parse trees grows faster than exponential with sentence length , Wang et al . ( 2006 ) have derived a generalized inside-outside algorithm by applying the standard EM algorithm .
However , the complexity of this algorithm is 6th order of sentence length , thus it is computationally too expensive to be practical for a large corpus even with the use of pruning on charts ( Jelinek and Chelba , 1999 ; Jelinek , 2004 ) .
N- best list approximate EM Similar to SLM ( Chelba and Jelinek , 2000 ) , we adopt an N - best list approximate EM re-estimation with modular modifications to seamlessly incorporate the effect of n-gram and PLSA components .
Instead of maximizing the likelihood L( D , p ) , we maximize the N - best list likelihood , max T ?
N L( D , p , T ? N ) = Y d?D Y l max T ? l N ?T ? N X G l 0 @ X T l ?T ?l N , ||T ? l N ||=N Pp ( W l , T l , G l |d ) 1 A 1 A 1 A where T ?
l N is a set of N parse trees for sentence W l in document d and || ? || denotes the cardinality and T ?
N is a collection of T ? l N for sentences over entire corpus D. The N-best list approximate EM involves two steps : 1 . N- best list search :
For each sentence W in document d , find N - best parse trees , T l N = arg max T ?l N n X G l X T l ?T ?l N Pp( W l , T l , G l | d ) , ||T ?l N || = N L( D , p , TN ) = Y d?D ( Y l ( X G l ( X T l ?T l N ?T N Pp( W l , T l , G l | d ) ) ) )
That is , ( a ) E-step : Compute the auxiliary function of the N - best-list likelihood Q( p ? , p , TN ) = X d?D X l X G l X T l ?T l N ?T N Pp( T l , G l | W l , d ) log P p ?
( W l , T l , G l |d ) ( b) M-step : Maximize Q( p ? , p , T N ) with respect to p ? to get new update for p.
Iterate steps ( 1 ) and ( 2 ) until the convergence of the N - best-list likelihood .
Due to space constraints , we omit the proof of the convergence of the N-best list approximate EM algorithm which uses Zangwill 's global convergence theorem ( Zangwill , 1969 ) .
N - best list search strategy :
To extract the Nbest parse trees , we adopt a synchronous , multistack search strategy that is similar to the one in ( Chelba and Jelinek , 2000 ) , which involves a set of stacks storing partial parses of the most likely ones for a given prefix W k and the less probable parses are purged .
Each stack contains hypotheses ( partial parses ) that have been constructed by the same number of WORD - PREDICTOR and the same number of CONSTRUCTOR operations .
The hypotheses in each stack are ranked according to the log ( G k P p ( W k , T k , G k |d ) ) score with the highest on top , where P p ( W k , T k , G k |d ) is the joint prob- ability of prefix W k = w 0 , ? ? ? , w k with its parse structure T k and semantic annotation string G k = g 1 , ? ? ? , g k in a document d .
A stack vector consists of the ordered set of stacks containing partial parses with the same number of WORD - PREDICTOR operations but different number of CONSTRUCTOR operations .
In WORD - PREDICTOR and TAGGER operations , some hypotheses are discarded due to the maximum number of hypotheses the stack can contain at any given time .
In CONSTRUCTOR operation , the resulting hypotheses are discarded due to either finite stack size or the log-probability threshold : the maximum tolerable difference between the log-probability score of the top-most hypothesis and the bottom-most hypothesis at any given state of the stack .
EM update : Once we have the N - best parse trees for each sentence in document d and N - best topics for document d , we derive the EM algorithm to estimate model parameters .
In E-step , we compute the expected count of each model parameter over sentence W l in document d in the training corpus D. For the WORD - PREDICTOR and the SEMANTIZER , the number of possible semantic annotation sequences is exponential , we use forward - backward recursive formulas that are similar to those in hidden Markov models to compute the expected counts .
We define the forward vector ? l ( g|d ) to be ? l k+1 ( g|d ) = X G l k Pp ( W l k , T l k , w k k?n+ 2 w k+1 h ?1 ?m g , G l k | d ) that can be recursively computed in a forward manner , where W l k is the word k-prefix for sentence W l , T l k is the parse for k-prefix .
We define backward vector ? l ( g|d ) to be ? l k+1 ( g|d ) = X G l k +1 , ? Pp ( W l k +1 , ? , T l k +1 , ? , G l k +1 , ?
|w k k?n+2 w k+1 h ?1 ?m g , d ) that can be computed in a backward manner , here W l k +1 , ?
is the subsequence after k+1th word in sentence W l , T l k +1 , ?
is the incremental parse structure after the parse structure T l k +1 of word k+1 prefix W l k+1 that generates parse tree T l , G l k +1 , ?
is the semantic subsequence in G l relevant to W l k +1 , ? .
Then , the expected count of w ?1 ?n+1 wh ?1 ?m g for the WORD - PREDICTOR on sentence W l in document d is X G l Pp( T l , G l | W l , d ) #( w ?1 ?n+1 wh ?1 ?m g , W l , T l , G l , d ) = X l X k ? l k+1 ( g|d ) ? l k+1 ( g|d ) p ( g |d ) ?( w k k?n+2 w k+1 h ?1 ?m g k +1 = w ?1 ?n+1 wh ?1 ?m g ) / Pp ( W l | d ) where ?(? ) is an indicator function and the expected count of g for the SEMANTIZER on sentence W l in document d is X G l Pp( T l , G l | W l , d ) #( g , W l , G l , d ) = j?1 X k=0 ? l k+1 ( g|d ) ? l k+1 ( g|d ) p ( g | d ) / Pp( W l |d )
For the TAGGER and the CONSTRUCTOR , the expected count of each event of twh ?1 ?m .tag and ah ?1 ?m over parse T l of sentence W l in 204 document d is the real count appeared in parse tree T l of sentence W l in document d times the conditional distribution P p ( T l | W l , d ) = P p ( T l , W l | d ) / T l ?T l P p ( T l , W l | d ) respectively .
In M-step , the recursive linear interpolation scheme ( Jelinek and Mercer , 1981 ) is used to obtain a smooth probability estimate for each model component , WORD - PREDICTOR , TAGGER , and CONSTRUCTOR .
The TAGGER and CONSTRUCTOR are conditional probabilistic models of the type p( u|z 1 , ? ? ? , z n ) where u , z 1 , ? ? ? , z n belong to a mixed set of words , POS tags , NTtags , CONSTRUCTOR actions ( u only ) , and z 1 , ? ? ? , z n form a linear Markov chain .
The recursive mixing scheme is the standard one among relative frequency estimates of different orders k = 0 , ? ? ? , n as explained in ( Chelba and Jelinek , 2000 ) .
The WORD - PREDICTOR is , however , a conditional probabilistic model p( w|w ?1 ?n+1 h ?1 ?m g ) where there are three kinds of context w ?1 ?n+ 1 , h ?1 ?m and g , each forms a linear Markov chain .
The model has a combinatorial number of relative frequency estimates of different orders among three linear Markov chains .
We generalize Jelinek and Mercer 's original recursive mixing scheme ( Jelinek and Mercer , 1981 ) and form a lattice to handle the situation where the context is a mixture of Markov chains .
Follow- up EM
As explained in ( Chelba and Jelinek , 2000 ) , for the SLM component , a large fraction of the partial parse trees that can be used for assigning probability to the next word do not survive in the synchronous , multistack search strategy , thus they are not used in the N-best approximate EM algorithm for the estimation of WORD - PREDICTOR to improve its predictive power .
To remedy this weakness , we estimate WORD - PREDICTOR using the algorithm below .
The language model probability assignment for the word at position k+1 in the input sentence of document d can be computed as Pp( w k+1 | W k , d ) = X h ?1 ?m ?T k ; T k ?Z k , g k+1 ?G d p( w k +1 |w k k?n+2 h ?1 ?m g k +1 ) Pp ( T k | W k , d ) p( g k +1 | d ) ( 2 ) where P p ( T k | W k , d ) = P G k Pp( W k , T k , G k |d ) P T k ?Z k P G k Pp( W k , T k , G k |d ) and Z k is the set of all parses present in the stacks at the current stage k during the synchronous multi-stack pruning strategy and it is a function of the word k-prefix W k .
The likelihood of a training corpus D under this language model probability assignment that uses partial parse trees generated during the process of the synchronous , multi-stack search strategy can be written as L( D , p ) = Y d?D Y l " X k Pp ( w ( l ) k+1 | W l k , d ) " ( 3 ) We employ a second stage of parameter reestimation for p( w k+1 | w k k?n+2 h ?1 ?m g k +1 ) and p( g k + 1 | d ) by using EM again to maximize Equation ( 3 ) to improve the predictive power of WORD - PREDICTOR .
Distributed architecture
When using very large corpora to train our composite language model , both the data and the parameters ca n't be stored in a single machine , so we have to resort to distributed computing .
The topic of large scale distributed language models is relatively new , and existing works are restricted to n-grams only ( Brants et al. , 2007 ; Emami et al. , 2007 ; Zhang et al. , 2006 ) .
Even though all use distributed architectures that follow the client-server paradigm , the real implementations are in fact different .
Zhang et al. ( 2006 ) and Emami et al . ( 2007 ) store training corpora in suffix arrays such that one sub-corpus per server serves raw counts and test sentences are loaded in a client .
This implies that when computing the language model probability of a sentence in a client , all servers need to be contacted for each ngram request .
The approach by Brants et al . ( 2007 ) follows a standard MapReduce paradigm ( Dean and Ghemawat , 2004 ) : the corpus is first divided and loaded into a number of clients , and n-gram counts are collected at each client , then the n-gram counts mapped and stored in a number of servers , resulting in exactly one server being contacted per n-gram when computing the language model probability of a sentence .
We adopt a similar approach to Brants et al .
and make it suitable to perform iterations of N - best list approximate EM algorithm , see Figure 2 .
The corpus is divided and loaded into a number of clients .
We use a public available parser to parse the sentences in each client to get the initial counts for w ?1 ?n+1 wh ?1 ?m g etc. , finish the Map part , and then the counts for a particular w ?1 ?n+1 wh ?1 ?m g at different clients are summed up and stored in one of the servers by hashing through the word w ?1 ( or h ?1 ) and its topic g , finish the Reduce part .
This is the initialization of the N - best list approximate EM step .
Each client then calls the servers for parameters to perform synchronous multi-stack search for each sentence to get the N - best list parse trees .
Again , the expected count for a particular parameter of w ?1 ?n+1 wh ?1 ?m g at the clients are computed , thus we finish a Map part , then summed up and stored in one of the servers by hashing through the word w ?1 ( or h ?1 ) and its topic g , thus we finish the Reduce part .
We repeat this procedure until convergence .
Similarly , we use a distributed architecture as in Figure 2 to perform the follow - up EM algorithm to re-estimate WORD - PREDICTOR .
Experimental results
We have trained our language models using three different training sets : one has 44 million tokens , another has 230 million tokens , and the other has 1.3 billion tokens .
An independent test set which has 354 k tokens is chosen .
The independent check data set used to determine the linear interpolation coefficients has 1.7 million tokens for the 44 million tokens training corpus , 13.7 million tokens for both 230 million and 1.3 billion tokens training corpora .
All these data sets are taken from the LDC English Gigaword corpus with non-verbalized punctuation and we remove all punctuation .
Table 1 gives the detailed information on how these data sets are chosen from the LDC English Gigaword corpus .
The vocabulary sizes in all three cases are : ? word ( also WORD - PREDICTOR operation ) ( Chelba and Jelinek , 2000 ) , after the parses undergo headword percolation and binarization , each model component of WORD - PREDICTOR , TAGGER , and CONSTRUCTOR is initialized from a set of parsed sentences .
We use the " openNLP " software ( Northedge , 2005 ) to parse a large amount of sentences in the LDC English Gigaword corpus to generate an automatic treebank , which has a slightly different word-tokenization than that of the manual treebank such as the Upenn Treebank used in ( Chelba and Jelinek , 2000 ) .
For the 44 and 230 million tokens corpora , all sentences are automatically parsed and used to initialize model parameters , while for 1.3 billion tokens corpus , we parse the sentences from a portion of the corpus that contain 230 million tokens , then use them to initialize model parameters .
The parser at " openNLP " is trained by Upenn treebank with 1 million tokens and there is a mismatch between Upenn treebank and LDC English Gigaword corpus .
Nevertheless , experimental results show that this approach is effective to provide initial values of model parameters .
As we have explained , the proposed EM algorithms can be naturally cast into a MapReduce framework , see more discussion in ( Lin and Dyer , 2010 ) .
If we have access to a large cluster of machines with Hadoop installed that are powerful enough to process a billion tokens level corpus , we just need to specify a map function and a reduce function etc. , Hadoop will automatically parallelize and execute programs written in this functional style .
Unfortunately , we do n't have this kind of resources available .
Instead , we have access to a supercomputer at a supercomputer center with MPI installed that has more than 1000 core processors usable .
Thus we implement our algorithms using C ++ under MPI on the supercomputer , where we have to write C ++ codes for Map part and Reduce part , and the MPI is used to take care of massage passing , scheduling , synchronization , etc. between clients and servers .
This involves a fair amount of programming work , even though our implementation under MPI is not as reliable as under Hadoop but it is more efficient .
We use up to 1000 core processors to train the composite language models for 1.3 billion tokens corpus where 900 core processors are used to store the parameters alone .
We decide to use linearly smoothed trigram as the baseline model for 44 million token corpus , linearly smoothed 4 - gram as the baseline model for 230 million token corpus , and linearly smoothed 5 - gram as the baseline model for 1.3 billion token corpus .
Model size is a big issue , we have to keep only a small set of topics due to the consideration in both computational time and resource demand .
Table 2 shows the perplexity results and computation time of composite n-gram / PLSA language models that are trained on three corpora when the pre-defined number of total topics is 200 but different numbers of most likely topics are kept for each document in PLSA , the rest are pruned .
For composite 5 - gram / PLSA model trained on 1.3 billion tokens corpus , 400 cores have to be used to keep top 5 most likely topics .
For composite tri-gram / PLSA model trained on 44 M tokens corpus , the computation time increases drastically with less than 5 % percent perplexity improvement .
So in the following experiments , we keep top 5 topics for each document from total 200 topics and all other 195 topics are pruned .
All composite language models are first trained by performing N-best list approximate EM algorithm until convergence , then EM algorithm for a second stage of parameter re-estimation for WORD - PREDICTOR and SEMANTIZER until convergence .
We fix the size of topics in PLSA to be 200 and then prune to 5 in the experiments , where the unpruned 5 topics in general account for 70 % probability in p ( g | d ) .
Table 3 shows comprehensive perplexity results for a variety of different models such as composite n-gram / m- SLM , n-gram / PLSA , m-SLM / PLSA , their linear combinations , etc. , where we use online EM with fixed learning rate to reestimate the parameters of the SEMANTIZER of test document .
The m-SLM performs competitively with its counterpart n-gram ( n=m + 1 ) on large scale corpus .
In Table 3 , for composite n-gram / m-SLM model ( n = 3 , m = 2 and n = 4 , m = 3 ) trained on 44 million tokens and 230 million tokens , we cut off its fractional expected counts that are less than a threshold 0.005 , this significantly reduces the number of predictor 's types by 85 % .
When we train the composite language on 1.3 billion tokens corpus , we have to both aggressively prune the parameters of WORD - PREDICTOR and shrink the order of n-gram and m-SLM in order to store them in a supercomputer having 1000 cores .
In particular , for composite 5 - gram / 4 - SLM model , its size is too big to store , thus we use its approximation , a linear combination of 5 - gram / 2- SLM and 2 - gram / 4 - SLM , and for 5 - gram / 2 - SLM or 2 - gram / 4 - SLM , again we cut off its fractional expected counts that are less than a threshold 0.005 , this significantly reduces the number of predictor 's types by 85 % .
For composite 4- SLM / PLSA model , we cut off its fractional expected counts that are less than a threshold 0.002 , again this significantly reduces the number of predictor 's types by 85 % .
For composite 4- SLM / PLSA model or its linear combination with models , we ignore all the tags and use only the words in the 4 head words .
In this We have applied our composite 5 - gram/2-SLM +2-gram/4-SLM +5-gram / PLSA language model that is trained by 1.3 billion word corpus for the task of re-ranking the N - best list in statistical machine translation .
We used the same 1000 - best list that is used by Zhang et al . ( 2006 ) .
This list was generated on 919 sentences from the MT03 Chinese-English evaluation set by Hiero ( Chiang , 2005 ; Chiang , 2007 ) , a state - of - the - art parsing - based translation model .
Its decoder uses a trigram language model trained with modified Kneser - Ney smoothing ( Kneser and Ney , 1995 ) on a 200 million tokens corpus .
Each translation has 11 features and language model is one of them .
We substitute our language model and use MERT ( Och , 2003 ) to optimize the BLEU score ( Papineni et al. , 2002 ) .
We partition the data into ten pieces , 9 pieces are used as training data to optimize the BLEU score ( Papineni et al. , 2002 ) by MERT ( Och , 2003 ) , a remaining single piece is used to re-rank the 1000 - best list and obtain the BLEU score .
The cross-validation process is then repeated 10 times ( the folds ) , with each of the 10 pieces used exactly once as the validation data .
The 10 results from the folds then can be averaged ( or otherwise combined ) to produce a single estimation for BLEU score .
Table 4 shows the BLEU scores through 10 - fold cross-validation .
The composite 5 - gram/2-SLM +2gram/4-SLM +5-gram / PLSA language model gives 1.57 % BLEU score improvement over the baseline and 0.79 % BLEU score improvement over the 5 - gram .
This is because there is not much diversity on the 1000 - best list , and essentially only 20 ? 30 distinct sentences are there in the 1000 - best list .
Chiang ( 2007 ) studied the performance of machine translation on Hiero , the BLEU score is 33.31 % when n-gram is used to re-rank the N - best list , however , the BLEU score becomes significantly higher 37.09 % when the n-gram is embedded directly into Hiero 's one pass decoder , this is because there is not much diversity in the N - best list .
It is expected that putting the our composite language into a one pass decoder of both phrase - based ( Koehn et al. , 2003 ) and parsing - based ( Chiang , 2005 ; Besides reporting the BLEU scores , we look at the " readability " of translations similar to the study conducted by Charniak et al . ( 2003 ) .
The translations are sorted into four groups : good / bad syntax crossed with good / bad meaning by human judges , see Table 5 .
We find that many more sentences are perfect , many more are grammatically correct , and many more are semantically correct .
The syntactic language model ( Charniak , 2001 ; Charniak , 2003 ) only improves translations to have good grammar , but does not improve translations to preserve meaning .
The composite 5 - gram/2-SLM+2-gram/4-SLM +5gram / PLSA language model improves both significantly .
Bear in mind that Charniak et al . ( 2003 ) integrated Charniak 's language model with the syntaxbased translation model Yamada and Knight proposed ( 2001 ) to rescore a tree-to-string translation forest , whereas we use only our language model for N - best list re-ranking .
Also , in the same study in ( Charniak , 2003 ) , they found that the outputs produced using the n-grams received higher scores from BLEU ; ours did not .
The difference between human judgments and BLEU scores indicate that closer agreement may be possible by incorporating syntactic structure and semantic information into the BLEU score evaluation .
For example , semantically similar words like " insure " and " ensure " in the example of BLEU paper ( Papineni et al. , 2002 ) should be substituted in the formula , and there is a weight to measure the goodness of syntactic structure .
This modification will lead to a better metric and such information can be provided by our composite language models .
Conclusion
As far as we know , this is the first work of building a complex large scale distributed language model with a principled approach that is more powerful than ngrams when both trained on a very large corpus with up to a billion tokens .
We believe our results still hold on web scale corpora that have trillion tokens , since the composite language model effectively encodes long range dependencies of natural language that n-gram is not viable to consider .
Of course , this implies that we have to take a huge amount of resources to perform the computation , nevertheless this becomes feasible , affordable , and cheap in the era of cloud computing .
209 o and denote T N as the collection of N - best list parse trees for sentences over entire corpus D under model parameter p. 2 . EM update : Perform one iteration ( or several iterations ) of EM algorithm to estimate model parameters that maximizes N - best - list likelihood of the training corpus D ,
