title
Hierarchical Modeling of Global Context for Document -Level Neural Machine Translation
abstract
Document - level machine translation ( MT ) remains challenging due to the difficulty in efficiently using document context for translation .
In this paper , we propose a hierarchical model to learn the global context for documentlevel neural machine translation ( NMT ) .
This is done through a sentence encoder to capture intra-sentence dependencies and a document encoder to model document - level intersentence consistency and coherence .
With this hierarchical architecture , we feedback the extracted global document context to each word in a top-down fashion to distinguish different translations of a word according to its specific surrounding context .
In addition , since largescale in- domain document - level parallel corpora are usually unavailable , we use a twostep training strategy to take advantage of a large-scale corpus with out - of- domain parallel sentence pairs and a small -scale corpus with in- domain parallel document pairs to achieve the domain adaptability .
Experimental results on several benchmark corpora show that our proposed model can significantly improve document - level translation performance over several strong NMT baselines .
Introduction
Due to its flexibility and much less demand of manual efforts for feature engineering , neural machine translation ( NMT ) has achieved remarkable progress and become the de-facto standard choice in machine translation .
During the last few years , a variety of NMT models have been proposed to reduce the quality gap between human translation and machine translation Bahdanau et al. , 2015 ; Gehring et al. , 2017 ; Vaswani et al. , 2017 ) .
Among them , the Transformer model ( Vaswani et al. , 2017 ) has achieved the state - of - the - art performance in sentence - level * Corresponding author .
translation and results on news benchmark test sets have shown its " translation quality at human parity when compared to professional human translators " ( Hassan et al. , 2018 ) .
However , when turning to document- level translation , even the Transformer model yields a low performance as it translates each sentence in the document independently and suffers from the problem of ignoring document context .
To address above challenge , various extractionbased methods ( Maruf and Haffari , 2018 ; Wang et al. , 2017 ; Miculicich et al. , 2018 ) have been proposed to extract previous context ( pre-context ) to guide the translation of the current sentence s i , as shown in Figure 1 .
However , when there exists a huge gap between the pre-context and the context after the current sentence s i , the guidance from pre-context is not sufficient for the NMT model to fully disambiguate the sentence s i .
On the one hand , the translation of the current sentence s i may be inaccurate due to the one-sidedness of partial context .
On the other hand , translating the succeeding sentences in the document may much suffer from the semantic bias due to the transmissibility of the improper pre-context .
To alleviate the aforementioned issues , we propose to improve document - level translation with the aid of global context , which is hierarchically extracted from the entire document with a sen -
Sentence Encoder Sentence Encoder MultiHead-Ctx
Document Encoder MultiHead - Ctx Self - Attn Self - Attn tence encoder modeling intra-sentence dependencies and a document encoder modeling documentlevel inter-sentence context .
To avoid the issue of translation bias propagation caused by improper pre-context , we propose to extract global context from all sentences of a document once for all .
Additionally , we propose a novel method to feed back the extracted global document context to each word in a top-to - down manner to clarify the translation of words in specific surrounding contexts .
In this way , the proposed model can better translate each sentence under the guidance of the global context , thus effectively avoiding the transmissibility of improper pre-context .
Furthermore , motivated by and Miculicich et al . ( 2018 ) who exploit a large amount of sentencelevel parallel pairs to improve the performance of document- level translation , we employ a two -step training strategy in taking advantage of a largescale corpus of out - of- domain sentence - level parallel pairs to pre-train the model and a small - scale corpus of in-domain document - level parallel pairs to fine - tune the pretrained model .
We conduct experiments on both the traditional RNNSearch model and the state - of- the - art Transformer model .
Experimental results on Chinese-English and German-English translation show that our proposed model can achieve the state - of - theart performance due to its ability in well capturing global document context .
It is also inferential to notice that our proposed model can explore a large dataset of out - of- domain sentence - level parallel pairs and a small dataset of in-domain documentlevel parallel pairs to achieve domain adaptability .
NMT with Hierarchical Modeling of Global Document Context
In this work , our ultimate goal is to incorporate the global document context into NMT to improve the performance of document- level translation .
This is first achieved with the hierarchical modeling of global document context ( HM - GDC ) based on sentence - level hidden representation and document-level consistency and coherence modeling .
Then , we integrate the proposed HM - GDC model into NMT models to help improve the performance of document- level translation .
Hierarchically Modeling Global Context
To avoid the one-sidedness of partial context and the transmissibility of the improper pre-context in previous studies , we take all sentences of the document into account and extract the global context once for all .
Inspired by Sordoni et al. ( 2015 ) , we build our HM - GDC model in a hierarchical way which contains two levels of encoder structure , i.e. , the bottom sentence encoder layer to capture intra-sentence dependencies and the upper document encoder layer to capture document - level context .
In this way , the global document context is captured for NMT .
In order to make the translation of each word in specific surrounding context more robust , we propose to equip each word with global document context .
This is done by backpropagating the extracted global context to each word in a top-down fashion , as shown in Figure 2 .
The following is the detailed description of the proposed HM - GDC model .
Sentence encoder .
Given an input document with N sentences ( S 1 , S 2 , ... , S N ) , the sentence encoder maps each word x i , k in the sentence S i into the corresponding hidden state h i , k , obtaining : H i = SentEnc ( S i ) ( 1 ) where S i = ( x i,1 , x i,2 , . . . , x i , n ) is the i th sentence in the document , SentEnc is the sentence encoder function ( corresponding to Bi-RNNs and multi-head self-attention ( Vaswani et al. , 2017 ) for the RNNSearch model and the Transformer model respectively ) , and H i = ( h i,1 , h i,2 , . . . , h i , n ) ?
R D?n is the output hidden state .
Document encoder .
Following the Transformer model ( Vaswani et al. , 2017 ) , we employ the multi-head self-attention mechanism to determine the relative importance of different H i .
The model architecture of the document encoder here is the same as the sentence - level encoding stated before .
And the document context is formulated as : h S i = MultiHead - Self ( H i , H i , H i ) ( 2 ) hS i = h?h S i h ( 3 ) H S = DocEnc ( hS ) ( 4 ) where MultiHead - Self ( Q , K , V ) is the multi-head self-attention mechanism corresponding to Self-Attn in Figure 2 , Backpropagation of global context .
To equip each word with global document context , we propose to assign the context information to each word in the sentence using another multi-head attention ( Vaswani et al. , 2017 ) , which we refer to as the multi-head context attention ( MultiHead - Ctx in Figure 2 ) .
And the context information assigned to the j th word in the sentence S i is detailed as : h S i ?
R D?n , hS i ?
R D?1 , hS = ( hS 1 , hS 2 , . . . , hS N ) ? R D?N , ? i , j = MultiHead - Ctx( h i , j , h i , j , H S i ) ( 5 ) d ctx i , j = ? i , j H S i ( 6 ) where ?
i , j is the attention weight assigned to the word and d ctx i , j is the corresponding context information distributed to the word .
Integrating the HM - GDC model into NMT Different from previous works , we equip the representation of each word with global document context .
The word representations are sequential in format , which makes it easy to integrate our proposed HM - GDC model into sequential models like RNNSearch and Transformer .
In this section , we mainly introduce the process of integrating HM - GDC into the state - of- the - art Transformer model .
Integration in the Encoding Phase Consider that the global document context is first extracted during the encoding phase and then distributed to each word in the document , as stated in Section 2.1 .
On this basis , we propose to employ the residual connection function to incorporate the extracted global context information into the word representation .
And the integrated representation of the j th word in the i th sentence is detailed as : h ctx i , j = h i , j + ResidualDrop ( d ctx i , j , P ) ( 7 ) where ResidualDrop is the residual connection function , P = 0.1 is the rate of residual dropout , h i , j is the corresponding hidden state of the word during the sentence encoding phase , d ctx i , j is the global document context assigned to the word , and h ctx i , j is the integrated representation of the word .
Integration in the Decoding Phase With the help of the multi-head attention sub-layer in the decoder , the Transformer model is capable of well employing the information obtained from the encoder .
Inspired by this , we introduce an additional sub-layer into the decoder that performs multi-head attention over the output of the document encoder , which we refer to as DocEnc- Decoder attention ( shown in Figure 3 ) .
Different from ( Vaswani et al. , 2017 ) , the keys and values of our DocEnc- Decoder attention come from the output of the document encoder .
In this way , the global document context is well employed to supervise the decoding process .
And Specially , the additional DocEnc- Decoder attention is formulated as : C = [ h ctx 1 ; h ctx 2 ; ... ; h ctx N ] ( 8 ) G ( n ) = MultiHead - Attn ( T ( n ) , C ( n ) , C ( n ) ) ( 9 ) where h ctx i denotes the integrated representation of the i th sentence , ) is the output of the multi-head self-attention sub-layer in the decoder .
C ( 0 ) = C is the concatenated global document context , T ( n
On this basis , we combine the outputs of both the Encoder-Decoder attention sub-layer and the DocEnc- Decoder attention sub-layer into one single output H ( n ) : H ( n ) = E ( n ) + G ( n ) ( 10 ) where E ( n ) is the output of the Encoder - Decoder attention sub-layer , and G ( n ) is the output of the DocEnc- Decoder attention sub-layer .
Model Training
In document - level translation , the standard training objective is to maximize the log-likelihood of the document- level parallel corpus .
However , due to the size limitation of document- level parallel corpora , previous studies Miculicich et al. , 2018 ; Shen et al. , 2016 ) use two -step training strategies to take advantage of large-scale sentence - level parallel pairs .
Following their studies , we also take a two -step training strategy to train our model .
Specially , we borrow a large-scale corpus with out - of- domain sentencelevel parallel pairs D s to pre-train our model first , and then use a small -scale corpus with in - domain document - level parallel pairs D d to fine -tune it .
In this work , we follow Voita et al . ( 2018 ) to make the sentence and document encoders share the same model parameters .
For the RNNSearch model , we share the parameters in the hidden layers of Bi-RNNs in the sentence and document encoders .
For the Transformer model , we share the parameters of the multi-head self-attention layers in the sentence and document encoders .
During training , we first optimize the sentencelevel parameters ?
s ( colored in wheat in Figure 3 ) with the large-scale sentence - level parallel pairs D s : ?s = arg max ?s <x , y >? Ds logP ( y|x ; ? s ) ( 11 )
Then , we optimize the document- level parameters ?
d ( colored in pale blue in Figure 3 ) with the document- level parallel corpus D d and fine - tune the pre-trained sentence - level parameters ?s as follows : ?d = arg max ? d < x , y >?D d logP ( y|x ; ? d , ?s ) ( 12 ) 3 Experimentation
To examine the effect of our proposed HM - GDC model , we conduct experiments on both Chinese -English and German-English translation .
Experimental Settings
Datasets For Chinese -English translation , we carry out experiments with sentence - and document - level corpora on two different domains : news and talks .
For the sentence - level corpus , we use 2.8 M sentence pairs from news corpora LDC2003E14 , LDC2004T07 , LDC2005T06 , LDC2005T10 and LDC2004T08 ( Hongkong Hansards / Laws / News ) .
We use the Ted talks corpus from the IWSLT 2017 ( Cettolo et al. , 2012 ) evaluation campaigns 1 as the document- level parallel corpus , including 1,906 documents with 226 K sentence pairs .
We use dev2010 which contains 8 documents with 879 sentence pairs for development and tst2012 - tst2015 which contain 62 documents with 5566 sentence pairs for testing .
For German-English translation , we use the document- level parallel
Ted talks corpus from the IWSLT 2014 ( Cettolo et al. , 2012 )
Model Settings
We integrate our proposed HM - GDC into the original Transformer model implemented by Open-NMT ( Klein et al. , 2017 ) .
Following the Transformer model ( Vaswani et al. , 2017 ) , the hidden size and filter size are set to 512 and 2048 respectively .
The numbers of layers in encoder and decoder are all set to 6 .
The multi-head attention mechanism of each layer contains 8 individual attention heads .
We set both the source and target vocabulary size as 50 K and each batch contains 4096 tokens .
The beam size and dropout ( Srivastava et al. , 2014 ) rate are set to 5 and 0.1 respectively .
Other settings with the Adam ( Kingma and Ba , 2015 ) optimization and regularization methods are the same as the default Transformer model .
To comprehensively evaluate the performance of our proposed HM - GDC model , we integrate the HM - GDC into the standard RNNSearch model to serve as a supplementary experiment .
For the RNNSearch network , we borrow the implementation from OpenNMT ( Klein et al. , 2017 ) .
The encoder and decoder layers are all set to 2 , the size of the hidden layer is set to 500 , and the batch size is set to 64 .
Same as the Transformer model , we use the most frequent 50 K words for both source and target vocabularies .
We borrow other settings from ( Bahdanau et al. , 2015 ) .
The evaluation metric for both tasks is case-insensitive BLEU ( multi-bleu ) ( Papineni et al. , 2002 ) .
Experimental Results
In this paper , we compare our model with four strong baselines as shown in Table 1 . Among them , the RNNSearch ( Bahdanau et al. , 2015 )
As shown in Table 1 , we divide the results into two main groups , i.e. , in the framework of RNNSearch ( the first three rows ) and Transformer ( the remaining rows ) .
The results in the first group reveal that our proposed model can significantly improve the RNNSearch model and can further improve the model of Wang et al . ( 2017 ) in document- level translation by 0.76 BLEU points .
In addition , the results in the second group is further split into two parts depending on whether the pre-training strategy is used .
For the first part , we train our model and the two baselines with only the small - scale document - level parallel corpus without pre-training ( the first three rows in the second group ) .
From the results , the model of achieves much worse results ( ? 4.21 BLEU points ) when compared with the standard Transformer model , which is consistent with what they state in their paper .
By contrast , our proposed model can achieve 0.62 BLEU points over the Transformer model , which indicates the robustness of our model .
For the second part , to further compare with , we use the pretraining strategy to take advantage of large-scale sentence - level parallel corpus D s for these models ( the last three rows ) .
From the results , our proposed HM - GDC can significantly improve the performance of the Transformer model by 2.12 BLEU points and can further improve the performance of by 0.54 BLEU points .
From the overall results , it 's not difficult to find that using a large-scale corpus with out-ofdomain parallel pairs D s to pre-train the Transformer model results in worse performance due to domain inadaptability ( the first and the fourth row in the second group ) .
By contrast , our proposed model can effectively eliminate this domain inadaptability ( the third and sixth row in the second group ) .
In general , our proposed HM - GDC model is robust when integrated into frameworks like RNNSearch and Transformer and it can help improve the performance of document- level translation .
Analysis and Discussion
To further demonstrate the effectiveness of our proposed HM - GDC model , we illustrate several experimental results in this section and give our analysis on them .
The Effect of HM - GDC Integration
As the state- of- the- art Transformer model is welldesigned in the model structure , an analysis of the integration in Transformer is thus necessary .
Therefore , we perform experiments on Chinese -English document - level translation for analyzing .
decoder and both sides of the Transformer model with respect to the number of layers in the multihead self-attention in the document encoder ( see Section 2.1 ) .
From the results , the overall performance of integrating HM - GDC into both the encoder and decoder is better than integrating it into the encoder or decoder only .
However , the layer number of the multi-head self-attention does not make much difference in our experiments .
It shows that when the HM - GDC is integrated into both the encoder and decoder and the layer number equals to 5 , the Transformer model can achieve a relatively better performance .
Different Language Pairs
In this paper , we aim to propose a robust document context extraction model .
To achieve this goal , we perform experiments on different language pairs to further illustrate the effectiveness of our proposed HM - GDC model .
Table 3 shows the performance of our model on German-English document - level translation and the baseline here refers to the Transformer model .
For clarity , we only use the German- English document - level parallel corpus to train these two models without pretraining .
From the results , our proposed HM - GDC model can help improve the Transformer model on German-English document - level translation by 0.90 BLEU points .
sults further validate the robustness of our model in different language pairs .
The Effect of Pre-training Due to the size limitation of document- level parallel corpora , previous studies Miculicich et al. , 2018 ; Shen et al. , 2016 )
From the results , the performance of our model is significantly improved by 0.93 BLEU points ( the first two rows in Table 4 ) when the largescale sentence - level parallel corpus is used for the pre-training process .
In particular , when we use the mixed data of both sentence - and document - level parallel corpora 2 to first pre-train our model , the performance of our model is significantly improved by 5.26 BLEU points ( the last row in Table 4 ) .
The overall results prove that our proposed model is robust and promising .
It can significantly improve the performance of document- level translation when a two -step training strategy is used .
Pronoun & Noun Translation
To intuitively illustrate how the translation performance is improved by our proposed HM - GDC model , we conduct a further analysis on pronoun and noun translation .
For the pronoun translation , we evaluate the coreference and anaphora using the referencebased metric : the accuracy of pronoun translation ( Miculicich Werlen and Popescu-Belis , 2017 ) in Chinese-English and German-English translation as shown in Table 5 .
From the results , our proposed HM - GDC model can well improve the performance of pronoun translation in both corpora due to the well captured global document context assigned to each word .
Correspondingly , we display a translation example in Table 6 to further illustrate this .
From the example , given the surrounding context , our proposed HM - GDC model can well infer the latent pronoun it and thus improve the translation performance of the Transformer model .
For the analysis of noun translation , we display another example in Table 7 .
From the example , Source dan xifang zhengfu ye tongyang dui tamen ziji zheyang zuo ta .
Reference
But western governments are doing it to themselves as well .
Baseline
But the western governments do the same for themselves .
Ours
But the western governments do it the same for themselves .
els , there are also some works ( Bawden et al. , 2018 ; Voita et al. , 2018 ) that pay much attention to discourse phenomena ( Mitkov , 1999 ) related to document- level translation .
Although these approaches have achieved some progress in document- level machine translation , they still suffer from incomplete document context .
Further more , most of previous works are based on the RNNSearch model , and only few exceptions Miculicich et al. , 2018 ) are on top of the state - of- the - art Transformer model .
Conclusion
We have presented a hierarchical model to capture the global document context for documentlevel NMT .
The proposed model can be integrated into both the RNNSearch and the stateof - the- art Transformer frameworks .
Experiments on two benchmark corpora show that our proposed model can significantly improve documentlevel translation performance over several strong document - level NMT baselines .
Additionally , we observe that pronoun and noun translations are significantly improved by our proposed HM - GDC model .
In our future work , we plan to enrich our HM - GDC model to solve discourse phenomena such as ( zero ) anaphora .
? Figure 1 : 1 Figure 1 : An illustration of document-level translation under the guidance of context .
