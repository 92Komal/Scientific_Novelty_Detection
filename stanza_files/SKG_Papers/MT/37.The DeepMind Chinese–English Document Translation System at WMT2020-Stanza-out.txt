title
The DeepMind Chinese-English Document Translation System at WMT2020
abstract
This paper describes the DeepMind submission to the Chinese ?
English constrained data track of the WMT2020 Shared Task on News Translation .
The submission employs a noisy channel factorization as the backbone of a document translation system .
This approach allows the flexible combination of a number of independent component models which are further augmented with back - translation , distillation , fine-tuning with in- domain data , Monte - Carlo Tree Search decoding , and improved uncertainty estimation .
In order to address persistent issues with the premature truncation of long sequences we included specialized length models and sentence segmentation techniques .
Our final system provides a 9.9 BLEU points improvement over a baseline Transformer on our test set ( newstest 2019 ) .
Introduction
The WMT2020 Shared Task on translating news data from Chinese into English provides a challenging test for machine translation systems and an ideal domain for researchers to evaluate new techniques .
The DeepMind submission to the constrained data track is based on the modular noisy channel document translation architecture advocated by Yu et al . ( 2020 ) .
In this formulation , the posterior probability of a translation is the product of the unconditional probability of the output document ( the language model ) and the conditional probability of the translation from the output to source ( the channel model ) .
By assuming sentences within a document are independently translated , we can train the channel model using readily available parallel sentences , rather than being reliant on less numerous parallel documents , and the language model on monolingual documents .
This modular approach allows the components of the system to be implemented and optimized independently while at inference time , when we reason over the posterior distribution of translations given the source document , conditional dependencies between translations are induced by the language model prior .
The core of our document-level translation architecture is the noisy channel reranker .
It requires proposal , channel , and language models , each of which is optimized separately using different techniques and approaches .
For the proposal and channel models we use Transformer models ( Vaswani et al. , 2017 ) ( ?4.1 ) with data augmentation ( ?4.2 ) , such as back translation ( Edunov et al. , 2018 ) , distillation ( Kim and Rush , 2016 ; Liu et al. , 2016 ) , and forward - translated parallel documents .
We further improve these sequence-to-sequence ( seq2seq ) models by fine-tuning them with in - domain data ( ?4.3 ) .
To improve the robustness of the reranker we apply adversarial training and contrastive learning methods for uncertainty estimation ( ?4.4 ) .
Finally , we include candidate translations generated by Monte-Carlo Tree Search ( MCTS ) ( ?B ) in order to improve the diversity of the candidate pool for the reranker .
Our language models are based on the Transformer - XL architecture ( Dai et al. , 2019 ) and optimized with distillation and fine-tuning with in - domain data ( ?5 ) .
During development , we observed weaknesses in our system 's translations for long sentences , largely due to premature truncations .
We developed several techniques to mitigate this issue such as sentence segmentation ( breaking sentences into logical complete segments ) and training specialized models with synthetically constructed long sequences to generate additional proposals for our reranker ( ?A ) .
Experiments show that the aforementioned techniques are very effective : our system outperforms the Transformer baseline by 9.9 BLEU points on our test set ( newstest 2019 ) .
Our final system achieves a BLEU score of 35.4 on the Chinese ?
English news test set of WMT2020 .
Document Translation via Bayes ' Rule Following
Yu et al. ( 2020 ) , we model document translation via Bayes ' rule .
We define X = ( x 1 , x 2 , . . . , x I ) as the source document with I sentences , and similarly , Y = ( y 1 , y 2 , . . . , y J ) as the target document with J sentences , where x i and y j denote the ith sentence in the source document and the jth sentence in the target document respectively .
We assume that I = J .
The translation of a document X is determined by finding the document ? , where p( ? | X ) is maximal .
? = arg max Y p( Y | X ) = arg max Y p( X | Y ) channel model ?
p( Y ) language model . ( 1 ) We further assume that sentences are independently translated , and that the sentences within a document admit a left-to - right factorization according to the chain rule .
Therefore , we have ? ? arg max Y | Y | i=1 p( x i | y i ) ? p( y i | Y < i ) , ( 2 ) where Y < i = ( y 1 , . . . , y i?1 ) denotes a document prefix consisting of the first i ?
1 target sentences .
The advantages of this formulation is that during training the translation models can be learned from parallel sentences and monolingual documents which are vastly available in practice unlike parallel documents .
During test time , when a source document is observed , conditional dependencies between the translation of the source sentences are created in the posterior .
Reranking Because of the global dependencies in the posterior distribution , decoding in the aforementioned document translation model is computationally expensive .
Following Yu et al. ( 2020 ) , we use an auxiliary proposal model q(y | x ) , that approximates the posterior distribution using a direct model , to focus our search on promising parts of the output space .
We then carry out the reranking process using an iterative beam search , over candidates generated by the proposal model q , to optimize the objective : O(X , Y < i , y i ) = ?
1 log p PM ( y i | x i ) + ?
2 log p AM ( y i | x i ) + ?
3 log p CM ( x i | y i ) + log p LM ( y i | Y < i ) + ? 4 |y i |+ O( X , Y < i?1 , y i?1 ) , ( 3 ) where p PM is the proposal probabilities model , p AM is the adversarially trained proposal model ( ?4.4.1 ) , p CM is the channel model ( ?4.4.2 ) , p LM is the language model ( ?5.1 ) , and |y | denotes the number of tokens in the sentence y .
The weights of component models ( ? s ) are hyperparameters to be tuned in experiments .
In practice , we generate for each source sentence x i in the document X , a series of candidates y i , using the proposal model q .
As all of the terms in the objective , except for p LM , only involve independent target sentences , they can be computed ahead of time in a scoring phase .
The scored candidates are then passed to the reranker , where the language model is evaluated on the successive prefixes explored by the search , and which outputs the final document ? .
Iterative beam search
The algorithm starts with k complete documents using randomly selected candidates for each of the source sentences .
We then iterate through every source sentence x i , replacing the randomly picked initial candidate with every available candidate y i .
We pick the top k scoring complete documents and continue iterating over the document .
Unlike traditional beam search used by Yu et al . ( 2020 ) , we go through every sentence in the document multiple times , until the top 1 translation converges ( usually 2 to 4 full iterations ) .
This allows for context from latter sentences in the document to inform the choice of earlier candidates .
Iterative beam search found improvements in the model objective over traditional beam search in 63 % of the documents in our test set .
Improvements in objective did not translate in a stable improvement in BLEU or META scores ( Eqn. 4 ) in fact those scores were slightly reduced for a number of documents .
Nevertheless , an informal human evaluation of translated documents showed preference for iterative beam search .
Selection of the hyperparameters ?
We perform a grid search over the hyperparameters ? to maximize a metric on the validation set .
The metric we use is the following META score , combining corpus-level BLEU , TER , METEOR , and the 0.1 quantile of per-document BLEU , such that : ( 1 ? META ) 4 = TER ? ( 1 ? BLEU ) ? ( 1 ? METEOR ) ? ( 1 ? q 0.1 ( BLEU ) ) .
( 4 )
When several configurations of hyperparameters achieve values of META very close to the maximum ( within 0.02 ) , we pick the one maximizing BLEU and / or minimizing the L 2 norm of the ?s , considered as a vector .
This corresponds to an intuitive prior towards giving more weight to the language model .
Training Data
To train all of the models used in our system , we made use only of the constrained data provided to shared task participants .
In this section , we discuss the preprocessing and normalization techniques we carried out in an attempt to reduce spurious uncertainty in the modeling problem .
Text preprocessing
We carried out the following text normalization steps prior to use in any models : ?
Text normalization .
Unicode canonicalization ( NKFD from ) , replacement of common multiple encoding errors present in training data , standardization of quotation marks into " directional " variants , conversion of any traditional Chinese characters into simplified forms .
Replacement of non-American spelling variants with American spellings using the aspell library .
1 ? Segmentation into words .
Chinese was segmented into word-like units using the Jieba segmentation tool .
2 Punctuation was split from English words using a purpose-built library .
These processes were not completely invertible , but they could be undone with simple rules so as to generate presentation - ready English and Chinese .
?
True-casing .
Words containing only an initial capital letter that occurred at the start of a sentence were replaced with the capitalized variant that occurred most frequently in other positions of the English monolingual training data .
Thus , in the previous sentence the initial token would have been words rather than Words .
Subword units
To encode text into sub-word units , we used the sentencepiece tool ( Kudo and Richardson , 2018 ) .
For seq2seq models ( i.e. , the channel model and proposal models ) , we trained the segmentation model on the first 10 million sentences of the parallel training corpus , 3 using joint source and target unigram ( Kudo , 2018 ) subword segmentation algorithm with a target vocabulary of 32 K tokens and minimum character coverage of 0.9995 , which resulted in 32,768 word pieces .
4
For the language model , we used the English side alone with the same vocabulary size and a character coverage of 1.0 .
Proposal and Channel Models
The proposal model , used to generate candidate translations , and the scoring models ( proposal probability model , adversarially - trained proposal model , channel model ) , used to compute features for the reranker , are seq2seq models .
We describe here how we train and use them .
Sequence-to-Sequence Model
All our models are based on the Transformer architecture ( Vaswani et al. , 2017 ) .
We increased the inner dimension of the feed -forward network from 4,096 to 8,096 and decreased the model size ( d model ) from 1,024 to 512 , which allowed us to use 12 layers with 16 attention heads each .
Additionally , we tied the source and target embedding layers .
Following ( Vaswani et al. , 2018 ) , we applied layer normalization to the input of every sublayer as opposed to its original placement after the element - wise residual addition .
We used different dropout values for different components : 0.1 for the multi-head attention , 0.05 in the feed -forward network , and finally 0.3 after the sub-layer .
Learning rate schedule and dropout were found using the Batched Gaussian Process Bandits ( Desautels et al. , 2014 ) algorithm as implemented by Vizier ( Golovin et al. , 2017 ) .
All other hyperparameters were decided upon using grid search .
During training , we used a maximum sequence length of 96 .
For decoding , we used beam search with beam size 6 , and set the length penalty alpha to 0.8 , and a maximum decoding length of 384 .
Multi model ensembling was done via softmax output averaging as described in ( Freitag et al. , 2017 ) .
Data Augmentation
In this section , we introduce how we augment data based on the given bilingual data and monolingual data .
When we train the proposal and channel models , we use all the augmented data along with the original bilingual data .
Back-translation
We perform back -translation from monolingual English data using fine-tuned channel models ( English ?
Chinese ) with top-k sampling following ( Edunov et al. , 2018 ) with k = 50 during decoding .
We used the same indomain monolingual data as described in ?5.2 .
We score the back - translated data with fine-tuned proposal ( Chinese ?
English ) models , and filter them based on the quantiles of length ratios , sequence log-probability and cross-entropy between one - hot empirical translations and logits from the scorer model .
The filtering helped to reduce the size of data from 43.4 M to 29.9 M paired sentences .
Forward translation to generate synthetic parallel documents
We applied a version of our system to monolingual Chinese documents from Gigaword to get synthetic English documents .
We only kept documents having between 4 and 25 sentences , we rejected outliers according to their probabilities under the language model , the channel model , and to the overall objective .
These were then used to train subsequent versions of the forward ( Chinese ?
English ) models .
Data distillation
We use knowledge distillation ( Kim and Rush , 2016 ) to do distillation on the original dataset .
Specifically , we translate the source-side of the bilingual data using previously trained proposal models ( including Right - to - Left ( Liu et al. , 2016 ) and Left-to- Right models ) and generate distilled candidates .
The generated sentences are filtered if BLEU scores are below 30 ( Wang et al. , 2018 ; .
We then train models on the filtered data along with the original bilingual data and back -translation data .
We repeat this process three times using models trained on newly generated data from the previous iteration .
We empirically do not find Right - to - Left models significantly differ from Left- to - Right models in performance .
Qualitatively we find that distilled data correct few errors in the original bilingual data .
Fine-tuning Fine-tuning with in- domain data has been an effective approach for improving translation quality as shown by existing work Ng et al. , 2019 ) .
After training the proposal models with the mix of real and synthetic parallel data , we fine - tuned the models with CWMT and a subset of newstest2017 and newstest 2018 which were not used for validation .
Improving Uncertainty Estimation
To improve the robustness of noisy channel reranking , we explore two approaches for improving uncertainty estimation of the seq2seq scoring models .
Adversarially Trained Proposal Models
To simulate different wordings and noises in source and candidate sentences , we follow to train the models on noisy adversarial inputs and targets .
We use bidirectional languagemodels to provide the noisy candidates and select the candidates with highest loss ( i.e. , adversarial source - target inputs ) .
During the training , we optimize the original loss with clean source - target pairs , the language model losses for source and target sides , and the adversarial loss using adversarial source -target inputs .
In the final scoring , we use an ensemble of eight adversarially trained models with few differences from Cheng et al . ( 2019 ) : ( a) We explore training with and without the language model losses .
Though the models trained without the language model loss generate quite noisy sentences , we empirically find this approach still helps the overall performance .
( b) In addition to using the clean hard - labels for the noisy source - target pairs for the adversarial loss as in the original work , we explore a variation using a KL loss between the adversarial source - target logits and the clean source -target logits .
We find this variant also improves the overall performance .
Contrastive Channel Models
When scoring candidates , we want the channel models to be sensitive to translation noise ( dropped words , permutation , or blanked words ) ( Edunov et al. , 2018 ) .
Hence , we develop contrastive training Welbl et al. , 2020 ) to train the models such that it will be more robust in estimating the channel probabilities .
Specifically , we use n-gram Transformers ( Chelba et al. , 2020 ) with the contrastive loss : ( 4 ) Instead of using the contrastive loss , we include two models trained to minimize the perturbed loss terms directly .
( 5 ) Unlike , where the authors firstly train with the maximum likelihood objective and then finetune with the contrastive loss , we find it empirically works better to train models with linearly increased weights ( increasing from 0 to 1 during training ) to the contrastive loss ( Eq. ( 5 ) ) along with the original negative log likelihood loss .
max { log p( x | y ) + ? ? log p( x | y ) , 0 } , ( 5 )
Filtering Candidate Translations
After obtaining candidate translations from strong proposal models , we filter out candidates with length ratio outside of [ e ?1 , e 1 ] , or which do not end with end-of-sentence punctuation when the source does , or with more than 4 consecutive identical tokens , or which are excessively compressible , indicating repeated contents , according to the following .
We learn a piece-wise linear ordinary least squares model of the zlib-compressed length of true English sentences from their uncompressed length in UTF - 8 , using the English side of the training data .
We then reject candidates the actual compressed length of which is more than 12 standard deviations below their predicted compressed length .
Language Model
In this section , we describe the architecture of the language models we used and how we trained them .
Model
The auto-regressive document language model is a Transformer - XL ( Dai et al. , 2019 ) Razavi ( 2020 ) , we also used 4 - layer blocks of short and long ( 128-128-128-512 ) attention memories , capturing short - range correlations in the earlier layers and long-range correlations in the later ones .
This led to a 20 % speedup of training , and helped the model generalize better to the validation set .
We also used knowledge distillation in our Transformer - XL model with a setup similar to Born Again Neural Networks ( BANN ) ( Furlanello et al. , 2018 ) , where we regularize the original loss function with term based on the cross-entropy between the new models outputs ( student ) and the outputs of the original ( teacher ) model .
Let L denote cross entropy loss function , y onehot encoded label , s and t outputs of the student and teacher model respectively , then the BANN loss is defined as follows : L BANN = T i=1 L(y i , s i ) + ? ? L( t i , s i ) .
( 6 ) We trained our student network on the loss function in Eqn. 6 and found that ? = 1 had the best validation perplexity .
Data
The English data used to train our language models was prepared as described in ?3 .
In -domain document data for LM training
We found that training LMs on a subset of training data that was more closely aligned with the validation set vastly improved the perplexity on the validation and test sets ( ? 10 % ) .
To select a well - aligned subset of training data , we ranked the training data according to TF - IDF similarity with each validation document and collected the top 1,000 documents for each validation query together , to form our training data for the LM training .
We also tried mixing this sub-sampled in- domain data with the raw data using different weights ( essentially equivalent to up-weighting the in- domain data ) and found that using purely in- domain data outperformed all other mixing schemes in terms of held - out perplexities and thus , the in-domain data became our training dataset .
As an auxiliary benefit , the model trained on the in-domain dataset ( 340 K iterations ) also converged much earlier than the one trained on the raw dataset ( 500 K iterations ) .
Similar to the sequence model fine-tuning outlined in ?4.3 , we also fine- tuned our trained language model in order to align the model more closely with the language constructs and domain information in our test data .
Table 1 shows the perplexity numbers on the validation set obtained by different train data and model variants described above on the validation dataset .
Experiments and Results
We use the original Chinese subset of newstest2017 and newstest2018 as our validation set and new-stest2019 as our test set .
The candidate translations for the reranker are generated by 8 ensemble models ( 6 from each ) .
Table 2 presents the results of our models on the test set .
We report case-sensitive SacreBLEU scores ( Post , 2018 ) .
Both data augmentation and fine-tuning significantly improve the performance .
Ensembling and noisy channel reranking gives about 0.8 and 0.6 BLEU boost , respectively .
Finally , our specialized methods for handling long sequences ( described in ?A ) yield a further 0.8 BLEU improvement .
In our final submitted system , we tune the weights of component models and the hyperparameters of sentence segmentation models using a combination of our validation set and test set .
For the candidate translations of the reranker , apart from the existing 48 proposals generated by 8 ensemble models , we include additional 48 proposals generated by 8 ensemble models which are finetuned with CWMT , newstest 2017 , newstest2018 , and newstest2019 .
We also include translations generated by MCTS decoding ( ?B ) in our nonprimary system .
We find that adding a feature marking the length of source sentences longer than 60 words helps the reranker handle long sentences better .
We therefore include this feature in addition to proposal probability , adversarial proposal probability , channel probability , language model probability , and length bonus ( Eqn. 3 ) .
Our system achieves a 35.4 BLEU score on new-stest2020 .
Conclusion
This paper describes the DeepMind submission to the WMT2020 news Chinese-English translation task .
Using the noisy channel model ( Yu et al. , 2020 ) as our core document translation system , we optimized its component models using data augmentation , fine -tuning with in - domain data , MCTS decoding ( ?B ) , and knowledge distillation .
We also addressed premature termination in long sentences by training specialized length expert models and segmenting long sentences into multiple shorter sentences ( ?A ) .
We have demonstrated the marginal contributions of these methods in our analysis and our final system comprising all these methods outperforms the Transformer baseline by 9.9 BLEU points on newstest2019 .
In the appendix , we additionally include the description of our specialized methods for handling long sequences and the MCTS decoding algorithm .
The candidate translations generated by MCTS decoding are added to the candidate pool of the noisy channel reranker in our non-primary system .
