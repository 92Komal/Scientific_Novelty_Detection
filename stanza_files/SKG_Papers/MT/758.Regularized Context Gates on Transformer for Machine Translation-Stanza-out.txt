title
Regularized Context Gates on Transformer for Machine Translation
abstract
Context gates are effective to control the contributions from the source and target contexts in the recurrent neural network ( RNN ) based neural machine translation ( NMT ) .
However , it is challenging to extend them into the advanced Transformer architecture , which is more complicated than RNN .
This paper first provides a method to identify source and target contexts and then introduce a gate mechanism to control the source and target contributions in Transformer .
In addition , to further reduce the bias problem in the gate mechanism , this paper proposes a regularization method to guide the learning of the gates with supervision automatically generated using pointwise mutual information .
Extensive experiments on 4 translation datasets demonstrate that the proposed model obtains an averaged gain of 1.0 BLEU score over a strong Transformer baseline .
Introduction
An essence to modeling translation is how to learn an effective context from a sentence pair .
Statistical machine translation ( SMT ) models the source context from the source-side of a translation model and models the target context from a target - side language model ( Koehn et al. , 2003 ; Koehn , 2009 ; Chiang , 2005 ) .
These two models are trained independently .
On the contrary , neural machine translation ( NMT ) advocates a unified manner to jointly learn source and target context using an encoderdecoder framework with an attention mechanism , leading to substantial gains over SMT in translation quality ( Sutskever et al. , 2014 ; Bahdanau et al. , 2014 ; Gehring et al. , 2017 ; Vaswani et al. , 2017 ) .
Prior work on attention mechanism ( Luong et al. , 2015 ; Liu et al. , 2016 ; Mi et al. , 2016 ; Li et al. , 2018 ; Elbayad et al. , 2018 ; Yang et al. , 2020 ) have shown a better context representation is helpful to translation performance .
former obtain an unfaithful translation by wrongly translate " t ? q?u " into " play golf " because referring too much target context .
By regularizing the context gates , the purposed method corrects the translation of " t ? q?u " into " play soccer " .
The light font denotes the target words to be translated in the future .
For original Transformer , the source and target context are added directly without any rebalancing .
However , a standard NMT system is incapable of effectively controlling the contributions from source and target contexts 2017 ) carefully designed context gates to dynamically control the influence from source and target contexts and observed significant improvements in the recurrent neural network ( RNN ) based NMT .
Although Transformer ( Vaswani et al. , 2017 ) delivers significant gains over RNN for translation , there are still one third translation errors related to context control problem as described in Section 3.3 .
Obviously , it is feasible to extend the context gates in RNN based NMT into Transformer , but an obstacle to accomplishing this goal is the complicated archi-tecture in Transformer , where the source and target words are tightly coupled .
Thus , it is challenging to put context gates into practice in Transformer .
In this paper , under the Transformer architecture , we firstly provide a way to define the source and target contexts and then obtain our model by combining both source and target contexts with context gates , which actually induces a probabilistic model indicating whether the next generated word is contributed from the source or target sentence ( Li et al. , 2019 ) .
In our preliminary experiments , this model only achieves modest gains over Transformer because the context selection error reduction is very limited as described in Section 3.3 .
To further address this issue , we propose a probabilistic model whose loss function is derived from external supervision as regularization for the context gates .
This probabilistic model is jointly trained with the context gates in NMT .
As it is too costly to annotate this supervision for a large-scale training corpus manually , we instead propose a simple yet effective method to automatically generate supervision using pointwise mutual information , inspired by word collocation ( Bouma , 2009 ) .
In this way , the resulting NMT model is capable of controlling the contributions from source and target contexts effectively .
We conduct extensive experiments on 4 benchmark datasets , and experimental results demonstrate that the proposed gated model obtains an averaged improvement of 1.0 BLEU point over corresponding strong Transformer baselines .
In addition , we design a novel analysis to show that the improvement of translation performance is indeed caused by relieving the problem of wrongly focusing on the source or target context .
Methodology Given a source sentence x = x 1 , ? ? ? , x |x| and a target sentence y = y 1 , ? ? ? , y |y | , our proposed model is defined by the following conditional probability under the Transformer architecture : 1 P ( y | x ) = |y | i=1 P ( y i | y < i , x ) = |y | i=1 P y i | c L i , ( 1 ) where y < i = y 1 , . . . , y i?1 denotes a prefix of y with length i ?
1 , and c L i denotes the L th layer context in the decoder with L layers which is obtained from the representation of y < i and h L , i.e. , the top layer hidden representation of x , similar to the original Transformer .
To finish the overall definition of our model in equation 1 , we will expand the definition c L i based on context gates in the following subsections .
Context Gated Transformer
To develop context gates for our model , it is necessary to define the source and target contexts at first .
Unlike the case in RNN , the source sentence x and the target prefix y < i are tightly coupled in our model , and thus it is not trivial to define the source and target contexts .
Suppose the source and target contexts at each layer l are denoted by s l i and t l i .
We recursively define them from c l?1 < i as follows .
2 t l i = rn ? ln ? att c l?1 i , c l?1 < i , s l i = ln ?
att t l i , h L , ( 2 ) where ? is functional composition , att ( q , kv ) denotes multiple head attention with q as query , k as key , v as value , and rn as a residual network ( He et al. , 2016 ) , ln is layer normalization ( Ba et al. , 2016 ) , and all parameters are removed for simplicity .
In order to control the contributions from source or target side , we define c l i by introducing a context gate z l i to combine s l i and t l i as following : c l i = rn ? ln ? ff ( 1 ? z l i ) ?
t l i + z l i ? s l i ( 3 ) with z l i = ? ff t l i s l i , ( 4 ) where ff denotes a feedforward neural network , denotes concatenation , ?( ? ) denotes a sigmoid function , and ? denotes an element-wise multiplication .
z l i is a vector ( Tu et al . ( 2017 ) reported that a gating vector is better than a gating scalar ) .
Note that each component in z l i actually induces a probabilistic model indicating whether the next generated word y i is mainly contributed from the source ( x ) or target sentence ( y < i ) , as shown in Figure 1 . Remark
It is worth mentioning that our proposed model is similar to the standard Transformer with boiling down to replacing a residual connection with a high way connection ( Srivastava et al. , 2015 ; : if we replace ( 1 ? z l i ) ?
t l i + z l i ?
s l i in equation 3 by t l i + s l i , the proposed model is reduced to Transformer .
Regularization of Context Gates
In our preliminary experiments , we found learning context gates from scratch cannot effectively reduce the context selection errors as described in Section 3.3 .
To address this issue , we propose a regularization method to guide the learning of context gates by external supervision z * i which is a binary number representing whether y i is contributed from either source ( z * i = 1 ) or target sentence ( z * i = 0 ) .
Formally , the training objective is defined as follows : = ? log P ( y | x ) +?
l, i z * i max ( 0.5 ? z
l i , 0 ) + ( 1 ? z * i ) max ( z l i ? 0.5 , 0 ) , ( 5 ) where z l i is a context gate defined in equation 4 and ? is a hyperparameter to be tuned in experiments .
Note that we only regularize the gates during the training , but we skip the regularization during inference .
Because golden z * i are inaccessible for each word y i in the training corpus , we ideally have to annotate it manually .
However , it is costly for human to label such a large scale dataset .
Instead , we propose an automatic method to generate its value in practice in the next subsection .
Generating Supervision z * i
To decide whether y i is contributed from the source ( x ) or target sentence ( y < i ) ( Li et al. , 2019 ) , a metric to measure the correlation between a pair of words ( y i , x j or y i , y k for k < i ) is first required .
This is closely related to a well - studied problem , i.e. , word collocation ( Liu et al. , 2009 ) , and we simply employ the pointwise mutual information ( PMI ) to measure the correlation between a word pair ? , ? following Bouma ( 2009 ) : pmi ( ? , ? ) = log P ( ? , ? ) P ( ?) P ( ? ) = log Z + log C ( ? , ? ) C ( ? ) C ( ? ) , ( 6 ) where C ( ? ) and C ( ? ) are word counts , C ( ? , ? ) is the co-occurrence count of words ? and ? , and Z is the normalizer , i.e. , the total number of all possible ( ? , ? ) pairs .
To obtain the context gates , we define two types of PMI according to different C ( ? , ? ) including two scenarios as follows .
PMI in the Bilingual Scenario
For each parallel sentence pair x , y in training set , C ( y i , x j ) is added by one if both y i ? y and x j ? x. PMI in the Monolingual Scenario
In the translation scenario , only the words in the preceding context of a target word should be considered .
So for any target sentence y in the training set , C ( y i , y k ) is added by one if both y i ?
y and y k ?
y < i .
Given the two kinds of PMI for a bilingual sentence x , y , each z * i for each y i is defined as follows , z * i = 1 max j pmi(y i , x j ) > max k<i pmi(y i , y k ) , ( 7 ) where 1 b is a binary function valued by 1 if b is true and 0 otherwise .
In equation 7 , we employ max strategy to measure the correlation between y i and a sentence ( x or y < i ) .
Indeed , it is similar to use the average strategy , but we did not find its gains over max in our experiments .
Experiments
The proposed methods are evaluated on NIST ZH?EN 3 , WMT14 EN?DE 4 , IWSLT14 DE?EN 5 and IWSLT17 FR?EN 6 tasks .
To make our NMT models capable of open-vocabulary translation , all datasets are preprocessed with Byte Pair Encoding ( Sennrich et al. , 2015 ) .
All proposed methods are implemented on top of Transformer ( Vaswani et al. , 2017 ) which is the state - of- the - art NMT system .
Case-insensitive BLEU score ( Papineni et al. , 2002 ) is used to evaluate translation quality of ZH?EN , DE?EN and FR?EN .
For the fair comparison with the related work , EN ?
DE is evaluated with case-sensitive BLEU score .
Setup details are described in Appendix A .
Tuning Regularization Coefficient
In the beginning of our experiments , we tune the regularization coefficient ? on the DE?EN task .
Table 2 shows the robustness of ? , because the translation performance only fluctuates slightly over various ?.
In particular , the best performance is achieved when ? = 1 , which is the default setting throughout this paper .
Translation Performance
Table 1 shows the translation quality of our methods in BLEU .
Our observations are as follows : 1 ) The performance of our implementation of the Transformer is slightly higher than Vaswani et al . ( 2017 ) , which indicates we are in a fair comparison .
2 ) The proposed Context Gates achieves modest improvement over the baseline .
As we mentioned in Section 2.1 , the structure of RNN based NMT is quite different from the Transformer .
Therefore , naively introducing the gate mechanism to the Transformer without adaptation does not obtain similar gains as it does in RNN based NMT .
3 ) The proposed Regularized Context Gates improves nearly 1.0 BLEU score over the baseline and outperforms all existing related work .
This indicates that the regularization can make context gates more effective in relieving the context control problem as discussed following .
Error Analysis
To explain the success of Regularized Context Gates , we analyze the error rates of translation and context selection .
Given a sentence pair x and y , the forced decoding translation error is defined as P ( y i | y < i , x ) < P (? i | y < i , x ) , where ?i arg max v P ( v | y < i , x ) and v denotes any to -ken in the vocabulary .
The context selection error is defined as z * i ( y i ) = z * i ( ? i ) , where z * i is defined in equation 7 .
Note that a context selection error must be a translation error but the opposite is not true .
The example shown in Figure 1 As shown in Table 3 , the Regularized Context Gates significantly reduce the translation error by avoiding the context selection error .
The Context Gates are also able to avoid few context selection error but cannot make a notable improvement in translation performance .
It is worth to note that there is approximately one third translation error is related to context selection error .
The Regularized Context Gates indeed alleviate this severe problem by effectively rebalancing of source and target context for translation .
means the model tends to trust its language model more than the source context , and we call this context imbalance bias of the freely learned context gate .
Specifically , this bias will make the translation unfaithful for some source tokens .
As shown in Table 4 , the Regularized Context Gates demonstrates more balanced behavior ( 0.51 ? 0.5 ) over the source and target context with similar variance .
Statistics of Context Gates
Regularization in Different Layers
To investigate the sensitivity of choosing different layers for regularization , we only regularize the context gate in every single layer .
Table 5 shows that there is no significant performance difference , but all single layer regularized context gate models are slightly inferior to the model , which regularizes all the gates .
Moreover , since nearly no computation overhead is introduced and for design simplicity , we adopt regularizing all the layers .
Effects on Long Sentences
In Tu et al . ( 2017 ) , context gates alleviate the problem of long sentence translation of attentional RNN based system ( Bahdanau et al. , 2014 ) .
We follow Tu et al . ( 2017 ) and compare the translation performances according to different lengths of the sentences .
As shown in Figure 2 , we find Context Gates does not improve the translation of long sentences but translate short sentences better .
Fortunately , the Regularized Context Gates indeed significantly improves the translation for both short sentences and long sentences .
Conclusions
This paper transplants context gates from the RNN based NMT to the Transformer to control the source and target context for translation .
We find [ 0,10 ) [ 10,20 ) [ 20,30 ) [ 30,40 ) [ 40,50 ) [ 50,60 ) [ 60,130 ) Length of Source Sentence that context gates only modestly improve the translation quality of the Transformer , because learning context gates freely from scratch is more challenging for the Transformer with the complicated structure than for RNN .
Based on this observation , we propose a regularization method to guide the learning of context gates with an effective way to generate supervision from training data .
Experimental results show the regularized context gates can significantly improve translation performances over different translation tasks even though the context control problem is only slightly relieved .
In the future , we believe more work on alleviating context control problem has the potential to improve translation performance as quantified in Table 3 . w ? j?ng ch?ng h? w? d? t?ng h?ng m?n y? q? t? q?u ?
Figure 1 : 1 Figure1 : A running example to raise the context control problem .
Both original and context gated Transformer obtain an unfaithful translation by wrongly translate " t ? q?u " into " play golf " because referring too much target context .
By regularizing the context gates , the purposed method corrects the translation of " t ? q?u " into " play soccer " .
The light font denotes the target words to be translated in the future .
For original Transformer , the source and target context are added directly without any rebalancing .
to deliver highly adequate translations as shown in Figure 1 .
As a result , Tu et al . (
Figure 2 : 2 Figure 2 : Translation performance on MT08 test set with respect to different lengths of source sentence .
Regularized Context Gates significantly improves the translation of short and long sentences .
Table 1 : 1 Translation performances ( BLEU ) .
TheRNN based NMT et al. , 2014 ) is reported from the baseline model in Tu et al . ( 2017 ) . " params " shows the number of parameters of models when training ZH?EN except Vaswani et al. ( 2017 ) is for EN ?
DE tasks .
Models params ?10 6 ZH?EN MT05 MT06 MT08 EN?DE DE?EN FR?EN RNN based NMT 84 30.6 31.1 23.2 - - - Tu et al . ( 2017 ) 88 34.1 34.8 26.2 - - - Vaswani et al . ( 2017 ) 65 - - - 27.3 - - Ma et al. ( 2018 ) - 36.8 35.9 27.6 - - - Zhao et al. ( 2018 ) - 43.9 44.0 33.3 - - - Cheng et al. ( 2018 ) - 44.0 44.4 34.9 - - - Transformer 74 46.9 47.4 38.3 27.4 32.2 36.8
This Work Context Gates Regularized Context Gates 92 92 47.1 47.7 47.6 48.3 39.1 39.7 27.9 28.1 32.5 33.0 37.7 38.3 ? 0.1 0.5 1 2 10 BLEU 32.7 32.6 33.0 32.7 32.6 * Results are measured on DE?EN task .
Table 2 : 2 Translation performance over different regularization coefficient ?.
also demonstrates a context selection error indicating the translation error is related with the bad context selection .
Models FER CER CE / FE Transformer 40.5 13.8 33.9 Context Gates 40.5 13.7 33.7 Regularized Context Gates 40.0 13.4 33.4 * Results are measured on MT08 of ZH ?EN task .
Table 3 3 : Forced decoding translation error rate ( FER ) , context selection error rate ( CER ) and the proportion of context selection errors over forced decoding trans - lation errors ( CE / FE ) of the original and context gated Transformer with or without regularization .
Table 4 summarizes the mean and variance of each context gate ( every dimension of the context gate vectors ) over the MT08 test set .
It shows that learning context gates freely from scratch tends to pay more attention to target context ( 0.38 < 0.5 ) , which Results are measured on MT08 of ZH ?EN task .
Models Mean Variance Context Gates 0.38 0.10 Regularized Context Gates 0.51 0.13 *
Table 4 : 4 Mean and variance of context gates
Table 5 : 5 Regularize context gates on different layers . " N/ A " indicates regularization is not added .
" ALL " indicates regularization is added to all the layers .
Throughout this paper , a variable in bold font such as x denotes a sequence while regular font such as x denotes an element which may be a scalar x , vector x or matrix X .
For the base case , c 0 < i is word embedding of y<i .
LDC2000T50 , LDC2002L27 , LDC2002T01 , LDC2002E18 , LDC2003E07 , LDC2003E14 , LDC2003T17 , LDC2004T07 4 WMT14 : http://www.statmt.org/wmt14/ 5 IWSLT14 : http://workshop2014.iwslt.org/ 6 IWSLT17 : http://workshop2017.iwslt.org/
