title
abstract
Introduction Welcome to the Third Workshop on Neural Generation and Translation .
This workshop aims to cultivate research on the leading edge in neural machine translation and other aspects of machine translation , generation , and multilinguality that utilize neural models .
In this year 's workshop we are extremely pleased to be able to host four invited talks from leading lights in the field , namely : Michael Auli , Mohit Bansal , Mirella Lapata and Jason Weston .
In addition this year 's workshop will feature a session devoted to a new shared task on efficient machine translation .
We received a total of 68 submissions , from which we accepted 36 .
There were three crosssubmissions , seven long abstracts and 26 full papers .
There were also seven system submission papers .
All research papers were reviewed twice through a double blind review process , and avoiding conflicts of interest .
The workshop had an acceptance rate of 53 % .
Due to the large number of invited talks , and to encourage discussion , only the two papers selected for best paper awards will be presented orally , and the remainder will be presented in a single poster session .
We would like to thank all authors for their submissions , and the program committee members for their valuable efforts in reviewing the papers for the workshop .
We would also like to thank Google and Apple for their generous sponsorship .
Table of Contents Findings of the Third Workshop on Neural Generation and TranslationHiroaki Hayashi , Yusuke Oda , Alexandra Birch , Ioannis Konstas , Andrew Finch , Minh - Thang Luong , Graham Neubig and Katsuhito Sudoh . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 Hello , It 's GPT -2 - How Can I Help You ?
Towards the Use of Pretrained Language Models for Task - Oriented Dialogue Systems Pawe ?
Budzianowski and Ivan Vuli ?
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 Recycling a Pre-trained BERT Encoder for Neural Machine Translation Kenji Imamura and Eiichiro Sumita . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 Generating a Common Question from Multiple Documents using Multi-source Encoder-Decoder Models Woon Sang Cho , Yizhe Zhang , Sudha Rao , Chris Brockett and Sungjin Lee . . . . . . . . . . . . . . . . . . 32 Generating Diverse Story Continuations with Controllable Semantics Lifu Tu , Xiaoan Ding , Dong Yu and Kevin Gimpel . . Controlled Text Generation for Data Augmentation in Intelligent Artificial Agents Nikolaos Malandrakis , Minmin Shen , Anuj Goyal , Shuyang Gao , Abhishek Sethi and Angeliki Metallinou . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90 Zero-Resource Neural Machine Translation with Monolingual Pivot Data Anna Currey and Kenneth Heafield . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
On the use of BERT for Neural Machine Translation Stephane Clinchant , Kweon Woo Jung and Vassilina Nikoulina .
. . . . . . . . . . . . . . . . . . . . . . . . . . . .108
On the Importance of the Kullback - Leibler Divergence Term in Variational Autoencoders for Text Generation Victor Prokhorov , Ehsan Shareghi , Yingzhen Li , Mohammad Taher Pilehvar and Nigel Collier 118 Decomposing Textual Information For Style Transfer Ivan P. Yamshchikov , Viacheslav Shibaev , Aleksander Nagaev , J?rgen Jost and Alexey Tikhonov 128 Unsupervised Evaluation Metrics and Learning Criteria for Non-Parallel Textual Transfer Richard Yuanzhe Pang and Kevin Gimpel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138 Enhanced Transformer Model for Data-to-Text Generation Li GONG , Josep Crego and Jean Senellart . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148 Generalization in Generation : A closer look at Exposure Bias Florian Schmidt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 Domain Differential Adaptation for Neural Machine Translation Zi-Yi Dou , Xinyi Wang , Junjie Hu and Graham Neubig . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 Transformer - based Model for Single Documents Neural Summarization Elozino Egonmwan and Yllias Chali . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 Making Asynchronous Stochastic Gradient Descent Work for Transformers Alham Fikri Aji and Kenneth Heafield . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
