title
Fine-grained linguistic evaluation for state- of- the- art Machine Translation
abstract
This paper describes a test suite submission providing detailed statistics of linguistic performance for the state - of- the - art German - English systems of the Fifth Conference of Machine Translation ( WMT20 ) .
The analysis covers 107 phenomena organized in 14 categories based on about 5,500 test items , including a manual annotation effort of 45 person hours .
Two systems ( Tohoku and VolcanTrans ) appear to have significantly better test suite accuracy than the others , although the best system of WMT20 is not significantly better than the one from WMT19 in a macro-average .
Additionally , we identify some linguistic phenomena where all systems suffer ( such as idioms , resultative predicates and pluperfect ) , but we are also able to identify particular weaknesses for individual systems ( such as quotation marks , lexical ambiguity and sluicing ) .
Most of the systems of WMT19 which submitted new versions this year show improvements .
Introduction Fine-grained evaluation has recently had increasing interest on several natural language processing ( NLP ) tasks .
Focusing on particular issues gives the possibility to analyse the automatic output in ways that cannot be seen by generic metrics .
This is of particular importance in the era of deep learning , which has led to high performances and differences that are relatively difficult to distinguish .
Additionally , detailed evaluation can provide indications for the improvement of the systems and the data collection , or allow focusing on phenomena of the long tail that might be of particular interest for certain cases ( e.g. social biases ; Stanovsky et al. , 2019 ) .
The most common method for fine- grained or focused evaluation are the test suites ( also known as challenge sets or benchmarks ; Guillou and Hardmeier , 2016 ; Ribeiro et al. , 2020 ) .
These are test sets engineered in a particular way , so that they can test the performance of NLP tasks on concrete issues ( M?ller et al. , 2018 ; Bawden et al. , 2018 ) .
This paper is presenting the use of such a test suite for the evaluation of the 11 German ?
English Machine Translation ( MT ) systems that participated at the Shared Task of the Fifth Conference of Machine Translation ( WMT20 ; Barrault et al. , 2020 ) .
The evaluation applies the DFKI test suite on German-English , via 5,514 test items which cover 107 linguistically motivated phenomena organized in 14 categories .
After a reference in related work ( Section 2 ) , we explain shortly the structure of the test suite ( Section 3 ) and present the results ( Section 4 ) and the conclusions ( Section 5 ) .
Related Work
The use of test suites was introduced along with the early steps of MT in the 1990 's ( King and Falkedal , 1990 ; Way , 1991 ; Heid and Hildenbrand , 1991 ) .
With the emergence of deep learning , recent works re-introduced test suites that focus on the evaluation of particular linguistic phenomena ( e.g. pronoun translation ; Guillou and Hardmeier , 2016 ) or more generic test suites that aim at comparing different MT technologies ( Isabelle et al. , 2017 ; Burchardt et al. , 2017 ) and Quality Estimation methods ( Avramidis et al. , 2018 ) .
The test suite track of the Conference of Machine Translation has already taken place two years in a row , allowing the presentation of several test suites , focusing on various linguistic phenomena and supporting different language directions .
These include work in grammatical contrasts ( Cinkova and Bojar , 2018 ) , discourse ( Bojar et al. , 2018 ) , morphology ( Burlot et al. , 2018 ) , pronouns ( Guillou et al. , 2018 ) and word sense disambiguation ( Rios et al. , 2018 ) .
When compared to the vast majority of the previous test suites , the one presented here is the only one Lexical Ambiguity Er las gerne Novellen .
He liked to read novels .
fail
He liked to read novellas .
pass Phrasal verb Warum starben die Dinosaurier aus ?
Why did the dinosaurs die ?
fail
Why did the dinosaurs die out ?
pass
Why did the dinosaurs become extinct ?
pass Ditransitive Perfect Ich habe Tim einen Kuchen gebacken .
I have baked a cake .
fail I baked Tim a cake .
pass Table 1 : Examples of passing and failing MT outputs that performs a systematic evaluation of more than one hundred phenomena on the state - of - the - art systems participating in WMT20 .
Method
The test suite is a test set that has been devised manually with the aim to allow testing the MT output for several linguistic phenomena .
The entire test suite consists of subsets that test one particular phenomenon each , through several test items .
Each test item of the test suite consists of a source sentence and a set of correct and / or incorrect MT outputs .
At the evaluation time , the test items are given as input to the MT systems and it is tested on whether the respective MT output consists a correct translation .
By observing the amount of the test items that are translated correctly , one can calculate the performance of the MT systems regarding the respective phenomenon .
The evaluation presented in this paper is based on the DFKI Test Suite for MT on German to English , which has been presented in Burchardt et al . ( 2017 ) and applied extensively in the WMT shared task of 2018 ( Macketanz et al. , 2018 ) and 2019 ( Avramidis et al. , 2019 ) .
The current version includes 5,560 test items in order to control 107 phenomena organised in 14 categories .
Some sample test items can be seen in Table 1 whereas a more detailed list of test sentences with correct and incorrect translations can be found on GitHub 1 .
Application of the test suite
The construction of the test suite has been thoroughly explained in the papers from the previous years ( Avramidis et al. , 2018 ( Avramidis et al. , , 2019 and depicted in Figure 1 ( steps a - c ) .
The test items of the test suite are 1 https://github.com/DFKI-NLP/TQ_
AutoTest given as input to the MT systems ( step d ) .
Their MT outputs are tested using a set of rules ( regular expressions or fixed strings ) , each rule specific for a phenomenon , that defines whether the translations are correct with respect to the tested phenomenon ( step e ) .
When the automatic application of the rules cannot lead to a clear decision on whether the translation is correct or not , the test item is left with a warning .
The warnings are consequently resolved by human annotators with linguistic knowledge , who inspect the MT output , provide a clear judgment and also augment the set of the rules to cover similar cases in the future ( step e ) .
For every system we calculate the phenomenonspecific translation accuracy as the the number of the test sentences for the phenomenon which were translated properly , divided by the number of all test sentences for this phenomenon : accuracy = correct translations sum of test items Each phenomenon is covered by at least 20 test items 2 , whereas the same test items are given to multiple systems to achieve comparisons among them .
In order to achieve a fair comparison among the systems , only the test items that do not contain any warnings for any of the systems are included in the calculation .
In order to define which systems have the best performance for a particular phenomenon , all systems are compared with the system with the highest accuracy .
When comparing the highest scoring system with the rest , the significance of the comparison is confirmed with a one-tailed Z-test with ? = 0.95 .
The systems whose difference with the best system is not significant are considered to be in the first performance cluster and indicated with boldface in the tables .
Experiment setup
In the evaluation presented in the paper , MT outputs are obtained from the 11 systems that are part of the news translation task of the Fifth Conference on Machine Translation ( WMT20 ) .
These are 6 systems submitted by the shared task participants , one baseline system from the shared task of the biomedical domain ( WMTBiomedBaseline ; Bawden et al. , 2020 ) and 4 online commercial systems whose output has been obtained by the workshop organizers and therefore have been anonymized Er las gerne Novellen .
and Zlabs , whereas a 7th system under the name " yolo " was ignored because it contained arbitrary translations .
Unfortunately , contrary to previous years , very few system descriptions were provided by the time this paper was written and it is therefore not possible to associate linguistic performance with system types and settings .
As explained earlier , the application of the test suite on the output of the 11 systems left about 10 % of unresolved warnings which needed to be manually edited .
A human annotator with linguistic background devoted about 45 working hours in order to resolve 99 % of them ( resulting into 5,514 valid out of 5,560 total items ) .
Results
The accuracy of each system per linguistic category is briefly shown in Table 5 whereas the detailed statistics depicting the accuracy for every linguistic phenomenon , grouped in the respective linguistic categories are shown in Table 7 .
Since every category and every phenomenon have a different amount of test items , the average scores , shown in the last rows of the tables , are computed in three different ways :
The first aggregates the contributions of all test items to compute the average percentages ( micro-average ) , the second ( Table 5 ) computes the percentages independently for each category and then takes the average ( hence treating all categories equally ; category macro-average ) and the third ( Table 7 ) computes the percentages independently for each phenomenon and then takes the average ( hence treating all phenomena equally ; phenome-non macro-average ) .
The significantly best systems for every category or phenomenon are bold-faced .
Very high scores do not necessarily mean that the MT of the respective grammatical phenomenon has been solved , but rather that the current test items of the test suite ( which was engineered with the emergence of the first neural MT systems in 2017 ) are unable to expose difficulties of the systems .
The artificial nature of the test suite and the variable number of test items per category and phenomenon should also be taken in consideration when doing comparisons between categories and phenomena .
Comparison between systems
Two systems are standing out for their overall performance .
Tohoku achieves the best category macro-averaged accuracy of 88.1 % , whereas it is sharing the first position with VolcTrans based on their micro-averaged accuracy ( 85.3- 85.4 % ) .
The systems UEdin , Online-B , Online -G and Online - A are next .
Tohoku and VolcTrans are also the best performing systems for all linguistic categories , whereas UEdin is losing in one category and Online - A is losing in two categories .
Two systems , WMTBiomedBaseline and ZLabs show very low performances and are assumed to be non state - of - the - art systems .
We will therefore exclude these two systems from the discussion and conclusions for phenomena and categories .
Whereas no description was available for ZLabs , the lower performance of the WMTBiomedBaseline can be attributed to the fact that it was trained with only 56 % of the parallel training data used by Tohoku and no synthetic data .
Among the rest of the systems , the worst performing one is Online - Z , achieving the lowest accuracy ( 74 % on both micro- and macro- average ) , being on par with the best systems BLEU scores ( Papineni et al. , 2002 ) on the official test-set are also calculated for further comparison .
The order of the systems based on BLEU seems to correlate with the order given by the category macro-average with the exception of one system ( OPPO ) .
Since BLEU scores are calculated on a different test set , further investigation is needed to confirm if this correlation is of any significance .
According to the official human evaluation campaign ( Barrault et al. , 2020 , table 11 ) , the first nine systems are tied , so it is hard to compare this system order with theirs .
Linguistic categories
The average accuracy regarding the linguistic categories ranges in relatively high numbers , between 68.9 % and 97.3 % .
The categories with the highest accuracy in average are the negation ( 97.3 % ) , the composition ( 85.3 % ) , the subordination ( 85.3 % ) and the named entinties and terminology ( 82 % ) .
The ones with the lowest accuracy are the multiword expressions ( MWE ) , the ambiguity , the false friends and the verb valency ( 68.9- 71.5 % ) .
When one tries to identify weaknesses of particular systems , OPPO is suffering mostly concerning function words and long distance dependencies ( LDD ) / interrogatives .
Online -Z and PROMT have issues with ambiguity and several systems have issues with punctuation .
Some of these issues are discussed in a more fine - grained level below .
The comparison of the state - of - the - art systems with the low-resource WMTBiomedBaseline indicates that some categories , such as ambiguity and composition , are particularly sensitive to low resources , as their accuracy is proportionally lower than other categories if compared to the respective category accuracies of the state - of - the - art systems .
Table 2 contains examples from the two low accuracy categories verb valency and false friends .
Verb valency refers to the arguments that are being controlled by the predicate .
Certain verbs require a specific grammatical case .
In our example , the German verb sich erinnern ( to remember ) requires a genitive object , in this case seiner .
Seiner , however , can also mean his as in the possessive pronoun , which explains the mistranslation of I remember his .
False friends are words in different languages that look similar and are therefore often mistaken for being translations of one another , even though their meanings differ .
The German noun Novelle does not translate to novel , but to novella or short story .
While you would expect a human to make these kind of translation errors , it is surprising to see that also MT systems are prone to mistranslating false friends .
Linguistic phenomena
The accuracy regarding individual linguistic phenomena has a wide range , between very low scores ( 15 % ) and full success ( 100 % ) .
The phenomena which all systems had difficulty to handle were the idioms and the resultative predicates , with most systems scoring only 20 % and 26 % respectively .
However , the overall performance on these phenomena has improved : last year only 3 systems could achieve this performance , with the majority of the systems having 5 - 10 % less accuracy .
Modal pluperfect is also ranging very low , scoring between 2.2 % and 50.6 % and similar is the case for its negated version .
Other moods of the pluperfect make it particularly difficult for some systems , e.g. PROMT and Online - Z suffer in translating the ditransitive and intransitive pluperfect .
When trying to find the cases that consist a weakness for particular systems , Online - Z indicates one of the lowest scores in punctuation , which appears to derive from the fact that the system removes all quotation marks .
A similar issue is observed with Online -G and OPPO which could correctly convey almost half of the quotation marks , whereas another two systems have some way to go .
Interestingly enough , despite strongly depending on preprocessing , quotation marks are a common issue , since similar cases have been noted in previous years .
In other phenomena , Online -Z has a very low accuracy for sluicing whereas PROMT is relatively weak concerning lexical ambiguity .
of a verb and an adjective in which the verb describes an action and the adjective describes the result of that action .
In many cases , resultative predicates lead to translation errors as they do not exist in English and a literal translation leads to an ungrammatical translation , as can be seen in the example .
Since none of the systems could produce a correct output for this sentence , we have provided two possible correct translations here as examples .
Intransitive verbs do not require further objects ( as opposed to transitive or ditransitive verbs ) .
Pluperfect is a tense which is used in German to describe completed actions that have taken place in the past .
It should be translated to English in pluperfect as well .
In the example , the incorrect translation contains past progressive were sleeping instead of the correct had slept .
Sluicing is a type of ellipsis that can occur in direct and indirect interrogative clauses .
A wh-word precedes the part of the sentence that contains the ellipsis .
In our example , all constituents following the wh-word are elided :
John mag die Nudeln nicht , aber er wei ?
nicht , warum er die Nudeln nicht mag .
Sluicing exists in both German and in English : John does n't like the noodles , but he does n't know why he does n't like the noodles .
One difference between the German and the English sluicing sentence is that in German there are two commas , while in English there is only one .
Since this phenomenon concerns the complete sentence , punctuation should be correct when translating a sentence containing sluicing .
In our example , the Table 4 : The accuracy ( % ) of the best system of each year as measured over 5,555 test items that were common over the last years .
The scores that are significantly higher in each column are boldfaced missing comma leads to fail .
The fourth example in the Table contains the lexical ambiguity Gericht .
Gericht can either mean court or dish but the context provided in the sentence ( war lecker , English : was delicious ) serves as disambiguation so that only a translation referring to dish ( or to food in some way ) can be a pass .
Any translation referring to court / courthouse / tribunal or the like is a fail .
Comparison with previous years
One can notice some improvements on the overall performance of the best system , as compared with the previous two years .
As seen in Table 4 from 2018 to 2019 there was a 6.4 % improvement on the macro- averaged accuracy but there was no significant improvement from 2019 to 2020 .
The best system of 2019 was not submitted in 2020 and this is unfortunate , as it performed better than this year 's best system in four categories ( mostly regarding ambiguity ; Table 6 ) .
When considering micro-averaged accuracy , there is significant improvement since last year ( 2.1 % ) , but the best system of 2018 is competing with the one of this year , due to its high performance regarding verb tense / mood .
One can also consider the improvement of individual systems submitted to WMT from one year to another , starting from 2018 .
This year , only 5 of the 2019 systems submitted their new version and the yearly difference of their test suite accuracy can be seen in Table 6 .
The accuracy is measured over the test items that are common over all three years ( or at least the last two ) .
All systems indicate considerable improvements on the macro- average since the previous year , ranging between 2,4 % and 8,5 % .
Online - G had a major improvement for a second year in a row ( 21.6 % in total ) , whereas it is the only system that achieved such an improvement without having an accuracy drop for any of the linguistic categories , whereas it improved 6 categories for more than 10 % .
PROMT had improvement in all categories apart from verb tense / aspect / mood , UEdin deteriorated in verb tense / aspect / mood , whereas Online - B deteriorated in composition , named entity / terminology and punctuation .
The linguistic categories that improved mostly in average are the long distance dependencies / interrogatives , the verb valency , the ambiguity and the punctuation .
Conclusions and further work
In this paper we present the results of the application of the DFKI test suite in the output of the stateof - the - art MT systems participating in the Shared Task of the Fifth Conference of Machine Translation ( WMT20 ) .
Based on about 5,500 test items , we present detailed accuracies regarding 107 phenomena organized in 14 categories .
Additionally , the evolution of systems submitted also in previous years is observed .
The best system of this year is not significantly better than the one from 2019 in a macro-average , but one can see significant improvement from two years ago .
The systems that seem to have the best accuracies are Tohoku and VolcanTrans .
The phenomena that most systems face difficulties are again this year the idioms , the resultative predicates and some moods of the pluperfect , whereas some systems still have issues with quotation marks and lexical ambiguity .
As discussed previously , the high accuracies achieved for particular phenomena or categories raise questions on whether these phenomena are getting solved , or whether the test suite ( which was originally built to challenge the systems from 2017 ) should raise the difficulty by including more test items .
In further work , we would like to be able to associate the performance on specific phenomena with decisions related to decisions during the development of the systems , once there is enough information about this process for all systems .
Table 7 : Accuracies ( % ) of successful translations for 11 systems regarding all phenomena , organized in categories .
Boldface indicates the best scoring system in each row , including all systems which are not significantly inferior than the best scoring system .
Grey rows average the accuracies of the phenomena per category .
Figure 1 : 1 Figure 1 : Example of the preparation and application of the test suite for one test sentence
1 . He liked to read novellas .
2 . He liked to read novels .
He liked to read novellas .2 .
He liked to read novels .
3 . He liked to read short stories .
4 . He liked reading novellas .
5 . He liked to read a novel .
a. produce paradigms d. apply regex e. f. ? check b. fetch sample translations 1 . ? 2 . ? 1 . ? 2 . ? c. write regular expressions ? ... 3 . ? 4 . ? 5 . ? ... ? 3 . ? 4 . ? 5 . ? ... regex : ( + ) novellas ( -) novels 1 .
Table 2 : 2 Examples of linguistic categories with lower accuracy with passing and failing MT outputs on only 6 categories .
Verb Valency
Ich erinnere mich seiner .
I remember his .
fail I remember him .
pass False Friends
Er las gerne Novellen .
He liked to read novels .
fail
He liked to read novellas .
pass
Table 3 : 3 Table 3 contains further translation examples from linguistic phenomena with low accuracy .
A resultative predicate is a construction that consists Examples of linguistic phenomena with low accuracy with passing and failing MT outputs Resultative Predicate Sie trinkt die Tasse leer .
She drinks the cup empty .
fail
She empties the cup .
pass
She is drinking the whole cup .
pass Intransitive Pluperfect Sie hatten geschlafen .
They were sleeping .
fail
They had slept .
pass Sluicing John mag die Nudeln nicht , aber er wei ? nicht , warum .
John does n't like the noodles but he does n't know why .
fail John does n't like the noodles , but he does n't know why .
pass Lexical Ambiguity Das Gericht gestern
Abend war lecker .
The court last night was delicious .
fail
The dish last night was delicious .
pass
Table 5 : 5 Franck Burlot , Yves Scherrer , Vinit Ravishankar , Ond?ej Bojar , Stig-Arne Gr?nroos , Maarit Koponen , Tommi Nieminen , and Franc ?ois Yvon . 2018 .
The WMT'18 Morpheval test suites for English - Czech , English - German , English -Finnish and Turkish - English .
In Proceedings of the Third Conference on Machine Translation , pages 550 - 564 , Belgium , Brussels .
Association for Computational Linguistics .
Accuracies ( % ) of successful translations for 11 systems and 14 categories .
Boldface indicates the significantly best performing systems in each row evaluation of German-English Machine Translation based on a Test Suite .
In Proceedings of the Third Conference on Machine Translation ( WMT18 ) , Brus - sels , Belgium .
Association for Computational Lin- guistics .
Alexander Molchanov. 2020 .
PROMT Systems for WMT 2020 Shared News Translation
Task .
In Pro- ceedings of the Fifth Conference on Machine Trans - lation , pages 247 - 252 , Online .
Association for Com- Silvie Cinkova and Ond?ej Bojar . 2018 .
Testsuite on putational Linguistics .
Czech -English Grammatical Contrasts .
In Procee-dings of the Third Conference on Machine Translati-on , pages 565 - 575 , Belgium , Brussels .
Association for Computational Linguistics .
Mathias M?ller , Annette Rios , Elena Voita , and Rico Sennrich .
2018 .
A large-scale test set for the evalua-tion of context - aware pronoun translation in neural machine translation .
In Proceedings of the Third Ulrich Germann .
2020 .
The University of Edinburgh 's submission to the German-to-English and English-to - German Tracks in the WMT 2020 News Trans - Conference on Machine Translation : Research Pa-pers , pages 61 - 72 , Brussels , Belgium .
Association for Computational Linguistics .
lation and Zero-shot Translation Robustness Tasks .
In Proceedings of the Fifth Conference on Machine Translation , pages 196-200 , Online .
Association for Computational Linguistics .
Kishore Papineni , Salim Roukos , Todd Ward , and Wei-Jing Zhu. 2002 .
Bleu : a method for automatic eva-luation of machine translation .
In Proceedings of the 40th Annual Meeting of the Association for Com- Liane Guillou and Christian Hardmeier .
2016 .
PRO-TEST : A Test Suite for Evaluating Pronouns in Ma-chine Translation .
Tenth International Conference putational Linguistics , pages 311-318 , Philadelphia , Pennsylvania , USA .
Association for Computational Linguistics .
on Language Resources and Evaluation ( LREC 2016 ) .
Marco Tulio Ribeiro , Tongshuang Wu , Carlos Guestrin , and Sameer Singh .
2020 .
Beyond accuracy : Behavi - Liane Guillou , Christian Hardmeier , Ekaterina Lapshinova-Koltunski , and Sharid Lo?iciga. 2018 .
A Pronoun Test Suite Evaluation of the English - German MT Systems at WMT 2018 .
In Proceedings oral testing of NLP models with CheckList .
In Pro-ceedings of the 58th Annual Meeting of the Associati-on for Computational Linguistics , pages 4902 - 4912 , Online .
Association for Computational Linguistics .
of the Third Conference on Machine Translation , Annette Rios , Mathias M?ller , and Rico Sennrich .
pages 576- 583 , Belgium , Brussels .
Association for 2018 .
The Word Sense Disambiguation Test Sui- Computational Linguistics .
te at WMT18 .
In Proceedings of the Third Confe- Ulrich Heid and Elke Hildenbrand .
1991 .
Some prac-tical experience with the use of test suites for the evaluation of SYSTRAN .
In the Proceedings of the rence on Machine Translation , pages 594-602 , Bel-gium , Brussels .
Association for Computational Lin-guistics .
Evaluators ' Forum , Les Rasses .
Citeseer .
Tingxun Shi , Shiyu Zhao , Xiaopu Li , Xiaoxue Wang , Pierre Isabelle , Colin Cherry , and George Foster . 2017 .
A Challenge Set Approach to Evaluating Machine Translation .
In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Proces-sing , pages 2486- 2496 , Copenhagen , Denmark .
As - Qian Zhang , Di Ai , Dawei Dang , Xue Zhengshan , and JIE HAO . 2020 .
OPPO 's machine translation systems for WMT20 .
In Proceedings of the Fifth Conference on Machine Translation , pages 281- 291 , Online .
Association for Computational Linguistics .
sociation for Computational Linguistics .
Gabriel Stanovsky , Noah A. Smith , and Luke Zettle- Margaret King and Kirsten Falkedal .
1990 .
Using test suites in evaluation of machine translation systems .
In Proceedings of the 13th conference on Computa-tional Linguistics , volume 2 , pages 211 - 216 , Morri-stown , NJ , USA .
Association for Computational Lin- moyer .
2019 .
Evaluating gender bias in machine translation .
In Proceedings of the 57th Annual Mee-ting of the Association for Computational Lingui-stics , pages 1679 - 1684 , Florence , Italy .
Association for Computational Linguistics .
guistics .
Andrew Way .
1991 .
Developer -Oriented Evaluation of MT Systems .
In Proceedings of the Evaluators ' Fo- Shun Kiyono , Takumi Ito , Ryuto Konno , Makoto Mo - rum , pages 237- 244 , Les Rasses , Vaud , Switzerland .
rishita , and Jun Suzuki .
2020 .
Tohoku-aip - ntt at ISSCO .
wmt 2020 news translation task .
In Proceedings of the Fifth Conference on Machine Translation , pa- ges 144 - 154 , Online .
Association for Computatio- nal Linguistics .
Vivien Macketanz , Eleftherios Avramidis , Aljoscha Burchardt , and Hans Uszkoreit . 2018 .
Fine-grained Liwei Wu , Xiao Pan , Zehui Lin , YaomingZHU , Mingxuan Wang , and Lei Li. 2020 .
The Volctrans Machine Translation System for WMT20 .
In Proceedings of the Fifth Conference on Machine Translation , pages 304- 310 , Online .
Association for Computational Linguistics .
Table 6 : 6 Difference of the test suite accuracy from one year to the next one per category , for the systems participating in the shared tasks WMT18 - 20 , measured over the test items that are common over all these years .
avg items Tohoku VolcTrans UEdin Onl-B Onl-G Onl-A PROMT OPPO Onl -Z ZLabs WMTBi 81 82.7 77.8 72.8 79.0 84.0 76.5 64.2 82.7 45.7 30.9 69.5 67.9 63 85.7 79.4 77.8 77.8 87.3 79.4 65.1 85.7 49.2 36.5 72.0 68.3 18 72.2 72.2 55.6 83.3 72.2 66.7 61.1 72.2 33.3 11.1 60.6 66.7 49 98.0 98.0 93.9 93.9 95.9 93.9 89.8 95.9 49.0 44.9 85.3 85.7 29 96.6 96.6 89.7 93.1 93.1 89.7 86.2 96.6 82.8 phenomenon Ambiguity Lexical ambiguity Structural ambiguity Composition Compound
with the exception of 7 phenomena which have 9 - 19 items
