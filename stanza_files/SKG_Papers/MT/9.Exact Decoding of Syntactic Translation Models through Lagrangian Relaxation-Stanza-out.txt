title
Exact Decoding of Syntactic Translation Models through Lagrangian Relaxation
abstract
We describe an exact decoding algorithm for syntax - based statistical translation .
The approach uses Lagrangian relaxation to decompose the decoding problem into tractable subproblems , thereby avoiding exhaustive dynamic programming .
The method recovers exact solutions , with certificates of optimality , on over 97 % of test examples ; it has comparable speed to state - of - the - art decoders .
Introduction Recent work has seen widespread use of synchronous probabilistic grammars in statistical machine translation ( SMT ) .
The decoding problem for a broad range of these systems ( e.g. , ( Chiang , 2005 ; Marcu et al. , 2006 ; Shen et al. , 2008 ) ) corresponds to the intersection of a ( weighted ) hypergraph with an n-gram language model .
1
The hypergraph represents a large set of possible translations , and is created by applying a synchronous grammar to the source language string .
The language model is then used to rescore the translations in the hypergraph .
Decoding with these models is challenging , largely because of the cost of integrating an n-gram language model into the search process .
Exact dynamic programming algorithms for the problem are well known ( Bar - Hillel et al. , 1964 ) , but are too expensive to be used in practice .
2 Previous work on decoding for syntax - based SMT has therefore been focused primarily on approximate search methods .
This paper describes an efficient algorithm for exact decoding of synchronous grammar models for translation .
We avoid the construction of ( Bar-Hillel 1
This problem is also relevant to other areas of statistical NLP , for example NL generation ( Langkilde , 2000 ) . 2 E.g. , with a trigram language model they run in O ( | E|w 6 ) time , where | E | is the number of edges in the hypergraph , and w is the number of distinct lexical items in the hypergraph .
et al. , 1964 ) by using Lagrangian relaxation to decompose the decoding problem into the following sub-problems :
1 . Dynamic programming over the weighted hypergraph .
This step does not require language model integration , and hence is highly efficient .
2 . Application of an all- pairs shortest path algorithm to a directed graph derived from the weighted hypergraph .
The size of the derived directed graph is linear in the size of the hypergraph , hence this step is again efficient .
Informally , the first decoding algorithm incorporates the weights and hard constraints on translations from the synchronous grammar , while the second decoding algorithm is used to integrate language model scores .
Lagrange multipliers are used to enforce agreement between the structures produced by the two decoding algorithms .
In this paper we first give background on hypergraphs and the decoding problem .
We then describe our decoding algorithm .
The algorithm uses a subgradient method to minimize a dual function .
The dual corresponds to a particular linear programming ( LP ) relaxation of the original decoding problem .
The method will recover an exact solution , with a certificate of optimality , if the underlying LP relaxation has an integral solution .
In some cases , however , the underlying LP will have a fractional solution , in which case the method will not be exact .
The second technical contribution of this paper is to describe a method that iteratively tightens the underlying LP relaxation until an exact solution is produced .
We do this by gradually introducing constraints to step 1 ( dynamic programming over the hypergraph ) , while still maintaining efficiency .
We report experiments using the tree-to-string model of ( Huang and Mi , 2010 ) .
Our method gives exact solutions on over 97 % of test examples .
The method is comparable in speed to state - of - the - art decoding algorithms ; for example , over 70 % of the test examples are decoded in 2 seconds or less .
We compare our method to cube pruning ( Chiang , 2007 ) , and find that our method gives improved model scores on a significant number of examples .
One consequence of our work is that we give accurate estimates of the number of search errors for cube pruning .
Related Work
A variety of approximate decoding algorithms have been explored for syntax - based translation systems , including cube-pruning ( Chiang , 2007 ; Huang and Chiang , 2007 ) , left-to- right decoding with beam search ( Watanabe et al. , 2006 ; Huang and Mi , 2010 ) , and coarse- to-fine methods ( Petrov et al. , 2008 ) .
Recent work has developed decoding algorithms based on finite state transducers ( FSTs ) .
Iglesias et al. ( 2009 ) show that exact FST decoding is feasible for a phrase - based system with limited reordering ( the MJ1 model ( Kumar and Byrne , 2005 ) ) , and de Gispert et al . ( 2010 ) show that exact FST decoding is feasible for a specific class of hierarchical grammars ( shallow - 1 grammars ) .
Approximate search methods are used for more complex reordering models or grammars .
The FST algorithms are shown to produce higher scoring solutions than cube-pruning on a large proportion of examples .
Lagrangian relaxation is a classical technique in combinatorial optimization ( Korte and Vygen , 2008 ) .
Lagrange multipliers are used to add linear constraints to an existing problem that can be solved using a combinatorial algorithm ; the resulting dual function is then minimized , for example using subgradient methods .
In recent work , dual decomposition - a special case of Lagrangian relaxation , where the linear constraints enforce agreement between two or more models - has been applied to inference in Markov random fields ( Wainwright et al. , 2005 ; Komodakis et al. , 2007 ; Sontag et al. , 2008 ) , and also to inference problems in NLP Koo et al. , 2010 ) .
There are close connections between dual decomposition and work on belief propagation ( Smith and Eisner , 2008 ) .
Background : Hypergraphs Translation with many syntax - based systems ( e.g. , ( Chiang , 2005 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Huang and Mi , 2010 ) ) can be implemented as a two-step process .
The first step is to take an input sentence in the source language , and from this to create a hypergraph ( sometimes called a translation forest ) that represents the set of possible translations ( strings in the target language ) and derivations under the grammar .
The second step is to integrate an n-gram language model with this hypergraph .
For example , in the system of ( Chiang , 2005 ) , the hypergraph is created as follows : first , the source side of the synchronous grammar is used to create a parse forest over the source language string .
Second , transduction operations derived from synchronous rules in the grammar are used to create the target - language hypergraph .
Chiang 's method uses a synchronous context-free grammar , but the hypergraph formalism is applicable to a broad range of other grammatical formalisms , for example dependency grammars ( e.g. , ( Shen et al. , 2008 ) ) .
A hypergraph is a pair ( V , E ) where V = { 1 , 2 , . . . , | V |} is a set of vertices , and E is a set of hyperedges .
A single distinguished vertex is taken as the root of the hypergraph ; without loss of generality we take this vertex to be v = 1 .
Each hyperedge e ?
E is a tuple v 1 , v 2 , . . . , v k , v 0 where v 0 ? V , and v i ?
{ 2 . . . | V |} for i = 1 . . . k.
The vertex v 0 is referred to as the head of the edge .
The ordered sequence v 1 , v 2 , . . . , v k is referred to as the tail of the edge ; in addition , we sometimes refer to v 1 , v 2 , . . . v k as the children in the edge .
The number of children k may vary across different edges , but k ?
1 for all edges ( i.e. , each edge has at least one child ) .
We will use h( e ) to refer to the head of an edge e , and t( e ) to refer to the tail .
We will assume that the hypergraph is acyclic : intuitively this will mean that no derivation ( as defined below ) contains the same vertex more than once ( see ( Martin et al. , 1990 ) for a formal definition ) .
Each vertex v ?
V is either a non-terminal in the hypergraph , or a leaf .
The set of non-terminals is V N = {v ? V : ?e ? E such that h( e ) = v} Conversely , the set of leaves is defined as V L = { v ? V : ?e ? E such that h( e ) = v} Finally , we assume that each v ?
V has a label l ( v ) .
The labels for leaves will be words , and will be important in defining strings and language model scores for those strings .
The labels for non-terminal nodes will not be important for results in this paper .
3
We now turn to derivations .
Define an index set I = V ? E . A derivation is represented by a vector y = {y r : r ?
I } where y v = 1 if vertex v is used in the derivation , y v = 0 otherwise ( similarly y e = 1 if edge e is used in the derivation , y e = 0 otherwise ) .
Thus y is a vector in { 0 , 1 } | I| .
A valid derivation satisfies the following constraints : ? y 1 = 1 ( the root must be in the derivation ) .
?
For all v ?
V N , y v = e:h( e ) =v y e . ?
For all v ?
2 . . . | V | , y v = e:v?t ( e ) y e .
We use Y to refer to the set of valid derivations .
The set Y is a subset of { 0 , 1 } | I | ( not all members of { 0 , 1 } | I | will correspond to valid derivations ) .
Each derivation y in the hypergraph will imply an ordered sequence of leaves v 1 . . . v n .
We use s( y ) to refer to this sequence .
The sentence associated with the derivation is then l( v 1 ) . . . l( v n ) .
In a weighted hypergraph problem , we assume a parameter vector ? = {? r : r ? I} .
The score for any derivation is f ( y ) = ? ? y = r?I ? r y r .
Simple bottom - up dynamic programming -essentially the CKY algorithm - can be used to find y * = arg max y?Y f ( y ) under these definitions .
The focus of this paper will be to solve problems involving the integration of a k'th order language model with a hypergraph .
In these problems , the score for a derivation is modified to be f ( y ) = r?I ? r y r + n i=k ?( v i?k+1 , v i?k+ 2 , . . . , v i ) ( 1 ) where v 1 . . . v n = s( y ) .
The ?( v i?k+1 , . . . , v i ) parameters score n-grams of length k .
These parameters are typically defined by a language model , for example with k = 3 we would have ?( v i?2 , v i?1 , v i ) = log p( l ( v i ) |l ( v i?2 ) , l( v i?1 ) ) .
The problem is then to find y * = arg max y?Y f ( y ) under this definition .
Throughout this paper we make the following assumption when using a bigram language model : Assumption 3.1 ( Bigram start / end assump - tion . )
For any derivation y , with leaves s( y ) = v 1 , v 2 , . . . , v n , it is the case that : ( 1 ) v 1 = 2 and v n = 3 ; ( 2 ) the leaves 2 and 3 cannot appear at any other position in the strings s ( y ) for y ? Y ; ( 3 ) l ( 2 ) = < s> where < s > is the start symbol in the language model ; ( 4 ) l ( 3 ) = </s> where </s> is the end symbol .
This assumption allows us to incorporate language model terms that depend on the start and end symbols .
It also allows a clean solution for boundary conditions ( the start / end of strings ) .
4
A Simple Lagrangian Relaxation Algorithm
We now give a Lagrangian relaxation algorithm for integration of a hypergraph with a bigram language model , in cases where the hypergraph satisfies the following simplifying assumption : Assumption 4.1 ( The strict ordering assumption . )
For any two leaves v and w , it is either the case that : 1 ) for all derivations y such that v and w are both in the sequence l( y ) , v precedes w ; or 2 ) for all derivations y such that v and w are both in l( y ) , w precedes v .
Thus under this assumption , the relative ordering of any two leaves is fixed .
This assumption is overly restrictive : 5 the next section describes an algorithm that does not require this assumption .
However deriving the simple algorithm will be useful in developing intuition , and will lead directly to the algorithm for the unrestricted case .
A Sketch of the Algorithm
At a high level , the algorithm is as follows .
We introduce Lagrange multipliers u( v ) for all v ?
V L , with initial values set to zero .
The algorithm then involves the following steps : ( 1 ) For each leaf v , find the previous leaf w that maximizes the score ?( w , v ) ? u( w ) ( call this leaf ? * ( v ) , and define ? v = ?(? * ( v ) , v ) ? u(? * ( v ) ) ) . ( 2 ) find the highest scoring derivation using dynamic programming over the original ( non - intersected ) hypergraph , with leaf nodes having weights ? v + ? v + u( v ) . ( 3 )
If the output derivation from step 2 has the same set of bigrams as those from step 1 , then we have an exact solution to the problem .
Otherwise , the Lagrange multipliers u( v ) are modified in a way that encourages agreement of the two steps , and we return to step 1 .
Steps 1 and 2 can be performed efficiently ; in particular , we avoid the classical dynamic programming intersection , instead relying on dynamic programming over the original , simple hypergraph .
A Formal Description
We now give a formal description of the algorithm .
Define B ? V L ?V
L to be the set of all ordered pairs v , w such that there is at least one derivation y with v directly preceding w in s(y ) .
Extend the bit-vector y to include variables y(v , w ) for v , w ?
B where y(v , w ) = 1 if leaf v is followed by w in s( y ) , 0 otherwise .
We redefine the index set to be I = V ? E ? B , and define Y ? { 0 , 1 } | I | to be the set of all possible derivations .
Under assumptions 3.1 and 4.1 above , Y = {y : y satisfies constraints C0 , C1 , C2 } where the constraint definitions are : ? ( C0 )
The y v and y e variables form a derivation in the hypergraph , as defined in section 3 . ? ( C1 ) For all v ?
V L such that v = 2 , y v = w : w, v ?B y(w , v ) . ? ( C2 ) For all v ?
V L such that v = 3 , y v = w : v,w ?B y(v , w ) .
C1 states that each leaf in a derivation has exactly one in- coming bigram , and that each leaf not in the derivation has 0 incoming bigrams ; C2 states that each leaf in a derivation has exactly one out -going bigram , and that each leaf not in the derivation has 0 outgoing bigrams .
6
The score of a derivation is now f ( y ) = ? ? y , i.e. , f ( y ) = v ? v y v + e ? e y e + v ,w ?B ?( v , w ) y ( v , w ) where ?( v , w ) are scores from the language model .
Our goal is to compute y * = arg max y?Y f ( y ) .
6 Recall that according to the bigram start / end assumption the leaves 2/3 are reserved for the start / end of the sequence s( y ) , and hence do not have an incoming / outgoing bigram .
Initialization : Set u 0 ( v) = 0 for all v ?
V L Algorithm : For t = 1 . . . T : ? y t = arg max y?Y ? L( u t?1 , y ) ?
If y t satisfies constraints C2 , return y t , Else ?v ? V L , u t ( v ) = u t?1 ( v ) ? ? t y t ( v ) ? w : v,w ?B y t ( v , w ) . Figure 1 : A simple Lagrangian relaxation algorithm .
? t > 0 is the step size at iteration t. Next , define Y ? as Y ? = {y : y satisfies constraints C0 and C1 }
In this definition we have dropped the C2 constraints .
To incorporate these constraints , we use Lagrangian relaxation , with one Lagrange multiplier u( v ) for each constraint in C2 .
The Lagrangian is L ( u , y ) = f ( y ) + v u( v ) ( y ( v ) ? w : v,w ?B y(v , w ) ) = ? ? y where ? v = ? v + u( v ) , ? e = ? e , and ?( v , w ) = ?( v , w ) ? u( v ) .
The dual problem is to find min u L ( u ) where L ( u ) = max y?Y ? L( u , y )
Figure 1 shows a subgradient method for solving this problem .
At each point the algorithm finds y t = arg max y?Y ? L( u t?1 , y ) , where u t?1 are the Lagrange multipliers from the previous iteration .
If y t satisfies the C2 constraints in addition to C0 and C1 , then it is returned as the output from the algorithm .
Otherwise , the multipliers u( v ) are updated .
Intuitively , these updates encourage the values of y v and w : v,w ?B y(v , w ) to be equal ; formally , these updates correspond to subgradient steps .
The main computational step at each iteration is to compute arg max y?Y ? L( u t?1 , y )
This step is easily solved , as follows ( we again use ?
v , ?
e and ?( v 1 , v 2 ) to refer to the parameter values that incorporate Lagrange multipliers ) : ?
For all v ?
V L , define ? * ( v) = arg max w : w, v ?B ?( w , v ) and ? v = ?(? * ( v ) , v ) .
For all v ?
V N define ? v = 0 . ?
Using dynamic programming , find values for the y v and y e variables that form a valid derivation , and that maximize f ? ( y ) = v ( ?
v + ? v ) y v + e ? e y e . ?
Set y( v , w ) = 1 iff y( w ) = 1 and ? * ( w ) = v.
The critical point here is that through our definition of Y ? , which ignores the C2 constraints , we are able to do efficient search as just described .
In the first step we compute the highest scoring incoming bigram for each leaf v .
In the second step we use conventional dynamic programming over the hypergraph to find an optimal derivation that incorporates weights from the first step .
Finally , we fill in the y(v , w ) values .
Each iteration of the algorithm runs in O ( |E | + | B | ) time .
There are close connections between Lagrangian relaxation and linear programming relaxations .
The most important formal results are : 1 ) for any value of u , L ( u ) ? f ( y * ) ( hence the dual value provides an upper bound on the optimal primal value ) ; 2 ) under an appropriate choice of the step sizes ?
t , the subgradient algorithm is guaranteed to converge to the minimum of L ( u ) ( i.e. , we will minimize the upper bound , making it as tight as possible ) ; 3 ) if at any point the algorithm in figure 1 finds a y t that satisfies the C2 constraints , then this is guaranteed to be the optimal primal solution .
Unfortunately , this algorithm may fail to produce a good solution for hypergraphs where the strict ordering constraint does not hold .
In this case it is possible to find derivations y that satisfy constraints C0 , C1 , C2 , but which are invalid .
As one example , consider a derivation with s(y ) = 2 , 4 , 5 , 3 and y( 2 , 3 ) = y( 4 , 5 ) = y( 5 , 4 ) = 1 .
The constraints are all satisfied in this case , but the bigram variables are invalid ( e.g. , they contain a cycle ) .
The Full Algorithm
We now describe our full algorithm , which does not require the strict ordering constraint .
In addition , the full algorithm allows a trigram language model .
We first give a sketch , and then give a formal definition .
A Sketch of the Algorithm A crucial idea in the new algorithm is that of paths between leaves in hypergraph derivations .
Previously , for each derivation y , we had defined s( y ) = v 1 , v 2 , . . . , v n to be the sequence of leaves in y .
In addition , we will define g( y ) = p 0 , v 1 , p 1 , v 2 , p 2 , v 3 , p 3 , . . . , p n?1 , v n , p n where each p i is a path in the derivation between leaves v i and v i + 1 .
The path traces through the nonterminals that are between the two leaves in the tree .
As an example , consider the following derivation ( with hyperedges 2 , 5 , 1 and 3 , 4 , 2 ) : 1 2 3 4 5 For this example g( y ) is 1 ? , 2 ? 2 ? , 3 ? 3 ? , 3 , 3 ? 3 ? , 4 ? 4 ? , 4 , 4 ? 4 ? , 2 ? 2 ? , 5 ? 5 ? , 5 , 5 ? 5 ? , 1 ? . States of the form a ? and a ?
where a is a leaf appear in the paths respectively before / after the leaf a .
States of the form a , b correspond to the steps taken in a top-down , left- to - right , traversal of the tree , where down and up arrows indicate whether a node is being visited for the first or second time ( the traversal in this case would be 1 , 2 , 3 , 4 , 2 , 5 , 1 ) .
The mapping from a derivation y to a path g ( y ) can be performed using the algorithm in figure 2 . For a given derivation y , define E( y ) = {y : y e = 1 } , and use E(y ) as the set of input edges to this algorithm .
The output from the algorithm will be a set of states S , and a set of directed edges T , which together fully define the path g ( y ) .
In the simple algorithm , the first step was to predict the previous leaf for each leaf v , under a score that combined a language model score with a Lagrange multiplier score ( i.e. , compute arg max w ?( w , v ) where ?( w , v ) = ?( w , v ) + u( w ) ) .
In this section we describe an algorithm that for each leaf v again predicts the previous leaf , but in addition predicts the full path back to that leaf .
For example , rather than making a prediction for leaf 5 that it should be preceded by leaf 4 , we would also predict the path 4 ? 4 ? , 2 ? 2 ? , 5 ? 5 ? between these two leaves .
Lagrange multipliers will be used to enforce consistency between these predictions ( both paths and previous words ) and a valid derivation .
Input : A set E of hyperedges .
Output : A directed graph S , T where S is a set of vertices , and T is a set of edges .
Step 1 : Creating S : Define S = ? e?E S( e ) where S( e ) is defined as follows .
Assume e = v 1 , v 2 , . . . , v k , v 0 . Include the following states in S ( e ) : ( 1 ) v 0 ? , v 1 ? and v k ? , v 0 ? . ( 2 ) v j ? , v j+1 ? for j = 1 . . . k ?
1 ( if k = 1 then there are no such states ) .
( 3 ) In addition , for any v j for j = 1 . . . k such that v j ?
V L , add the states v j ? and v j ? . Step 2 : Creating T : T is formed by including the following directed arcs : ( 1
A Formal Description
We first use the algorithm in figure 2 with the entire set of hyperedges , E , as its input .
The result is a directed graph ( S , T ) that contains all possible paths for valid derivations in V , E ( it also contains additional , ill-formed paths ) .
We then introduce the following definition : Definition 5.1
A trigram path p is p = v 1 , p 1 , v 2 , p 2 , v 3 where : a ) v 1 , v 2 , v 3 ? V L ; b) p 1 is a path ( sequence of states ) between nodes v 1 ? and v 2 ? in the graph ( S , T ) ; c) p 2 is a path between nodes v 2 ? and v 3 ? in the graph ( S , T ) .
We define P to be the set of all trigram paths in ( S , T ) .
The set P of trigram paths plays an analogous role to the set B of bigrams in our previous algorithm .
We use v 1 ( p ) , p 1 ( p ) , v 2 ( p ) , p 2 ( p ) , v 3 ( p ) to refer to the individual components of a path p .
In addition , define S N to be the set of states in S of the form a , b ( as opposed to the form c ? or c ? where c ? V L ) .
We now define a new index set , I = V ? E ? S N ? P , adding variables y s for s ?
S N , and y p for p ?
P . If we take Y ? { 0 , 1 } | I | to be the set of valid derivations , the optimization problem is to find y * = arg max y?Y f ( y ) , where f ( y ) = ? ? y , that is , f ( y ) = v ?
v y v + e ? e y e + s ? s y s + p ?
p y p
In particular , we might define ?
s = 0 for all s , and ? p = log p ( l ( v 3 ( p ) ) |l( v 1 ( p ) ) , l( v 2 ( p ) ) ) where ? D0 .
The y v and y e variables form a valid derivation in the original hypergraph .
? D1 .
For all s ?
S N , y s = e:s?S ( e ) y e ( see figure 2 for the definition of S ( e ) ) .
? D2 .
For all v ?
V L , y v = p:v3 ( p ) =v y p ? D3 .
For all v ?
V L , y v = p:v2 ( p ) =v y p ? D4 .
For all v ?
V L , y v = p:v1 ( p ) =v L(y , ? , ? , u , v ) = ? ? y + v ?
v y v ? p:v2 ( p ) =v y p + v ? v y v ? p:v1 ( p ) =v y p + s u s y s ? p:s?p1 ( p ) y p + s v s y s ? p:s?p2 ( p ) y p . Figure 3 : Constraints D0 - D6 , and the Lagrangian .
p( w 3 |w 1 , w 2 ) is a trigram probability .
The set P is large ( typically exponential in size ) : however , we will see that we do not need to represent the y p variables explicitly .
Instead we will be able to leverage the underlying structure of a path as a sequence of states .
The set of valid derivations is Y = {y : y satisfies constraints D0 - D6 } where the constraints are shown in figure 3 . D1 simply states that y s = 1 iff there is exactly one edge e in the derivation such that s ? S ( e ) .
Constraints D2 - D4 enforce consistency between leaves in the trigram paths , and the y v values .
Constraints D5 and D6 enforce consistency between states seen in the paths , and the y s values .
The Lagrangian relaxation algorithm is then derived in a similar way to before .
Define Y ? = {y : y satisfies constraints D0 - D2 }
We have dropped the D3 - D6 constraints , but these will be introduced using Lagrange multipliers .
The resulting Lagrangian is shown in figure 3 , and can be written as L(y , ? , ? , u , v ) = ? ? y where ( y , ? , ? , u , v ) ; figure 4 shows a subgradient method that minimizes this dual .
The key step in the algorithm at each iteration is to compute Initialization : Set ?
0 = 0 , ? 0 = 0 , u 0 = 0 , v 0 = 0 Algorithm : For t = 1 . . . T : ? v = ? v +? v +?
v , ? s = ? s +u s +v s , ? p = ? p ?( v 2 ( p ) ) ? ?( v 1 ( p ) ) ? s?p 1 ( p ) u( s ) ? s?p 2 ( p ) v( s ) .
The dual is L ( ? , ? , u , v ) = max y?Y ?
L ? y t = arg max y?Y ? L(y , ? t?1 , ? t?1 , u t?1 , v t?1 ) ?
If y t satisfies the constraints D3 - D6 , return y t , else : -?v ? V L , ? t v = ? t?1 v ? ? t ( y t v ? p:v2 ( p ) =v y t p ) -?v ? V L , ? t v = ? t?1 v ? ? t ( y t v ? p:v1 ( p ) =v y t p ) -?s ?
S N , u t s = u t?1 s ? ? t ( y t s ? p:s?p1 ( p ) y t p ) -?s ?
S N , v t s = v t?1 s ? ? t ( y t s ? p:s?p2 ( p ) y t p ) Figure 4 : The full Lagrangian relaxation algortihm .
? t > 0 is the step size at iteration t. arg max y?Y ? L(y , ? , ? , u , v ) = arg max y?Y ? ? ? y where ? is defined above .
Again , our definition of Y ? allows this maximization to be performed efficiently , as follows : 1 . For each v ?
V L , define * v = arg max p:v 3 ( p ) =v ?( p ) , and ? v = ?(? * v ) . ( i.e. , for each v , compute the highest scoring trigram path ending in v. )
2 . Find values for the y v , y e and y s variables that form a valid derivation , and that maximize f ? ( y ) = v ( ?
v + ? v ) y v + e ? e y e + s ? s y s 3 . Set y p = 1 iff y v 3 ( p ) = 1 and p = ? * v 3 ( p ) .
The first step involves finding the highest scoring incoming trigram path for each leaf v .
This step can be performed efficiently using the Floyd - Warshall allpairs shortest path algorithm ( Floyd , 1962 ) over the graph ( S , T ) ; the details are given in the appendix .
The second step involves simple dynamic programming over the hypergraph ( V , E ) ( it is simple to integrate the ? s terms into this algorithm ) .
In the third step , the path variables y p are filled in .
Properties
We now describe some important properties of the algorithm : Efficiency .
The main steps of the algorithm are : 1 ) construction of the graph ( S , T ) ; 2 ) at each iteration , dynamic programming over the hypergraph ( V , E ) ; 3 ) at each iteration , all - pairs shortest path algorithms over the graph ( S , T ) .
Each of these steps is vastly more efficient than computing an exact intersection of the hypergraph with a language model .
Exact solutions .
By usual guarantees for Lagrangian relaxation , if at any point the algorithm returns a solution y t that satisfies constraints D3 - D6 , then y t exactly solves the problem in Eq. 1 .
Upper bounds .
At each point in the algorithm , L ( ?
t , ?
t , u t , v t ) is an upper bound on the score of the optimal primal solution , f ( y * ) .
Upper bounds can be useful in evaluating the quality of primal solutions from either our algorithm or other methods such as cube pruning .
Simplicity of implementation .
Construction of the ( S , T ) graph is straightforward .
The other steps-hypergraph dynamic programming , and allpairs shortest path - are widely known algorithms that are simple to implement .
Tightening the Relaxation
The algorithm that we have described minimizes the dual function L ( ? , ? , u , v ) .
By usual results for Lagrangian relaxation ( e.g. , see ( Korte and Vygen , 2008 ) ) , L is the dual function for a particular LP relaxation arising from the definition of Y ? and the additional constaints D3 - D6 .
In some cases the LP relaxation has an integral solution , in which case the algorithm will return an optimal solution y t .
7
In other cases , when the LP relaxation has a fractional solution , the subgradient algorithm will still converge to the minimum of L , but the primal solutions y t will move between a number of solutions .
We now describe a method that incrementally adds hard constraints to the set Y ? , until the method returns an exact solution .
For a given y ?
Y ? , for any v with y v = 1 , we can recover the previous two leaves ( the trigram ending in v ) from either the path variables y p , or the hypergraph variables y e .
Specifically , define v ?1 ( v , y ) to be the leaf preceding v in the trigram path p with y p = 1 and v 3 ( p ) = v , and v ?2 ( v , y ) to be the leaf two positions before v in the trigram path p with y p = 1 and v 3 ( p ) = v. Similarly , define v ? ?1 ( v , y ) and v ? ?2 ( v , y ) to be the preceding two leaves under the y e variables .
If the method has not converged , these two trigram definitions may not be consistent .
For a con-sistent solution , we require v ?1 ( v , y ) = v ? ?1 ( v , y ) and v ?2 ( v , y ) = v ? ?2 ( v , y ) for all v with y v = 1 .
Unfortunately , explicitly enforcing all of these constraints would require exhaustive dynamic programming over the hypergraph using the ( Bar - Hillel et al. , 1964 ) method , something we wish to avoid .
Instead , we enforce a weaker set of constraints , which require far less computation .
Assume some function ? : V L ? { 1 , 2 , . . . q} that partitions the set of leaves into q different partitions .
Then we will add the following constraints to Y ? : y ) ) for all v such that y v = 1 .
Finding arg max y?Y ? ? ? y under this new definition of Y ? can be performed using the construction of ( Bar -Hillel et al. , 1964 ) , with q different lexical items ( for brevity we omit the details ) .
This is efficient if q is small .
8
The remaining question concerns how to choose a partition ? that is effective in tightening the relaxation .
To do this we implement the following steps : 1 ) run the subgradient algorithm until L is close to convergence ; 2 ) then run the subgradient algorithm for m further iterations , keeping track of all pairs of leaf nodes that violate the constraints ( i.e. , pairs ( v , y ) such that a = b ) ; 3 ) use a graph coloring algorithm to find a small partition that places all pairs a , b into separate partitions ; 4 ) continue running Lagrangian relaxation , with the new constraints added .
We expand ? at each iteration to take into account new pairs a , b that violate the constraints .
?( v ?1 ( v , y ) ) = ?( v ? ?1 ( v , y ) ) ?( v ?2 ( v , y ) ) = ?( v ? ?2 ( v , a = v ?1 ( v , y ) / b = v ? ?1 ( v , y ) or a = v ?2 ( v , y ) / b = v ? ?2
In related work , Sontag et al . ( 2008 ) describe a method for inference in Markov random fields where additional constraints are chosen to tighten an underlying relaxation .
Other relevant work in NLP includes ( Tromble and Eisner , 2006 ; Riedel and Clarke , 2006 ) .
Our use of partitions ? is related to previous work on coarse- to - fine inference for machine translation ( Petrov et al. , 2008 ) .
Experiments
We report experiments on translation from Chinese to English , using the tree-to-string model described
We ran the full algorithm with the tightening method described in section 6 .
We ran the method for a limit of 200 iterations , hence some examples may not terminate with an exact solution .
Our method gives exact solutions on 598/616 development set sentences ( 97.1 % ) , and 675/691 test set sentences ( 97.7 % ) .
In cases where the method does not converge within 200 iterations , we can return the best primal solution y t found by the algorithm during those iterations .
We can also get an upper bound on the difference f ( y * ) ? f ( y t ) using min t L( u t ) as an upper bound on f ( y * ) .
Of the examples that did not converge , the worst example had a bound that was 1.4 % of f ( y t ) ( more specifically , f ( y t ) was - 24.74 , and the upper bound on f ( y * ) ? f ( y t ) was 0.34 ) .
Figure 5 gives information on decoding time for our method and two other exact decoding methods : integer linear programming ( using constraints D0 - D6 ) , and exhaustive dynamic programming using the construction of ( Bar -Hillel et al. , 1964 ) .
Our method is clearly the most efficient , and is comparable in speed to state - of - the - art decoding algorithms .
We also compare our method to cube pruning ( Chiang , 2007 ; Huang and Chiang , 2007 ) .
We reimplemented cube pruning in C + + , to give a fair comparison to our method .
Cube pruning has a parameter , b , dictating the maximum number of items stored at each chart entry .
With b = 50 , our decoder finds higher scoring solutions on 50.5 % of all examples ( 349 examples ) , the cube-pruning method gets a strictly higher score on only 1 example ( this was one of the examples that did not converge within 200 iterations ) .
With b = 500 , our decoder finds better solutions on 18.5 % of the examples ( 128 cases ) , cubepruning finds a better solution on 3 examples .
The median decoding time for our method is 0.79 seconds ; the median times for cube pruning with b = 50 and b = 500 are 0.06 and 1.2 seconds respectively .
Our results give a very good estimate of the percentage of search errors for cube pruning .
A natural question is how large b must be before exact solutions are returned on almost all examples .
Even at b = 1000 , we find that our method gives a better solution on 95 test examples ( 13.7 % ) .
Figure 5 also gives a speed comparison of our method to a linear programming ( LP ) solver that solves the LP relaxation defined by constraints D0 - D6 .
We still see speed-ups , in spite of the fact that our method is solving a harder problem ( it provides integral solutions ) .
The Lagrangian relaxation method , when run without the tightening method of section 6 , is solving a dual of the problem being solved by the LP solver .
Hence we can measure how often the tightening procedure is absolutely necessary , by seeing how often the LP solver provides a fractional solution .
We find that this is the case on 54.0 % of the test examples : the tightening procedure is clearly important .
Inspection of the tightening procedure shows that the number of partitions required ( the parameter q ) is generally quite small : 59 % of examples that require tightening require q ? 6 ; 97.2 % require q ? 10 .
Conclusion
We have described a Lagrangian relaxation algorithm for exact decoding of syntactic translation models , and shown that it is significantly more efficient than other exact algorithms for decoding tree - to-string models .
There are a number of possible ways to extend this work .
Our experiments have focused on tree-to-string models , but the method should also apply to Hiero-style syntactic translation models ( Chiang , 2007 ) .
Additionally , our experiments used a trigram language model , however the constraints in figure 3 generalize to higher - order language models .
Finally , our algorithm recovers the 1 - best translation for a given input sentence ; it should be possible to extend the method to find kbest solutions .
