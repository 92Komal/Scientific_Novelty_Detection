title
Hierarchical Phrase -Based Translation Representations
abstract
This paper compares several translation representations for a synchronous context-free grammar parse including CFGs / hypergraphs , finite -state automata ( FSA ) , and pushdown automata ( PDA ) .
The representation choice is shown to determine the form and complexity of target LM intersection and shortest - path algorithms that follow .
Intersection , shortest path , FSA expansion and RTN replacement algorithms are presented for PDAs .
Chinese-to-English translation experiments using HiFST and HiPDT , FSA and PDA - based decoders , are presented using admissible ( or exact ) search , possible for HiFST with compact SCFG rulesets and HiPDT with compact LMs .
For large rulesets with large LMs , we introduce a two -pass search strategy which we then analyze in terms of search errors and translation performance .
Introduction Hierarchical phrase - based translation , using a synchronous context-free translation grammar ( SCFG ) together with an n-gram target language model ( LM ) , is a popular approach in machine translation ( Chiang , 2007 ) .
Given a SCFG
G and an ngram language model M , this paper focuses on how to decode with them , i.e. how to apply them to the source text to generate a target translation .
Decoding has three basic steps , which we first describe in terms of the formal languages and relations involved , with data representations and algorithms to follow .
Translating the source sentence s with G to give target translations : T = {s} ? G , a ( weighted ) context - free language resulting from the composition of a finite language and the algebraic relation G for SCFG G. 2 .
Applying the language model to these target translations : L = T ?M , a ( weighted ) contextfree language resulting from the intersection of a context-free language and the regular language M for M .
3 . Searching for the translation and language model combination with the highest - probablity path : L = argmax l?L L
Of course , decoding requires explicit data representations and algorithms for combining and searching them .
In common to the approaches we will consider here , s is applied to G by using the CYK algorithm in Step 1 and M is represented by a finite automaton in Step 2 .
The choice of the representation of T in many ways determines the remaining decoder representations and algorithms needed .
Since { s } is a finite language and we assume throughout that G does not allow unbounded insertions , T and L are , in fact , regular languages .
As such , T and L have finite automaton representations T f and L f .
In this case , weighted finite -state intersection and single-source shortest path algorithms ( using negative log probabilities ) can be used to solve Steps 2 and 3 ( Mohri , 2009 ) .
This is the approach taken in ( Iglesias et al. , 2009a ; de Gispert et al. , 2010 ) .
Instead T and L can be represented by hypergraphs T h and L h ( or very similarly context- free rules , and - or trees , or deductive systems ) .
In this case , hypergraph intersection with a finite automaton and hypergraph shortest path algorithms can be used to solve Steps 2 and 3 ( Huang , 2008 ) .
This is the approach taken by Chiang ( 2007 ) .
In this paper , we will consider another representation for context - free languages T and L as well , pushdown automata ( PDA ) T p and L p , familiar from formal language theory ( Aho and Ullman , 1972 ) .
We will describe PDA intersection with a finite automaton and PDA shortest - path algorithms in Section 2 that can be used to solve Steps 2 and 3 .
It cannot be over-emphasized that the CFG , hypergraph and PDA representations of T are used for their compactness rather than for expressing non-regular languages .
As presented so far , the search performed in Step 3 is admissible ( or exact ) - the true shortest path is found .
However , the search space in MT can be quite large .
Many systems employ aggressive pruning during the shortest - path computation with little theoretical or empirical guarantees of correctness .
Further , such pruning can greatly complicate any complexity analysis of the underlying representations and algorithms .
In this paper , we will exclude any inadmissible pruning in the shortest - path algorithm itself .
This allows us in Section 3 to compare the computational complexity of using these different representations .
We show that the PDA representation is particularly suited for decoding with large SCFGs and compact LMs .
We present Chinese-English translation results under the FSA and PDA translation representations .
We describe a two -pass translation strategy which we have developed to allow use of the PDA representation in large-scale translation .
In the first pass , translation is done using a lattice - generating version of the shortest path algorithm .
The full translation grammar is used but with a compact , entropy - pruned version ( Stolcke , 1998 ) of the full language model .
This first-step uses admissible pruning and lattice generation under the compact language model .
In the second pass , the original , unpruned LM is simply applied to the lattices produced in the first pass .
We find that entropy - pruning and first - pass translation can be done so as to introduce very few search errors in the overall process ; we can identify search errors in this experiment by comparison to exact translation under the full translation grammar and language model using the FSA representation .
We then investigate a translation grammar which is large enough that exact translation under the FSA representation is not possible .
We find that translation is possible using the two -pass strategy with the PDA translation representation and that gains in BLEU score result from using the larger translation grammar .
Related Work
There is extensive prior work on computational efficiency and algorithmic complexity in hierarchical phrase - based translation .
The challenge is to find algorithms that can be made to work with large translation grammars and large language models .
Following the original algorithms and analysis of Chiang ( 2007 ) , Huang and Chiang ( 2007 ) developed the cube-growing algorithm , and more recently Huang and Mi ( 2010 ) developed an incremental decoding approach that exploits left-to- right nature of the language models .
Search errors in hierarchical translation , and in translation more generally , have not been as extensively studied ; this is undoubtedly due to the difficulties inherent in finding exact translations for use in comparison .
Using a relatively simple phrasebased translation grammar , Iglesias et al . ( 2009 b ) compared search via cube-pruning to an exact FST implementation ( Kumar et al. , 2006 ) and found that cube-pruning suffered significant search errors .
For Hiero translation , an extensive comparison of search errors between the cube pruning and FSA implementation was presented by Iglesias et al . ( 2009a ) and de Gispert et al . ( 2010 ) .
Relaxation techniques have also recently been shown to finding exact solutions in parsing ( Koo et al. , 2010 ) and in SMT with tree-to-string translation grammars and trigram language models ( Rush and Collins , 2011 ) , much smaller models compared to the work presented in this paper .
Although entropy - pruned language models have been used to produce real-time translation systems ( Prasad et al. , 2007 ) , we believe our use of entropy - pruned language models in two -pass translation to be novel .
This is an approach that is widelyused in automatic speech recognition ( Ljolje et al. , 1999 ) and we note that it relies on efficient representation of very large search spaces T for subsequent rescoring , as is possible with FSAs and PDAs .
Pushdown Automata
In this section , we formally define pushdown automata and give intersection , shortest - path and related algorithms that will be needed later .
Informally , pushdown automata are finite automata that have been augmented with a stack .
Typ - ically this is done by adding a stack alphabet and labeling each transition with a stack operation ( a stack symbol to be pushed onto , popped or read from the stack ) in additon to the usual input label ( Aho and Ullman , 1972 ; Berstel , 1979 ) and weight ( Kuich and Salomaa , 1986 ; Petre and Salomaa , 2009 ) .
Our equivalent representation allows a transition to be labeled by a stack operation or a regular input symbol but not both .
Stack operations are represented by pairs of open and close parentheses ( pushing a symbol on and popping it from the stack ) .
The advantage of this representation is that is identical to the finite automaton representation except that certain symbols ( the parentheses ) have special semantics .
As such , several finite -state algorithms either immediately generalize to this PDA representation or do so with minimal changes .
The algorithms described in this section have been implemented in the PDT extension of the OpenFst library ( Allauzen et al. , 2007 ) .
Definitions A ( restricted ) Dyck language consist of " wellformed " or " balanced " strings over a finite number of pairs of parentheses .
Thus the string ( [ ( ) ( ) ] { } [ ] ) ( ) is in the Dyck language over 3 pairs of parentheses .
More formally , let A and A be two finite alphabets such that there exists a bijection f from A to A.
Intuitively , f maps an open parenthesis to its corresponding close parenthesis .
Let ? denote f ( a ) if a ?
A and f ?1 ( a ) if a ?
A. The Dyck language D A over the alphabet A = A ?
A is then the language defined by the following context- free grammar : S ? ? , S ? SS and S ? aS? for all a ?
A . We define the mapping c A : A * ?
A * as follow .
c A ( x ) is the string obtained by iteratively deleting from x all factors of the form a ?
with a ?
A. Observe that D A = c ?1 A ( ? ) .
Let A and B be two finite alphabets such that B ?
A , we define the mapping r B : A * ? B * by r B ( x 1 . . . x n ) = y 1 . . . y n with y i = x i if x i ?
B and y i = ? otherwise .
A weighted pushdown automaton ( PDA ) T over the tropical semiring ( R ? {?} , min , + , ? , 0 ) is a 9 - tuple ( ? , ? , ? , Q , E , I , F , ? ) where ? is the finite input alphabet , ? and ? are the finite open and close parenthesis alphabets , Q is a finite set of states , I ?
Q the initial state , F ?
Q the set of final states , E ? Q ? (? ? ? ? {?} ) ? ( R ? {?} ) ?
Q a fi- nite set of transitions , and ? : F ? R ? {? } the final weight function .
Let e = ( p[ e ] , i [ e ] , w[ e ] , n[ e ] ) denote a transition in E .
A path ? is a sequence of transitions ? = e 1 . . . e n such that n[ e i ] = p[ e i+ 1 ] for 1 ? i < n. We then de- fine p [ ? ] = p[ e 1 ] , n [ ? ] = n[ e n ] , i [ ? ] = i[ e 1 ] ? ? ? i[ e n ] , and w [ ? ]
= w[ e 1 ] + . . . + w[ e n ] .
A path ? is accepting if p [ ? ] = I and n [ ? ] ?
F . A path ? is balanced if r ? ( i [ ? ] ) ?
D ? . A balanced path ? accepts the string x ? ? * if it is a balanced accepting path such that r ? ( i [ ? ] ) = x .
The weight associated by T to a string x ? ? * is T ( x ) = min ?P ( x ) w [ ? ] + ?( n [ ? ] ) where P ( x ) denotes the set of balanced paths accepting x .
A weighted language is recognizable by a weighted pushdown automaton iff it is context-free .
We define the size of T as | T | = |Q |+|E| .
A PDA
T has a bounded stack if there exists K ?
N such that for any sub-path ? of any balanced path in T : |c ? ( r ? ( i [ ? ] ) ) | ? K .
If T has a bounded stack , then it represents a regular language .
Figure 1 shows non-regular , regular and bounded - stack PDAs .
A weighted finite automaton ( FSA ) can be viewed as a PDA where the open and close parentheses alphabets are empty , see ( Mohri , 2009 ) for a standalone definition .
Expansion Algorithm Given a bounded - stack PDA T , the expansion of T is the FSA T ? equivalent to T defined as follows .
A state in T ? is a pair ( q , z ) where q is a state in T and z ? ? * .
A transition ( q , a , w , q ? ) in T results in a transition ( ( q , z ) , a ? , w , ( q ? , z ? ) ) in T ? only when : ( a ) a ? ? ? {?} , z ? = z and a ? = a , ( b ) a ? ? , z ? = za and a ? = ? , or ( c ) a ? ? , z ? is such that z = z ? a and a ? = ?.
The initial state of T ? is I ? = ( I , ? ) .
A state ( q , z ) in T ? is final if q is final in T and z = ? (? ? ( ( q , ? ) ) = ?( q ) ) .
The set of states of T ? is the set of pairs ( q , z ) that can be reached from an initial state by transitions defined as above .
The condition that T has a bounded stack ensures that this set is finite ( since it implies that for any ( q , z ) , |z| ? K ) .
The complexity of the algorithm is linear in 1d show the result of the algorithm when applied to the PDA of Figure 1 c . O ( |T ? |) = O(e | T | ) .
Figure
Intersection Algorithm
The class of weighted pushdown automata is closed under intersection with weighted finite automata ( Bar - Hillel et al. , 1964 ; Nederhof and Satta , 2003 ) .
Considering a pair ( T 1 , T 2 ) where one element is an FSA and the other element a PDA , then there exists a PDA T 1 ? T 2 , the intersection of T 1 and T 2 , such that for all x ? ? * : ( T 1 ? T 2 ) ( x ) = T 1 ( x ) + T 2 ( x ) .
We assume in the following that T 2 is an FSA .
We also assume that T 2 has no input -? transitions .
When T 2 has input -?
transitions , an epsilon filter ( Mohri , 2009 ; generalized to handle parentheses can be used .
A state in T = T 1 ?T 2 is a pair ( q 1 , q 2 ) where q 1 is a state of T 1 and q 2 a state of T 2 .
The initial state is I = ( I 1 , I 2 ) .
Given a transition e 1 = ( q 1 , a , w 1 , q ? 1 ) in T 1 , transitions out of ( q 1 , q 2 ) in T are obtained using the following rules .
If a ? ? , then e 1 can be matched with a transition ( q 2 , a , w 2 , q ? 2 ) in T 2 resulting a transition ( ( q 1 , q 2 ) , a , w 1 +w 2 , ( q ? 1 , q ? 2 ) ) in T .
If a = ? , then e 1 is matched with staying in q 2 resulting in a transition ( ( q 1 , q 2 ) , ? , w 1 , ( q ? 1 , q 2 ) ) .
Finally , if a ? ? , e 1 is also matched with staying in q 2 , resulting in a transition ( ( q 1 , q 2 ) , a , w 1 , ( q ? 1 , q 2 ) ) in T .
A state ( q 1 , q 2 ) in T is final when both q 1 and q 2 are final , and then ?(( q 1 , q 2 ) ) = ? 1 ( q 1 ) +? 2 ( q 2 ) .
SHORTESTDISTANCE (T ) 1 for each q ?
Q and a ? ? do 2 B[ q , a ] ? ? 3 GETDISTANCE (T , I ) 4 return d [ f , I ] RELAX ( q , s , w , S ) 1 if d[ q , s ] > w then 2 d[ q , s ] ?
w 3 if q ?
S then 4 ENQUEUE ( S , q) GETDISTANCE ( T , s ) 1 for each q ?
Q do 2 d[ q , s ] ? ? 3 d[ s , s ] ?
0 4 Ss ? s 5 while Ss = ? do 6 q ? HEAD ( Ss ) 7 DEQUEUE ( Ss ) 8 for each e ? E [ q ] do 9 if i [ e ] ? ? ? {? } then 10 RELAX ( n [ e ] , s , d [ q , s ] + w[e ] , Ss ) 11 elseif i [ e ] ? ? then 12 B [s , i [ e ] ] ? B [s , i [ e ] ] ? {e} 13 elseif i [ e ] ? ? then 14 if d [ n [ e ] , n[ e ] ] is undefined then 15 GETDISTANCE ( T , n[ e ] ) 16 for each e ? ? B [ n[e ] , i [ e ] ] do 17 w ? d [ q , s ] + w[e ] + d [ p [ e ? ] , n[ e ] ] + w[ e ? ] 18 RELAX ( n[ e ? ] , s , w , Ss ) Figure 2 : PDA shortest distance algorithm .
We assume that F = {f } and ?( f ) = 0 to simplify the presentation .
The complexity of the algorithm is in O ( |T 1 ||T 2 | ) .
Shortest Distance and Path Algorithms
A shortest path in a PDA T is a balanced accepting path with minimal weight and the shortest distance in T is the weight of such a path .
We show that when 2 . GETDISTANCE (T , s ) starts a new instance of the shortest - distance algorithm from s using the queue S s , initially containing s .
While the queue is not empty , a state is dequeued and its outgoing transitions examined ( line 5 - 9 ) .
Transitions labeled by non-parenthesis are treated as in Mohri ( 2009 )
In effect , we are solving a k-sources shortestpath problem with k single-source solutions .
A potentially better approach might be to solve the ksources or k-pairs problem directly ( Hershberger et al. , 2003 ) .
When T has been obtained by converting an RTN or an hypergraph into a PDA ( Section 2.5 ) , the polynomial dependency in | T | becomes a linear dependency both for the time and space complexities .
Indeed , for each q in T , there exists a unique s such that d[ q , s ] is non-infinity .
Moreover , for each close parenthesis transistion e , there exists a unique open parenthesis transition e ? such that e ? B [ n [ e ? ] , i [ e ? ] ] .
When each component of the RTN is acyclic , the complexity of the algorithm is hence in O ( |T | ) in time and space .
The algorithm can be modified to compute the shortest path by keeping track of parent pointers .
Replacement Algorithm
A recursive transition network ( RTN ) can be specified by ( N , ? , ( T ? ) ?N , S ) where N is an alphabet of nonterminals , ? is the input alphabet , ( T ? ) ?N is a family of FSAs with input alphabet ? ?
N , and S ? N is the root nonterminal .
A string x ? ? * is accepted by R if there exists an accepting path ? in T S such that recursively replacing any transition with input label ? ?
N by an accepting path in T ? leads to a path ? * with input x .
The weight associated by R is the minimum over all such ? * of w [ ? * ] +? S ( n [ ? * ] ) .
Given an RTN R , the replacement of R is the PDA T equivalent to R defined by the 9 - tuple ( ? , ? , ? , Q , E , I , F , ? , ? ) with ? = Q = ?N Q ? , I = I S , F = F S , ? = ? S , and E = ?N e?E ?
E e where E e = {e} if i [ e ] ? N and E e = { ( p[ e ] , n[ e ] , w[ e ] , I ? ) , ( f , n[e ] , ? ? ( f ) , n [ e ] ) |f ? F ? } with ? = i [ e ] ?
N otherwise .
The complexity of the construction is in O ( |T | ) .
If | F ? | = 1 , then | T | = O ( ?N |T ? | ) = O ( | R | ) .
Creating a superfinal state for each T ? would lead to a T whose size is always linear in the size of R .
Hierarchical Phrase - Based Translation Representation
In this section , we compare several different representations for the target translations T of the source sentence s by synchronous CFG G prior to language model M application .
As discussed in the introduction , T is a context-free language .
For example , suppose it corresponds to : S?abXdg , S?acXf g , and X?bc.
Figure 3 shows several alternative representations of T : Figure 3a shows the hypergraph representation of this grammar ; there is a 1:1 correspondence between each production in the CFG and each hyperedge in the hypergraph .
Figure 3 b shows the RTN representation of this grammar with a 1:1 correspondence between each production in the CFG and each path in the RTN ; this is the translation representation pro- duced by the HiFST decoder ( Iglesias et al. , 2009a ; de Gispert et al. , 2010 ) .
Figure 3 c shows the pushdown automaton representation generated from the RTN with the replacement algorithm of Section 2.5 .
Since s is a finite language and G does not allow unbounded insertion , T p has a bounded stack and T is , in fact , a regular language .
Figure 3d shows the finite-state automaton representation of T generated by the PDA using the expansion algorithm of Section 2.2 .
The HiFST decoder converts its RTN translation representation immediately into the finite-state representation using an algorithm equivalent to converting the RTN into a PDA followed by PDA expansion .
As shown in Figure 4 , an advantage of the RTN , PDA , and FSA representations is that they can benefit from FSA epsilon removal , determinization and minimization algorithms applied to their components ( for RTNs and PDAs ) or their entirety ( for FSAs ) .
For the complexity discussion below , however , we disregard these optimizations .
Instead we focus on the complexity of each MT step described in the introduction : 1 . SCFG
Translation :
Assuming that the parsing of the input is performed by a CYK parse , then the CFG , hypergraph , RTN and PDA represen -
Shortest Path :
The shortest path algorithm on the hypergraph , RTN , and FSA representations requires linear time and space ( given the underlying acyclicity ) ( Huang , 2008 ; Mohri , 2009 ) .
As presented in Section 2.4 , the PDA representation can require time cubic and space quadratic in | M |. 2 Table 1 summarizes the complexity results .
Note the PDA representation is equivalent in time and superior in space to the CFG / hypergraph representation , in general , and it can be superior in both space 1 The modified Bar-Hillel construction described by Chiang ( 2007 ) has time and space complexity O ( |T h ||M | 4 ) ; the modifications were introduced presumably to benefit the subsequent pruning method employed ( but see Huang et al . ( 2005 ) ) .
2
The time ( resp. space ) complexity is not cubic ( resp. quadratic ) in | Tp | |M |. Given a state q in Tp , there exists a unique sq such that q belongs to Cs q .
Given a state ( q1 , q2 ) in Tp ? M , ( q1 , q2 ) ?
C ( s 1 , s 2 ) only if s1 = sq 1 , and hence ( q1 , q2 ) belongs to at most | M | components .
and time to the FSA representation depending on the relative SCFG and LM sizes .
The FSA representation favors smaller target translation sets and larger language models .
Should a better complexity PDA shortest path algorithm be found , this conclusion could change .
In practice , the PDA and FSA representations benefit hugely from the optimizations mentioned above , these optimizations improve the time and space usage by one order of magnitude .
Representation Time Complexity Space Complexity CFG / hypergraph O ( |s | 3 |G | | M | 3 ) O ( |s | 3 |G | | M | 3 ) PDA O ( |s | 3 |G | | M | 3 ) O ( |s | 3 |G | | M | 2 ) FSA O(e | s| 3 |G | |M |) O(e | s| 3 |G | |M |)
Experimental Framework
We use two hierarchical phrase - based SMT decoders .
The first one is a lattice - based decoder implemented with weighted finite -state transducers ( de Gispert et al. , 2010 ) and described in Section 3 .
The second decoder is a modified version using PDAs as described in Section 2 .
In order to distinguish both decoders we call them HiFST and HiPDT , respectively .
The principal difference between the two decoders is where the finite-state expansion step is done .
In HiFST , the RTN representation is immediately expanded to an FSA .
In HiPDT , this expansion is delayed as late as possible - in the output of the shortest path algorithm .
Another possible configuration is to expand after the LM intersection step but before the shortest path algorithm ; in practice this is quite similar to HiFST .
In the following sections we report experiments in Chinese-to - English translation .
For translation model training , we use a subset of the GALE 2008 evaluation parallel text ; 3 this is 2.1 M sentences and approximately 45 M words per language .
We report translation results on a development set tune - nw ( 1,755 sentences ) and a test set test - nw ( 1,671 sentences ) .
These contain translations produced by the GALE program and portions of the newswire sections of MT02 through MT06 .
In tuning the sys - 0 7.5 ? 10 ?9 7.5 ? 10 ?8 7.5 ? 10 ?7 207.5 20.2 4.1 0.9 tems , standard MERT ( Och , 2003 ) iterative parameter estimation under IBM BLEU 4 is performed on the development set .
The parallel corpus is aligned using MTTK ( Deng and Byrne , 2008 ) in both source - to- target and target - to -source directions .
We then follow standard heuristics ( Chiang , 2007 ) and filtering strategies ( Iglesias et al. , 2009 b ) to extract hierarchical phrases from the union of the directional word alignments .
We call a translation grammar the set of rules extracted from this process .
We extract two translation grammars : ?
A restricted grammar where we apply the following additional constraint : rules are only considered if they have a forward translation probability p > 0.01 .
We call this G 1 .
As will be discussed later , the interest of this grammar is that decoding under it can be exact , that is , without any pruning in search . ?
An unrestricted one without the previous constraint .
We call this G 2 .
This is a superset of the previous grammar , and exact search under it is not feasible for HiFST : pruning is required in search .
The initial English language model is a Kneser - Ney 4 - gram estimated over the target side of the parallel text and the AFP and Xinhua portions of monolingual data from the English Gigaword Fourth Edition ( LDC2009T13 ) .
This is a total of 1.3B words .
We will call this language model M 1 .
For large language model rescoring we also use the LM M 2 obtained by interpolating M 1 with a zero-cutoff stupidbackoff ( Brants et al. , 2007 ) 5 - gram estimated using 6.6B words of English newswire text .
We next describe how we build translation systems using entropy - pruned language models .
1 . We build a baseline HiFST system that uses M 1 and a hierarchical grammar G , parameters being optimized with MERT under BLEU .
2 . We then use entropy - based pruning of the language model ( Stolcke , 1998 ) under a relative perplexity threshold of ? to reduce the size of M 1 .
We will call the resulting language model as M ? 1 .
Table 2 shows the number of n-grams ( in millions ) obtained for different ? values .
3 . We translate with M ? 1 using the same parameters obtained in MERT in step 1 , except for the word penalty , tuned over the lattices under BLEU performance .
This produces a translation lattice in the topmost cell that contains hypotheses with exact scores under the translation grammar and M ? 1 . 4 . Translation lattices in the topmost cell are pruned with a likelihood - based beam width ?.
5 . We remove the M ? 1 scores from the pruned translation lattices and reapply M 1 , moving the word penalty back to the original value obtained in MERT .
These operations can be carried out efficiently via standard FSA operations .
6 . Additionally , we can rescore the translation lattices obtained in steps 1 or 5 with the larger language model M 2 . Again , this can be done via standard FSA operations .
Note that if ? = ? or if ? = 0 , the translation lattices obtained in step 1 should be identical to the ones of step 5 .
While the goal is to increase ? to reduce the size of the language model used at Step 3 , ? will have to increase accordingly so as to avoid pruning away desirable hypotheses in Step 4 .
If ? defines a sufficiently wide beam to contain the hypotheses which would be favoured by M 1 , faster decoding with M ? 1 would be possible without incurring search errors M 1 .
This is investigated next .
Entropy - Pruned LM in Rescoring In Table 3 we show translation performance under grammar G 1 for different values of ?.
Performance is reported after first - pass decoding with M ? 1 ( see step 3 in Section 4 ) , after rescoring with M 1 ( see step 5 ) and after rescoring with M 2 ( see step 6 ) .
The baseline ( experiment number 1 ) uses ? = 0 ( that is , M 1 ) for decoding .
Under translation grammar G 1 , HiFST is able to generate an FSA with the entire space of possible candidate hypotheses .
Therefore , any degradation in performance is only due to the M ? 1 involved in decoding and the ? applied prior to rescoring .
As shown in row number 2 , for ? ? 10 ?9 the system provides the same performance to the baseline when ? > 8 , while decoding time is reduced by roughly 40 % .
This is because M ? 1 is 10 % of the size of the original language model M 1 , as shown in Table 2 . As M ? 1 is further reduced by increasing ?
( see rows number 3 and 4 ) , decoding time is also reduced .
However , the beam width ?
required in order to recover the good hypotheses in rescoring increases , reaching 12 for experiment 3 and 15 for experiment 4 .
Regarding rescoring with the larger M 2 ( step 6 in Section 4 ) , the system is also able to match the baseline performance as long as ? is wide enough , given the particular M ? 1 used in first - pass decoding .
Interestingly , results show that a similar ?
value is needed when rescoring either with M 1 or M 2 .
The usage of entropy - pruned language models increments speed at the risk of search errors .
For instance , comparing the outputs of systems 1 and 2 with ? = 10 in Table 3 we find 45 different 1 - best hypotheses , even though the BLEU score is identical .
In other words , we have 45 cases in which system 2 is not able to recover the baseline output because the 1st- pass likelihood beam ? is not wide enough .
Similarly , system 3 fails in 101 cases ( ? = 12 ) and system 4 fails in 95 cases .
Interestingly , some of these sentences would require impractically huge beams .
This might be due to the Kneser - Ney smoothing , which interacts badly with entropy pruning ( Chelba et al. , 2010 ) .
Hiero with PDAs and FSAs
In this section we contrast HiFST with HiPDT under the same translation grammar and entropy - pruned language models .
Under the constrained grammar G 1 their performance is identical as both decoders can generate the entire search space which can then be rescored with M 1 or M 2 as shown in the previous section .
Therefore , we now focus on the unconstrained grammar G 2 , where exact search is not feasible for HiFST .
In order to evaluate this problem , we run both decoders over tune -nw , restricting memory usage to 10 gigabytes .
If this limit is reached in decod - Table 5 : HiPDT performance on grammar G 2 with ? = 7.5 ? 10 ?7 . Exact search with HiFST is not possible under these conditions : pruning during search would be required .
ing , the process is killed 5 .
We report what internal decoding operation caused the system to crash .
For HiFST , these include expansion into an FSA ( Expand ) and subsequent intersection with the language model ( Compose ) .
For HiPDT , these include PDA intersection with the language model ( Compose ) and subsequent expansion into an FSA ( Expand ) , using algorithms described in Section 2 .
Table 4 shows the number of times each decoder succeeds in finding a hypothesis given the memory limit , and the operations being carried out when they fail to do so , when decoding with various M ? 1 . With ? = 7.5 ? 10 ?9 ( row 2 ) , HiFST can only decode 218 sentences , while HiPDT succeeds in 703 cases .
The 5 We used ulimit command .
The experiment was carried out over machines with different configurations and load .
Therefore , these numbers must be considered as approximate values .
differences between both decoders increase as the M ? 1 is more reduced , and for ? = 7.5 ? 10 ?7 ( row 4 ) , HiPDT is able to perform exact search over all but three sentences .
Table 5 shows performance using the latter configuration ( Table 4 , row 4 ) .
After large language model rescoring , HiPDT improves 0.5 BLEU over baseline with G 1 ( Table 3 , row 1 ) .
Discussion and Conclusion HiFST fails to decode mainly because the expansion into an FST leads to far too big search spaces ( e.g. fails 938 times under ? = 7.5 ? 10 ?8 ) .
If it succeeds in expanding the search space into an FST , the decoder still has to compose with the language model , which is also critical in terms of memory us-age ( fails 536 times ) .
In contrast , HiPDT creates a PDA , which is a more compact representation of the search space and allows efficient intersection with the language model before expansion into an FST .
Therefore , the memory usage is considerably lower .
Nevertheless , the complexity of the language model is critical for the PDA intersection and very specially the PDA expansion into an FST ( fails 403 times for ? = 7.5 ? 10 ?8 ) .
With the algorithms presented in this paper , decoding with PDAs is possible for any translation grammar as long as an entropy pruned LM is used .
While this allows exact decoding , it comes at the cost of making decisions based on less complex LMs , although this has been shown to be an adequate strategy when applying compact CFG rulesets .
On the other hand , HiFST cannot decode under large translation grammars , thus requiring pruning during lattice construction , but it can apply an unpruned LM in this process .
We find that with carefully designed pruning strategies , HiFST can match the performance of HiPDT reported in Table 5 .
But without pruning in search , expansion directly into an FST would lead to an explosion in terms of memory usage .
Of course , without memory constraints both strategies would reach the same performance .
Overall , these results suggest that HiPDT is more robust than HiFST when using complex hierarchical grammars .
Conversely , FSTs might be more efficient for search spaces described by more constrained hierarchical grammars .
This suggests that a hybrid solution could be effective : we could use PDAs or FSTs e.g. depending on the number of states of the FST representing the expanded search space , or other conditions .
Figure 1 : 1 Figure 1 : PDA Examples : ( a) Non-regular PDA accepting { a n b n |n ? N}. ( b) Regular ( but not bounded - stack ) PDA accepting a * b * . ( c ) Bounded - stack PDA accepting a * b * and ( d ) its expansion as an FSA .
( line 9 -10 ) .
When the considered transition e is labeled by a close parenthesis , it is remembered that it balances all incoming open parentheses in s labeled by i [ e ] by adding e to B [s , i [ e ] ] ( line 11 - 12 ) .
Finally , when e is labeled with an open parenthesis , if its destination has not already been visited , a new instance is started from n[ e ] ( line 14 - 15 ) .
The destination states of all transitions balancing e are then relaxed ( line 16 - 18 ) .
The space complexity of the algorithm is quadratic for two reasons .
First , the number of non-infinity d[ q , s ] is | Q| 2 . Second , the space required for storing B is at most in O ( |E | 2 ) since for each open parenthesis transition e , the size of |B [ n [ e ] , i [ e ] ] | is O ( |E | ) in the worst case .
This last observation also implies that the cumulated number of transitions examined at line 16 is in O ( N | Q| |E| 2 ) in the worst case , where N denotes the maximal number of times a state is inserted in the queue for a given call of GETDISTANCE .
Assuming the cost of a queue operation is ?( n ) for a queue containing n elements , the worst - case time complexity of the algorithm can then be expressed as O ( N | T | 3 ?( |T | ) ) .
When T contains no negative weights , using a shortest - first queue discipline leads to a time complexity in O ( |T | 3 log | T | ) .
When all the C s 's are acyclic , using a topological order queue discipline leads to a O ( |T | 3 ) time complexity .
Figure 3 : Alternative translation representations
Figure 4 : 4 Figure 4 : Optimized translation representations
T has a bounded stack , shortest distance and shortest path can be computed in O ( |T | 3 log | T | ) time ( assuming T has no negative weights ) and O ( |T | 2 ) space .
Given a state s in T with at least one incoming open parenthesis transition , we denote by C s the set of states that can be reached from s by a balanced path .
If s has several incoming open parenthesis transitions , a naive implementation might lead to the states in C s to be visited up to exponentially many times .
The basic idea of the algorithm is to memoize the shortest distance from s to states in C s .
The pseudo-code is given in Figure
Table 1 : 1 Complexity using various target translation representations .
Table 2 : 2 Number of ngrams ( in millions ) in the 1st pass 4 - gram language models obtained with different ? values ( top row ) .
Table 3 : 3 Results ( lowercase IBM BLEU scores ) under G 1 with various M ? 1 as obtained with several values of ?.
Performance in subsequent rescoring with M 1 and M 2 after likelihood - based pruning of the translation lattices for various ? is also reported .
Decoding time , in seconds / word over test -nw , refers strictly to first - pass decoding .
HiFST ( G1 + M ? 1 ) + M 1 +M 2 # ? tune - nw test - nw time ? tune - nw test - nw tune - nw test - nw 1 2 7.5 ? 10 ?9 0 ( M 1 ) 34.3 32.0 34.5 32.8 0.68 0.38 - 10 9 -34.3 - 34.5 34.8 34.8 34.9 35.6 35.6 35.5 8 3 7.5 ? 10 ?8 29.5 30.0 0.28 12 9 34.2 34.3 34.5 34.4 34.7 34.8 35.6 35.2 8 34.2 35.1 4 7.5 ? 10 ?7 26.0 26.4 0.20 15 12 34.2 34.5 34.4 34.7 35.6 35.5
Exact search for G2 + M ? 1 with memory usage under 10 GB # ? HiFST HiPDT Success Failure Success Failure Expand Compose Compose Expand 2 7.5 ? 10 ?9 3 7.5 ? 10 ?8 4 7.5 ? 10 ?7 12 16 18 51 53 53 37 31 29 40 76 99.8 8 1 0 52 23 0.2
Table 4 : 4 Percentage of success in producing the 1 - best translation under G 2 with various M ? 1 when applying a hard memory limitation of 10 GB , as measured over tune - nw ( 1755 sentences ) .
If decoder fails , we report what step was being done when the limit was reached .
HiFST could be expanding into an FSA or composing the FSA with M ? 1 ; HiPDT could be PDA composing with M ? 1 or PDA expanding into an FSA .
HiPDT ( G2 + M ? 1 ) + M1 + M2 ? tune - nw test - nw ? tune - nw test - nw tune - nw test -nw 7.5 ? 10 ?7 25.7 26.3 15 34.6 34.8 35.2 36.1
See http://projects.ldc.upenn.edu/gale/data/catalog.html.
We excluded the UN material and the LDC2002E18 , LDC2004T08 , LDC2007E08 and CUDonga collections .
See ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13.pl
