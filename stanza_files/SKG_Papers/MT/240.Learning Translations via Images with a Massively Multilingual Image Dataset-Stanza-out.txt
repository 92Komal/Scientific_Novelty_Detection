title
Learning Translations via Images with a Massively Multilingual Image Dataset
abstract
We conduct the most comprehensive study to date into translating words via images .
To facilitate research on the task , we introduce a large-scale multilingual corpus of images , each labeled with the word it represents .
Past datasets have been limited to only a few high- resource languages and unrealistically easy translation settings .
In contrast , we have collected by far the largest available dataset for this task , with images for approximately 10,000 words in each of 100 languages .
We run experiments on a dozen high resource languages and 20 low resources languages , demonstrating the effect of word concreteness and part- of-speech on translation quality .
To improve image- based translation , we introduce a novel method of predicting word concreteness from images , which improves on a previous stateof - the - art unsupervised technique .
This allows us to predict when image - based translation may be effective , enabling consistent improvements to a state - of - the - art text - based word translation system .
Our code and the Massively Multilingual Image Dataset ( MMID ) are available at http : //multilingual-images.org/ .
Introduction
Learning the translations of words is important for machine translation and other tasks in natural language processing .
Typically this learning is done using sentence - aligned bilingual parallel texts .
However , for many languages , there are not Figure 1 : Our dataset and approach allow translations to be discovered by comparing the images associated with foreign and English words .
Shown here are five images for the Indonesian word kucing , a word with high predicted concreteness , along with its top 4 ranked translations using CNN features .
sufficiently large parallel texts to effectively learn translations .
In this paper , we explore the question of whether it is possible to learn translations with images .
We systematically explore an idea originally proposed by Bergsma and Van Durme ( 2011 ) : translations can be identified via images associated with words in different languages that have a high degree of visual similarity .
This is illustrated in Figure 1 .
Most previous image datasets compiled for the task of learning translations were limited to the translation of nouns in a few high- resource languages .
In this work , we present a new large-scale dataset that contains images for 100 languages , and is not restricted by part-of-speech .
We collected images using Google Image Search for up to 10,000 words in each of 100 foreign languages , and their English translations .
For each word , we collected up to 100 images and the text on images ' corresponding web pages .
We conduct a broad range of experiments to evaluate the utility of image features across a number of factors : ?
We evaluate on 12 high- resource and 20 lowresource languages . ?
We evaluate translation quality stratified by part- of-speech , finding that nouns and adjectives are translated with much higher accuracy than adverbs and verbs . ?
We present a novel method for predicting word concreteness from image features that better correlates with human perception than existing methods .
We show that choosing concrete subsets of words to translate results in higher accuracy .
?
We augment a state - of - the - art text - based word translation system with image feature scores and find consistent improvements to the textonly system , ranging from 3.12 % absolute top - 1 accuracy improvement at 10 % recall to 1.30 % absolute improvement at 100 % recall .
A further contribution of this paper is our dataset , which is the largest of its kind and should be a standard for future work in learning translations from images .
The dataset may facilitate research into multilingual , multimodal models , and translation of low-resource languages .
Related Work
The task of learning translations without sentencealigned bilingual parallel texts is often called bilingual lexicon induction ( Rapp , 1999 ; Fung and Yee , 1998 ) .
Most work in bilingual lexicon induction has focused on text - based methods .
Some researchers have used similar spellings across related languages to find potential translations ( Koehn and Knight , 2002 ; Haghighi et al. , 2008 ) .
Others have exploited temporal similarity of word frequencies to induce translation pairs ( Schafer and Yarowsky , 2002 ; Klementiev and Roth , 2006 ) . Irvine and Callison- Burch ( 2017 ) provide a systematic study of different text - based features used for bilingual lexicon induction .
Recent work has focused on building joint distributional word embedding spaces for multiple languages , leveraging a range of levels of language supervision from bilingual dictionaries to comparable texts ( Vuli ? and Korhonen , 2016 ; Wijaya et al. , 2017 ) .
The most closely related work to ours is research into bilingual lexicon induction using image similarity by Bergsma and Van Durme ( 2011 ) and Kiela et al . ( 2015 ) .
Their work differs from ours in that they focused more narrowly on the translation of nouns for a limited number of high resource languages .
Bergsma and Van Durme ( 2011 ) compiled datasets for Dutch , English , French , German , Italian , and Spanish by downloading 20 images for up to 500 concrete nouns in each of the foreign languages , and 20,000 English words .
Another dataset was generated by Vulic and Moens ( 2013 ) who collected images for 1,000 words in Spanish , Italian , and Dutch , along with the English translations for each .
Their dataset also consists of only nouns , but includes abstract nouns .
Our corpus will allow researchers to explore image similarity for bilingual lexicon induction on a much wider range of languages and parts of speech , which is especially desirable given the potential utility of the method for improving translation between languages with little parallel text .
The ability of images to usefully represent a word is strongly dependent on how concrete or abstract the word is .
The terms abstractness and concreteness are used in the psycholinguistics and cognitive psychology literature .
Concrete words directly reference a sense experience ( Paivio et al. , 1968 ) , while abstract words can denote ideas , emotions , feelings , qualities or other abstract or intangible concepts .
Concreteness ratings are closely correlated with imagery ratings , defined as the ease with which a word arouses a mental image ( Gilhooly and Logie , 1980 ; Friendly et al. , 1982 ) .
Intuitively , concrete words are easier to represent visually , so a measure of a word 's concreteness ought to be able to predict the effectiveness of using images to translate the word .
Kiela et al. ( 2014 ) defines an unsupervised method called image dispersion that approximates a word 's concreteness by taking the average pairwise cosine distance of a set of image representations of the word .
Kiela et al. ( 2015 ) show that image dispersion helps predict the usefulness of image representations for translation .
In this paper , we introduce novel supervised approaches for predicting word concreteness from image and textual features .
We make use of a dataset created by Brysbaert et al . ( 2014 ) containing human evaluations of concreteness for 39,954 English words .
Concurrently with our work , Hartmann and S?gaard ( 2017 ) released an unpublished arXiv draft challenging the efficacy of using images for translation .
Their work presents several difficulties of using image features for translation , difficulties which our methods address .
They find that image features are only useful in translating simple nouns .
While we did indeed find that nouns perform better than other parts of speech , we do not find that images are only effective in translating simple words .
Instead , we show a gradual degradation in performance as words become more abstract .
Their dataset is restricted to six high- resource languages and a small vocabulary of 557 English words .
In contrast , we present results for over 260,000 English words and 32 foreign languages .
Recent research in the NLP and computer vision communities has been enabled by large collections of images associated with words or longer texts .
Object recognition has seen dramatic gains in part due to the ImageNet database ( Deng et al. , 2009 ) , which contains 500 - 1000 images associated with 80,000 synsets in WordNet .
Ferraro et al. ( 2015 ) surveys existing corpora that are used in vision and language research .
Other NLP + Vision tasks that have been enabled by the availability of large datasets include caption generation for images , action recognition in videos , visual question answering , and others .
Most existing work on multilingual NLP + Vision relies on having a corpus of images manually annotated with captions in several languages , as in the Multi30 K dataset ( Elliott et al. , 2016 ) .
Several works have proposed using image features to improve sentence level translations or to translate image captions ( Gella et al. , 2017 ; Hitschler and Riezler , 2016 ; Miyazaki and Shimizu , 2016 ) . Funaki and Nakayama ( 2015 ) show that automatically scraped data from websites in English and Japanese can be used to effectively perform zero-shot learning for the task of cross-lingual document retrieval .
Since collecting multilingual annotations is difficult at a large-scale or for low-resource languages , our approach relies only on data scraped automatically from the web .
Corpus Construction
We present a new dataset for image - based word translation that is more expansive than any previous ones , encompassing all parts - of-speech , the gamut of abstract to concrete , and both low - and highresource languages .
Dictionaries
We collect images for words in 100 bilingual dictionaries created by Pavlick et al . ( 2014 ) .
They selected the 10,000 most frequent words on Wikipedia pages in the foreign language , and then collected their translations into English via crowdsourcing .
We will denote these dictionaries as CROWDTRANS .
The superset of English translations for all foreign words consists of 263,102 translations .
The English portion of their data tends to be much noisier than the foreign portion due to its crowdsourced nature ( e.g. misspellings , or definition included with translations . )
We computed part- of-speech for entries in each dictionary .
We found that while nouns are the most common , other parts - of-speech are reasonably represented ( Section 5.1 ) .
Method For each English and foreign word , we query Google Image Search to collect 100 images associated with the word .
A potential criticism of our use of Google Image Search is that it may be using a bilingual dictionary to translate queries into English ( or other high resource languages ) and returning images associated with the translated queries ( Kilgarriff , 2007 ) .
We take steps ( Section 3.3 ) to filter out images that did not appear on pages written in the language that we are gathering images for .
After assembling the collection of images associated with words , we construct low-dimensional vector representations of the images using convolutional neural networks ( CNNs ) .
We also save the text from each web page that an image appeared on .
Further detail on our corpus construction pipeline can be found in Section 2 of the supplemental materials .
Filtering by Web Page Language
We used the following heuristic to filter images : if text could be extracted from an image 's web page , and the expected language was in the top - 3 most likely languages output by the CLD2 1 language detection system then we kept the image ; otherwise it was discarded .
This does not filter all images from webpages with English text ; instead it acknowledges the presence of English in the multilingual web and keeps images from pages with some targetlanguage presence .
An average of approximately 42 % of images for each foreign language remained after the language - filtering step .
1 : The proportion of images determined to be good representations of their corresponding word .
In columns 2 - 5 , we bucket the results by the word 's ground - truth concreteness , while column 6 shows the results over all words .
The last row shows the number of words in each bucket of concreteness , and the number of words overall for each language .
Manual Evaluation of Images
By using a dataset scraped from the web , we expect some fraction of the images for each word to be incorrectly labeled .
To confirm the overall quality of our dataset , we asked human evaluators on Amazon Mechanical Turk to label a subset of the images returned by queries in four languages : our target language , English ; a representative highresource language , French ; and two low-resource languages , Indonesian and Uzbek .
In total , we collected 36,050 judgments of whether the images returned by Google Image Search were a good match for the keyword .
Details on the experimental setup can be found in Section 1 of the Supplemental Materials .
Table 1 shows the fraction of images that were judged to be good representations of the search word .
It also demonstrates that as the concreteness of a word increases , the proportion of good images associated with that word increases as well .
We further discuss the role of concreteness in Section 6.1 .
Overall , 85 % of the English images , 72 % of French , 66 % of Indonesian , and 60 % of Uzbek were judged to be good .
Finding Translations Using Images
Can images help us learn translations for lowresource languages ?
In this section we replicate prior work in high- resource languages , and then evaluate on a wide array of low-resource languages .
Although we scraped images and text for 100 languages , we have selected a representative set of 32 for evaluation .
Kiela et al. ( 2015 ) established that CNN features are superior to the SIFT plus color histogram features used by Bergsma and Van Durme ( 2011 ) , and so we restrict all analysis to the former .
Translation Prediction with AVGMAX
To learn the English translation of each foreign word , we rank the English words as candidate translations based on their visual similarity with the foreign words .
We take the cosine similarity score for each image i f associated the foreign word w f with each of image i e for the English word w e , and then compute the average maximum similarity as AVGMAX ( w f , w e ) = 1 | w f | i f ?w f max ie ?
we ( cosine ( i f , i e ) )
Each image is represented by a 4096 - dimensional vector from the fully connected 7th ( FC7 ) layer of a CNN trained on ImageNet ( Krizhevsky et al. , 2012 ) .
AvgMax is the best-performing method described by Bergsma and Van Durme ( 2011 ) on images created with SIFT and color histogram features .
It was later validated on CNN features by Kiela et al . ( 2015 ) .
The number of candidate English words is the number of entries in the bilingual dictionary after filtering out dictionary entries where the English word and foreign word are identical .
In order to compare with Kiela et al . ( 2015 ) , we evaluate the models ' rankings using Mean Reciprocal Rank ( MRR ) , top - 1 , top - 5 and top - 20 accuracy .
We prefer the more interpretable top-k accuracy in our subsequent experiments .
We choose to follow Wijaya et al . ( 2017 ) in standardizing to k = 10 , and we report top - 1 accuracy only when it is particularly informative .
Replication of Prior Work
We evaluate on the five languages - Dutch , French , German , Italian , and Spanish - which have been the focus of prior work .
Table 2 shows the results reported by Kiela et al . ( 2015 ) on the BERGSMA500 dataset , along with results using our image crawl method ( Section 3.2 ) on BERGSMA500 's vocabulary .
On all five languages , our dataset performs better than that of Kiela et al . ( 2015 ) .
We attribute this to improvements in image search since they collected images .
We additionally note that in the BERGSMA500 vocabularies , approximately 11 % of the translation pairs are string -identical , like film ? film .
In all subsequent experiments , we remove trivial translation pairs like this .
We also evaluate the identical model on our full data set , which contains 8,500 words , covering all parts of speech and the full range of concreteness ratings .
The top - 1 accuracy of the model is 23 % on our more realistic and challenging data set , versus 68 % on the easier concrete nouns set .
High - and Low-resource Languages
To determine whether image- based translation is effective for low resource languages , we sample 12 high- resource languages ( HIGHRES ) , and 20 lowresource languages ( LOWRES ) .
Table 3 reports the top -10 accuracy across all 32 languages .
For each language , we predict a translation for each foreign word in the language 's CROWD - TRANS dictionary .
This comes to approximately 7,000 to 10,000 foreign words per language .
We find that high- resource languages ' image features are more predictive of translation than those of low-resource languages .
Top - 10 accuracy is 29 % averaged across high- resource languages , but only 16 % for low-resource languages .
This may be due to the quality of image search in each language , and the number of websites in each language indexed by Google , as suggested by Table 1 .
The difficulty of the translation task is dependent on the size of the English vocabulary used , as distinguishing between 5 , 000 English candidates as in Slovak is not as difficult as distinguishing between 10 , 000 words as in Tamil .
Large Target Vocabulary
How does increasing the number of candidate translations affect accuracy ?
Prior work used an English vocabulary of 500 or 1,000 words , where the correct English translation is guaranteed to appear .
This is unrealistic for many tasks such as machine translation , where the target language vocabulary is likely to be large .
To evaluate a more realistic scenario , we take the union of the English vocabulary of every dictionary in CROWDTRANS , and run the same translation experiments as before .
We call this large common vocabulary LARGEENG .
Confirming our intuition , experiments with LARGEENG give significantly lower top - 10 accuracies across parts of speech , but still provide discriminative power .
We find .181 average top - 10 accuracy using LARGEENG , whereas on the same languages , average accuracy on the CROWDTRANS vocabularies was .260 .
The full results for these experiments are reported in Table 4 .
Evaluation by Part-of-speech Can images be used to translate words other than nouns ?
This section presents our methods for de -
Table 4 : Top - 10 accuracy on the expanded English dictionary task .
For each experiment , 263,102 English words were used as candidate translations for each foreign word .
The SMALL average is given for reference , averaging the results from Table 3 across the same 10 languages .
termining part- of-speech for foreign words even in low-resource languages , and presents our imagebased translation results across part- of-speech .
Assigning POS Labels
To show the performance of our translation method for each particular POS , we first assign a POS tag to each foreign word .
Since we evaluate on highand low-resource languages , many of which do not have POS taggers , we POS tag English words , and transfer the tag to their translations .
We scraped the text on the web pages associated with the images of each English word , and collected the sentences that contained each query ( English ) word .
We chose to tag words in sentential context , rather than simply collecting parts of speech from a dictionary , because many words have multiple senses , often with different parts of speech .
We assign universal POS tags ( Petrov et al. , 2012 ) using spaCy 2 , giving each word its majority tag .
We gathered part- of-speech tags for 42 % of the English words in our translations .
Of the remaining untagged English entries , 40 % were multi-word expressions , and 18 % were not found in the text of the web pages that we scraped .
When transferring POS tags to foreign words , we only considered foreign words where every English translation had the same POS .
Across all 32 languages , on average , we found that , after filtering , 65 % of foreign words were nouns , 14 % were verbs , 14 % were adjectives , 3 % were adverbs , and 3 % were other ( i.e. they were labeled a different POS ) .
2 https://spacy.io
Accuracy by Part-of-speech
As we see in the results in Table 3 , the highest translation performance is obtained for nouns , which confirms the observation by Hartmann and S?gaard ( 2017 ) .
However , we see considerable signal in translating adjectives as well , with top - 10 accuracies roughly half that of nouns .
This trend extends to low-resource languages .
We also see that translation quality is relatively poor for adverbs and verbs .
There is higher variation in our performance on adverbs across languages , because there were relatively few adverbs ( 3 % of all words . )
From these results , it is clear that one can achieve higher accuracy by choosing to translate only nouns and adjectives .
Analysis by part- of-speech only indirectly addresses the question of when translation with images is useful .
For example , Figure 2 shows that nouns like concept translate incorrectly because of a lack of consistent visual representation .
However , verbs like walk may have concrete visual representation .
Thus , one might perform better overall at translation on concrete words , regardless of part- of-speech .
Evaluation by Concreteness
Can we effectively predict the concreteness of words in a variety of languages ?
If so , can these predictions be used to determine when translation via images is helpful ?
In this section , we answer both of these questions in the affirmative .
Predicting Word Concreteness
Previous work has used image dispersion as a measure of word concreteness ( Kiela et al. , 2014 ) .
We introduce a novel supervised method for predicting word concreteness that more strongly correlates with human judgements of concreteness .
To train our model , we took Brysbaert et al . ( 2014 ) 's dataset , which provides human judgments for about 40 k words , each with a 1 - 5 abstractnessto-concreteness score , and scraped 100 images from English Google Image Search for each word .
We then trained a two -layer perceptron with one hidden layer of 32 units , to predict word concreteness .
The inputs to the network were the elementwise mean and standard deviation ( concatenated into a 8094 - dimensional vector ) of the CNN features for each of the images corresponding to a word .
To better assess this image-only approach , we also experimented with using the distributional word embeddings of Salle et al . ( 2016 ) as input .
We used these 300 - dimensional vectors either seperately or concatentated with the image - based features .
Our final network was trained with a crossentropy loss , although an L2 loss performed nearly as well .
We randomly selected 39,000 words as our training set .
Results on the remaining held - out validation set are visualized in Figure 3 .
Although the concatenated image and word embedding features performed the best , we do not expect to have high-quality word embeddings for words in low-resource languages .
Therefore , for the evaluation in Section 6.2 , we used the imageembeddings -only model to predict concreteness for every English and foreign word in our dataset .
Accuracy by Predicted Concreteness
It has already been shown that the images of more abstract words provide a weaker signal for translation ( Kiela et al. , 2015 ) .
Using our method for predicting concreteness , we determine which images sets are most concrete , and thereby estimate the likelihood that we will obtain a high quality translation .
Figure 4 shows the reduction in translation accuracy as increasingly abstract words are included in the set .
The concreteness model can be used to establish recall thresholds .
For the 25 % of foreign words we predict to be most concrete , ( 25 % recall , ) AVGMAX achieves top - 10 accuracy of 47.0 % for high- resource languages and 32.8 % for lowresource languages .
At a 50 % most-concrete recall treshold , top - 10 translation accuracies are 25.0 % and 37.8 % for low - and high- resource languages respectively , compared to 18.6 % and 29.3 % at 100 % recall .
Translation with Images and Text Translation via image features performs worse than state - of - the - art distributional similarity - based methods .
For example , Wijaya et al. ( 2017 ) demonstrate top - 10 accuracies in range of above 85 % on the VULIC1000 a 1,000 - word dataset , whereas with only image features , Kiela et al . ( 2015 ) report top - 10 accuracies below 60 % .
However , there may be utility in combining the two methods , as it is likely that visual and textual distributional representations are contributing different information , and fail in different cases .
We test this intuition by combining image scores with the current state - of - the - art system of Wijaya et al . ( 2017 ) , which uses Bayesian Personalized Ranking ( BPR ) .
In their arXiv draft , Hartmann and S?gaard ( 2017 ) presented a negative result when directly combining image representations with distributional representations into a single system .
Here , we present a positive result by formalizing the problem as a reranking task .
Our intuition is that we hope to guide BPR , clearly the stronger system , with aid from image features and a predicted concreteness value , instead of joining them as equals and potentially washing out the stronger signal .
Reranking Model
For each foreign word w f and each English word w e , we have multiple scores for the pair p f,e = ( w f , w e ) , used to rank w e against all other w e ?
E , where E is the English dictionary used in the experiment .
Specifically , we have TXT ( p f,e ) and IM - AGE ( p f,e ) for all pairs .
For each foreign word , we also have the concreteness score , CNC ( w f ) , predicted from its image set by the method described in Section 6.1 .
We use a small bilingual dictionary , taking all pairs p f,e and labeling them { ? 1 } , with 1 denoting the words are translations .
We construct training data out of the dictionary , treating each labeled pair as an independent observation .
We then train a 2 - layer perceptron ( MLP ) , with 1 hidden layer of 4 units , to predict translations from the individual scores , minimizing the squared loss .
Once the model is trained , we fix each foreign word w f , and score all pairs ( w f , w e ) for all e ?
E , using the learned model MLP ( p f,e ) .
Using these scores , we sort E for each w f .
Evaluation
We evaluate our text - based and image- based combination method by translating Bosnian , Dutch , French , Indonesian , Italian , and Spanish into English .
For each language , we split our bilingual dictionary ( of 8,673 entries , on average ) into 2,000 entries for a testing set , 20 % for training the textbased BPR system , 35 % for training the reranking MLP , and the rest for a development set .
We filtered out multi-word phrases , and translations where w f and w e are string identical .
We compare three models : TXT is Wijaya et al . ( 2017 ) 's text - based state - of - the - art model .
TXT +IMG is our MLP - learned combination of the two features .
TXT +IMG + CNC uses our predicted concreteness of the foreign word as well .
We evaluate all models on varying percents of testing data sorted by predicted concreteness , as in Section 6.2 .
As shown in Figure 5 , both imageaugmented methods beat TXT across concreteness thresholds on the top - 1 accuracy metric .
Results across the 6 languages are reported in Table 5 .
Confirming our intuition , images are useful at high concreteness , improving the SOA textbased method 3.21 % at 10 % recall .
At 100 % recall our method with images still improves the SOA by 1.3 % .
For example , the text-only system translates the Bosnian word ko ?arka?
ki incorrectly as football , whereas the image + text system produces the correct basketball .
Further , gains are more pronounced for lowresource languages than for high- resource languages .
Concreteness scores are useful for highresource languages , for example Spanish , where TXT +IMG falls below TXT alone on more abstract words , but TXT +IMG + CNC remains an improvement .
Finally , we note that the text-only system also performs better on concrete words than abstract words , indicating a general trend of ease in translating concrete words regardless of method .
Summary
We have introduced a large-scale multilingual image resource , and used it to conduct the most comprehensive study to date on using images to learn translations .
Our Massively Multilingual Image Dataset will serve as a standard for future work in image - based translation due to its size and generality , covering 100 languages , hundreds of thousands of words , and a broad range of parts of speech .
Using this corpus , we demonstrated the substantial utility in supervised prediction of word concreteness when using image features , improving over the unsupervised state - of - the - art and finding that image - based translation is much more accurate for concrete words .
Because of the text we collected with our corpus , we were also able to collect partof-speech information and demonstrate that image features are useful in translating adjectives and nouns .
Finally , we demonstrate a promising path forward , showing that incorporating images can improve a state - of - the - art text - based word translation system .
Dataset and Code
The MMID will be distributed both in raw form and for a subset of languages in memory compact featurized versions from http : //multilingual-images.org along with code we used in our experiments .
Additional details are given in our Supplemental Materials doc-ument , which also describes our manual image annotation setup , and gives numerous illustrative examples of our system 's predictions .
Figure 2 : 2 Figure 2 : Shown here are five images for the abstract Indonesian word konsep , along with its top 4 ranked translations using CNN features .
The actual translation , concept , was ranked 3,465 .
3 MLP ( p f,e ) = MLP [ TXT ( p f,e ) ; IMAGE ( p f,e ) ; CNC ( w f ) ] = {?1 }
Figure 3 : Figure 4 : 34 Figure3 : Plots visualizing the distribution of concreteness predictions on the validation set for our three trained models and for image dispersion .
Spearman correlation coefficients are shown .
For the model trained only on images , the three worst failure cases are annotated .
False positives tend to occur when one concrete meaning of an abstract word dominates the search results ( i.e. many photos of " satisfyingly " show food ) .
False negatives often stem from related proper nouns or an overabundance of clipart , as is the case for " fishhook . "
Figure 5 : 5 Figure 5 : Reranking top - 1 and top - 10 accuracies of our image + text combination sytems compared to the text-only Bayesian Personalized Ranking system .
The X-axis shows percent of foreign words evaluated on , sorted by decreasing predicted concreteness .
Table 5 : 5 Top - 1 accuracy results across high-resource ( Dutch , French , Italian , Spanish ) and low-resource ( Bosnian , Indonesian ) languages .
Words evaluated on are again sorted by concreteness for the sake of analysis .
The best result on each % of test data is bolded .
TXT .746 .696 .673 High- Res TXT+IMG .771 .708 .678 TXT +IMG + Cnc .773 .714 .685 TXT .601 .565 .549 Low-Res TXT+IMG .646 .590 .562 TXT +IMG + Cnc .643 .589 .563
https://github.com/CLD2Owners/cld2
We use DyNet( Neubig et al. , 2017 ) for constructing and training our network with the Adam optimization method ( Kingma and Ba , 2014 ) .
