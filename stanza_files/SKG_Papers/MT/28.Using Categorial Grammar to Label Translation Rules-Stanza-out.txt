title
Using Categorial Grammar to Label Translation Rules
abstract
Adding syntactic labels to synchronous context-free translation rules can improve performance , but labeling with phrase structure constituents , as in GHKM ( Galley et al. , 2004 ) , excludes potentially useful translation rules .
SAMT ( Zollmann and Venugopal , 2006 ) introduces heuristics to create new non-constituent labels , but these heuristics introduce many complex labels and tend to add rarely - applicable rules to the translation grammar .
We introduce a labeling scheme based on categorial grammar , which allows syntactic labeling of many rules with a minimal , well - motivated label set .
We show that our labeling scheme performs comparably to SAMT on an Urdu-English translation task , yet the label set is an order of magnitude smaller , and translation is twice as fast .
Introduction
The Hiero model of Chiang ( 2007 ) popularized the usage of synchronous context-free grammars ( SCFGs ) for machine translation .
SCFGs model translation as a process of isomorphic syntactic derivation in the source and target language .
But the Hiero model is formally , not linguistically syntactic .
Its derivation trees use only a single non-terminal label X , carrying no linguistic information .
Consider Rule 1 . X ? maison ; house ( 1 ) We can add syntactic information to the SCFG rules by parsing the parallel training data and projecting parse tree labels onto the spans they yield and their translations .
For example , if house was parsed as a noun , we could rewrite Rule 1 as N ? maison ; house
But we quickly run into trouble : how should we label a rule that translates pour l'? tablissement de into for the establishment of ?
There is no phrase structure constituent that corresponds to this English fragment .
This raises a model design question : what label do we assign to spans that are natural translations of each other , but have no natural labeling under a syntactic parse ?
One possibility would be to discard such translations from our model as implausible .
However , such non-compositional translations are important in translation ( Fox , 2002 ) , and they have been repeatedly shown to improve translation performance ( Koehn et al. , 2003 ; DeNeefe et al. , 2007 ) . Syntax -Augmented Machine Translation ( SAMT ; Zollmann and Venugopal , 2006 ) solves this problem with heuristics that create new labels from the phrase structure parse : it labels for the establishment of as IN +NP + IN to show that it is the concatenation of a noun phrase with a preposition on either side .
While descriptive , this label is unsatisfying as a concise description of linguistic function , fitting uneasily alongside more natural labels in the phrase structure formalism .
SAMT introduces many thousands of such labels , most of which are seen very few times .
While these heuristics are effective ( Zollmann et al. , 2008 ) , they inflate grammar size , hamper effective parameter estimation due to feature sparsity , and slow translation speed .
Our objective is to find a syntactic formalism that enables us to label most translation rules without relying on heuristics .
Ideally , the label should be small in order to improve feature estimation and reduce translation time .
Furthering an insight that informs SAMT , we show that combinatory categorial grammar ( CCG ) satisfies these requirements .
Under CCG , for the establishment of is labeled with ( ( S\NP ) \( S\NP ) ) / NP .
This seems complex , but it describes exactly how the fragment should combine with other English words to create a complete sentence in a linguistically meaningful way .
We show that CCG is a viable formalism to add syntax to SCFG - based translation .
?
We introduce two models for labeling SCFG rules .
One uses labels from a 1 - best CCG parse tree of training data ; the second uses the top labels in each cell of a CCG parse chart . ?
We show that using 1 - best parses performs as well as a syntactic model using phrase structure derivations . ?
We show that using chart cell labels performs almost as well than SAMT , but the nonterminal label set is an order of magnitude smaller and translation is twice as fast .
Categorial grammar Categorial grammar ( CG ) ( Adjukiewicz , 1935 ; Bar-Hillel et al. , 1964 ) is a grammar formalism in which words are assigned grammatical types , or categories .
Once categories are assigned to each word of a sentence , a small set of universal combinatory rules uses them to derive a sentence -spanning syntactic structure .
Categories may be either atomic , like N , VP , S , and other familiar types , or they may be complex function types .
A function type looks like A/B and takes an argument of type B and returns a type A .
The categories A and B may themselves be either primitives or functions .
A lexical item is assigned a function category when it takes an argument - for example , a verb may be function that needs to be combined with its subject and object , or an a adjective may be a function that takes the noun it modifies as an argument .
We can combine two categories with function application .
Formally , we write X/ Y Y ? X ( 2 ) to show that a function type may be combined with its argument type to produce the result type .
Backward function application also exists , where the argument occurs to the left of the function .
Combinatory categorial grammar ( CCG ) is an extension of CG that includes more combinators ( operations that can combine categories ) .
Steedman and Baldridge ( 2011 ) give an excellent overview of CCG .
As an example , suppose we want to analyze the sentence " They own properties in various cities and villages " using the lexicon shown in Table 1 .
We assign categories according to the lexicon , then combine the categories using function application and other combinators to get an analysis of S for the complete sentence .
Figure 1 shows the derivation .
As a practical matter , very efficient CCG parsers are available ( Clark and Curran , 2007 ) .
As shown by Fowler and Penn ( 2010 ) , in many cases CCG is context-free , making it an ideal fit for our problem .
Labels for phrases Consider the German- English phrase pair der gro?e Mann - the tall man .
It is easily labeled as an NP and included in the translation table .
By contrast , der gro?e- the tall , does n't typically correspond to a complete subtree in a phrase structure parse .
Yet translating the tall is likely to be more useful than translating the tall man , since it is more general - it can be combined with any other noun translation .
Using CG -style labels with function types , we can assign the type ( for example ) NP / N to the tall to show that it can be combined with a noun on its right to create a complete noun phrase .
1
In general , CG can produce linguistically meaningful labels of most spans in a sentence simply as a matter of course .
Minimal , well - motivated label set
By allowing slashed categories with CG , we increase the number of labels allowed .
Despite the increase in the number of labels , CG is advantageous for two reasons : 1 . Our labels are derived from CCG derivations , so phrases with slashed labels represent wellmotivated , linguistically - informed derivations , and the categories can be naturally combined .
2 . The set of labels is small , relative to SAMTit's restricted to the labels seen in CCG parses of the training data .
In short , using CG labels allows us to keep more linguistically - informed syntactic rules without making the set of syntactic labels too big .
Translation models
Extraction from parallel text
To extract SCFG rules , we start with a heuristic to extract phrases from a word-aligned sentence pair 1 We could assign NP / N to the determiner the and N/N to the adjective tall , then combine those two categories using function composition to get a category NP / N for the two words together .
( Tillmann , 2003 ) .
Figure 2 shows a such a pair , with a consistent phrase pair inside the box .
A phrase pair ( f , e ) is said to be consistent with the alignment if none of the words of f are aligned outside the phrase e , and vice versa - that is , there are no alignment points directly above , below , or to the sides of the box defined by f and e. Given a consistent phrase pair , we can immediately extract the rule X ? f , e ( 3 ) as we would in a phrase - based MT system .
However , whenever we find a consistent phrase pair that is a sub-phrase of another , we may extract a hierarchical rule by treating the inner phrase as a gap in the larger phrase .
For example , we may extract the rule X ? Pour X ; For X ( 4 ) from Figure 3 .
The focus of this paper is how to assign labels to the left - hand non-terminal X and to the nonterminal gaps on the right - hand side .
We discuss five models below , of which two are novel CG - based labeling schemes .
Baseline : Hiero Hiero ( Chiang , 2007 ) uses the simplest labeling possible : there is only one non-terminal symbol , X , for all rules .
Its advantage over phrase - based translation in its ability to model phrases with gaps in them , enabling phrases to reorder subphrases .
However , since there 's only one label , there 's no way to include syntactic information in its translation rules .
Phrase structure parse tree labeling
One first step for adding syntactic information is to get syntactic labels from a phrase structure parse tree .
For each word-aligned sentence pair in our training data , we also include a parse tree of the target side .
Then we can assign syntactic labels like this : for each consistent phrase pair ( representing either the left- hand non-terminal or a gap in the right hand side ) we see if the target - language phrase is the exact span of some subtree of the parse tree .
If a subtree exactly spans the phrase pair , we can use the root label of that subtree to label the nonterminal symbol .
If there is no such subtree , we throw away any rules derived from the phrase pair .
As an example , suppose the English side of the phrase pair in Figure 3
The rules extracted by this scheme are very similar to those produced by GHKM ( Galley et al. , 2004 ) , in particular resulting in the " composed rules " of Galley et al . ( 2006 ) , though we use simpler heuristics for handling of unaligned words and scoring in order to bring the model in line with both Hiero and SAMT baselines .
Under this scheme we throw away a lot of useful translation rules that do n't translate exact syntactic constituents .
For example , we ca n't label X ?
Pour la majorit ?
des ; For most ( 6 ) because no single node exactly spans
For most : the PP node includes people , and the NP node does n't include For .
We can alleviate this problem by changing the way we get syntactic labels from parse trees .
SAMT
The Syntax -Augmented Machine Translation ( SAMT ) model ( Zollmann and Venugopal , 2006 ) extracts more rules than the other syntactic model by allowing different labels for the rules .
In SAMT , we try several different ways to get a label for a span , stopping the first time we can assign a label : ?
As in simple phrase structure labeling , if a subtree of the parse tree exactly spans a phrase , we assign that phrase the subtree 's root label .
?
If a phrase can be covered by two adjacent subtrees with labels A and B , we assign their concatenation A+B . ?
If a phrase spans part of a subtree labeled A that could be completed with a subtree B to its right , we assign A / B. ?
If a phrase spans part of a subtree A but is missing a B to its left , we assign A\B .
?
Finally , if a phrase spans three adjacent subtrees with labels A , B , and C , we assign A+B +C .
Only if all of these assignments fail do we throw away the potential translation rule .
Under SAMT , we can now label Rule 6 .
For is spanned by an IN node , and most is spanned by a JJ node , so we concatenate the two and label the rule as IN + JJ ?
Pour la majorit ?
des ; For most ( 7 )
CCG 1 - best derivation labeling Our first CG model is similar to the first phrase structure parse tree model .
We start with a word-aligned sentence pair , but we parse the target sentence using a CCG parser instead of a phrase structure parser .
When we extract a rule , we see if the consistent phrase pair is exactly spanned by a category generated in the 1 - best CCG derivation of the target sentence .
If there is such a category , we assign that category label to the non-terminal .
If not , we throw away the rule .
To continue our extended example , suppose the English side of Figure 3
This does not take advantage of CCG 's ability to label almost any fragment of language : the fragments with labels in any particular sentence depend on the order that categories were combined in the sentence 's 1 - best derivation .
We ca n't label Rule 6 , because no single category spanned For most in the derivation .
In the next model , we increase the number of spans we can label .
S/S S/S N ( S/S ) / N N/N N For people most Figure 4 : A portion of the parse chart for a sentence starting with " For most people . . . . "
Note that the gray chart cell is not included in the 1 - best derivation of this fragment in Section 3.5 .
CCG parse chart labeling For this model , we do not use the 1 - best CCG derivation .
Instead , when parsing the target sentence , for each cell in the parse chart , we read the most likely label according to the parsing model .
This lets us assign a label for almost any span of the sentence just by reading the label from the parse chart .
For example , Figure 4 represents part of a CCG parse chart for our example fragment of " For most people . "
Each cell in the chart shows the most probable label for its span .
The white cells of the chart are in fact present in the 1 - best derivation , which means we could extract Rule 8 just as in the previous model .
But the 1 - best derivation model cannot label Rule 6 , and this model can .
The shaded chart cell in Figure 4 holds the most likely category for the span For most .
So we assign that label to the X : S/S ?
Pour la majorit ?
des ; For most ( 9 ) By including labels from cells that were n't used in the 1 - best derivation , we can greatly increase the number of rules we can label .
Comparison of resulting grammars 4.1 Effect of grammar size and label set on parsing efficiency
There are sound theoretical reasons for reducing the number of non-terminal labels in a grammar .
Translation with a synchronous context-free grammar requires first parsing with the source - language projection of the grammar , followed by intersection of the target - language projection of the resulting grammar with a language model .
While there are many possible algorithms for these operations , they all depend on the size of the grammar .
Consider for example the popular cube pruning algorithm of Chiang ( 2007 ) , which is a simple extension of CKY .
It works by first constructing a set of items of the form A , i , j , where each item corresponds to ( possibly many ) partial analyses by which nonterminal A generates the sequence of words from positions i through j of the source sentence .
It then produces an augmented set of items A , i , j , u , v , in which items of the first type are augmented with left and right language model states u and v .
In each pass , the number of items is linear in the number of nonterminal symbols of the grammar .
This observation has motivated work in grammar transformations that reduce the size of the nonterminal set , often resulting in substantial gains in parsing or translation speed ( Song et al. , 2008 ; DeNero et al. , 2009 ; Xiao et al. , 2009 ) .
More formally , the upper bound on parsing complexity is always at least linear in the size of the grammar constant G , where G is often loosely defined as a grammar constant ; Iglesias et al . ( 2011 ) give a nice analysis of the most common translation algorithms and their dependence on G. Dunlop et al . ( 2010 ) provide a more fine- grained analysis of G , showing that for a variety of implementation choices that it depends on either or both the number of rules in the grammar and the number of nonterminals in the grammar .
Though these are worst- case analyses , it should be clear that grammars with fewer rules or nonterminals can generally be processed more efficiently .
Number of rules and non-terminals Table 2 shows the number of rules we can extract under various labeling schemes .
The rules were extracted from an Urdu-English parallel corpus with 202,019 translations , or almost 2 million words in each language .
As we described before , moving from the phrasestructure syntactic model to the extended SAMT model vastly increases the number of translation rules - from about 7 million to 40 million rules .
But the increased rule coverage comes at a cost : the non-terminal set has increased in size from 70 ( the size of the set of Penn Treebank tags ) to over 18,000 .
Comparing the phrase structure syntax model to the 1 - best CCG derivation model , we see that the number of extracted rules increases slightly , and the grammar uses a set of about 500 non-terminal labels .
This does not seem like a good trade- off ; since we are extracting from the 1 - best CCG derivation there really are n't many more rules we can label than with a 1 - best phrase structure derivation .
But when we move to the full CCG parse chart model , we see a significant difference : when reading labels off of the entire parse chart , instead of the 1 - best derivation , we do n't see a significant increase in the non-terminal label set .
That is , most of the labels we see in parse charts of the training data already show up in the top derivations : the complete chart does n't contain many new labels that have never been seen before .
But by using the chart cells , we are able to assign syntactic information to many more translation rules : over 28 million rules , for a grammar about 3 4 the size of SAMT 's .
The parse chart lets us extract many more rules without significantly increasing the size of the syntactic label set .
Sparseness of nonterminals Examining the histograms in Figure 5 gives us a different view of the non-terminal label sets in our models .
In each histogram , the horizontal axis measures label frequency in the corpus .
The height of each bar shows the number of non-terminals with that frequency .
For the phrase structure syntax model , we see there are maybe 20 labels out of 70 that show up on rules less than 1000 times .
All the other labels show up on very many rules .
Moving to SAMT , with its heuristically - defined labels , shows a very different story .
Not only does the model have over 18,000 non-terminal labels , but thousands of them show up on fewer than 10 rules apiece .
If we look at the rare label types , we see that a lot of them are improbable three way concatenations A+B +C .
The two CCG models have similar sparseness profiles .
We do see some rare labels occurring only a few times in the grammars , but the number of singleton labels is an order of magnitude smaller than SAMT .
Most of the CCG labels show up in the long tail of very common occurrences .
Interestingly , when we move to extracting labels from parse charts rather than derivations , the number of labels increases only slightly .
However , we also obtain a great deal more evidence for each observed label , making estimates more reliable .
Experiments
Data
We tested our models on an Urdu-English translation task , in which syntax - based systems have been quite effective ( Baker et al. , 2009 ; Zollmann et al. , 2008 )
Experimental design
We used the scripts included with the Moses MT toolkit to tokenize and normalize the English data .
We used a tokenizer and normalizer developed at the SCALE 2009 workshop ( Baker et al. , 2009 ) to preprocess the Urdu data .
We used GIZA ++ ( Och and Ney , 2000 ) to perform word alignments .
For phrase structure parses of the English data , we used the Berkeley parser ( Petrov and Klein , 2007 ) .
For CCG parses , and for reading labels out of a parse chart , we used the C&C parser ( Clark and Curran , 2007 ) .
After aligning and parsing the training data , we used the Thrax grammar extractor ( Weese et al. , 2011 ) to extract all of the translation grammars .
We used the same feature set in all the translation grammars .
This includes , for each rule C ? f ; e , relative -frequency estimates of the probabil -
The feature set also includes lexical weighting for rules as defined by Koehn et al . ( 2003 ) and various binary features as well as counters for the number of unaligned words in each rule .
To train the feature weights we used the Z-MERT implementation ( Zaidan , 2009 ) of the Minimum Error-Rate Training algorithm ( Och , 2003 ) .
To decode the test sets , we used the Joshua machine translation decoder ( Weese et al. , 2011 ) .
The language model is a 5 - gram LM trained on English GigaWord Fourth Edition .
5
Evaluation criteria
We measure machine translation performance using the BLEU metric ( Papineni et al. , 2002 ) .
We also report the translation time for the test set in seconds per sentence .
These results are shown in Table 3 .
All of the syntactic labeling schemes show an improvement over the Hiero model .
Indeed , they all fall in the range of approximately 27 - 28 BLEU .
We can see that the 1 - best derivation CCG model performs slightly better than the phrase structure model , and the CCG parse chart model performs a little better than that .
SAMT has the highest BLEU score .
The models with a larger number of rules perform better ; this supports our assertion that we should n't throw away too many rules .
When it comes to translation time , the three smaller models ( Hiero , phrase structure syntax , and CCG 1 - best derivations ) are significantly faster than the two larger ones .
However , even though the CCG parse chart model is almost 3 4 the size of SAMT in terms of number of rules , it does n't take 3 4 of the 5 LDC2009T13 time .
In fact , it takes only half the time of the SAMT model , thanks to the smaller rule label set .
6 Discussion and Future Work Finding an appropriate mechanism to inform phrasebased translation models and their hierarchical variants with linguistic syntax is a difficult problem that has attracted intense interest , with a variety of promising approaches including unsupervised clustering ( Zollmann and Vogel , 2011 ) , merging ( Hanneman et al. , 2011 ) , and selection ( Mylonakis and Sima'an , 2011 ) of labels derived from phrasestructure parse trees very much like those used by our baseline systems .
What we find particularly attractive about CCG is that it naturally assigns linguistically -motivated labels to most spans of a sentence using a reasonably concise label set , possibility obviating the need for further refinement .
Indeed , the analytical flexibility of CCG has motivated its increasing use in MT , from applications in language modeling Hassan et al. , 2007 ) to more recent proposals to incorporate it into phrase - based ( Mehay , 2010 ) and hierarchical translation systems ( Auli , 2009 ) .
Our new model builds on these past efforts , representing a more fully instantiated model of CCGbased translation .
We have shown that the label scheme allows us to keep many more translation rules than labels based on phrase structure syntax , extracting almost as many rules as the SAMT model , but keeping the label set an order of magnitude smaller , which leads to more efficient translation .
This simply scratches the surface of possible uses of CCG in translation .
In future work , we plan to move from a formally context-free to a formally CCGbased model of translation , implementing combinatorial rules such as application , composition , and type-raising .
Figure 2 : 2 Figure 2 : A word-aligned sentence pair fragment , with a box indicating a consistent phrase pair .
Figure 3 : 3 Figure3 : A consistent phrase pair with a sub-phrase that is also consistent .
We may extract a hierarchical SCFG rule from this training example .
assign syntactic labels to Rule 4 to produce PP ? Pour NP ; For NP ( 5 )
Then just as in the phrase structure model , we project the syntactic labels down onto the extractable rule yielding S/S ? Pour N ; For N ( 8 )
Figure 5 : 5 Figure 5 : Histograms of label frequency for each model , illustrating the sparsity of each model .
. The training corpus was the National Institute of Standards and Technology Open Machine Translation 2009 Evaluation ( NIST Open MT09 ) .
According to the MT09 Constrained Training Con-ditions Resources list 2 this data includes NIST Open MT08 Urdu Resources 3 and the NIST Open MT08 Current Test Set Urdu-English 4 .
This gives us 202,019 parallel translations , for approximately 2 million words of training data .
Table 1 : 1 An example lexicon , mapping words to categories .
Lexical item Category and conj cities NP in ( NP \NP ) / NP own ( S\NP ) / NP properties NP they NP various NP / NP villages NP
Omar F.Zaidan . 2009 .
Z-MERT : A fully configurable open source tool for minimum error rate training of machine translation systems .
The Prague Bulletin of Mathematical Linguistics , 91 ( 1):79 - 88 .
Andreas Zollmann and Ashish Venugopal .
2006 .
Syntax augmented machine translation via chart parsing .
In Proceedings on the Workshop on Statistical Machine Translation .
Andreas Zollmann and Stephan Vogel . 2011 .
A wordclass approach to labeling PSCFG rules for machine translation .
In Proc. of ACL -HLT .
Andreas Zollmann , Ashish Venugopal , Franz Och , and Jay Ponte . 2008 .
A systematic comparison of phrasebased , hierarchical and syntax -augmented statistical MT .
In Proc. of COLING .
http://www.itl.nist.gov/iad/mig/tests/
mt/2009/MT09_ConstrainedResources.pdf 3 LDC2009E12 4 LDC2009E11
