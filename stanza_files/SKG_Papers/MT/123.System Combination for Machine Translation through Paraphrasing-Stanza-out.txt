title
System Combination for Machine Translation through Paraphrasing
abstract
In this paper , we propose a paraphrasing model to address the task of system combination for machine translation .
We dynamically learn hierarchical paraphrases from target hypotheses and form a synchronous context- free grammar to guide a series of transformations of target hypotheses into fused translations .
The model is able to exploit phrasal and structural system-weighted consensus and also to utilize existing information about word ordering present in the target hypotheses .
In addition , to consider a diverse set of plausible fused translations , we develop a hybrid combination architecture , where we paraphrase every target hypothesis using different fusing techniques to obtain fused translations for each target , and then make the final selection among all fused translations .
Our experimental results show that our approach can achieve a significant improvement over combination baselines .
Introduction
In the past several years , many machine translation ( MT ) combination approaches have been developed .
Word- level combination approaches , such as the confusion network decoding model , have been quite successful ( Matusov et al. , 2006 ; Rosti et al. , 2007a ; He et al. 2008 ; Karakos et al. 2008 ; Chen et al. 2009a ; Narsale 2010 ; Leusch 2011 ; Freitag et al. 2014 ) .
In addition to word-level combination approaches , some phrase - level combination approaches have also recently been developed ; the goal is to retain coherence and consistency be-tween the words in a phrase .
The most common phrase-level combination approaches are redecoding methods : by constructing a new phrase table from each MT system 's source - to - target phrase alignments , the source sentence can also be re-decoded using the new translation table ( Rosti et al. , 2007 b ; Huang and Papineni , 2007 ; Chen et al. , 2007 ; Chen et al. , 2009 b ) .
One problem with these approaches is that , just with a new phrase table , existing information about word ordering present in the target hypotheses is not utilized ; thus the approaches are likely to make new mistakes of word reordering which do not appear in the target hypotheses of MT engines .
Huang and Papineni ( 2007 ) attacked this issue through a reordering cost function that encourages search along with decoding paths from all MT engines ' decoders .
Another phrase-level combination approach relies on a lattice decoding model to carry out the combination ( Feng et al 2009 ; Du and Way 2010 ; Ma and McKeown 2012 ) .
In a lattice , each edge is associated with a phrase ( a single word or a sequence of words ) rather than a single word .
The construction of the lattice is based on the extraction of phrase pairs from word alignments between a selected best MT system hypothesis ( the backbone ) and the other translation hypotheses .
One challenge of the lattice decoding model is that it is difficult to consider structural consensus among target hypotheses from multiple MT engines , i.e , the consensus among occurrences of discontinuous words .
In this paper , we propose another phrase -level combination approacha paraphrasing model using hierarchical paraphrases ( paraphrases contain subparaphrases ) , to fuse target hypotheses .
We dynamically learn hierarchical paraphrases from target hypotheses without any syntactic annotations and form a synchronous context-free grammar ( SCFG ) ( Aho and Ullman 1969 ) to guide a series of transformations of target hypotheses into fused translations .
Through these structural transformations , the paraphrasing model is able to exploit phrasal and structural system-weighted consensus and also able to utilize existing information about word ordering present in the target hypotheses .
In addition , to consider a diverse set of plausible fused translations , we develop a hybrid combination architecture , where we paraphrase every target hypothesis using different fusing techniques to obtain fused translations for each target , and then make the final selection among all fused translations through a sentence - level selection - based model .
In short , compared with other related work , our approach features the following advantages : 1 . It can consider structural system-weighted consensus among target hypotheses from multiple MT engines through its hierarchical paraphrases , which non-hierarchical paraphrases are not able to do .
2 . It can utilize existing information about word ordering present in the target hypotheses .
3 . It can retain coherence and consistency between the words in a phrase .
4 . The hybrid combination architecture enables us to consider a diverse set of plausible fused translations produced by different fusing techniques .
Hybrid Combination Architecture
In the context of system combination , discriminative reranking or post editing , MT researchers ( Rosti et al. , 2007a ; Huang and Papineni , 2007 ; Devlin and Matsoukas , 2012 , Matusov et al. , 2008 ; Gimpel et al. , 2013 ) have recently shown many positive results if more diverse translations are considered .
Inspired by them , we develop a hybrid combination architecture in order to consider more diverse fused translations .
We paraphrase every target hypothesis to obtain the corresponding fused translation , and then make the final selection among all fused translations through a sentence - level selection - based model , shown in Figure 1 .
In the architecture , different fusing techniques can be used to generate fused translations for the further sentence - level selection , enabling us to exploit more sophisticated information of the whole sentence .
Paraphrasing Model
In this section , we introduce our paraphrasing model .
For each single target hypothesis , we extract a set of hierarchical paraphrases from monolingual word alignments between the hypothesis and other hypotheses .
Each set of hierarchical paraphrases forms a synchronous context-free grammar to guide a series of transformations of that target hypothesis into a fused translation .
Any monolingual word aligner can be used to produce the monolingual word alignments .
In our system , we adopt TERp ( Snover et al. 2009 ) , one of the state - of - the - art alignment tools , to serve this purpose .
TERp is an extension of TER ( Snover et al. 2006 ) .
Both TERp and TER are automatic evaluation metrics for MT , based on measuring the ratio of the number of edit operations between the reference sentence and the MT system hypothesis .
The edit operations of TERp include TER 's Matches , Insertions , Deletions , Substitutions and Shifts - as well as three new edit operations : Stem Matches , Synonym Matches and Paraphrases .
A valuable side product of TERp is the monolingual word alignment .
A constructed example is shown in Figure 2 .
Hierarchical Paraphrase Extraction
We first introduce our notation .
For a given sentence i , we use i h E to denote the target hypothesis from MT system h , use ?
If ? ?
i k i h ep ep , is consistent 1 with the monolingual word alignment , then ? ?
i k i h e ep X , is added to i k h Q , . ?
If ? ? ? ? , X is in i k h Q , , and ? ?
i k i h ep ep , is consistent with monolingual word alignment such that is added to i k h Q , , where a is an index .
Please note that for each extracted hierarchical paraphrase - ? ? ? ? , X , ? would include information of word positions while ? would not .
For a certain target hypothesisi h EP , our goal is to paraphrase it to get the fusion output by using a set of hierarchical paraphrases , denoted by i h Q .
Thus we create the union of all related hierarchical paraphrases learned from i h EP and other target hypotheses .
Two special " glue " rules - ? ? 2 1 2 1 X , X S S S and ? ?
1 1 X , X S are also added to i h Q .
The process can be represented formally in the following : ? ? } X , X , X , X { 1 1 2 1 2 1 1 , ? ? ? ? ? ?
S S S S Q Q N k i k h i h where N is the total number of MT systems .
An Example Figure 2 . A constructed example of a sentence - " ?
( the book that you bought ) " and its translations from three MT systemsi
E 1 , i E 2 and i E 3 , and word alignments between i E 2 and i E 1 , and between i E 2 and i E 3 , obtained through TERp .
We use a Chinese-to - English example in Figure 2 to illustrate the extraction process .
The extract - 1
This means that words in a legal paraphrase are not aligned to words outside of the paraphrase , and should include at least one pair of words aligned with each other .
ed hierarchical paraphrases to paraphrase i EP 2 -" you 1 buy 2 the 3 book 4 " are shown in Table 1 .
Because of limited space , only part of the paraphrases , i.e , part of the rules of i Q 2 , are shown .
i Q 2 of rules of part i Q 1 , 2 in ?
i Q 2 , 2 in ?
i Q 3 , 2 in ? ? ? you , you X 1 ( a ) ? ? ? ? ? buy you , buy you X 2 1 ( b ) ? ? ? bought you , buy you X 2 1 ( c ) ? ? ? bring you , buy you X 2 1 ( d ) ? ? ? book , book X 4 ( e ) ? ? ? ? books , book X 4 ( f ) ? ? ? book the , book the X 4 3 ( g ) ? ? ? ? books the , book the X 4 3 ( h ) ? ( i ) bought that you books the , book the buy you X 4 3 2 1 ? ? ? ? ? 1 4 3 1 X that books the , book the X X ( j ) ? ? ? bought that you X the , X the buy you X 1 1 3 2 1 ( k ) ? ? ? 1 2 2 3 1 X that X the , X the X X ( l ) ? ? Table 1 . Part of extracted hierarchical paraphrases to paraphrase i EP 2 , i.e , part of the rules of i Q 2 .
Note that , in Table 1 , the rules ( j ) , ( k ) and ( l ) can be regarded as structural paraphrases , and they utilize existing information about word ordering present in the target hypotheses .
Since rule ( l ) is included in both i Q 1 , 2 and i Q 3 , 2 , we can say that rule ( l ) has more structural consensus than rule ( j ) and ( k ) .
And rule ( l ) also models the word reordering through reversing the order of X1 and X2 .
By the example , we can see the reason why our model is able to exploit structural consensus and also to utilize existing information about word ordering present in the target hypotheses .
Decoding Given a certain target hypothesisi h EP , and its set of hierarchical paraphrasesi h Q , the decoder aims to paraphrase i h EP using i h Q by performing a search for the single most probable derivation via the CKY algorithm with a Viterbi approximation .
The derivation is the paraphrased result , i.e , the fusion result indicated in Figure 1 .
The single most probable derivation can be represented as ? ? ? ? ? otherwise 0 if 1 ) , ( , , , i k h j i h j i h Q q k q f the i E 1 i E 2 i E 3 ) ( * ) ) ( log ( * * ) , ( * max arg ) | ( log max arg 1 1 , i h w i h l p J j N k j i h k E i h i h E E length E LM J k q f EP E p i h i h ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? where j i h q , is the jth paraphrase in i h Q used to generate i h E , J is the number of paraphrases used to generate i h E .
N is the total number of MT systems .
k ? is the weight of MT system k , in charge of the system-weighted consensus .
p ? is phrase penalty .
l ? is the LM weight and w ? is word penalty .
All weights are trained discriminatively for Bleu score using Minimum Error Rate Training ( MERT ) procedure ( Och 2004 ) .
The ideal result of paraphrasing i EP 2 is shown in the following , which is supposed to be generated with a higher chance if , regardless of system weights .
That is because of the use of the rules with higher degree of structural consensus , such as ( l ) and ( e ) .
Sentence -Level Selection - based Model
For a given sentence i and its M multiple fusion outputs - i f E , M f ? ?
1 generated by the paraphrasing model or the lattice decoding model , the goal here is to select the best one among them , as shown in Figure 1 ( For the case shown in the figure , M is 2N ) .
The idea is to compare systemweighted consensus among all fusion outputs and translations from all MT systems , and then select the one with the highest consensus .
We adopt Minimum Bayes Risk ( MBR ) decoding ( Kumar and Byrne , 2004 ; Sim et al. , 2007 ) to serve our purpose and develop the following TER - based MBR : where TER is Translation Tdit Ratio .
Experiments
Our experiments are conducted and reported on three datasets :
The first dataset includes Chinese-English system translations and reference translations from DARPA GALE 2008 ( GALE Chi-Eng ) .
The second dataset includes Chinese-English system translations and reference translations and from NIST 2008 ( NIST Chi-Eng ) .
And the third dataset includes Arabic-English system translations and reference translations and from NIST 2008 ( NIST Ara-Eng ) .
Table 3 lists distinguishing machine translation approaches of top five MT of GALE Chi-Eng Dataset .
And " rwth - pbt - sh " performs the best in Bleu score .
Two combination baselines are implemented for comparison : one is an implementation based on confusion network decoding , and the other is Lattice Decoding from ( Ma and McKeown 2012 ) , both of which are using TERp to obtain word alignments between a selected backbone hypothesis and other target hypotheses .
The former uses these word alignments to construct a confusion network while the latter extracts phrases which are consistent with these word alignments to construct a lattice .
For both baselines , backbone hypotheses are selected sentence by sentence based on system-weighted consensus among translation of all MT systems .
Results In Table 4 From Table 4 , we can first observe that , for the three datasets , Backbone_PARA and Back- bone_LD outperform Backbone_CN , which shows the advantage of using phrases over words in combination .
However , Backbone_PARA does not show improvement over Backbone_LD .
The reason could be that selected backbones already have a high level of quality and fewer words need to be replaced or re-ordered in contrast with other target hypotheses .
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
N k i k i f k M m i m i f m i f l f i f f E E TER E E TER E LM E p 1 1 ) ) , ( 1 We find that Arch_PARA performs better than Backbone_PARA , and Arch_LD performs better than Backbone_LD .
This observation supports our claim that it is beneficial to consider more diverse sets of plausible fused translations .
Arch_LD_PARA achieves the best performance among all techniques used in this paper .
It not only supports our claim , but also brings a conclusion that the paraphrasing model and lattice decoding can compensate for the weaknesses of the other in our architecture .
Since the paraphrasing model uses hierarchical paraphrases to carry out the fusion , it is able to make a bigger degree of word- reordering or structural change on the input hypothesis in comparison with lattice decoding .
We suppose that when more word- reordering and structural changes are needed , paraphrasing model can bring more benefits than lattice decoding .
Because the quality of a given translation hypothesis is highly related to word reordering and structural change , it can be expected that when a poorly translated hypothesis is paraphrased , paraphrasing model can bring more benefits than lattice decoding .
In order to obtain the evidence to support this hypothesis , we carried out the following experiment on NIST Chi-Eng Dataset .
For each MT system from the selected top 5 system A-E , we paraphrase its translations using the paraphrasing model and lattice decoding separately , aiming to compare the performances of the two models on each MT system .
In other words , we do not first do backbone selection .
Every MT system 's translation is regarded as a backbone .
The results are shown in Table 5 . Table 5 .
The Bleu score of each MT system , the Bleu score of paraphrasing each MT system using lattice decoding and the Bleu score of paraphrasing each MT system using paraphrasing model .
Among the five MT systems , " Sys C " and " Sys D " perform poorer than the other three MT systems .
When we paraphrase the two systems , we find that paraphrasing model outperforms lattice decoding .
These results support our hypothesis that when more word- reordering and structural changes are needed , paraphrasing model can bring more benefits than lattice decoding .
Conclusion
We view MT combination as a paraphrasing process using a set of hierarchical paraphrases , in which more complicated paraphrasing phenomena are able to be modeled , such as phrasal and structural consensus .
Existing information about word ordering present in the target hypotheses are also considered .
The experimental results show that our approach can achieve a significant improvement over combination baselines .
There are many possibilities for enriching the simple framework .
Many ideas from recent translation developments can be borrowed and modified for combination .
Our future work aims to incorporate syntactic or semantic information into our paraphrasing framework .
Figure 1 . 1 Figure 1 .
An example of hybrid combination architecture
