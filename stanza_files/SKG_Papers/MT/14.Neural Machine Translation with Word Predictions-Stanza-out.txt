title
Neural Machine Translation with Word Predictions
abstract
In the encoder-decoder architecture for neural machine translation ( NMT ) , the hidden states of the recurrent structures in the encoder and decoder carry the crucial information about the sentence .
These vectors are generated by parameters which are updated by back - propagation of translation errors through time .
We argue that propagating errors through the end-to - end recurrent structures are not a direct way of control the hidden vectors .
In this paper , we propose to use word predictions as a mechanism for direct supervision .
More specifically , we require these vectors to be able to predict the vocabulary in target sentence .
Our simple mechanism ensures better representations in the encoder and decoder without using any extra data or annotation .
It is also helpful in reducing the target side vocabulary and improving the decoding efficiency .
Experiments on Chinese-English and German-English machine translation tasks show BLEU improvements by 4.53 and 1.3 , respectively .
Introduction
The encoder-decoder based neural machine translation ( NMT ) models have been developing rapidly .
propose to encode the source sentence as a fixed - length vector representation , based on which the decoder generates the target sequence , where both the encoder and decoder are recurrent neural networks ( RNN ) or their variants Chung et al. , 2014 ; .
In this framework , the fixedlength vector plays the crucial role of transition - ing the information of the sentence from the source side to the target side .
Later , attention mechanisms are proposed to enhance the source side representations Luong et al. , 2015 b ) .
The source side context is computed at each time -step of decoding , based on the attention weights between the source side representations and the current hidden state of the decoder .
However , the hidden states in the recurrent decoder still originate from the single fixed - length representation ( Luong et al. , 2015 b ) , or the average of the bi-directional representations .
Here we refer to the representation as initial state .
Interestingly , Britz et al. ( 2017 ) find that the value of initial state does not affect the translation performance , and prefer to set the initial state to be a zero vector .
On the contrary , we argue that initial state still plays an important role of translation , which is currently neglected .
We notice that beside the end-to - end error back propagation for the initial and transition parameters , there is no direct control of the initial state in the current NMT architectures .
Due to the large number of parameters , it may be difficult for the NMT system to learn the proper sentence representation as the initial state .
Thus , the model is very likely to get stuck in local minimums , making the translation process arbitrary and unstable .
In this paper , we propose to augment the current NMT architecture with a word prediction mechanism .
More specifically , we require the initial state of the decoder to be able to predict all the words in the target sentence .
In this way , there is a specific objective for learning the initial state .
Thus the learnt source side representation will be better constrained .
We further extend this idea by applying the word predictions mechanism to all the hidden states of the decoder .
So the transition between different decoder states could be controlled as well .
Our mechanism is simple and requires no additional data or annotation .
The proposed word predictions mechanism could be used as a training method and brings no extra computing cost during decoding .
Experiments on the Chinese-English and German-English translation tasks show that both the constraining of the initial state and the decoder hidden states bring significant improvement over the baseline systems .
Furthermore , using the word prediction mechanism on the initial state as a word predictor to reduce the target side vocabulary could greatly improve the decoding efficiency , without a significant loss on the translation quality .
Related Work Many previous works have noticed the problem of training an NMT system with lots of parameters .
Some of them prefer to use the dropout technique ( Srivastava et al. , 2014 ; Luong et al. , 2015 b ; Meng et al. , 2016 ) .
Another possible choice is to ensemble several models with random starting points Jean et al. , 2015 ; Luong and Manning , 2016 ) .
Both techniques could bring more stable and better results .
But they are general training techniques of neural networks , which are not specifically targeting the modeling of the translation process like ours .
We will make empirical comparison with them in the experiments .
The way we add the word prediction is similar to the research of multi-task learning .
Dong et al. ( 2015 ) propose to share an encoder between different translation tasks .
Luong et al. ( 2015a ) propose to jointly learn the translation task for different languages , the parsing task and the image captioning task , with a shared encoder or decoder .
Zhang and Zong ( 2016 ) propose to use multitask learning for incorporating source side monolingual data .
Different from these attempts , our method focuses solely on the current translation task , and does not require any extra data or annotation .
In the other sequence to sequence tasks , Suzuki and Nagata ( 2017 ) propose the idea for predicting words by using encoder information .
However , the purpose and the way of our mechanism are different from them .
The word prediction technique has been applied in the research of both statistical machine transla-tion ( SMT ) ( Bangalore et al. , 2007 ; Mauser et al. , 2009 ; Jeong et al. , 2010 ; Tran et al. , 2014 ) and NMT ( Mi et al. , 2016 ; L'Hostis et al. , 2016 ) .
In these research , word prediction mechanisms are employed to decide the selection of words or constrain the target vocabulary , while in this paper , we use word prediction as a control mechanism for neural model training .
Notations and Backgrounds
We present a popular NMT framework with the encoder-decoder architecture and the attention networks ( Luong et al. , 2015 b ) , based on which we propose our word prediction mechanism .
Denote a source- target sentence pair as {x , y} from the training set , where x is the source word sequence ( x 1 , x 2 , ? ? ? , x | x| ) and y is the target word sequence ( y 1 , y 2 , ? ? ? , y |y | ) , |x | and |y | are the length of x and y , respectively .
In the encoding stage , a bi-directional recurrent neural network is used to encode x into a sequence of vectors ( h 1 , h 2 , ? ? ? , h | x | ) .
For each x i , the representation h i is : h i = [ ? ? h i ; ? ? h i ] ( 1 ) where [ ? ; ?] denotes the concatenation of column vectors ; ? ? h i and ? ? h i denote the hidden vectors for the word x i in the forward and backward RNNs , respectively .
The gated recurrent unit ( GRU ) is used as the recurrent unit in each RNN , which is shown to have promising results in speech recognition and machine translation .
Formally , the hidden state h i at time step i of the forward RNN encoder is defined by the GRU function g? ? e ( ? , ? ) , as follows : ? ? h i = g? ? e ( ? ? h i?1 , emb x i ) ( 2 ) = ( 1 ? ? ? z i ) ? ? ? h i?1 + ? ? z i ? ? ? h ? i ? ? z i = ?( ? ? W z [ emb x i ; ? ? h i?1 ] ) ( 3 ) ? ? h ?
i = tanh ( ? ? W[ emb x i ; ( ? ? r i ? ? ? h i?1 ) ] ) ( 4 ) ? ? r i = ?( ? ? W r [ emb x i ; ? ? h i?1 ] ) ( 5 ) where ? denotes element - wise product between vectors and emb x i is the word embedding of the x i . tanh ( ? ) and ?(? ) are the tanh and sigmoid transformation functions that can be applied elementwise on vectors , respectively .
For simplicity , we omit the bias term in each network layer .
The backward RNN encoder is defined likewise .
In the decoding stage , the decoder starts with the initial state s 0 , which is the average of source representations . s 0 = ?( W s 1 | x| | x | ? i=1 h i ) ( 6 ) At each time step j , the decoder maximizes the conditional probability of generating the jth target word , which is defined as : P ( y j |y <j , x ) = f d ( t d ( [ emb y j?1 ; s j ; c j ] ) ) ( 7 ) f d ( u ) = softmax ( W f u ) ( 8 ) t d ( v ) = tanh ( W t v ) ( 9 ) where s j is the decoder 's hidden state , which is computed by another GRU ( as in Equation 2 ) : s j = g d ( s j?1 , [ emb y j?1 ; c j ] ) ( 10 ) and the context vector c j is from the attention mechanism ( Luong et al. , 2015 b ) : c j = | x | ?
i=1 a ji h i ( 11 ) a ji = exp ( e ji ) ? | x| k=1 exp( e jk ) ( 12 ) e ji = tanh ( W att d [ s j?1 ; h i ] ) . ( 13 )
NMT with Word Predictions
Word Prediction for the Initial State
The decoder starts the generation of target sentence from the initial state s 0 ( Equation 6 ) generated by the encoder .
Currently , the update for the encoder only happens when a translation error occurs in the decoder .
The error is propagated through multiple time steps in the recurrent structure until it reaches the encoder .
As there are hundreds of millions of parameters in the NMT system , it is hard for the model to learn the exact representation of source sentences .
As a result , the values of initial state may not be exact during the translation process , leading to poor translation performances .
We propose word prediction as a mechanism to control the values of initial state .
The intuition is that since the initial state is responsible for the translation of whole target sentence , it should at least contain information of each word in the target sentence .
Thus , we optimize the initial state by making prediction for all target words .
For simplicity , we assume each target word is independent of each other .
Here the word prediction mechanism is a simpler sub-task of translation , where the order of words is not considered .
The prediction task could be trained jointly with the translation task in a multi-task learning way ( Luong et al. , 2015a ; Dong et al. , 2015 ; Zhang and Zong , 2016 ) , where both tasks share the same encoder .
In other words , word prediction for the initial state could be interpreted as an improvement for the encoder .
We denote this mechanism as WP E .
As shown in Figure 1 , a prediction network is added to the initial state .
We define the conditional probability of WP E as follows : P WP E ( y|x ) = |y | ? j=1 P WP E ( y j | x ) ( 14 ) P WP E (y j | x ) = f p ( t p ( [ s 0 ; c p ] ) ) ( 15 ) where f p ( ? ) and t p ( ? ) are the softmax layer and non-linear layer as defined in Equation 8 - 9 , with different parameters ; c p is defined similar as the attention network , so the source side information could be enhanced .
c p = | x | ?
i=1 a i h i ( 16 ) a i = exp ( e i ) ? | x| k=1 exp( e k ) ( 17 ) e i = tanh ( W attp [ s 0 , h i ] ) . ( 18 )
Word Predictions for Decoder 's Hidden States Similar intuition is also applied for the decoder .
Because the hidden states of the decoder are responsible for the translation of target words , they should be able to predict the target words as well .
The only difference is that we remove the already generated words from the prediction task .
So each hidden state in the decoder is required to predict the target words which remain untranslated .
For the first state s 1 of the decoder , the prediction task is similar with the task for the initial state .
Since then , the prediction is no longer a separate training task , but integrated into each time step of the training process .
We denote this mechanism as WP D .
As shown in Figure 2 , for each time step j in the decoder , the hidden state s j is used for the prediction of ( y j , y j+1 , ? ? ? , y |y | ) .
The conditional probability of WP D is defined as : P WP D (y j , y j+1 , ? ? ? , y |y | |y <j , x ) ( 19 ) = |y | ? k=j P WP D (y k |y <j , x ) P WP D (y k |y <j , x ) =f d ( p( t d ( [ emb y j?1 ; s j ; c j ] ) ) ) ( 20 ) where f d ( ? ) and t d ( ? ) are the softmax layer and non-linear layer as defined in Equation 8 - 9 ; p( ? ) is another non-linear transformation layer , which prepares the current state for the prediction : p( u ) = tanh ( W p u ) . ( 21 )
Training NMT models optimize the networks by maximizing the likelihood of the target translation y given source sentence x , denoted by L T . L T = 1 |y | |y | ? j=1 log P ( y j |y <j , x ) ( 22 ) where P ( y j |y <j , x ) is defined in Equation 7 .
To optimize the word prediction mechanism , we propose to add extra likelihood functions L WP E and L WP D into the training procedure .
For the WP E , we directly optimize the likelihood of translation and word prediction : L 1 = L T + L WP E ( 23 ) L WP E = log P WP E ( 24 ) where P WP E is defined in Equation 14 .
For the WP D , we optimize the likelihood as : L 2 = L T + L WP D ( 25 ) L WP D = |y | ? j=1 1 |y | ? j + 1 log P WP D ( 26 ) where P WP D is defined in Equation 19 ; the coefficient of the logarithm is used to calculate the average probability of each prediction .
The two mechanisms could also work together , so that both the encoder and the decoder could be improved : L 3 = L T + L WP E + L WP D . ( 27 )
Making Use of the Word Predictor
The previously proposed word prediction mechanism could be used only as a extra training objective , which will not be computed during the translation .
Thus the computational complexity of our models for translation stays exactly the same .
On the other hand , using a smaller and specific vocabulary for each sentence or batch will improve translation efficiency .
If the vocabulary is accurate enough , there is also a chance to improve the translation quality ( Jean et al. , 2015 ; Mi et al. , 2016 ; L'Hostis et al. , 2016 ) .
Our word prediction mechanism WP E provides a natural solution for generating a possible set of target words at sentence level .
The prediction could be made from the initial state s 0 , without using extra resources such as word dictionaries , extracted phrases or frequent word lists , as in Mi et al . ( 2016 ) .
Experiments
Data
We perform experiments on the Chinese-English ( CH - EN ) and German-English ( DE - EN ) machine translation tasks .
For the CH-EN , the training data consists of about 8 million sentence pairs 1 . We use NIST MT02 as our validation set , and the NIST MT03 , MT04 and MT05 as our test sets .
These sets have 878 , 919 , 1597 and 1082 source sentences , respectively , with 4 references for each sentence .
For the DE-EN , the experiments trained on the standard benchmark WMT14 , and it has about 4.5 million sentence pairs .
We use newstest 2013 ( NST13 ) as validation set , and newstest 2014 ( NST14 ) as test set .
These sets have 3000 and 2737 source sentences , respectively , with 1 reference for each sentence .
Sentences were encoded using byte-pair encoding ( BPE ) ( Britz et al. , 2017 ) .
Systems and Techniques
We implement a baseline system with the bidirectional encoder and the attention mechanism ( Luong et al. , 2015 b ) as described in Section 3 , denoted as baseNMT .
Then our proposed word prediction mechanism on initial state and hidden states of decoder are implemented on the baseNMT system , denoted as WP E and WP D , respectively .
We denote the system use both techniques as WP ED .
We implement systems with variable - sized vocabulary following ( Mi et al. , 2016 ) .
For comparison , we also implement systems with dropout ( with dropout rate 0.5 on the output layer ) and ensemble ( ensemble of 4 systems at the output layer ) techniques .
Implementation Details Both our CH-EN and DE-EN experiments are implemented on the open source toolkit dl4mt 2 , with most default parameter settings kept the same .
We train the NMT systems with the sentences of length up to 50 words .
The source and target vocabularies are limited to the most frequent 30 K words for both Chinese and English , respectively , with the out- of- vocabulary words mapped to a special token UNK .
The dimension of word embedding is set to 512 and the size of the hidden layer is 1024 .
The recurrent weight matrices are initialized as random orthogonal matrices , and all the bias vectors as zero .
Other parameters are initialized by sampling from the Gaussian distribution N ( 0 , 0.01 ) .
We use the mini-batch stochastic gradient descent ( SGD ) approach to update the parameters , with a batch size of 32 .
The learning rate is controlled by AdaDelta ( Zeiler , 2012 ) .
For efficient training of our system , we adopt a simple pre-train strategy .
Firstly , the baseNMT system is trained .
The training results are used as the initial parameters for pre-training our proposed models with word predictions .
For decoding during test time , we simply decode until the end-of-sentence symbol eos occurs , using a beam search with a beam width of 5 .
Translation Experiments
To see the effect of word predictions in translation , we evaluate these systems in case-insensitive IBM - BLEU ( Papineni et al. , 2002 ) BLEU , but the improvement is smaller than on the baseNMT .
On the DE-EN experiments , the phenomenon of experiments is similar to CH -EN experiments .
The baseNMT system improves 0.94 through dropout method and 0.9 BLEU through ensemble method .
The dropout technique also does not work on WP ED and the ensemble technique improves 1.79 BLEU .
These comparisons suggests that our system already learns better and stable values for the parameters , enjoying some of the benefits of general training techniques like dropout and ensemble .
Compared to dropout and ensemble , our method WP ED achieves the highest improvement against the baseline system on both CH -EN and DE -EN experiments .
Along with ensemble method , the improvement could be up to 5.79 BLEU and 1.79 BLEU respectively .
Word Prediction Experiments
Since we include an explicit word prediction mechanism during the training of NMT systems , we also evaluate the prediction performance on the CH-EN experiments to see how the training is improved .
For each sentence in the test set , we use the initial state of the given model to make prediction about the possible words .
We denote the set of top n words as T n , the set of words in all the references as R .
We define the precision , recall of the word prediction as follows : precision = | T n ? R| | T n | * 100 % ( 28 ) recall = | T n ? R| | R | * 100 % ( 29 )
We compare the prediction performance of baseNMT and WP E . WP ED has similar prediction results with WP E , so we omit its results .
As shown in Table 5 , baseNMT system has a relatively lower prediction precision , for example , 45 % in top 10 prediction .
With an explicit training , the WP E could achieve a much higher precision in all conditions .
Specifically , the precision reaches 73 % in top 10 .
This indicates that the initial state in WP E contains more specific information about the prediction of the target words , which may be a step towards better semantic representation , and leads to better translation quality .
Because the total words in the references are limited ( around 50 ) , the precision goes down , as expected , when a larger prediction set is considered .
On the other hand , the recall of WP E is also much higher than baseNMT .
When given 1 k predictions , WP E could successfully predict 89 % of the words in the reference .
The recall goes up to 95 % with 5 k predictions , which is only 1/6 of the current vocabulary .
To analyze the process of word prediction , we draw the attention heatmap ( Equation 16 ) between the initial state s 0 and the bi-directional representation of each source side word h i for an example sentence .
As shown in Figure 3 , both examples show that the initial states have a very strong attention with all the content words in the source sentence .
The blank cells are mostly functions words or high frequent tokens such as " ? ( ' s ) " , " ? ( is ) " , " ? ( and ) " , " ? ( it ) " , comma and period .
This indicates that the initial state successfully encodes information about most of the content words in the source sentence , which contributes for a high prediction performance and leads to better translation .
Improving Decoding Efficiency
To make use of the word prediction , we conduct experiments using the predicted vocabulary , with different vocabulary size ( 1 k to 10k ) on the CH-EN experiments , denoted as WP E - V and WP ED -V .
The comparison is made in both translation quality and decoding time .
As all our models with fixed vocabulary size have exactly the same number of parameters for decoding ( extra mechanism is used only for training ) , we only plot the decoding time of the WP ED for comparison .
Figure 4 and 5 show the results .
When we start the experiments with top 1 k vocabulary ( 1/30 of the baseline settings ) , the translation quality of both WP E - V and WP ED - V are already higher than the baseNMT ; while their decoding time is less than 1/3 of an NMT system with 30 k vocabulary .
When the size of vocabulary increases , the translation quality improves as well .
With a 6 k predicted vocabulary ( 1/5 of the baseline settings ) , the decoding time is about 60 % of a full - vocabulary system ; the performances of both systems with variable size vocabulary are comparable their corresponding fixed - vocabulary systems , which is higher than the baseNMT by 2.53 and 4.53 BLEU , respectively .
Although the comparison may not be fair enough due to the language pair and training conditions , the above relative improvements ( e.g. WP ED - V v.s. baseNMT ) is much higher than previous research of manipulating the vocabularies ( Jean et al. , 2015 ; Mi et al. , 2016 ; L'Hostis et al. , 2016 ) .
This is because our mechanism is not only about reducing the vocabulary itself for each sentence or batch , it also brings improvement to the overall translation model .
Please note that un- like these research , we keep the target vocabulary to be 30 k in all our experiments , because we are not focusing on increasing the vocabulary size in this paper .
It will be interesting to combine our mechanism with larger vocabulary to further enhance the translation performance .
Again , our mechanism requires no extra annotation , dictionary , alignment or separate discriminative predictor , etc .
Translation Analysis
We also analyze real-case translations to see the difference between different systems ( Table 6 ) .
It is easy to see that the baseNMT system misses the translations of several important words , such as " advertising " , " 1.5 " , which are marked with underline in the reference .
It also wrongly translates the company name " time warner inc. " as the redundant information " internet company " ; " america online " as " us line " .
The results of dropout or ensemble show improvement compared to the baseNMT .
But they still make mistakes about the translation of " online " and the company name " time warner inc. " .
With WP ED , most of these errors no longer exist , because we force the encoder and decoder to carry the exact information during translation .
Conclusions
The encoder-decoder architecture provides a general paradigm for learning machine translation from the source language to the target language .
However , due to the large amount of parameters and relatively small training data set , the end-toend learning of an NMT model may not be able to learn the best solution .
We argue that at least part of the problem is caused by the long error backpropagation pipeline of the recurrent structures in multiple time steps , which provides no direct control of the information carried by the hidden states in both the encoder and decoder .
Instead of looking for other annotated data , we notice that the words in the target language sentence could be viewed as a natural annotation .
We propose to use the word prediction mechanism to enhance the initial state generated by the encoder and extend the mechanism to control the hidden states of decoder as well .
Experiments show promising results on the Chinese-English and German-English translation tasks .
As a byproduct , the word predictor could be used to improve the efficiency of decoding , which may be Table 6 : Comparisons of different systems in translating the same example sentence , which from CH -EN test sets .
( " source " indicates the source sentence ; " reference " indicates the human translation ; the translation results are indicated by their system names , including our best " WP ED " systems .
The underline words in the reference are missed in the baseNMT output ; the bold font indicates improvements over the baseNMT system ; and the italic font indicates remaining translation errors . ) crucial for large scale applications .
Our attempts demonstrate that the learning of the large scale neural network systems is still not good enough .
In the future , it might be helpful to analyze the benefits of jointly learning other related tasks together with machine translation , to provide further control of the learning process .
It is interesting to demonstrate the effectiveness of the proposed mechanism on other sequence to sequence learning tasks as well .
Figure 1 : 1 Figure 1 : The NMT model with word prediction for the initial state .
Figure 2 : 2 Figure 2 : The NMT model with word predictions for the decoder 's hidden states .
Figure 3 : 3 Figure 3 : Two examples of the attention heatmap between the initial state s 0 and the bi-directional representation of each source side word h i from the CH -EN test sets .
( The English translation of each source word is annotated in the parentheses after it . )
Figure 4 : 4 Figure 4 : BLEU scores with different vocabulary sizes for each sentence on the CH-EN experiments .
( The performance of baseNMT , WP E , WP D , WP ED are plotted as horizontal lines for comparison . )
source ? , ? ? ? ? ? ? ? ? reference america online , the internet arm of time warner conglomerate , said it expects advertising and commerce revenue to decline from us $ 2.7 billion in 2001 to us $ 1.5 in 2002 . baseNMT in the us line , the internet company 's internet company said on the internet that it expected that the business sales in 2002 would fall from $ UNK billion to $ UNK billion in 2001 . baseNMT + dropout on the united states line , UNK 's internet company said on the internet that it expects to reduce the annual advertising and commercial sales from $ UNK billion in 2001 to $ 1.5 billion .
baseNMT
+ ensemble in the us line , the internet company 's internet company said that it expected that the advertising and commercial sales volume for 2002 would be reduced from us $ UNK billion to us $ 1.5 billion in 2001 . WP ED the internet company of time warner inc. , the us online , said that it expects that the advertising and commercial sales in 2002 will decrease from $ UNK billion in 2001 to us $ 1.5 billion .
Table 1 : 1 Case-insensitive 4 - gram BLEU scores of baseNMT , WP E , WP D , WP ED systems on the CH-EN experiments .
( The " IMP " column presents the improvement of test average compared to the baseNMT . ) on both CH-EN
Table 2 : 2 Case-insensitive 4 - gram BLEU scores of baseNMT , WP E , WP D , WP ED systems on the DE-EN experiments .
Models Test IMP baseNMT 20.68 ? WP ED 21.98 + 1.3 baseNMT -dropout 21.62 +0.94 WP ED - dropout 21.71 + 1.03 baseNMT -ensemble ( 4 ) 21.58 + 0.9 WP ED - ensemble ( 4 ) 22.47 + 1.79 Models Test IMP baseNMT 34.86 ? WP ED 39.49 + 4.53 baseNMT -dropout 37.02 +2.06 WP ED - dropout 39.25 +4.29 baseNMT -ensemble ( 4 ) 37.71 + 2.75 WP ED - ensemble ( 4 ) 40.75 + 5.79
Table 3 : Average case-insensitive 4 - gram BLEU scores on the CH-EN experiments for baseNMT and WP ED systems , with the dropout and ensemble techniques .
tions to the hidden states in the decoder ( WP D ) leads to further improvements against baseNMT ( 4.15 BLEU ) , because WP D adds constraints to the state transitions through different time steps in the decoder .
Using both techniques improves the baseline by 4.53 BLEU .
On the DE-EN ex- periments , the improvement of WP E model is 0.41 BLEU and WP D model is 0.86 BLEU on test set .
When use both techniques , the WP ED improves on the test set is 1.3 BLEU .
We compare our models with systems using dropout and ensemble techniques .
The results show in Table 3 and 4 .
On the CH-EN experi- ments , the dropout method successfully improves the baseNMT system by 2.06 BLEU .
However , it does not work on our WP ED system .
The en- semble technique improves the baseNMT system by 2.75 BLEU .
It still improves WP ED by 1.26
Table 4 : 4 Case-insensitive 4 - gram BLEU scores on the DE-EN experiments for baseNMT and WP ED systems , with the dropout and ensemble techniques .
Table 5 : 5 Comparison between baseNMT and WP E in precision and recall for the different prediction size on the CH-EN experiments .
top-n baseNMT Prec. Recall Prec. Recall WP E top -10 45 % 17 % 73 % 30 % top -20 33 % 21 % 63 % 43 % top -50 21 % 30 % 41 % 55 % top -100 14 % 39 % 28 % 68 % top -1 k 2 % 67 % 4 % 89 % top -5 k 0.7 % 84 % 0.9 % 95 % top -10k 0.4 % 90 % 0.5 % 97 %
includes LDC2002E18 , LDC2003E07 , LDC2003E14 , LDC2004E12 , LDC2004T08 , LDC2005T06 , LDC2005T10 , LDC2006E26 and LDC2007T09
https://github.com/nyu-dl/dl4mt-tutorial
