title
Twitter Translation using Translation - Based Cross-Lingual Retrieval
abstract
Microblogging services such as Twitter have become popular media for real-time usercreated news reporting .
Such communication often happens in parallel in different languages , e.g. , microblog posts related to the same events of the Arab spring were written in Arabic and in English .
The goal of this paper is to exploit this parallelism in order to eliminate the main bottleneck in automatic Twitter translation , namely the lack of bilingual sentence pairs for training SMT systems .
We show that translation - based cross-lingual information retrieval can retrieve microblog messages across languages that are similar enough to be used to train a standard phrasebased SMT pipeline .
Our method outperforms other approaches to domain adaptation for SMT such as language model adaptation , meta-parameter tuning , or self-translation .
Introduction
Among the various social media platforms , microblogging services such as Twitter 1 have become popular communication tools .
This is due to the easy accessibility of microblogging platforms via internet or mobile phones , and due to the need for a fast mode of communication that microblogging satisfies :
Twitter messages are short ( limited to 140 characters ) and simultaneous ( due to frequent updates by prolific microbloggers ) .
Twitter users form a social network by " following " the updates of other users , either reciprocal or one-way .
The topics discussed in Twitter messages range from private chatter to important real-time witness reports .
Events such as the Arab spring have shown the power and also the shortcomings of this new mode of communication .
Microblogging services played a crucial role in quickly spreading the news about important events , furthermore they were useful in helping organizers plan their protest .
The fact that news on microblogging platforms is sometimes ahead of newswire is one of the most interesting facets of this new medium .
However , while Twitter messaging is happening in multiple languages , most networks of " friends " and " followers " are monolingual and only about 40 % of all messages are in English 2 .
One solution to sharing news quickly and internationally was crowdsourcing manual translations , for example at Meedan 3 , a nonprofit organization built to share news and opinion between the Arabic and English speaking world , by translating articles and blogs , using machine translation and human expert corrections .
The goal of our research is to automate this translation process , with a further aim of providing rapid crosslingual data access for downstream applications .
The automated translation of microblogging messages is facing two main problems .
First , there are no bilingual sentence pair data from microblogging domains available .
Second , the colloquial , nonstandard language of many microblogging messages makes it very difficult to adapt a machine translation system trained on any of the available bilingual resources such as transcriptions from political organizations or news text .
The approach presented in this paper aims to exploit the fact that microblogging often happens in parallel in different languages , e.g. , microblog posts related to the same events of the Arab spring were published in parallel in Arabic and in English .
The central idea is to crawl a large set of topically related Arabic and English microblogging messages , and use Arabic microblog messages as search queries in a cross-lingual information retrieval ( CLIR ) setup .
We use the probabilistic translation - based retrieval technique of Xu et al . ( 2001 ) that naturally integrates translation tables for cross-lingual retrieval .
The retrieval results are then used as input to a standard SMT pipeline to train translation models , starting from unsupervised induction of word alignments ( Och and Ney , 2000 ) to phrase-extraction ( Och and Ney , 2004 ) and phrase - based decoding .
We investigate several filtering techniques for retrieval and phrase extraction ( Munteanu and Marcu , 2006 ; Snover et al. , 2008 ) and find a straightforward application of phrase extraction from symmetrized alignments to be optimal .
Furthermore , we compare our approach to related domain adaptation techniques for SMT and find our approach to yield large improvements over all related techniques .
Finally , a side-product of our research is a corpus of around 1,000 Arabic Twitter messages with 3 manual English translations each , which were created using crowdsourcing techniques .
This corpus is used for development and testing in our experiments .
Related Work SMT for user- generated noisy data has been pioneered at the 2011 Workshop on Statistical Machine Translation that featured a translation task of Haitian Creole emergency SMS messages 4 .
This task is very similar to the problem of Twitter translation since SMS contain noisy , abbreviated language .
The research papers related to the featured translation task deploy several approaches to domain adaptation , including crowdsourcing ( Hu et al. , 2011 ) or extraction of parallel sentences from comparable data ( Hewavitharana et al. , 2011 ) .
The use of crowdsourcing to evaluate machine translation and to build development sets was pioneered by Callison - Burch ( 2009 ) and Zaidan and Callison - Burch ( 2009 ) .
Crowdsourcing has its limits when it comes to generating parallel training data on the scale of millions of parallel sentences .
In our work , we use crowdsourcing via Amazon Mechanical Turk 5 to create a development and test corpus that includes 3 English translations for each of around 1,000 Arabic microblog messages .
There is a substantial amount of previous work on extracting parallel sentences from comparable data such as newswire text ( Fung and Cheung , 2004 ; Munteanu and Marcu , 2005 ; Tillmann and ming Xu , 2009 ) and on finding parallel phrases in non-parallel sentences ( Munteanu and Marcu , 2006 ; Quirk et al. , 2007 ; Cettolo et al. , 2010 ; Vogel and Hewavitharana , 2011 ) .
The approach that is closest to our work is that of Munteanu and Marcu ( 2006 ) :
They use standard information retrieval together with simple word - based translation for CLIR , and extract phrases from the retrieval results using a clean bilingual lexicon and an averaging filter .
In this approach , filtering and cleaning techniques in alignment and phrase extraction have to compensate for low-quality retrieval results .
In our approach , the focus is on high-quality retrieval .
As our experimental results show , the main improvement of our technique is a decrease in out-ofvocabulary ( OOV ) rate at an increase of the percentage of correctly translated unigrams and bigrams .
Similar work on solving domain adaptation for SMT by mining unseen words has been presented by Snover et al . ( 2008 ) and Daum? and Jagarlamudi ( 2011 ) .
Both approaches show improvements by adding new phrase tables ; however , both approaches rely on techniques that require larger comparable texts for mining unseen words .
Since in our case documents are very short ( they consist of 140 character sequences ) , these techniques are not applicable .
However , the advantage of the fact that microblog messages resemble sentences is that we can apply standard word - and phrase - alignment techniques directly to the retrieval results .
Further approaches to domain adaptation for SMT include adaptation using in - domain language models ( Bertoldi and Federico , 2009 ) , meta-parameter tuning on in- domain development sets ( Koehn and Schroeder , 2007 ) , or translation model adaptation using self-translations of in- domain source language texts ( Ueffing et al. , 2007 ) .
In our experiments we compare our approach to these domain adaptation techniques .
Cross-Lingual Retrieval via Statistical Translation
Retrieval Model
In our approach , comparable candidates for domain adaptation are selected via cross-lingual retrieval .
In a probabilistic retrieval framework , we estimate the probability of a relevant document microblog message D given a query microblog message Q , P ( D|Q ) .
Following Bayes rule , this can be simplified to ranking documents according to the likelihood P ( Q|D ) if we assume a uniform prior over documents .
score ( Q , D ) = P ( D|Q ) = P ( D ) P ( Q|D ) P ( Q ) ( 1 ) Our model is defined as follows : score ( Q , D ) = P ( Q|D ) = q?Q P ( q|D ) ( 2 ) P ( q|D ) = ? Pmix ( q |D ) mixture model + ( 1 ? ? ) PML ( q |C ) query collection backoff ( 3 ) Pmix ( q | D ) = ? d?D T ( q|d ) PML ( d|D ) translation model ( 4 ) + ( 1 ? ? ) PML ( q|D ) self-translation
Our retrieval model is related to monolingual retrieval models such as the language -modeling approach of Ponte and Croft ( 1998 ) and the monolingual statistical translation approach of Berger and Lafferty ( 1999 ) .
Xu et al. ( 2001 ) extend the former approaches to the cross-lingual setting by adding a term translation table .
They describe their model in terms of a Hidden Markov Model with two states that generate query terms :
First , a document state generates terms d in the document language and then translates them into a query term q.
Second , a backoff state generates query terms q directly in the query language .
In the document state the probability of emitting q depends on all d that translate to q , according to a translation distribution T .
This is estimated by marginalizing out d as d T ( q|d ) P ( d|D ) .
In the backoff state the probability P M L ( q|C ) of emitting a query term is estimated as the relative frequency of this term within a corpus in the query language .
The probability of transitioning into the document state or the backoff state is given by ? and 1 ? ?.
We view this model from a smoothing perspective where the backoff state is linearly interpolated with the translation probability using a mixture weight ? to control the weighting between both terms .
Furthermore , we expand Xu et al . ( 2001 ) 's generative model to incorporate the concept of " selftranslation " , introduced by Xue et al . ( 2008 ) in a monolingual question - answering context :
Twitter messages across languages usually share relevant terms such as hashtags , named entities or user mentions .
Therefore , we model the event of a query term literally occurring in the document in a separate model that is itself linearly interpolated with a parameter ? with the translation model .
We implemented the model based on a Lucene 6 index , which allows efficient storage of termdocument and document -term vectors .
To minimize retrieval time , we consider only those documents as retrieval candidates where at least one term translates to a query term , according to the translation table T .
Stopwords were removed for both queries and documents .
Compared to common inverted index retrieval implementations , our model is quite slow since the document - term vectors have to be loaded .
However , multi-threading support and batch retrieval on a Hadoop cluster made the model tractable .
On the upside , the translationbased model allows greater precision in finding the candidates for comparable microblog messages than simpler approaches that use a combination of tfidf matching and n-best query term expansion :
The translation - based retrieval exploits all possible alignments between query and document terms which is particularly important for short documents such as microblog messages .
In -Domain Phrase Extraction
To prepare the extraction of phrases from retrieval results , we conducted cross-lingual retrieval in both directions : retrieving Arabic documents using English microblog messages as queries and vice versa .
For each run we kept the top N retrieved documents .
Each document was then paired with its query to generate pseudo-parallel data .
We tried two approaches for using this data to improve our translations .
The first , more restrictive method makes use of the word alignments we obtained from 5.8 million clean parallel training data from the NIST evaluation campaign .
The retrieval step generates word-alignments in the direction D ? Q. After retrieval , the reverse alignment for each query - document pair is also generated by using a translation table in the direction Q ? D .
An alignment point between a query term q and a docu - ment term d is created , iff T ( q|d ) or T ( d|q ) exist in the translation tables D ? Q or Q ? D. Based on these word-alignments , we extract phrases by applying the grow-diag-final - and heuristic and using Och and Ney ( 2004 ) 's phrase extraction algorithm as implemented in Moses 7 .
We conducted experiments using different constraints on the number of alignment points required for a pair to be considered as well as the value of N .
Our first technique resembles the technique of Munteanu and Marcu ( 2006 ) who also perform phrase extraction by combining clean alignment lexica for initial signals with heuristics to smooth alignments for final fragment extraction .
While we obtained some gains using our heuristics , we are aware that our method is severely restricted in that it only learns new words which are in the vicinity of known words .
We therefore also tried the bolder approach of treating our data as parallel and running unsupervised word alignment 8 ( Och and Ney , 2000 ) directly on the query - document pairs to obtain new world alignments and build a phrase table .
In contrast to previous work ( Snover et al. , 2008 ; Daum? and Jagarlamudi , 2011 ) , we can take advantage of the sentence - like character of microblog messages and treat queries and retrieval results similar to sentence aligned data .
For both extraction methods , the standard five translation features from the new phrase table ( phrase translation probability and lexical weighting in both directions , phrase penalty ) were added to the translation features in Moses .
We tried different al - Gaddafi , al - Qaddhafi , assad , babrain , bahrain , egypt , gadaffi , gaddaffi , gaddafi , Gheddafi , homs , human rights , human-rights , humanrights , libia , libian , libya , libyan , lybia , lybian , lybya , lybyan , manama , Misrata , nabeelrajab , nato , oman , Pos-itiveLibyaTweets , Qaddhafi , sirte , syria , tripoli , tripolis , yemen ; Table 1 : Keywords used for Twitter crawl .
modes of combining new and original phrase table , namely using either one or using the new phrase table as backoff in case no phrase translation is found in the original phrase table .
Data
Twitter Crawl
We crawled Twitter messages from September 20 , 2011 until January 23 , 2012 via the Streaming API 9 in keyword - tracking mode , obtaining 25.5 M
Twitter messages ( tweets ) in various languages .
Table 1 shows the list of keywords that were chosen to retrieve microblog messages related to the events of the Arab spring .
10
In order to separate the microblog message corpus by languages , we applied a Naive Bayes language identifier 11 .
This yielded a distribution with the six most common languages ( of 52 ) being Arabic ( 57 % ) , English ( 33 % ) , Somali ( 2 % ) , Spanish ( 2 % ) , Indonesian ( 1.5 % ) , German ( 0.7 % ) .
We kept only microblog messages classified as English or Arabic with confidence greater 0.9 .
Keyword - based crawling creates a strong bias towards the domain of the keywords and it does not guarantee that all microblog messages regarding a certain topic or region are retrieved or that all retrieved messages are related to the Arab Spring and human righs in the middle east .
Additionally , retweets artificially in - 9 https://dev.twitter.com/docs/ streaming - api/ 10
The Twitter Streaming API allows up to 400 tracking keywords that are matched to uppercase , lowercase and quoted variations of the keywords .
Partial matching such as " tripolis " matching " tripoli " as well as Arabic Unicode characters are not supported .
We extended our keywords over time by analyzing the crawl , e.g. , by introducing spelling variants and hashtags .
Creating a Small Parallel Twitter Corpus using Crowdsourcing
For the evaluation of our method , a small amount of parallel in-domain data was required .
Since there are no corpora of translated microblog messages , we decided to use Amazon Mechanical Turk 12 to create our own evaluation set , following the exploratory work of Zaidan and Callison - Burch ( 2011 b ) .
We randomly selected 2,000 Arabic microblog messages .
Hashtags , user mentions and URLs were removed from each microblog message beforehand , because they do not need to be translated and would just artificially inflate scores at test time .
The microblog messages were then manually cleaned and pruned .
We discarded messages which contained almost no text or large portions of other languages and removed remaining Twitter markup .
In the end , 1,022 microblog messages were used in the Mechanical Turk task .
We split the data into batches of ten sentences which comprised one HIT ( human intelligence task ) .
Each HIT had to be completed by three workers .
In order to have some control over translation quality , we inserted one control sentence per HIT , taken from the LDC - GALE Phase 1 Arabic Blog Parallel Text .
Turkers were rewarded 10 cents per translation .
Following Zaidan and Callison-Burch ( 2011 b ) , all Arabic sentences were converted into images in order to prevent turkers from pasting them into online machine translation engines .
Our final corpus consists of 1,022 translated microblog messages with three translations each .
An example containing translations for one of the sentences which we inserted for quality checking purposes , along with the reference translation , is given in table 3 .
It can be seen that translators sometimes made grammar mistakes or odd word choices .
They also tended to omit punctuation marks .
However , translations also contained reasonable translation alternatives ( such as " gathered " or " collected " ) .
We also asked translators to insert an " unknown " token whenever they were unable to translate a word .
Our HIT setup did not allow workers to skip a sentence , forcing them to complete an entire batch .
In order to account for translation variants we decided to use all three translations obtained via Mechanical Turk as multiple references instead of just keeping the top translation .
We randomly split our small parallel corpus , using half of the microblog messages for development and half for testing .
Preprocessing Besides removal of Twitter markup , several additional preprocessing steps such as digit normalization were applied to the data .
We also decided to apply the Buckwalter Arabic transliteration scheme 13 to avoid encoding difficulties .
Habash and Sadat ( 2006 ) have shown that tokenization is helpful for translating Arabic .
We therefore decided to apply a more involved tokenization scheme than simple whitespace splitting to our data .
As the retrieval relies on translation tables , all data need to be tokenized the same way .
We are aware of the MADA+ TOKAN Arabic morphological analyzer and tokenizer ( Habash and Rambow , 2005 ) , however , this toolkit produces very in - depth analyses of the data and thus led to difficulties when we tried to scale it to millions of sentences / microblog messages .
That is why we only used MADA for transliteration and chose to implement the simpler approach by Lee et al . ( 2003 ) for tokenization .
This approach only requires a small set of annotated data to obtain a list of prefixes and suffixes and uses n-REFERENCE breaking the silence , a campaign group made up of israeli soldiers , gathered anonymous accounts from 26 soldiers .
TRANSLATION1 and breaking silence is a group of israeli soldiers that had unknown statistics from 26 soldiers israeli TRANSLATION2 breaking the silence by a group of israeli soldiers who gathered unidentified statistics from 26 israeli soldier .
TRANSLATION3 breaking the silence is a group of israeli soldiers that collected unknown statistics of 26 israeli soldiers gram-models to determine the most likely prefix * stem-suffix * split of a word .
14
Twitter Translation Experiments
We conducted a series of experiments to evaluate our strategy of using CLIR and phrase-extraction to extract comparable data in the Twitter domain .
We also explored more standard ways of domain adaptation such as using English microblog messages to build an in-domain language model , or generating synthetic bilingual corpora from monolingual data .
All experiments were conducted using the Moses machine translation system 15 with standard settings .
Language models were built using the SRILM toolkit 16 ( Stolcke , 2002 ) .
For all experiments , we report lowercased BLEU - 4 scores ( Papineni et al. , 2001 ) as calculated by Moses ' multi- bleu script .
For assessing significance , we apply the approximate randomization test ( Noreen , 1989 ; Riezler and Maxwell , 2005 ) .
We consider pairwise differing results scoring a p-value < 0.05 as significant .
Our baseline model was trained using 5,823,363 million parallel sentences in Modern Standard Arabic ( MSA ) ( 198,500,436 tokens ) and English ( 193,671,201 tokens ) from the NIST evaluation campaign .
This data contains parallel text from different domains , including UN reports , newsgroups , newswire , broadcast news and weblogs .
Domain Adaption using Monolingual Resources
As a first step , we used the available in- domain data for a combination of domain adaptation tech -
14
The n-gram-model required for tokenization was trained on 5.8 million Modern Standard Arabic sentences from the NIST evaluation campaign .
This data had previously been tokenized with the same method , trained to match the Penn Arabic Treebank , v3 . 15 http://statmt.org/moses/ 16 http://www.speech.sri.com/projects/ srilm / niques similar to Bertoldi and Federico ( 2009 ) .
There were three different adaptation measures :
First , the turker - generated development set was used for optimizing the weights of the decoding metaparameters , as introduced by Koehn and Schroeder ( 2007 ) .
Second , the English microblog messages in our crawl were used to build an in-domain language model .
This adaptation technique was first proposed by Zhao et al . ( 2004 ) .
Third , the Arabic portion of our crawl was used to synthetically generate additional parallel training data .
This was accomplished by machine - translating the Arabic microblog messages with the best system after performing the first two adaptation steps .
Since decoding is very timeintensive , only 1 million randomly selected Arabic microblog messages were used to generate synthetic parallel data .
This new data was then used to train another phrase table .
Such self-translation techniques have been introduced by Ueffing et al . ( 2007 ) .
All results were evaluated against a baseline of using only NIST data for translation model , language model and weight optimization .
Our results are shown in table 4 . Using an indomain development set while leaving everything else untouched led to an improvement of approximately 1 BLEU point .
Three experiments involving the Twitter language model confirm Bertoldi and Federico ( 2009 ) 's findings that the language model was most helpful .
The BLEU - score could be improved by 1.5 to 2 points in all experiments .
When using an in- domain language model , there was no significant difference between deploying an in- domain or out - of- domain development set .
We also compared the effect of using only the in-domain language model to that of adding the in-domain language model as an extra feature while keeping the NIST language model .
17
There was no signif- icant difference between both runs .
However , for further adaptation experiments we used the system with the highest absolute BLEU score .
In our case , using synthetically generated data was not helpful , yielding similar results as the language model experiments above .
As has been observed before by Bertoldi and Federico ( 2009 ) , it did not matter whether the synthetic data were used on their own or in addition to the original training data .
Domain Adaptation using Translation - based CLIR Meta-parameters ? , ? ? [ 0 , 1 ] of the retrieval model were tuned in a mate-finding experiment :
Matefinding refers to the task of retrieving the single relevant document for a query .
In our case , each source tweet in the crowdsourced development set had exactly one " mate " , namely the crowdsourced translation that was ranked best in a further crowdsourced ranking task .
Using the retrieval model described in section 3 we achieved precision@1 scores above 95 % in finding the translations of a tweet when ? and ? were set to 0.9 .
We fixed these parameter settings for all following experiments .
The translation table was taken from the baseline experiments in table 4 .
During retrieval , we kept up to 10 highest scoring documents per query .
both strategies yielded the same results .
We first employed heuristic phrase extraction based on the word alignments generated from the NIST data as described above .
To avoid learning too much noise , maximum phrase length was restricted to 3 ( the default is 7 ) .
To evaluate the effects of choosing more restrictive or more lax settings , we ran experiments varying the following configurations :
1 . Constraints on alignment points : ? no constraints , ? 3 + alignment points in each direction , ? 3 + alignment points in both directions , ? 5 + alignment points in both directions .
Constraints on retrieval ranking : ? top 10 results , ? top 3 results , ? top 1 results , ? retrieval intersection ( results found in both retrieval directions )
We obtained improvements for all combinations of these configurations .
However , we observed that requiring 5 common alignment points was too strict , since few pairs met this constraint .
We also noticed that using only the top 3 retrieval results was beneficial to performance , suggesting that more comparable microblog messages were indeed ranked higher .
Using extraction heuristics we gained maximally 1.0 BLEU using the top 3 retrieval results and requiring at least 3 alignment points in both alignment directions ( see first line in table 5 ) .
However , other configurations produced very similar results .
While heuristics led to small incremental improvements , we achieved a much larger improvement by training a new phrase table from scratch using GIZA ++.
Again , we restricted maximum phrase length to 3 words .
In order to keep phrase table size manageable , we had to restrict retrieval to top - 1 results or only use retrieval results in the intersection of retrieval directions .
Best results are obtained when combining phrase tables extracted from GIZA ++ alignments in the intersection of retrieval results with NIST phrase tables in backoff mode ( see last line in table 5 ) .
Error Analysis
Our cross-lingual retrieval approach succeeded in finding nearly parallel tweets , confirming our hypothesis that such data actually exists .
Examples are given in table 6 . Table 7 shows a more detailed breakdown of our translation scores .
First , standard adaptation methods increased n-gram precision , suggesting that using in -domain adaptation data caused the system to choose more suitable words .
As expected , there was no reduction in OOVs , since using an in-domain language model and development set does not introduce new vocabulary .
Heuristic phrase extraction again produced small improvements in n-gram precision while reducing the number of unknown words .
Learning a new phrase table with GIZA ++ produced substantial improvements both in OOVrate and in n-gram precision .
Nevertheless , even the scores of the adapted system are still fairly low and translation quality as judged by inspection of the output can be very poor .
This suggests that the language used on Twitter still poses a great challenge , due to its variety of styles as well as the users ' tendency to use non-standard spelling and colloquial or dialectal expressions .
Our development set contained many different genres , from Qu'ran verses over news headlines to personal chatter .
Another difficulty was posed by dialectal Arabic content .
To gain an impression of the amount of dialectal content in our data , we used the Arabic Online Commentary Dataset created by Zaidan and Callison - Burch ( 2011a ) to classify our test set .
Table 8 shows the distribution of dialects in our test data according to language model probability .
This distribution should be viewed with a grain of salt , since the shortness of tweets might cause unreliable results when using a model based on word frequencies for classification .
Still , the results suggest that there is a high proportion of dialectal content and spelling variation in our data , causing a large number of OOVs .
For example , the preposition
? , ? meaning " in " is often written as Table 9 gives examples of translations generated using different adaptation methods in comparison to the references and the Google translation service to illustrate strengths and weaknesses of our approach .
Example 1 shows a case where unknown words were learned through translation model adaptation .
Note that even the Google translator did not recognize the word ? which was transliterated as " Msellat " .
Zaidan and Callison-Burch ( 2011a ) point out that dialectal variants are often transliterated by Google .
Note also , that the unadapted translation erroneously translated the place name " sitra " as " jacket " , a mistake which was also made in two of the references and by Google .
The same happened to the place name " wadyan " , which could also be taken as meaning " and religions " .
This error was enforced by our preprocessing step incorrectly splitting off the prefix " w " which often carries the meaning " and " .
In addition to that , the two runs which used translation model adaptation each dropped a part of the input sentence ( " in sitra " , " firing " ) .
We
Conclusion
We presented an approach to translation of microblog messages from the Twitter domain .
The main obstacle to state - of- the - art SMT of such data is the complete lack of sentence - parallel training data .
We presented a technique that uses translationbased CLIR to find relevant Arabic Twitter messages given English Twitter queries , and applies a standard pipeline for unsupervised training of phrase - based SMT to retrieval results .
We found this straightforward technique to outperform more conservative techniques to extract phrases from comparable data and also to outperform techniques using monolingual resources for language model adaptation , metaparameter tuning , or self-translation .
The greatest benefit of our approach is a significant reduction of OOV terms at a simultaneous improvement of correct unigram and bigram translations .
Despite this positive net effect , we still find a considerable amount of noise in the automatically extracted phrase tables .
Noise reduction by improved pre-processing and by more sophisticated training will be subject to future work .
Furthermore , we would like to investigate a tighter integration of CLIR and SMT training by using forced decoding techniques for CLIR and by a integrating a feedback loop into retrieval and training .
? ? ? ? ? ? ? ? ? ?GOOGLE TRANSLATIONAFP confirms that the French President Gaddafi Libyans tried to call and forgivenessENGLISH TWEET french president assures that will be taken to court and tells the libyans to forgive each other ARABIC TWEET ?
? ? ? ? ? ? ? ? ? ? ? ? ? ?GOOGLE TRANSLATIONNTRA decide to increase the number of all mobile operators in Egypt a commencement from ThursdayENGLISH TWEET ntra decide to increase the number of all mobile operators in starting from thursday ARABIC TWEET ? ? ? ? ? ? ? ? ? ? GOOGLE TRANSLATION
Shahid Amin AA Day January through gunshot ENGLISH TWEET martyr amin ali ahmed on jan by gunshot
