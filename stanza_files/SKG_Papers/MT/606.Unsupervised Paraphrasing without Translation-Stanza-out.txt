title
Unsupervised Paraphrasing without Translation
abstract
Paraphrasing exemplifies the ability to abstract semantic content from surface forms .
Recent work on automatic paraphrasing is dominated by methods leveraging Machine Translation ( MT ) as an intermediate step .
This contrasts with humans , who can paraphrase without being bilingual .
This work proposes to learn paraphrasing models from an unlabeled monolingual corpus only .
To that end , we propose a residual variant of vector-quantized variational auto-encoder .
We compare with MT - based approaches on paraphrase identification , generation , and training augmentation .
Monolingual paraphrasing outperforms unsupervised translation in all settings .
Comparisons with supervised translation are more mixed : monolingual paraphrasing is interesting for identification and augmentation ; supervised translation is superior for generation .
Introduction Many methods have been developed to generate paraphrases automatically ( Madnani and J. Dorr , 2010 ) .
Approaches relying on Machine Translation ( MT ) have proven popular due to the scarcity of labeled paraphrase pairs ( Callison - Burch , 2007 ; Mallinson et al. , 2017 ; Iyyer et al. , 2018 ) . Recent progress in MT with neural methods Vaswani et al. , 2017 ) has popularized this latter strategy .
Conceptually , translation is appealing since it abstracts semantic content from its linguistic realization .
For instance , assigning the same source sentence to multiple translators will result in a rich set of semantically close sentences ( Callison - Burch , 2007 ) .
At the same time , bilingualism does not seem necessary to humans to generate paraphrases .
This work evaluates if data in two languages is necessary for paraphrasing .
We consider three settings : supervised translation ( parallel bilingual data is used ) , unsupervised translation ( nonparallel corpora in two languages are used ) and monolingual ( only unlabeled data in the paraphrasing language is used ) .
Our comparison devises comparable encoder-decoder neural networks for all three settings .
While the literature on supervised Vaswani et al. , 2017 ) and unsupervised translation ( Lample et al. , 2018a ; Artetxe et al. , 2018 ; Lample et al. , 2018 b ) offer solutions for the bilingual settings , monolingual neural paraphrase generation has not received the same attention .
We consider discrete and continuous autoencoders in an unlabeled monolingual setting , and contribute improvements in that context .
We introduce a model based on Vector-Quantized Auto-Encoders , VQ - VAE ( van den Oord et al. , 2017 ) , for generating paraphrases in a purely monolingual setting .
Our model introduces residual connections parallel to the quantized bottleneck .
This lets us interpolate from classical continuous autoencoder ( Vincent et al. , 2010 ) to VQ - VAE .
Compared to VQ - VAE , our architecture offers a better control over the decoder entropy and eases optimization .
Compared to continuous auto-encoder , our method permits the generation of diverse , but semantically close sentences from an input sentence .
We compare paraphrasing models over intrinsic and extrinsic metrics .
Our intrinsic evaluation evaluates paraphrase identification , and generations .
Our extrinsic evaluation reports the impact of training augmentation with paraphrases on text classification .
Overall , monolingual approaches can outperform unsupervised translation in all settings .
Comparison with supervised translation shows that parallel data provides valuable information for paraphrase generation compared to purely monolingual training .
Related Work Paraphrase Generation Paraphrases express the same content with alternative surface forms .
Their automatic generation has been studied for decades : rule-based ( McKeown , 1980 ; Meteer and Shaked , 1988 ) and data-driven methods ( Madnani and J. Dorr , 2010 ) have been explored .
Data-driven approaches have considered different source of training data , including multiple translations of the same text ( Barzilay and McKeown , 2001 ; Pang et al. , 2003 ) or alignments of comparable corpora , such as news from the same period ( Dolan et al. , 2004 ; Barzilay and Lee , 2003 ) .
Machine translation later emerged as a dominant method for paraphrase generation .
Bannard and Callison- Burch ( 2005 ) identify equivalent English phrases mapping to the same non-English phrases from an MT phrase table .
Kok and Brockett ( 2010 ) performs random walks across multiple phrase tables .
Translation - based paraphrasing has recently benefited from neural networks for MT Vaswani et al. , 2017 ) .
Neural MT can generate paraphrase pairs by translating one side of a parallel corpus Iyyer et al. , 2018 ) .
Paraphrase generation with pivot / round - trip neural translation has also been used ( Mallinson et al. , 2017 ; Yu et al. , 2018 ) .
Although less common , monolingual neural sequence models have also been proposed .
In supervised settings , Prakash et al . ( 2016 ) ; Gupta et al. ( 2018 ) learn sequence - to-sequence models on paraphrase data .
In unsupervised settings , Bowman et al . ( 2016 ) apply a VAE to paraphrase detection while Li et al . ( 2017 ) train a paraphrase generator with adversarial training .
Paraphrase Evaluation Evaluation can be performed by human raters , evaluating both text fluency and semantic similarity .
Automatic evaluation is more challenging but necessary for system development and larger scale statistical analysis ( Callison - Burch , 2007 ; Madnani and J. Dorr , 2010 ) .
Automatic evaluation and generation are actually linked : if an automated metric would reliably assess the semantic similarity and fluency of a pair of sentences , one would generate by searching the space of sentences to maximize that metric .
Automated evaluation can report the overlap with a reference paraphrase , like for transla-tion ( Papineni et al. , 2002 ) or summarization ( Lin , 2004 ) . BLEU , METEOR and TER metrics have been used ( Prakash et al. , 2016 ; Gupta et al. , 2018 ) .
These metrics do not evaluate whether the generated paraphrase differs from the input sentence and large amount of input copying is not penalized .
Galley et al. ( 2015 ) compare overlap with multiple references , weighted by quality ; while Sun and Zhou ( 2012 ) explicitly penalize overlap with the input sentence .
Grangier and Auli ( 2018 ) alternatively compare systems which have first been calibrated to a reference level of overlap with the input .
We follow this strategy and calibrate the generation overlap to match the average overlap observed in paraphrases from humans .
In addition to generation , probabilistic models can be assessed through scoring .
For a sentence pair ( x , y ) , the model estimate of P ( y|x ) can be used to discriminate between paraphrase and non-paraphrase pairs ( Dolan and Brockett , 2005 ) .
The correlation of model scores with human judgments ( Cer et al. , 2017 ) can also be assessed .
We report both types of evaluation .
Finally , paraphrasing can also impact downstream tasks , e.g. to generate additional training data by paraphrasing training sentences ( Marton et al. , 2009 ; Zhang et al. , 2015 ; Yu et al. , 2018 ) .
We evaluate this impact for classification tasks .
Residual VQ - VAE for Unsupervised Monolingual Paraphrasing Auto-encoders can be applied to monolingual paraphrasing .
Our work combines Transformer networks ( Vaswani et al. , 2017 ) and VQ - VAE ( van den Oord et al. , 2017 ) , building upon recent work in discrete latent models for translation .
VQ - VAEs , as opposed to continuous VAEs , rely on discrete latent variables .
This is interesting for paraphrasing as it equips the model with an explicit control over the latent code capacity , allowing the model to group multiple related examples under the same latent assignment , similarly to classical clustering algorithms ( Macqueen , 1967 ) .
This is conceptually simpler and more effective than rate regularization ( Higgins et al. , 2016 ) or denoising objectives ( Vincent et al. , 2010 ) for continuous auto-encoders .
At the same time , training auto-encoder with discrete bottleneck is difficult .
We address this difficulty with an hybrid model using a continuous residual connection around the quantization module .
We modify the Transformer encoder ( Vaswani et al. , 2017 ) as depicted in Figure 1 .
Our encoder maps a sentence into a fixed size vector .
This is simple and avoids choosing a fixed length compression rate between the input and the latent representation .
Our strategy to produce a fixed sized representation from transformer is analogous to the special token employed for sentence classification in ( Devlin et al. , 2018 ) .
At the first layer , we extend the input sequences with one or more fixed positions which are part of the self-attention stack .
At the output layer , the encoder output is restricted to these special positions which constitute the encoder fixed sizedoutput .
As in , this vector is split into multiple heads ( sub-vectors of equal dimensions ) which each goes through a quantization module .
For each head h , the encoder output e h is quantized as , q h ( e h ) = c k , where k = argmin i e h ?
c i 2 where {c i } K i=0 denotes the codebook vectors .
The codebook is shared across heads and training combines straight - through gradient estimation and exponentiated moving averages ( van den Oord et al. , 2017 ) .
The quantization module is completed with a residual connection , with a learnable weight ? , z h ( e h ) = ?e h + ( 1 ? ?) q h ( e h ) .
One can observe that residual vectors and quantized vectors always have similar norms by definition of the VQ module .
This is a fundamental difference with classical continuous residual networks , where the network can reduce activation norms of some modules to effectively rely mostly on the residual path .
This makes ? an important parameter to trade- off continuous and discrete auto-encoding .
Our learning encourages the quantized path with a squared penalty ?
2 . After residual addition , the multiple heads of the resulting vector are presented as a matrix to which a regular transformer decoder can attend .
Models are trained to maximize the likelihood of the training set with Adam optimizer using the learning schedule from ( Vaswani et al. , 2017 ) .
Experiments & Results
We compare neural paraphrasing with and without access to bilingual data .
For bilingual settings , we consider supervised and unsupervised translation using round-trip translation ( Mallinson ( Chelba et al. , 2013 ) . Monolingual Residual VQ - VAE is trained only on LM1B with K = 2 16 , with 2 heads and fixed window of size 16 .
We also evaluate plain VQ - VAE ? = 0 to highlight the value of our residual modification .
We further compare with a monolingual continuous denoising auto-encoder ( DN - AE ) , with noising from Lample et al . ( 2018 b ) . Paraphrase Identification
For classification of sentence pairs ( x , y ) over Microsoft Research Paraphrase Corpus ( MRPC ) from Dolan and Brockett ( 2005 ) , we train logistic regression on P ( y|x ) and P ( x|y ) from the model , complemented with encoder outputs in fixed context settings .
We also perform paraphrase quality regression on Semantic Textual Similarity ( STS ) from Cer et al . ( 2017 ) ( 2002 ) .
MTC contains English paraphrases collected as translations of the same Chinese sentences from multiple translators ( Mallinson et al. , 2017 ) .
We pair each MTC sentence x with a paraphrase y and 100 randomly chosen nonparaphrases y .
We compare the paraphrase score P ( y|x ) to the 100 non-paraphrase scores P ( y | x ) and report the fraction of comparisons where the paraphrase score is higher .
Table 1 ( left ) reports that our residual model outperforms alternatives in all identification setting , except for STS , where our Pearson correlation is slightly under supervised translation .
Paraphrases for Data Augmentation
We augment the training set of text classification tasks for sentiment analysis on Stanford Sentiment Treebank ( SST - 2 ) ( Socher et al. , 2013 ) and question classification on Text REtrieval Conference ( TREC ) ( Voorhees and Tice , 2000 ) .
In both cases , we double training set size by paraphrasing each sentence and train Support Vector Machines with Naive Bayes features ( Wang and Manning , 2012 ) .
In Table 2 , augmentation with monolingual models yield the best performance for SST - 2 sentiment classification .
TREC question classification is better with supervised translation augmentation .
Unfortunately , our monolingual training set LM1B does not contain many question sentences .
Future work will revisit monolingual training on larger , more diverse resources .
Paraphrase Generation Paraphrase generation are evaluated on MTC .
We select the 4 best translators according to MTC documentation and paraphrase pairs with a length ratio under 1.2 .
Our evaluation prevents trivial copying solutions .
We select sampling temperature for all models such that their generation overlap with the input is 20.9 BLEU , the average overlap between humans on MTC .
We report BLEU overlap with the target and run a blind human evaluation where raters pick the best generation among supervised translation , unsupervised translation and monolingual .
a worthy substitute
Out : A worthy replacement .
In : Local governments will manage the smaller enterprises .
Out : Local governments will manage smaller companies .
In : Inchon is 40 kilometers away from the border of North Korea .
Out : Inchon is 40 km away from the North Korean border .
In : Executive Chairman of Palestinian Liberation Organization , Yasar Arafat , and other leaders are often critical of aiding countries not fulfilling their promise to provide funds in a timely fashion .
Out : Yasar Arafat , executive chairman of the Palestinian Liberation Organization and other leaders are often critical of helping countries meet their pledge not to provide funds in a timely fashion .
lights the value of parallel data for paraphrase generation .
Discussions
Our experiments highlight the importance of the residual connection for paraphrase identification .
From Table 1 , we see that a model without the residual connection obtains 66.3 % , 10.6 % and 69.0 % accuracy on MRPC , STS and MTC .
Adding the residual connection improves this to 73.3 % , 59.8 % and 94.0 % respectively .
The examples in Table 3 show paraphrases generated by the model .
The overlap with the input from these examples is high .
It is possible to generate sentences with less overlap at higher sampling temperatures , we however observe that this strategy impairs fluency and adequacy .
We plan to explore strategies which allow to condition the decoding process on an overlap requirement instead of varying sampling temperatures ( Grangier and Auli , 2018 ) .
Conclusion
We compared neural paraphrasing with and without access to bilingual data .
Bilingual settings considered supervised and unsupervised translation .
Monolingual settings considered autoencoders trained on unlabeled text and introduced continuous residual connections for discrete autoencoders .
This method is advantageous over both discrete and continuous auto-encoders .
Overall , we showed that monolingual models can outperform bilingual ones for paraphrase identification and data-augmentation through paraphrasing .
We also reported that generation quality from monolingual models can be higher than model based on unsupervised translation but not supervised translation .
Access to parallel data is therefore still advantageous for paraphrase generation and our monolingual method can be a helpful resource for languages where such data is not available .
Table 1 : 1 by training ridge regression on the same features .
Finally , we perform paraphrase ranking on Multiple Translation Chinese ( MTC ) from Huang et al .
Paraphrase Identification & Generation .
Identification is evaluated with accuracy on MRPC , Pearson Correlation on STS and ranking on MTC .
Generation is evaluated with BLEU and human preferences on MTC .
Parapharase Identification Generation MRPC STS MTC BLEU Pref. Supervised Translation 70.6 46.0 78.6 8.73 36.8 + Distillation 66.5 60.0 55.6 7.08 - Unsupervised Translation 66.0 13.2 65.8 6.59 28.1 + Distillation 66.9 45.0 52.0 6.45 - Mono. DN -AE 66.8 46.2 91.6 5.13 - Mono. VQVAE 66.3 10.6 69.0 3.85 - + Residual 73.3 59.8 94.0 7.26 31.9 + Distillation 71.3 54.3 88.4 6.88 - SST -2 TREC Acc. F1 Acc F1 NB -SVM ( trigram ) 81.93 83.15 89.77 84.81 Supervised Translation 81.55 82.75 90.78 85.44 + Distillation 81.16 66.59 90.38 86.05 Unsupervised Translation 81.87 83.18 88.17 83.42 + Distillation 81.49 82.78 89.18 84.41 Mono. DN -AE 81.11 82.48 89.37 84.08 Mono . VQ-VAE 81.98 82.95 89.17 83.64 + Residual 82.12 83.23 89.98 84.31 + Distillation 81.60 82.81 89.78 84.31
