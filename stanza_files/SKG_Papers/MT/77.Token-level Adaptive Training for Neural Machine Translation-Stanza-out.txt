title
Token-level Adaptive Training for Neural Machine Translation
abstract
There exists a token imbalance phenomenon in natural language as different tokens appear with different frequencies , which leads to different learning difficulties for tokens in Neural Machine Translation ( NMT ) .
The vanilla NMT model usually adopts trivial equal-weighted objectives for target tokens with different frequencies and tends to generate more high - frequency tokens and less lowfrequency tokens compared with the golden token distribution .
However , low-frequency tokens may carry critical semantic information that will affect the translation quality once they are neglected .
In this paper , we explored target token - level adaptive objectives based on token frequencies to assign appropriate weights for each target token during training .
We aimed that those meaningful but relatively low-frequency words could be assigned with larger weights in objectives to encourage the model to pay more attention to these tokens .
Our method yields consistent improvements in translation quality on ZH-EN , EN - RO , and EN - DE translation tasks , especially on sentences that contain more low-frequency tokens where we can get 1.68 , 1.02 , and 0.52 BLEU increases compared with baseline , respectively .
Further analyses show that our method can also improve the lexical diversity of translation .
Introduction Neural machine translation ( NMT ) systems ( Kalchbrenner and Blunsom , 2013 ; Cho et al. , 2014
The ' Average Frequency ' column represents the average frequencies of the tokens in each interval , which show the token imbalance phenomenon in natural language .
The last two columns show the vanilla NMT model tends to generate more high - frequency tokens and less low-frequency tokens than reference .
Sutskever et al. , 2014 ; Bahdanau et al. , 2015 ; Gehring et al. , 2017 ; Vaswani et al. , 2017 ) are data driven models , which highly depend on the training corpus .
NMT models have a tendency towards over-fitting to frequent observations ( e.g. words , word co-occurrences ) while neglecting those lowfrequency observations .
Unfortunately , there exists a token imbalance phenomenon in natural languages as different tokens appear with different frequencies , which roughly obey the Zipf's Law ( Zipf , 1949 ) .
Table 1 shows that there is a serious imbalance between high - frequency tokens and lowfrequency tokens .
NMT models rarely have the opportunity to learn and generate those groundtruth low-frequency tokens in the training process .
Some work tries to improve the rare word translation by maintaining phrase tables or back - off vocabulary Jean et al. , 2015 ; Li et al. , 2016 ; Pham et al. , 2018 ) or adding extra components ( G?lc ? ehre et al. , 2016 ; Zhao et al. , 2018 ) , which bring in extra training complexity and computing expense .
Some NMT techniques which are based on smaller translation granularity can alleviate this issue , such as hybrid word- character - based model ( Luong and Manning , 2016 ) , BPE - based model ( Sennrich et al. , 2016 ) and word- piece - based model ( Wu et al. , 2016 ) .
These effective work alleviate the token imbalance phenomenon to a certain extent and become the de-facto standard in most NMT models .
Although sub-word based NMT models have achieved significant improvements , they still face the token - level frequency imbalance phenomenon , as Table 1 shows .
Furthermore , current NMT models generally assign equal training weights to target tokens without considering their frequencies .
It is very likely for NMT models to ignore the loss produced by the low-frequency tokens because of their small proportion in the training sets .
The parameters related to them can not be adequately trained , which will , in turn , make NMT models tend to prioritize output fluency over translation adequacy , and ignore the generation of low-frequency tokens during decoding , which is illustrated in Table 1 .
It shows that the vanilla NMT model tends to generate more highfrequency tokens and less low-frequency tokens .
However , low-frequency tokens may carry critical semantic information which may affect translation quality once they are neglected .
To address the above issue , we proposed tokenlevel adaptive training objectives based on target token frequencies .
We aimed that those meaningful but relatively low-frequency tokens could be assigned with larger loss weights during training so that the model will learn more about them .
To explore suitable adaptive objectives for NMT , we first applied existing adaptive objectives from other tasks to NMT and analyzed their performance .
We found that though they could bring modest improvement on the translation of low-frequency tokens , they did much damage to the translation of highfrequency tokens , which led to an obvious degradation on the overall performance .
This implies that the objective should ensure the training of highfrequency tokens first .
Then , based on our observations , we proposed two heuristic criteria for design-ing the token - level adaptive objectives based on the target token frequencies .
Last , we presented two specific forms for different application scenarios according to the criteria .
Our method yields consistent improvements in translation quality on ZH-EN , EN - RO , and EN - DE translation tasks , especially on sentences that contain more low-frequency tokens where we can get 1.68 , 1.02 , and 0.52 BLEU increases compared with baseline , respectively .
Further analyses show that our method can also improve the lexical diversity of translation .
Our contributions can be summarized as follows : ?
We analyzed the performance of the existing adaptive objectives when they were applied to NMT .
Based on our observations , we proposed two heuristic criteria for designing token - level adaptive objectives and present two specific forms to alleviate the problem brought by the token imbalance phenomenon .
?
The experimental results validate that our method can improve not only the translation quality , especially on those low-frequency tokens , but also the lexical diversity .
Background
In our work , we apply our method in the framework of Transformer ( Vaswani et al. , 2017 ) which will be briefly introduced here .
We denote the input sequence of symbols as x = ( x 1 , . . . , x J ) , the ground- truth sequence as y * = ( y * 1 , . . . , y * K ) and the translation as y = ( y 1 , . . . , y K ) .
The Encoder & Decoder
The encoder is composed of N identical layers .
Each layer has two sublayers .
The first sublayer is a multi-head attention unit used to compute the self-attention of the input , named self-attention multi-head sublayer , and the second one is a fully connected feed -forward network , named FNN sublayer .
Both of the sublayers are followed by a residual connection operation and a layer normalization operation .
The input sequence x will be first converted to a sequence of vectors E x = [ E x [ x 1 ] ; . . . ; E x [ x J ] ] , where E x [ x j ] is the sum of the word embedding and the position embedding of the source word x j .
Then , this input sequence of vectors will be fed into the encoder and the output of the N -th layer will be taken as source hidden states .
The decoder is also composed of N identical layers .
In addition to the same kind of two sublayers in each encoder layer , the third crossattention sublayer is inserted between them , which performs multi-head attention over the output of the encoder .
The final output of the N -th layer gives the target hidden states S = [ s 1 ; . . . ; s I ] , where s i is the hidden states of y k .
The Objective
The model is optimized by minimizing a cross-entropy loss with the ground - truth : L = ?
1 K K k=1 log p(y * k |y <k , x ) , ( 1 ) where K is the length of the target sentence .
Method
Our work aims to explore suitable adaptive objectives that can not only improve the learning of low-frequency tokens but also avoid harming the translation quality of high - frequency tokens .
We first investigated two existing adaptive objectives , which were proposed for solving the token imbalance problems in other tasks , and analyzed their performance .
Then , based on our observations , we introduced two heuristic criteria for designing the adaptive objective .
Based on the proposed criteria , we put forward two simple but effective functional forms from different perspectives , which can be adapted to various application scenarios in NMT .
Existing Adaptive Objectives Investigation
The form of adaptive objective is as follows : L = ?
1 I I i=1 w(y i ) log p(y i |y < i , x ) , ( 2 ) where w(y i ) is the weight assigned to the target token y i , which varies as the token frequency changes .
Actually , there are some existing adaptive objectives which have been proven effective for other tasks .
It can help us understand what is necessary for a suitable adaptive objective for NMT if we apply these methods to it .
The first objective we have investigated is the form in Focal loss ( Lin et al. , 2017 ) , which was proposed for solving the label imbalance problem in the object detection task : w(y i ) = ( 1 ? p( y i ) ) ? . ( 3 ) Although it does n't utilize the frequency information directly , it actually reduces the weights of the high- frequency classes more because they are usually easier to classify with higher prediction probabilities .
We set ? to 1 as suggested by their experiments .
We noticed that this method greatly reduced the weights of high- frequency tokens , and the variance of weights is large .
The second is the linear weighting function ( Jiang et al. , 2019 ) , which was proposed for the dialogue response generation task : w(y i ) = ? Count(y i ) max ( Count (y k ) ) + 1 , y k ?
V t , ( 4 ) where Count(y k ) is the frequency of token y k in the training set and V t denotes the target vocabulary .
Then , the normalized weights w(y i ) , which have a mean of 1 , are assigned to the target tokens .
We noticed that the weights of high- frequency tokens are only slightly less than 1 , and the variance of weights is small .
We tested these two objectives on the Chinese to English translation task and the results on the validation set are given in Table 2 1 .
To verify their effects on the high - and low-frequency tokens , we also divided the validation set into two subsets based on the average token frequency of the sentences , the results of which are also given in Table 2 .
It shows that although these two methods can bring modest improvement in the translation of the low-frequency tokens , it does much harm to high - frequency tokens , which has a negative impact on the overall performance .
We noted that both of these two methods reduced the weights of the high- frequency tokens to different degrees , and we argued that when the highfrequency tokens account for a large proportion in NMT corpus , this hinders the normal training of them .
To validate our argument , we simply add 1 to the weighting term of focal loss : w(y i ) = ( 1 ? p( y i ) ) ? + 1 . ( 5 ) The results are also given in Table 2 ( Row 5 ) , which indicates that this method actually avoids the damage to the high- frequency tokens .
The overall results indicate that it is not robust enough to improve the learning of low-frequency tokens by reducing the weight of high - frequency tokens during the training of NMT .
Although our goal is to improve the training of low-frequency tokens , we should first ensure the training of high- frequency tokens , and then increase the weights of low-frequency tokens appropriately .
Based on the above findings , we proposed the following criteria .
Heuristic Criteria for Token Weighting
We proposed two heuristic criteria for designing the token - level training weights : Minimum Weight Ensurence .
The training weight of any token in the target vocabulary should be equal to or bigger than 1 , which can be described as : ?y k ?
V t , w(y k ) ? 1 ( 6 ) Although we can force the model to pay more attention to low-frequency tokens by shrinking the weights of high- frequency tokens , the previous analyses have proved that the training performance is more sensitive to the change of high- frequency tokens ' weights due to their large proportion in the training set .
A relatively small decrease in the weights of high- frequency tokens will prevent the generation probabilities of ground - truth tokens from ascending continually , which may result in an obvious degradation of the overall performance .
Therefore , we ensure that all the token weights are equal to or bigger than 1 considering the training stability as well as designing convenience .
Weights Expectation Range Control .
On the condition that the first criterion is satisfied , those high - frequency tokens could have already been well learned without any extra attention .
Now , those low-frequency tokens could be assigned with higher weights .
Meanwhile , we also need to ensure that the weights of low-frequency tokens ca n't be too large , or it will hurt the training of highfrequency tokens certainly .
Therefore , the expectation of the training weights on the whole training set should be in [ 1 , 1 + ? ] : | Vt | k=1 Count(y k ) w( y k ) | Vt | k=1 Count(y k ) = 1 + ? , ? ? 0 , ( 7 ) where | V t | denotes the size of the target vocabulary , ? is a relatively small number compared with 1 .
A larger weight expectation means we allocate larger weights to those low-frequency tokens .
In contrast , an appropriate weight expectation as defined in this criterion can help improve the overall performance .
The two criteria proposed here are not the only options for NMT , but the adaptive objective satisfying these two criteria can improve not only the translation performance of low-frequency tokens but also the overall performance based on our experimental observations .
Two Specific Adaptive Objectives
In this paper , we proposed two simple functional forms for w(y k ) heuristically based on the previous criteria and justified them with some intuitions .
Exponential : Given the target token y k , we define the exponential weighting function as : w(y k ) = A ? e ?T? Count(y k ) + 1 . ( 8 ) There are two hyperparameters in it , i.e. , A and T , which control the shape and the value range of the function .
They can be set up according to the two criteria above .
The plot of this weighting function is presented in Figure 1 .
In this case , we do n't consider the factor of noisy tokens so that the weight increases monotonically as the frequency decreases .
Therefore , this weighting function is suitable for cleaner training data where the extremely low-frequency tokens only take up a small proportion .
Chi-Square :
The exponential form weighting function is not suitable for the training data which contain many noisy tokens , because they would be assigned with relatively large weights and have bigger impacts when their weights are summed together .
To alleviate this problem , we proposed another form of the weighting function : w(y k ) = A ? Count 2 ( y k ) e ?T? Count(y k ) + 1 . ( 9 ) The form of this function is similar to the form of chi-square distribution , so we named it as chisquare .
Plot of this weighting function is presented in Figure 1 .
We can see from the plot that the weight increases as the frequency decreases at first .
Then , after a specific frequency threshold , which is decided by the hyperparameter T , the weight decreases as the frequency decreases .
In this case , the most frequent tokens and the extremely rare tokens , which could be noise , all will be assigned with small weights .
Meanwhile , those middle -frequency words will have larger weights .
Most of them are meaningful and valuable for translation but ca n't be well learned with an equal-weighted objective function .
This form of weighting function is suitable for more noisy training data .
Experiments 4.1 Data Preparation ZH?EN .
The training data consists of 1.25 M sentence pairs from LDC corpora which has 27.9M Chinese words and 34.5 M English words , respectively 2 .
The data set MT02 was used as validation and MT03 , MT04 , MT05 , MT06 , MT08 were used for the test .
We tokenized and lowercased English sentences using the Moses scripts 3 , and segmented the Chinese sentences with the Stanford Segmentor 4 .
The two sides were further segmented into subword units using Byte-Pair Encoding ( BPE ) ( Sennrich et al. , 2016 ) with 30 K merge operations separately .
EN ?RO .
We used the preprocessed version of the WMT2016 English -Romanian dataset released by Lee et al . ( 2018 ) which includes 0.6 M sentence pairs .
We used news - dev 2016 for validation and news -test 2016 for the test .
The two languages shared the same vocabulary generated with 40 K merge operations of BPE .
EN ?DE .
The training data is from WMT2016 which consists of about 4.5 M sentences pairs with 118M English words and 111M German words .
We chose the news test - 2013 for validation and newstest 2014 for the test .
32 K merge operations BPE were performed on both sides jointly .
Systems
We used the open-source toolkit called Fairseqpy ( Edunov et al. , 2017 ) released by Facebook as our Transformer system .
?
Baseline .
The baseline system was implemented as the base model configuration in Vaswani et al . ( 2017 ) strictly .
Since our method is further trained based on the pre-trained model at a low learning rate , we also trained another baseline model following the same procedures as our methods have except that all the target tokens share equal weights in the objective , denoted as Baseline - FT . ? Fine Tuning ( Luong and Manning , 2015 ) .
This model was first trained with all the training sentence pairs and then further trained with sentences containing more low-frequency tokens .
To filter out sentences containing more low-frequency tokens , the method in Platanios et al . ( 2019 ) was adopted as our judging metric with a small modification : d rarity ( y ) ?
1 I I i=1 log Count(y i ) | Vt | k=1 Count(y k ) , ( 10 ) where I is the sentence length .
We added a factor 1 I to eliminate the influence of sentence length .
All the target sentences were ranked by this metric in ascending order and the bottom one third of the training sentences were chosen as the in-domain data .
This method tries to utilize frequency information at the sentence level , while our work uses it at the token level in contrast .
?
Sampler ( Chu et al. , 2017 ) .
This method oversampled the sentences containing more lowfrequency tokens filtered by Eq. 10 three times and then concatenated them with the rest of the training data .
Thus the NMT model will be trained with more low-frequency tokens in every epoch .
? Entropy Regularization ( ER ) ( Pereyra et al. , 2017 ) .
This method was proposed for solving the overconfidence problem , which adds a confidence penalty term to the original objective : L ER = L ? ? 1 I I i=1 p(y i |x ) log ( p( y i | x ) ) .
( 11 ) It is known that token imbalance is one of the causes of overconfidence problem ( Jiang and de Rijke , 2018 ) , so this method may also alleviate the token imbalance problem .
We varied ? from 0.05 to 0.4 and chose the best one according to the results on the validation sets for different languages .
Noting that the label smoothing is applied in the vanilla transformer model which has a similar effect on the output , we removed it from the model when we tested this method .
Table 3 : Performance of our methods on the validation sets for all the three language pairs with different hyperparameters T. Although the best hyperparameter for different languages may be different , it is easy for our method to get a stable improvement .
? Linear ( Jiang et al. , 2019 ) .
This method was proposed for solving the token imbalance problem in the the dialogue response generation task : w(y i ) = ? Count(y i ) max ( Count (y k ) ) + 1 , y k ? V t . ( 12 )
Then , the normalized weights , which had a mean of 1 , were applied to the training objective .
?
Our Exp .
This system was first trained with the normal objective ( Equation 1 ) , where all the target tokens have the same training weights .
Then the model was further trained with the adaptive objective at a low learning rate .
The weights were produced by the Exponential form ( Equation 8 ) .
For computing stability , we used Count(y k ) C median instead of Count(y k ) in the weighting function , where C median is the median of the token frequency .
?
Our K2 .
This system was trained following the same procedure as system Our Exp except that the training weights were produced by the Chi-Square form ( Equation 9 ) .
The translation quality was evaluated by 4 - gram BLEU ( Papineni et al. , 2002 ) with the multi-bleu.pl script .
Besides , we used beam search with a beam size of 4 and a length penalty of 0.6 during the decoding process .
Hyperparameters
There are two hyperparameters in our weighting functions , A and T .
In our experiments , we fixed A to narrow search space and the overall weight range is [ 1 , e ] .
We tuned another hyperparameter T on the validation data sets under the criteria proposed in section 3.2 .
The results are shown in Table 3 .
According to the results , the best hyperparameters differed across different language pairs .
It is affected by the proportion of low-frequency words and high- frequency words .
Generally speaking , when the proportion of low-frequency words gets smaller , the hyperparameter T should be set smaller too .
But it also shows that it is easy for our methods to get a stable improvement over the baseline system following the criteria above .
Finally , we used the best hyperparameters as found on the validation data sets for the final evaluation of the test data sets .
For example , T = 0.35 in the exponential form for ZH?EN and T = 4.00 in the chi-square form for EN?RO .
Main Results
The results are shown in Table 4 .
It shows that the contrast methods can not bring stable improvements over the baseline system .
They bring excessive damages to the translation of high- frequency tokens which can be proved by the analyzing experiments in the next section .
As a contrast , our methods can bring stable improvements over Baseline - FT almost without any additional computing or storage expense .
On the EN?RO and EN ?
DE translation tasks , Our Exp is more effective than Our K2 while on the ZH?EN translation task the result is reversed .
The reason is that the NIST training data set contains more noisy tokens , which can be ignored by the Our K2 method .
More analyses based on the token frequency are shown in the next section .
Analysis
Effects on Translation Quality with Considering Token Frequencies
To further illustrate the effects of our method , we evaluated the performance based on the token frequency .
For the ZH?EN translation task , we concatenated the MT03 - 08 test sets together as a big test set .
For the EN?RO and EN ?
DE translation tasks , we just used their test sets .
Each sentence was scored according to Eq. 10 and sorted in ascending order .
Then the test set was divided into Table 5 : BLEU scores on different test subsets which are grouped by their rarities according to Eq. 10 .
Sentences in the ' Low ' contain more low-frequency tokens while the ' High ' is reverse .
The results show that our methods can improve the translation of low-frequency tokens significantly without hurting the translation of high- frequency tokens .
Table 6 : EN?DE BLEU scores on different test subsets .
The conclusion is identical to that in Table 5 . three subsets with equal size , denoted as HIGH , MIDDLE , and LOW , respectively .
Sentences in the subset LOW contain more low-frequency tokens while the HIGH is reverse .
The results are given in Table 5 and Table 6 .
The contrast methods outperform the Baseline - FT on the LOW subset but are worse than it in the HIGH and MIDDLE subsets , which indicates that the gains on the translation of low-frequency tokens come at the expense of the translation of high - frequency tokens .
As a contrast , both of our methods can not only bring a significant improvement on the LOW subset but also get a modest improvement on the HIGH and MIDDLE subsets .
It can be concluded that our methods can ameliorate the translation of low-frequency tokens without hurting the translation of high- frequency tokens .
Effects on Translation Quality with Different BPE Sizes
It is known that the BPE sizes have a large impact on the data distribution .
Intuitively , a smaller size of BPE will bring a more balanced data distribu- TTR ( ?10 tion , but it will also increase the average sentence length and neglect some token co-occurrences .
To verify the effectiveness of our method with different BPE sizes , we varied the BPE sizes from 1 K to 40 K on the ZH?EN translation task .
The results are shown in Figure 2 .
It shows that as the number of BPE size increases , the BLEU of baseline rises first and then declines .
Compared with the baseline systems , our method can always bring improvements , and the larger the BPE size , i.e. , the more imbalanced the data distribution , the larger the improvement brought by our method .
In practice , the BPE size either comes from the experience or is chosen from several trial - and - errors .
No matter what the situation is , our method can always bring a stable improvement .
Effects on Token Distribution and Lexical Diversity Compared with the reference , the outputs of the vanilla NMT model contain more high - frequency tokens and have lower lexical diversity ( Van - Source b?du?n gu?nb ? n?xi? w?r?n hu?nj?ng de m?iku?ng .
Reference those coalmines pollute the environment should be continuously shut down .
Baselie -FT continually close down those mines that pollute the environment .
Our Exp those coalmines that pollute the environment should be continuously closed .
Our K2 those coalmines that pollute the environment should be continuously closed .
Source y?h?u k?y ? g?i w? d?nd ? p?i ji?n b?ng?ngsh ?
. Reference an exclusive office could be assigned me later on .
Baselie -FT later i could match my office alone .
Our Exp i could be assigned an office alone later .
Our K2 later i could be assigned an office alone . massenhove et al. , 2019 b ) .
To verify whether our methods can alleviate these problems , we did the following experiments based on the ZH?EN translation task .
The tokens in the target vocabulary were first arranged in descending order according to their token frequencies .
Then they were divided into ten intervals equally .
Finally , we counted the number of tokens in each token frequency interval of the reference and the translation of different systems .
The results are shown in Figure 3 and we did a common logarithm for display convenience .
It shows that there is an obvious gap between the Baseline - FT and reference , and the curve of Baseline - FT is lower than the curve of reference in every frequency interval except for the top 10 % .
As a contrast , our methods can reduce this gap , and the tokens distribution is closer to the real distribution .
Besides , we also measure the lexical diversity of the translations with several criteria , namely , type-token ratio ( TTR ) ( Templin , 1957 ) , the approximation of hypergeometric distribution ( HD - D ) and the measure of textual lexical diversity ( MTLD ) ( Mccarthy and Jarvis , 2010 ) .
The results are given in Table 7 .
It shows that our method can also improve the lexical diversity of the translation .
Case Study Table 8 shows two translation examples in the ZH?EN translation direction .
In the first sentence , the Baseline - FT system failed to generate the low-frequency noun ' coalmine ' ( frequency : 43 ) , but generated a relatively high- frequency word ' mine ' ( frequency : 1155 ) .
We can see that this lowfrequency token carries the central information of this sentence , and the mistranslation of it prevents people from understanding this sentence correctly .
In the second sentence , our methods generated the low-frequency verb ' assigned ' ( frequency : 841 ) correctly , while the Baseline - FT generated a more frequent token ' match ' ( frequency : 1933 ) , which reduced the translation accuracy and fluency .
These examples can be part of the evidence to show the effectiveness of our methods .
Related Work Rare Word Translation .
Rare word translation is one of the key challenges for NMT .
For word- level NMT models , NMT has its limitation in handling a larger vocabulary because of the training complexity and computing expense .
Some work tries to solve this problem by maintaining phrase tables or back - off vocabulary Jean et al. , 2015 ; Li et al. , 2016 ) .
The subword - based NMT ( Sennrich et al. , 2016 ; Luong and Manning , 2016 ; Wu et al. , 2016 ) reduces the size of vocabulary greatly and become the mainstream technology gradually .
Gowda and May ( 2020 ) gave a detailed analysis about the effects of the BPE size on the data distribution and translation quality .
Some recent work tried to further improve the translation of the rare words with the help of the memory network or the pointer network ( Zhao et al. , 2018 ; Pham et al. , 2018 ) .
In contrast , our methods can improve the translation performance without extra cost and can be combined with other techniques .
Class Imbalance .
Class imbalance means the total number of some classes of data is far less than the total number of other classes .
This problem can be observed in various tasks ( Wei et al. , 2013 ; Johnson and Khoshgoftaar , 2019 ) .
In NMT , the class imbalance problem might be the underlying cause of , among others , the gender- biased output problem ( Vanmassenhove et al. , 2019a ) , the inability of MT system to handle morphologically richer language correctly ( Passban et al. , 2018 ) , or the exposure bias problem ( Ranzato et al. , 2016 ; Shao et al. , 2018 ; Zhang et al. , 2019 ) .
The methods of trying to solve this can be divided into two types .
The data- based methods ( Baloch and Rafi , 2015 ; Ofek et al. , 2017 ) make use of over - and undersampling to reduce the imbalance .
The algorithmbased methods ( Zhou and Liu , 2005 ; Lin et al. , 2017 ) give extra reward to different classes .
Our method is algorithm - based which brings no extra cost .
Word Frequency - based Methods .
Some work also makes use of word frequency information to help learning , such as in the word segmentation ( Sun et al. , 2014 ) and term extraction ( Frantzi et al. , 1998 ; Vu et al. , 2008 ) .
In NMT , word frequency information is used for curriculum learning ( Kocmi and Bojar , 2017 ; Platanios et al. , 2019 ) and domain adaptation data selection ( Wang et al. , 2017 ; Zhang and Xiong , 2018 ; Gu et al. , 2019 ) .
Wang et al. ( 2020 ) analyzed the miscalibration problem on the low-frequency tokens .
Jiang et al. ( 2019 ) proposed a linear weighting function to solve the word imbalance problem in the dialogue response generation task .
Compared with it , our method is more suitable for NMT .
Conclusion
In this work , we focus on the token imbalance problem of NMT .
We show that the output of vanilla NMT contains more high - frequency tokens and has lower lexical diversity .
To alleviate this problem , we investigated existing adaptive objectives for other tasks and then proposed two heuristic criteria based on the observations .
Next , we gave two simple but effective forms based on the criteria , which can assign appropriate training weights to target tokens .
The final results show that our methods can achieve significant improvement in performance , especially on sentences that contain more low-frequency tokens .
Further analyses show that our method can also improve the lexical diversity .
Figure 1 : 1 Figure 1 : Plots of our two weighting functions .
The blue curve is the Exponential form and the orange curve is the Chi-Square form .
Both of the hyperparamters are set to 1 .
