title
Simultaneous Translation Policies : From Fixed to Adaptive
abstract
Adaptive policies are better than fixed policies for simultaneous translation , since they can flexibly balance the tradeoff between translation quality and latency based on the current context information .
But previous methods on obtaining adaptive policies either rely on complicated training process , or underperform simple fixed policies .
We design an algorithm to achieve adaptive policies via a simple heuristic composition of a set of fixed policies .
Experiments on Chinese ?
English and German ?
English show that our adaptive policies can outperform fixed ones by up to 4 BLEU points for the same latency , and more surprisingly , it even surpasses the BLEU score of full-sentence translation in the greedy mode ( and very close to beam mode ) , but with much lower latency .
Introduction Simultaneous translation ( ST ) aims to provide good translation quality while keeping the latency of translation process as low as possible .
This is very important for the scenarios that require simultaneity , such as international summits and negotiations .
For this , human interpreters usually start translation before the source sentence ends .
However , this makes the translation process much more challenging than the full-sentence translation , because to balance the translation quality and latency , interpreters need to make decisions on when to continue translation and when to stop temporarily to wait for more source side information , which are difficult , especially for syntactically divergent language pairs , such as German and English .
The above decisions can be considered as two actions : READ ( wait for a new source word ) and WRITE ( emit a translated target word ) ( Gu et al. , 2017 ) .
Then we only need to decide which action to choose at each step , and the solution can be represented by a policy .
Earlier works ( Yarmohammadi et Bangalore et al. , 2012 ; F?gen et al. , 2007 ; Jaitly et al. , 2016 ) study policies as a part of speech - to-speech ST system , where the policies usually try to separate the source sentence into several chunks that can be translated safely .
Recent works focus on obtaining policies for text - to- text ST , which can be generally divided into two categories : fixed and adaptive .
Fixed policies ( Ma et al. , 2019 ; Dalvi et al. , 2018 ) usually follow some simple rules to choose actions .
For example , the wait -k policy by Ma et al . ( 2019 ) first chooses k READ actions , and then chooses WRITE and READ alternatively .
This kind of policies do not utilize the context information and can be either too aggressive or too conservative in different cases .
By contrast , adaptive policies try to make decisions on the fly using the currently available information .
It is obvious that this kind of policies is more desirable for ST than the fixed ones , and different methods are explored to achieve an adaptive policy .
The majority of such methods ( Grissom II et al. , 2014 ; Cho and Esipova , 2016 ; Gu et al. , 2017 ; Alinejad et al. , 2018 ; Zheng et al. , 2019a ) are based on full-sentence translation models , which may be simple to use but cannot outperform fixed policies applied with " genuinely simultaneous " models trained for ST ( Ma et al. , 2019 ) .
Other meth-ods ( Arivazhagan et al. , 2019 ; Zheng et al. , 2019 b ) try to learn a policy together with the underlying translation model , but they rely on complicated and time - consuming training process .
In this paper , we propose to achieve an adaptive policy via a much simpler heuristic composition of a set of wait -k policies ( e.g. , k = 1 ? 10 ) .
See Fig. 1 for an example .
To further improve the translation quality of our method , we apply ensemble of models trained with different wait -k policies .
Our experiments on Chinese ?
English and German ?
English translation show that our method can achieve up to 4 BLEU points improvement over the wait -k method for same latency .
More interestingly , compared with full-sentence translation , our method achieves higher BLEU scores than greedy search but with much lower latency , and is close to the results from beam search .
Preliminaries Full-sentence translation .
Neural machine translation ( NMT ) model usually consists of two components : an encoder , which encodes the source sentence x = ( x 1 , . . . , x m ) into a sequence of hidden states , and a decoder , which sequentially predicts target tokens conditioned on those hidden states and previous predictions .
The probability of the predicted target sequence y = ( y 1 , . . . , y n ) will be p(y | x ) = | y | t=1 p(y t | x , y <t ) where y <t = ( y 1 , . . . , y t?1 ) denotes the target sequence predicted before step t. Simultaneous translation .
Ma et al. ( 2019 ) propose a prefix-to- prefix framework to train models to make predictions conditioned on partial source sentences .
In this way , the probability of predicted sequence y becomes p g ( y | x ) = | y | t=1 p(y t | x ? g( t ) , y <t ) where g ( t ) is a monotonic non-decreasing function of t , denoting the number of processed source tokens when predicting y t .
This function g ( t ) can be used to represent a policy for ST .
Ma et al. ( 2019 ) introduce a kind of fixed policies , called wait -k policy , that can be defined by the following g k ( t ) = min { | x | , t + k ?
1 }. Intuitively , this policy first waits k source tokens and then outputs predicted tokens concurrently with the rest of source sentence .
In this example , we will choose an action based on the top probability p top , and apply a new policy ( the dotted arrows ) after the chosen action .
Obtaining an Adaptive Policy Assume we have a set of wait -k policies and the corresponding models M k ( k = k min . . . k max ) .
We can obtain an adaptive policy , whose lag at each step is between k min and k max , meaning that at each step , the target sequence falls behind the source sequence at most k max tokens and at least k min tokens .
At each step , there is a wait -k policy synchronizing the adaptive policy , meaning that they have the same lag at that step .
Specifically , at any step t , if the lag of the adaptive policy is k , then we apply the NMT model with the wait -k policy and force it to predict existing target tokens until step t , when the model will make a new prediction as the output of step t.
However , the above method only shows how to simulate the adaptive policy to make a prediction at one step if we would like to write at that step , but it does not tell us at which steps we should write .
We utilize the model confidence to make such a decision .
Specifically , we set a probability threshold ? k for each wait -k policy .
At each step , if the NMT model follows a wait -k policy , and predicts the most likely token with probability higher than the threshold ?
k , then we consider the model is confident on this prediction , and choose WRITE action ; otherwise , we choose READ action .
Figure 2 gives an example for this process .
We define the process of applying a wait -k model M k with a wait -k policy on a given sequence pair ( x , y ) by the following y top , p top ?
P k ( M k , x , y ) which forces model M k to predict y , and returns the top token y top at the final step with the corresponding probability p top .
The process of reading and returning a new source token is denoted by READ ( ) , and expression x ?
x represents to append an element x to the end of sequence x .
We denote by < s> and </s> the start symbol and end symbol of a sequence .
Then Algorithm 1 gives the pseudocode of the above method .
Algorithm 1 ST decoding with an adaptive policy Input : two integers k min and k max , a set of NMT models M k , and a sequence of thresholds ?
k for k min ? k ? k max . while x | x| = </s> and y |y | = </s> do k ? | x | ?
| y | y top , p top ?
P k ( M k , x , y ) if k ?
k max or ( k ? k min and p top ? ? k ) y ?
y ? y top Write action else x ? x ? READ ( ) Read
Ensemble of Wait-k Models Using the corresponding model M k with each waitk policies may not give us the best performance .
If we have a set of models trained independently with different wait -k policies , then we can apply ensemble of those models ( Dietterich , 2000 ; Hansen and Salamon , 1990 ) to improve the translation quality , which is also used to improve the translation quality of full-sentence translation ( Stahlberg and Byrne , 2017 ) .
However , there may be two issues to apply ensemble of all models : ( 1 ) the runtime for each prediction could be longer , resulting in higher latency ; and ( 2 ) the translation accuracy may be worse , for the best model for one policy may give bad performance when doing inference with another policy .
To avoid these , we propose to apply ensemble of the top - 3 models for each policy .
That is , we first generate distribution with the top - 3 models independently with the same policy , and then take the arithmetic average of the three distributions as the final token distribution at that step .
Experiments Datasets and models .
We conduct experiments on Chinese ?
English ( ZH?EN ) and German ?
English ( DE?EN ) translation .
For ZH?EN , we use NIST corpus ( 2 M sentence pairs ) as training set , NIST 2006 as dev set , and NIST 2008 as test set .
For DE?EN , we use WMT15 parallel corpus for training , newstest - 2013 for validation and newstest - 2015 for testing .
All datasets are tokenized and segmented into sub-word units with byte-pair encoding ( Sennrich et al. , 2016 ) .
We take Transformer - base ( Vaswani et al. , 2017 ) as our model architecture , and follow Ma et al . ( 2019 ) to train our model with wait -k policies for integer 1 ? k ? 10 .
In the following experiments , we only use catchup ( Ma et al. , 2019 ) for DE?EN translation , where we read one additional source token after every 6 predictions .
We use BLEU ( Papineni et al. , 2002 ) as the translation quality metric , and Average Lagging ( AL ) ( Ma et al. , 2019 ) as the latency metric , which measures the lag behind source in terms of the number of source tokens .
Performance with different policies .
We first evaluate the performance of each model with different policies , which helps us to choose models for different policies .
Specifically , we apply each model with ten different wait -k policies on dev set to compare the performance .
Fig. 3 shows the results of five models .
We find the best model for one policy may not be the one trained with that policy .
For example , on ZH?EN translation , the best model for wait - 1 policy is the one trained with wait - 3 policy .
Further , there is no one model could achieve the best performance for all policies .
Comparing different methods .
We compare our method with others from literature : wait -k method ( Ma et al. , 2019 ) ( train and test models with the same wait -k policy ) , test -time waitk method ( Ma et al. , 2019 ) Figure 4 : Performance of different methods on test set .
Our single method achieves better BLEU scores than wait -k method with same latency .
And our ensemble top - 3 method achieves the highest BLEU scores with same latency , and outperforms full-sentence greedy search with AL < 9 . # $ : full-sentence translation with greedy search and beam search ( beam size = 10 ) respectively .
s 0 ? { 4 , 6 } and ? ? { 2 , 4 } ; and for wait - if- worse we set s 0 ? { 1 , 2 , 4 , 6 } and ? ? { 1 , 2 } .
For our method , we test three different cases : ( 1 ) single , where for each policy we apply the corresponding model that trained with the same policy ; ( 2 ) ensemble top - 3 , where for each policy we apply the ensemble of 3 models that achieve the highest BLEU scores with that policy on dev set ; ( 3 ) ensemble all , where we apply the ensemble of all 10 models for each policy .
For thresholds , we first choose ?
1 and ? 10 , and the other thresholds are computed in the following way : ? i = ? 1 ?d?( i?1 ) for integer 1 ? i ? 10 and d = ( ? 1 ? ? 10 ) /9 .
We test with ?
1 ? { 0.2 , 0.3 , 0.4 , 0.5 , 0.6 , 0.7 , 0.8 , 0.9 } , ? 10 = 0 and ?
1 = 1 , ? 10 ? { 0 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 , 0.6 , 0.7 , 0.8 , 0.9 } , totally 18 different settings in our experiments .
The reason behind these settings is that we assume our adaptive policy cannot be either too aggressive or too conservative ( as mentioned at the beginning of Section 3 ) .
The policy is the most aggressive for k = 1 , so we set ?
1 as the largest ; while for k = 10 the policy is the most conservative , so we set ? 10 the smallest .
The comparison is provided in Fig. 4 ( the corresponding numeric scores are provided in Appendix A ) .
Compared with wait -k method , our single method achieves improvement of up to 2 BLEU point , and our ensemble top - 3 achieves improvement up to 4 BLEU points .
Compared with full-sentence translation , our ensemble top - 3 surprisingly outperforms greedy search with much lower latency ( AL < 9 ) , and achieves BLEU scores close to that from beam search ( see Table 2 ) .
We also give one ZH?EN translation example from dev set in Table 1 to compare different methods , showing that our method achieves an adaptive policy with low latency and good translation quality .
Efficiency .
To evaluate the efficiency , we present in Table 3 the averaged time needed to predict one token for different methods .
These methods are tested on one GeForce GTX TITAN -X GPU for ZH ?EN test set .
We can see that our ensemble top - 3 method needs about 0.2 seconds to make a prediction on average .
However , if the source sentence is revealed in the same speed as general " we express the most sincere sympathy and condol-ences to the families of the victims . " Table 1 : One example from ZH ?EN dev set .
Although wait - 3 method has low latency , it makes anticipations on " offered " and " wishes " , and adds additional words " he said " , which are not accurate translation .
Our ensemble top - 3 method could provide better translation with lower latency .
speech , which is about 0.6 seconds per token in Chinese ( Zheng et al. , 2019 c ) , then our method is still faster than that ( which means that it could be used for real-time ) .
Further , we believe the efficiency of our method could be improved with other techniques , such as parallelizing the running of three models in the ensemble , making it less an issue .
Method Method Time per Token Full-sentence 0.0122 s Wait -3 0.0162 s Single ( ? 1 = 0.4 , ? 10 = 0 ) 0.1057 s Ensemble Top -3 ( ? 1 = 0.4 , ? 10 = 0 ) 0.2085 s
Conclusions
We have designed a simple heuristic algorithm to obtain an adaptive policy based on a set of wait -k policies , and applied ensemble in our method to improve the translation quality while maintaining low latency .
Experiments show that our method not only outperforms the original wait -k method with relatively large gap , but also surpasses greedy full-sentence translation with much lower latency .
A Appendices
We provide the complete results of Figure 4 from Section 5 in the following tables , where AL is Average Lagging .
Note that for ZH?EN , we use 4reference BLEU ; while for DE ?EN we use singlereference BLEU .
Figure 1 : 1 Figure 1 : An adaptive policy ( in bold arrows ) composed of three wait -k policies ( k = 1 , 2 , 3 ) .
Figure 2 : 2 Figure 2 : Choose actions based on model confidence .
In this example , we will choose an action based on the top probability p top , and apply a new policy ( the dotted arrows ) after the chosen action .
( apply full- sentence model with wait -k policies ) , wait - if- diff ( Cho and Esipova , 2016 ) ( start with s 0 source tokens , choose to read only if top token at t-th step diffs from that at ( t ? ? ) - th step ) , and wait - if- worse ( Cho and Esipova , 2016 ) ( start with s 0 source tokens , choose to read only if the top probability at t-th step is smaller than that at ( t ? ? ) - th step ) .
For wait - if- diff we set Performance of models with different policies on dev set .
Each model is trained with one wait -k policy ( i.e. wait -k model ) and tested with ten different wait -k policies for integer 1 ? k ? 10 .
Each line corresponds to one model .
# $ : full-sentence translation with greedy search and beam search ( beam size = 10 ) respectively .
4- ref BLEU 27 29 31 33 35 37 39 45 43 41 4 ZH EN 6 8 10 12 33 AL wait - 1 model wait - 3 model wait - 5 model wait - 7 model wait - 9 model 1 - ref BLEU 28 30 16 18 20 22 24 26 DE EN 4 6 8 10 12 wait - 1 model 30 AL wait - 3 model wait - 5 model wait - 7 model wait - 9 model 40 Figure 3 : 2 ZH EN 4 28 30 32 34 36 38 4 - ref BLEU 6 8 10 12 14 wait -k method single ensemble all ensemble top3 test- time wait -k 30 AL wait -if- diff wait-if- worse 1- ref BLEU 30 20 22 24 26 28 DE EN 2 4 6 8 10 12 wait -k method single ensemble all ensemble top3 test- time wait -k 29 AL wait - if- diff wait - if- worse
Table 2 : 2 Compare our method with full-sentence translation .
Our ensemble top - 3 method could outperform the greedy search and get close to beam search ( beam size = 10 ) with lower latency .
ZH?EN DE?EN BLEU AL BLEU AL Full-sentence ( greedy ) 39.47 29.551 29.74 28.581 Full-sentence ( beam ) 40.71 29.551 30.24 28.581 Ensemble Top -3 40.15 8.209 30.15 8.766
Table 3 : 3 Averaged time needed by different methods to predict one token on ZH ?EN test set .
Table 4 : 4 Complete results of wait - if- diff , ensemble top - 3 and ensemble all .
= 1.0 , ? 10 = 0.5 38.77 10.141 29.29 8.079 ? 1 = 1.0 , ? 10 = 0.6 38.75 10.463 29.21 8.589 ? 1 = 1.0 , ? 10 = 0.7 38.76 10.733 29.25 9.044 ? 1 = 1.0 , ? 10 = 0.8 38.51 10.944 29.19 9.491 ? 1 = 1.0 , ? 10 = 0.9 38.49 11.201 29.10 Hyper-parameters ZH?EN BLEU AL DE?EN BLEU AL wait - if- diff ensemble top-3 Hyper-parameters s 0 = 4 , ? = 2 s 0 = 6 , ? = 2 s 0 = 4 , ? = 4 s 0 = 6 , ? = 4 ? 1 = 0.2 , ? 10 = 0.0 32.10 ZH?EN BLEU AL 28.52 5.493 30.02 6.108 33.91 9.764 34.13 10.075 25.45 DE?EN BLEU AL 22.16 5.121 22.56 5.731 25.16 8.763 9.177 2.880 24.55 2.171 ? 1 = 0.3 , ? 10 = 0.0 33.94 3.729 25.63 2.592 ? 1 = 0.4 , ? 10 = 0.0 35.92 4.762 26.52 3.068 ? 1 = 0.5 , ? 10 = 0.0 37.43 5.710 27.20 3.523 ? 1 = 0.6 , ? 10 = 0.0 38.56 6.538 27.97 4.096 ? 1 = 0.7 , ? 10 = 0.0 38.96 7.109 28.71 4.628 ? 1 = 0.8 , ? 10 = 0.0 39.82 7.675 29.06 5.101 ? 1 = 0.9 , ? 10 = 0.0 40.15 8.209 29.40 5.616 ? 1 = 1.0 , ? 10 = 0.0 40.35 8.520 29.62 6.038 ? 1 = 1.0 , ? 10 = 0.1 40.18 9.013 29.88 6.482 ? 1 = 1.0 , ? 10 = 0.2 40.36 9.462 29.80 6.923 ? 1 = 1.0 , ? 10 = 0.3 40.32 9.848 29.84 7.379 ? 1 = 1.0 , ? 10 = 0.4 40.56 10.185 29.99 7.882 ? 1 = 1.0 , ? 10 = 0.5 40.61 10.480 30.04 8.347 ? 1 = 1.0 , ? 10 = 0.6 40.52 10.739 30.15 8.766 ? 1 = 1.0 , ? 10 = 0.7 40.51 10.939 30.16 9.182 ? 1 = 1.0 , ? 10 = 0.8 40.41 11.134 30.17 9.582 ? 1 = 1.0 , ? 10 = 0.9 40.36 11.310 30.15 10.023 ? 1 = 0.2 , ? 10 = 0.0 26.81 1.231 24.55 2.383 wait - if- worse single s 0 = 1 , ? = 1 s 0 = 2 , ? = 1 s 0 = 4 , ? = 1 s 0 = 6 , ? = 1 s 0 = 1 , ? = 2 s 0 = 2 , ? = 2 s 0 = 4 , ? = 2 s 0 = 6 , ? = 2 ? 1 = 0.2 , ? 10 = 0.0 31.24 31.67 32.28 33.36 34.78 36.28 12.731 26.52 10.268 6.857 21.77 4.930 7.170 22.26 5.005 7.964 23.30 5.697 9.319 24.27 6.914 36.62 13.133 26.39 10.138 36.89 13.629 26.68 10.806 37.50 14.662 27.09 11.877 3.335 22.72 1.989 3.781 23.85 2.211 ? 1 = 0.3 , ? 10 = 0.0 32.96 4.455 25.05 2.672 ? 1 = 0.4 , ? 10 = 0.0 34.39 5.254 25.61 3.047 ? 1 = 0.5 , ? 10 = 0.0 36.23 5.750 26.73 3.627 ? 1 = 0.6 , ? 10 = 0.0 36.75 6.526 27.21 4.187 ? 1 = 0.7 , ? 10 = 0.0 36.95 7.030 27.84 4.785 ? 1 = 0.8 , ? 10 = 0.0 37.67 7.604 28.41 5.330 ? 1 = 0.9 , ? 10 = 0.0 38.41 ? 1 = 1.0 , ? 10 = 0.0 37.89 8.021 28.81 5.813 8.458 29.02 6.169 ? 1 = 1.0 , ? 10 = 0.1 38.45 8.839 29.20 6.596 ? 1 = 1.0 , ? 10 = 0.2 38.20 9.386 29.32 7.042 ? 1 = 1.0 , ? 10 = 0.3 38.59 9.805 29.19 7.581 ? 1 = 1.0 , ? 10 = 0.4 38.81 ? 1 9.972 ensemble all ? 1 = 0.3 , ? 10 = 0.0 32.61 ? 1 = 0.4 , ? 10 = 0.0 35.96 ? 1 = 0.5 , ? 10 = 0.0 37.31 ? 1 = 0.6 , ? 10 = 0.0 38.40 ? 1 = 0.7 , ? 10 = 0.0 38.64 ? 1 = 0.8 , ? 10 = 0.0 39.10 ? 1 = 0.9 , ? 10 = 0.0 39.18 ? 1 = 1.0 , ? 10 = 0.0 38.80 ? 1 = 1.0 , ? 10 = 0.1 38.67 ? 1 = 1.0 , ? 10 = 0.2 38.62 ? 1 = 1.0 , ? 10 = 0.3 38.62 10.029 27.98 3.536 25.74 5.219 26.46 6.270 26.97 6.959 27.20 7.590 27.63 8.134 27.78 8.523 27.89 8.761 27.89 9.264 27.94 9.682 27.86 ? 1 = 1.0 , ? 10 = 0.4 38.62 10.274 28.17 ? 1 = 1.0 , ? 10 = 0.5 38.57 10.477 28.17 ? 1 = 1.0 , ? 10 = 0.6 38.60 10.632 28.23 ? 1 = 1.0 , ? 10 = 0.7 38.59 10.770 28.31 ? 1 = 1.0 , ? 10 = 0.8 38.58 10.890 28.32 ? 1 = 1.0 , ? 10 = 0.9 38.56 11.029 28.34 2.851 3.367 3.973 4.666 5.241 5.828 6.290 6.650 7.151 7.594 8.014 8.395 8.710 8.989 9.253 9.517 9.830 wait -k test- time wait -k k = 1 k = 2 k = 3 k = 4 k = 5 k = 6 k = 7 k = 8 k = 9 k = 10 k = 1 k = 2 k = 3 k = 4 k = 5 k = 6 k = 7 k = 8 28.30 30.74 32.45 33.80 34.67 35.80 36.77 37.49 38.17 10.560 28.92 2.968 21.31 3.519 23.10 5.076 25.22 5.896 26.29 7.041 27.42 8.175 27.73 9.033 28.53 9.542 28.64 38.44 11.337 29.06 10.261 1.695 2.652 3.768 4.697 5.771 6.658 7.569 8.548 9.379 27.54 2.884 21.84 3.204 29.57 3.873 22.64 3.954 30.70 5.103 22.96 4.729 31.37 5.941 23.60 5.558 32.67 6.993 24.48 6.412 33.92 8.051 24.92 7.298 34.16 8.850 25.23 8.144 34.95 9.720 25.48 9.025 k = 9 35.34 10.566 26.05 9.867 k = 10 35.87 11.383 26.28 10.699
Table 5 : 5 Complete results of wait - if- worse , single , wait -k and test- time wait -k .
