title
A Multi-Task Architecture on Relevance - based Neural Query Translation
abstract
We describe a multi-task learning approach to train a Neural Machine Translation ( NMT ) model with a Relevance - based Auxiliary Task ( RAT ) for search query translation .
The translation process for Cross-lingual Information Retrieval ( CLIR ) task is usually treated as a black box and it is performed as an independent step .
However , an NMT model trained on sentence - level parallel data is not aware of the vocabulary distribution of the retrieval corpus .
We address this problem with our multitask learning architecture that achieves 16 % improvement over a strong NMT baseline on Italian - English query - document dataset .
We show using both quantitative and qualitative analysis that our model generates balanced and precise translations with the regularization effect it achieves from multi-task learning paradigm .
Introduction CLIR systems retrieve documents written in a language that is different from search query language ( Nie , 2010 ) .
The primary objective of CLIR is to translate or project a query into the language of the document repository ( Sokokov et al. , 2013 ) , which we refer to as Retrieval Corpus ( RC ) .
To this end , common CLIR approaches translate search queries using a Machine Translation ( MT ) model and then use a monolingual IR system to retrieve from RC .
In this process , a translation model is treated as a black box ( Sokolov et al. , 2014 ) , and it is usually trained on a sentence level parallel corpus , which we refer to as Translation Corpus ( TC ) .
We address a pitfall of using existing MT models for query translation ( Sokokov et al. , 2013 ) .
An MT model trained on TC does not have any knowledge of RC .
In an extreme setting , where there are no common terms between the target side of TC and RC , a well trained and tested translation model would fail because of vocabulary mismatch between the translated query and documents of RC .
Assuming a relaxed scenario where some commonality exists between two corpora , a translation model might still perform poorly , favoring terms that are more likely in TC but rare in RC .
Our hypothesis is that a search query translation model would perform better if a translated query term is likely to appear in the both retrieval and translation corpora , a property we call balanced translation .
To achieve balanced translations , it is desired to construct an MT model that is aware of RC vocabulary .
Different types of MT approaches have been adopted for CLIR task , such as dictionarybased MT , rule- based MT , statistical MT etc .
( Zhou et al. , 2012 ) .
However , to the best of our knowledge , a neural search query translation approach has yet to be taken by the community .
NMT models with attention based encoder-decoder techniques have achieved state - of - the - art performance for several language pairs ( Bahdanau et al. , 2015 ) .
We propose a multi-task learning NMT architecture that takes RC vocabulary into account by learning Relevance - based Auxiliary Task ( RAT ) .
RAT is inspired from two word embedding learning approaches : Relevance - based Word Embedding ( RWE ) ( Zamani and Croft , 2017 ) and Continuous Bag of Words ( CBOW ) embedding ( Mikolov et al. , 2013 ) .
We show that learning NMT with RAT enables it to generate balanced translation .
NMT models learn to encode the meaning of a source sentence and decode the meaning to generate words in a target language ( Luong et al. , 2015 ) .
In the proposed multi-task learning model , RAT shares the decoder embedding and final representation layer with NMT .
Our architecture answers the following question :
In the decoding stage , can we restrict an NMT model so that it does not only generate terms that are highly likely in TC ? .
We show that training a strong baseline NMT with RAT roughly achieves 16 % improvement over the baseline .
Using a qualitative analysis , we further show that RAT works as a regularizer and prohibits NMT to overfit to TC vocabulary .
Balanced Translation Approach
We train NMT with RAT to achieve better query translations .
We improve a recently proposed NMT baseline , Transformer , that achieves state - of - theart results for sentence pairs in some languages ( Vaswani et al. , 2017 ) .
We discuss Transformer , RAT , and our multi-task learning architecture that achieves balanced translation .
NMT and Transformer
In principle , we could adopt any NMT and combine it with RAT .
An NMT system directly models the conditional probability P ( t i |s i ) of translating a source sentence , s i = s 1 i , . . . , s n i , to a target sentence t i = t 1 i , . . . , t n i .
A basic form of NMT comprises two components : ( a ) an encoder that computes the representations or meaning of s i and ( b ) a decoder that generates one target word at a time .
State - of- the- art NMT models have an attention component that " searches for a set of positions in a source sentence where the most relevant information is concentrated " ( Bahdanau et al. , 2015 ) .
For this study , we use a state - of- the - art NMT model , Transformer ( Vaswani et al. , 2017 ) , that uses positional encoding and self attention mechanism to achieve three benefits over the existing convolutional or recurrent neural network based models : ( a ) reduced computational complexity of each layer , ( b ) parallel computation , and ( c ) path length between long- range dependencies .
Relevance - based Auxiliary Task ( RAT )
We define RAT a variant of word embedding task ( Mikolov et al. , 2013 ) .
Word embedding approaches learn high dimensional dense representations for words and their objective functions aim to capture contextual information around a word .
Zamani and Croft ( 2017 ) proposed a model that learns word vectors by predicting words in relevant documents retrieved against a search query .
We follow the same idea but use a simpler learning approach that is suitable for our task .
They tried to predict words from the relevance model ( Lavrenko and Croft , 2001 ) computed from a query , which does not work for our task because the connection between a query and ranked sentences falls rapidly after the top one ( see below ) .
We consider two data sources for learning NMT and RAT jointly .
The first one is a sentence - level parallel corpus , which we refer to as translation corpus , T C = {( s i , t i ) ; i = 1 , 2 , . . . m} .
The second one is the retrieval corpus , which is a collection of k documents RC = { D 1 , D 2 , . . .
D k } in the same language as t i .
Our word-embedding approach takes each t i ?
T C , uses it as a query to retrieve the top document D top i .
After that we obtain t i by concatenating t i with D top i and randomly shuffling the words in the combined sequence .
We then augment T C using t i and obtain a dataset , T C = {( s i , t i , t i ) ; i = 1 , 2 , . . . m} .
We use t i to learn a continuous bag of words ( CBOW ) embedding as proposed by Mikolov et al . ( 2013 ) .
This learning component shares two layers with the NMT model .
The goal is to expose the retrieval corpus ' vocabulary to the NMT model .
We discuss layer sharing in the next section .
We select the single top document retrieved against a sentence t i because a sentence is a weak representation of information need .
As a result , documents at lower ranks show heavy shift from the context of the sentence query .
We verified this by observing that a relevance model constructed from the top k documents does not perform well in this setting .
We thus deviate from the relevance model based approach taken by Zamani and Croft ( 2017 ) and learn over the random shuffling of t i and a single document .
Random shuffling has shown reasonable effectiveness for word embedding construction for comparable corpus ( Vuli ? and Moens , 2015 ) .
Multi-task NMT Architecture
Our balanced translation architecture is presented in Figure 1 .
This architecture is NMT - model agnostic as we only propose to share two layers common to most NMTs : the trainable target embedding layer and the transformation function ( Luong et al. , 2015 ) that outputs a probability distribution over the union of the vocabulary of TC and RC .
Hence , the size of the vocabulary , | RC ? T C| , is much larger compared to TC and it enables the model to access RC .
In order to show task sharing clearly we placed two shared layers between NMT and RAT in Figure 1 .
We also show the two different paths taken by two different tasks at training time : the NMT path in shown with red arrows while the RAT path is shown in green arrows .
On NMT path training loss is computed as the sum of term-wise softmax with cross-entropy loss of the predicted translation and the human translation and it summed over a batch of sentence pairs , L N M T = ( s i , t i ) ?
T |t i | j=1 ? log P ( t j i |t <j i , s i ) .
We also use a similar loss function to train word embedding over a set of context ( ctx ) and pivot ( pvt ) pairs formed using t i as query to retrieve D top i using Query Likelihood ( QL ) ranker , L W E = ? ( ctx , pvt ) ? log P ( pvt | ctx ) .
This objective is similar to CBOW word embedding as context is used to predict pivot word Here , we use a scaling factor ? , to have a balance between the gradients from the NMT loss and RAT loss .
For RAT , the context is drawn from a context window following Mikolov et al . ( 2013 ) .
In the figure , ( s i , t i ) ?
T C and D top i represents the top document retrieved against t i .
The shuffler component shuffles t i and D top i and creates ( context , pivot ) pairs .
After that those data points are passed through a fully connected linear projection layer and eventually to the transformation function .
Intuitively , the word embedding task is similar to NMT as it tries to assign a large probability mass to a target word given a context .
However , it enables the transformation function and decoding layer to assign probability mass not only to terms from TC , but also to terms from RC .
This implicitly prohibits NMT to overfit and provides a regularization effect .
A similar technique was proposed by Katsuki Chousa ( 2018 ) to handle out - of- vocabulary or less frequent words for NMT .
For these terms they enabled the transformation ( also called the softmax cross-entropy layer ) to fairly distribute probability mass among similar words .
In contrast , we focus on relevant terms rather than similar terms .
Experiments and Results Data .
We experiment on two language pairs : { Italian , Finnish } ?
English .
Topics and relevance judgments are obtained from the Cross-Language Evaluation Forum ( CLEF ) 2000 - 2003 campaigns for bilingual ad-hoc retrieval tracks 1 . The Italian and French topics are human translations of a set of two hundred English topics .
Our retrieval corpus is the Los Angeles Times ( LAT94 ) comprising over 113 k news articles .
Topics without any relevant documents on LAT94 are excluded resulting in 151 topics for both Italian and Finnish language .
Among the 151 topics in our dataset , we randomly selected 50 queries for validation and 101 queries for test .
In the CLEF literature , queries are constructed from either the title field or a concatenation of title and description fields of the topic sets .
Following Vuli ? and Moens ( 2015 ) , we work on the longer queries .
For TC we use Europarl v7 sentence - aligned corpus ( Koehn , 2005 ) .
TC statistics in Table 1 indicates that we had around two million sentence pairs for each language pairs .
Text Pre-processing .
For having text consistency across TC and RC , we apply the following pre-processing steps .
Characters are normalized by mapping diacritic characters to the corresponding unmarked characters and lower-casing .
We remove non-alphabetic , non-printable , and punctuation characters from each word .
The NLTK library ( Bird and Loper , 2004 ) is used for tokenization and stop-word removal .
No stemming is performed .
Retrieval .
For ranking documents , after query translation , we use the Galago 's implementation 2 of query likelihood using Dirichlet smoothing ( Zhai and Lafferty , 2004 ) with default parameters .
Training Technique .
Before applying multitasking we train the transformer to obtain a reasonable MAP on the Val set .
Then we spawn our multi-task transformer from that point , also continuing to train the transformer .
We use an early stopping criterion to stop both the models , and evaluate performance on the test set .
For NMT training we use Stochastic Gradient Descent ( SGD ) with Adam Optimizer and learning rate of 0.01 .
We found that a learning rate of 10 ?5 with the same optimizer works well for the word embedding loss minimization .
From a training batch ( we use dynamic size training batches ) , more data points are actually created for the word embedding task because of large number of ( context , pivot ) pairs .
We allow the gradients from word embedding loss to pass through the multi-tasking model at first , and then apply NMT loss .
Setting a lower learning rate for the word embedding optimizer , and ? = 0.1 allows the NMT gradient updates to be competitive .
Evaluation .
Given that in CLIR the primary goal is to get a better ranked list of documents against a translated query , we only report Mean Average Precision ( MAP ) .
Results and Analysis Table 2 shows the effectiveness of our model ( multitask transformer ) over the baseline transformer ( Vaswani et al. , 2017 ) .
Our model achieves significant performance gains in the test sets over the baseline for both Italian and Finnish query translation .
The overall low MAP for NMT can possibly be improved with larger TC .
Moreover , our model validation approach requires access to RC index , and it slows down overall training process .
Hence , we could not train our model for a large number of epochs - it may be another cause of the low performance .
Balance of Translations .
We want to show that translation terms generated by our multi-task transformer are roughly equally likely to be seen in the Europarl corpus ( TC ) or the CLEF corpus ( RC ) .
Given a translation term t , we compute the ratio of t?T C count T C ( t ) and P RC ( t ) is calculated similarly .
Given a query q i and its translation T m ( q i ) provided by model m , we calculate the balance of m , B ( T m ( q i ) ) = t?Tm ( q ) P T C ( t ) P RC ( t ) | Tm ( q ) | .
If B ( T m ( q i ) ) is close to 1 , the translation terms are as likely in TC as in RC .
Figure 2 shows the balance values for transformer and our model for a random sample of 20 queries from the validation set of Italian queries , respectively .
Figure 3 shows the balance values for transformer and our model for a random sample of 20 queries from the test set of Italian queries , respectively .
It is evident that our model achieves better balance compared to baseline transformer , except for a very few cases .
Precision and Recall of Translations .
Given a query Q , consider Q = {q 1 , q 1 , . . . , q p } as the set of terms from human translation of Q and Q M = {q M 1 , q M 2 , . . . , q M q } as the set of translation terms generated by model M .
We define P M ( Q ) = Q M ?Q |Q M | and R M ( Q ) = Q M ?Q | Q | as precision and recall of Q for model M . In Table 3 former and our model across our train and validation query set over two language pairs .
Our model generates precise translation , i.e. it avoids terms that might be useless or even harmful for retrieval .
Generally , from our observation , avoided terms are highly likely terms from TC and they are generated because of translation model overfitting .
Our model achieves a regularization effect through an auxiliary task .
This confirms results from existing multi-tasking literature ( Ruder , 2017 ) .
To explore translation quality , consider pair of sample translations provided by two models .
For example , against an Italian query , medaglia oro super vinse medaglia oro super olimpiadi invernali lillehammer , translated term set from our model is { gold , coin , super , free , harmonising , won , winter , olympics } , while transformer output is {olympic , gold , one , coin , super , years , won , parliament , also , two , winter} .
Term set from human translation is : { super , gold , medal , won , lillehammer , olypmic , winter , games } .
Transformer comes up with terms like parliament , also , two and years that never appears in human translation .
We found that these terms are very likely in Europarl and rare in CLEF .
Our model also generates terms such as harmonising , free , olympics that not generated by transformer .
However , we found that these terms are equally likely in Europarl and CLEF .
Conclusion
We present a multi-task learning architecture to learn NMT for search query translation .
As the motivating task is CLIR , we evaluated the ranking effectiveness of our proposed architecture .
We used sentences from the target side of the parallel corpus as queries to retrieve relevant document and use terms from those documents to train a word embedding model along with NMT .
One big challenge in this landscape is to sample meaningful queries from sentences as sentences do not directly convey information need .
In the future , we hope to learn models that are able to sample search queries or information needs from sentences and use the output of that model to get relevant documents .
Figure 1 : 1 Figure 1 : The architecture of our multi-task NMT .
Note that , rectangles indicate data sources and rectangles with rounded corners indicate functions or layers .
