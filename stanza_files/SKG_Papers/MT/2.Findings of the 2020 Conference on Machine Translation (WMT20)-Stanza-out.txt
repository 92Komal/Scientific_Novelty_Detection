title
Findings of the 2020 Conference on Machine Translation ( WMT20 )
abstract
This paper presents the results of the news translation task and the similar language translation task , both organised alongside the Conference on Machine Translation ( WMT ) 2020 .
In the news task , participants were asked to build machine translation systems for any of 11 language pairs , to be evaluated on test sets consisting mainly of news stories .
The task was also opened up to additional test suites to probe specific aspects of translation .
In the similar language translation task , participants built machine translation systems for translating between closely related pairs of languages .
Introduction
The Fifth Conference on Machine Translation ( WMT20 ) 1 was held online with EMNLP 2020 and hosted a number of shared tasks on various aspects of machine translation .
This conference built on 14 previous editions of WMT as workshops and conferences ( Koehn and Monz , 2006 ; , 2008 , 2010 , 2011 , 2012 Bojar et al. , 2013
Bojar et al. , , 2014 Bojar et al. , , 2015 Bojar et al. , , 2017 Barrault et al. , 2019 ) .
This year we conducted several official tasks .
We report in this paper on the news and similar translation tasks .
Additional shared tasks are described in separate papers in these proceedings : ? automatic post-editing ( Chatterjee et al. , 2020 ) ? biomedical translation ( Bawden et al. , 2020 b ) ? chat translation ( Farajian et al. , 2020 ) ? lifelong learning ( Barrault et al. , 2020 ) 1 http://www.statmt.org/wmt20/ ? metrics ( Mathur et al. , 2020 ) ? parallel corpus filtering ? quality estimation ( Specia et al. , 2020a ) ? robustness ( Specia et al. , 2020 b ) ? unsupervised and very low-resource translation ( Fraser , 2020 )
In the news translation task ( Section 2 ) , participants were asked to translate a shared test set , optionally restricting themselves to the provided training data ( " constrained " condition ) .
We included 22 translation directions this year , with translation between English and each of Chinese , Czech , German and Russian , as well as French to and from German being repeated from last year , and English to and from Inuktitut , Japanese , Polish and Tamil being new for this year .
Furthermore , English to and from Khmer and Pashto were included , using the same test sets as in the corpus filtering task .
The translation tasks covered a range of language families , and included both low-resource and high- resource pairs .
System outputs for each task were evaluated both automatically and manually , but we only include the manual evaluation here .
The human evaluation ( Section 3 ) involves asking human judges to score sentences output by anonymized systems .
We obtained large numbers of assessments from researchers who contributed evaluations proportional to the number of tasks they entered .
In addition , we used Mechanical Turk to collect further evaluations , as well as a pool of linguists .
This year , the official manual evaluation metric is again based on judgments of adequacy on a 100 - point scale , a method ( known as " direct assessment " ) that we explored in the previous years with convincing results in terms of the trade - off between annotation effort and reliable distinctions between systems .
The primary objectives of WMT are to evaluate the state of the art in machine translation , to disseminate common test sets and public training data with published performance numbers , and to refine evaluation and estimation methodologies for machine translation .
As before , all of the data , translations , and collected human judgments are publicly available .
2
We hope these datasets serve as a valuable resource for research into datadriven machine translation , automatic evaluation , or prediction of translation quality .
News translations are also available for interactive visualization and comparison of differences between systems at http://wmt.ufal.cz/ using MT - ComparEval ( Sudarikov et al. , 2016 ) .
In order to gain further insight into the performance of individual MT systems , we organized a call for dedicated " test suites " , each focusing on some particular aspect of translation quality .
A brief overview of the test suites is provided in Section 4 .
Following the success of the first Similar Language Translation ( SLT ) task at WMT 2019 and the interest of the community in this topic ( Costajuss ?
et al. , 2018 ; , we organize a second iteration of the SLT task at WMT 2020 .
The goal of the shared task is to evaluate the performance of state - of - the - art MT systems on translating between pairs of closely - related languages from the same language family .
SLT 2020 features five pairs of similar languages from three language families : Indo-Aryan ( Hindi and Marathi ) , Romance ( Catalan , Spanish , and Portuguese ) , and South -Slavic ( Croatian , Serbian , and Slovene ) .
Translations were evaluated in both directions using three automatic metrics : BLEU , RIBES , and TER .
Results and main findings of the SLT shared task are discussed in Section 5 .
News Translation Task
This recurring WMT task assesses the quality of MT on news domain text .
As in the previous year , we included Chinese , Czech , German and Russian ( into and out of English ) as well as French - German .
New language pairs for this year were Inuktitut , Japanese , Polish and Tamil ( to and from 2 http://statmt.org/wmt20/results.html English ) .
We also included the two language pairs from the corpus filtering task ( Pashto?
English and Khmer?
English ) , to give participants the opportunity to build and test MT systems using the large noisy corpora released for that task .
Test Data
As in previous years , the test sets consist ( as far as possible ) of unseen translations prepared specially for the task .
The test sets are publicly released to be used as translation benchmarks in the coming years .
Here we describe the production and composition of the test sets .
The test sets differed along several dimensions , which we list in Table 1 .
The differing aspects of the test sets are as follows : Domain
Most test sets are drawn from the " news " domain , which means the source texts were extracted from online news websites , and the translations were produced specifically for the task .
The Pashto ?English and Khmer ?
English test sets were drawn from wikipedia and , as last year , the French ?
German test sets concentrated on EU - related news .
Due to limited resources and data available , the Inuktitut ?
English test sets contain documentand sentence - aligned data collected from two domains : news and parliamentary .
The news data were extracted from the Nunatsiaq News online news website .
The parliamentary data were debates from the Nunavut Hansard that are more recent than the training corpus .
Development ?
For new languages we released a development set , produced in the same way as the test set .
Sentence-split ?
For some pairs , we did not sentence - split the source texts .
In these cases , we extracted the text from the HTML source with paragraph breaks retained , and asked translators to maintain only the paragraph breaks .
This was done in order to try to improve the quality of the human translation by allowing the translators more freedom .
Some analysis of the paragraph-split pairs is presented in Section 2.1.1 .
Directional ?
For most language pairs the source-side of the test set is the original , and the target -side of the test set is the translation .
This is in contrast to the situation up until 2018 when our test sets were constructed from both " sourceoriginal " and " target-original " parts .
Where a development set is provided , it is a mixture of both " source-original " and " target-original " texts , in order to maximise its size , although the original language is always marked in the sgm file , except for Inuktitut ?
English .
The consequences of directionality in test sets has been discussed recently in the literature ( Freitag et al. , 2019 ; Laubli et al. , 2020 ; , and the conclusion is that it can have an effect on detrimental effect on the accuracy of system evaluation .
We use " source-original " parallel sentences wherever possible , on the basis that it is the more realistic scenario for practical MT usage .
Exception : the test sets for the two Inuktitut ?
English translation directions contain the same data , without regard to original direction .
For most news text in the test and development sets , English was the original language and Inuktitut the translation , while the parliamentary data mixes the two directions .
The origins of the news test documents is shown in Table 5 , and the size of the test sets in terms of sentence pairs and words is given in Figure 4 .
We generally aimed for 1000 sentences for a new language pair , and 2000 sentences for a previously used language pair ( since there was no need to create a development set for a previously - used language pair ) .
For test sets where the source was not sentence -split ( see below ) we aimed for an equivalent to 2000 sentences , but in running words .
In order to improve the consistency and quality of the test set translations , this year we prepared common translator briefs to be sent to each agency we used .
We show the translator briefs in Appendix B ( for sentence - split sources ) and Appendix C ( for paragraph - split sources ) .
Paragraph-split Test Sets
For the language pairs English ?
Czech , English ?
German and English ?
Chinese , we provided the translators with paragraph-split texts , instead of sentence -split texts .
We did this in order to provide the translators with greater freedom and , hopefully , to improve the quality of the translation .
Allowing translators to merge and split sentences removes one of the " translation shifts " identified by Popovic ( 2019 ) , which can make translations create solely for MT evaluation different from translations produced for other purposes .
We first show some descriptive statistics of the source texts , for Czech , English and German , in Table 2 , where we used the Moses sentence splitter to provide sentence boundaries .
We can see that the number of sentences per paragraph is much lower for English , where in fact 70 % of paragraphs only have single sentence .
For Czech and German , the mean sentences per paragraph is quite similar ( 2.62 vs. 2.52 ) .
The main question though , is whether translators tended to preserve the sentence structure when translating .
To determine this , we split both source paragraphs and translations into sentence , and aligned them using hunalign ( Varga et al. , 2005 ) with the bitextor dictionaries ( Espl ?- Gomis , 2009 ) .
In Table 4 we show the counts of 1 - 1 sentence alignments , as well as cases where the translator merged or split neighbouring sentences .
Note that these counts are approximate , since they are affected by errors in the automatic splitting and alignment .
Looking through examples of merges and splits , we see that most of them are relatively simple changes , where the translator has merged to clauses into a sentence , or split a sentence to clauses .
Examples of such merges and splits are shown in Table 3 , where the first and second are simple merges or splits , whereas the third is a rare case of more complex reordering .
We leave a detailed analysis of the translators ' treatment of paragraph - split data for future work .
Training Data
As in past years we provided a selection of parallel and monolingual corpora for model training , and development sets to tune system parameters .
Participants were permitted to use any of the provided corpora to train systems for any of the language pairs .
As well as providing updates on many of the previously released data sets , we included several new data sets , mainly to support the new language pairs .
These included Wikimatrix ( Schwenk et al. , 2019 ) , which was added for all language pairs where it was available .
The news commentary and europarl corpora that we have been using since the earliest news task now have " data sheets " , describing the data sets in standardised format ( Costajuss ?
et al. , 2020 ) . For Tamil-English , we additionally included some recently crawled multilingual parallel corpora from Indian government websites ( Haddow and Kirefu , 2020 ; Siripragada et al. , 2020 ) , the Tanzil corpus ( Tiedemann , 2009 ) 14,948,900 17,380,340 48,125,573 50,506,059
14,691,199
16,995,232 47,517,102 55,366 , 34,801,119
39,197,172 113,445,806 118,077,685 -72,320,248 50,061,388 93,828,313 102,937,537 3,057,383 3,766,628 - 58,615,891 68,249,384 3 , 074 , 921 , 453 2 , 872 , 785 , 485 333 , 498 , 145 1 , 168 , 529 , 851 1 , 422 , 729 , 881 Words 65 , 104 , 585 , 881 65 , 147 , 123 , 742 6 , 702 , 445 , 552 23 , 332 , 529 , 629 40 , 639 , 985 , 955 Dist. 342 , 149 , 665 338 , 410 , 238 48 , 788 , 665 90 , 497 , 177 213 , 298 , 869 Chinese Inuktitut Tamil Pashto French Sent. 1 , 672 , 324 , 647 296 , 730 28 , 828 , 239 6 , 558 , 180 4 , 898 , 012 , 480 , 611 632 , 363 , 004 218 , 412 , 919 126 , 364 , 574 , 513 16 , 780 , 006 23 , 531 , 044 363 , 878 , 959 1 : The characteristics of the test sets for the news tasks .
We show the domain that the test set was drawn from , whether or not we released a development set this year , whether the texts were sentence - split before translation , and whether the direction of translation was preserved .
For " directional " test sets , the entire source side of the test set was originally written in the source language , and then translated to the target language .
Non-directional test sets are a mixture of " source-original " and " target-original " texts .
Finally , we record whether or not the test set contained the original document boundaries .
Als R?ckzieher sei das aber nicht zu verstehen : " Ganz ? gypten ist der Tahrirplatz " .
But that should not be understood as a withdrawal .
" All of Egypt is Tahrir square . "
" Ich f?hle mich unglaublich geehrt und dem?tig , neben JLo die Latino - Community zu repr?sentieren .
" I feel incredibly honored and humbled to be next to J.
Lo , representing the Latino community that is such an important force in the United States , " Shakira shared in a video .
Denn diese hat eine unglaubliche St?rke in den USA " , teilte Shakira in einem Video mit .
Man k?nne die Unternehmen zwar nicht von der Umsatzsteuer auf Sachspenden befreien , erkl?rte das Ministerium auf eine Frage der Gr?nen-Bundestagsfraktion , ? ber die die Zeitungen der Funke-Mediengruppe am Freitag berichteten .
Although it is impossible to exempt companies from VAT on donations in kind , retailers could set the market value of unsaleable returns so low that they would need to pay no or only very little VAT , the Ministry explained in response to a question from the Greens parliamentary group , as reported in newspapers of the Funke media group on Friday .
Die H?ndler k?nnten aber den Marktwert der unverk ? uflichen Retouren so niedrig ansetzen , dass sie keine oder nur wenig Umsatzsteuer zahlen m?ssten .
Table 4 : How the translators treated sentences when translating the paragraph-split texts .
We sentence -split and automatically aligned source and translation .
We show the number and percentage of sentences which were translated 1 - 1 , as well as the number of times translators merged or split sentences when translating .
( Kunchukuttan et al. , 2018 ) and English and Tamil wikipedia dumps .
The training corpus for Inuktitut ?
English is the recently released Nunavut Hansard Inuktitut -English Parallel Corpus 3.0 ( Joanis et al. , 2020 ) .
For the Japanese ?
English tasks , we added several freely available parallel corpora to the training data .
It includes JParaCrawl v2.0 ( Morishita et al. , 2020 ) , a large web-based parallel corpus , Japanese -English Subtitle Corpus ( JESC ) ( Pryzant et al. , 2017 ) , the Kyoto Free Translation Task ( KFTT ) corpus ( Neubig , 2011 ) , constructed from the Kyoto-related Wikipedia articles , and TED Talks ( Cettolo et al. , 2012 ) .
The monolingual data we provided was similar to last year 's , with a 2019 news crawl added to all the news corpora .
In addition , we provided versions of the news corpora for Czech , English and German , with both the document and paragraph structure retained .
In other words , we did not apply sentence splitting to these corpora , and we retained the document boundaries and text ordering of the originals .
Training , development , and test data for Pashto ?
English and Khmer ?
English are shared with the Parallel Corpus Filtering Shared Task .
The training data mostly comes from OPUS ( software localization , Tatoeba , Global Voices ) , the Bible , and specialprepared corpora from TED Talks and the Jehova Witness web site ( JW300 ) .
The development and test sets were created as part of the Flores initiative by professional translation of Wikipedia content with careful vetting of the translations .
Please refer the to the Parallel Corpus Filtering Shared Task overview paper for details on these corpora .
Some statistics about the training and test materials are given in Figures 1 , 2 , 3 and 4 .
Submitted Systems
In 2020 , we received a total of 153 submissions .
The participating institutions are listed in Table 6 and detailed in the rest of this section .
Each system did not necessarily appear in all translation tasks .
We also included online MT systems ( originating from 4 services ) , which we anonymized as ONLINE -A , B , G , Z .
This year we introduced a new submission tool , OCELoT 4 , replacing the matrix that has been used in most previous editions .
Using OCELoT gave us more control over the submission and scoring process , for example we were able to limit the number of test submissions by each team , and we also displayed the submissions anonymously to avoid publishing any automatic scores .
A screenshot of OCELoT is shown in Figure 5 .
For presentation of the results , systems are treated as either constrained or unconstrained , depending on whether their models were trained only on the provided data .
Since we do not know how they were built , the online systems are treated as unconstrained during the automatic and human evaluations .
In the rest of this section , we provide brief details of the submitted systems , for those where the authors provided such details .
AFRL ( Gwinnup and Anderson , 2020 ) AFRL -SYSCOMB20 is a system combination consisting of two Marian transformer ensembles , one OpenNMT transformer system and a Moses phrase - based system .
AFRL -FINETUNE is an OpenNMT transformer system fine-tuned on newstest 2014-2017 .
( Xv , 2020 ) ARIEL XV is a Transformer base model trained with the Sockeye sequence modeling toolkit us -
ARIEL XV
Team Institution
AFRL Air Force Research Laboratory ( Gwinnup and Anderson , 2020 ) ARIEL XV Independent submission ( Xv , 2020 ) CUNI Charles University ( Popel , 2020 ( Popel , , 2018 Kocmi , 2020 Zoho Corporation ( no associated paper )
Table 6 : Participants in the shared translation task .
Not all teams participated in all language pairs .
The translations from the online systems were not submitted by their respective companies but were obtained by us , and are therefore anonymized in a fashion consistent with previous years of the workshop .
Charles University ( CUNI ) CUNI-DOCTRANSFORMER ( Popel , 2020 ) is similar to the sentence - level version ( CUNI - T2T - 2018 , CUBBITT ) , but trained on sequences with multiple sentences of up to 3000 characters .
CUNI -T2T -2018 ( Popel , 2018 ) , also called CUBBITT , is exactly the same system as in WMT2018 .
It is the Transformer model trained according to Popel and Bojar ( 2018 ) plus a novel concat-regime backtranslation with checkpoint averaging , tuned separately for CZ - domain and non CZ - domain articles , possibly handling also translation - direction ( " translationese " ) issues .
For cs?en also a coreference preprocessing was used adding the female- gender pronoun where it was pro-dropped in Czech , referring to a human and could not be inferred from a given sentence .
CUNI-TRANSFER ( Kocmi , 2020 ) combines transfer learning from a high- resource language pair Czech - English into the low-resource Inuktitut - English with an additional backtranslation step .
Surprising behaviour is noticed when using synthetic data , which can be possibly attributed to a narrow domain of training and test data .
The system is the Transformer model in a constrained submission .
CUNI-TRANSFORMER ( Popel , 2020 )
Kim et al. ( 2020 ) base their approach on transferring knowledge of domain and linguistic characteristics by pre-training the encoder-decoder model with large amount of in-domain monolingual data through unsupervised and supervised prediction task .
The model is then fine-tuned with parallel data and in-domain synthetic data , generated with iterative back - translation .
For additional gain , final results are generated with an ensemble model and re-ranked with averaged models and language models .
2.3.9 ETRANSLATION
( Oravecz et al. , 2020 ) ETRANSLATION mainly use the standard training pipeline of Transformer in Marian , using tagged back -translation and other features .
Subword units are identified by SentencePiece .
The paper describes the group 's concern about computing resources and the practical utility of expensive features like ensembling 2 to 4 bigger models .
Techniques that were ineffective in ETRANSLATION 's case ( e.g. right - to - left model for rescoring English ?
German or Unicode preprocessing for Japanese ?
English ) are also described .
2.3.10 FACEBOOK AI ( Chen et al. , 2020a ) FACEBOOK
AI focus on low-resource language pairs involving Inuktitut and Tamil using two strategies : ( 1 ) exploiting all available data ( parallel and monolingual from all languages ) and ( 2 ) adapting the model to the test domain .
For ( 1 ) , FACEBOOK AI opt for non-constrained submission , using data derived from Common - Crawl to get strong translation models via iterative backtranslation and self-training and strong language models for noisy channel reranking .
Multilingual language models are created using mBART across all the 13 languages of WMT20 .
For ( 2 ) , the datasets are tagged for domain , finetuned on and further extended with in - domain data .
GRONINGEN GRONINGEN-ENIU
( Roest et al. , 2020 ) investigate the ( 1 ) importance of correct morphological segmentation of the polysynthetic Inuktitut , testing rule- based , supervised , semi-supervised as well as unsupervised word segmentation methods , ( 2 ) whether or not adding data from a related language ( Greenlandic ) helps , and ( 3 ) whether contextual word embeddings ( XLM ) improve translation .
GRONINGEN -ENIU use Transformer implemented in Marian with the default setting , improving the performance also with tagged backtranslation , domain-specific data , ensembling and finetuning .
( Dhar et al. , 2020 ) study the effects of various techniques such as linguistically motivated segmentation , backtranslation , fine-tuning and word dropout on the English ?
Tamil News Translation task .
Linguis-tically motivated subword segmentation does not consistently outperform the widely used Senten-cePiece segmentation despite the agglutinative nature of Tamil morphology .
The authors also found that fully -fledged back - translation remains more competitive than its cheaper alternative .
GRONINGEN-ENTAM 2.3.12 GTCOM GTCOM are unconstrained systems using mBART ( Multilingual Bidirectional and Auto-Regressive Transformers ) , back - translation and forward -translation .
Further gains are achieved using rules , language model and RoBERTa model to filter monolingual , parallel sentences and synthetic sentences .
The vocabularies are created from both monolingual and parallel data .
2.3.13 HELSINKINLP
( Scherrer et al. , 2020a ) HELSINKINLP for the Inuktitut - English news translation task focuses on the efficient use of monolingual and related bilingual corpora with multi-task learning as well as an optimized subword segmentation with sampling .
2.3.14 HUAWEI TSC ( Wei et al. , 2020a ) HUAWEI TSC use Transformer - big with a further increased model size , focussing on standard techniques of careful pre-processing and filtering , back - translation and forward translation , including self-training , i.e. translating one of the sides of the original parallel data .
Ensembling of individual training runs is used in the forward as well as backward translation , and single models are created from the ensembles using knowledge distillation .
The submission uses THUNMT ( Zhang et al. , 2017 )
Transformer implemented in fairseq is used , with smaller than " base " models due to limited training data .
2.3.17 NICT -KYOTO ( Marie et al. , 2020 ) NICT -KYOTO is a combination of neural machine translation systems processed through nbest list reranking .
The systems combined are Transformer - based trained with Marian and Fairseq with and without using tagged backtranslation .
All the systems are constrained , and the final primary submission is selected on the basis of the BLEU score obtained on the official validation data .
NICT - RUI NICT - RUI is closely related to SJTU - NICT using large XLM model to improve NMT but the exact relation is unclear .
NIUTRANS gain their performance from focussed attention to six areas : ( 1 ) careful data preprocessing and filtering , ( 2 ) iterative back - translation to generate additional training data , ( 3 ) using different model architectures , such as wider and / or deeper models , relative position representation and relative length , to enhance the diversity of translations , ( 4 ) iterative knowledge distillation by in- domain monolingual data , ( 5 ) iterative finetuning for domain adaptation using small training batches , ( 6 ) rule- based post- processing of numbers , names and punctuation .
NICT -RUI
NIUTRANS
For low-resource language pairs , multi-lingual seed models are used .
NRC ( Knowles et al. , 2020 )
The NRC systems are hybrids of Transformer models trained with Sockeye , with one ensembled system for news domain translation and one for Hansard domain translation .
Data was preprocessed with language-specific punctuation and character preprocessing , tokenization , and BPE .
They were trained with domain tagging , domainspecific finetuning , ensembles of 3 systems per domain , BPE - dropout ( EN - IU ) , and tagged backtranslation ( IU - EN ) .
( Shi et al. , 2020 ) OPPO train Marian for some language pairs and fairseq for others , relying on a number of mature techniques including careful corpus filtering , iterative forward and backward translation , finetuning on the original parallel data , ensembling of several different models , and complex reranking which uses forward ( source- to- target ) scorers , backward scorers ( target- to-source ) and language models ( monolingual ) , each group again building upon ensembles and being applied left-to - right as well as right - to- left .
OPPO
Each language pair received targeted attention , discussing training data properties , varying the process as needed and choosing from several possible final models .
( Molchanov , 2020 ) PROMT BASELINE TRANSFORMER uses Mar-ianNMT , shared vocabulary , 16 k BPE merge operations and it is trained on unconstrained data .
PROMT PROMT BASIC TRANSFORMER uses separate vocabs ( 16 k source + 32 k target ) , and tied embeddings .
PROMT MULTILINGUAL 4 - TO -EN is a multilingual system trained to translate from Croatian , Serbian , Slovak and Czech to English .
It is a basic Transformer configuration with shared vocabulary .
PROMT MULTILINGUAL PL -EN is a Polish ?
English system trained jointly in both directions .
It uses basic Transformer configuration and shared vocabulary .
None of PROMT systems are constrained .
SJTU -NICT ( Li et al. , 2020 ) SJTU - NICT represents two different main approaches .
For News Translation Task , ( 1 ) crosslingual language models ( XLM ) are used in an additional encoder to benefit from languageindependent sentence representations from both the source and target side for Polish ?
English .
For English ?
Chinese , which includes documentlevel information , three - stage training is used to train Longformer ( Transformer with attention extended to the full document ) .
SRPOL ( Krubi ?ski et al. , 2020 )
No short description provided .
2.3.25 TALP UPC ( Escolano et al. , 2020 )
No short description provided .
TENCENT TRANSLATION
( Wu et al. , 2020 b )
No short description provided .
THUNLP ( no associated paper )
No description provided .
2.3.28 TILDE ( Kri?lauks and Pinnis , 2020 ) For WMT 2020 , Tilde developed English ?
Polish ( separate constrained and unconstrained submissions ) and Polish ?
English ( constrained only ) NMT systems .
Tilde experimented with morpheme splitting prior to byte-pair encoding , dual conditional cross-entropy filtering , samplingbased backtranslation of source- domain- adherent monolingual data , and right - to - left reranking .
The submitted translations were produced using ensembles of Transformer base and Transformer big models , which were trained using back - translated data , and right - to - left re-ranking .
TOHOKU -AIP-NTT ( Kiyono et al. , 2020 ) TOHOKU -AIP -NTT used Transformer - based Encoder - Decoder model with 8 layers and feed forward dimension of 8192 .
Synthetic data were created via beam back -translation from monolingual data available for each language and incorporated to the training using tagged backtranslation .
The bitext was oversampled so that the model saw the bitext and synthetic data in 1:1 ratio .
After training , the model was finetuned with newstest corpus .
An ensemble of four models was used to generate candidate translation , which were in turn re-ranked using scores from following components : ( 1 ) source - to- target right - to - left model , ( 2 ) target - to - source left- to - right model , ( 3 ) target - tosource right - to - left model , ( 4 ) masked language model ( RoBERTa ) , and ( 5 ) uni-directional language model ( Transformer - LM ) .
( Hernandez and Nguyen , 2020 ) UBIQUS performed a single submission , based on an unconstrained multilingual setup .
The approach consists of jointly training a traditional Transformer model on several agglutinative languages in order to benefit from them for the lowresource English -Inuktitut task .
For that purpose , the dataset was extended with other linguistically near languages ( Finnish , Estonian ) , as well as inhouse datasets introducing more diversity to the domain .
MULTILINGUAL -UBIQUS
UEDIN UEDIN ( Bawden et al. , 2020a ) for the very low-resource English - Tamil involved exploring pretraining , using both language model objectives and translation using an unrelated high- resource language pair ( German- English ) , and iterative backtranslation .
For English -Inuktitut , UEDIN explored the use of multilingual systems .
UEDIN -DEEN and UEDIN-ENDE ( Germann , 2020 ) ensemble big transformer models trained in three stages :
First , base transformer models were trained on available high-quality parallel data .
These models were used to rank and select parallel data from crawled and automatically matched parallel data ( Paracrawl , Commoncrawl , etc. ) .
2nd-generation big transformers were then trained on the combined parallel data .
These models were used for back - translation .
Original and back - translated data was then used to the final 3rdgeneration models .
WMTBIOMEDBASELINE are the baseline systems from the Biomedical Translation Task .
YOLO ( no associated paper )
No description provided .
ZLABS -NLP ZLABS - NLP used SentencePiece for subword segmentation , otherwise the model including hyperparameters is the same as described by Ott et al . ( 2018 ) and implemented in FairSeq .
Probably , OpenNMT - py was used during training ( backtranslation for Tamil ) .
Human Evaluation
A human evaluation campaign is run each year to assess translation quality and to determine the official ranking of systems taking part in the news translation task .
This section describes how data for the human evaluation is prepared , the process of collecting human assessments , and computation of the official results of the shared task .
Direct Assessment Since running a comparison of direct assessments ( DA , Graham et al. , 2013 Graham et al. , , 2014 and relative ranking in 2016 and verifying a high correlation of system rankings for the two methods , as well as the advantages of DA , such as quality controlled crowd- sourcing and linear growth relative to numbers of submissions , we have employed DA as the primary mechanism for evaluating systems .
With DA human evaluation , human assessors are asked to rate a given translation by how adequately it expresses the meaning of the corresponding reference translation or source language input on an analogue scale , which corresponds to an underlying absolute 0 - 100 rating scale .
5
No sentence or document length restriction is applied during manual evaluation .
Direct Assessment is also employed for evaluation of video captioning systems at TRECvid Awad et al. , 2019 ) and multilingual surface realisation ( Mille et al. , 2018 ( Mille et al. , , 2019 .
Source and Reference - based Evaluations
The earlier DA evaluations that we performed were all referenced based , as described above , however in 2018 we trialled source - based evaluation for the first time , in English to Czech translation .
In this configuration , the human assessor is shown the source input and system output only ( with no reference translation shown ) .
This approach has the advantage of freeing up the humangenerated reference translation so that it can be included in the evaluation to provide an estimate of human performance .
As was the approach in WMT19 , since we would like to restrict human assessors to only evaluate translation into their native language , we again restrict bilingual / sourcebased evaluation to evaluation of translation for out - of - English language pairs .
This is especially relevant since we have a large group of volunteer human assessors with native language fluency in non-English languages and high fluency in English , while we generally lack the reverse , i.e. native English speakers with high fluency in non-English languages .
Translationese Prior to WMT19 , all the test sets included a mix of sentence pairs that were originally in the source language , and then translated to the target language , and sentence pairs that were originally in the target language but translated to the source language .
The inclusion of the latter " reversecreated " sentence pairs has been shown to introduce biases into the evaluations , particularly in terms of BLEU scores , so we avoid it where possible .
As detailed in Sec-tion 2 , most of our test sets do not include reversecreated sentence pairs , except when there were resource constraints on the creation of the test sets .
Document Context Prior to WMT19 , the issue of including document context was raised within the community ( L?ubli et al. , 2018 ; Toral et al. , 2018 ) and at WMT19 a range of DA styles were subsequently tested that included document context .
In WMT19 , two options were run , firstly , an evaluation that included the document context " + DC " ( with document context ) , and secondly , a variation that omitted document context " ? DC " ( without document context ) .
This year , for language pairs for which document context was available in the test set , we therefore include this context when evaluating translations for systems .
Although we include document context , ratings are nevertheless collected on the segment- level , motivated by the power analysis described in and .
The particular details on how document context is made available to assessors depends on the translation direction , as described in more detail in Sections 3.2 and 3.3 below for translation into English and out of English , resp .
In the following , we use the following abbreviations to describe annotation style : SR + DC for translation direction where assessors rank individual segments ( Segment Ranking , SR ) and have access to the full document , SR ?
DC for translation directions where document context is not available and assessors see individual sentences in random order .
Fully document - level evaluation ( DR + DC , document - level ranking with document context available ) as trialled last year where we asked for a single score given the whole document is problematic in terms of statistical power and inconclusive ties , as shown in ; , and we subsequently did not include this approach for any into-English language this year .
As in previous years , the SR ?
DC annotation is organized into " HITs " ( following the Mechanical Turk 's term " human intelligence task " ) , each containing 100 screens .
Human Evaluation of Translation into-English
A summary of the human evaluation configurations run this year in the news task for into-English language pairs is provided in Table 7 .
In terms of the News translation task manual evaluation for into-English language pairs , a total of 654 turker accounts were involved .
6 654,583 translation assessment scores were submitted in total by the crowd , of which 166,868 were provided by workers who passed quality control .
System rankings are produced from a large set of human assessments of translations , each of which indicates the absolute quality of the output of a system .
Table 8 shows total numbers of human assessments collected in WMT20 for into-English language pairs contributing to final scores for systems .
7
Crowd Quality Control
We run two configurations of DA , one with document context , segment-rating with document context ( SR + DC ) , for languages for which this information was available and one without document context , for the remainder , segment rating without document context ( SR - DC ) .
We describe quality control details and both methods of ranking systems for into-English language pairs in detail below .
Standard DA HIT Structure ( SR?DC )
In the standard DA HIT structure ( without document context ) , three kinds of quality control translation pairs are employed as described in Table 9 : we repeat pairs expecting a similar judgment ( Repeat Pairs ) , damage MT outputs expecting significantly worse scores ( Bad Reference Pairs ) and use references instead of MT outputs expecting high scores ( Good Reference Pairs ) .
For each of these three types , we include the MT output , along with its corresponding control .
In total , 60 items in a 100 - translation HIT serve in quality control checks but 40 of those are regular judgments of MT system outputs ( we exclude assessments of bad references and ordinary reference translations when calculating final scores ) .
The effort wasted for the sake of quality control is thus 20 % .
Also in the standard DA HIT structure , within each 100 - translation HIT , the same proportion of translations are included from each participating system for that language pair .
This ensures the final dataset for a given language pair contains roughly equivalent numbers of assessments for each participating system .
This serves three purposes for making the evaluation fair .
Firstly , for the point estimates used to rank systems to be reliable , a sufficient sample size is needed and the most efficient way to reach a sufficient sample size for all systems is to keep total numbers of judgments roughly equal as more and more judgments are collected .
Secondly , it helps to make the evaluation fair because each system will suffer or benefit equally from an overly lenient / harsh human judge .
Thirdly , despite DA judgments being absolute , it is known that judges " calibrate " the way they use the scale depending on the general observed translation quality .
With each HIT including all participating systems , this effect is systems comprising human-generated reference translations used to provide human performance estimates .
Repeat Pairs : Original System output ( 10 )
An exact repeat of it ( 10 ) ; Bad Reference Pairs : Original System output ( 10 ) A degraded version of it ( 10 ) ; Good Reference Pairs : Original System output ( 10 ) Its corresponding reference translation ( 10 ) .
averaged out .
Furthermore apart from quality control items , HITs are constructed using translations sampled from the entire set of outputs for a given language pair .
Document - Level DA HIT Structure ( SR + DC ) Collection of segment- level ratings with document context ( Segment Rating + Document Context ) involved constructing HITs so that each sentence belonging to a given document ( produced by a single MT system ) was displayed to and rated in turn by the human annotator .
Quality control items for this set - up was carried out as follows with the aim of constructing a HIT with as close as possible to 100 segments in total : 1 . All documents produced by all systems are pooled ; 8 2 .
Documents are then sampled at random ( without replacement ) and assigned to the current HIT until the current HIT comprises no more than 70 segments in total ;
3 . Once documents amounting to close to 70 segments have been assigned to the current HIT , we select a subset of these documents to be paired with quality control documents ; this subset is selected by repeatedly checking if the addition of the number of the segments belonging to a given document ( as quality control items ) will keep the total number of segments in the HIT below 100 ; if this is the case it is included ; otherwise it is skipped until the addition of all documents has been checked .
In doing this , the HIT is structured to bring the total number of segments as close as possible to 100 segments in total within a HIT but without selecting documents in any systematic way such as selecting them based on fewest segments , for example .
4 . Once we have selected a core set of original system output documents and a subset of them to be paired with quality control versions for each HIT , quality control documents are automatically constructed by altering the sentences of a given document into a mixture of three kinds of quality control items used in the original DA segment - level quality control : bad reference translations , reference translations and exact repeats ( see below for details of bad reference generation ) ;
5 . Finally , the documents belonging to a HIT are shuffled .
Construction of Bad References
As in previous years , bad reference pairs were created automatically by replacing a phrase within a given translation with a phrase of the same length , randomly selected from n-grams extracted from the full test set of reference translations belonging to that language pair .
This means that the replacement phrase will itself comprise a mostly fluent sequence of words ( making it difficult to tell that the sentence is low quality without reading the entire sentence ) while at the same time making its presence highly likely to sufficiently change the meaning of the MT output so that it causes a noticeable degradation .
The length of the phrase to be replaced is determined by the number of words in the original translation , as follows : Translation # Words Replaced Length ( N ) in Translation 1 1 2-5 2 6-8 3 9-15 4 16-20 5 >20 N/4
Annotator Agreement
When an analogue scale ( or 0 - 100 point scale , in practice ) is employed , agreement cannot be measured using the conventional Kappa coefficient , ordinarily applied to human assessment when judgments are discrete categories or preferences .
Instead , to measure consistency we fil-ter crowd-sourced human assessors by how consistently they rate translations of known distinct quality using the bad reference pairs described previously .
Quality filtering via bad reference pairs is especially important for the crowd-sourced portion of the manual evaluation .
Due to the anonymous nature of crowd- sourcing , when collecting assessments of translations , it is likely to encounter workers who attempt to game the service , as well as submission of inconsistent evaluations and even robotic ones .
We therefore employ DA 's quality control mechanism to filter out low quality data , facilitated by the use of DA 's analogue rating scale .
Assessments belonging to a given crowdsourced worker who has not demonstrated that he / she can reliably score bad reference translations significantly lower than corresponding genuine system output translations are filtered out .
A paired significance test is applied to test if degraded translations are consistently scored lower than their original counterparts and the p-value produced by this test is used as an estimate of human assessor reliability .
Assessments of workers whose p-value does not fall below the conventional 0.05 threshold are omitted from the evaluation of systems , since they do not reliably score degraded translations lower than corresponding MT output translations .
Table 10 shows the number of workers participating in the into-English translation evaluation who met our filtering requirement in WMT20 by showing a significantly lower score for bad reference items compared to corresponding MT outputs , and the proportion of those who simultaneously showed no significant difference in scores they gave to pairs of identical translations .
We removed data from the non-reliable workers in all language pairs .
Human Evaluation of Translation out - of- English Human evaluation of out - of - English translations features a bilingual / source- based evaluation campaign that enlists the help of participants in the shared task .
As usual , each team was asked to contribute around 8 hours annotation time , which we estimated at 16 HITs per each primary system submitted , with each HIT including 100 segment translations .
Unfortunately , not all participating teams were able to provide requested number of assessments , hence , to collect the required number of assessments per MT system , we also employed external translators in a separate campaign .
The contracted translators contributed with one third of total number of assessments .
Both campaigns utilized document- level DA and were run for all out - of - English language pairs , which test sets include document - level segmentation .
For English ?
Khmer , English ?
Pashto , French ?
German , and German ?
French , whose test sets do not provide document boundaries , segment - level DA evaluation without document context ( SR - DC ) was performed , enlisting the effort of translators .
For English ?
Inuktitut , since we expected no participants to speak Inuktitut , the NRC hired native speakers through the Pirurvik Centre to conduct most of the DA evaluation .
Due to the delays in starting the evaluation campaign , they were only able to complete the evaluation a few days before the conference , and could only annotate the news half of the test set .
The Hansard half of the test set was not assessed in time for this report , but plans are being made to continue the evaluation after the conference .
Updated rankings should be provided at a future date .
In terms of the News translation task documentlevel manual evaluation for out - of - English language pairs , a total of 1,189 researcher / translator accounts were involved , and 248,597 translation assessment scores were contributed in total ( with quality control pairs ) , including 18,108 document ratings .
For the segment- level campaigns ( i.e. English ?
Khmer , English ?
Pashto , German ?
French and French ?
German ) we had 300 accounts and 65872 scores collected in total .
Statistics per language pair are summarized in Table 11 .
For data collection we again used the open-source Appraise 9 ( Federmann , 2012 ) .
The effort that goes into the manual evaluation campaign each year is impressive , and we are grateful to all participating individuals and teams for their work .
Document - Level Assessment
This year 's human evaluation for out - of - English language pairs features an improved documentlevel direct assessment configuration that extends the context span to entire documents for a more reliable machine translation evaluation ( Castilho ment on a screen .
In the default scenario , an annotator scores individual segments one - by- one and , after scoring all of them , on the same screen , the annotator then judges the translation of the entire document displayed .
Annotators can , however , revisit and update scores of previously assessed segments at any point of the annotation of the given document .
Quality Control
For the document- level evaluation of out - of - English translations , HITs were generated using the same method as described for the SR + DC evaluation of into-English translations in Section 3.2.1 with minor modifications .
Source - based DA allows to include human references in the evaluation as another system to provide an estimate of human performance .
Human references were added to the pull of system outputs prior to sampling documents for tasks generation .
If multiple references are available , which is the case for English ?
German ( 3 alternative reference translations , including 1 generated using the paraphrasing method of ) and English ?
Chinese ( 2 translations ) , each reference is assessed individually .
Since the annotations are made by researchers and professional translators who ensure a bet - ter quality of assessments than the crowd-sourced workers , only bad references are used as quality control items .
Instead of sampling initial documents with close to 70 segments , we sample documents with 88 segments , and then a subset of documents with around 12 segments is selected to be converted into bad references .
The remaining of the HIT creation process remains the same .
Producing the Human Ranking
In all set-ups , similar to previous years , system rankings were arrived at in the following way .
Firstly , in order to iron out differences in scoring strategies of distinct human assessors , human assessment scores for translations were first standardized according to each individual human assessor 's overall mean and standard deviation score .
This year all rankings for to -English translation were arrived at via segment ratings ( SR? DC , SR + DC ) , average standardized scores for individual segments belonging to a given system were then computed , before the final overall DA score for a given system is computed as the average of its segment scores ( Ave z in Table 12 ) .
Results are also reported for average scores for systems , computed in the same way but without any score standardization applied ( Ave % in Table 12 ) .
clusters according to Wilcoxon rank- sum test p < 0.05 .
For evaluation of English ?
Inuktitut insufficient data resulted in a small sample size of human assessments per system and as a result some systems that fall within the same cluster are likely to do so simply due to low statistical power .
Human performance estimates arrived at by evaluation of human-produced reference translations are denoted by " HUMAN " in all tables .
Note that " HUMAN - P " is a human-produced paraphrase of HUMAN -A , according to the method proposed by .
Clusters are identified by grouping systems together according to which systems significantly outperform all others in lower ranking clusters , according to Wilcoxon rank - sum test .
Appendix
A shows the underlying head - to - head significance test official results for all pairs of systems .
All data collected during the human evaluation is available at http://www.statmt.org/wmt20/ results.html .
In terms of human and machine quality comparisons in results , it is clear from the sourcebased evaluation of English to German and English to Chinese translation that human translators vary in performance , with each human translator represented in a distinct cluster .
Without taking from the significant achievement of systems that have tied with a human translator , this fact should be taken into account when drawing conclusions about human parity .
A tie with a single human translator should not be interpreted as a tie with human performance in general .
Test Suites " Test Suites " have now become an established part of WMT News Translation .
Their purpose is to complement the standard one-dimensional manual evaluation .
Each test suite can focus on any aspect of translation quality and any subset of language pairs and MT systems .
Anyone can propose their own test suite and take part , and we also try to solicit evaluation from past successful test suite teams to support some cross -year insight .
Each team in the test suites track provides source texts ( and optionally references ) for any language pair that is being evaluated by WMT News Task .
We shuffle these additional texts into the inputs of News Task and ship them as inputs to MT system developers jointly with the regular news texts .
The shuffling happens at the document or sentence level as agreed with the test suite authors .
( Shuffling at the level of sentences can lead to a very high number of documents in the final test set because each sentence is treated as a separate document . )
MT system developers may decide to skip these documents based on their ID but most of them process test suites along with the main news texts .
After collecting the output translations from all WMT News Task Participants , test suites translations are made available back to the test suite authors for evaluation .
Test suite sentences do not go through the manual evaluation as described in Section 3 .
As in the previous years , test suites are not limited to the news domain , so News
Task system may actually underperform on them .
Test Suite Details
The following paragraphs briefly describe each of the test suites .
Please refer to the respective paper for all the details of the evaluation .
Covid Test Suite TICO -19
The TICO - 19 test suite was developed to evaluate how well can MT systems handle the newlyemerged topic of COVID-19 .
Accurate automatic translation can play an important role in facilitating communication in order to protect at-risk populations and combat the infodemic of misinformation , as described by the World Health Organization .
The test suite has no corresponding paper so its authors provided an analysis of the outcomes directly here .
The submitted systems were evaluated using the test set from the recently - released TICO - 19 dataset ( Anastasopoulos et al. , 2020 ) .
The dataset provides manually created translations of COVID -19 related data .
The test set consists of PubMed articles ( 678 sentences from 5 scientific articles ) , patient-medical professional conversations ( 104 sentences ) , as well as related Wikipedia articles ( 411 sentences ) , announcements ( 98 sentences from Wikisource ) , and news items ( 67 sentences from Wikinews ) , for a total of 2100 sentences .
Table 15 outlines the BLEU scores by each submitted system in the English - to -X directions , also breaking down the results per domain .
The analysis shows that some systems are significantly more prepared to handle highly narrow-domain data .
In addition , the variance of the output quality across languages and across domains highlights the importance of building MT systems that can generalize across domains .
Document Coherence Check via Markable Annotation ( Zouhar et al. , 2020 )
The test suite provided in 2020 by the ELITR project ( Zouhar et al. , 2020 ) follows upon Vojt?chov ?
et al. ( 2019 ) .
The focus this year is on " markables " , i.e. mainly domain-specific terms that have to be translated consistently and unambiguously throughout the whole document ( except news where style may require variation ) to maintain lexical coherence .
Manual annotation of the translation of markables is contrasted with manual annotation of fluency and adequacy and also BLEU scores .
The test suite is limited to 4 English ?
Czech documents and 2 Czech ?
English documents , covering 215 markable occurrences across 4 different domains .
The set of markables was collected in the first phase of the annotation , which amounted to 4 k assessments across the systems .
The second annotation phase with 6.5 k assessments compared markable translations , always checking outputs of all the 13 competing MT systems but still considering the document - level context of each of them .
Among other things , the observations indicate that the better the system , the lower the variance in manual scores .
Markables annotation then confirms that frequent errors like bad translation of a term need not be the most severe and conversely , even rare errors such as bad disambiguation , overtranslation or disappearance of a term or its translation which conflicts with other terms in the document can be critical .
The comparison of MT outputs with the reference ( hidden among MT systems ) in the evaluation is also interesting .
Man-made errors were always marked as less severe than those of MT .
The annotation also suggests that one of the documentlevel systems outperformed the reference in markable evaluation if error severity and frequency are weighted equally .
Fluency and adequacy collected as average sentence - level scores ( with access to the full documents of all systems ) are curious , revealing perhaps more about the annotators than the MT systems .
The test suite by focuses on the gender bias in professions ( e.g. physician , teacher , secretary ) for the translation from English into Czech , German , Polish and Russian .
These nouns are ambiguous with respect to gender in English but exhibit gender in the examined target languages .
Gender Coreference and Bias
The test suite is based on the fact that a pronoun referring to the ambiguous noun can reveal the gender of the noun in the English source sentence .
Once disambiguated , the gender needs to be preserved in translation .
To correctly translate the given noun , the translation system thus has to correctly resolve the coreference link and transfer information from the pronoun to the noun in the antecedent ( a less common direction of information flow ) , and then correctly express the noun in the target language .
The success of the MT system in this test can be established automatically , whenever the gender of the target word can be automatically identified .
build upon the WinoMT ( Stanovsky et al. , 2019 ) test set , which provides exactly the necessary type of sentences containing an ambiguous profession noun and a personal pronoun which unambiguously ( for the human eye ) refers to it based the situation described .
When extending WinMT with Czech and Polish , Stanovsky et al.
have to disregard some test patterns but the principle remains .
The results indicate that all MT systems fail in this test , following gender bias ( stereotypical patterns attributing the masculine gender to some professions and feminine gender to others ) rather than the coreference link .
Linguistic Evaluation of German-to- English ( Avramidis et al. , 2020 )
The test suite by DFKI covers 107 grammatical phenomena organized into 14 categories .
Since 2018 , the same set of phenomena are being tested annually ( Macketanz et al. , 2018 ; Avramidis et al. , 2019 ) .
Automatic evaluation is complemented with 45 hours of human annotation .
This year , the newcomers VOLCTRANS and TOHOKU -AIP - NTT perform particularly well in the tested phenomena , followed by the traditional systems UEDIN , ONLINE-B , ONLINE -G , and ONLINE -A .
The generally good news is that systems which participated in both WMT19 and WMT20 show an improvement this year .
Given that the test suite target side remains undisclosed , these scores can be deemed absolute , unlike the official DA scores which are only relative within each year and set of systems .
The test suite allows to report these improvements per linguistic category and specifically for each MT system that participated in two consecutive years .
The biggest improvements are observed in long distance dependencies or interrogatives , verb valency , ambiguity and punctuation , and we tend to attribute all these improvements to increased capacity ( which allows increased sensitivity to long- range relations ) of the models .
4.1.5 Word Sense Disambiguation ( Scherrer et al. , 2020 b )
Scherrer et al. ( 2020 b ) is a followup of last year 's evaluation ( Raganato et al. , 2019 ) , assessing the ability of MT systems to disambiguate a word given its context of the sentence .
The underlying MuCoW ( multilingual contrastive word sense disambiguation ) dataset contains approximately 2 k to 4 k sentences per language pair selected from large parallel corpora to contain particularly ambiguous words .
This year , the focus was on language pairs that appeared both in WMT19 and WMT20 ( and were available in the MuCoW dataset ) , namely English ?
Czech , English ?
German , and English ?
Russian .
Comparing overall numbers across the years , Scherrer et al . ( 2020 b ) report that ambiguous words are correctly disambiguated in the majority of cases .
Both precision ( percentage of correct choices out of sentences where either good or bad expected translation was found ) and recall ( percentage of correct choices out of all sentences ) are above 60 % and reaching 80 % for the best systems in a given language pair when mixing " in- domain " and " out- of- domain " evaluation .
The " out- of- domain " synsets are those that are represented in the test suite with more than half of cases coming from the colloquial subtitle domain ; other synsets are deemed " in- domain " .
The " in-domain " scores are generally higher , with precisions above 95 % for the best Czech and Russian systems .
Across the years , no real improvement is however observed .
Three cases suggest that training systems at the level of documents decreases their performance in this sentence - level evaluation ( each sentence forms a separate document ) :
DocTransformer vs .
Transformer by CUNI in 2019 and 2020 and Microsoft document- level vs. sentence - level submission in 2019 .
Similar Language Translation
Most shared tasks at WMT ( e.g. News , Biomedical ) have historically dealt with translating texts from and to English .
In recent years , we observed a growing interest in training systems to translate between languages other than English .
This includes a number of papers applying MT to translate between pairs of closely - related languages , national language varieties , and dialects of the same language ( Zhang , 1998 ; Marujo et al. , 2011 ; Hassani , 2017 ; Costa-juss ?
et al. , 2018 ; . To address this topic , the first Similar Language Translation ( SLT ) shared task at WMT 2019 has been organized .
It featured data from three pairs of closely - related languages from different language families : Spanish - Portuguese ( Romance languages ) , Czech - Polish ( Slavic languages ) , and Hindi - Nepali ( Indo - Aryan languages ) .
Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic , we organize , for the second time at WMT , this shared task to evaluate the performance of state - of - the - art translation systems on translating between pairs of languages from the same language family .
SLT 2020 features five pairs of similar languages from three different language families : Indo-Aryan , Romance , and South -Slavic .
Translations were evaluated in both directions using automatic evaluation metrics presented in this section .
Data Training
We have made available a number of data sources for the SLT shared task .
Some training datasets were used in the previous editions of the WMT News Translation shared task and were updated ( Europarl v10 , News Commentary v15 , Wiki Titles v2 ) , while some corpora were newly introduced ( JRC Acquis ) .
The released parallel HI - MR dataset was collected from news ( Siripragada et al. , 2020 ) , PMIndia ( Haddow and Kirefu , 2020 ) and Indic Wordnet ( Bhattacharyya , 2010 ; Kunchukuttan , 2020a ) datasets .
All data were initially combined , tokenized using indic-nlp tokenizer ( Kunchukuttan , 2020 b ) and randomly shuffled .
From the combined corpus , we randomly extracted 49,434 sentences for the training set and the rest are used as development and test sets .
For the South - Slavic language pairs we used large datasets available from Opus ( Tiedemann and Nygaard , 2004 ) 11 , more precisely the OpenSubtitles , MultiParaCrawl , DGT and JW300 data .
Different to the other language groups , for monolingual data web corpora of the three languages ( Ljube ?i? and Erjavec , 2011 ; Ljube ?i? and Klubi?ka , 2014 ; Erjavec et al. , 2015 ) were given to the participants .
Development and Test Data
The development and test sets for Spanish -Catalan and Spanish - 11 http://opus.nlpl.eu/
Portuguese language pairs were created from a corpus provided by Pangeanic 12 . First , we performed cleaning using CLEAN -CORPUS-N.PERL 13 script to retain sentences that have between 4 and 100 tokens .
This narrowed the number of sentences to 1,287 and 1,535 in dev and test sets respectively .
Finally , sentences containing metadata information were removed , which resulted in 1,283 and 1,495 sentences in dev and test sets respectively .
The aforementioned shuffled combined HI - MR dataset , 1411 sentences are used for development set and 3882 for the test set .
Finally , the test set was equally split into two different test sets : 1941 sentences used for HI to MR and 1941 sentences were used for MR to HI .
For the Slovene-Croatian and Slovene-Serbian language pairs , development and test data were obtained from the Ciklopea translation agency 14 in form of a data donation from the Bisnode business intelligence company 15 .
The data consists of public relations releases translated in various directions between the three languages .
The data was cleaned , deduplicated and shuffled , resulting in 2,457 dev and 2,582 instances for the Slovene - Croatian pair , and 1,259 dev and 1,260 test instances in the Slovene -Serbian pair .
Given that these translations sometimes form Slovene -Croatian - Serbian triangles , special care was invested in circumventing data leakage between development data on one side , and test data on the other , of the two language pairs .
Participants and Approaches
The second edition of the WMT SLT task attracted 68 teams who signed up to participate in the competition and 18 of them submitted their system outputs .
In the end of the competition , 14 teams submitted system description papers which are referred to in this report .
Table 22 summarizes the participation across language pairs and translation directions and includes references to the 14 system description papers .
Next we provide summaries for each of the entries we received :
A3 - 108
The team A3 - 108 submitted their system for HI - MR and MR - HI .
The team initially build SMT models for both language direction after three steps preprocessing : ( i ) default - in- dic_nlp_library 16 and moses tokenizer 17 , ( ii ) morfessor 18 and ( iii ) BPE 19 .
These SMT models were used for back - translation .
Finally , these backtranslation data were used to train their NMT system .
ADAPT -DCU
The ADAPT - DCU team participated in the SLT task on the Croatian - Slovene and Serbian - Slovene language pairs .
The team 's submissions were based on the Sockeye implementations of the Transformer , with a joint 32 klarge BPE vocabulary for all three languages .
The submission were regularly multilingual ( having Slovene on one side and Croatian and Serbian on the other ) .
The team used only OpenSubtitles bilingual training data , considering other available data to be too noisy .
The basic implementation of the multilingual system was submitted as the second contrastive system , the multilingual implementation trained on filtered parallel data as the first contrastive system , while the primary submission included backtranslation of target monolingual data of segments similar to the development data .
By performing n-gram-character - based filtering of training data the training time was cut in half with a minor improvement on the translation quality , while the largest improvements in translation quality were obtained by back - translating data similar to development data ( between 8 and 14 BLEU points ) .
f1 plusf6
During preprocessing as Marathi and Hindi are rich in terms of morphology , Applied two way segmentation as preprocessing , first supervised and unsupervised word based morphological segmentation and then BPE based segmentation to tackle low-resource language pairs .
The participants used shared vocab across training and utilised POS based features on the source side to create initial models for both directions .
For preparing unsupervised back -translation parallel data they used aligned embedding space to generate word by word parallel sentences for both language directions .
They also prepared initial models from the provided parallel data for back translation from monolingual data and pruned back - translation pairs based on perplexity score .
Their model is based on Luong 's attention on bi-LSTM network , copy attention on dynamically generated dictionary with label smoothing and dropouts to reduce overfitting .
Fast -MT Fast - MT team submitted their NMT system where Transformers and Recurrent Attention models are effectively used .
They combined the recurrence based layered encoder-decoder model with the Transformer model .
Their submitted system for Indo-Aryan Language ( Hindi to Marathi ) pair is trained on the parallel corpus of the training dataset provided by the organizers .
IIAI IIAI TEAM participate in both directions of the Hindi-Marathi translation task .
Their primary submission is a transformer model trained on the released parallel and back - translated monolingual data .
The team jointly learned BPE from the merged source-target corpus .
After BPE , sentences were corrupted and reconstructed using the two ways : ( i) 15 % of the subwords in the sentence are randomly selected and masked , ( ii ) 15 % of the subwords are randomly selected one by one and swapped with another randomly -selected subword in the sequence .
Team System Description Paper 22 : The teams that participated in the SLT 2020 task and their system description papers .
IITDELHI Team IITDELHI participated in the SLT task on Hindi-Marathi and Spanish - Portuguese language pairs .
The team 's primary submission builds on fine-tuning over pretrained mBART .
They used pre-trained weights of the mBART model ( Liu et al. , 2019 ) , which is pretrained on large amounts of monolingual data for 25 languages including Spanish , however Portuguese is not there .
The authors initialized a Transformer architecture with 12 encoder and decoder layers using the pre-trained weights , and then directly fine-tuned with the released training data .
The authors conclude that mBART is helpful for transfer learning , even though the languages that are not available in the pre-trained model .
INFOSYS Infosys system for Hindi-Marathi ( Primary ) task is designed to learn the nuances of translation of this low resource language pair by taking advantage of the fact that the source ( Hindi ) and target ( Marathi ) languages are same alphabet languages .
This system is an ensemble of FairSeq model built on anonymized parallel data and FairSeq back - translation model .
The common words / tokens between source and target languages are anonymized during pre-processing upon which the FairSeq model is trained .
The input statements during inferencing are anonymized based on the vocabulary of common tokens prepared during training and the predicted statements are deanonymized during post-processing accordingly .
This improved the accuracy ( BLEU ) of FairSeq considerably .
Pre-processing also applies traditional parallel corpus filtering techniques to clean parallel data followed by domain specific techniques .
There were records containing multiple statements delimited by slashes , where the domain specific techniques are applied to transform them in to records that retain only the matching single statement , identified based on its syntactic similarity with its parallel statement .
Synthetic data generated with the mono-lingual ( Marathi ) data during FairSeq back - translation has unknown words ( w.r.t parallel data vocabulary ) , resulting unknown words during prediction , which are downvoted while ensembling .
IPN-CIC
This team participated in the Spanish - Portuguese language pair .
The systems used the Transformer architecture with a fine-tuning for domain adaptation .
The team proposed experiments on the kind of tokens used ( words and sub-word units ) and the initialization of the word embeddings in the systems using either a random initialization or pre-trained word embeddings .
NICT NICT participated in two language pairs : Hindi-Marathi and Spanish - Catalan , for both translation directions .
Their primary submission is an unsupervised NMT system , initialized with a pre-trained cross-lingual language model ( XLM ) , that has been trained using only the monolingual data provided by the organizers .
They used the standard hyper-parameters for training XLM and unsupervised NMT .
Their contrastive submission is the same but supervised NMT system trained on the combination of the released bilingual and monolingual data .
NITS - CNLP NITS - CNLP system for HI - MR and MR - HI translation is based on cross-lingual language modelling with masked language modeling and translation language modeling .
These language models were pre-trained on monolingual corpus and fine-tuned on parallel data following the architecture of Conneau and Lample ( 2019 ) and employing 6 layers with 8 attention heads and with 32 batch size , trained on a single GPU .
NLPRL
This system submitted by the NLPRL team for the HI - MR is based on the Transformer approach .
The system were trained on only the released parallel corpus .
The team used Sentence - Piece library for preprocessing and set vocabulary size of 5000 symbols for source and target bytepair encoding , respectively .
NLPRL-BHU
The team participated in the HI ? MR language pair .
The participants used byte pair encoding to preprocess the data and fairseq library with the GRU - transformer for training .
NUIG -Panlingua-KMI
The NUIG - Panlingua -KMI team explored phrase - based SMT , dependecy - based SMT method and neural method ( used subword ) for Hindi ?
Marathi language pair .
NUST - FJWU NUST - FJWU system is an extension of state - of- the - art Transformer model with hierarchical attention networks to incorporate contextual information .
During training the model used back -translation .
Prompsit
This team is participating with a rulebased system based on Apertium ( Forcada et al. , 2009 - 11 ) .
Apertium is a free / open-source platform for developing rule- based machine translation systems and language technology that was first released in 2005 .
Apertium is hosted in Github where both language data and code are licensed under the GNU GPL .
It is a research and business platform with a very active community that loves small languages .
Language pairs are at a very different level of development and output quality in the platform , depending on two main variables : how much funded or in - kind effort has been devoted to it and the nature of the languages itself ( the closer , the better ) .
UBC-NLP
The UBC - NLP team participated in the SLT task on all the available language pairs .
The team regularly used all the parallel data and trained 6 - layer Transformer models based on the Fairseq library .
Only for the Slovene -Croatian language pair the team performed backtranslation , noticing a 3 BLEU point improvement in the results .
This team obtained better results with bilingual than with multilingual models ( training a single model for all language groups ) .
UPCTALP
The UPCTALP participated in the Romance pairs .
This team made use of the Transformer architecture improved with multilingual , back - translation and fine-tuning techniques .
Each of this techniques improved over the previous one .
WIPRO - RIT WIPRO - RIT submitted their system to the SLT 2020 Indo - Aryan track .
The presented system is a single multilingual NMT system based on the transformer architecture that can translate between multiple languages .
The presented model is inspired from the model described in Johnson et al . ( 2017 ) . WIPRO - RIT achieved competitive performance ranking 1 st in Marathi to Hindi and 2 nd in Hindi to Marathi translation among 22 systems .
in Hindi to Marathi translation among 22 systems .
Results
We present results for the three language families : three different language families : Indo-Aryan ( Hindi - Marathi ) , Romance ( Spanish - Catalan , Spanish - Portuguese ) , and South -Slavic ( Slovene - Croatian , Slovene - Serbian ) , all of them in the two possible directions .
Like last year edision , the second edition of the Similar Translation Task evaluation was also performed on automatic basis using BLEU ( Papineni et al. , 2002 ) , RIBES ( Isozaki et al. , 2010 ) and TER ( Snover et al. , 2006 ) measures .
Each language direction is reported in one different table which contain information of the team ; type of system , either contrastive ( CONTRASTIVE ) or primary ( PRIMARY ) , and the BLEU , RIBES and TER results .
The scores are sorted by BLEU .
In general , primary systems tend to be better than contrastive systems , as expected , but there are some exceptions .
This year we recived major number of participants for the case of Indo - Aryan language group i.e. Hindi-Marathi ( in both directions ) .
We received 22 submissions from 14 teams .
The best systems ( INFOSYS ) based on BLEU for Hindi-Marathi achieved score 18.26 , however based on other evaluation matric WIPRO - RIT achieved the best 62.45 RIBES and around 72 TER ( see Table 23 ) .
While in the other direction Marathi- Hindi the best performing system ( WIPRO - RIT ) reached 24.53 of BLEU and 66.39 on TER , but based on RIBES score 66.83 , IITDELHI performed the best ( see Table 24 ) .
Similarly to the previous edition of the SLT shared task , participants could submit systems for the Spanish - Portuguese language pair ( in both directions ) .
The best systems for Spanish -to - Portuguese achieved over 32 BLEU and around 52 TER .
While in the opposite direction ( Portugueseto - Spanish ) the best performing system reached 33.82 of BLEU , but its TER score was 52.41 , which is higher than in the case of best performing Spanish - to - Portuguese systems .
As the Spanish - Catalan dev and test sets were aligned with Spanish - Portuguese ones , we noticed that the best results for the Spanish - Catalan language pair are in general much better than for Spanish - Portuguese .
For Spanish - to - Catalan the best system attained over 86 BLEU and below 8 TER .
In the case of Catalan- to - Spanish , the best systems scored around 77 BLEU and less than 15 TER .
A new language group in this year 's SLT task is the group of ( Western ) South Slavic languages - Slovene , Croatian and Serbian , forming two language pairs - Slovene -Croatian and Slovene -Serbian , with one additional twist given the very high mutual intelligibility of Croatian and Serbian .
The best systems for Slovene-to - Croatian achieved 36 BLEU and 43 TER , which is significantly worse than the results of the same bestperforming system in the opposite direction - 43 BLEU and 36 TER .
On the Slovene-Serbian pair a similar phenomenon can be observed - Slovene to Serbian achieving 39 BLEU and 40 TER , while the opposite direction achieves 47 BLEU and 33 TER .
The reason for such a significant lack of symmetry is the better performance of the systems translating into Slovene , probably given that ( Croatian and Serbian ) multi-source translation ( into Slovene ) is simpler than multi-target translation , which was , finally , propagated to the backtranslation procedure , increasing the difference between the directions even further .
Summary
In this section , we presented the results of the WMT SLT 2020 task .
The second iteration of this competition featured data from five language pairs from three different language families : Hindi-Marathi ; Spanish -Catalan and Spanish -Portuguese ; Sloven -Croatian and Slovene-Serbian .
We evaluated the systems translating in both directions of the language pair using three automatic metrics : BLEU , RIBES , and TER .
We observed that the performance varies widely between language pairs .
For example .
the best performing systems trained to translate between Catalan and Spanish in both directions obtained significantly higher results than those trained to translate between other language pairs .
In terms of participation , SLT received system submissions from 18 teams .
In the end of the competition , 14 teams wrote system description papers that appear in the WMT proceedings .
The list of teams with references to the respective system description paper is presented in Table 22 .
Finally , short summaries of each entry , based on the description provided by the participants , were also presented in this section .
Conclusion
This paper presented the results of WMT20 news translation and similar language translation shared tasks , as well as the extra test suites added to the news translation task .
Our main findings rank participating systems in their sentence - level and document- level translation quality , as assessed in a large-scale manual evaluation using the method of Direct Assessment ( DA ) .
For out - of- English language pairs , DA was modified so that the context of the whole document is available while judging individual sentences and assessors are allowed to return to any sentence judgement within the document .
As in previous years , the effect of translationese ( translating from a source which itself was produced in translation ) was avoided except lowerresourced Inuktitut ?
English , Pashto?
English , Khmer ?
English , and German ?
French by creating reference translations always in the same direction as the MT systems are run .
Furthermore , 8 out - of- English language pairs would not need human reference for our evaluation at all because the assessors are evaluating translation candidates bilingually , comparing them to the source text ( as opposed to the reference ) in these language pairs .
The reference translations are nevertheless included as evaluation , hidden among participating MT systems .
This year , English ?
German included two independent reference translations and one humanproduced paraphrase , and English ?
Chinese included two references .
Each of these translations ended up significantly differing in quality from the other ones .
In German ?
English and also Chinese ?
English and English ?
Inuktitut , some MT systems fall in the same cluster with human translation .
The observed variance of human translation quality however demands modesty before making any claims about human parity .
The need for cautious interpretation of the results is also strengthened by the fact that even in English ?
German and English ?
Czech where human translation was seemingly significantly surpassed in 2018 and / or 2019 , the result is not confirmed this year .
Furthermore and similarly to previous year , a test suite this year again suggests that some aspects of translation are not handled by current systems at all .
This year all MT systems fall into the gender bias trap and they tend to make more severe errors than humans ( Zouhar et al. , 2020 ) .
The results of the task on similar language translation indicate that the performance when translating between pairs of closely - related languages is extremely varied across different language pairs .
The best performing systems trained to translate between Catalan and Spanish , for example , obtained significantly higher results in both directions than those trained to translate between other language pairs in terms of BLEU , RIBES , and TER .
human evaluation via Mechanical Turk .
We would like to thank Roland Kuhn for advising on the Inuktitut ?
English task organization and the Nunavut Maligaliurvia ( Legislative Assembly of Nunavut ) and Nunatsiaq News for supplying all the Inuktitut ?
English parallel data .
The organizers of the similar languages task would like to thank Ciklopea and Bisnode for the Croatian , Serbian , and Slovene data , and Pangeanic for the Catalan , Portuguese , and Spanish data .
The work of the organizers of the similar languages task is supported in part by the Spanish Ministerio de Ciencia e Innovaci ? n , through the postdoctoral senior grant Ram?n y Cajal and by the Agencia Estatal de Investigaci ?n through the projects EUR2019-103819 , PCIN -2017-079 and PID2019-107579RB-I00 / AEI / 10.13039/501100011033 .
