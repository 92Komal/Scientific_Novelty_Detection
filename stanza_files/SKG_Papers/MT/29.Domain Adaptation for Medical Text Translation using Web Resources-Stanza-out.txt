title
Domain Adaptation for Medical Text Translation Using Web Resources
abstract
This paper describes adapting statistical machine translation ( SMT ) systems to medical domain using in- domain and general - domain data as well as webcrawled in -domain resources .
In order to complement the limited in- domain corpora , we apply domain focused webcrawling approaches to acquire indomain monolingual data and bilingual lexicon from the Internet .
The collected data is used for adapting the language model and translation model to boost the overall translation quality .
Besides , we propose an alternative filtering approach to clean the crawled data and to further optimize the domain-specific SMT system .
We attend the medical summary sentence unconstrained translation task of the Ninth Workshop on Statistical Machine Translation ( WMT2014 ) .
Our systems achieve the second best BLEU scores for Czech-English , fourth for French - English , English - French language pairs and the third best results for reminding pairs .
Introduction
In this paper , we report the experiments carried out by the NLP 2 CT Laboratory at University of Macau for WMT2014 medical sentence translation task on six language pairs : Czech-English ( cs-en ) , French -English ( fr-en ) , German-English ( de-en ) and the reverse direction pairs ( i.e. , en-cs , en-fr and en-de ) .
As data in specific domain are usually relatively scarce , the use of web resources to com-plement the training resources provides an effective way to enhance the SMT systems ( Resnik and smith , 2003 ; Espl ?
- Gomis and Forcada , 2010 ; Pecina et al. , 2011 ; Pecina et al. , 2012 ; Pecina et al. , 2014 ) .
In our experiments , we not only use all available training data provided by the WMT2014 standard translation task 1 ( generaldomain data ) and medical translation task 2 ( indomain data ) , but also acquire addition indomain bilingual translations ( i.e. dictionary ) and monolingual data from online sources .
First of all , we collect the medical terminologies from the web .
This tiny but significant parallel data are helpful to reduce the out-ofvocabulary words ( OOVs ) in translation models .
In addition , the use of larger language models during decoding is aided by more efficient storage and inference ( Heafield , 2011 ) .
Thus , we crawl more in- domain monolingual data from the Internet based on domain focused web-crawling approach .
In order to detect and remove outdomain data from the crawled data , we not only explore text - to - topic classifier , but also propose an alternative filtering approach combined the existing one ( text-to- topic classifier ) with perplexity .
After carefully pre-processing all the available training data , we apply language model adaptation and translation model adaptation using various kinds of training corpora .
Experimental results show that the presented approaches are helpful to further boost the baseline system .
The reminder of this paper is organized as follows .
In Section 2 , we detail the workflow of web resources acquisition .
Section 3 describes the pre-processing steps for the corpora .
Section 5 presents the baseline system .
Section 6 reports the experimental results and discussions .
Finally , the submitted systems and the official results are reported in Section 7 .
Domain Focused Web-Crawling
In this section , we introduce our domain focused web-crawling approaches on acquisition of indomain translation terminologies and monolingual sentences .
Bilingual Dictionary
Terminology is a system of words used to name things in a particular discipline .
The in- domain vocabulary size directly affects the performance of domain-specific SMT systems .
Small size of in- domain vocabulary may result in serious OOVs problem in a translation system .
Therefore , we crawl medical terminologies from some online sources such as dict.cc 3 , where the vocabularies are divided into different subjects .
We obtain the related bilingual entries in medicine subject by using Scala build - in XML parser and XPath .
After cleaning , we collected 28,600 , 37,407 , and 37,600 entries in total for cs-en , deen , and fr-en respectively .
Monolingual Data
The workflow for acquiring in- domain resources consists of a number of steps such as domain identification , text normalization , language identification , noise filtering , and post-processing as well as parallel sentence identification .
Firstly we use an open-source crawler , Combine 4 , to crawl webpages from the Internet .
In order to classify these webpages as relevant to the medical domain , we use a list of triplets < term , relevance weight , topic class > as the basic entries to define the topic .
Term is a word or phrase .
We select terms for each language from the following sources : ?
The Wikipedia title corpus , a WMT2014 official data set consisting of titles of medical articles .
?
The dict.cc dictionary , as is described in Section 2.1 . ?
The DrugBank corpus , which is a WMT2014 official data set on bioinformatics and cheminformatics .
For the parallel data , i.e.
Wikipedia and dict.cc dictionary , we separate the source and target text into individual text and use either side of them for constructing the term list for different lan-guages .
Regarding the DrugBank corpus , we directly extract the terms from the " name " field .
The vocabulary size of collected text for each language is shown in Table 1 . Wikipedia Titles 12,684 3,404 10,396
8 Relevance weight is the score for each occurrence of the term , which is assigned by its length , i.e. , number of tokens .
The topic class indicates the topics .
In this study , we are interested in medical domain , the topic class is always marked with " MED " in our topic definition .
EN CS DE FR
The topic relevance of each document is calculated 5 as follows : ? ? ( 1 ) where is the amount of terms in the topic definition ; is the weight of term ; is the weight of term at location . is the number of occurrences of term at position .
In implementation , we use the default values for setting and parameters .
Another input required by the crawler is a list of seed URLs , which are web sites that related to medical topic .
We limit the crawler from getting the pages within the http domain guided by the seed links .
We acquired the list from the Open Directory Project 6 , which is a repository maintained by volunteer editors .
Totally , we collected 12,849 URLs from the medicine category .
Text normalization is to convert the text of each HTML page into UTF - 8 encoding according to the content_charset of the header .
In addition , HTML pages often consist of a number of irrelevant contents such as the navigation links , advertisements disclaimers , etc. , which may negatively affect the performance of SMT system .
Therefore , we use the Boilerpipe tool ( Kohlsch ? tter et al. , 2010 ) to filter these noisy data and preserve the useful content that is marked by the tag , < canonicalDocument >.
The resulting text is saved in an XML file , which will be further processed by the subsequent tasks .
For language identification , we use the languagedetection 7 toolkit to determine the possible lan-guage of the text , and discard the articles which are in the right language we are interested .
Data Filtering
The web-crawled documents ( described in Section 2.2 ) may consist a number of out-domain data , which would harm the domain-specific language and translation models .
We explore and propose two filtering approaches for this task .
The first one is to filter the documents based on their relative score , Eq. ( 1 ) .
We rank all the documents according to their relative scores and select top K percentage of entire collection for further processing .
Second , we use a combination method , which takes both the perplexity and relative score into account for the selection .
Perplexity - based data selection has shown to be a powerful mean on SMT domain adaptation ( Wang et al. , 2013 ; Wang et al. , 2014 ; Toral , 2013 ; Rubino et al. , 2013 ; Duh et al. , 2013 ) .
The combination method is carried out as follows : we first retrieve the documents based on their relative scores .
The documents are then split into sentences , and ranked according to their perplexity using Eq. ( 2 ) ( Stolcke et al. , 2002 ) .
The used language model is trained on the official in- domain data .
Finally , top N percentage of ranked sentences are considered as additional relevant in - domain data . ( ) ( ) ( 2 ) where is a input sentence or document , ( ) is the probability of - gram segments estimated from the training set .
is the number of tokens of an input string .
Pre-processing Both official training data and web-crawled resources are processed using the Moses scripts 8 , this includes the text tokenization , truecasing length cleaning .
For trusecasing , we use both the target side of parallel corpora and monolingual data to train the trucase models .
We consider the target system is intended for summary translation , the sentences tend to be short in length .
We remove sentence pairs which are more than 80 words at length in either sides of the parallel text .
In addition to these general data filtering steps , we introduce some extra steps to pre-process the training data .
The first step is to remove the duplicate sentences .
In data-driven methods , the more frequent a term occurs , the higher probabil -ity it biases .
Duplicate data may lead to unpredicted behavior during the decoding .
Therefore , we keep only the distinct sentences in monolingual corpus .
By taking into account multiple translations in parallel corpus , we remove the duplicate sentence pairs .
We also use a biomedical sentence splitter 9 ( Rune et al. , 2007 ) to split sentences in monolingual corpora .
The statistics of the data are provided in Table 2 .
Baseline System
We built our baseline system on an optimized level .
It is trained on all official in- domain training corpora and a portion of general - domain data .
We apply the Moore-Lewis method ( Moore and Lewis , 2010 ) and modified Moore - Lewis method ( Axelrod et al. , 2011 ) for selecting in- domain data from the general - domain monolingual and parallel corpora , respectively .
The top M percentages of ranked sentences are selected as a pseudo in-domain data to train an additional LM and TM .
For LM , we linearly interpolate the additional LM with in - domain LM .
For TM , the additional model is log-linearly interpolated with the in-domain model using the multi-decoding method described in ( Koehn and Schroeder , 2007 ) .
Finally , LM adaptation and TM adaptation are combined to further improve the translation quality of baseline system .
Experiments and Results
The official medical summary development sets ( dev ) are used for tuning and evaluating the comparative systems .
The official medical summary test sets ( test ) are only used in our final submitted systems .
The experiments were carried out with the Moses 1.0 10 ( .
The translation and the re-ordering model utilizes the " growdiag -final " symmetrized word - to - word alignments created with MGIZA ++ 11 ( Och and Ney , 2003 ; Gao and Vogel , 2008 ) and the training scripts from Moses .
A 5 - gram LM was trained using the SRILM toolkit 12 ( Stolcke et al. , 2002 ) , exploiting improved modified Kneser - Ney smoothing , and quantizing both probabilities and back -off weights .
For the log-linear model training , we take the minimum-error-rate training ( MERT ) method as described in ( Och , 2003 ) .
In the following sub-sections , we describe the results of baseline systems , which are trained on the official corpora .
We also present the enhanced systems that make use of the webcrawled bilingual dictionary and monolingual data as the additional training resources .
Two variants of enhanced system are constructed based on different filtering criteria .
Baseline System
The baseline systems is constructed based on the combination of TM adaptation and LM adaptation , where the corresponding selection thresholds ( ) are manually tuned .
Table 3 shows the BLEU scores of baseline systems as well as the threshold values of for general - domain monolingual corpora and parallel corpora selection , respectively .
By looking into the results , we find that en-cs system performs poorly , because of the limited in - domain parallel and monolingual corpora ( shown in Table 2 ) .
While the fr-en and en-fr systems achieve the best scores , due the availability of the high volume training data .
We experiment with different values of = {0 , 25 , 50 , 75 , 100 } that indicates the percentages of sentences out of the general corpus used for con-structing the LM adaptation and TM adaptation .
After tuning the parameter , we find that BLEU scores of different systems peak at different values of .
LM adaptation can achieve the best translation results for cs-en , en-fr and de-en pairs when = 25 , en-cs and en-de pairs when = 50 , and fr-en pair when = 75 .
While TM adaptation yields the best scores for en-fr and ende pairs at = 25 and cs-en and fr-en pairs at =50 , de-en pair when = 75 and en-cs pair at = 100 .
The relevance score filtering approach yields an improvement of 3.08 % of BLEU score for deen pair that is the best result among the language pairs .
On the other hand , en- cs pair obtains a marginal gain .
The reason is very obvious that the training data is very insufficient .
Empirical results of all language pairs expect fr-en indicate that data filtering is the necessity to improve the system performance .
Lang
Based on Moore-Lewis Filtering
In this approach , we need to determine the values of two parameters , top documents and top sentences , where = {100 , 75 , 50 } and = {75 , 50 , 25 } , . When = 100 , it is a conventional perplexity - based data selection method , i.e. no document will be filtered .
Table 5 shows the combination of different and that gives the best translation score for each language pair .
We provide the absolute BLEU for each system , together with relative improvements ( ? % ) that compared to the baseline system .
In this shared task , we have a quality and quantity in-domain monolingual training data for English .
All the systems that take English as the target translation always outperform the other reverse pairs .
Besides , we found the systems based on the perplexity data selection method tend to achieve a better scores in BLEU .
Lang .
Pair
Docs
Official Results and Conclusions
We described our study on developing unconstrained systems in the medical translation task of 2014 Workshop on Statistical Machine Translation .
In this work , we adopt the web crawling strategy for acquiring the in-domain monolingual data .
In detection the domain data , we exploited Moore - Lewis data selection method to filter the collected data in addition to the build - in scoring model provided by the crawler toolkit .
However , after investigation , we found that the two methods are very competitive to each other .
The systems we submitted to the shared task were built using the language models and translation models that yield the best results in the individual testing .
The official test set is converted into the recased and detokenized SGML format .
Table 9 presents the official results of our submissions for every language pair .
Lang .
Pair
BLEU of Table 1 : 1 Size of terms used for topic definition .
,436
Table 2 : 2 Statistics summary of corpora after pre-processing . ={0 , 25 , 50 , 75 , 100 } that represents the percentages of crawled documents we used for training the LMs .
In Table4 , we show the absolute BLEU scores of the evaluated systems , listed with the optimized thresholds , and the relative improvements ( ? % ) in compared to the baseline system .
The size of additional training data ( for LM ) is displayed at the last column .
. Pair BLEU Mono. ( M % ) Parallel ( M % ) en-cs 17.57 50 % 100 % cs-en 31.29 25 % 50 % en-fr 38.36 25 % 25 % fr-en 44.36 75 % 50 % en-de 18.01 50 % 25 % de-en 32.50 25 % 75 % Table 3 : BLEU scores of baseline systems for different language pairs .
5.2 Based on Relevance Score Filtering
As described in Section 2.3 , we use the relevance score to filter out the non-in-domain documents .
Once again , we evaluate different values of
Table 4 : 4 Evaluation results for systems that trained on relevance -score-filtered documents .
Table 5 : 5 Evaluation results for systems that trained on combination filtering approach .
( % ) Target Size ( % ) BLEU ? ( % ) en-cs 50 25 17.69 0.68 en-de 100 50 18.03 0.11 en-fr 100 50 38.73 0.96 cs-en 100 25 32.20 2.91 de-en 100 25 33.10 1.85 fr-en 100 25 45.22 1.94
Table 6 : 6 BLEU scores of the submitted systems for the medical translation task in six language pairs .
Combined Official systems BLEU en-cs 23.16 ( + 5.59 ) 22.10 cs-en 36.8 ( + 5.51 ) 37.40 en-fr 40.34 ( + 1.98 ) 40.80 fr-en 45.79 ( + 1.43 ) 43.80 en-de 19.36 ( + 1.35 ) 18.80 de-en 34.17 ( + 1.67 ) 32.70
http://www.statmt.org/wmt14/translation-task.html.
2 http://www.statmt.org/wmt14/medical-task/.
http://www.dict.cc/.
4 http://combine.it.lth.se/.
http://combine.it.lth.se/documentation/DocMain/node6.html.6
http://www.dmoz.org/Health/Medicine/.
7 https://code.google.com/p/language-detection/.
http://www.statmt.org/moses/?n=Moses.Baseline.
http://www.nactem.ac.uk/y-matsu/geniass/. 10 http://www.statmt.org/moses/. 11 http://www.kyloo.net/software/doku.php/mgiza:overview.
12 http://www.speech.sri.com/projects/srilm/.
