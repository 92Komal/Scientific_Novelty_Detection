title
TESLA at WMT 2011 : Translation Evaluation and Tunable Metric
abstract
This paper describes the submission from the National University of Singapore to the WMT 2011 Shared Evaluation Task and the Tunable Metric Task .
Our entry is TESLA in three different configurations : TESLA -M , TESLA -F , and the new TESLA -B .
Introduction TESLA ( Translation Evaluation of Sentences with Linear-programming - based Analysis ) was first proposed in Liu et al . ( 2010 ) .
The simplest variant , TESLA -M ( M stands for minimal ) , is based on Ngram matching , and utilizes light - weight linguistic analysis including lemmatization , part- of-speech tagging , and WordNet synonym relations .
TESLA -B ( B stands for basic ) additionally takes advantage of bilingual phrase tables to model phrase synonyms .
It is a new configuration proposed in this paper .
The most sophisticated configuration TESLA -F ( F stands for full ) additionally uses language models and a ranking support vector machine instead of simple averaging .
TESLA - F was called TESLA in Liu et al . ( 2010 ) .
In this paper , we rationalize the naming convention by using TESLA to refer to the whole family of metrics .
The rest of this paper is organized as follows .
Sections 2 to 4 describe the TESLA variants TESLA -M , TESLA -B , and TESLA -F , respectively .
Section 5 describes MT tuning with TESLA .
Section 6 shows experimental results for the evaluation and the tunable metric task .
The last section concludes the paper .
TESLA -M
The version of TESLA - M used in WMT 2011 is exactly the same as in Liu et al . ( 2010 ) .
The description is reproduced here for completeness .
We consider the task of evaluating machine translation systems in the direction of translating a source language to a target language .
We are given a reference translation produced by a professional human translator and a machine - produced system translation .
At the highest level , TESLA - M is the arithmetic average of F-measures between bags of Ngrams ( BNGs ) .
A BNG is a multiset of weighted N-grams .
Mathematically , a BNG
B consists of tuples ( b i , b W i ) , where each b i is an N-gram and b W i is a positive real number representing the weight of b i .
In the simplest case , a BNG contains every N-gram in a translated sentence , and the weights are just the counts of the respective N-grams .
However , to emphasize the content words over the function words , we discount the weight of an N-gram by a factor of 0.1 for every function word in the N-gram .
We decide whether a word is a function word based on its POS tag .
In TESLA -M , the BNGs are extracted in the target language , so we call them bags of target language N-grams ( BTNGs ) .
Similarity functions
To match two BNGs , we first need a similarity measure between N-grams .
In this section , we define the similarity measures used in our experiments .
We adopt the similarity measure from MaxSim ( Chan and Ng , 2008 ; Chan and Ng , 2009 )
The synsets are obtained by querying WordNet ( Fellbaum , 1998 ) .
For languages other than English , a synonym dictionary is used instead .
We define two other similarity functions between unigrams : s lem ( x , y ) = I( lemma ( x ) = lemma( y ) ) s pos ( x , y ) = I ( POS ( x ) = POS ( y ) )
All the three unigram similarity functions generalize to N-grams in the same way .
For two N-grams x = x 1,2 , ... , n and y = y 1,2 , ... , n , s( x , y ) = 0 if ?i , s( x i , y i ) = 0 1 n n i=1 s( x i , y i ) otherwise
Matching two BNGs
Now we describe the procedure of matching two BNGs .
We take as input BNGs X and Y and a similarity measure s.
The i-th entry in X is x i and has weight x W i ( analogously for y j and y W j ) .
Intuitively , we wish to align the entries of the two BNGs in a way that maximizes the overall similarity .
An example matching problem for bigrams is shown in Figure 1a , where the weight of each node is shown , along with the hypothetical similarity for each edge .
Edges with a similarity of zero are not shown .
Note that for each function word , we discount the weight by a factor of ten .
The solution to the matching problem is shown in Figure 1 b , and the overall similarity is 0.5 ? 0.01 + 0.8 ? 0.1 + 0.8 ? 0.1 = 0.165 .
Mathematically , we formulate this as a ( realvalued ) linear programming problem 1 . The variables are the allocated weights for the edges w( x i , y j ) ?i , j 1
While integer linear programming is NP - complete , realvalued linear programming can be solved efficiently .
s( x i , y j ) w( x i , y j ) subject to w( x i , y j ) ?
0 ?i , j j w( x i , y j ) ? x W i ? i i w( x i , y j ) ? y W j ?j
The value of the objective function is the overall similarity S. Assuming X is the reference and Y is the system translation , we have Precision = S j y W j Recall = S i x W i The F-measure is derived from the precision and the recall : F = Precision ? Recall ? ? Precision + ( 1 ? ? ) ? Recall
In this work , we set ? = 0.8 , following MaxSim .
The value gives more importance to the recall than the precision .
If the similarity function is binary - valued and transitive , such as s lem and s pos , then we can use a much simpler and faster greedy matching procedure : the best match is simply g min( x i =g x W i , y i =g y W i ) .
Scoring
The TESLA - M sentence - level score for a reference and a system translation is the arithmetic average of the BTNG F-measures for unigrams , bigrams , and trigrams based on similarity functions s ms and s pos .
We thus have 3 ? 2 = 6 BTNG F-measures for TESLA -M .
We can compute a system- level score for a machine translation system by averaging its sentencelevel scores over the complete test set .
TESLA -B TESLA - B uses the average of two types of Fmeasures : ( 1 ) BTNG F-measures as in TESLA -M and ( 2 ) F-measures between bags of N-grams in one or more pivot languages , called bags of pivot language N-grams ( BPNGs ) ,
The rest of this section focuses on the generation of the BPNGs .
Their matching is done in the same way as described for BTNGs in the previous section .
Phrase level semantic representation Given a sentence - aligned bitext between the target language and a pivot language , we can align the text at the word level using well known tools such as GIZA ++ ( Och and Ney , 2003 ) or the Berkeley aligner ( Liang et al. , 2006 ; Haghighi et al. , 2009 ) .
We observe that the distribution of aligned phrases in a pivot language can serve as a semantic representation of a target language phrase .
That is , if two target language phrases are often aligned to the same pivot language phrase , then they can be inferred to be similar in meaning .
Similar observations have been made by previous researchers ( Bannard and Callison - Burch , 2005 ; Callison - Burch et al. , 2006 ; Snover et al. , 2009 ) .
We note here two differences from WordNet synonyms : ( 1 ) the relationship is not restricted to the word level only , and ( 2 ) the relationship is not binary .
The degree of similarity can be measured by the percentage of overlap between the semantic representations .
Segmenting a sentence into phrases
To extend the concept of this semantic representation of phrases to sentences , we segment a sentence in the target language into phrases .
Given a phrase table , we can approximate the probability of a phrase p by : P r( p ) = N ( p ) p N ( p ) ( 1 ) where N ( ? ) is the count of a phrase in the phrase table .
We then define the likelihood of segmenting a sentence S into a sequence of phrases ( p 1 , p 2 , . . . , p n ) by : P r( p 1 , p 2 , . . . , p n | S ) = 1 Z( S ) n i=1 P r( p i ) ( 2 ) where Z( S ) is a normalizing constant .
The segmentation of S that maximizes the probability can be determined efficiently using a dynamic programming algorithm .
The formula has a strong preference for longer phrases , as every P r( p ) is a small fraction .
To deal with out - of- vocabulary ( OOV ) words , we allow any single word w to be considered a phrase , and if N ( w ) = 0 , we set N ( w ) = 0.5 instead .
BPNGs as sentence level semantic representation Simply merging the phrase-level semantic representation is insufficient to produce a sensible sentencelevel semantic representation .
As an example , we consider two target language ( English ) sentences segmented as follows : 1
The collection of all such N-grams and their corresponding weights forms the BPNG of a sentence .
The reference and system BPNGs are then matched using the algorithm outlined in Section 2.2 .
Scoring
The TESLA - B sentence - level score is a linear combination of ( 1 ) BTNG F-measures for unigrams , bigrams , and trigrams based on similarity functions s ms and s pos , and ( 2 ) BPNG
F-measures for unigrams , bigrams , and trigrams based on similarity functions s lem and s pos .
We thus have 3 ? 2 F-measures from the BTNGs and 3 ? 2 ? # pivot languages F-measures from the BPNGs .
We average the BTNG and BPNG scores to obtain s BTNG and s BPNG , respectively .
The sentencelevel TESLA - B score is then defined as 1 2 ( s BTNG + s BPNG ) .
The two-step averaging process prevents the BPNG scores from overwhelming the BTNG scores , especially when we have many pivot languages .
The system- level TESLA - B score is the arithmetic average of the sentence - level TESLA - B scores .
Unlike the simple arithmetic averages used in TESLA -M and TESLA-B , TESLA - F uses a general linear combination of three types of scores : ( 1 ) BTNG F-measures as in TESLA -M and TESLA -B , ( 2 ) BPNG F-measures as in TESLA -B , and ( 3 ) normalized language model scores of the system translation , defined as 1 n log P , where n is the length of the translation , and P the language model probability .
The method of training the linear model depends on the development data .
In the case of WMT , the development data is in the form of manual rankings , so we train SVM rank ( Joachims , 2006 ) on these instances to build the linear model .
In other scenarios , some form of regression can be more appropriate .
The BTNG and BPNG scores are the same as used in TESLA -B .
In the WMT campaigns , we use two language models , one generated from the Europarl dataset and one from the news-train dataset .
We thus have 3 ? 2 features from the BTNGs , 3 ? 2 ? # pivot languages features from the BPNGs , and 2 features from the language models .
Again , we can compute system-level scores by averaging the sentence - level scores .
Scaling of TESLA -F Scores
While machine translation evaluation is concerned only with the relative order of the different translations but not with the absolute scores , there are practical advantages in normalizing the evaluation scores to a range between 0 and 1 .
For TESLA -M and TESLA -B , this is already the case , since every F-measure has a range of [ 0 , 1 ] and so do their averages .
In contrast , the SVM rank - produced model typically gives scores very close to zero .
To remedy that , we note that we have the freedom to scale and shift the linear SVM model without changing the metric .
We observe that the Fmeasures have a range of [ 0 , 1 ] , and studying the data reveals that [ ? 15 , 0 ] is a good approximation of the range for normalized language model scores , for all languages involved in the WMT campaign .
Since we know the range of values of an F-measure feature ( between 0 and 1 ) and assuming that the range of the normalized LM score is between - 15 and 0 , we can find the maximum and minimum possible score given the weights .
Then we linearly scale the range of scores from [ min , max ] to [ 0 , 1 ] .
We provide an option of scaling TESLA - F scores in the new release of TESLA .
MT tuning with TESLA All variants of TESLA can be used for automatic MT tuning using Z-MERT ( Zaidan , 2009 ) . Z-MERT 's modular design makes it easy to integrate a new metric .
As TESLA already computes scores at the sentence level , integrating TESLA into Z-MERT was straightforward .
First , we created a " streaming " version of each TESLA metric which reads translation candidates from standard input and prints the sentence - level scores to standard output .
This allows Z-MERT to easily query the metric for sentencelevel scores during MT tuning .
Second , we wrote a Java wrapper that calls the TESLA code from Z-MERT .
The resulting metric can be used for MERT tuning in the standard fashion .
All that a user has to do is to change the metric in the Z-MERT configuration file to TESLA .
All the necessary code for Z-MERT tuning is included in the new release of TESLA .
Experiments
Evaluation Task
We evaluate TESLA using the publicly available data from WMT 2009 for into-English and outof-English translation .
The pivot language phrase tables and language models are built using the WMT 2009 training data .
The SVM rank model for TESLA - F is trained on manual rankings from WMT 2008 .
The results for TESLA -M and TESLA - F have previously been reported in Liu et al . ( 2010 ) 3 . We add results for the new variant TESLA - B here .
Tables 1 and 2 show the sentence - level consistency and system-level Spearman 's rank correlation , respectively for into-English translation .
For comparison , we include results for some of the best performing metrics in WMT 2009 .
Tables 3 and 4 show the same results for out - of - English translation .
We do not include the English - Czech language pair in our experiments , as we unfortunately do not have good linguistic resources for the Czech language .
cz-en fr-en de-en es-en hu-en Overall
The new TESLA - B metric proves to be competitive to its siblings and is often on par with the more sophisticated TESLA - F metric .
The exception is the English - German language pair , where TESLA - B has very low system-level correlation .
We have two possible explanations for this .
First , the systemlevel correlation is computed on a very small sample size ( the ranked list of MT systems ) .
This makes the system-level correlation score more volatile compared to the sentence - level consistency score which is computed on thousands of sentence pairs .
Second , German has a relatively free word order which potentially makes word alignment and phrase table extraction more noisy .
Interestingly , all participating metrics in WMT 2009 had low system-level correlation for the English - German language pair .
et al. , 2002 ) .
To allow for a fair comparison , the WMT organizers provided participants with a complete Joshua MT system for an Urdu-English translation task .
We tuned models for each variant of TESLA , using Z-MERT in the default configuration provided by the organizers .
There are four reference translations for each Urdu source sentence .
The size of the N-best list is set to 300 .
For our own experiments , we randomly split the development set into a development portion ( 781 sentences ) and a held - out test portion ( 200 sentences ) .
We run the same Z-MERT tuning process for each TESLA variant on this reduced development set and evaluate the resulting models on the held out test set .
We include a model trained with BLEU as an additional reference point .
The results are shown in Table 5 .
We observe that the model trained with TESLA - F achieves the best results when evaluated with any of the TESLA metrics , although the differences between the scores are small .
We found that TESLA produces slightly longer translations than BLEU : 22.4 words ( TESLA - M ) , 21.7 words ( TESLA - B ) , and 22.5 words ( TESLA - F ) , versus 18.7 words ( BLEU ) .
The average reference length is 19.8 words .
The official evaluation for the tunable metric task is performed using manual rankings .
The score of a system is calculated as the percentage of times the system is judged to be either better or equal ( score1 ) or strictly better ( score2 ) compared to each other system in pairwise comparisons .
Although we submit results for all TESLA variants , only our primary submission TESLA - F is included in the manual evaluation .
The results for TESLA - F are mixed .
When evaluated with score1 , TESLA - F is
Conclusion
We introduce TESLA -B , a new variant of the TESLA machine translation metric and present experimental results for all TESLA variants in the setting of the WMT evaluation task and tunable metric task .
All TESLA variants are integrated into Z-MERT for automatic machine translation tuning .
as s ms .
For unigrams x and y , ? If lemma( x ) = lemma( y ) , then s ms = 1 . ?
Otherwise , let a = I(synsets ( x ) overlap with synsets ( y ) ) b = I ( POS ( x ) = POS ( y ) ) where I ( ? ) is the indicator function , then s ms = ( a + b ) /2 .
