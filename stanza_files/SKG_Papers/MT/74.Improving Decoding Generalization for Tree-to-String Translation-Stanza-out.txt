title
Improving Decoding Generalization for Tree-to- String Translation
abstract
To address the parse error issue for tree-tostring translation , this paper proposes a similarity - based decoding generation ( SDG ) solution by reconstructing similar source parse trees for decoding at the decoding time instead of taking multiple source parse trees as input for decoding .
Experiments on Chinese-English translation demonstrated that our approach can achieve a significant improvement over the standard method , and has little impact on decoding speed in practice .
Our approach is very easy to implement , and can be applied to other paradigms such as tree - to - tree models .
Introduction
Among linguistically syntax - based statistical machine translation ( SMT ) approaches , the tree-tostring model ( Huang et al. 2006 ; Liu et al. 2006 ) is the simplest and fastest , in which parse trees on source side are used for grammar extraction and decoding .
Formally , given a source ( e.g. , Chinese ) string c and its auto-parsed tree T 1 - best , the goal of typical tree-to-string SMT is to find a target ( e.g. , English ) string e* by the following equation as where Pr(e|c , T 1 - best ) is the probability that e is the translation of the given source string c and its T 1 - best .
A typical tree- to-string decoder aims to search for the best derivation among all consistent derivations that convert source tree into a target - language string .
We call this set of consistent derivations the tree-to-string search space .
Each derivation in the search space respects the source parse tree .
Parsing errors on source parse trees would cause negative effects on tree-to-string translation due to decoding on incorrect source parse trees .
To address the parse error issue in tree-to-string translation , a natural solution is to use n-best parse trees instead of 1 - best parse tree as input for decoding , which can be expressed by where < T sim > denotes a set of similar parse trees of T 1 - best that are dynamically reconstructed at the de-coding time .
Roughly speaking , < T n-best > is a subset of { T 1 - best , < T sim >}.
Along this line of thinking , Equation ( 2 ) can be considered as a special case of Equation ( 3 ) .
In our SDG solution , given a source parse tree T 1 - best , the key is how to generate its < T sim > at the decoding time .
In practice , it is almost intractable to directly reconstructing < T sim > in advance as input for decoding due to too high computation complexity .
To address this crucial challenge , this paper presents a simple and effective technique based on similarity - based matching constraints to construct new similar source parse trees for decoding at the decoding time .
Our SDG approach can explicitly increase the tree-to-string search space for decoding without changing any grammar extraction and pruning settings , and has little impact on decoding speed in practice .
Tree-to - String Derivation
We choose the tree-to-string paradigm in our study because this is the simplest and fastest among syntax - based models , and has been shown to be one of the state - of - the - art syntax - based models .
Typically , by using the GHKM algorithm ( Galley et al. 2004 ) , translation rules are learned from word-aligned bilingual texts whose source side has been parsed by using a syntactic parser .
Each rule consists of a syntax tree in the source language having some words ( terminals ) or variables ( nonterminals ) at leaves , and sequence words or variables in the target language .
With the help of these learned translation rules , the goal of tree- to-string decoding is to search for the best derivation that converts the source tree into a target - language string .
A derivation is a sequence of translation steps ( i.e. , the use of translation rules ) .
Figure 1 shows an example derivation d that performs translation over a Chinese source parse tree , and how this process works .
In the first step , we can apply rule r 1 at the root node that matches a subtree { IP [ 1 ] ( NP [ 2 ] VP [ 3 ] ) }.
The corresponding target side {x 1 x 2 } means to preserve the top-level word-order in the translation , and results in two unfinished subtrees with root labels NP [ 2 ] and VP [ 3 ] , respectively .
The rule r 2 finishes the translation on the subtree of NP [ 2 ] , in which the Chinese word " ? " is translated into an English string " the Chinese side " .
The rule r 3 is applied to perform translation on the subtree of VP [ 3 ] , and results in an An example tree -to-string derivation d consisting of five translation rules is given as follows : r 1 : IP [ 1 ] ( x 1 : NP [ 2 ] x 2 :VP [ 3 ] ) ? x 1 x 2 r 2 : NP [ 2 ] ( NN ( ? ) ) ? the Chinese side r 3 : VP [ 3 ] ( ADVP ( AD ( ? ) ) VP ( VV ( ? ) AS ( ? ) x 1 :NP [ 4 ] ) ) ? highly appreciated x 1 r 4 : NP [ 4 ] ( DP ( DT ( ? ) CLP ( M ( ? ) ) ) x 1 :NP [ 5 ] ) ? this x 1 r 5 : NP [ 5 ] ( NN ( ? ) ) ? talk Translation results :
The Chinese side highly appreciated this talk .
unfinished subtree of NP [ 4 ] .
Similarly , rules r 4 and r 5 are sequentially used to finish the translation on the remaining .
This process is a depth-first search over the whole source tree , and visits every node only once .
3 Decoding Generalization
Similarity - based Matching Constraints
In typical tree- to-string decoding , an ordered sequence of rules can be reassembled to form a derivation d whose source side matches the given source parse tree T .
The source side of each rule in d should match one of subtrees of T , referred to as matching constraint .
Before discussing how to apply our similarity - based matching constraints to reconstruct new similar source parse trees for decoding at the decoding time , we first define the similarity between two tree- to-string rules .
Definition
1 Given two tree- to-string rules t and u , we say that t and u are similar such that their source sides t s and u s have the same root label and frontier nodes , written as u t ? , otherwise not .
Given a source - language parse tree T , in typical tree-to-string matching constraint scheme shown in Figure 3 Since an incorrect source parse tree might filter out good derivations during tree - to-string decoding , our similarity - based scheme is much more likely to recover the correct tree for decoding at the decoding time , and does not rule out good ( potentially correct ) translation choices .
In our method , many new source - language trees T * that are similar to but different from the original source tree T can be reconstructed at the decoding time .
In theory our similarity - based scheme can increase the search space of the tree- to-string decoder , but we did not change any rule extraction and pruning settings .
In practice , our similarity - based scheme can effectively keep the advantage of fast decoding for tree - to-string translation because its implementation is very simple .
Let 's revisit the example derivation d in Figure 1 , i.e. , d=r 1 ?r 2 ?r 3 ?r 4 ?r 5 1 .
In such a case , the decoder can effectively produce a new derivation d* by simply replacing rule r 3 with its similar rule ?
3 ( 3 r ? ) shown in Figure 2 , that is , d*=r 1 ?r 2 ? 3 ?r 4 ?r 5 .
With beam search , typical tree- to-string decoding with an integrated language model can run in time 2 O( ncb 2 ) in practice ( Huang 2007 ) .
For our decoding time complexity computation , only the parameter c value can be affected by our similarity - based scheme .
In other words , our similaritybased scheme would result in a larger c value at decoding time as many similar translation rules might be matched at each node .
In practice , there are two feasible optimization techniques to alleviate this problem .
The first technique is to limit the maximum number of similar translation rules matched at each node .
The second one is to predefine a similarity threshold to filter out less similar translation rules in advance .
In the implementation , we add a new feature into the model : similarity - based matching counting feature .
This feature counts the number of similar rules used to form the derivation .
The weight ? sim of this feature is tuned via minimal error rate training ( MERT ) ( Och 2003 ) with other feature weights .
Pseudo-rule Generation
In the implementation of tree- to-string decoding , the first step is to load all translation rules matched at each node of the source tree T .
It is possible that some nonterminal nodes do not have any matched rules when decoding some new sentences .
If the root node of the source tree has no any matched rules , it would cause decoding failure .
To tackle this problem , motivated by " glue " rules ( Chiang 2005 ) , for some node S without any matched rules , we introduce a special pseudo-rule which reassembles all child nodes with local reordering to form new translation rules for S to complete decoding .
S S(x 1 :A x 2 :B x 3 :C x 4 :D ) ?x
1 x 2 x 3 x 4 S( x 1 :A x 2 :B x 3 :C x 4 :D ) ?x
2 x 1 x 3 x 4 S( x 1 :A x 2 :B x 3 :C x 4 :D ) ?x
1 x 3 x 2 x 4 A B C D S(x 1 :A x 2 :B x 3 :C x 4 :D ) ?x
1 x 2 x 4 x 3 ( a ) ( b) Figure 4 : ( a) An example unseen substree , and ( b ) its four pseudo-rules .
Figure 4 ( a ) depicts an example unseen substree where no any rules is matched at its root node S. Its simplest pseudo-rule is to simply combine a sequence of S's child nodes .
To give the model more options to build partial translations , we utilize a local reordering technique in which any two adjacent frontier ( child ) nodes are reordered during decoding .
Figure 4 ( b ) shows four pseudo-rules in total generated from this example unseen substree .
In the implementation , we add a new feature to the model : pseudo- rule counting feature .
This feature counts the number of pseudo-rules used to form the derivation .
The weight ?
pseudo of this feature is tuned via MERT with other feature weights .
Evaluation
Setup Our bilingual training data consists of 140K Chinese-English sentence pairs in the FBIS data set .
For rule extraction , the minimal GHKM rules ( Galley et al. 2004 ) were extracted from the bitext , and the composed rules were generated by combining two or three minimal GHKM rules .
A 5 - gram language model was trained on the target-side of the bilingual data and the Xinhua portion of English Gigaword corpus .
The beam size for beam search was set to 20 .
The base feature set used for all systems is similar to that used in ( Marcu et al. 2006 ) and MT05 ) .
Each small test set ( < =20 ) was built by removing the sentences with more than 20 words from the full set ( ALL ) .
+ and * indicate significantly better on performance comparison at p < .05 and p < .01 , respectively .
Table 1 depicts the BLEU scores of various methods on the Dev set and four test sets .
Compared to typical tree-to-string decoding ( the baseline ) , our method can achieve significant improvements on all datasets .
It is noteworthy that the improvement achieved by our approach on full test sets is bigger than that on small test sets .
For example , our method results in an improvement of 2.52 BLEU points over the baseline on the MT05 full test set , but only 0.55 points on the MT05 small test set .
As mentioned before , tree-to-string approaches are more vulnerable to parsing errors .
In practice , the Berkeley parser ( Petrov et al. 2006 ) we used yields unsatisfactory parsing performance on some long sentences in the full test sets .
In such a case , it would result in negative effects on the performance of the baseline method on the full test sets .
Experimental results show that our SDG approach can effectively alleviate this problem , and significantly improve tree -to-string translation .
Another issue we are interested in is the decoding speed of our method in practice .
To investigate this issue , we evaluate the average decoding speed of our SDG method and the baseline on the Dev set and all test sets .
Decoding Time ( seconds per sentence ) < =20 ALL Baseline 0.43 s 1.1s
This work 0.50s 1.3s Table 2 . Average decoding speed of various methods on small ( < =20 ) and full ( ALL ) datasets in terms of seconds per sentence .
The parsing time of each sentence is not included .
The decoders were implemented in C ++ codes on an X86 - based PC with two processors of 2.4GHZ and 4GB physical memory .
Table 2 shows that our approach only has little impact on decoding speed in practice , compared to the typical tree- to-string decoding ( baseline ) .
Notice that in these comparisons our method did not adopt any optimization techniques mentioned in Section 3.1 , e.g. , to limit the maximum number of similar rules matched at each node .
It is obviously that the use of such an optimization technique can effectively increase the decoding speed of our method , but might hurt the performance in practice .
Besides , to speed up decoding long sentences , it seems a feasible solution to first divide a long sentence into multiple short sub-sentences for decoding , e.g. , based on comma .
In other words , we can segment a complex source - language parse tree into multiple smaller subtrees for decoding , and combine the translations of these small subtrees to form the final translation .
This practical solution can speed up the decoding on long sentences in realworld MT applications , but might hurt the translation performance .
For convenience , here we call the rule ?
3 in Figure 2 ( b ) similar-rules .
It is worth investigating how many similar-rules and pseudo-rules are used to form the best derivations in our similarity - based scheme .
To do it , we count the number of similarrules and pseudo-rules used to form the best derivations when decoding on the MT05 full set .
Experimental results show that on average 13.97 % of rules used to form the best derivations are similarrules , and one pseudo-rule per sentence is used .
Roughly speaking , average five similar-rules per sentence are utilized for decoding generalization .
Related Work String - to- tree SMT approaches also utilize the similarity - based matching constraint on target side to generate target translation .
This paper applies it on source side to reconstruct new similar source parse trees for decoding at the decoding time , which aims to increase the tree-to-string search space for decoding , and improve decoding generalization for tree - to-string translation .
The most related work is the forest- based translation method Zhang et al. 2009 ) in which rule extraction and decoding are implemented over k-best parse trees ( e.g. , in the form of packed forest ) instead of one best tree as translation input .
Liu and Liu ( 2010 ) proposed a joint parsing and translation model by casting tree - based translation as parsing ( Eisner 2003 ) , in which the decoder does not respect the source tree .
These methods can increase the treeto-string search space .
However , the decoding time complexity of their methods is high , i.e. , more than ten or several dozen times slower than typical treeto-string decoding ( Liu and Liu 2010 ) .
Some previous efforts utilized the techniques of soft syntactic constraints to increase the search space in hierarchical phrase - based models ( Marton and Resnik 2008 ; Chiang et al.
2009 ; Huang et al. 2010 ) , string - to- tree models ( Venugopal et al. 2009 ) or tree-to- tree ( Chiang 2010 ) systems .
These methods focus on softening matching constraints on the root label of each rule regardless of its internal tree structure , and often generate many new syntactic categories 3 .
It makes them more difficult to satisfy syntactic constraints for the tree-to-string decoding .
Conclusion and Future Work
This paper addresses the parse error issue for treeto-string translation , and proposes a similaritybased decoding generation solution by reconstructing new similar source parse trees for decoding at the decoding time .
It is noteworthy that our SDG approach is very easy to implement .
In principle , forest - based and tree sequence - based approaches improve rule coverage by changing the rule extraction settings , and use exact tree - to-string matching constraints for decoding .
Since our SDG approach is independent of any rule extraction and pruning techniques , it is also applicable to forest - based approaches or other tree - based translation models , e.g. , in the case of casting tree - to - tree translation as tree parsing ( Eisner 2003 ) .
Figure 1 . 1 Figure 1 .
An example derivation performs translation over the Chinese parse tree T .
