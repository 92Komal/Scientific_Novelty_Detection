title
The IIT Bombay Hindiâ‡”English Translation System at WMT 2014
abstract
In this paper , we describe our English -Hindi and Hindi-English statistical systems submitted to the WMT14 shared task .
The core components of our translation systems are phrase based ( Hindi-English ) and factored ( English - Hindi ) SMT systems .
We show that the use of number , case and Tree Adjoining Grammar information as factors helps to improve English -Hindi translation , primarily by generating morphological inflections correctly .
We show improvements to the translation systems using pre-procesing and post-processing components .
To overcome the structural divergence between English and Hindi , we preorder the source side sentence to conform to the target language word order .
Since parallel corpus is limited , many words are not translated .
We translate out - of- vocabulary words and transliterate named entities in a post-processing stage .
We also investigate ranking of translations from multiple systems to select the best translation .
Introduction India is a multilingual country with Hindi being the most widely spoken language .
Hindi and English act as link languages across the country and languages of official communication for the Union Government .
Thus , the importance of English ?
Hindi translation is obvious .
Over the last decade , several rule based ( Sinha , 1995 ) , interlingua based ( Dave et. al. , 2001 ) and statistical methods ( Ramanathan et. al. , 2008 ) have been explored for English -Hindi translation .
In the WMT 2014 shared task , we undertake the challenge of improving translation between the English and Hindi language pair using Statistical Machine Translation ( SMT ) techniques .
The WMT 2014 shared task has provided a standardized test set to evaluate multiple approaches and avails the largest publicly downloadable English -Hindi parallel corpus .
Using these resources , we have developed a phrase- based and a factored based system for Hindi-English and English -Hindi translation respectively , with pre-processing and post-processing components to handle structural divergence and morphlogical richness of Hindi .
Section 2 describes the issues in Hindi ?
English translation .
The rest of the paper is organized as follows .
Section 3 describes corpus preparation and experimental setup .
Section 4 and Section 5 describe our English -Hindi and Hindi-English translation systems respectively .
Section 6 describes the postprocessing operations on the output from the core translation system for handling OOV and named entities , and for reranking outputs from multiple systems .
Section 7 mentions the details regarding our systems submitted to WMT shared task .
Section 8 concludes the paper .
Problems in Hindi?English Translation Languages can be differentiated in terms of structural divergences and morphological manifestations .
English is structurally classified as a Subject- Verb-Object ( SVO ) language with a poor morphology whereas Hindi is a morphologically rich , Subject - Object - Verb ( SOV ) language .
Largely , these divergences are responsible for the difficulties in translation using a phrase based / factored model , which we summarize in this section .
English-to- Hindi
The fundamental structural differences described earlier result in large distance verb and modifier movements across English -Hindi .
Local reordering models prove to be inadequate to over- come the problem ; hence , we transformed the source side sentence using pre-ordering rules to conform to the target word order .
Availability of robust parsers for English makes this approach for English -Hindi translation effective .
As far as morphology is concerned , Hindi is more richer in terms of case-markers , inflectionrich surface forms including verb forms etc .
Hindi exhibits gender agreement and syncretism in inflections , which are not observed in English .
We attempt to enrich the source side English corpus with linguistic factors in order to overcome the morphological disparity .
Hindi-to-English
The lack of accurate linguistic parsers makes it difficult to overcome the structural divergence using preordering rules .
In order to preorder Hindi sentences , we build rules using shallow parsing information .
The source side reordering helps to reduce the decoder 's search complexity and learn better phrase tables .
Some of the other challenges in generation of English output are : ( 1 ) generation of articles , which Hindi lacks , ( 2 ) heavy overloading of English prepositions , making it difficult to predict them .
Experimental Setup
We process the corpus through appropriate filters for normalization and then create a train-test split .
English Corpus Normalization
To begin with , the English data was tokenized using the Stanford tokenizer and then true-cased using truecase .
perl provided in MOSES toolkit .
Hindi Corpus Normalization For Hindi data , we first normalize the corpus using NLP Indic Library ( Kunchukuttan et. al. , 2014 ) 1 . Normalization is followed by tokenization , wherein we make use of the trivtokenizer.pl 2 provided with WMT14 shared task .
In Table 1 , we highlight some of the post normalization statistics for en-hi parallel corpora .
English
Data Split Before splitting the data , we first randomize the parallel corpus .
We filter out English sentences longer than 50 words along with their parallel Hindi translations .
After filtering , we select 5000 sentences which are 10 to 20 words long as the test data , while remaining 284,832 sentences are used for training .
4 English-to-Hindi ( en-hi ) translation
We use the MOSES toolkit ( Koehn et. al. , 2007a ) for carrying out various experiments .
Starting with Phrase Based Statistical Machine Translation ( PB - SMT ) ( Koehn et. al. , 2003 ) as baseline system we go ahead with pre-order PBSMT described in Section 4.1 .
After pre-ordering , we train a Factor Based SMT ( Koehn , 2007 b ) model , where we add factors on the pre-ordered source corpus .
In Factor Based SMT we have two variations -( a ) using Supertag as factor described in Section 4.2 and ( b ) using number , case as factors described in Section 4.3 .
Pre-ordering source corpus Research has shown that pre-ordering source language to conform to target language word order significantly improves translation quality ( Collins et. al , 2005 ) .
There are many variations of preordering systems primarily emerging from either rule based or statistical methods .
We use rule based pre-ordering approach developed by ( Patel et. al. , 2013 ) , which uses the Stanford parser for parsing English sentences .
This approach is an extension to an earlier approach developed by ( Ramanathan et. al. , 2008 ) .
The existing source reordering system requires the input text to contain only surface form , however , we extended it to support surface form along with its factors like POS , lemma etc . .
An example of improvement in translation after preordering is shown below : Example : trying to replace bad ideas with good ideas .
Phr : replace ? ? a ? ( replace bure vichaaron ko acche vichaaron ke saath )
Gloss : replace bad ideas good ideas with Pre-order PBSMT : a ? ? ? ? ( acche vichaaron se bure vichaaron ko badalane ki koshish kara rahe hain )
Gloss : good ideas with bad ideas to replace trying
Supertag as Factor
The notion of Supertag was first proposed by Joshi and Srinivas ( 1994 ) .
Supertags are elementary trees of Lexicalized Tree Adjoining Grammar ( LTAG ) ( Joshi and Schabes , 1991 ) .
They provide syntactic as well as dependency information at the word level by imposing complex constraints in a local context .
These elementary trees are combined in some manner to form a parse tree , due to which , supertagging is also known as " An approach to almost parsing " ( Bangalore and Joshi , 1999 ) .
A supertag can also be viewed as fragments of parse trees associated with each lexical item .
Figure 1 shows an example of supertagged sentence " The purchase price includes taxes " described in ( Hassan et. al. , 2007 ) .
It clearly shows the sub-categorization information available in the verb include , which takes subject NP to its left and an object NP to its right .
Use of supertags as factors has already been studied by Hassan ( 2007 ) in context of Arabic- English SMT .
They use supertag language model along with supertagged English corpus .
Ours is the first study in using supertag as factor for English -to - Hindi translation on a pre-ordered source corpus .
We use MICA Parser ( Bangalore et. al. , 2009 ) for obtaining supertags .
After supertagging we run pre-ordering system preserving the supertags in it .
For translation , we create mapping from source-word | supertag to target-word .
An example of improvement in translation by using supertag as factor is shown below :
Example : trying to understand what your child is saying to you Phr : a ? a ( aapkaa bacchaa aapse kya kaha rahaa hai yaha )
Gloss : your child you what saying is this Supertag Fact : a ? a , u ( aapkaa bacchaa aapse kya kaha rahaa hai , use samajhane kii koshish karnaa )
Gloss : your child to you what saying is , that understand try
Number , Case as Factor
In this section , we discuss how to generate correct noun inflections while translating from English to Hindi .
There has been previous work done in order to solve the problem of data sparsity due to complex verb morphology for English to Hindi translation ( Gandhe , 2011 ) .
Noun inflections in Hindi are affected by the number and case of the noun only .
Number can be singular or plural , whereas , case can be direct or oblique .
We use the factored SMT model to incorporate this linguistic information during training of the translation models .
We attach root-word , number and case as factors to English nouns .
On the other hand , to Hindi nouns we attach root-word and suffix as factors .
We define the translation and generation step as follows : ?
With the help of syntactic and morphological tools , we extract the number and case of the English nouns as follows : ?
Number factor :
We use Stanford POS tagger 3 to identify the English noun entities ( Toutanova , 2003 ) .
The POS tagger itself differentiates between singular and plural nouns by using different tags .
?
Case factor :
It is difficult to find the direct / oblique case of the nouns as English nouns do not contain this information .
Hence , to get the case information , we need to find out features of an English sentence that correspond to direct / oblique case of the parallel nouns in Hindi sentence .
We use object of preposition , subject , direct object , tense as our features .
These features are extracted using semantic relations provided by Stanford 's typed dependencies ( Marneffe , 2008 ) .
Results Listed below are different statistical systems trained using Moses : ? Phrase Based model ( Phr ) ?
Phrase Based model with pre-ordered source corpus ( PhrReord ) ? Factor Based Model with factors on preordered source corpus - Supertag as factor ( PhrReord + STag ) - Number , Case as factor ( PhrReord + NC )
We evaluated translation systems with BLEU and TER as shown in Table 2 . Evaluation on the development set shows that factor based models achieve competitive scores as compared to the baseline system , whereas , evaluation on the WMT14 test set shows significant improvement in the performance of factor based models .
5 Hindi-to-English ( hi-en ) translation As English follows SVO word order and Hindi follows SOV word order , simple distortion penalty in phrase - based models can not handle the reordering well .
For the shared task , we follow the approach that pre-orders the source sentence to conform to target word order .
A substantial volume of work has been done in the field of source -side reordering for machine translation .
Most of the experiments are based on applying reordering rules at the nodes of the parse tree of the source sentence .
These reordering rules can be automatically learnt ( Genzel , 2010 ) .
But , many source languages do not have a good robust parser .
Hence , instead we can use shallow parsing techniques to get chunks of words and then reorder them .
Reordering rules can be learned automatically from chunked data ( Zhang , 2007 ) .
Hindi does not have a functional constituency or dependency parser available , as of now .
But , a shallow parser 4 is available for Hindi .
Hence , we follow a chunk - based pre-ordering approach , wherein , we develop a set of rules to reorder the chunks in a source sentence .
The following are the chunks tags generated by this shallow parser : Noun chunks ( NP ) , Verb chunks ( VGF , VGNF , VGNN ) , Adjectival chunks ( JJP ) , Adverb chunks ( RBP ) , Negatives ( NEGP ) , Conjuncts ( CCP ) , Chunk fragments ( FRAGP ) , and miscellaneous entities ( BLK ) ( Bharati , 2006 ) .
Development of rules
After chunking an input sentence , we apply handcrafted reordering rules on these chunks .
Following sections describe these rules .
Note that we apply rules in the same order they are listed below .
Merging of chunks
After chunking , we merge the adjacent chunks , if they follow same order in target language .
1 . Merge { JJP VGF } chunks ( Consider this chunk as a single VGF chunk ) e.g. , ( varnit hai ) , ( sthit hai ) 2 . Merge adjacent verb chunks ( Consider this chunk as a single verb chunk ) e.g. , ( girataa hai ) , ( lubhaataa hai ) 3 . Merge NP and JJP chunks separated by commas and CCP ( Consider this chunk as a single NP chunk ) e.g. , ? a a ( badaa aur aham )
Preposition chunk reordering Next we find sequence of contiguous chunks separated by prepositions ( Can end in verb chunks ) .
We apply following reordering rules on these contiguous chunks :
1 . Reorder multi-word preposition locally by reversing the order of words in that chunk e.g. , a ( ke alaawaa ) ? a , ( ke saamane ) ?
2 . Reorder contiguous preposition chunk by reversing the order of chunks ( Consider this chunk as a single noun chunk ) e.g. , ? ? ( hinduu dharma me tirtha ka badaa mahatva ) ? ? ?
Verb chunk reordering
We find contiguous verb chunks and apply followreordering rules : 1 . Reorder chunks locally by reversing the order of the chunks e.g. , ( varnit hai ) ?
2 . Verb chunk placement :
We place the new verb chunk after first NP chunk .
Same rule applies for all verb chunks in a sentence , i.e. , we place each verb chunk after first NP chunk of the clause to which the verb belongs .
Note that , even though placing verb chunk after first NP chunk may be wrong reordering .
But we also use distortion window of 6 to 20 while using phrase - based model .
Hence , further reordering of verb chunks can be somewhat handled by phrase - based model itself .
Thus , using chunker and reordering rules , we get a source-reordered Hindi sentence .
Results
We trained two different translation models : ?
Phrase - based model without source reordering ( Phr )
Post processing All experimental results reported in this paper are after post processing the translation output .
In post processing , we remove some Out-of-Vocabulary ( OOV ) words as described in subsection 6.1 , after which we transliterate the remaining OOV words .
Removing OOV
We noticed , there are many words in the training corpus which were not present in the phrase table , but , were present in the lexical tranlsation table .
So we used the lexical table as a dictionary to lookup bilingual translations .
Transliteration of Untranslated Words OOV words which were not present in the lexical translation table were then transliterated using a naive transliteration system .
The transliteration step was applied on Hindi-to - English translation outputs only .
After transliteration we noticed fractional improvements in BLEU score varying from 0.1 to 0.5 .
Ranking of Ensemble MT Output
We propose a ranking framework to select the best translation output from an ensemble of multiple MT systems .
In order to exploit the strength of each system , we augment the translation pipeline with a ranking module as a post processing step .
For English- to - Hindi ranking we combine the output of both factor based models , whereas , for Hindi-to - English ranking we combine phrase based and phrase based with pre-ordering outputs .
For most of the systems , the output translations are adequate but not fluent enough .
So , based on their fluency scores , we decided to rank the candidate translations .
Fluency is well quantified by LM log probability score and Perplexity .
For a given translation , we compute these scores by querying the 5 - gram language model built using SRILM .
Table 5 shows more than 4 % relative improvement in BLEU score for en-hi as well as hi-en translation system after applying ranking module .
For English - to -Hindi , we submitted the ranked output of factored models trained on pre-ordered source corpus .
For Hindi-to - English , we submitted the ranked output of phrase based and preordered phrase based models .
Table 6 shows evaluation scores of these systems on WMT14 test set .
Model Lang. pair BLEU TER en-hi 10.4 0.83 hi-en 14.5 0.89
Table 6 : WMT14 evaluation for en-hi and hi-en .
Conclusion
We conclude that the difficulties in English - Hindi MT can be tackled by the use of factor based SMT and various pre-processing and post processing techniques .
Following are our primary contributions towards English -Hindi machine translation : ?
Use of supertag factors for better translation of structurally complex sentences ?
Use of number-case factors for accurately generating noun inflections in Hindi ?
Use of shallow parsing for pre-ordering Hindi source corpus
We also observed that simple ranking strategy benefits in getting the best translation from an ensemble of translation systems .
Figure 1 : 1 Figure 1 : LTAG supertag sequence obtained using Parser .
