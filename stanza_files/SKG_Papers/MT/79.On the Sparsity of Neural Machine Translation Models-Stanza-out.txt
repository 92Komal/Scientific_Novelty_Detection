title
On the Sparsity of Neural Machine Translation Models
abstract
Modern neural machine translation ( NMT ) models employ a large number of parameters , which leads to serious over-parameterization and typically causes the underutilization of computational resources .
In response to this problem , we empirically investigate whether the redundant parameters can be reused to achieve better performance .
Experiments and analyses are systematically conducted on different datasets and NMT architectures .
We show that : 1 ) the pruned parameters can be rejuvenated to improve the baseline model by up to + 0.8 BLEU points ; 2 ) the rejuvenated parameters are reallocated to enhance the ability of modeling low-level lexical information .
Introduction Modern neural machine translation ( NMT ) ( Bahdanau et al. , 2015 ; Gehring et al. , 2017 ; Vaswani et al. , 2017 ) models employ sufficient capacity to fit the massive data well by utilizing a large number of parameters , and suffer from the widely recognized issue , namely , over- parameterization .
For example , See et al . ( 2016 ) showed that over 40 % of the parameters in an RNN - based NMT model can be pruned with negligible performance loss .
However , the low utilization efficiency of parameters results in a waste of computational resources ( Qiao et al. , 2019 ) , as well as renders the model stuck in a local optimum ( Han et al. , 2017 ; Yu et al. , 2019 ) .
In response to the over-parameterization issue , network pruning has been widely investigated for both computer vision ( CV ) ( Han et al. , 2016 ; Luo et al. , 2017 ) and natural language processing ( NLP ) tasks ( See et al. , 2016 ; Lan et al. , 2020 ) .
Recent work has proven that such spare parameters can be reused to maximize the utilization of models in CV tasks such as image classification ( Han et al. , 2017 ; * Work was done when interning at Tencent AI Lab .
Qiao et al. , 2019 ) .
The leverage of parameter rejuvenation in sequence - to-sequence learning , however , has received relatively little attention from the research community .
In this paper , we empirically study the efficiency issue for NMT models .
Specifically , we first investigate the effects of weight pruning on advanced Transformer models , showing that 20 % parameters can be directly pruned , and by continuously training the sparse networks , we can prune 50 % with no performance loss .
Starting from this observation , we then exploit whether these redundant parameters are able to be re-utilized for improving the performance of NMT models .
Experiments are systematically conducted on different datasets ( i.e. Zh?En , De?En and En?Fr ) and NMT architectures ( i.e. Transformer , RNNSearch and LightConv ) .
Results demonstrate that the rejuvenation approach can significantly and consistently improve the translation quality by up to + 0.8 BLEU points .
Further analyses reveal that the rejuvenated parameters are reallocated to enhance the ability to model the source-side low-level information , lacking of which leads to a number of problems in NMT models ( Tu et al. , 2016 ; Dou et al. , 2018 ; Emelin et al. , 2019 ) .
Contributions
Our key contributions are : ?
We try early attempts to empirically investigate parameter rejuvenation for NMT models across different datasets and architectures . ?
We explore to interpret where the gains come from in two perspectives : learning dynamics and linguistic insights .
Approach A standard NMT model directly optimizes the conditional probability of a target sentence y = y 1 , . . . , y J given its corresponding source sentence x = x 1 , . . . , x I , namely P ( y|x ; ? ) = Pruning
The redundant parameters in neural networks can be pruned according to a certain criterion while the left ones are significant to preserve the accuracy of the model .
Specifically , we mask weight connections with low magnitudes in the forward pass and these weights are not updated during optimization .
Given the weight matrix W with N parameters , we rank the parameters according to their absolute values .
Supposed that the pruning ratio is ? ( i.e. ? % of parameters should be pruned ) , we keep the top n parameters ( n = N ? ( 1 ? ? ) ) , and remove the others with a binary mask matrix , which is the same size of W .
We denote the pruned parameters as ?
p , subject to ? p ? ?.
There are two pruning strategies ( Liu et al. , 2019 ) : 1 ) local pruning , which prunes ? % of parameters in each layer ; and 2 ) global pruning , which compares the importance of parameters across layers .
Following See et al. ( 2016 ) , we retrain the pruned networks after the pruning phase .
Specifically , we continue to train the remaining parameters , but maintain the sparse structure , that is we optimize P ( y|x ; ? ) with the constraint : a = 0 , ?a ? ? p . Rejuvenation
After the pruning and retraining phases , we aim to restore the model capacity by rejuvenating the pruned parameters .
This is a common method in optimization to avoid useless computations and further improve performances ( Han
Pruning Rejuvenation BLEU local global zero external ? ? 28.12 ? ? 28.08 ? ? 28.12 ? ? 28.14 Table 1 : Effects of different strategies on Transformer on WMT14 En?De. " zero " denotes using zero as initialization , while " external " denotes using corresponding parameters in the baseline model as initialization .
Qiao et al. , 2019 ) .
Thus , we release the sparsity constraint ( a = 0 , ?a ? ? p ) , which inversely recovers the pruned connections , and redense the whole networks .
The recovered weight connections are then initialized by some strategy ( e.g. zero or external ) .
The entire networks are retrained with one order of magnitude lower learning rate since the sparse network is already at a good local optimum .
As seen , the rejuvenation method contains three phases : 1 ) training a baseline model ( BASE ) ; 2 ) pruning ? % parameters and then retraining remaining ones ( PruTrain ) ; 3 ) restoring pruned parameters and training entire networks ( RejTrain ) .
3 Experiments
Setup Data
We conduct experiments on English ?
German ( En? De ) , Chinese ?
English ( Zh?En ) , German ?
English ( De?En ) and English ?
French ( En? Fr ) translation tasks .
For En?De task , we use WMT14 corpus which contains 4 million sentence pairs .
The Zh?En task is conducted on WMT17 corpus , consisting of 21 million sentence pairs .
We follow Dou et al . ( 2018 ) to select the development and test sets .
Furthermore , we evaluate low-resourced translation on IWSLT14 De?En and IWSLT17 En?
Fr corpora .
We preprocess our data using byte-pair encoding ( Sennrich et al. , 2016 ) with 40 K merge operations for En? De , 32 K for Zh?En , and 10 K for De?En and En?Fr , and keep all tokens in the vocabulary .
We use 4 - gram BLEU score ( Papineni et al. , 2002 ) as the evaluation metric and sign-test ( Koehn , 2004 ) for statistical significance .
Models
We implement our approach on top of three popular architectures , namely Transformer ( Vaswani et al. , 2017 ) , RNNSearch ( Luong et al. , 2015 ) and LightConv ( Wu et al. , 2019 ) the open-source toolkit - fairseq ( Ott et al. , 2019 ) . For Transformer , we investigate big , base and small settings .
About RNNSearch and LightConv , we employ corresponding configurations in fairseq .
The implementation is detailed in Appendix ?A.1 .
All baseline models are trained for 100K updates using Adam optimizer ( Kingma and Ba , 2015 ) .
Based on the baselines , the proposed pruning and rejuvenation methods are trained with additional 100K updates ( i.e. 50 K for each one ) .
To rule out the circumstance that more training steps may bring improvements , we also conduct continuous training ( ConTrain ) as strong baselines and they employ the same training steps as our approach .
Results of Pruning
To study the effect of sparsity , we investigate the effects of different pruning ratios on Transformer base models .
Experiments are conducted on WMT14 En?De and WMT17 Zh ?
En tasks .
As shown in Figure 1 , over 20 % of parameters can be directly pruned without degrading the translation performance .
When adding a simple continuous training phase after pruning , we are able to prune 50 % with no performance loss .
Compared with findings in See et al . ( 2016 ) , Transformer is less over-parameterized than RNN - based NMT models ( 20 % vs. 40 % and 50 % vs. 80 % ) .
This provides the evidence that different NMT models are overparameterized to a different extent .
Accordingly , we set the pruning threshold of 50 % as a default in the following experiments ( i.e. Tables 1?4 ) .
Results of Rejuvenation Ablation Study
As shown in strategies on the translation task .
As seen , the local pruning strategy performs better than the global one , especially with the rejuvenation counterpart ( 28.12 vs. 28.08 BLEU ) .
However , See et al. ( 2016 ) found that the global pruning outperforms the local one without considering rejuvenation factors .
Regarding the rejuvenation strategy , zero and external initialization perform similarly in terms of BLEU score .
Therefore , we use local pruning and zero initialization strategies for the rest of the experiments ( i.e. Tables 2?4 ) .
Main Results
We evaluate the rejuvenation approach on the Transformer using En? De dataset .
As shown in Table 2 ( Rows 1?4 ) , our model ( Re- jTrain ) outperforms the baseline model and continuous training method ( ConTrain ) by + 0.58 and + 0.38 BLEU points , respectively .
In addition , iterative rejuvenation can incrementally improve the baseline model up to 28.33 BLEU points ( + 0.79 and + 0.59 over BASE and ConTrain ) .
The results clearly demonstrate the effectiveness of rejuvenating redundant parameters for NMT models .
To verify the robustness , we evaluate different model sizes .
As shown in Table 2 ( Rows 5?7 ) , the Transformer BIG model performs better than the base with an increase of 196.7 M parameters .
Surprisingly , the performance can be further improved by + 0.57 BLEU points by our method .
As seen , the continuous training can only slightly gain + 0.2 BLEU over BIG , and RejTrain outperforms the strong baseline .
This confirms that the rejuvenation method can consistently improve NMT models by alleviating the over-parameterization issue .
Different Datasets
Analysis
To better understand the effectiveness of the proposed method , the analyses are carried out in two Adequacy and Fluency Table 5 shows an example randomly selected from the test set in the Zh?En task .
As seen , incorporating the rejuvenation approach into NMT can generate more fluent translation with higher adequacy .
For instance , the Chinese word " ? " is under-translated by the baseline model , while the RejTrain model can correctly translate it into " olympics " .
Besides , the nominal modifier " 21 ? " is mistranslated into a simple number by the baseline while RejTrain can fix the error .
This confirms that the rejuvenation improves the adequacy of translation by enhancing the ability to understand the lexical information .
To better evaluate the fluency of our models , we calculate the perplexity on the WMT14 En - De test set .
As shown in may harm the fluency of translation outputs ( 5.25 vs. 5.14 ) .
This demonstrates that our rejuvenation approach improves the fluency of translation .
Conclusion
In this paper , we prove that existing NMT systems are over-parameterized and propose to improve the utilization efficiency of parameters in NMT models by introducing a rejuvenation approach .
Empirical results on a variety of language pairs and architectures demonstrate the effectiveness and universality of the presented method .
We also analyze the gains from perspectives of learning dynamics and linguistic probing , which give insightful research directions for future work .
Future directions include continuing the exploration of this research topic for large sequenceto-sequence pre-training models ( Liu et al. , 2020 ) and multi-domain translation models ( Wang et al. , 2019 b ) .
We will employ recent analysis methods to better understand the behaviors of rejuvenated models ( He et al. , 2019 ; Yang et al. , 2020 ) . Figure 1 : 1 Figure 1 : Effects of different pruning ratios on Transformer .
" Prune " denotes directly pruning parameters while " PruTrain " indicates adding continuous training after the pruning phase .
