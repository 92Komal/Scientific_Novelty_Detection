title
Lexicalized Reordering for Left-to- Right Hierarchical Phrase- based Translation
abstract
Phrase - based and hierarchical phrasebased ( Hiero ) translation models differ radically in the way reordering is modeled .
Lexicalized reordering models play an important role in phrase - based MT and such models have been added to CKY - based decoders for Hiero .
Watanabe et al. ( 2006 ) propose a promising decoding algorithm for Hiero ( LR - Hiero ) that visits input spans in arbitrary order and produces the translation in left to right ( LR ) order which leads to far fewer language model calls and leads to a considerable speedup in decoding .
We introduce a novel shift-reduce algorithm to LR - Hiero to decode with our lexicalized reordering model ( LRM ) and show that it improves translation quality for Czech -English , Chinese -English and German- English .
Introduction
Phrase - based machine translation handles reordering between source and target languages by visiting phrases in the source in arbitrary order while generating the target from left to right .
A distortion penalty is used to penalize deviation from the monotone translation ( no reordering ) ( Koehn et al. , 2003 ; Och and Ney , 2004 ) .
Identical distortion penalties for different types of phrases ignore the fact that certain phrases ( with certain words ) were more likely to reorder than others .
State - of- the- art phrase based translation systems address this issue by applying a lexicalized reordering model ( LRM ) ( Tillmann , 2004 ; Koehn et al. , 2007 ; Galley and Manning , 2008 ; Galley and Manning , 2010 ) which uses word aligned data to score phrase pair reordering .
These models distinguish three orientations with respect to the previously translated phrase : monotone ( M ) , swap ( S ) , and discontinuous ( D ) , which are primarily designed to handle local re-orderings of neighbouring phrases .
Hierarchical phrase - based translation ( Hiero ) ( Chiang , 2007 ) uses hierarchical phrases for translations represented as lexicalized synchronous context-free grammar ( SCFG ) .
Non-terminals in the SCFG rules correspond to gaps in phrases which are recursively filled by other rules ( phrases ) .
The SCFG rules are extracted from word and phrase alignments of a bitext .
Hiero uses CKY - style decoding which parses the source sentence with time complexity O(n 3 ) and synchronously generates the target sentence ( translation ) .
Watanabe et al. ( 2006 ) proposed a left-to- right ( LR ) decoding algorithm for Hiero ( LR - Hiero ) which follows the Earley ( Earley , 1970 ) algorithm to parse the source sentence and synchronously generate the translation in a left-to- right manner .
This algorithm is combined with beam search and has time complexity O( n 2 b ) where n is the length of source sentence and b is the size of beam ( Huang and Mi , 2010 ) . LR - Hiero constrains the SCFG rules to be prefix-lexicalized on the target side aka Greibach Normal Form ( GNF ) .
Throughout this paper we abuse the notation for simplicity and use the term GNF grammars for such SCFGs .
This leads to a single language model ( LM ) history for each hypothesis and speeds up decoding significantly , up to four times faster ( Siahbani et al. , 2013 ) .
The Hiero translation model handles reordering very differently from a phrase - based model , through weighted translation rules ( SCFGs ) determined by non-terminal mappings .
The rule X ? ne X 1 pas , do not X 1 indicates the translation of the phrase between ne and pas will be after the English phrase do not .
However , reordering features can also be added to the Hiero log-linear translation model .
Siahbani et al. ( 2013 ) introduce a new distortion feature to Hiero and LR - Hiero which significantly improves translation quality in LR - Hiero and improves Hiero results to a lesser extent .
Nguyen and Vogel ( 2013 ) integrate phrase - based distortion and lexicalized reordering features with CKY - based Hiero decoder which significantly improve the translation quality .
In their approach , each partial hypothesis during decoding is mapped into a sequence of phrase - pairs then the distortion and reordering features are computed similar to phrase - based MT .
They use a LRM trained for phrase - based MT ( Galley and Manning , 2010 ) which applies some restrictions on the Hiero rules .
( Cao et al. , 2014 ; Huck et al. , 2013 ) propose different approaches to directly train LRM for Hiero rules .
However , these approaches are designed for CKY - decoding and cannot be directly used or adapted for LR - Hiero decoding which uses an Earley - style parsing algorithm .
The crucial difference is the nature of bottom - up versus left to right decisions for lexicalized reordering and generating the translation in left-to- right manner .
In this paper , we introduce a novel shift-reduce algorithm to learn a lexicalized reordering model ( LRM ) for LR - Hiero .
We show that augmenting LR - Hiero with an LRM improves translation quality for Czech -English , significantly improves results for Chinese -English and German- English , while performing three times fewer language model queries on average , compared to CKY - Hiero .
1 ) X ? ? ? ? , X 1 / He added that X 1 ? 2 ) X ? ? ? X 1 / the coalition government X 1 ? 3 ) X ? ? X 1 ? X 2 / is now i n stable X 1 X 2 ? 4 ) X ? ? / condition ? 5 ) X ? ./.? ? , ?[ 0,10 ] ? , 0 ? ? He
Lexicalized Reordering for LR - Hiero
The main idea in phrase - based LRM is to divide possible reorderings into three orientations that can be easily determined during decoding and also from word-aligned sentence pairs ( parallel corpus ) .
Given a source sentence f , a sequence of target language phrases e = ( ?1 , . . . , ?n ) is generated by the decoder .
A phrase alignment a= ( a 1 , . . . a n ) defines a source phrase fa i for each target phrase ? i .
For each phrase - pair fa i , e i , the orientations are described in terms of the previously translated source phrase fa i?1 : Monotone ( M ) : fa i immediately follows fa i?1 . Swap ( S ) : fa i?1 immediately follows fa i .
Discontinuous ( D ) : fa i and fa i?1 are not adjacent in the source sentence .
We only define the left- to - right case here ; the right - to - left case ( fa i + 1 ) is symmetrical .
The probability of an orientation given a phrase pair f , ? can be estimated using relative frequency : P ( o| f , ? ) = cnt ( o , f , ? ) o ?{ M, S , D} cnt ( o , f , ? ) ( 1 ) where , o ?
{ M , S , D} and cnt is computed on word-aligned parallel data ( count phrase - pairs and their orientations ) .
Given the sparsity of the orientation types , we use smoothing .
As the decoder develops a new hypothesis by translating a source phrase , fa i , it scores the orientation , o i wrt a i?1 .
The log probability of the orientation is added as a feature function to the log-linear translation model .
LR - Hiero uses a subset of the Hiero SCFG rules where the target rules are in Greibach Normal Form ( GNF ) : ? , ? ? where ? is a string of nonterminal and source words , ? is a target phrase and ? is a possibly empty sequence of non-terminals .
We abuse notation slightly and call this a GNF SCFG grammar .
In LR - Hiero each hypothesis consists of a translation prefix , h t , an ordered sequence of untranslated spans on the source sen-tence , h s and a numeric cost , h c .
The initial hypothesis consists of an empty translation ( s ) , a span of the whole source sentence and cost 0 ( Figure 1 ) .
To develop a new hypothesis from a current hypothesis , the LR - Hiero decoder applies a GNF rule to the first untranslated span , h s [ 0 ] , of old hypothesis .
The translation prefix of the new hypothesis is generated by appending the target side of the applied rule , ? , to the translation prefix of the old hypothesis , h t .
Corresponding to the applied rule , the uncovered spans of the old hypothesis are also updated and assigned to the new hypothesis ( Figure 1 ) .
Target generation in LR - Hiero is analogous to phrase - based MT .
Given an input sentence f , the output translation is a sequence of contiguous target - language phrases e = ( ?1 , . . . , ?n ) incrementally concatenated during decoding .
We can define a phrase alignment a = ( a 1 , . . . a n ) which align each target phrase , ? i to a source phrase f a i corresponding to source side of a rule , r i used at step i .
But unlike target , source phrases can be discontiguous .
Figure 1 illustrates the process of translating a Chinese-English sentence pair by LR - Hiero .
Corresponding to each rule a phrase pair can be created ( shown in Figure 2 ) .
The final translation is the ordered sequence of target side of these phrase pairs .
Although the target generation is similar to phrase - based MT , the LR - Hiero decoder parse the source sentence using the SCFG rules and the order for translating source spans is determined by the grammar .
However the LR - Hiero decoder uses an Earley - style parsing algorithm and unlike CKY does not utilise translated smaller spans to generate translations for bigger spans bottom - up .
Training
We compute P ( o| f , ? ) , which is the probability of an orientation given phrase pair of a rule , r.p = f , ? , on word-aligned data using relative frequency .
We assume that phrase ? spans the word range s . . . t in the target sentence and the phrase f spans the range u . . . v in the source sentence .
For a given phrase pair f , ? , we set o = M if there is a phrase pair , f , ? , where its target side , ? , appears just before the target side of the given phrase , ? , or s = t + 1 , and its source side , f , also appears just before f , or u = v +
1 .
Orientation is S if there is a phrase pair , f , ? , where ?
appears just before ? , or s = t + 1 , and f appears just after f , or v = u ?1 .
Otherwise orientation is D .
We consider phrase pairs of any length to compute orientation .
Note that although phrase pairs extracted from the rules that can be discontinuous ( on source ) , just continuous source phrases in each sentence pair are used to compute orientation ( previously translated phrases ) .
Once orientation counts for rules ( phrase - pairs obtained form rules ) are collected from the bitext , the probability model P ( o| f , ? ) is estimated using recursive MAP smoothing as discussed in ( Cherry , 2013 ) .
Decoding Phrase - based LRM uses local information to determine orientation for a new phrase pair , fa i , ? i , during decoding ( Koehn et al. , 2007 ; Tillmann , 2004 ) .
For left - to - right order , fa i is compared to the previously translated phrase fa i? 1 . Galley and Manning ( 2008 ) introduce the hierarchical phrase reordering model ( HRM ) which increases the consistency of orientation assignments .
In HRM , the emphasis on the previously translated phrase is removed and instead a compact representation of the full translation history , as represent by a shiftreduce stack , is used .
Once a source span is translated , it is shifted onto the stack ; if the two spans on the top are adjacent , then a reduction merges the two .
During decoding , orientations are always determined with respect to the top of this stack , rather than the previously translated phrase .
Although we reduce rules to phrase pairs to train the reordering model , LR - Hiero decoder uses SCFG rules for translation and the order of source phrases ( spans ) are determined by the non-terminals in SCFG rules .
Therefore we cannot simply rely on the previously translated phrase to compute the orientation and reordering scores .
Since LR - Hiero uses lexicalized glue rules ( Watanabe et al. , 2006 ) , non-terminals can be matched to very long spans on the source sentence .
It makes LRM in LR - Hiero comparable to HRM in phrase - based MT .
However , we cannot rely on the full translation history like HRM , since translation model is a SCFG grammar encoding reordering information .
We employ a shift-reduce approach to find a compact representation of the recent translated source spans which is also represented by a stack , S , for each hypothesis .
However , S always contains just one source span ( which might be discontiguous ) , unlike HRM which maintains all previously translated solid spans ( In Figure 4 , the dotted lines shows the only span in the stack during LR - Hiero decoding ) .
As the decoder applies a rule , r i , the corresponding source phrase r i . f is compared respect to the span in S to determine the orientation .
If they are adjacent or S covers the span r i .
f , they are reduced .
Otherwise stack is set to the span of new rule , S = r i .
f .
The orientation of r i .
f is computed with respect to S but if they are not adjacent ( M or S ) , we still need to consider the possible local reordering with respect to the previous rule r i?1 . f .
In Figure 3 , rules # 5 , # 4 are monotone , while both are covered by the current span in S. Since the stack always contains one span , this algorithm runs in O ( 1 ) .
Therefore , only a limited number of comparisons is used to update S and compute orientation .
Unlike HRM which needs to maintain a sequence of contiguous spans in the stack and runs in linear time .
Figure 3 illustrates the application of shiftreduce approach to compute orientation for initial decoding steps of a Chinese -English sentence pair shown in Figure 4 .
We show source words in the rules with the corresponding index in the source sentence .
S and r i . f for the initial hypothesis are set to ?1 , corresponding to the start of sentence symbol , making it easy to compute the correct orientation for spans at the beginning of the input ( with index 0 ) .
Experiments
We evaluate lexicalized reordering model for LR - Hiero on three language pairs : German-English ( De-En ) , Czech-English ( Cs- En ) and Chinese-English ( Zh-En ) .
Table 1 shows the corpus statistics for all language .
We train a 5 - gram LM on the Gigaword corpus using KenLM ( Heafield , 2011 ) .
The weights in the log-linear model are tuned by minimizing BLEU loss through MERT ( Och , 2003 ) on the dev set for each language pair and then report BLEU scores on the test set .
Pop limit for Hiero and LR - Hiero is 500 and beam size for Moses is 1000 .
Other extraction and decoder settings such as maximum phrase length , etc. are identical across different settings .
We use 3 baselines in our experiments : ?
Hiero : we use our in-house implementation of Hiero , Kriya , in Python .
Kriya can obtain statistically significantly equal BLEU scores when compared with Moses ( Koehn et al. , 2007 ) for several language pairs Callison - Burch et al. , 2012 ) . ? phrase- based : Moses ( Koehn et al. , 2007 ) with and without lexicalized reordering features .
? LR - Hiero : LR - Hiero decoding with cube pruning and queue diversity of 10 ( Siahbani and Sarkar , 2014 b ) .
To make the results comparable we use the standard SMT features for log-linear model in translation systems .
relative -frequency translation probabilities p( f |e ) and p( e |f ) , lexical translation probabilities p l ( f |e ) and p l ( e|f ) , a language model probability , word count , phrase count and distortion .
In addition , two distortion features proposed by ( Siahbani et al. , 2013 ) are added to both Hiero and LR - Hiero .
The LRM proposed in this paper uses a GNF grammar and LR decoding , therefore we apply it only to LR - Hiero .
The GNF rules are obtained from word and phrase aligned bitext using the rule extraction algorithm proposed by ( Siahbani and Sarkar , 2014a ) .
Table 3 compares the performance of different translation systems in terms of translation quality ( BLEU ) .
In all language pairs the proposed lexicalized reordering model improves the translation quality of LR - Hiero .
These observations are comparable to the effect of LRM in phrase - based translation system .
In Cs-En , LRM gets the best results and it significantly improves the the LR - Hiero results for De-En and Zh-En ( p-value < 0.05 , evaluated by MultEval ( Clark et al. , 2011 ) ) .
To compare our approach to Nguyen and Vogel ( 2013 ) , we adopt their algorithm to LR - Hiero and use the same LRM trained for GNF rules ( marked as NVLRM in Table 3 ) .
Unsurprisingly this approach could not improve the translation quality in LR - Hiero .
This approach computes the LRM for all candidate translation of each span after obtain- ing the full translations .
In bottom - up decoders it helps to prune the hypotheses effectively while in LR - Hiero decoder as we apply a rule before knowing the translation of smaller spans the computation of LRM will be postponed and gets less effective in decoding .
Table 2 shows the performance in terms of decoding speed .
We use the same wrapper for Hiero and LR - Hiero to query the language model and report the average on a sample set of 50 sentences from test sets .
We can see LR - Hiero + LRM still works 3 times faster than Hiero in terms of number of LM calls which leads to a faster decoder speed .
Conclusion
We have proposed a novel lexicalized reordering model ( LRM ) for the left-to- right variant of Hiero called LR - Hiero distinct from previous LRM models .
The previous LRM models proposed for Hiero are just applicable to bottom - up decoders like CKY .
We proposed a model for the left-toright decoding algorithm of LR - Hiero .
We showed that our novel shift-reduce algorithm to decode with the lexicalized reordering model significantly improved the translation quality of LR - Hiero on three different language pairs .
Figure 3 :?
3 Figure 3 : Computing correct orientation for each rule during decoding in LR - Hiero for the example in Fig. 4 . rules : the rules used in the derivation .
ri. f : the position of rule 's lexical terms in the source sentence ;
Oi : the identified orientation .
S is the recent translated source span ( possibly discontinuous ) .
At each step Oi is identified by comparing ri .
f to S in the previous step or last translated source phrase ri?1 .
f .
