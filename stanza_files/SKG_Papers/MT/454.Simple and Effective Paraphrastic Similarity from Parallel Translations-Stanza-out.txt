title
Simple and Effective Paraphrastic Similarity from Parallel Translations
abstract
We present a model and methodology for learning paraphrastic sentence embeddings directly from bitext , removing the timeconsuming intermediate step of creating paraphrase corpora .
Further , we show that the resulting model can be applied to cross-lingual tasks where it both outperforms and is orders of magnitude faster than more complex stateof - the - art baselines .
1
Introduction Measuring sentence similarity is a core task in semantics ( Cer et al. , 2017 ) , and prior work has achieved strong results by training similarity models on datasets of paraphrase pairs ( Dolan et al. , 2004 ) .
However , such datasets are not produced naturally at scale and therefore must be created either through costly manual annotation or by leveraging natural annotation in specific domains , like Simple English Wikipedia ( Coster and Kauchak , 2011 ) or Twitter ( Lan et al. , 2017 ) .
One of the most promising approaches for inducing paraphrase datasets is via manipulation of large bilingual corpora .
Examples include bilingual pivoting over phrases ( Callison - Burch et al. , 2006 ; Ganitkevitch et al. , 2013 ) , and automatic translation of one side of the bitext Wieting and Gimpel , 2018 ; Hu et al. , 2019 ) .
However , this is costly - Wieting and Gimpel ( 2018 ) report their large-scale database of sentential paraphrases required 10,000 GPU hours to generate .
In this paper , we propose a method that trains highly performant sentence embeddings ( Pham et al. , 2015 ; Hill et al. , 2016 ; Pagliardini et al. , 2017 ; McCann et al. , 2017 ; Conneau et al. , 2017 ) directly on bitext , obviating these intermediate steps and avoiding the noise and error propagation from automatic dataset preparation methods .
This approach eases data collection , since bitext occurs naturally more often than paraphrase data and , further , has the additional benefit of creating cross-lingual representations that are useful for tasks such as mining or filtering parallel data and cross-lingual retrieval .
Most previous work for cross-lingual representations has focused on models based on encoders from neural machine translation ( Espana - Bonet et al. , 2017 ; Schwenk and Douze , 2017 ; Schwenk , 2018 ) or deep architectures using a contrastive loss ( Gr?goire and Langlais , 2018 ; Guo et al. , 2018 ; Chidambaram et al. , 2018 ) .
However , the paraphrastic sentence embedding literature has observed that simple models such as pooling word embeddings generalize significantly better than complex architectures ( Wieting et al. , 2016 b ) .
Here , we find a similar effect in the bilingual setting .
We propose a simple model that not only produces state - of - the - art monolingual and bilingual sentence representations , but also encode sentences hundreds of times fasteran important factor when applying these representations for mining or filtering large amounts of bitext .
Our approach forms the simplest method to date that is able to achieve state - of - the - art results on multiple monolingual and cross-lingual semantic textual similarity ( STS ) and parallel corpora mining tasks .
2 Lastly , since bitext is available for so many language pairs , we analyze how the choice of language pair affects the performance of English paraphrastic representations , finding that using related languages yields the best results .
Learning Sentence Embeddings
We first describe our objective function and then describe our encoder , in addition to several baseline encoders .
The methodology proposed here borrows much from past work ( Wieting and Gimpel , 2018 ; Guo et al. , 2018 ; Gr?goire and Langlais , 2018 ; Singla et al. , 2018 ) , but this specific combination has not been explored and , as we show in experiments , is surprisingly effective .
Training .
The training data consists of a sequence of parallel sentence pairs ( s i , t i ) in source and target languages respectively .
For each sentence pair , we randomly choose a negative target sentence t i during training that is not a translation of s i .
Our objective is to have source and target sentences be more similar than source and negative target examples by a margin ? : min ?src , ?tgt i ?f ? ( s i , t i ) + f ? ( s , t i ) ) + .
The similarity function is defined as : f ? ( s , t ) = cos g(s ; ? src ) , g( t ; ? tgt ) where g is the sentence encoder with parameters for each language ? = ( ? src , ? tgt ) .
To select t i we choose the most similar sentence in some set according to the current model parameters , i.e. , the one with the highest cosine similarity .
Negative Sampling .
The described objective can also be applied to monolingual paraphrase data , which we explore in experiments .
The choice of negative examples differs whether we are using a monolingual parallel corpus or a bilingual parallel corpus .
In the monolingual case , we select from all examples in the batch except the current pair .
However , in the bilingual case , negative examples are only selected from the sentences in the batch from the opposing language .
To select difficult negative examples that aid training , we use the mega-batching procedure of Wieting and Gimpel ( 2018 ) , which aggregates M mini-batches to create one mega-batch and selects negative examples therefrom .
Once each pair in the megabatch has a negative example , the mega-batch is split back up into M mini-batches for training .
Encoders .
Our primary sentence encoder simply averages the embeddings of subword units generated by sentencepiece ( Kudo and Richardson , 2018 ) ; we refer to it as SP .
This means that the sentence piece embeddings themselves are the only learned parameters of this model .
As baselines we explore averaging character trigrams ( TRIGRAM ) ( Wieting et al. , 2016 a ) and words ( WORD ) .
SP provides a compromise between averaging words and character trigrams , combining the more distinct semantic units of words with the coverage of character trigrams .
We also use a bidirectional LSTM encoder ( Hochreiter and Schmidhuber , 1997 ) , with LSTM parameters fully shared between languages , as well as LSTM - SP , which uses sentence pieces instead of words as the input tokens .
For all encoders , when the vocabularies of source and target languages overlap , the corresponding encoder embedding parameters are shared .
As a result , languages pairs with more lexical overlap share more parameters .
We utilize several regularization methods including dropout ( Srivastava et al. , 2014 ) and shuffling the words in the sentence when training the LSTM -SP .
Additionally , we find that annealing the mega-batch size by increasing it during training improved performance by a significant margin for LSTM -SP .
Experiments Experiments are split into two groups .
First , we compare training on parallel data to training on back - translated parallel data .
We evaluate these models on the 2012 - 2016 SemEval Semantic Textual Similarity ( STS ) shared tasks ( Agirre et al. , 2012 ( Agirre et al. , , 2013 ( Agirre et al. , , 2014 ( Agirre et al. , , 2015 ( Agirre et al. , , 2016 , which predict the degree to which sentences have the same meaning as measured by human judges .
The evaluation metric is Pearson 's r with the gold labels .
We use the small STS English - English dataset from Cer et al . ( 2017 ) for model selection .
Second , we compare our best model , SP , on two semantic crosslingual tasks : the 2017 SemEval STS task ( Cer et al. , 2017 ) which consists of monolingual and cross-lingual datasets and the 2018 Building and Using Parallel Corpora ( BUCC ) shared bitext mining task ( Zweigenbaum et al. , 2018 ) .
Hyperparameters and Optimization Unless otherwise specified , we fix the hyperparameters in our model to the following : megabatch size to 120 , margin ? to 0.4 , annealing rate to 150 , 3 dropout to 0.3 , shuffling rate for BLSTM -SP to 0.3 , and the size of the SentencePiece vocabulary to 20,000 .
For WORD and TRIGRAM , we limited the vocabulary to the 200,000 most frequent types in the training data .
We optimize our models using Adam ( Kingma and Ba , 2014 ) with a learning rate of 0.001 and trained the models for 10 epochs .
Back -Translated Text vs. Parallel Text
We first compare sentence encoders and sentence embedding quality between models trained on backtranslated text and those trained on bitext directly .
As our bitext , we use the Czeng1.6 English - Czech parallel corpus ( Bojar et al. , 2016 ) .
We compare it to training on ParaNMT ( Wieting and Gimpel , 2018 ) 1 show two observations .
First , models trained on En-En , in contrast to those trained on En-CS , have higher correlation for all encoders except SP .
However , when the same number of English sentences is used , models trained on bitext have greater than or equal performance across all encoders .
Second , SP has the best performance in the En-CS setting .
It also has fewer parameters and is therefore faster to train than LSTM -SP and TRIGRAM .
Further , it is much faster at encoding new sentences at test time .
Monolingual and Cross-Lingual Similarity
We evaluate on the cross-lingual STS tasks from SemEval 2017 .
This evaluation contains Arabic-Arabic ( Ar ) , Arabic-English , Spanish - Spanish ( Es ) , Spanish -English , and Turkish - English STS datsets .
These datasets were created by translating one or both pairs of an English STS pair into Arabic , Spanish , or Turkish .
Baselines .
We compare to several models from prior work ( Guo et al. , 2018 ; Chidambaram et al. , 2018 ) .
A fair comparison to other models is difficult due to different training setups .
Therefore , we perform a variety of experiments at different scales to demonstrate that even with much less data , our method has the best performance .
In the case of Schwenk ( 2018 ) , we replicate their setting in order to do a fair comparison .
4
As another baseline , we analyze the performance of averaging randomly initialized embeddings .
We experiment with SP having Sentence - Piece vocabulary sizes of 20,000 and 40,000 as well as TRIGRAM .
The embeddings have 300 dimensions and are initialized from a normal distribution with mean 0 and variance 1 . Results .
The results are shown in Table 2 .
We make several observations .
The first is that the 1024 dimension SP model has the top performance on 4 of the 6 STS datasets .
Our results also show that performance increases substantially from using 1 million to using 10 million sentence pairs and is also correlated positively with dimension and size of the SP vocabulary .
This result outperforms the baselines from the literature as well , all of which use deep architectures .
5 Our SP model trained on Europarl ( EP ) also surpasses the model from Schwenk ( 2018 ) which is trained on the same corpus .
Since that model is based on many - to -many translation , Schwenk ( 2018 ) on nine ( related ) languages in Europarl .
We only train on the splits of interest ( En - Es for STS and En- De / En - Fr for the BUCC tasks ) in our experiments Secondly , random encoders , especially random TRIGRAM , perform strongly in the monolingual setting .
In fact , the random encoders are competitive or outperform all three models from the literature in these cases .
For cross-lingual similarity , however , random encoders lag behind because they are essentially measuring the lexical overlap in the two sentences and there is little lexical overlap in the cross-lingual setting , especially for distantly related languages like Arabic and English .
Mining Bitext Lastly , we evaluate on the BUCC shared task on mining bitext .
The task consists of finding the gold aligned parallel sentences given two large corpora in two distinct languages .
Typically , only about 2.5 % of the sentences are aligned .
Following Schwenk ( 2018 ) , we train our models on Europarl and evaluate on the publicly available BUCC data .
Results in Table 3 on the French and German mining tasks demonstrate the proposed model outperforms Schwenk ( 2018 ) , although the gap is substantially smaller than on the STS tasks .
The reason for this is likely the domain mismatch between the STS data ( image captions ) and the training data ( Europarl ) .
We suspect that the deep NMT encoders of Schwenk ( 2018 )
Analysis
We next conduct experiments on encoding speed and analyze the effect of language choice .
Encoding Speed
In addition to outperforming more complex models ( Schwenk , 2018 ; Chidambaram et al. , 2018 ) , the simple SP models proposed here are much faster at encoding sentences .
Since implementations to encode sentences are publicly available several baselines , we are able to test their encoding speed and compare .
To do so , we randomly select 128,000 English sentences from the English - Spanish Europarl corpus .
We then encode these sentences in batches of 128 on an Nvidia Quadro GP100 GPU .
The number of sentences encoded per second is shown in Table 4 , showing that SP is hundreds of times faster .
Does Language Choice Matter ?
We next investigate the impact of the non-English language in the bitext when training English paraphrastic sentence embeddings .
We took all 46 languages with at least 100k parallel sentence pairs in the 2016 OpenSubtitles Corpus ( Lison and Tiedemann , 2016 ) and made a plot of their average STS performance on the 2012 - 2016 English datasets compared to their SP overlap 6 and language distance .
7
We segmented the languages separately and trained the models for 10 epochs using the 2017 STS task for model selection .
The plot , shown in Figure 1 , shows that SentencePieces ( SP ) overlap is highly correlated with STS score .
There are also two clusters in the plot , languages that have a similar alphabet to English and those that do not .
In each cluster we find that performance is negatively correlated with language distance .
Therefore , languages similar to English yield better performance .
The Spearman 's correlations ( multiplied by 100 ) for all languages and these two clusters are shown in Table 5 .
When choosing a language to pair up with English for learning paraphrastic embeddings , ideally there will be a lot of SP overlap .
However , beyond or below a certain threshold ( approximately 0.3 judging by the plot ) , the linguistic distance to English is more predictive of performance .
Of the Table 5 : Spearman's ? ? 100 between average performance on the 2012 - 2016 STS tasks compared to SP overlap ( SP Ovl. ) and language distance as defined by Littell et al . ( 2017 ) .
We included correlations for all languages as well as those with low and high SP overlap with English .
factors in URIEL , syntactic distance was the feature most correlated with STS performance in the two clusters with correlations of - 56.1 and - 29.0 for the low and high overlap clusters respectively .
This indicates that languages with similar syntax to English helped performance .
One hypothesis to explain this relationship is that translation quality is higher for related languages , especially if the languages have the same syntax , resulting in a cleaner training signal .
We also hypothesize that having high SP overlap is correlated with improved performance because the English SP embeddings are being updated more frequently during training .
To investigate the effect , we again learned segmentations separately for both languages then prefixed all tokens in the non-English text with a marker to ensure that there would be no shared parameters between the two languages .
Results showed that SP overlap was still correlated ( correlation of 24.9 ) and language distance was still negatively correlated with performance albeit significantly less so at - 10.1 .
Of all the linguistic features , again the syntactic distance was the highest correlated at - 37.5 .
Conclusion
We have shown that using automatic dataset preparation methods such as pivoting or backtranslation are not needed to create higher performing sentence embeddings .
Moreover by using the bitext directly , our approach also produces strong paraphrastic cross-lingual representations as a byproduct .
Our approach is much faster than comparable methods and yields stronger performance on cross-lingual and monolingual semantic similarity and cross-lingual bitext mining tasks .
Figure 1 : 1 Figure 1 : Plot of average performance on the 2012 - 2016 STS tasks compared to SP overlap and language distance as defined by Littell et al . ( 2017 ) .
