title
Rule Markov Models for Fast Tree-to- String Translation
abstract
Most statistical machine translation systems rely on composed rules ( rules that can be formed out of smaller rules in the grammar ) .
Though this practice improves translation by weakening independence assumptions in the translation model , it nevertheless results in huge , redundant grammars , making both training and decoding inefficient .
Here , we take the opposite approach , where we only use minimal rules ( those that cannot be formed out of other rules ) , and instead rely on a rule Markov model of the derivation history to capture dependencies between minimal rules .
Large-scale experiments on a state - of - the - art tree- to-string translation system show that our approach leads to a slimmer model , a faster decoder , yet the same translation quality ( measured using Bleu ) as composed rules .
Introduction Statistical machine translation systems typically model the translation process as a sequence of translation steps , each of which uses a translation rule , for example , a phrase pair in phrase - based translation or a tree-to-string rule in tree-to-string translation .
These rules are usually applied independently of each other , which violates the conventional wisdom that translation should be done in context .
To alleviate this problem , most state - of - the - art systems rely on composed rules , which are larger rules that can be formed out of smaller rules ( including larger phrase pairs that can be formerd out of smaller phrase pairs ) , as opposed to minimal rules , which are rules that cannot be formed out of other rules .
Although this approach does improve translation quality dramatically by weakening the independence assumptions in the translation model , they suffer from two main problems .
First , composition can cause a combinatorial explosion in the number of rules .
To avoid this , ad-hoc limits are placed during composition , like upper bounds on the number of nodes in the composed rule , or the height of the rule .
Under such limits , the grammar size is manageable , but still much larger than the minimal - rule grammar .
Second , due to large grammars , the decoder has to consider many more hypothesis translations , which slows it down .
Nevertheless , the advantages outweigh the disadvantages , and to our knowledge , all top- performing systems , both phrase - based and syntax - based , use composed rules .
For example , Galley et al. ( 2004 ) initially built a syntax - based system using only minimal rules , and subsequently reported ( Galley et al. , 2006 ) that composing rules improves Bleu by 3.6 points , while increasing grammar size 60 - fold and decoding time 15 - fold .
The alternative we propose is to replace composed rules with a rule Markov model that generates rules conditioned on their context .
In this work , we restrict a rule 's context to the vertical chain of ancestors of the rule .
This ancestral context would play the same role as the context formerly provided by rule composition .
The dependency treelet model developed by Quirk and Menezes ( 2006 ) takes such an approach within the framework of dependency translation .
However , their study leaves unanswered whether a rule Markov model can take the place of composed rules .
In this work , we investigate the use of rule Markov models in the context of tree- to-string translation ( Liu et al. , 2006 ; Huang et al. , 2006 ) .
We make three new contributions .
First , we carry out a detailed comparison of rule Markov models with composed rules .
Our experiments show that , using trigram rule Markov models , we achieve an improvement of 2.2 Bleu over a baseline of minimal rules .
When we compare against vertically composed rules , we find that our rule Markov model has the same accuracy , but our model is much smaller and decoding with our model is 30 % faster .
When we compare against full composed rules , we find that our rule Markov model still often reaches the same level of accuracy , again with savings in space and time .
Second , we investigate methods for pruning rule Markov models , finding that even very simple pruning criteria actually improve the accuracy of the model , while of course decreasing its size .
Third , we present a very fast decoder for tree -tostring grammars with rule Markov models .
Huang and Mi ( 2010 ) have recently introduced an efficient incremental decoding algorithm for tree - to-string translation , which operates top-down and maintains a derivation history of translation rules encountered .
This history is exactly the vertical chain of ancestors corresponding to the contexts in our rule Markov model , which makes it an ideal decoder for our model .
We start by describing our rule Markov model ( Section 2 ) and then how to decode using the rule Markov model ( Section 3 ) .
Rule Markov models
Our model which conditions the generation of a rule on the vertical chain of its ancestors , which allows it to capture interactions between rules .
Consider the example Chinese-English tree-tostring grammar in Figure 1 and the example derivation in Figure 2 .
Each row is a derivation step ; the tree on the left is the derivation tree ( in which each node is a rule and its children are the rules that substitute into it ) and the tree pair on the right is the source and target derived tree .
For any derivation node r , let anc 1 ( r ) be the parent of r ( or ? if it has no parent ) , anc 2 ( r ) be the grandparent of node r ( or ? if it has no grandparent ) , and so on .
Let anc n 1 ( r ) be the chain of ancestors anc 1 ( r ) ? ? ? anc n ( r ) .
The derivation tree is generated as follows .
With probability P(r 1 | ? ) , we generate the rule at the root node , r 1 .
We then generate rule r 2 with probability P(r 2 | r 1 ) , and so on , always taking the leftmost open substitution site on the English derived tree , and generating a rule r i conditioned on its chain of ancestors with probability P(r i | anc n 1 ( r i ) ) .
We carry on until no more children can be generated .
Thus the probability of a derivation tree T is P( T ) = r?T P( r | anc n 1 ( r ) ) ( 1 )
For the minimal rule derivation tree in Figure 2 , the probability is : P( T ) = P( r 1 | ? ) ? P( r 2 | r 1 ) ? P( r 3 | r 1 ) ? P( r 4 | r 1 , r 3 ) ? P( r 6 | r 1 , r 3 , r 4 ) ? P( r 7 | r 1 , r 3 , r 4 ) ? P( r 5 | r 1 , r 3 ) ( 2 ) Training
We run the algorithm of Galley et al . ( 2004 ) on word-aligned parallel text to obtain a single derivation of minimal rules for each sentence pair .
( Unaligned words are handled by attaching them to the highest node possible in the parse tree . )
The rule Markov model can then be trained on the path set of these derivation trees .
Smoothing
We use interpolation with absolute discounting ( Ney et al. , 1994 ) : P abs ( r | anc n 1 ( r ) ) = max c( r | anc n 1 ( r ) ) ?
D n , 0 r ? c( r ? | anc n 1 ( r ? ) ) + ( 1 ? ? n ) P abs ( r | anc n?1 1 ( r ) ) , ( 3 ) where c( r | anc n 1 ( r ) ) is the number of times we have seen rule r after the vertical context anc n 1 ( r ) , D n is the discount for a context of length n , and ( 1 ? ? n ) is set to the value that makes the smoothed probability distribution sum to one .
We experiment with bigram and trigram rule Markov models .
For each , we try different values of D 1 and D 2 , the discount for bigrams and trigrams , respectively .
Ney et al. ( 1994 ) suggest using the following value for the discount D n : D n = n 1 n 1 + n 2 ( 4 ) rule id translation rule r 1 IP ( x 1 :NP x 2 :VP ) ? x 1 x 2 r 2 NP ( B?sh ? ) ? Bush r 3 VP( x 1 : PP x 2 :VP ) ? x 2 x 1 r 4 PP ( x 1 :P x 2 :NP ) ? x 1 x 2 r 5 VP ( VV ( j?x?ng ) AS ( le ) NPB ( hu?t ? n ) ) ? held talks r 6 P(y ? ) ? with r ? 6 P(y ? ) ? and r 7 NP ( Sh ?l?ng ) ?
Sharon derivation tree derived tree pair ? IP @? : IP @? r 1 IP @? NP @1 VP @2 IP @? NP @1 VP @2 : NP @1 VP @2 r 1 r 2 r 3 IP @? NP @1 B?sh ? VP @2 PP @ 2.1 VP @ 2.2 IP @? NP @1 B?sh ?
VP @2 PP @ 2.1 VP @ 2.2 : Bush VP @2.2 PP @ 2.1
Here , n 1 and n 2 are the total number of n-grams with exactly one and two counts , respectively .
For our corpus , D 1 = 0.871 and D 2 = 0.902 .
Additionally , we experiment with 0.4 and 0.5 for D n . r 1 r 2 r 3 r 4 r 5 IP @? NP @1 B?sh ? VP @2 PP @ 2.1 P @2.1.1 NP @2.1.2 VP @ 2.2 VV j?x?ng AS le NP hu?t?n IP @? NP @1 B?sh ? VP @2 PP @ 2.1 P @2.1.1 NP @2.1.2 VP @ 2.2 VV j?x?ng AS le NP hu?t? n : Bush held talks P @ 2.1.1 NP @ 2.1.2 r 1 r 2 r 3 r 4 r 6 r 7 r 5 IP @? NP @1 B?sh ? VP @2 PP @ 2.1 P @ 2.1.1 y? NP @2.1.2 Sh?l?ng VP @ 2.2 VV j?x?ng AS le NP hu?t?n IP @? NP @1 B?sh ? VP @2 PP @ 2.1 P @ 2.1.1 y? NP @2.1.2 Sh?l?ng VP @ 2.2 Pruning
In addition to full n-gram Markov models , we experiment with three approaches to build smaller models to investigate if pruning helps .
Our results will show that smaller models indeed give a higher Bleu score than the full bigram and trigram models .
The approaches we use are : ? RM-A : We keep only those contexts in which more than P unique rules were observed .
By optimizing on the development set , we set P = 12 .
? RM -B : We keep only those contexts that were observed more than P times .
Note that this is a superset of RM - A .
Again , by optimizing on the development set , we set P = 12 .
? RM -C : We try a more principled approach for learning variable - length Markov models inspired by that of Bejerano and Yona ( 1999 ) , who learn a Prediction Suffix Tree ( PST ) .
They grow the PST in an iterative manner by starting from the root node ( no context ) , and then add contexts to the tree .
A context is added if the KL divergence between its predictive distribution and that of its parent is above a certain threshold and the probability of observing the context is above another threshold .
Tree-to-string decoding with rule Markov models
In this paper , we use our rule Markov model framework in the context of tree-to-string translation .
Tree-to-string translation systems ( Liu et al. , 2006 ; Huang et al. , 2006 ) have gained popularity in recent years due to their speed and simplicity .
The input to the translation system is a source parse tree and the output is the target string .
Huang and Mi ( 2010 ) have recently introduced an efficient incremental decoding algorithm for tree - to-string translation .
The decoder operates top-down and maintains a derivation history of translation rules encountered .
The history is exactly the vertical chain of ancestors corresponding to the contexts in our rule Markov model .
This IP @? NP @1 B?sh ? VP @2 PP @ 2.1 P @ 2.1.1 y? NP @2.1.2 Sh?l?ng VP @ 2.2 VV @2.2.1 j?x?ng AS @ 2.2.2 le NP @ 2.2.3 hu?t?n
Figure 3 : Example input parse tree with tree addresses .
makes incremental decoding a natural fit with our generative story .
In this section , we describe how to integrate our rule Markov model into this incremental decoding algorithm .
Note that it is also possible to integrate our rule Markov model with other decoding algorithms , for example , the more common non-incremental top-down / bottom - up approach ( Huang et al. , 2006 ) , but it would involve a non-trivial change to the decoding algorithms to keep track of the vertical derivation history , which would result in significant overhead .
Algorithm Given the input parse tree in Figure 3 , Figure 4 illustrates the search process of the incremental decoder with the grammar of Figure 1 .
We write X @? for a tree node with label X at tree address ?
( Shieber et al. , 1995 ) .
The root node has address ? , and the ith child of node ? has address ?.i.
At each step , the decoder maintains a stack of active rules , which are rules that have not been completed yet , and the rightmost ( n ? 1 ) English words translated thus far ( the hypothesis ) , where n is the order of the word language model ( in Figure 4 , n = 2 ) .
The stack together with the translated English words comprise a state of the decoder .
The last column in the figure shows the rule Markov model probabilities with the conditioning context .
In this example , we use a trigram rule Markov model .
After initialization , the process starts at step 1 , where we predict rule r 1 ( the shaded rule ) with probability P(r 1 | ? ) and push its English side onto the stack , with variables replaced by the corresponding tree nodes : x 1 becomes NP @1 and x 2 becomes VP @2 .
This gives us the following stack : s = [ NP @1 VP @2 ]
The dot ( ) indicates the next symbol to process in stack hyp .
MR prob. 0 [ <s> IP @? </s >] <s> 1 [ <s> IP @? </s > ] [ NP @1 VP @2 ] < s> P ( r 1 | ? ) 2 [ <s> IP @? </s > ] [ NP @1 VP @2 ] [ Bush ] < s> P ( r 2 | r 1 ) 3 [ < s> IP @? </s > ] [ NP @1 VP @2 ] [ Bush ] . . . Bush 4 [ <s> IP @? </s >] [ NP @1 VP @2 ] . . . Bush 5 [ <s> IP @? </s >] [ NP @1 VP @2 ] [ VP @2.2 PP @ 2.1 ] . . . Bush P( r 3 | r 1 ) 6 [ < s> IP @? </s >] [ NP @1 VP @2 ] [ VP @ 2.2 PP @ 2.1 ] [ held talks ] . . . Bush P( r 5 | r 1 , r 3 ) 7 [ < s> IP @? </s >] [ NP @1 VP @2 ] [ VP @ 2.2 PP @ 2.1 ] [ held talks ] . . . held 8 [ <s> IP @? </s >] [ NP @1 VP @2 ] [ VP @ 2.2 PP @ 2.1 ] [ held talks ] . . . talks 9 [ <s> IP @? </s >] [ NP @1 VP @ 2 ] [ VP @ 2.2 PP @ 2.1 ] . . . talks 10 [ <s> IP @? </s >] [ NP @1 VP @ 2 ] [ VP @ 2.2 PP @ 2.1 ] [ P @2.1.1 NP @ 2.1.2 ] . . . talks P( r 4 | r 1 , r 3 ) 11 [ < s> IP @? </s >] [ NP @1 VP @ 2 ] [ VP @ 2.2 PP @ 2.1 ] [ P @ 2.1.1 NP @ 2.1.2 ] [ with ] . . . with P( r 6 | r 3 , r 4 ) 12 [ < s> IP @? </s >] [ NP @1 VP @ 2 ] [ VP @ 2.2 PP @ 2.1 ] [ P @ 2.1.1 NP @ 2.1.2 ] [ with ] . . . with 13 [ < s> IP @? </s >] [ NP @1 VP @ 2 ] [ VP @ 2.2 PP @ 2.1 ] [ P @2.1.1 NP @ 2.1.2 ] . . . with 14 [ < s> IP @? </s >] [ NP @1 VP @ 2 ] [ VP @ 2.2 PP @ 2.1 ] [ P @2.1.1 NP @ 2.1.2 ] [ Sharon ] . . . with P( r 7 | r 3 , r 4 ) 11 ? [ < s> IP @? </s >] [ NP @1 VP @ 2 ] [ VP @ 2.2 PP @ 2.1 ] [ P @ 2.1.1 NP @ 2.1.2 ] [ and ] . . . and P( r ? 6 | r 3 , r 4 ) 12 ? [ < s> IP @? </s >] [ NP @1 VP @ 2 ] [ VP @ 2.2 PP @ 2.1 ] [ P @2.1.1 NP @ 2.1.2 ] [ and ] . . . and 13 ? [ < s> IP @? </s >] [ NP @1 VP @ 2 ] [ VP @ 2.2 PP @ 2.1 ] [ P @2.1.1 NP @ 2.1.2 ] . . . and 14 ? [ < s> IP @? </s >] [ NP @1 VP @ 2 ] [ VP @ 2.2 PP @ 2.1 ] [ P @2.1.1 NP @ 2.1.2 ] [ Sharon ] . . . and P( r 7 | r 3 , r 4 ) VP @2 VP @ 2.2 PP @ 2.1 P @ 2.1.1 y? NP @2.1.2 VP @2 VP @ 2.2 PP @ 2.1 P @ 2.1.1 y? NP @ 2.1.2 15 [<s> IP @? </s >] [ NP @1 VP @ 2 ] [ VP @ 2.2 PP @ 2.1 ] [ P @2.1.1 NP @ 2.1.2 ] [ Sharon ] . . . Sharon 16 [ <s> IP @? </s >] [ NP @1 VP @ 2 ] [ VP @ 2.2 PP @ 2.1 ] [ P @2.1.1 NP @ 2.1.2 ] . . . Sharon 17 [ <s> IP @? </s >] [ NP @1 VP @ 2 ] [ VP @ 2.2 PP @ 2.1 ] . . . Sharon 18 [ <s> IP @? </s >] [ NP @1 VP @2 ] . . . Sharon 19 [ <s> IP @? </s >] . . . Sharon 20 [ <s> IP @? </s> ] . . . </s> Figure 5 : Vertical context r 3 r 4 which allows the model to correctly translate y? as with .
the English word order .
We expand node NP @1 first with English word order .
We then predict lexical rule r 2 with probability P(r 2 | r 1 ) and push rule r 2 onto the stack : [ NP @1 VP @2 ] [ Bush ]
In step 3 , we perform a scan operation , in which we append the English word just after the dot to the current hypothesis and move the dot after the word .
Since the dot is at the end of the top rule in the stack , we perform a complete operation in step 4 where we pop the finished rule at the top of the stack .
In the scan and complete steps , we do n't need to compute rule probabilities .
An interesting branch occurs after step 10 with two competing lexical rules , r 6 and r ?
6 . The Chinese word y? can be translated as either a preposition with ( leading to step 11 ) or a conjunction and ( leading to step 11 ? ) .
The word n-gram model does not have enough information to make the correct choice , with .
As a result , good translations might be pruned because of the beam .
However , our rule Markov model has the correct preference because of the conditioning ancestral sequence ( r 3 , r 4 ) , shown in Figure 5 . Since VP @ 2.2 has a preference for y?
translating to with , our corpus statistics will give a higher probability to P(r 6 | r 3 , r 4 ) than P( r ? 6 | r 3 , r 4 ) .
This helps the decoder to score the correct translation higher .
Complexity analysis
With the incremental decoding algorithm , adding rule Markov models does not change the time complexity , which is O ( nc | V | g?1 ) , where n is the sentence length , c is the maximum number of incoming hyperedges for each node in the translation forest , V is the target - language vocabulary , and g is the order of the n-gram language model ( Huang and Mi , 2010 ) .
However , if one were to use rule Markov models with a conventional CKY - style bottom - up decoder ( Liu et al. , 2006 ) , the complexity would increase to O ( nC m?1 | V | 4 ( g?1 ) ) , where C is the maximum number of outgoing hyperedges for each node in the translation forest , and m is the order of the rule Markov model .
Experiments and results
Setup
The training corpus consists of 1.5 M sentence pairs with 38M /32 M words of Chinese / English , respectively .
Our development set is the newswire portion of the 2006 NIST MT Evaluation test set ( 616 sentences ) , and our test set is the newswire portion of the 2008 NIST MT Evaluation test set ( 691 sentences ) .
We word- aligned the training data using GIZA ++ followed by link deletion ( Fossum et al. , 2008 ) , and then parsed the Chinese sentences using the Berkeley parser ( Petrov and Klein , 2007 ) .
To extract tree- to-string translation rules , we applied the algorithm of Galley et al . ( 2004 ) .
We trained our rule Markov model on derivations of minimal rules as described above .
Our trigram word language model was trained on the target side of the training corpus using the SRILM toolkit ( Stolcke , 2002 ) with modified Kneser - Ney smoothing .
The base feature set for all systems is similar to the set used in Mi et al . ( 2008 ) .
The features are combined into a standard log-linear model , which we trained using minimum error-rate training ( Och , 2003 ) to maximize the Bleu score on the development set .
At decoding time , we again parse the input sentences using the Berkeley parser , and convert them into translation forests using rule patternmatching ( Mi et al. , 2008 ) .
We evaluate translation quality using case- insensitive IBM Bleu - 4 , calculated by the script mteval - v13a.pl .
Results
Table 1 presents the main results of our paper .
We used grammars of minimal rules and composed rules of maximum height 3 as our baselines .
For decoding , we used a beam size of 50 .
Using the best bigram rule Markov models and the minimal rule grammar gives us an improvement of 1.5 Bleu over the minimal rule baseline .
These gains are statistically significant with p < 0.01 , using bootstrap resampling with 1000 samples ( Koehn , 2004 ) .
We find that by just using bigram context , we are able to get at least 1 Bleu point higher than the minimal rule grammar .
It is interesting to see that using just bigram rule interactions can give us a reasonable boost .
We get our highest gains from using trigram context where our best performing rule Markov model gives us 2.3 Bleu points over minimal rules .
This suggests that using longer contexts helps the decoder to find better translations .
We also compared rule Markov models against composed rules .
Since our models are currently limited to conditioning on vertical context , the closest comparison is against vertically composed rules .
We find that our approach performs equally well using much less time and space .
Comparing against full composed rules , we find that our system matches the score of the baseline composed rule grammar of maximum height 3 , while using many fewer parameters .
( It should be noted that a parameter in the rule Markov model is just a floating - point number , whereas a parameter in the composed - rule system is an entire rule ; therefore the difference in memory usage would be even greater . )
Decoding with our model is 0.2 seconds faster per sentence than with composed rules .
These experiments clearly show that rule Markov models with minimal rules increase translation quality significantly and with lower memory requirements than composed rules .
One might wonder if the best performance can be obtained by combining composed rules with a rule Markov model .
This respectively .
It is interesting that the full bigram and trigram rule Markov models do not give our highest Bleu scores ; pruning the models not only saves space but improves their performance .
We think that this is probably due to overfitting .
Table 4 shows that the RM - A trigram model does fairly well under all the settings of D n we tried .
Table 5 shows the performance of vertically composed rules at various settings .
Here we have chosen the setting that gives the best performance on the test set for inclusion in Table 1 .
Table 6 shows the performance of fully composed rules and fully composed rules with a rule Markov Model at various settings .
2
In the second line ( 2.9 million rules ) , the drop in Bleu score resulting from adding the rule Markov model is not statistically significant .
Related Work Besides the Quirk and Menezes ( 2006 ) work discussed in Section 1 , there are two other previous 2 For these experiments , a beam size of 100 was used .
efforts both using a rule bigram model in machine translation , that is , the probability of the current rule only depends on the immediate previous rule in the vertical context , whereas our rule Markov model can condition on longer and sparser derivation histories .
Among them , Ding and Palmer ( 2005 ) also use a dependency treelet model similar to Quirk and Menezes ( 2006 ) , and Gildea ( 2008 ) use a tree-to-string model more like ours .
Neither compared to the scenario with composed rules .
Outside of machine translation , the idea of weakening independence assumptions by modeling the derivation history is also found in parsing ( Johnson , 1998 ) , where rule probabilities are conditioned on parent and grand-parent nonterminals .
However , besides the difference between parsing and translation , there are still two major differences .
First , our work conditions rule probabilities on parent and grandparent rules , not just nonterminals .
Second , we compare against a composed - rule system , which is analogous to the Data Oriented Parsing ( DOP ) approach in parsing ( Bod , 2003 ) .
To our knowledge , there has been no direct comparison between a history - based PCFG approach and DOP approach in the parsing literature .
Conclusion
In this paper , we have investigated whether we can eliminate composed rules without any loss in translation quality .
We have developed a rule Markov model that captures vertical bigrams and trigrams of minimal rules , and tested it in the framework of treeto-string translation .
We draw three main conclusions from our experiments .
First , our rule Markov models dramatically improve a grammar of minimal rules , giving an improvement of 2.3 Bleu .
Second , when we compare against vertically composed rules we are able to get about the same Bleu score , but our model is much smaller and decoding with our model is faster .
Finally , when we compare against full composed rules , we find that we can reach the same level of performance under some conditions , but in order to do so consistently , we believe we need to extend our model to condition on horizontal context in addition to vertical context .
We hope that by modeling context in both axes , we will be able to completely replace composed - rule grammars with smaller minimal - rule grammars .
Figure 1 : 1 Figure 1 : Example tree-to-string grammar .
