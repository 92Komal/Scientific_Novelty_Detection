title
NRC Systems for Low Resource German-Upper Sorbian Machine Translation 2020 : Transfer Learning with Lexical Modifications
abstract
We describe the National Research Council of Canada ( NRC ) neural machine translation systems for the German - Upper Sorbian supervised track of the 2020 shared task on Unsupervised MT and Very Low Resource Supervised MT .
Our models are ensembles of Transformer models , built using combinations of BPE - dropout , lexical modifications , and backtranslation .
Introduction
We describe the National Research Council of Canada ( NRC ) neural machine translation systems for the shared task on Unsupervised MT and Very Low Resource Supervised MT .
We participated in the supervised track of the low resource task , building Upper Sorbian - German neural machine translation ( NMT ) systems in both translation directions .
Upper Sorbian is a minority language spoken in Germany .
We built baseline systems ( standard Transformer ( Vaswani et al. , 2017 ) with a byte-pair encoding vocabulary ( BPE ; Sennrich et al. , 2016 b ) ) trained on all available parallel data ( 60,000 lines ) , which resulted in unusually high BLEU scores for a language pair with such limited data .
In order to improve upon this baseline , we used transfer learning with modifications to the training lexicon .
We did this in two ways : by experimenting with the application of BPE - dropout ( Provilkov et al. , 2020 ) to the transfer learning setting ( Section 2.3 ) , and by modifying Czech data used for training parent systems with word and character replacements in order to make it more " Upper Sorbian - like " ( Section 2.4 ) .
Our final systems were ensembles of systems built using transfer learning and these two approaches to lexicon modification , along with iterative backtranslation .
Approaches 2.1 General System Notes
In both translation directions , our final systems consist of ensembles of multiple systems , built using transfer learning ( Section 2.2 ) , BPE - Dropout ( Section 2.3 ) , alternative preprocessing of Czech data ( Section 2.4 ) , and backtranslation ( Section 2.5 ) .
We describe these approaches and related work in the following sections , providing implementation details for reproducibility in Sections 3 , 4 and 5 .
Zoph et al. ( 2016 ) proposed a transfer learning approach for neural machine translation , using language pairs with larger amounts of data to pre-train a parent system , followed by finetuning a child system on the language pair of interest .
Nguyen and Chiang ( 2017 ) expand on that , showing improved performance using BPE and shared vocabularies between the parent and child .
We follow this approach : we build disjoint source and target BPE models and vocabularies , with one vocabulary for German ( DE ) and one for the combination of Czech ( CS ) and Upper Sorbian ( HSB ) ; see Section 4 .
Transfer Learning
We chose to use Czech - German data as the parent language pair due to the task suggestions , relative abundance of data , and the close relationship between Czech and Upper Sorbian ( cf. Lin et al. , 2019 ; Kocmi and Bojar , 2018 ) .
While Czech and Upper Sorbian cognates are often not identical at the character level ( Table 1 ) , there is a high level of character - level overlap ; trying to take advantage of that overlap without assuming complete characterlevel identity is a motivation for the explorations in subsequent sections ( Section 2.3 , Section 2.4 ) .
Another relatively high-resource language related to Upper Sorbian is Polish , but while the Czech and Upper Sorbian orthographies are fairly similar , mostly using the same characters for the same sounds ( with a few notable exceptions ) , Polish orthography is more distinct .
This , combined with the lack of a direct Polish - German parallel dataset in the constrained condition , led us to choose Czech as our transfer language for these experiments .
Other work on transfer learning for low-resource machine translation includes multilingual seed models ( Neubig and Hu , 2018 ) , dynamically adding to the vocabulary when adding languages ( Lakew et al. , 2018 ) , and using a hierarchical architecture to use multiple language pairs ( Luo et al. , 2019 ) .
Czech
BPE - Dropout
We apply the recently - proposed approach of performing BPE - dropout ( Provilkov et al. , 2020 ) , which takes an existing BPE model and randomly drops some merges at each merge step when applying the model to text .
The goal of this , beside leading to more robust subword representations in general , is to produce subword representations that are more likely to overlap between the pretraining ( Czech - German ) and finetuning ( Upper Sorbian - German ) stages .
We hypothesized that , in the same way that BPE - Dropout leads to robustness against accidental spelling errors and variant spellings ( Provilkov et al. , 2020 ) , it could likewise lead to robustness to the kind of spelling variations we see between two related languages .
For example , consider the putative Czech - Upper Sorbian cognates and shared loanwords presented in Table 1 . Sometimes a fixed BPE segmentation happens to separate shared characters into shared subwords ( e.g. CS analy@@ z@@ ovat vs .
HSB analy@@ z@@ owa ? ) , such that the presence of the former during pre-training can initialize at least some of the subwords that the model will later see in Upper Sorbian .
However , other times the character - level differences lead to segmentations where no subwords are shared ( e.g. CS hospod ?
@@ sk ? vs. HSB hospodar@@ sce or potom vs .
HSB po@@ tym ) .
Considering a wider variety of segmentations would , we hypothesized , mean that Upper Sorbian subwords would have more chance of being initialized during Czech pre-training ( see Appendix C ) .
Rather than modifying the NMT system itself to reapply BPE - dropout during training , we treated BPE - dropout as a preprocessing step .
Additionally , we experimented with BPE - dropout in the context of transfer learning , examining the effects of using source-side , both-sides , or no dropout in both parent and child systems .
Pseudo-Sorbian
For the Upper Sorbian - German direction , we also experimented with two techniques for modifying the Czech - German parallel data so that the Czech side is more like Upper Sorbian .
In particular , we concentrated on modification methods that require neither large amounts of data , nor in - depth knowledge of the historical relationships between the languages , since both of these are often lacking for the lower - resourced language .
We considered two variations of this idea : ? word- level modification , in which some frequent Czech words ( e.g. prepositions ) are replaced by likely Upper Sorbian equivalents , and ?
character - level modification , where we attempt to convert Czech words at the character level to forms that may more closely resemble Upper Sorbian words .
Note that in neither case do we know what particular conversions are correct ; we ourselves do not know enough about historical Western Slavic to predict the actual Upper Sorbian cognates of Czech words .
Rather , we took inspiration from stochastic segmentation methods like BPE - Dropout ( Provilkov et al. , 2020 ) and SentencePiece ( Kudo and Richardson , 2018 ) : when we have an idea of the possible solutions to the segmentation problem but do not know which one is the correct one , we can sample randomly from the possible segmentations as a sort of regularization , with the goal of discouraging the model from relying too heavily on a single segmentation scheme and giving it some exposure to a variety of possible segmentations .
Whereas BPE - dropout and Sentence - Piece focus on possible segmentations of the word , our pseudo-Sorbian experiments focus on possible word - and character - level replacements .
The goal was to discourage the parent Czech - German model from relying too heavily on regularities in Czech ( e.g. the presence of particular frequent words , the presence of particular Czech character n-grams ) and perhaps also gain some prior exposure to Upper Sorbian words and characters that will occur in the genuine Upper Sorbian data ; we can also think of this as a form of low-resource data augmentation ( Fadaee et al. , 2017 ; . See Appendix
C for an analysis of increased subword overlap between pseudo-Sorbian and test data , as compared to BPE - dropout and the baseline approach .
Word- level pseudo-Sorbian
To generate the word- level pseudo- Sorbian , we ran fast align ( Dyer et al. , 2013 ) on the Czech -German and German - Upper Sorbian parallel corpora , and took the product of the resulting word correspondences , to generate candidate Czech - Upper Sorbian word correspondences .
As this process produces many unlikely correspondences , particularly for words that occur only a few times in the corpora , we filtered this list so that any Czech - German word correspondence that occurred fewer than 500 times in the aligned corpus was ineligible , and likewise any German - Upper Sorbian correspondence that occurred fewer than 50 times .
We then used these correspondences to randomly replace 10 % of eligible Czech words in the Czech - German corpus with one of their putative equivalents in Upper Sorbian .
The result is a language that is mostly still Czech , but in which some high- frequency words ( especially prepositions ) are Upper Sorbian .
Character - level pseudo-Sorbian
To generate the character - level pseudo-Sorbian , we began with the same list of putative Czech - Upper Sorbian word correspondences , calculated the Levenshtein distances ( normalized by length ) between them , and filtered out pairs that exceeded 0.5 distance .
This gave a list of words that were likely cognates , from which we hand -selected a development set of about 200 ; a sample of these is seen in Table 1 . Using this set to identify character - level correspondences ( e.g. CS v to HSB w , CS d to HSB d? before front vowels , etc. ) , we wrote a program to randomly replace the appropriate Czech character sequences with possible correspondences in Upper Sorbian .
Again , as Czech - Upper Sorbian correspondences are not entirely predictable ( CS e might happen to correspond , in a particular cognate , to HSB e or ej or i or a or o , etc. ) , we cannot expect that any given result is correct Upper Sorbian .
Rather , we can think of this process as attempting to train a system that can respond to inputs from a variety of possible ( but not necessarily actual ) Western Slavic languages , rather than just a system that can respond to precisely -spelled Czech and only Czech .
Combined pseudo-Sorbian
In initial testing , we determined that a combination of word-level and character - level modification performed best ; we ran each process on the Czech - German corpus separately , then concatenated the resulting corpora and trained a parent model on it .
Due to time constraints we did not run the full set of ablation experiments .
Subsequent finetuning on genuine Upper Sorbian - German data proceeded as normal , without any modification .
For all pseudo-Sorbian systems , we used the BPE vocabulary trained on the original Czech and Upper Sorbian data , rather than the modified data , so that systems trained on pseudo-Sorbian data could still be ensembled with systems trained only on the original data ( Section 2.6 ) .
Backtranslation
We used backtranslation ( Sennrich et al. , 2016a ) to incorporate monolingual German and Upper Sorbian data into training .
We backtranslated all Upper Sorbian monolingual data ( after filtering as described in Section 3 ) .
We backtranslated the German monolingual news -commentary data and 1.2 M randomly sampled lines of 2019 German news .
We experiment with iterative backtranslation : backtranslating data using systems without backtranslation , and then using the new systems built using the backtranslated text to perform a second iteration of backtranslation ( Hoang et al. , 2018 ; Niu et al. , 2018 ; . Like Caswell et al. ( 2019 ) , we use source-side tags at the start of backtranslated sentences to indicate to the models which sentences are the product of backtranslation .
Ensembling
Our final systems are ensembles of several systems .
Because all systems used the same vocabulary sets and same model sizes , we could decode using Sockeye 's ( Hieber et al. , 2018 ) default ensembling mechanism .
Data
We used all provided parallel German - Upper Sorbian data and all monolingual Upper Sorbian data ( after filtering ) , along with German - Czech parallel data from Open Subtitles ( Lison and Tiedemann , 2016 ) , 1 DGT ( Tiedemann , 2012 ; Steinberger et al. , 2012 ) , JW300 ( Agi? and Vuli? , 2019 ) , Europarl v10 ( Koehn , 2005 ) , News - Commentary v15 , and WMT - News 2 for building the BPE vocabularies .
The monolingual Upper Sorbian Web and Witaj datasets 3 were filtered to remove lines containing characters that had not been observed in the Upper Sorbian parallel data or in the Czech data ; this removed sentences that contained text in other scripts and other languages .
The Czech - German data was used for training parent models , while monolingual German and Upper Sorbian were used ( along with parallel German - Upper Sorbian data ) for training child models .
A table of data sizes and how they were used is shown in Appendix A .
Preprocessing
We build BPE vocabularies of size 2 k , 5 k , 10k , 15 k , and 20 k using subword - nmt 4 ( Sennrich et al. , 2016 b ) .
After building the vocabulary , we add a set of 25 generic tags , plus a special backtranslation tag " < BT > " , which we use in future experiments for indicating when training data has been backtranslated ( Caswell et al. , 2019 ) .
We also add all Moses and Sockeye special tags ( ampersand , < unk > etc. ) to a glossary file used for applying BPE , which prevents them from being segmented .
Because there is so much more Czech data than Upper Sorbian data , we duplicate the in-domain parallel hsb- de data and the monolingual HSB data 25 times when training BPE in order to make sure that HSB data is adequately represented ( and not We ran BPE - dropout with a rate of 0.1 over the training data 5 times using the same BPE merge operations , vocabularies and glossaries as before , concatenating these variants to form an extended training set .
Software and Systems
We used Sockeye 's ( Hieber et al. , 2018 ) implementation of Transformer ( Vaswani et al. , 2017 ) with 6 layers , 8 attention heads , network size of 512 units , and feedforward size of 2048 units .
We have changed the default gradient clipping type to absolute , used the whole validation set during validation , an initial learning rate of 0.0001 , batches of ?8192 tokens / words , maximum sentence length of 200 tokens , optimizing for BLEU .
Parent systems used checkpoint intervals of 2500 and 4000 .
Child system checkpoint intervals varied from 65 to 4000 to balance frequent checkpointing with efficiency .
Decoding was performed with beam size 5 .
Results and Discussion 6.1 BPE - Dropout in Transfer Learning Provilkov et al. ( 2020 ) examine BPE - dropout when building translation systems for individual language pairs .
Here we apply it in a transfer learning setting , raising the question of whether BPEdropout should be applied to the parent system , the child system , or both , as well as the question of using source - side BPE - dropout or both source - and target - side BPE - dropout .
Our results for this are somewhat mixed , owing in part to the relatively small BLEU gains produced by BPE - dropout ( as compared to backtranslation ) .
In Table 2 we show BLEU scores for German - Upper Sorbian translation with a 10k vocabulary and no backtranslation .
The most promising systems in that experiment are those with source - side BPE - dropout in the child system , with either both side or source -side dropout in the parent .
In the 20k vocabulary DE - HSB setting with second iteration backtranslation , we saw a similar effect , with source BPE - dropout for both parent and child having a BLEU score of 58.4 on devel test , + 1.1 above the second - best system ( no BPE - dropout in parent or child ) .
Results in the other translation direction were more ambiguous , leaving room for future analysis of BPE - dropout in the transfer learning setting .
As a result of these experiments , many of the systems we used in our final ensembles were trained with source - side BPE - dropout , though when it appeared promising we also ensembled with systems without BPE - dropout .
Iterative Backtranslation
We performed two rounds of backtranslation of Upper Sorbian monolingual data and German monolingual data described in Section 2.5 .
The first round ( BT1 ) used our strongest system without backtranslation , while the second round ( BT2 ) used our strongest system including backtranslated data from the first round .
We ran experiments sweeping BPE vocabulary sizes and backtranslated corpora ; for German news we experimented 300k and 600k subsets as well as the full 1.2 M line random subselection .
In all experiments the 60 k sentence - pair parallel HSB - DE corpus was replicated a number of times to approximately match the included backtranslated data in number of lines .
The second round of backtranslation of the Upper Sorbian monolingual data improved the BLEU score by 0.7 BLEU points for the best configuration , with the vocabulary size of the best configuration increasing to 20 k from 15 k .
However , the second round of backtranslation of the German monolingual data did not improve the subsequent HSB - DE systems , instead showing a drop of 0.1 BLEU points ; our final system ( Section 6.5 ) uses a mix of systems trained using BT1 and BT2 .
For full details of the systems used for backtranslation , see Appendix B .
The final line shows the submitted primary systems and their performance on devel test .
Generating multiple translation for backtranslation ( i.e. multiple source sentences for each authentic target sentence ) is known to improve translation quality ; all of the systems we have described here used a single backtranslation per target sentence .
After the submission of our final systems , we experimented with backtranslation using n-best translations of the monolingual text .
In both directions , we found that building student systems using the 10 - best backtranslation list generated with sampling from the softmax 's top - 10 vocabulary ( rather than taking the max ) , but without BPE - dropout , produced improvements of around 0.2-0.8 BLEU .
5
The resulting systems had comparable BLEU scores to the systems trained with single variant backtranslation and BPE - dropout ; we leave as future work an examination of the result of combining multiple backtranslations with BPE - dropout .
Ablation
Here we first discuss the impact of our non-pseudo- Sorbian approaches : BPE - dropout , backtranslation , and transfer learning , showing how each contributed to the final systems used for ensembling .
Table 3 shows ablation experiments for DE - HSB ( 20 k vocabulary ) and HSB - DE ( 10 k vocabulary ) .
6
In the first four lines , we consider training a system without transfer learning , starting from a base - line built using only the parallel Upper Sorbian - German data .
Despite the small data size , and perhaps due to the close match between training and test data , this baseline has high BLEU scores on the devel test set : 44.2 ( DE - HSB ) and 44.1 ( HSB - DE ) .
Adding BPE - dropout to this setting ( with 5 runs of the algorithm ) results in a modest improvement ( + 0.2 BLEU for DE - HSB and + 0.6 BLEU for DE - HSB ) .
If we instead add backtranslated data ( translated in our second iteration of backtranslation ) , we see a much larger jump of + 10.7 and + 10.6 BLEU respectively over the baselines ; note that this also represents a huge increase in available data for training .
Combining the two approaches adds an additional + 1.2 and 0.3 BLEU , respectively .
In fact , these systems outperform both a parentchild baseline and a parent-child system with BPEdropout , highlighting the importance of incorporating additional target - side monolingual data in the low-resource setting .
Once we combine backtranslation we see a moderate improvement over the child systems with BPE - dropout ( + 2.6 and + 2.4 BLEU , respectively ) .
Again , combining BPEdropout and backtranslation still produces more improvement , as does eventual ensembling .
Due to time constraints , we did not run a full ablation study of word , character and combined pseudo-Sorbian .
Our initial results ( run with an earlier version of character pseudo- Sorbian , and a differently extracted BPE vocabulary ) found for the HSB - DE direction that word pseudo-Sorbian slightly outperformed ( on the order of 0.5 BLEU ) character pseudo-Sorbian for 10 k vocabulary , but was comparable for 2 k and 5 k vocabulary sizes ; these results are given in Appendix C .
The combination of the two had slightly higher scores across those three vocabulary sizes ( ranging from + 0.1 to + 0.6 BLEU ) than either of the two individual approaches , so we used the combination for the remaining experiments .
Our final German- Upper Sorbian system is an ensemble of four systems , with vocabulary size of 20 k merges .
All child models ensembled were trained on second iteration backtranslated monolingual HSB data ( all available , filtered ) and 12 replications of the de-hsb parallel text , with backtranslation tags .
Final German - Upper Sorbian System 1 . Child without BPE - dropout , de-cs parent without BPE - dropout .
2 . Child with source side BPE - dropout , de-cs parent with source side BPE - dropout 3 .
Child without BPE - dropout , pseudo-hsb- de parent without BPE - dropout .
4 . Child with source side BPE - dropout , pseudohsb - de parent with source side BPE - dropout
The system scores on devel test are shown in Table 4 .
The best scoring individual systems were transfer learning systems with source- side BPE - dropout , with the one using pseudo-Sorbian falling slightly behind the non-pseudo-Sorbian by 0.2 BLEU points .
Without BPE - dropout , the best pseudo-Sorbian system shown here outperforms its corresponding non-pseudo-Sorbian system by approximately 0.1 BLEU .
On the test set , this system had scores of ( as computed by the Matrix submission ) 57.3 BLEU - cased , TER ( Snover et al. , 2006 ) of 0.3 , BEER 2.0 ( Stanojevi ? and Sima'an , 2014 ) of 0.754 , and CharacTER ( Wang et al. , 2016 ) of 0.255 .
This was 3.4 BLEU - cased behind the best-scoring system ( SJTU - NICT ) , but within 0.6 BLEU of the second - and third - highest scoring systems ( University of Helsinki ) ; it was also tied with the third - highest scoring system ( University of Helsinki ) in terms of CharactTER .
The final Upper Sorbian - German system is an ensemble of systems with a BPE vocabulary of 10 k merges .
Final Upper Sorbian - German System 1 . Child with source side BPE - dropout , 20 times hsb- de data , 1.2 M lines of first iteration backtranslated news data ; cs- de parent with source side BPE - dropout 2 .
Child without BPE - dropout , 25 times hsb- de data , news commentary ( NC ) and 1.2 M lines of news second iteration backtranslated ; 7 csde parent without BPE - dropout 3 .
Child without BPE - dropout , 25 times hsb- de data , NC and 1.2 M lines of news first iteration backtranslated ; pseudo-hsb- de parent without BPE - dropout 4 .
Child with source side BPE - dropout , 25 times hsb- de data , NC and 1.2 M lines of news first iteration backtranslated ; pseudo-hsb- de parent with source side BPE - dropout 5 .
Child without BPE - dropout , 20 times hsb- de data and 1.2 M lines of second iteration backtranslated news data ; pseudo-hsb - de parent without BPE - dropout Table 5 shows that the five systems combined were very comparable in BLEU scores ( 57.1 and 57.2 ) , but their ensembled BLEU score showed an improvement of ?1.7 BLEU over each individual score .
The final ensemble had a BLEU - cased score of 58.9 on the test data ( calculated by the Matrix submission systems ) , a TER of 0.29 , a BEER 2.0 of 0.579 , and a CharacTER score of 0.268 .
This represented a - 0.7 BLEU - cased difference off of the best system ( University of Helsinki ) , but only a - 0.001 CharactTER difference .
Discussion
We experimented with a variety of ensembles , and found that our strongest ensembles were those that included both the pseudo-Sorbian systems and those built without pseudo-Sorbian .
In initial experiments with Upper Sorbian - German systems , with vocabulary size 5 k , we found that adding pseudo-Sorbian systems to ensembles produced improvements even if the pseudo- Sorbian system did not have quite as high of a BLEU score as the systems built without it .
For example , combining the top three systems without pseudo-Sorbian ( BLEU scores of 57.3 , 57.2 , and 57.0 , respectively ) or the top two of those systems resulted in ensemble system BLEU scores of 57.9 .
Replacing the thirdbest system with a pseudo-Sorbian system with a BLEU score of 56.6 resulted in an improved ensemble BLEU score of 58.5 .
Diverse ensembles ( e.g. , different architectures or runs ) are known to outperform less diverse ensembles ( e.g. , ensembles of checkpoints ) for neural machine translation ( Farajian et al. , 2016 ; Denkowski and Neubig , 2017 ; .
While diversity of models for ensembling is usually discussed in terms of model architecture or seeding of multiple runs , we could argue that the use of lexically modified training data could constitute another form of model diversity , contributing to a stronger ensembled model .
For baseline systems trained only on the parallel data , smaller vocabulary sizes performed best , as expected ( given only 60,000 lines of text , large vocabulary sizes may contain many tokens that are only observed a small number of times ) .
As we added transfer learning , backtranslation , and eventually ensembling , the best systems were those with slightly larger vocabulary sizes .
In the Upper Sorbian - German translation direction , some of our best performing systems that did not use pseudo- Sorbian were found with a 5 k vocabulary size , while 10 k was generally better for the pseudo-Sorbian systems .
We tried ensembles with both 5 k and 10 k that included pseudo-Sorbian and nonpseudo- Sorbian systems , and found the best results with 10k .
Conclusions
In this work , we demonstrated that transfer learning , BPE - dropout , and backtranslation all provide improvements for this low-resource setting .
Our experiments on lexical modifications , building pseudo- Sorbian text for training parent models , performed approximately on - par with standard transfer learning approaches , and could be trivially combined with BPE - dropout .
While the lexical modification approach did not outperform the standard transfer learning setup , we found that it still improved ensembles , possibly due to the increase in system diversity .
A Data
B Backtranslation Details
The configurations used to backtranslate the first round were : ?
For monolingual Upper Sorbian , the HSB - DE child system with 5 k vocabulary size and both source and target side BPE - dropout for both the HSB - DE system and its CS - DE parent ( 53.4 BLEU on devel test ) ?
For monolingual German , the DE- HSB child with 10 k vocabulary size and both source and target side BPE - dropout for both the DE - HSB system and its DE - CS parent ( 55.0 BLEU on devel test ) .
The following configurations were used to backtranslate the second round : ?
For monolingual Upper Sorbian , the HSB - DE child system with 5 k vocabulary size and source side BPE - dropout for both the HSB - DE system and its CS - DE parent ; 25 times hsb- de data , DE news commentary and 1.2 M lines of DE news backtranslated ( 57.25 BLEU on devel test ) ?
For monolingual German , the DE- HSB system with 15 k vocabulary size and source side BPE - dropout for both the DE - HSB system and its DE - CS parent ; 12 times hsb- de data , HSB Sorbian Institute , Witaj , and Web data backtranslated ( 57.7 BLEU on devel test ) .
After the second round of backtranslation , the top configurations were : ?
For HSB - DE , the 5 k vocabulary size child with source side BPE - dropout for both the HSB - DE system and its CS - DE parent ; 20 times hsb- de data , 1.2 M lines of ( second round ) backtranslated DE news ( 57.15 BLEU on devel test ) ?
For monolingual German , the 20k vocabulary size child with source side BPE - dropout for both the DE - HSB system and its DE - CS parent ; 12 times hsb- de data , backtranslated ( second round ) HSB Sorbian Institute , Witaj , and Web data ( 58.4 BLEU on devel test ) .
We note that the second round of backtranslating the German monolingual news data into Upper Sorbian did not improve the BLEU score for the subsequent HSB - DE systems , with the best configuration dropping by 0.1 BLEU points .
However , the second round of backtranslation of the Upper Sorbian monolingual data did improve the BLEU score by 0.7 BLEU points for the best configuration , with the vocabulary size of the best configuration increasing to 20 k from 15k .
C Pseudo-Sorbian Comparisons and Analysis Table 7 presents the results of our pseudo-Sorbian comparison discussed in Sections 2.4 and 6.3 ; as mentioned ; we find that both word - and characterlevel modifications are similar at small vocabulary sizes , but that word-level modification outperforms at a higher vocabulary size .
However , at all vocabulary sizes a combination of the two improves over either approach on its own .
It should be noted again that these preliminary results are not directly comparable to other results in this paper ( having trained on a smaller corpus , lacking the JW300 documents ) and are also not technically constrained ( as the word list used to create the character - level replacement was from bilingual dictionaries , not the constrained corpora ) .
In our submitted systems , we created a new characterlevel system using only the constrained corpora .
As pseudo-Sorbian lexical modification creates a new training corpus , this raises questions of how to appropriately create BPE vocabularies , in particular when the character - level version is used .
In wordlevel pseudo-Sorbian , the resulting corpus still only consists of words found in the original Czech and Upper Sorbian corpora , although the resulting ngram frequencies will differ somewhat because of some Czech words being replaced by Upper Sorbian ones .
Character - level pseudo-Sorbian , however , can create words and character - level n-grams that do not appear in the original corpus at all .
The systems in Table 7 use system-specific BPE ; that is , the BPE operations and vocabulary are constructed for each specific { pseudo -Sorbian , Upper Sorbian } training corpus .
However , in the final submitted systems , we used a fixed vocabulary from the original { Czech , Upper Sorbian } corpus , which made it possible to ensemble pseudo- Sorbian systems with our other systems , giving us better results than either type of system alone .
We do not know what effect ( negative or positive ) this may have on the quality of the pseudo- Sorbian - trained systems ( since they would be using a BPE vocabulary for a different set of " languages " , and thus may be over-segmented ) .
9
This raises a number of questions about appropriate choices of BPE models , which increases the complexity of ablation studies beyond what we are able to address in the scope of this paper .
Setting aside the complications of various BPE 9 Using our final BPE segmentation does result in a slightly higher number of segmentations per token than a BPE model trained directly on the pseudo-Sorbian ( combined version ) data .
training schemes , we return to the BPE segmentations used in our final systems to analyze whether pseudo-Sorbian and BPE - dropout do indeed achieve their goals of producing more overlap between the pseudo-Sorbian or Czech training data and the Upper Sorbian data .
We consider the devel test portion of the Upper Sorbian data .
With a 10 k BPE vocabulary , that test set contains 4540 unique subword types .
62.6 % of those types ( 2840 ) are observed in the baseline Czech parent model training data , and 52.9 % of the training tokens are in that set .
After applying BPE - dropout to the Czech parent training data , the percentage of observed types increases slightly , to 63.4 % ( 2878 ) , with 58.9 % of the training tokens in that set .
With the pseudo-Sorbian combined system , however , we see a much bigger increase in type overlap : 89.0 % of the Upper Sorbian devel test types ( 4041 ) were observed at least once in the pseudo- Sorbian parent data , making up 70.9 % of the training tokens .
Increased coverage of Upper Sorbian devel test subword tokens during parent training means that embeddings for those subword tokens will be updated during parent model training , hopefully in a way that improves their warm start in the Upper Sorbian student training .
10
Table 1 : 1 Upper Sorbian analyzovat analyzowa ?
donesl donjes ?
extern?ch eksternych hospod ?sk ?
hospodarsce kreativn ?
kreatiwne okres wokrjes potom potym projekt projekt s?mantick ?
semantisku velk ? m wulkim
A sample of probable Czech - Upper Sorbian cognates and shared loanwords , mined from the Czech -German and German - Upper Sorbian parallel corpora and filtered by Levenshtein distance .
Table 2 : 2 Comparison of BPE - dropout use in both parent and child systems for 10 k vocabulary DE - HSB translation ( measured on devel test set ) , without backtranslation .
All parent systems were trained on the German - Czech data , while child systems trained on the parallel DE - HSB data .
None involves no BPE - dropout , source applies BPE - dropout to the source side only , and both applies it to both the source and the target .
Child Dropout None Source Both None 54.6 54.5 54.3 Parent Dropout Source 55.0 55.5 54.2 Both 54.9 55.5 55.0 overwhelmed by Czech data ) in training the en- coding .
After training BPE , we extract ( and fix for the remainder of our experiments ) a single DE vocabulary and a single HSB - CS vocabulary , cov- ering all the relevant data used to train BPE for that language pair .
Table 4 : 4 System BLEU 1 . Child + BT2 57.7 2 . Child + Src. BPE -Dr. + BT2 58.4 3 . Pseudo-Sorbian + Child + BT2 57.8 4 . Pseudo . + Child + Src. BPE - Dr. + BT2 58.2 Ensemble 59.4 Primary German-Upper Sorbian ensemble submission BLEU score on devel test , with scores of each of its individual component systems .
The system numbers correspond to the list in Section 6.4 .
Table 5 : 5 Primary Upper Sorbian - German ensemble submission BLEU score on devel test , with scores of each of its individual component systems .
The numbers correspond to the list in Section 6.5 .
System BLEU 1 . Child + BPE -Dr. + BT1 57.2 2 . Child + BT2 57.1 3 . Pseudo . + Child + BT1 57.2 4 . Pseudo . + Child + BPE-dr. + BT1 57.1 5 . Pseudo . + Child + BT2 57.1 Ensemble 58.9
Table 6 shows the data sizes , including the size after filtering for the monolingual Upper Sorbian data , as well as how each dataset was used for BPE training and vocabulary extraction , parent training , and / or child training .
Table 6 : 6 8 Data and how it was used , whether for BPE training and vocabulary extraction , parent model training , or child model training .
Note that the monolingual German news .
2019 data was subsampled , and the number of lines shown represents the full set from which the subsample was drawn .
Data Lines BPE / Voc. Parent Child train.hsb-de.{de , hsb} 60,000 Y ?25 N Y sorbian institute monolingual.hsb 339,822 Y ?25 N Y web monolingual filtered.hsb 131,047 Y ?25 N Y witaj monolingual filtered .hsb 220,564 Y ?25 N Y OpenSubtitles.cs-de.{de , cs }
16,378,674 Y Y N DGT.cs-de.{de , cs } 4,853,298 Y Y N JW300.{ de , cs }
1,155,056 Y Y N Europarl.cs-de.{de , cs } 568,572 Y Y N News-Commentary.cs-de.{de , cs } 185,127 Y Y N WMT-News.cs-de.{de , cs } 20,567 Y Y N news.2019.de.shuffled.deduped.de
57,622,797 N N Y news-commentary-v15.dedup.de 233,111 N N Y Pseudo-Sorbian BPE 2 k BPE 5 k BPE 10 k Word-level 51.8 52.6 52.6 Character-level 51.9 52.6 52.1 Both 52.4 52.7 52.8
Table 7 : 7 Comparison of approaches to create Pseudo-Sorbian corpora for pre-training , by word- level or character - level replacement of Czech text , at different vocabulary sizes .
All scores represent BLEU scores on dev-test , in the HSB - DE direction .
http://www.opensubtitles.com 2 http://www.statmt.org/wmt20/ translation-task.html 3 http://www.statmt.org/wmt20/unsup_ and_very_low_res / 4 https://github.com/rsennrich/ subword-nmt
Authentic bitext was upsampled to keep the ratio identical to our prior experiments .
6
Smaller vocabulary sizes perform better on the baseline experiments , but the trends remain the same , so we show results for our final vocabulary sizes .
This version of the second iteration backtranslation differs slightly from that used in the remainder of the experiments , in that UNKs ( tokens representing unknown words ) were not filtered out .
In future work , it would probably be beneficial to guide the output of the modification with a character - level language model trained on target - language data , to better avoid the generation of n-grams that are unlikely or unattested in the target language .
While we could imagine that in some situations , they might end up with inappropriate representations , we expect those to be improved when the tokens are observed in student model training .
