title
Multi-Source Neural Translation
abstract
We build a multi-source machine translation model and train it to maximize the probability of a target English string given French and German sources .
Using the neural encoderdecoder framework , we explore several combination methods and report up to + 4.8 Bleu increases on top of a very strong attentionbased neural translation model .
1 Introduction Kay ( 2000 ) points out that if a document is translated once , it is likely to be translated again and again into other languages .
This gives rise to an interesting idea : a human does the first translation by hand , then turns the rest over to machine translation ( MT ) .
The translation system now has two strings as input , which can reduce ambiguity via " triangulation " ( Kay 's term ) .
For example , the normally ambiguous English word " bank " may be more easily translated into French in the presence of a second , German input string containing the word " Flussufer " ( river bank ) .
Och and Ney ( 2001 ) describe such a multi-source MT system .
They first train separate bilingual MT systems F ?E , G?E , etc .
At runtime , they separately translate input strings f and g into candidate target strings e 1 and e 2 , then select the best one of the two .
A typical selection factor is the product of the system scores .
Schwartz ( 2008 ) revisits such factors in the context of log-linear models and Bleu score , while Max et al . ( 2010 ) re-rank F ?E n-best lists using n-gram precision with respect to G?E translations .
Callison -Burch ( 2002 ) exploits hypothesis selection in multi-source MT to expand available corpora , via co-training .
Others use system combination techniques to merge hypotheses at the word level , creating the ability to synthesize new translations outside those proposed by the single-source translators .
These methods include confusion networks ( Matusov et al. , 2006 ; Schroeder et al. , 2009 ) , source-side string combination ( Schroeder et al. , 2009 ) , and median strings ( Gonz?lez-Rubio and Casacuberta , 2010 ) .
The above work all relies on base MT systems trained on bilingual data , using traditional methods .
This follows early work in sentence alignment ( Gale and Church , 1993 ) and word alignment ( Simard , 1999 ) , which exploited trilingual text , but did not build trilingual models .
Previous authors possibly considered a three - dimensional translation table t( e|f , g ) to be prohibitive .
In this paper , by contrast , we train a P( e|f , g ) model directly on trilingual data , and we use that model to decode an ( f , g ) pair simultaneously .
We view this as a kind of multi-tape transduction ( Elgot and Mezei , 1965 ; Kaplan and Kay , 1994 ; Deri and Knight , 2015 ) with two input tapes and one output tape .
Our contributions are as follows : ?
We train a P( e|f , g ) model directly on trilingual data , and we use it to decode a new source string pair ( f , g ) into target string e. ?
We show positive Bleu improvements over strong single-source baselines . ?
We show that improvements are best when the two source languages are more distant from each other .
We are able to achieve these results using A B C < EOS >
W X Y Z < EOS > Z Y X W Figure 1 : The encoder-decoder framework for neural machine translation ( NMT ) .
Here , a source sentence C B A ( presented in reverse order as A B C ) is translated into a target sentence W X Y Z .
At each step , an evolving realvalued vector summarizes the state of the encoder ( white ) and decoder ( gray ) .
the framework of neural encoder-decoder models , where multi-target MT ( Dong et al. , 2015 ) and multi-source , cross-modal mappings have been explored ( Luong et al. , 2015 a ) .
Multi-Source Neural MT
In the neural encoder-decoder framework for MT ( Neco and Forcada , 1997 ; Casta?o and Casacuberta , 1997 ; Bahdanau et al. , 2014 ; Luong et al. , 2015 b ) , we use a recurrent neural network ( encoder ) to convert a source sentence into a dense , fixed - length vector .
We then use another recurrent network ( decoder ) to convert that vector in a target sentence .
1
In this paper , we use a four-layer encoder- decoder system ( Figure 1 ) with long short - term memory ( LSTM ) units ( Hochreiter and Schmidhuber , 1997 ) trained for maximum likelihood ( via a softmax layer ) with back - propagation through time ( Werbos , 1990 ) .
For our baseline single-source MT system we use two different models , one of which implements the local attention plus feed - input model from Luong et al . ( 2015 b ) .
Figure 2 shows our approach to multi-source MT .
Each source language has its own encoder .
The question is how to combine the hidden states and cell states from each encoder , to pass on to the decoder .
Black combiner blocks implement a function whose input is two hidden states ( h 1 and h 2 ) and two cell states ( c 1 and c 2 ) , and whose output is a single hid -den state h and cell state c.
We propose two combination methods .
Basic Combination Method
The Basic method works by concatenating the two hidden states from the source encoders , applying a linear transformation W c ( size 2000 x 1000 ) , then sending its output through a tanh non-linearity .
This operation is represented by the equation : h = tanh W c [ h 1 ; h 2 ] ( 1 ) W c and all other weights in the network are learned from example string triples drawn from a trilingual training corpus .
The new cell state is simply the sum of the two cell states from the encoders .
c = c 1 + c 2 ( 2 ) We also attempted to concatenate cell states and apply a linear transformation , but training diverges due to large cell values .
Child - Sum Method
Our second combination method is inspired by the Child- Sum Tree -LSTMs of Tai et al . ( 2015 ) .
Here , we use an LSTM variant to combine the two hidden states and cells .
The standard LSTM input , output , and new cell value are all calculated .
Then cell states from each encoder get their own forget gates .
The final cell state and hidden state are calculated as in a normal LSTM .
More precisely : i = sigmoid W i 1 h 1 + W i 2 h 2 ( 3 ) f = sigmoid W f i h i ( 4 ) o = sigmoid W o 1 h 1 + W o 2 h 2 ( 5 ) u = tanh W u 1 h 1 + W u 2 h 2 ( 6 ) c = i f u f + f 1 c 1 + f 2 c 2 ( 7 ) h = o f tanh ( c f ) ( 8 )
This method employs eight new matrices ( the W 's in the above equations ) , each of size 1000 x 1000 .
The symbol represents an elementwise multiplication .
In equation 3 , i represents the input gate of a typical LSTM cell .
In equation 4 , Each language has its own encoder ; it passes its final hidden and cell state to a set of combiners ( in black ) .
The output of a combiner is a hidden state and cell state of the same dimension .
31 A B C < EOS > W X Y Z A B C < EOS > W X Y Z < EOS > Z Y X W I J K there are two forget gates indexed by the subscript i that serve as the forget gates for each of the incoming cells for each of the encoders .
In equation 5 , o represents the output gate of a normal LSTM .
i , f , o , and u are all size - 1000 vectors .
Multi-Source Attention
Our single-source attention model is modeled off the local -p attention model with feed input from Luong et al . ( 2015 b ) , where hidden states from the top decoder layer can look back at the top hidden states from the encoder .
The top decoder hidden state is combined with a weighted sum of the encoder hidden states , to make a better hidden state vector ( ht ) , which is passed to the softmax output layer .
With input-feeding , the hidden state from the attention model is sent down to the bottom decoder layer at the next time step .
The local -p attention model from Luong et al . ( 2015 b ) works as follows .
First , a position to look at in the source encoder is predicted by equation 9 : p t = S ? sigmoid ( v T p tanh ( W p h t ) ) ( 9 ) S is the source sentence length , and v p and W p are learned parameters , with v p being a vector of dimension 1000 , and W p being a matrix of dimension 1000 x 1000 .
After p t is computed , a window of size 2D + 1 is looked at in the top layer of the source encoder centered around p t ( D = 10 ) .
For each hidden state in this window , we compute an alignment score a t ( s ) , between 0 and 1 .
This alignment score is computed by equations 10 , 11 and 12 : a t ( s ) = align( h t , h s ) exp ?( s ? p t ) 2 2 ? 2 ( 10 ) align( h t , h s ) = exp( score ( h t , h s ) ) s exp( score ( h t , h s ) ) ( 11 ) score ( h t , h s ) = h T t W a h s ( 12 )
In equation 10 , ? is set to be D/2 and s is the source index for that hidden state .
W a is a learnable parameter of dimension 1000 x 1000 .
Once all of the alignments are calculated , c t is created by taking a weighted sum of all source hidden states multiplied by their alignment weight .
The final hidden state sent to the softmax layer is given by : ht = tanh W c [ h t ; c t ] ( 13 )
We modify this attention model to look at both source encoders simultaneously .
We create a context vector from each source encoder named c 1 t and c 2 t instead of the just c t in the single-source attention model : ht = tanh W c [ h t ; c 1 t ; c 2 t ] ( 14 )
In our multi-source attention model we now have two p t variables , one for each source encoder .
We also have two separate sets of alignments and therefore now have two c t values denoted by c 1 t and c 2 t as mentioned above .
We also have distinct W a , v p , and W p parameters for each encoder .
Experiments
We use English , French , and German data from a subset of the WMT 2014 dataset ( Bojar et al. , 2014 ) .
Figure 3 shows statistics for our training set .
For development , we use the 3000 sentences supplied by WMT .
For testing , we use a 1503 - line trilingual subset of the WMT test set .
For the single-source models , we follow the training procedure used in Luong et al . ( 2015 b ) , but with 15 epochs and halving the learning rate every full epoch after the 10th epoch .
We also re-scale the normalized gradient when norm >
5 . For training , we use a minibatch size of 128 , a hidden state size of 1000 , and dropout as in Zaremba et al . ( 2014 ) .
The dropout rate is 0.2 , the initial parameter range is [ - 0.1 , +0.1 ] , and the learning rate is 1.0 .
For the normal and multi-source attention models , we adjust these parameters to 0.3 , [ -0.08 , + 0.08 ] , and 0.7 , respectively , to adjust for overfitting .
Figure 4 shows our results for target English , with source languages French and German .
We see that the Basic combination method yields a + 4.8 Bleu improvement over the strongest single -source , attention - based system .
It also improves Bleu by + 2.2 over the non-attention baseline .
The Child - Sum method gives improvements of + 4.4 and + 1.4 .
We confirm that two copies of the same French input yields no BLEU improvement .
Figure 5 shows the action of the multi-attention model during decoding .
When our source languages are English and French ( Figure 6 ) , we observe smaller BLEU gains ( up to + 1.1 ) .
This is evidence that the more distinct the source languages , the better they disambiguate each other .
Source 1 : UNK
Aspekte sind ebenfalls wichtig .
Target : UNK aspects are important , too .
Source 2 : Les aspects UNK sont ? galement importants .
Conclusion
We describe a multi-source neural MT system that gets up to + 4.8 Bleu gains over a very strong attention - based , single -source baseline .
We obtain this result through a novel encoder-vector combination method and a novel multi-attention system .
We release the code for these experiments at www.github.com/isi-nlp/Zoph RNN .
33
