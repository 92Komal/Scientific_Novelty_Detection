title
ENGINE : Energy - Based Inference Networks for Non-Autoregressive Machine Translation
abstract
We propose to train a non-autoregressive machine translation model to minimize the energy defined by a pretrained autoregressive model .
In particular , we view our non-autoregressive translation system as an inference network ( Tu and Gimpel , 2018 ) trained to minimize the autoregressive teacher energy .
This contrasts with the popular approach of training a non-autoregressive model on a distilled corpus consisting of the beam-searched outputs of such a teacher model .
Our approach , which we call ENGINE ( ENerGy - based Inference NEtworks ) , achieves state - of- the - art non-autoregressive results on the IWSLT 2014 DE -EN and WMT 2016 RO - EN datasets , approaching the performance of autoregressive models .
1
Introduction
The performance of non-autoregressive neural machine translation ( NAT ) systems , which predict tokens in the target language independently of each other conditioned on the source sentence , has been improving steadily in recent years ( Lee et al. , 2018 ; Ghazvininejad et al. , 2019 ; Ma et al. , 2019 ) .
One common ingredient in getting non-autoregressive systems to perform well is to train them on a corpus of distilled translations ( Kim and Rush , 2016 ) .
This distilled corpus consists of source sentences paired with the translations produced by a pretrained autoregressive " teacher " system .
As an alternative to training non-autoregressive translation systems on distilled corpora , we instead propose to train them to minimize the energy defined by a pretrained autoregressive teacher model .
That is , we view non-autoregressive machine trans-lation systems as inference networks Gimpel , 2018 , 2019 ; trained to minimize the teacher 's energy .
This provides the nonautoregressive model with additional information related to the energy of the teacher , rather than just the approximate minimizers of the teacher 's energy appearing in a distilled corpus .
In order to train inference networks to minimize an energy function , the energy must be differentiable with respect to the inference network output .
We describe several approaches for relaxing the autoregressive teacher 's energy to make it amenable to minimization with an inference network , and compare them empirically .
We experiment with two non-autoregressive inference network architectures , one based on bidirectional RNNs and the other based on the transformer model of Ghazvininejad et al . ( 2019 ) .
In experiments on the IWSLT 2014 DE-EN and WMT 2016 RO -EN datasets , we show that training to minimize the teacher 's energy significantly outperforms training with distilled outputs .
Our approach , which we call ENGINE ( ENerGy - based Inference NEtworks ) , achieves state - of - the - art results for non-autoregressive translation on these datasets , approaching the results of the autoregressive teachers .
Our hope is that ENGINE will enable energy - based models to be applied more broadly for non-autoregressive generation in the future .
Related Work Non-autoregressive neural machine translation began with the work of Gu et al . ( 2018a ) , who found benefit from using knowledge distillation ( Hinton et al. , 2015 ) , and in particular sequence -level distilled outputs ( Kim and Rush , 2016 ) .
Subsequent work has narrowed the gap between nonautoregressive and autoregressive translation , including multi-iteration refinements ( Lee et al. , 2018 ; Ghazvininejad et al. , 2019 ; Saharia et al. , 2020 ; Kasai et al. , 2020 ) and rescoring with autoregressive models ( Kaiser et al. , 2018 ; Wei et al. , 2019 ; Ma et al. , 2019 ; . and Saharia et al. ( 2020 ) proposed aligned cross entropy or latent alignment models and achieved the best results of all non-autoregressive models without refinement or rescoring .
We propose training inference networks with autoregressive energies and outperform the best purely non-autoregressive methods .
Another related approach trains an " actor " network to manipulate the hidden state of an autoregressive neural MT system ( Gu et al. , 2017 ; Chen et al. , 2018 ; in order to bias it toward outputs with better BLEU scores .
This work modifies the original pretrained network rather than using it to define an energy for training an inference network .
Energy - based models have had limited application in text generation due to the computational challenges involved in learning and inference in extremely large search spaces ( Bakhtin et al. , 2020 ) .
The use of inference networks to output approximate minimizers of a loss function is popular in variational inference ( Kingma and Welling , 2013 ; Rezende et al. , 2014 ) , and , more recently , in structured prediction Gimpel , 2018 , 2019 ; , including previously for neural MT ( Gu et al. , 2018 b ) .
Energy - Based Inference Networks for Non-Autoregressive NMT
Most neural machine translation ( NMT ) systems model the conditional distribution p ? ( y | x ) of a target sequence y = y 1 , y 2 , ... , y T given a source sequence x = x 1 , x 2 , ... , x
Ts , where each y t comes from a vocabulary V , y T is eos , and y 0 is bos .
It is common in NMT to define this conditional distribution using an " autoregressive " factorization ( Sutskever et al. , 2014 ; Bahdanau et al. , 2015 ; Vaswani et al. , 2017 ) : log p ? ( y | x ) = | y | t=1 log p ? ( y t | y 0:t?1 , x )
This model can be viewed as an energy - based model ( LeCun et al. , 2006 ) by defining the energy function E ? ( x , y ) = ? log p ? ( y | x ) .
Given trained parameters ? , test time inference seeks to find the translation for a given source sentence x with the lowest energy : ? = arg min y E ? ( x , y ) .
Finding the translation that minimizes the energy involves combinatorial search .
In this paper , we train inference networks to perform this search approximately .
The idea of this approach is to replace the test time combinatorial search typically employed in structured prediction with the output of a network trained to produce approximately optimal predictions Gimpel , 2018 , 2019 ) .
More formally , we define an inference network A ? which maps an input x to a translation y and is trained with the goal that A ? ( x ) ? arg min y E ? ( x , y ) .
Specifically , we train the inference network parameters ? as follows ( assuming ? is pretrained and fixed ) : ? = arg min ? x,y ?D E ? ( x , A ? ( x ) ) ( 1 ) where D is a training set of sentence pairs .
The network architecture of A ? can be different from the architectures used in the energy function .
In this paper , we combine an autoregressive energy function with a non-autoregressive inference network .
By doing so , we seek to combine the effectiveness of the autoregressive energy with the fast inference speed of a non-autoregressive network .
Energies for Inference Network Training
In order to allow for gradient - based optimization of the inference network parameters ? , we now define a more general family of energy functions for NMT .
First , we change the representation of the translation y in the energy , redefining y = y 0 , . . . , y |y | as a sequence of distributions over words instead of a sequence of words .
In particular , we consider the generalized energy E ? ( x , y ) = |y | t=1 e t ( x , y ) ( 2 ) where e t ( x , y ) = ?y t log p ? (? | y 0 , y 1 , . . . , y t?1 , x ) .
( 3 ) We use the ? notation in p ? (? | . . . ) above to indicate that we may need the full distribution over words .
Note that by replacing the y t with one- hot distributions we recover the original energy .
In order to train an inference network to minimize this energy , we simply need a network architecture that can produce a sequence of word distributions , which is satisfied by recent nonautoregressive NMT models ( Ghazvininejad et al. , 2019 ) .
However , because the distributions involved in the original energy are one-hot , it may be advantageous for the inference network too to output distributions that are one - hot or approximately so .
We will accordingly view inference networks as producing a sequence of T logit vectors z t ?
R | V | , and we will consider two operators O 1 and O 2 that will be used to map these z t logits into distributions for use in the energy .
Figure 1 provides an overview of our approach , including this generalized energy function , the inference network , and the two operators O 1 and O 2 .
We describe choices for these operators in the next section .
Choices for Operators
We now consider ways of defining the two operators that govern the interface between the inference network and the energy function .
As shown in Figure 1 , we seek an operator O 1 to modulate the way that logits z t output by the inference network are fed to the decoder input slots in the energy function , and an operator O 2 to determine how the distribution p ? (? | . . . ) is used to compute the log probability of a word in y. Explicitly , then , we O(z ) ?O( z ) ?z SX q ?q ?z STL onehot ( arg max ( z ) ) I SG onehot ( arg max ( q ) ) ? q ? z ST onehot ( arg max ( q ) ) ?q ?z GX q ? q ? z Table 1 : Let O ( z ) ? ? | V|?1 be the result of applying an O 1 or O 2 operation to logits z output by the inference network .
Also let z = z + g , where g is Gumbel noise , q = softmax ( z ) , and q = softmax ( z ) .
We show the Jacobian ( approximation ) ?O( z ) ?z we use when computing ? Loss ?z = ? Loss ?O( z ) ?O( z ) ?z , for each O( z ) considered .
rewrite each local energy term ( Eq. 3 ) as e t ( x , y ) = ?O 2 ( z t ) log p ? ( ? | O 1 ( z 0 ) , O 1 ( z 1 ) , . . . , O 1 ( z t?1 ) , x ) , which our inference networks will minimize with respect to the z t .
The choices we consider for O 1 and O 2 , which we present generically for operator O and logit vector z , are shown in Table 1 , and described in more detail below .
Some of these O operations are not differentiable , and so the Jacobian matrix ?O( z ) ?z must be approximated during learning ; we show the approximations we use in Table 1 as well .
We consider five choices for each O : ( a ) SX : softmax .
Here O( z ) = softmax ( z ) ; no Jacobian approximation is necessary .
( b) STL : straight - through logits .
Here O( z ) = onehot ( arg max i z ) .
?O( z ) ?z is approximated by the identity matrix I ( see Bengio et al . ( 2013 ) ) . ( c ) SG : straight - through Gumbel-Softmax .
Here O( z ) = onehot ( arg max i softmax ( z + g ) ) , where g i is Gumbel noise .
2 ?O( z ) ?z is approximated with ? softmax ( z + g ) ?z ( Jang et al. , 2016 ) . ( d ) ST : straight - through .
This setting is identical to SG with g = 0 ( see Bengio et al . ( 2013 ) ) . ( e) GX : Gumbel -Softmax .
Here O( z ) = softmax ( z + g ) , where again g i is Gumbel noise ; no Jacobian approximation is necessary .
4 Experimental Setup
Datasets
We evaluate our methods on two datasets : IWSLT14 German ( DE ) ? English ( EN ) and WMT16 Romanian ( RO ) ? English ( EN ) .
All data are tokenized and then segmented into subword units using byte-pair encoding ( Sennrich et al. , 2016 ) .
We use the data provided by Lee et al . ( 2018 ) for RO-EN .
Autoregressive Energies
We consider two architectures for the pretrained autoregressive ( AR ) energy function .
The first is an autoregressive sequence - to-sequence ( seq2seq ) model with attention ( Luong et al. , 2015 ) .
The encoder is a two -layer BiLSTM with 512 units in each direction , the decoder is a two -layer LSTM with 768 units , and the word embedding size is 512 .
The second is an autoregressive transformer model ( Vaswani et al. , 2017 ) , where both the encoder and decoder have 6 layers , 8 attention heads per layer , model dimension 512 , and hidden dimension 2048 .
Inference Network Architectures
We choose two different architectures : a BiLSTM " tagger " ( a 2 - layer BiLSTM followed by a fullyconnected layer ) and a conditional masked language model ( CMLM ; Ghazvininejad et al. , 2019 ) , a transformer with 6 layers per stack , 8 attention heads per layer , model dimension 512 , and hidden dimension 2048 .
Both architectures require the target sequence length in advance ; methods for handling length are discussed in Sec. 4.5 .
For baselines , we train these inference network architectures as non-autoregressive models using the standard perposition cross-entropy loss .
For faster inference network training , we initialize inference networks with the baselines trained with cross-entropy loss in our experiments .
The baseline CMLMs use the partial masking strategy described by Ghazvininejad et al . ( 2019 ) .
This involves using some masked input tokens and some provided input tokens during training .
At test time , multiple iterations ( " refinement iterations " ) can be used for improved results ( Ghazvininejad et al. , 2019 ) .
Each iteration uses partially - masked input from the preceding iteration .
We consider the use of multiple refinement iterations for both the CMLM baseline and the CMLM inference network .
3
Hyperparameters
For inference network training , the batch size is 1024 tokens .
We train with the Adam optimizer ( Kingma and Ba , 2015 ) .
We tune the learning rate in { 5e? 4 , 1e?4 , 5e?5 , 1e?5 , 5e?6 , 1e?6 } .
For regularization , we use L2 weight decay with rate 0.01 , and dropout with rate 0.1 .
We train all models for 30 epochs .
For the baselines , we train the models with local cross entropy loss and do early stopping based on the BLEU score on the dev set .
For the inference network , we train the model to minimize the energy ( Eq. 1 ) and do early stopping based on the energy on the dev set .
Predicting Target Sequence Lengths
Non-autoregressive models often need a target sequence length in advance ( Lee et al. , 2018 ) .
We report results both with oracle lengths and with a simple method of predicting it .
We follow Ghazvininejad et al . ( 2019 ) Table 3 : Test BLEU scores of non-autoregressive models using no refinement ( # iterations = 1 ) and using refinement ( # iterations = 10 ) .
Note that the # iterations = 1 results are purely non-autoregressive .
ENGINE uses a CMLM as the inference network architecture and the transformer AR energy .
The length beam size is 5 for CMLM and 3 for ENGINE .
translation using a representation of the source sequence from the encoder .
The length loss is added to the cross-entropy loss for the target sequence .
During decoding , we select the top k = 3 length candidates with the highest probabilities , decode with the different lengths in parallel , and return the translation with the highest average of log probabilities of its tokens .
Results Effect of choices for O 1 and O 2 .
Training with distilled outputs vs. training with energy .
We compared training nonautoregressive models using the references , distilled outputs , and as inference networks on both datasets .
Conclusion
We proposed a new method to train nonautoregressive neural machine translation systems via minimizing pretrained energy functions with inference networks .
In the future , we seek to expand upon energy - based translation using our method .
Figure 1 : 1 Figure 1 : The ENGINE framework trains a nonautoregressive inference network A ? to produce translations with low energy under a pretrained autoregressive energy E.
