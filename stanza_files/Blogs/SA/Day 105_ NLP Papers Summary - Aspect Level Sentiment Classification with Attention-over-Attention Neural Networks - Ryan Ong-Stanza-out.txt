title
Day 105 : NLP Papers Summary - Aspect Level Sentiment Classi cation With Attention-Over-Attention Neural Networks Objective and Contribution
abstract
Introduce an attention- over-attention ( AOA ) neural network for aspect- based sentiment analysis .
The AOA module jointly learns the representations for aspects and sentences and explicitly captures the interaction between aspects and context sentences .
The results on laptop and restaurant datasets outperforms previous LSTM - based architectures .
FINAL PREDICTION
The nal sentence representation is a weighted sum of sentence hidden states using sentence attention from AOA module , as follows : .
This nal sentence representation is feed into a linear layer with a softmax function to output probabilities of sentiment classes .
The sentiment class with the highest probability is the predicted label for the sentence , given the aspect target .
Experiments and Results
MODEL COMPARISONS
Majority
RESULTS
AOA - LSTM performed the best in comparisons to other baseline methods according to the result table
We also included the table that showcase which word contributes the most to the aspect sentiment polarity by visualising the sentence attention vectors .
Conclusion and Future Work
In the error analysis , there are cases that the model ca n't handle ef ciently .
One is complex sentiment expression .
Another is uncommon idioms .
In future work , we could incorporate sentences ' grammar structures or feed prior language knowledge to the AOA neural network .
21/02/2022 , 22:02 Day 105 : NLP Papers Summary - Aspect Level Sentiment Classification with Attention- over-Attention Neura ?
https://ryanong.co.uk/2020/04/14/day-105-nlp-research-papers-aspect-level-sentiment-classification-with-attention-over-attenti?
2/8
Experimented with two domain-speci c datasets from SemEval 2014 Task 4 : laptop and restaurant .
Accuracy is the evaluation metric .
Dataset summary is shown in the gure below : Methodology
In this task , we are given a sentence and an aspect target and our goal is to classify the sentiment polarity of the aspect target in the sentence .
There are 4 main components in the architecture shown below : word embedding , bi- LSTM , attention - over-attention ( AOA ) , and nal prediction .
WORD EMBEDDING AND BI-LSTM
Word embedding is a standardised step where we convert the text sentence and aspect target into its numerical representations .
Nothing special here .
Once we get the word vectors , we feed them into two bi-LSTM respectively , to learn the hidden semantics of words in the sentence and aspect target .
ATTENTION-OVER -ATTENTION ( AOA ) ? ?
21/02/2022 , 22:02 Day 105 : NLP Papers Summary - Aspect Level Sentiment Classification with Attention- over-Attention Neura ?
https://ryanong.co.uk/2020/04/14/day-105-nlp-research-papers-aspect-level-sentiment-classification-with-attention-over-attenti?
3/8
The next step is to calculate the attention weights for the text using the AOA module .
Here 's the following steps :
1 . Calculate a pair-wise interaction matrix between the two hidden states , where the value of each entry represents the correlation of a word pair between sentence and target 2 .
Perform column - wise softmax to get , target- to-sentence attention 3 .
Perform row-wise softmax to get , sentence - to- target attention 4 .
Calculate the column- wise average of to get a target - level attention , which tells us the important parts in an aspect target 5 .
The nal sentence - level attention is the weighted sum of each individual target - tosentence attention as follows : .
