title
abstract
often infeasible or expensive to manually translate large amounts of sentences for low-resource language pairs , such as Nepali-English , Sinhala - English , etc .
To this end , the goal of the so-called multilingual universal machine translation , a.k.a. , universal machine translation ( UMT ) , is to learn to translate between any pair of languages using a single system , given pairs of translated documents for some of these languages .
The hope is that by learning a shared " semantic space " between multiple source and target languages , the model can leverage language - invariant structure from high- resource translation pairs to transfer to the translation between lowresource language pairs , or even enable zero-shot translation .
Indeed , training such a single massively multilingual model has gained impressive empirical results , especially in the case of low-resource language pairs ( see Fig. 2 ) .
However , such success also comes with a cost .
From Fig.
2 we observe that the translation quality over high- resource language pairs by using such a single UMT system is worse than the corresponding bilingual baselines .
that perform well on any unseen language pair , i.e. , zero-shot translation is possible .
An Impossibility Theorem on UMT via Language -Invariant Representations
Suppose we have an unlimited amount of parallel sentences for each pair of languages , with unbounded computational resources .
Could we train a single model that performs well on all pairs of translation tasks based on a common representation space ?
Put it in other words , is there any information - theoretic limit of such systems for the task of UMT ?
In this paragraph we will show that there is an inherent tradeoff between the translation quality and the degree of representation invariance w.r.t. \ languages : the better the language invariance , the higher the cost on at least one of the translation pairs .
At a high- level , this result holds due to the general dataprocessing principle : if a representation is invariant to multiple source languages , then any decoder based on this representation will have to generate the same language model on the target language .
But on the other hand , the parallel corpora we use to train such a system could have drastically different sentence distributions on the target language , thus leading to a discrepancy ( error ) between the generated sentence distribution and the ground -truth sentence distribution over the target language .
To keep our discussions simple and transparent , let 's start with a basic Two - to - One setup where there are only two source languages and and one target language .
Furthermore , for each source language , let 's assume that there is a perfect translator that takes a sentence ( or string , sequence ) from and outputs the corresponding translation in .
Under this setup , it is easy to see that there exists a perfect translator in this Two - to - One task :
In words : upon receiving a sentence , simply checks which source language comes from and then call the corresponding ground -truth translator .
To make the idea of language - invariant representations formal , let be an encoder that takes a sentence ( string ) from alphabet to a representation in a vector space .
We call anuniversal language mapping if the distributions of sentence representations from different languages and are - close to each other .
In words , for some divergence measure , where is the induced distribution of sentence ( from ) representations in the shared space .
Subsequently , a multilingual system will train a decoder that takes a sentence representation and outputs the corresponding target translation in language .
The hope here is that encodes the language - invariant semantic information about the input sentence ( either from or from ) based on which to translate to the target language .
So far so good , but could we recover the perfect translator by learning a common , shared representation , i.e. , is small ?
Unfortunately , the answer here is negative if we do n't have any assumption on the parallel corpora we use to train our encoder and decoder : L 0 L 1 L L i , i ? { 0 , 1 } f * L i ?L L i L f * L f * L ( x ) = ? i?{0,1 } I ( x ? L i ) ? f * L i ?L ( x ) x f * L x g : ? * ? Z ? Z g ?
L 0 L 1 ? d( g ? D 0 , g ? D 1 ) ? ? d g ?
D i L i Z h z L z L 0 L 1 L f * L Z ? ? [ Home ] [ Submissions ]
Here the error term measures the translation performance given by the encoder-decoder pair from to over distribution .
The rst term in the lower bound measures the difference of distributions over sentences from the target language in the two parallel corpora , i.e. , and .
For example , in many practical scenarios , it may happen that the parallel corpus of high- resource language pair , e.g. , German - English , contains sentences over a diverse domain whereas as a comparison , the parallel corpus of low-resource language pair , e.g. , Sinhala - English , only contains target translations from a speci c domain , e.g. , sports , news , product reviews , etc .
In this case , despite the fact that the target is the same language , the corresponding sentence distributions from English are quite different between different corpora , leading to a large lower bound .
As a result , our theorem , which could be interpreted as a kind of uncertainty principle in UMT , says that no matter what kind of decoder we are going to use , it has to incur a large error on at least one of the translation pairs .
It is also worth pointing out that our lower bound is algorithm - independent and it holds even with unbounded computation and data .
As a nal note , realize that for xed distributions , the smaller the ( hence the better the language - invariant representations ) , the larger the lower bound , demonstrating an inherent tradeoff between language - invariance and translation performance in general .
Proof Sketch :
Here we provide a proof-by-picture ( Fig. 3 ) in the special case of perfectly language - invariant representations , i.e. , , to highlight the main idea in our proof of the above impossibility theorem .
Please refer to our paper for more detailed proof as well as an extension of the above impossibility theorem in the more general many - to -many translation setting .
Err L i ?L D i ( h ? g ) 0 ? 1 h ? g L i L D i d( D 0 ( L ) , D 1 ( L ) ) L 0 ? L L 1 -L L D i , i ? { 0 , 1 } ? ? = 0 ? [ Home ] [ Submissions ]
How can we Bypass this Limitation ?
One way is to allow the decoder to have access to the input sentences ( besides the languageinvariant representations ) during the decoding process - e.g. via an attention mechanism on the input level .
Technically , such information ow from input sentences during decoding would break the Markov structure of " input-representation - output " in Fig. 3 , which is an essential ingredient in the proof of our theorem .
Intuitively , in this case both language - invariant ( hence languageindependent ) and language - dependent information would be used .
Another way would be to assume extra structure on the distributions of our corpora , i.e. , by assuming some natural generative process capturing the distribution of the parallel corpora that are used for training .
Since languages share a lot of semantic and syntactic characteristics , this would make a lot of sense - and intuitively , this is what universal translation approaches are banking on .
In the next paragraph , we will do exactly this - we will show that under a suitable generative model , not only will there be a language - invariant representation , but it will be learnable using corpora from a very small ( linear ) number of pairs of language .
A Generative Model for UMT : A Linear Number of Translation Pairs Suf ces !
In this section we will discuss a generative model , under which not only will there be a languageinvariant representation , but it will be learnable using corpora from a very small ( linear ) number of pairs of language .
Note that there are a quadratic number of translation pairs in our universe , hence our result shows that under this generative model zero-shot translation is actually possible .
To start with , what kind of generative model is suitable for the task of UMT ?
Ideally , we would like to have a feature space where vectors correspond to the semantic encoding of sentences from different languages .
One could also understand it as a sort of " meaning " space .
Then , languagedependent decoders would take these semantic vectors and decode them as the observable sentences .
Figure 1 illustrates the generative process of our model , where we assume there is a common distribution over the feature space , from which parallel sentences are sampled and generated .
For ease of presentation , let 's rst assume that each encoder -decoder pair consists of deterministic mappings ( see our paper on extensions with randomized encoders / decoders ) .
The rst question to ask is : how does this generative model assumption circumvent our previous lower bound in the last paragraph ?
We can easily observe that under the encoder-decoder generative assumption in Figure 1 , the rst term in our lower bound , , gracefully reduces to 0 , hence even if we try to learn perfectly language - invariant representations ( ) , there will be no loss of translation accuracy using universal language mapping .
Perhaps what 's more interesting is that , under proper assumptions on the structure of , the class of encoders and decoders we learn from , by using the traditional empirical risk minimization ( ERM ) framework to learn the language - dependent encoders and decoders on a small number of language pairs , we could expect the learned encoders / decoders to well generalize on unseen language pairs as well !
Informally ,
In the theorem above , is some complexity measure of the class .
If we slightly simplify the theorem above by de ning and realizing that the path length is upper bounded by the diameter of the graph , , we immediately obtain the following intuitive result :
The above corollary says that graphs that do not have long paths are preferable .
For example , could be a star graph , where a central ( high - resource ) language acts as a pivot node .
The proof of the theorem above essentially boils down to two steps : rst , we use an epsilon-net argument to show that the learned encoders / decoders generalize on a pair of language that appears in our training corpora , and then by using the connectivity of the graph , we apply a chain of trianglelike inequalities to bound the error along the path connecting any pair of languages .
Some Concluding Thoughts
The prospect of building a single system for universal machine translation is appealing .
Compared with building a quadratic number of bilingual translators , such a single system is easier to train , L i L j ? i , j ?( 1 / ?
2 i , j ? log C ( F ) ) L L ? L = L 0 , L 1 , ? , L m = L ? H O (? m?1 k=0 ? k , k+1 ) C( F ) F ? := max ( L i , L j ) ?H ? i , j m H diam ( H ) For any pair of languages ( the parallel corpus between and may not necessarily appear in our training corpora ) , the translation error between and is upper bounded by . build , deploy , and maintain .
More importantly , this could potentially allow the system to transfer some common knowledge in translation from high- resource languages to low-resource ones .
However , such promise often comes with a price , which calls for proper assumptions on the generative process of the parallel corpora used for training .
Our paper takes a rst step towards better understanding the tradeoff in this regard and proposes a simple setup that allows for zeroshot translation .
On the other hand , there are still some gaps between theory and practice .
For example , it would be interesting to see whether the BLEU score , a metric used in the empirical evaluation of translation quality , bears a similar kind of lower bound .
Also , could we further extend our generative modeling of sentences so that there are more hierarchical structures in the semantic space ?
Empirically , it would be interesting to implement the above generative model [ Submissions ] [ About ] ? Figure 1 : 1 Figure 1 : An encoder- decoder generative model of translation pairs , which helps to circumvent the limitation discussed before .
There is a global distribution over the representation space , from which sentences of language are generated via decoder .
Similarly , sentences could also be encoded via to .
Figure 2 : 2 Figure 2 : Translation quality by using a single massively multilingual model against bilingual baselines that are trained for each one of the 103 language pairs .
While the translation performances over low resource languages increase , the performances over high resource languages decrease .
Our work provides a theoretical explanation for this empirical phenomenon .
