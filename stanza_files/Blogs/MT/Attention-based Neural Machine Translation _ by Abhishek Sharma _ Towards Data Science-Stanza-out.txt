title
abstract
NMT directly models the conditional probability p( y /x ) of translating a source ( x1 , x2 ?. xn ) sentence into a target sentence ( y1,y2 ?.yn ) .
NMT consist of two components : 1 . An encoder which computes a representation S for each source sentence 2 .
A decoder which generates translation one word at a time and hence decomposes the conditional probability as : A probability of translation y given the source sentence x
One could parametrize the probability of decoding each word y( j ) as where h( j ) could be modeled as
RNN hidden unit definition ( h ) where g : a transformative function that outputs a vocabulary size vector h : RNN hidden unit f : computes the current hidden state given the previously hidden state .
The training objective for the translation process could be framed as
Loss Function
What makes NMT so popular ?
1 . NMT has achieved the state of the art performances in large scale translation tasks like English to French / German .
2 . NMT requires minimal domain knowledge and is conceptually very simple .
3 . NMT has a small memory footprint as it does not store gigantic phase tables and language models .
4 . NMT has the ability to generalize well to very long word sentences .
Difference between attention and non-attention based networks
In most of the non-attention based RNN architecture source representation , S is used only once to initialize the decoder hidden state .
[ In Figure 1 Both the attention based methods have the following common steps :
1 . Both approaches first take as input the hidden state h( t ) at the top layer of a stacking LSTM .
( The brown cell / The target state of the decoder ) 2 . Derive c ( t ) to capture relevant source side information to help predict y( t ) ( Top blue cell ) . c( t ) is basically the context that you have built for every word depending upon its alignment weights and hidden state of encoders .
Compute h_bar ( t ) from a simple concatenation of h( t ) and c ( t ) ( Top grey cell ) .
In contrast to the non-attention based architectures where only the final output of the encoder is provided to the decoder , h_bar ( t ) has access to all the states of hidden states of the encoder which provides an informative view of the source sentence .
4 . The attentional vector is transformed using the softmax layer to produce the predictive distribution .
We are using the softmax layer as we have to found the most probable word from all the available words in our vocabulary .
The above para explains a barebone architecture of the attention based networks .
In the following para , we will understand how is
Global Attention Global attention takes into consideration all encoder hidden states to derive the context vector ( c ( t ) ) .
In order to calculate c ( t ) , we compute a ( t ) which is a variable length alignment vector .
The alignment vector is derived by computing a similarity measure between h( t ) and h_bar ( s ) where h( t ) is the source hidden state while h_bar ( s ) is the target hidden state .
Similar states in encoder and decoder are actually referring to the same meaning .
Alignment Vector ( a ( t ) )
The alignment vector ( a( t , s ) ) is defined as The score is a content - based function for which any of the following alternatives could have been used :
The score function
Through score function , we are trying to calculate the similarity between the hidden states of the target and the source .
Intuitively similar states in hidden and source refer to the same meaning but in different languages .
The connecting lines in Figure 4 represent the interdependent variables .
For example 1 . a( t ) is dependent on h( t ) and h_bar ( s ) 2 . c( t ) is dependent on a ( t ) and h_bar ( s )
h_bar ( t ) is dependent on c ( t ) and h( t ) 2 . Local Attention As Global attention focus on all source side words for all target words , it is computationally very expensive and is impractical when translating for long sentences .
To overcome this deficiency local attention chooses to focus only on a small subset of the hidden states of the encoder per target word .
Local attention has the following steps which are different from what is present in global attention : 1 . The model first generates an aligned position p( t ) for each target word at time t.
In contrast to the global attention model where we assume monotonic alignment , we learn aligned positions in local attention .
In other words apart from learning translations you also learn if the order of translation is different from the source sentence ( word 1 of the source could be word 4 in the translated sentence , hence we need to calculate this otherwise our similarity score will be all wrong as our attention will be focussed on a word in the source sentence which is not related to word 1 of source sentence ) .
2 . The context vector ( c( t ) ) is derived as a weighted average over the set of source hidden states within the window [ p ( t ) - D , p ( t ) + D ] ; D is empirically selected .
As compared to the global alignment vector local alignment vector a ( t ) is now fixed dimensional .
Until now we have assumed that both the translated and source sentence are monotonically aligned .
On the basis of this , we have a further differentiation of the local attention which is as follows : To favor alignment position p ( t ) , we place a Gaussian distribution centered around p ( t ) .
This gives more weight to the position p ( t ) .
We modify our alignment weights as assumed to be the same ) and local -p ( where we calculate the p ( t ) ) .
Input-feeding Approach
In the proposed attention mechanisms the attention decisions are made independently ( previously predicted alignments does not influence next alignment ) which is suboptimal .
In order to make sure that future alignment decisions take into consideration past alignment information h_bar ( t ) is concatenated with inputs at the next time steps as illustrated .
This is done so as to : Neural Machine Translation | by Abhishek Sharma | Towards Data Science https://towardsdatascience.com/attention-based-neural-machine-translation-b5d129742e2c 2/7
the decoder has access only to the last layer of the encoder ]
On the other hand , attention - based networks refer to a set of source hidden states S throughout the translation process .
[ In Figure 2the decoder has access to all the hidden states of the encoder ]
Types of attention mechanism Open in app 07/02/2022 , 17:32 Attention - based Neural Machine Translation | by Abhishek Sharma | Towards Data Science https://towardsdatascience.com/attention-based-neural-machine-translation-b5d129742e2c 3/7
Figure 2 : 2 Figure 2 : NMT with attention and input-feeding approach
Figure 3 : 3 Figure 3 : Hidden state of NMT architecture with global attention
the context vector c ( t ) calculated differently in local and global attention and what are the repercussions of it .
Neural Machine Translation | by Abhishek Sharma | Towards Data Science https://towardsdatascience.com/attention-based-neural-machine-translation-b5d129742e2c 5/7
Figure 4 : 4 Figure 4 : Global attentional model :
At each time step t , the model infers a variable - length alignment weight vector a ( t ) based on the current target state h( t ) and all source states h_bar ( s ) .
A global context vector , c( t ) is then computed asthe weighted average , according to a ( t ) , over all the source states .
1 . Monotonic Alignment ( local -m ) Set p ( t ) =t , which means that we are assuming that source and target sequences are roughly monotonically aligned .
Alignment vector is the same as the global alignment Open in app 07/02/2022 , 17:32 Attention - based Neural Machine Translation | by Abhishek Sharma | Towards Data Science https://towardsdatascience.com/attention-based-neural-machine-translation-b5d129742e2c 6/7
Local alignment is the same as that of global alignment 2 .
Predictive alignment ( local - p ) Instead of assuming monotonic alignments , our model predicts an aligned position as follows : Alignment Position for local -p model W ( p ) and v( p ) are models parameters which will be learned to predict positions .
S is the source sentence length p ( t ) : [ 0 , S ]
Alignment vector for local -p model to capture the same .
To summarise , global attention is computationally more expensive and is useless for long sentences while local attention focusses on D hidden states on both sides of p ( t ) to overcome this .
Local attention has 2 flavors local-m ( the source and target alignment are
