title
Machine learning for dialog state tracking : a review | the morning paper
abstract
Given what we 've been learning over the last couple of weeks , you wo n't be at all surprised to hear that following many years of building systems using hand -crafted rules and generative approaches , machine - learned methods came top in all metrics in the second DSTC .
Like every area of research , the DST problem has its own terminology .
Dialog systems that help a user complete a task are framed in terms of slots .
Ef cient operation of a spoken dialog system requires a component tha can track what has happened in a dialog , incorporating system outputs , user speech , context from previous turns , and other external information .
The output of this Dialog State Tracking ( DST ) component is then used by the dialog policy to decide what action the system should take next , and so DST is essential for the nal performance of a complete system .
?
The slots , and possible values of a slot-based dialog system specify its domain , Informable slots are slots where the user may provide a value ( perhaps to constrain a search for example )
Requestable slots are attributes which the user may ask the value of .
The dialog state comprises everything that is used when the system makes its decision about what to say next .
The drawback with hand -crafted rules is that they do n't account for uncertainty in a principled way .
The need to maintain probability distributions over what the participants of a dialog desire has been argued for since 1996 ( Pulman ) .
( SLU = Spoken Language Understanding ) .
Generative approaches use Bayesian dynamic networks to model a dialog , with the true dialog state and the true user action treated as unobserved random variables .
Bayesian inference is used to update the probability distribution over dialog states given the system 's action and a noisy observation of the true user action .
complete .
The slots inform the set of possible actions the system can take , the possible semantics of the user utterances , and the possible dialog states .
Early spoken dialog systems used hand -crafted rules for DST , keeping a single top hypothesis for each component of the dialog state , often with corresponding con dence scores .
Such systems require no data to implement , and provide an accessible method for system designers to incorporate knowledge of the dialog domain .
?
The current dominant DST methods for estimating distributions of multiple competing dialog states can be split into two categories - generative models that jointly model both the inputs ( typically the SLU hypotheses ) and the dialog state , and discriminative models that directly capture the conditional probability over dialog states , having observed the dialog up to a certain point .
Since there can be a huge number of possible dialog states ( given the slots and all their possible values ) , approximations to the full distribution are typically used , either maintaining a beam of candidate dialog states , or assuming conditional independence between components .
Generative approaches also cannot easily exploit arbitrary but potentially useful features of the dialog history .
There are two main families of discriminative DSTs : static classi ers and sequence models .
Static classi ers Static classi ers learn a probability distribution for the current state based on the sequence of observations so far .
They model the sequential nature of the input by using feature functions to summarise the sequence .
" One key issue with this approach is that it is necessary to extract a xed dimensional feature vector to summarise a sequence of arbitrary length observations . "
( Perhaps giving the network a memory might help here ? ) .
Static classi ers have been built using maximum entropy linear classi ers , neural networks , and web-style ranking models .
To get good accuracy , it has proven necessary to include value-speci c feature representations .
Metallinou et al. , for example , extract a feature vector f from an observation , and a set of vectors f for each v that has appeared in the SLU hypothesis so far .
Sequence models Sequence models inherently model the sequential nature of the problem ( and we 've seen a lot of them used in this series so far ) .
LSTMs have also been used , modeling both the sequence of turns , and the sequence of words within a turn .
Tips and tricks Delexicalizing ( is that really a word ? ? ? ) helps with generalization to unseen dialog states .
The idea is to introduce special ' slot-name ' and ' slot-value ' generic tokens , and turn a sentence such as " I 'm looking for a red car " into " I 'm looking for a slot-value slot-name . "
Delexicalized features for RNN word - based tracking can also be used to share training data across disparate domains , improving performance in each individual domain .
( Using if you do n't have a lot of training data for your target domain yet - more on this tomorrow ) .
RNNs for tracking individual slots can be combined using a softmax combination layer .
" The basic idea is to order the slots , and add a recurrent layer that selects an assignment for each slot , given the assignments selected so far . "
The paper contains a great set of references for studying some of the techniques used to build DST systems , so I encourage you to check some of them out if you are interested .
? < PREVIOUS
End-to- end learning of semantic role labeling using recurrent neural networks NEXT > Multi-domain dialog state tracking using recurrent neural networks a random walk through Computer Science research , by Adrian Colyer Made delightfully fast by Machine learning for dialog state tracking : a review JULY 6 , 2016 ~ ADRIAN COLYER
Machine learning for dialog state tracking : a review Henderson MLSLP 2015
Today we turn our attention to the task of guring out , potentially over multiple interactions with a bot , what it is the user is requesting the bot to do .
This task goes by the name of Dialog State Tracking , and it 's something that Matthew Henderson , today 's paper author , knows quite a bit about .
Henderson was the lead organiser for the second and third Dialog State Tracking Challenges ( DSTC ) .
DSTC is for spoken dialog systems , but if you 're building a chatbot you have a very similar problem , albeit without the added ambiguity of trying to gure out what the user actually said from the spoken audio .
dialog state tracking : a review | the morning paper https://blog.acolyer.org/2016/07/06/machine-learning-for-dialog-state-tracking-a-review/ 3/6
learned methods are now the state - of- the - art in DST , as demonstrated in the of ine corpus-based evaluations of the DSTCs , and in on - line trials with live users .
i.e. the scope of what it can talk about and the tasks that it can help the user 13/03/2022 , 15:19
Machine learning for dialog state tracking : a review | the morning paper the morning paper ? MENU ?
https://blog.acolyer.org/2016/07/06/machine-learning-for-dialog-state-tracking-a-review/ 2/6
The f features are intended to convey information about the correctness of the v hypothesis for the goal constraint on slot s.
13/03/2022 , 15:19
Machine learning for dialog state tracking : a review | the morning paper ?
v ? Recurrent Neural Networks ( RNNs ) have been proposed as sequential models model .
unexpectedly .
Second , it avoids the engineering task of building a separate SLU of omitting an important feature , which can degrade performance has two key bene ts : rst , it removes the need for feature design , and the risk gram features from the ASR N-best list .
This is termed word - based DST , and sentences from the ASR , without requiring SLU and instead using weighted n-such models can be applied to operating directly on the distribution over that are able to deal with high dimensional continuous input features .
Notably , v https://blog.acolyer.org/2016/07/06/machine-learning-for-dialog-state-tracking-a-review/ 5/6
