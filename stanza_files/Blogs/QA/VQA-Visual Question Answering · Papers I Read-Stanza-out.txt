title
abstract
Given an image and a free-form , open-ended , natural language question ( about the image ) , produce the answer for the image .
Link to the paper
VQA Challenge and Workshop
The authors organise an annual challenge and workshop to discuss the state - of - the - art methods and best practices in this domain .
Interestingly , the second version is starting on 27th April 2017 ( today ) .
Benefits over tasks like image captioning :
Simple , n-gram statistics based methods are not suf cient .
Requires the system to blend in different aspects of knowledge - object detection , activity recognition , commonsense reasoning etc .
Since only short answers are expected , evaluation is easier .
Dataset Created a new dataset of 50000 realistic , abstract images .
Used AMT to crowdsource the task of collecting questions and answers for MS COCO dataset ( > 200 K images ) and abstract images .
Three questions per image and ten answers per question ( along with their con dence ) were collected .
The entire dataset contains over 760K questions and 10M answers .
The authors also performed an exhaustive analysis of the dataset to establish its diversity and to explore how the content of these question - answers differ from that of standard image captioning datasets .
Highlights of data collection methodology Emphasis on questions that require an image , and not just common sense , to be answered correctly .
Workers were shown previous questions when writing new questions to increase diversity .
Answers collected from multiple users to account for discrepancies in answers by humans .
Two modalities supported : Open-ended - produce the answer multiple - choice - select from a set of options provided ( 18 options comprising of popular , plausible , random and ofc correct answer )
Highlights from data analysis
Most questions range from four to ten words while answers range from one to three words .
Around 40 % questions are " yes / no " questions .
Signi cant ( > 80 % ) inter-human agreement for answers .
The authors performed a study where human evaluators were asked to answer the questions without looking at the images .
Further , they performed a study where evaluators were asked to label if a question could be answered using common sense and what was the youngest age group , they felt , could answer the question .
The idea was to establish that a suf cient number of questions in the dataset required more than just common sense to answer .
Baseline Models random selection prior ( " yes " ) - always answer as yes .
Methods 2 - channel model ( using vision and language models ) followed by softmax over ( K = 1000 ) most frequent answers .
BoW Q + I method - concatenate BoW Q and I embeddings .
Image
LSTM Q + I , deeper LSTM Q + norm I methods - image embedding transformed to 1024 - dim using a FC layer and tanh nonlinearity followed by element - wise multiplication of image and question vectors .
Pass combined embedding to an MLP - FC neural network with 2 hidden layers ( 1000 neurons and 0.5 dropout ) with tanh , followed by softmax .
Cross-entropy loss with VGGNet parameters frozen .
The best model performs well for answers involving common visual objects but performs poorly for answers involving counts .
Vision only model performs even worse than the model which always produces " yes " as the answer .
Related Posts Question Answering ?
Papers I Read https://shagunsodhani.com/papers-I-read/VQA-Visual-Question-Answering 2/4
Question Answering ?
Papers I Read https://shagunsodhani.com/papers-I-read/VQA-Visual-Question-Answering 3/4 per Q-type prior - pick the most popular answer per question type.nearest neighbor -nd the k nearest neighbors for the given ( image , question ) pair .
Bag- of- Words representation for the questions using the top 1000 words plus the top 1 - rst , second and third words of the questions .
LSTM Q - Each word is encoded into 300 - dim vectors using fully connected + tanh nonlinearity .
These embeddings are fed to an LSTM to obtain 1024d - dim embedding .
Deeper LSTM Q - Same as LSTM
Q but uses two hidden layers to obtain 2048 - dim embedding .
Multi-Layer Perceptron ( MLP ) - Combine image and question embeddings to obtain a single embedding .
+ norm I is the best model with 58.16 % accuracy on open-ended dataset and 63.09 % on multiple - choice but far behind the human evaluators ( > 80 % and >90 % respectively ) .
Hints for Computer System Design 07 Jan 2022 Synthesized Policies for Transfer and Adaptation across Tasks and Environments 29 Mar 2021 Deep Neural Networks for YouTube Recommendations 22 Mar 2021
