title
Data Science Natural Language Processing NLP Papers Summary
abstract
Proposed a novel extractive summarisation model that combines both global and local context to summarise a single long document .
In a given long document , it usually contains various topics .
We believe that using these topics to guide summarisation could see improvement .
We utilise both the global context ( whole document ) and the local context ( section ) to determine if a sentence is informative enough to be in the summary .
The contributions are as follows : ? Natural Language Processing 365 ? ? ? ? ? ? ? 21/02/2022 , 21:41 Day 118 : NLP Papers Summary - Extractive Summarization of Long Documents by Combining Global and L ?
https://ryanong.co.uk/2020/04/27/day-118-nlp-papers-summary-extractive-summarization-of-long-documents-by-combining-gl? 2/11 https://ryanong.co.uk/2020/04/27/day-118-nlp-papers-summary-extractive-summarization-of-long-documents-by-combining-gl?
6/11 Low Lead results showcase no sentence position bias exist in scienti c papers like in news articles although there might be other position biases that comes with scienti c papers , which could be potential future work .
Our models outperformed all the other extractive models in all evaluation metrics .
The increase in performance in our models from the baseline models demonstrate the bene t of including local and global context .
The goal of our method is to effectively deal with summarising long documents .
The gure below showcase that as the document length increase , our models have a good performance gain in comparison to current SOTA models .
ABLATION STUDY
The ablation study is to study the effect of including global and local context into our summarisation approach and assess which contribute more to the overall performance .
From the results below , it seems that local topic context drives a strong improvement in the overall performance of the model .
In contrast , there seems to be no signi cant bene t in including the global context .
The reasons for this is left for future work .
? ? 21/02/2022 , 21:41 Day 118 : NLP Papers Summary - Extractive Summarization of Long Documents by Combining Global and L ?
https://ryanong.co.uk/2020/04/27/day-118-nlp-papers-summary-extractive-summarization-of-long-documents-by-combining-gl?
7/11 Conclusion and Future Work Potential future work would be to investigate methods to deal with redundancy .
We could also integrate traditional methods such as feature engineering ( e.g. sentence position , saliency ) into our neural models .
In addition , we could investigate a better structure in representing the document like discourse tree .
Evaluation on summarisation has been a big issue in summarisation .
ROUGE scores is n't as reliable as an evaluation metric and so it would be good ? ? 21/02/2022 , 21:41 Day 118 : NLP Papers Summary - Extractive Summarization of Long Documents by Combining Global and L ?
https://ryanong.co.uk/2020/04/27/day-118-nlp-papers-summary-extractive-summarization-of-long-documents-by-combining-gl?
8/11 Ryan Data Scientist You May Also Like to involve human evaluation in the future .
Finally , an integration of both extractive and abstractive summarisation approach is worth exploring in the future .
To compute the sentence embedding , we just use the simple method of averaging the word embeddings .
This has shown to be just as effective as RNN and CNN when computing sentence embeddings .
BERT sentence embedding performed poorly .
DOCUMENT ENCODER
The document encoder is a biGRU , which means that for each sentence there are two hidden states , a forward ( yellow ) and a backward hidden state ( blue ) .
This document encoder creates
Experimental Setup and Results
There are two evaluation datasets : arXiv and PubMed .
We will be using ROUGE and METEOR as automatic evaluation metrics to compare our models with previous abstractive and extractive models .
MODELS COMPARISON
The models for comparison are split into different categories :
1 . First to use LSTM - minus for text summarisation 2 .
The model achieved SOTA results on ArXiv and PubMed datasets and has shown to have an increase in performance as document length increase 3 . Found that the performance of the model can be mainly attributed to the modelling of NLP Papers Summary - Extractive Summarization of Long Documents by Combining Global and L ?
https://ryanong.co.uk/2020/04/27/day-118-nlp-papers-summary-extractive-summarization-of-long-documents-by-combining-gl?
3/11
, the sentence representation would simply be the concatenation of both the forward and backward hidden states .
For document representation , it would be the concatenation of the nal end state of both the forward and backward hidden states ( in reds ) .
For topic segment representation , we used LSTM - Minus .
LSTM - Minus is used for learning text span embeddings .
It was rst proposed for dependency parsing , where a sentence is divided into three segments ( pre x , in x , and suf x ) and LSTM - Minus is used to create embeddings for each segment .
It works as follows :
1 . Apply an LSTM to the whole sentence to obtain hidden states of each word in the sentence 2 .
To create embeddings for each segment ( from word i to word j ) , compute the difference between hidden state i and hidden state j .
The idea behind this is that , given the nature LSTM , each hidden state contains information from previous hidden states plus current word , which allows the model to create segment embeddings using information outside and inside the segments 3 . The nal topic representation is the concatenation of the forward and backward segment embeddings ( Detail of C in the gure above ) SENTENCE CLASSIFIER
With the sentence and document encoder , we now have sentence representations , document representation , and topic segment representations .
These three representations are feed into ? ?
21/02/2022 , 21:41 Day 118 : NLP Papers Summary - Extractive Summarization of Long Documents by Combining Global and L ?
https://ryanong.co.uk/2020/04/27/day-118-nlp-papers-summary-extractive-summarization-of-long-documents-by-combining-gl?
4/11 the multi-layer perceptron ( MLP ) to predict whether the sentence should be included in the summary .
Two methods of combining these three representations have been explored : 1 . Concatenation .
Simply concatenate all three representations together to form the nal representation 2 . Attentive Context .
This is where we compute the weighted context vector for each sentence , allowing the model to learn how much weight to focus on the document and topic segment representations .
This weighted context vector is concatenated with the respective sentence representation and feed into the MLP with sigmoid activation function to determine if the sentence should be included in the summary
1 . Traditional extractive models .
SumBasic , LSA , and LexRank 2 .
Neural abstractive models .
Attention Seq2Seq , Pointer-Generator Network , and Discourseaware 3 . Neural extractive models .
Cheng & Lapata and SummaRuNNer 4 . Baseline , Lead , and Oracle .
Baseline is only feed sentence representation to MLP ( without local or global context ) , lead is the rst k words , and oracle measures the upper bound of the model 's performance using ground - truth extractive labels RESULTS ? ? 21/02/2022 , 21:41 Day 118 : NLP Papers Summary - Extractive Summarization of Long Documents by Combining Global and L ?
https://ryanong.co.uk/2020/04/27/day-118-nlp-papers-summary-extractive-summarization-of-long-documents-by-combining-gl?
5 ROUGE -1 and ROUGE - 2 by a huge margin .
Results on ROUGE -L are mixed which could due to the fact that our models are trained on ROUGE - 1 ground - truth extractive labels .
The Discourse- aware abstractive model outperformed all the extractive models in ROUGE -L and this could due to them being trained on abstract summaries directly .
? ?
