title
Day 174 : NLP Papers Summary - PEGASUS : Pre-Training With Extracted Gap-Sentences For Abstractive Summarization
abstract
day-174-nlp-papers-summary-pegasus-pre-training-with-extracted-gap-sentences-for-abstra?
1/12
https://ryanong.co.uk/2020/06/22/day-174-nlp-papers-summary-pegasus-pre-training-with-extracted-gap-sentences-for-abstra?
3/12
GAP-SENTENCE GENERATION ( GSG )
We believe that using a pre-training objective that 's more inline with the downstream task would lead to better and faster ne-tuning performance .
Our new pre-training objective GSG select and mask whole sentences from documents .
Those masked sentences would then be concatenated to form pseudo summaries .
We also computed the gap sentences ratio which computes the number of selected gap sentences over the total number of sentences in the document .
We experimented with three strategies to select gap sentences ( without We evaluated six variants of GSG with 30 % GSR .
Figure 4a suggests that independently scoring sentences using the original method ( ind-orig ) achieved the best performance .
Seq-uniq achieved the second best performance .
Random and lead method underperform consistently against all the variants of principal method .
As expected , lead method performed well on the two news datasets but performed badly in the two non-news datasets .
We selected principal method , speci cally ind-orig variant to train our PEGASUS - LARGE .
In addition , we also evaluated whether we show train MLM with or without GSG .
Figure 4a shows that MLM does n't improve the performance of Ind -Orig variant and also does n't improve ne-tuning performance with long training steps and so we decided to exclude the MLM .
An important hyperparameter of GSG is GSR , which determines how many sentences to mask .
We compared the performance of our downstream datasets across different level of GSR .
Figure 4 b shown mix results in terms of the optimal GSR , however , the best performance GSR is always under 50 % .
We decided to choose the GSR of 30 % when scaling to PEGASUS - LARGE .
Overall the ablations study allows us to set the best choices to scale our PEGASUS model .
Our
PEGASUS -LARGE uses GSG ( Ind - Orig ) without MLM as pre-training objective , SentencePiece Unigram vocabulary size of 96K , and pre-train on C4 and HugeNews corpus separately .
In addition , to encourage our PEGASUS to copy , 20 % of selected sentences were left unchanged instead of mask in the original input .
This also led us to increasing GSR to 45 % in order to achieve the optimal GSR ratio found in our ablation studies .
Results
Table 1 showcase the performance improvements between PEGASUS - BASE and PEGASUS
HUMAN EVALUATION
The table below displayed the results of our human evaluation comparing generated and human-written summaries .
Human are asked to rate summaries on 1 - 5 , with higher being better .
Our results show that both our PEGASUS - LARGE pretrained on C4 and HugeNews respectively were able to generate summaries as good as the reference summaries .
In addition , even with low ne-tuning samples , PEGASUS - LARGE was able to perform relatively well when compared to human summaries .
Conclusion and Future Work selecting top-m scored sentences based on their importance .
Our proxy for importance is measured by the ROUGE - 1 score between the sentence and the rest of ? ? 21/02/2022 , 21:37 Day 174 : NLP Papers Summary - PEGASUS : Pre-training with Extracted Gap-sentences for Abstractive Sum ?
https://ryanong.co.uk/2020/06/22/day-174-nlp-papers-summary-pegasus-pre-training-with-extracted-gap-sentences-for-abstra?
4/12 the document .
There are four variants to the principal strategy : ABLATIONS ON PEGASUS - BASE
We use PEGASUS - BASE and normalised ROUGE scores to determine the optimal choices of pre-training corpus , objective , and vocabulary size .
The results are displayed in the three gures below .
Firstly , we found that pre-training language models with similar domain datasets as the ? ? 21/02/2022 , 21:37 Day 174 : NLP Papers Summary - PEGASUS : Pre-training with Extracted Gap-sentences for Abstractive Sum ?
https://ryanong.co.uk/2020/06/22/day-174-nlp-papers-summary-pegasus-pre-training-with-extracted-gap-sentences-for-abstra?
5/12 downstream tasks yield better summarisation results .
As shown in gure 3 , pre-training with HugeNews corpus led to better performance in downstream datasets of XSum and CNN / DM as they are both news datasets whereas both WikiHow and Reddit TIFU bene t more from pretraining language model with C4 dataset .
