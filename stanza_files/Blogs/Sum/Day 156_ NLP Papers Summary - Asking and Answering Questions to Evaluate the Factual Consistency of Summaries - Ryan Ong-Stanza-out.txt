title
abstract
https://ryanong.co.uk/2020/06/04/day-156-nlp-papers-summary-asking-and-answering-questions-to-evaluate-the-factual-consi?
2/11 interpretability as to which part of the summary are factual inconsistent using the answers and questions generated .
removing duplicates and questions with three tokens or less .
We also feed the questions into the QA model and remove questions that are predicted with no answer .
And so , in this step , we generated K questions based on the summary .
QAGS Framework
QUESTION ANSWERING
Here , we have an extractive QA models to extract the answers as text spans from the source document and summary .
Future work could experiment with abstractive QA models .
In this step , we answer those generated questions using both the source and summary to obtain two sets of answers .
ANSWER SIMILARITY
Here , we have a simple token - level F1 score to compare the answers and measure answer similarity .
In this nal step , we compare the answers using the similarity metric and averaging the answer similarity score over all questions .
Experiments
We have two evaluation datasets : CNN / DM and XSUM .
We measured the correlations between QAGS and human judgements of factual consistency .
For each summary , we collected 3 annotations and obtain a single correctness score per summary by taking the majority vote for each sentence and averaging the binary scores across summary sentences .
We compared our QAGS metric with other common summarisation metrics such as ROUGE , METEOR , BLEU , and BERTScore .
Results
The table below showcase the correlation between different evaluation metrics and human judgements of factual consistency .
We show that QAGS achieved the highest correlation by a substantial margin .
QAGS performed 2x better than the next best performing metric .
QAGS scored signi cantly lower in XSUM but still outperformed other metrics by a wide margin .
This showcase that XSUM dataset is more abstractive .
DOMAIN EFFECTS
The QAGS framework requires labelled data to train both QG and QA models .
This might be effective in a data rich domain but in niche domains , we might not have access to labelled data .
In those situations , we are forced to use out - of- domain data to train our models which may negatively impact our QAGS quality due to domain shift .
We assess the impact of this domain shift by training our QG model using SQUAD which it 's a collection of wikipedia articles rather than CNN articles .
The new correlations score with SQUAD - QG model is 51.53 and 15.28 on CNN / DM and XSUM dataset respectively .
This is lower than the correlation scores when using NewsQA - QG model but it still signi cantly outperformed other evaluation metrics .
NUMBER OF QUESTIONS
Lastly , we explore how the number of questions would affect the correlation with human judgements .
The results are showcase below and it shows that as the number of questions increase , we see a consistent increase in correlation scores in both evaluation datasets .
We also observed that a ) with only 5 questions , we are able to achieve correlations higher than other evaluation metrics and b ) there is only a small increase in correlation scores when increasing number of questions from 20 to 50 , showcasing decreasing marginal bene t of including more than 50 questions .
To determine the quality of our generated questions , article and summary answers , we manually annotated 400 triplets on the XSUM summaries and label them by their quality .
We found that 8.75 % of generated questions are nonsense and 3 % are well - formed but could n't be answer by the generated summary .
This shows that a large proportion of our generated questions are easy to understanding , meaningful , and relatable .
8.25 % of questions are wellformed but could n't be answer by the source document , largely due to non-sensical facts that QG model turns into questions .
We have a large 32.50 % of incorrectly answers using the source article , indicating that our QA model is weak .
Finally , 8 % of questions are answered correctly using both source article and summary but due to little or no overlap in tokens , it was identify as incorrect .
Conclusion and Future Work similarity is computed based on how similar the answers are QUESTION GENERATION
We train a seq2seq model to generate questions based on both the answer and source article .
We over-sample questions and use different methods to lter out low-quality questions such as ? ? 21/02/2022 , 21:38 Day 156 : NLP Papers Summary - Asking and Answering Questions to Evaluate the Factual Consistency of S ?
https://ryanong.co.uk/2020/06/04/day-156-nlp-papers-summary-asking-and-answering-questions-to-evaluate-the-factual-consi?
3/11
