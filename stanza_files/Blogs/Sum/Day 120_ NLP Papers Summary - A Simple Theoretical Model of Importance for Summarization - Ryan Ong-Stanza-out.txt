title
Day 120 : NLP Papers Summary - A Simple Theoretical Model Of Importance For Summarization Objective and Contribution
abstract
Proposed a simple theoretical model to capture the information importance in summarisation .
The model captures redundancy , relevance , and informativeness , all three of which contributes to the information importance in summarisation .
We showcase how someone could use this framework to guide and improve summarisation systems .
The contributions are as follows : 1 . De ne three key concepts in summarisation : redundancy , relevance , and informativeness ?
Natural Language Processing 365 ? ? ? ? ? ? ? Loading [ MathJax ] / extensions / MathZoom.js
2 . Formulate the Importance concept using the three key concepts in summarisation and how to interpret the results
Showed that our theoretical model of importance for summarisation has a good correlation with human summarisation , making it useful for guiding future empirical works
The Overall Framework Semantic unit is considered a small piece of information .
represents all the possible semantic units .
A text input X is considered to be made up of many semantic units and so can be represented by a probability distribution over . can simply means the frequency distribution of semantic units in the overall text .
can be interpreted as the probability that the semantic unit appears in text X or it could be interepreted as the contribution of to the overall meaning of text X .
REDUNDANCY
The level of information presented in a summary is measured by entropy as follows :
Entropy measures the coverage level and H ( S ) is maximised when every semantic unit in the summary only appears once and so the Redundancy formula is as follows :
RELEVANCE
A relevant summary should be one that closely approximates the original text .
In other words , a relevant summary should have the minimum loss of information .
For us to measure relevancy , we would need to compare the probability distributions of the source document and summary using cross-entropy as follows :
The formula is seen as the average surprise of producing S summary when expecting D source
INFORMATIVENESS P K I nf ( S , K ) = C E( S , K ) I nf ( S , K ) = ? ( ) xlog ( ( ) ) ? w i P S w i P K w i f ( , ) d i k i = ( ) d i P D w i ? ? Loading [ MathJax ] /
CORRELATION WITH HUMAN JUDGEMENTS
We assess how well our quantities correlate with human judgements .
Each quantity of our framework can be used to score sentences for summary and so we can evaluate how well they
COMPARISON WITH REFERENCE SUMMARIES
Ideally we would want our generated summaries ( using ) to be similar to human reference summaries ( ) .
We scored both summaries using and found that human reference summaries scored signi cantly higher than our generated summaries , proving the reliability of our scoring function .
Conclusion and Future Work S is measured by the cross entropy between the summary and background knowledge as follows :
The cross entropy for relevance should be low as we want the summary to be as similar and relevant to the source document whereas the cross entropy for informativeness should be high as we are measuring the amount of background knowledge we used to generate the summary .
This introduction of background knowledge allows us to customisethe model depending on what kind of knowledge we want to include , whether that be domain-speci c knowledge or user-speci c knowledge or general knowledge .
It also introduces the notion of update summarisation .
Update summarisation involves summarising source document D having already seen document / summary U. Document / summary U could be modelled by background knowledge K , which makes U a previous knowledge .
IMPORTANCE
Importance is the metric that guides what information should be included in the summary .
Given a user with knowledge K , the summary should be generated with the objective to bring the most new information to the user .
Therefore , for each semantic unit , we need a function that takes in the probability of semantic unit in source document D ( ) and K L( S | |D ) = C E( S , D ) -H ( S ) ?K L( S | |D ) = Rel ( S , D ) -Red ( S )
