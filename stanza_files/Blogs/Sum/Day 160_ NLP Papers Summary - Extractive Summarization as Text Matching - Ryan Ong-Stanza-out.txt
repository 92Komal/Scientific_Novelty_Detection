title
Day 160 : NLP Papers Summary - Extractive Summarization As Text Matching Objective and Contribution
abstract
We tackle extractive summarisation task as a semantic text matching problem rather than the common used sequence labelling problem .
We proposed MATCHSUM , a novel summary - level framework that uses Siamese - BERT to match source document and candidate summaries in the semantic space .
The idea is that a good summary should be semantically similar to the source document as a whole .
This method achieved 44.41 ROUGE - 1 score in CNN / DM dataset ?
Natural Language Processing 365 ? ? ? ? ? ? ? 21/02/2022 , 21:38 Day 160 : NLP Papers Summary - Extractive Summarization as Text Matching - Ryan Ong https://ryanong.co.uk/2020/06/08/day-160-nlp-papers-summary-extractive-summarization-as-text-matching/ 2/11
and achieved similar results in other evaluation datasets .
We also analyse the performance difference between sentence - level and summary - level extractive models .
Datasets
We have six evaluation datasets as shown below and our evaluation metrics are ROUGE -1 , ROUGE - 2 , and ROUGE -L.
MATCHSUM SIAMESE-BERT
We use siamese - BERT , which consists of two BERTs with the same weight and a cosine similarity layer , to match document and candidate summary .
We use BERT to encode both the document and candidate summaries and compute the similarity between the two embeddings .
The basic idea is that gold summary has the highest matching score to the source document and good candidate summary should obtain high score .
CANDIDATES PRUNING
To avoid having to score all possible candidates , we use a simple candidate pruning strategy .
Sentence-level vs Summary - level Extractive Summarisation
Speci cally , we used a content
Results
The results for CNN / DM are displayed below .
As shown , our MATCHSUM outperformed all the other baseline models .
We observed the best performance is achieved when when we change the encoder to RoBERTa - base .
We believe this is due to RoBERTa was pre-trained using 63 million news articles .
Conclusion and Future Work
In the future , we could work on different forms of matching models to further explore the performance of the proposed framework .
In addition , a better understanding of the characteristics of the datasets we are dealing with would give us an advantage over which types of models to use .
