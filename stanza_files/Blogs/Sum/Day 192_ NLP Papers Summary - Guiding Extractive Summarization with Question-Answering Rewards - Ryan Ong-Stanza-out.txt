title
Data Science Natural Language Processing NLP Papers Summary
abstract
Objective and Contribution Proposed an extractive summarisation model with question - answering rewards as we believe that informative summaries should include answers to important questions .
Our generated summaries yielded competitive results as measured by automatic metrics and human assessors .
To create question answer ( QA ) pairs , we limit our answer token to either be a salient word or named entity .
We identify salient word or named entity in all the sentences in the human abstract and replace the answer token with a blank to create Cloze-style QA pair .
Note that at least one QA pair should be extracted from each sentence of the abstract so that our summary includes all the useful content to answer all the questions .
Overall we know have a set of QA pairs extracted from the human abstract and we can train our LSTM and attention mechanism to answer these questions using the source document .
We believe that extracting summary chunks rather than sentence level is key to building a concise summary but it does makes the summarisation task more challenging as the search space is larger .
We also observed that the ROOT - type QA pairs have the least number of unique answers .
Our QASumm + ROOT performed the best amongst the variant in daily mail dataset and QASumm + NER performed the best in CNN dataset .
We suspect that maintaining a good number of unique answers is important to maximise performance .
A REINFORCEMENT LEARNING FRAMEWORK
EXTRACTION UNITS
We want to nd out whether words or chunks are better as the extraction units .
We compared the performance of our LSTM and CNN encoder and found that chunks with LSTM performed the best and chunks with CNN outperformed LSTM and CNN with words .
HUMAN EVALUATION
Each participant is given the document and three ll- in- the - blank questions .
The answer tokens is chosen randomly and can be root word , the subj / obj word , or NER word .
We asked the participants to rate the informativeness of the summary from 1 - 5 , 5 being the most informative .
We evaluated the summaries from our models and PG network .
The table below showcase the average time it takes to complete a single question , the overall accuracy , and the informativeness score .
Excluding human performance , our QASumm with NER - type QA pairs was able to achieved the highest accuracy and informativeness .
We found that our best performing model has a wide margin in QA accuracy despite similar level of informativeness score .
Conclusion and Future Work NLP Papers Summary - Guiding Extractive Summarization with Question - Answering Rewards - Ry ?
https://ryanong.co.uk/2020/07/10/day-192-nlp-papers-summary-guiding-extractive-summarization-with-question-answering-re?
3/10 4 . Reinforcement learning EXTRACTION UNIT
We experimented with words or chunks ( phrases ) as extraction units .
We obtain text chunks using the sentence constituent parse tree and each chunk has at most 5 words .
Note that we did not experiment with sentence level extraction like most existing work .
Instead , we focused on ner-grained extraction units .
We experimented with CNN and biLSTM to encode these extraction units .
CONSTRUCTING EXTRACTIVE SUMMARY
We need to identify text segments from source articles to form our extractive summary and this can be seen as a sequence labelling problem .
We decided to use the framework whereby the importance of the t-th source extraction unit is determined by its informativeness , its position in the document , and the relationship with the previously selected extraction units .
We have positional embeddings to encode the position of the extraction unit .
At each time step , we build the vector representation of our summary up to time t - 1 and used it along with positional embeddings and our encoded hidden states to determine whether we should include the new extraction unit .
The architecture for this is an unidirectional LSTM as shown below .
NLP Papers Summary - Guiding Extractive Summarization with Question - Answering Rewards - Ry ?
https://ryanong.co.uk/2020/07/10/day-192-nlp-papers-summary-guiding-extractive-summarization-with-question-answering-re?
4/10
